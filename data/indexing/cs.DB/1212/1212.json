[{"id": "1212.0017", "submitter": "Sabeur Aridhi", "authors": "Sabeur Aridhi, Laurent d'Orazio, Mondher Maddouri and Engelbert Mephu\n  Nguifo", "title": "A large-scale and fault-tolerant approach of subgraph mining using\n  density-based partitioning", "comments": "The paper is under reviewing and we want to cancel the submission.\n  Thank you for your understanding", "journal-ref": null, "doi": "10.1016/j.is.2013.08.005", "report-no": null, "categories": "cs.DB cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph mining approaches have become very popular, especially in\ndomains such as bioinformatics, chemoinformatics and social networks. In this\nscope, one of the most challenging tasks is frequent subgraph discovery. This\ntask has been motivated by the tremendously increasing size of existing graph\ndatabases. Since then, an important problem of designing efficient and scaling\napproaches for frequent subgraph discovery in large clusters, has taken place.\nHowever, failures are a norm rather than being an exception in large clusters.\nIn this context, the MapReduce framework was designed so that node failures are\nautomatically handled by the framework. In this paper, we propose a large-scale\nand fault-tolerant approach of subgraph mining by means of a density-based\npartitioning technique, using MapReduce. Our partitioning aims to balance\ncomputation load on a collection of machines. We experimentally show that our\napproach decreases significantly the execution time and scales the subgraph\ndiscovery process to large graph databases.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 21:17:59 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 13:37:24 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Aridhi", "Sabeur", ""], ["d'Orazio", "Laurent", ""], ["Maddouri", "Mondher", ""], ["Nguifo", "Engelbert Mephu", ""]]}, {"id": "1212.0190", "submitter": "Xu He", "authors": "Xu He, Fan Min, William Zhu", "title": "A Comparative Study of Discretization Approaches for Granular\n  Association Rule Mining", "comments": "17 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1210.0065", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granular association rule mining is a new relational data mining approach to\nreveal patterns hidden in multiple tables. The current research of granular\nassociation rule mining considers only nominal data. In this paper, we study\nthe impact of discretization approaches on mining semantically richer and\nstronger rules from numeric data. Specifically, the Equal Width approach and\nthe Equal Frequency approach are adopted and compared. The setting of interval\nnumbers is a key issue in discretization approaches, so we compare different\nsettings through experiments on a well-known real life data set. Experimental\nresults show that: 1) discretization is an effective preprocessing technique in\nmining stronger rules; 2) the Equal Frequency approach helps generating more\nrules than the Equal Width approach; 3) with certain settings of interval\nnumbers, we can obtain much more rules than others.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2012 06:51:40 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 02:36:56 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2012 02:29:40 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["He", "Xu", ""], ["Min", "Fan", ""], ["Zhu", "William", ""]]}, {"id": "1212.0243", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Estimation for Monotone Sampling: Competitiveness and Customization", "comments": "28 pages; Improved write up, presentation in the context of the more\n  general monotone sampling formulation (instead of coordinated sampling).\n  Bounds on universal ratio removed to make the paper more focused, since it is\n  mainly of theoretical interest", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DB stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random samples are lossy summaries which allow queries posed over the data to\nbe approximated by applying an appropriate estimator to the sample. The\neffectiveness of sampling, however, hinges on estimator selection. The choice\nof estimators is subjected to global requirements, such as unbiasedness and\nrange restrictions on the estimate value, and ideally, we seek estimators that\nare both efficient to derive and apply and {\\em admissible} (not dominated, in\nterms of variance, by other estimators). Nevertheless, for a given data domain,\nsampling scheme, and query, there are many admissible estimators. We study the\nchoice of admissible nonnegative and unbiased estimators for monotone sampling\nschemes. Monotone sampling schemes are implicit in many applications of massive\ndata set analysis. Our main contribution is general derivations of admissible\nestimators with desirable properties. We present a construction of {\\em\norder-optimal} estimators, which minimize variance according to {\\em any}\nspecified priorities over the data domain. Order-optimality allows us to\ncustomize the derivation to common patterns that we can learn or observe in the\ndata. When we prioritize lower values (e.g., more similar data sets when\nestimating difference), we obtain the L$^*$ estimator, which is the unique\nmonotone admissible estimator. We show that the L$^*$ estimator is\n4-competitive and dominates the classic Horvitz-Thompson estimator. These\nproperties make the L$^*$ estimator a natural default choice. We also present\nthe U$^*$ estimator, which prioritizes large values (e.g., less similar data\nsets). Our estimator constructions are both easy to apply and possess desirable\nproperties, allowing us to make the most from our summarized data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2012 20:46:37 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2013 18:23:59 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2014 15:32:57 GMT"}, {"version": "v4", "created": "Wed, 9 Apr 2014 00:01:30 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1212.0254", "submitter": "Bruno Marnette", "authors": "Bruno Marnette", "title": "Resolution and Datalog Rewriting Under Value Invention and Equality\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper present several refinements of the Datalog +/- framework based on\nresolution and Datalog-rewriting. We first present a resolution algorithm which\nis complete for arbitrary sets of tgds and egds. We then show that a technique\nof saturation can be used to achieve completeness with respect to First-Order\n(FO) query rewriting. We then investigate the class of guarded tgds (with a\nloose definition of guardedness), and show that every set of tgds in this class\ncan be rewritten into an equivalent set of standard Datalog rules. On the\nnegative side, this implies that Datalog +/- has (only) the same expressive\npower as standard Datalog in terms of query answering. On the positive side\nhowever, this mean that known results and existing optimization techniques\n(such as Magic-Set) may be applied in the context of Datalog +/- despite its\nricher syntax.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2012 22:50:25 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Marnette", "Bruno", ""]]}, {"id": "1212.0317", "submitter": "M HM Krishna Prasad Dr", "authors": "B. Adinarayana Reddy, O. Srinivasa Rao and M. H. M. Krishna Prasad", "title": "An Improved UP-Growth High Utility Itemset Mining", "comments": "(0975 8887)", "journal-ref": "International Journal of Computer Applications Volume 58, No.2,\n  2012, 25-28", "doi": "10.5120/9255-3424", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient discovery of frequent itemsets in large datasets is a crucial task\nof data mining. In recent years, several approaches have been proposed for\ngenerating high utility patterns, they arise the problems of producing a large\nnumber of candidate itemsets for high utility itemsets and probably degrades\nmining performance in terms of speed and space. Recently proposed compact tree\nstructure, viz., UP Tree, maintains the information of transactions and\nitemsets, facilitate the mining performance and avoid scanning original\ndatabase repeatedly. In this paper, UP Tree (Utility Pattern Tree) is adopted,\nwhich scans database only twice to obtain candidate items and manage them in an\nefficient data structured way. Applying UP Tree to the UP Growth takes more\nexecution time for Phase II. Hence this paper presents modified algorithm\naiming to reduce the execution time by effectively identifying high utility\nitemsets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 08:50:50 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Reddy", "B. Adinarayana", ""], ["Rao", "O. Srinivasa", ""], ["Prasad", "M. H. M. Krishna", ""]]}, {"id": "1212.0610", "submitter": "Keke Chen", "authors": "Huiqi Xu, Shumin Guo, Keke Chen", "title": "Building Confidential and Efficient Query Services in the Cloud with\n  RASP Data Perturbation", "comments": "18 pages, to appear in IEEE TKDE, accepted in December 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide deployment of public cloud computing infrastructures, using\nclouds to host data query services has become an appealing solution for the\nadvantages on scalability and cost-saving. However, some data might be\nsensitive that the data owner does not want to move to the cloud unless the\ndata confidentiality and query privacy are guaranteed. On the other hand, a\nsecured query service should still provide efficient query processing and\nsignificantly reduce the in-house workload to fully realize the benefits of\ncloud computing. We propose the RASP data perturbation method to provide secure\nand efficient range query and kNN query services for protected data in the\ncloud. The RASP data perturbation method combines order preserving encryption,\ndimensionality expansion, random noise injection, and random projection, to\nprovide strong resilience to attacks on the perturbed data and queries. It also\npreserves multidimensional ranges, which allows existing indexing techniques to\nbe applied to speedup range query processing. The kNN-R algorithm is designed\nto work with the RASP range query algorithm to process the kNN queries. We have\ncarefully analyzed the attacks on data and queries under a precisely defined\nthreat model and realistic security assumptions. Extensive experiments have\nbeen conducted to show the advantages of this approach on efficiency and\nsecurity.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 04:08:09 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2013 15:00:50 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Xu", "Huiqi", ""], ["Guo", "Shumin", ""], ["Chen", "Keke", ""]]}, {"id": "1212.0763", "submitter": "Modou Gueye M.", "authors": "Modou Gueye, Talel Abdessalem, Hubert Naacke", "title": "Dynamic recommender system : using cluster-based biases to improve the\n  accuracy of the predictions", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is today accepted that matrix factorization models allow a high quality of\nrating prediction in recommender systems. However, a major drawback of matrix\nfactorization is its static nature that results in a progressive declining of\nthe accuracy of the predictions after each factorization. This is due to the\nfact that the new obtained ratings are not taken into account until a new\nfactorization is computed, which can not be done very often because of the high\ncost of matrix factorization.\n  In this paper, aiming at improving the accuracy of recommender systems, we\npropose a cluster-based matrix factorization technique that enables online\nintegration of new ratings. Thus, we significantly enhance the obtained\npredictions between two matrix factorizations. We use finer-grained user biases\nby clustering similar items into groups, and allocating in these groups a bias\nto each user. The experiments we did on large datasets demonstrated the\nefficiency of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 13:00:27 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Gueye", "Modou", ""], ["Abdessalem", "Talel", ""], ["Naacke", "Hubert", ""]]}, {"id": "1212.0967", "submitter": "Sameer Singh", "authors": "Sameer Singh and Thore Graepel", "title": "Compiling Relational Database Schemata into Probabilistic Graphical\n  Models", "comments": "NIPS 2012 Workshop on Probabilistic Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Instead of requiring a domain expert to specify the probabilistic\ndependencies of the data, in this work we present an approach that uses the\nrelational DB schema to automatically construct a Bayesian graphical model for\na database. This resulting model contains customized distributions for columns,\nlatent variables that cluster the data, and factors that reflect and represent\nthe foreign key links. Experiments demonstrate the accuracy of the model and\nthe scalability of inference on synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 08:52:33 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Singh", "Sameer", ""], ["Graepel", "Thore", ""]]}, {"id": "1212.1046", "submitter": "Yuqing  Zhu", "authors": "Yuqing Zhu, Philip S. Yu, Jianmin Wang", "title": "Latency Bounding by Trading off Consistency in NoSQL Store: A Staging\n  and Stepwise Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latency is a key service factor for user satisfaction. Consistency is in a\ntrade-off relation with operation latency in the distributed and replicated\nscenario. Existing NoSQL stores guarantee either strong or weak consistencies\nbut none provides the best consistency based on the response latency. In this\npaper, we introduce dConssandra, a NoSQL store enabling users to specify\nlatency bounds for data access operations. dConssandra dynamically bounds data\naccess latency by trading off replica consistency. dConssandra is based on\nCassandra. In comparison to Cassandra's implementation, dConssandra has a\nstaged replication strategy enabling synchronous or asynchronous replication on\ndemand. The main idea to bound latency by trading off consistency is to\ndecompose the replication process into minute steps and bound latency by\nexecuting only a subset of these steps. dConssandra also implements a different\nin-memory storage architecture to support the above features. Experimental\nresults for dConssandra over an actual cluster demonstrate that (1) the actual\nresponse latency is bounded by the given latency constraint; (2) greater write\nlatency bounds lead to a lower latency in reading the latest value; and, (3)\ngreater read latency bounds lead to the return of more recently written values.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 15:16:30 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Zhu", "Yuqing", ""], ["Yu", "Philip S.", ""], ["Wang", "Jianmin", ""]]}, {"id": "1212.1469", "submitter": "Wendy Osborn", "authors": "Marc Moreau and Wendy Osborn", "title": "mqr-tree: A 2-dimensional Spatial Access Method", "comments": "Link to article:\n  http://www.scribd.com/doc/113916780/mqr-tree-A-2-dimensional-Spatial-Access-Method;\n  Journal of Computer Science and Engineering 15(2), October 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the mqr-tree, a two-dimensional spatial access\nmethod that organizes spatial objects in a two-dimensional node and based on\ntheir spatial relationships. Previously proposed spatial access methods that\nattempt to maintain spatial relationships between objects in their structures\nare limited in their incorporation of existing one-dimensional spatial access\nmethods, or have lower space utilization in its nodes, and higher tree height,\novercoverage and overlap than is necessary. The mqr-tree utilizes a node\norganization, set of spatial relationship rules and insertion strategy in order\nto gain significant improvements in overlap and overcoverage. In addition,\nother desirable properties are identified as a result of the chosen node\norganization and insertion strategies. In particular, zero overlap is achieved\nwhen the mqr-tree is used to index point data. A comparison of the mqr-tree\ninsertion strategy versus the R-tree shows significant improvements in overlap\nand overcoverage, with comparable space utilization. In addition, a comparison\nof region searching shows that the mqr-tree achieves a lower number of disk\naccesses in many cases\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 21:05:47 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Moreau", "Marc", ""], ["Osborn", "Wendy", ""]]}, {"id": "1212.1881", "submitter": "Georg Gottlob", "authors": "Georg Gottlob", "title": "Deciding Monotone Duality and Identifying Frequent Itemsets in Quadratic\n  Logspace", "comments": "Preprint of a paper which appeared in: Proceedings of the 32nd ACM\n  SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS 2013,\n  New York, NY,USA, June 22-27,2013, pp.25-36", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monotone duality problem is defined as follows: Given two monotone\nformulas f and g in iredundant DNF, decide whether f and g are dual. This\nproblem is the same as duality testing for hypergraphs, that is, checking\nwhether a hypergraph H consists of precisely all minimal transversals of a\nsimple hypergraph G. By exploiting a recent problem-decomposition method by\nBoros and Makino (ICALP 2009), we show that duality testing for hypergraphs,\nand thus for monotone DNFs, is feasible in DSPACE[log^2 n], i.e., in quadratic\nlogspace. As the monotone duality problem is equivalent to a number of problems\nin the areas of databases, data mining, and knowledge discovery, the results\npresented here yield new complexity results for those problems, too. For\nexample, it follows from our results that whenever for a Boolean-valued\nrelation (whose attributes represent items), a number of maximal frequent\nitemsets and a number of minimal infrequent itemsets are known, then it can be\ndecided in quadratic logspace whether there exist additional frequent or\ninfrequent itemsets.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2012 11:36:09 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2013 18:16:02 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2013 16:55:31 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Gottlob", "Georg", ""]]}, {"id": "1212.2071", "submitter": "Youssef Bassil", "authors": "Youssef Bassil", "title": "A Data Warehouse Design for a Typical University Information System", "comments": "LACSC - Lebanese Association for Computational Sciences,\n  http://www.lacsc.org", "journal-ref": "Journal of Computer Science & Research (JCSCR), vol. 1, no. 6, pp.\n  12-17, 2012", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presently, large enterprises rely on database systems to manage their data\nand information. These databases are useful for conducting daily business\ntransactions. However, the tight competition in the marketplace has led to the\nconcept of data mining in which data are analyzed to derive effective business\nstrategies and discover better ways in carrying out business. In order to\nperform data mining, regular databases must be converted into what so called\ninformational databases also known as data warehouse. This paper presents a\ndesign model for building data warehouse for a typical university information\nsystem. It is based on transforming an operational database into an\ninformational warehouse useful for decision makers to conduct data analysis,\npredication, and forecasting. The proposed model is based on four stages of\ndata migration: Data extraction, data cleansing, data transforming, and data\nindexing and loading. The complete system is implemented under MS Access 2010\nand is meant to serve as a repository of data for data mining operations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 14:09:03 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Bassil", "Youssef", ""]]}, {"id": "1212.2251", "submitter": "Sudeepa Roy", "authors": "Susan B. Davidson, Tova Milo and Sudeepa Roy", "title": "A Propagation Model for Provenance Views of Public/Private Workflows", "comments": "To appear in ICDT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of concealing functionality of a proprietary or private\nmodule when provenance information is shown over repeated executions of a\nworkflow which contains both `public' and `private' modules. Our approach is to\nuse `provenance views' to hide carefully chosen subsets of data over all\nexecutions of the workflow to ensure G-privacy: for each private module and\neach input x, the module's output f(x) is indistinguishable from G -1 other\npossible values given the visible data in the workflow executions. We show that\nG-privacy cannot be achieved simply by combining solutions for individual\nprivate modules; data hiding must also be `propagated' through public modules.\nWe then examine how much additional data must be hidden and when it is safe to\nstop propagating data hiding. The answer depends strongly on the workflow\ntopology as well as the behavior of public modules on the visible data. In\nparticular, for a class of workflows (which include the common tree and chain\nworkflows), taking private solutions for each private module, augmented with a\n`public closure' that is `upstream-downstream safe', ensures G-privacy. We\ndefine these notions formally and show that the restrictions are necessary. We\nalso study the related optimization problems of minimizing the amount of hidden\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 23:47:25 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Davidson", "Susan B.", ""], ["Milo", "Tova", ""], ["Roy", "Sudeepa", ""]]}, {"id": "1212.2287", "submitter": "Jimmy Lin", "authors": "Nima Asadi, Jimmy Lin, and Arjen P. de Vries", "title": "Runtime Optimizations for Prediction with Tree-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-based models have proven to be an effective solution for web ranking as\nwell as other problems in diverse domains. This paper focuses on optimizing the\nruntime performance of applying such models to make predictions, given an\nalready-trained model. Although exceedingly simple conceptually, most\nimplementations of tree-based models do not efficiently utilize modern\nsuperscalar processor architectures. By laying out data structures in memory in\na more cache-conscious fashion, removing branches from the execution flow using\na technique called predication, and micro-batching predictions using a\ntechnique called vectorization, we are able to better exploit modern processor\narchitectures and significantly improve the speed of tree-based models over\nhard-coded if-else blocks. Our work contributes to the exploration of\narchitecture-conscious runtime implementations of machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 03:20:46 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 16:33:08 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Asadi", "Nima", ""], ["Lin", "Jimmy", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "1212.2309", "submitter": "Ganzhao Yuan", "authors": "Ganzhao Yuan and Zhenjie Zhang and Marianne Winslett and Xiaokui Xiao\n  and Yin Yang and Zhifeng Hao", "title": "Low Rank Mechanism for Optimizing Batch Queries under Differential\n  Privacy", "comments": "VLDB 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a promising privacy-preserving paradigm for\nstatistical query processing over sensitive data. It works by injecting random\nnoise into each query result, such that it is provably hard for the adversary\nto infer the presence or absence of any individual record from the published\nnoisy results. The main objective in differentially private query processing is\nto maximize the accuracy of the query results, while satisfying the privacy\nguarantees. Previous work, notably \\cite{LHR+10}, has suggested that with an\nappropriate strategy, processing a batch of correlated queries as a whole\nachieves considerably higher accuracy than answering them individually.\nHowever, to our knowledge there is currently no practical solution to find such\na strategy for an arbitrary query batch; existing methods either return\nstrategies of poor quality (often worse than naive methods) or require\nprohibitively expensive computations for even moderately large domains.\n  Motivated by this, we propose the \\emph{Low-Rank Mechanism} (LRM), the first\npractical differentially private technique for answering batch queries with\nhigh accuracy, based on a \\emph{low rank approximation} of the workload matrix.\nWe prove that the accuracy provided by LRM is close to the theoretical lower\nbound for any mechanism to answer a batch of queries under differential\nprivacy. Extensive experiments using real data demonstrate that LRM\nconsistently outperforms state-of-the-art query processing solutions under\ndifferential privacy, by large margins.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 06:22:36 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Yuan", "Ganzhao", ""], ["Zhang", "Zhenjie", ""], ["Winslett", "Marianne", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Yin", ""], ["Hao", "Zhifeng", ""]]}, {"id": "1212.2338", "submitter": "Stephane Martin", "authors": "St\\'ephane Martin (INRIA Nancy - Grand Est / LORIA), Mehdi Ahmed-Nacer\n  (INRIA Nancy - Grand Est / LORIA), Pascal Urso (INRIA Nancy - Grand Est /\n  LORIA)", "title": "Controlled conflict resolution for replicated document", "comments": "Controlled conflict resolution for replicated document (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative working is increasingly popular, but it presents challenges due\nto the need for high responsiveness and disconnected work support. To address\nthese challenges the data is optimistically replicated at the edges of the\nnetwork, i.e. personal computers or mobile devices. This replication requires a\nmerge mechanism that preserves the consistency and structure of the shared data\nsubject to concurrent modifications. In this paper, we propose a generic design\nto ensure eventual consistency (every replica will eventually view the same\ndata) and to maintain the specific constraints of the replicated data. Our\nlayered design provides to the application engineer the complete control over\nsystem scalability and behavior of the replicated data in face of concurrent\nmodifications. We show that our design allows replication of complex data types\nwith acceptable performances.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 09:02:45 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Martin", "St\u00e9phane", "", "INRIA Nancy - Grand Est / LORIA"], ["Ahmed-Nacer", "Mehdi", "", "INRIA Nancy - Grand Est / LORIA"], ["Urso", "Pascal", "", "INRIA Nancy - Grand Est /\n  LORIA"]]}, {"id": "1212.3357", "submitter": "Andrea Cal\\`i PhD", "authors": "Andrea Cali, Georg Gottlob and Michael Kifer", "title": "Taming the Infinite Chase: Query Answering under Expressive Integrity\n  Constraints", "comments": "Pre-print", "journal-ref": "Journal of Artificial Intelligence Research, vol. 48, pp. 115-174,\n  2013", "doi": "10.1613/jair.3873", "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chase algorithm is a fundamental tool for query evaluation and query\ncontainment under constraints, where the constraints are (sub-classes of)\ntuple-generating dependencies (TGDs) and equality generating depencies (EGDs).\nSo far, most of the research on this topic has focused on cases where the chase\nprocedure terminates, with some notable exceptions. In this paper we take a\ngeneral approach, and we propose large classes of TGDs under which the chase\ndoes not always terminate. Our languages, in particular, are inspired by\nguarded logic: we show that by enforcing syntactic properties on the form of\nthe TGDs, we are able to ensure decidability of the problem of answering\nconjunctive queries despite the non-terminating chase. We provide tight\ncomplexity bounds for the problem of conjunctive query evaluation for several\nclasses of TGDs. We then introduce EGDs, and provide a condition under which\nEGDs do not interact with TGDs, and therefore do not take part in query\nanswering. We show applications of our classes of constraints to the problem of\nanswering conjunctive queries under F-Logic Lite, a recently introduced\nontology language, and under prominent tractable Description Logics languages.\nAll the results in this paper immediately extend to the problem of conjunctive\nquery containment.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 22:17:47 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2013 22:57:37 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Cali", "Andrea", ""], ["Gottlob", "Georg", ""], ["Kifer", "Michael", ""]]}, {"id": "1212.3480", "submitter": "Jens Dittrich", "authors": "Stefan Richter, Jorge-Arnulfo Quian\\'e-Ruiz, Stefan Schuh, Jens\n  Dittrich", "title": "Towards Zero-Overhead Adaptive Indexing in Hadoop", "comments": "Tech Report, Saarland University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several research works have focused on supporting index access in MapReduce\nsystems. These works have allowed users to significantly speed up selective\nMapReduce jobs by orders of magnitude. However, all these proposals require\nusers to create indexes upfront, which might be a difficult task in certain\napplications (such as in scientific and social applications) where workloads\nare evolving or hard to predict. To overcome this problem, we propose LIAH\n(Lazy Indexing and Adaptivity in Hadoop), a parallel, adaptive approach for\nindexing at minimal costs for MapReduce systems. The main idea of LIAH is to\nautomatically and incrementally adapt to users' workloads by creating clustered\nindexes on HDFS data blocks as a byproduct of executing MapReduce jobs. Besides\ndistributing indexing efforts over multiple computing nodes, LIAH also\nparallelises indexing with both map tasks computation and disk I/O. All this\nwithout any additional data copy in main memory and with minimal\nsynchronisation. The beauty of LIAH is that it piggybacks index creation on map\ntasks, which read relevant data from disk to main memory anyways. Hence, LIAH\ndoes not introduce any additional read I/O-costs and exploit free CPU cycles.\nAs a result and in contrast to existing adaptive indexing works, LIAH has a\nvery low (or invisible) indexing overhead, usually for the very first job.\nStill, LIAH can quickly converge to a complete index, i.e. all HDFS data blocks\nare indexed. Especially, LIAH can trade early job runtime improvements with\nfast complete index convergence. We compare LIAH with HAIL, a state-of-the-art\nindexing technique, as well as with standard Hadoop with respect to indexing\noverhead and workload performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 14:31:24 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Richter", "Stefan", ""], ["Quian\u00e9-Ruiz", "Jorge-Arnulfo", ""], ["Schuh", "Stefan", ""], ["Dittrich", "Jens", ""]]}, {"id": "1212.3501", "submitter": "Martin Schuster", "authors": "Henrik Bj\\\"orklund, Martin Schuster, Thomas Schwentick, Joscha\n  Kulbatzki", "title": "On optimum left-to-right strategies for active context-free games", "comments": "To appear in ICDT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active context-free games are two-player games on strings over finite\nalphabets with one player trying to rewrite the input string to match a target\nspecification. These games have been investigated in the context of exchanging\nActive XML (AXML) data. While it was known that the rewriting problem is\nundecidable in general, it is shown here that it is EXPSPACE-complete to decide\nfor a given context-free game, whether all safely rewritable strings can be\nsafely rewritten in a left-to-right manner, a problem that was previously\nconsidered by Abiteboul et al. Furthermore, it is shown that the corresponding\nproblem for games with finite replacement languages is EXPTIME-complete.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 15:28:45 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Bj\u00f6rklund", "Henrik", ""], ["Schuster", "Martin", ""], ["Schwentick", "Thomas", ""], ["Kulbatzki", "Joscha", ""]]}, {"id": "1212.5108", "submitter": "Florent Jacquemard", "authors": "Florent Jacquemard (Inria Paris-Rocquencourt, STMS), Michael\n  Rusinowitch (INRIA Lorraine - LORIA / LIFC)", "title": "Rewrite Closure and CF Hedge Automata", "comments": null, "journal-ref": "7th International Conference on Language and Automata Theory and\n  Application (2013)", "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an extension of hedge automata called bidimensional context-free\nhedge automata. The class of unranked ordered tree languages they recognize is\nshown to be preserved by rewrite closure with inverse-monadic rules. We also\nextend the parameterized rewriting rules used for modeling the W3C XQuery\nUpdate Facility in previous works, by the possibility to insert a new parent\nnode above a given node. We show that the rewrite closure of hedge automata\nlanguages with these extended rewriting systems are context-free hedge\nlanguages.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 16:01:21 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Jacquemard", "Florent", "", "Inria Paris-Rocquencourt, STMS"], ["Rusinowitch", "Michael", "", "INRIA Lorraine - LORIA / LIFC"]]}, {"id": "1212.5303", "submitter": "Ryan Wisnesky", "authors": "David I. Spivak and Ryan Wisnesky", "title": "Relational Foundations For Functorial Data Migration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB math.CT math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the data transformation capabilities associated with schemas that\nare presented by directed multi-graphs and path equations. Unlike most\napproaches which treat graph-based schemas as abbreviations for relational\nschemas, we treat graph-based schemas as categories. A schema $S$ is a\nfinitely-presented category, and the collection of all $S$-instances forms a\ncategory, $S$-inst. A functor $F$ between schemas $S$ and $T$, which can be\ngenerated from a visual mapping between graphs, induces three adjoint data\nmigration functors, $\\Sigma_F:S$-inst$\\to T$-inst, $\\Pi_F: S$-inst $\\to\nT$-inst, and $\\Delta_F:T$-inst $\\to S$-inst. We present an algebraic query\nlanguage FQL based on these functors, prove that FQL is closed under\ncomposition, prove that FQL can be implemented with the\nselect-project-product-union relational algebra (SPCU) extended with a\nkey-generation operation, and prove that SPCU can be implemented with FQL.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 23:58:26 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2013 22:37:17 GMT"}, {"version": "v3", "created": "Sat, 21 Sep 2013 05:19:29 GMT"}, {"version": "v4", "created": "Tue, 22 Apr 2014 01:40:46 GMT"}, {"version": "v5", "created": "Thu, 8 May 2014 15:37:39 GMT"}, {"version": "v6", "created": "Wed, 13 May 2015 00:10:24 GMT"}, {"version": "v7", "created": "Fri, 24 Jul 2015 20:26:26 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Spivak", "David I.", ""], ["Wisnesky", "Ryan", ""]]}, {"id": "1212.5389", "submitter": "Alexandros Kalousis", "authors": "Nabil Stendardo and Alexandros Kalousis", "title": "Relationship-aware sequential pattern mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relationship-aware sequential pattern mining is the problem of mining\nfrequent patterns in sequences in which the events of a sequence are mutually\nrelated by one or more concepts from some respective hierarchical taxonomies,\nbased on the type of the events. Additionally events themselves are also\ndescribed with a certain number of taxonomical concepts. We present RaSP an\nalgorithm that is able to mine relationship-aware patterns over such sequences;\nRaSP follows a two stage approach. In the first stage it mines for frequent\ntype patterns and {\\em all} their occurrences within the different sequences.\nIn the second stage it performs hierarchical mining where for each frequent\ntype pattern and its occurrences it mines for more specific frequent patterns\nin the lower levels of the taxonomies. We test RaSP on a real world medical\napplication, that provided the inspiration for its development, in which we\nmine for frequent patterns of medical behavior in the antibiotic treatment of\nmicrobes and show that it has a very good computational performance given the\ncomplexity of the relationship-aware sequential pattern mining problem.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 10:42:06 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Stendardo", "Nabil", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1212.5636", "submitter": "Katja Hose", "authors": "Luis Gal\\'arraga, Katja Hose, Ralf Schenkel", "title": "Partout: A Distributed Engine for Efficient RDF Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing interest in Semantic Web technologies has led not only to a\nrapid growth of semantic data on the Web but also to an increasing number of\nbackend applications with already more than a trillion triples in some cases.\nConfronted with such huge amounts of data and the future growth, existing\nstate-of-the-art systems for storing RDF and processing SPARQL queries are no\nlonger sufficient. In this paper, we introduce Partout, a distributed engine\nfor efficient RDF processing in a cluster of machines. We propose an effective\napproach for fragmenting RDF data sets based on a query log, allocating the\nfragments to nodes in a cluster, and finding the optimal configuration. Partout\ncan efficiently handle updates and its query optimizer produces efficient query\nexecution plans for ad-hoc SPARQL queries. Our experiments show the superiority\nof our approach to state-of-the-art approaches for partitioning and distributed\nSPARQL query processing.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 23:51:05 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Gal\u00e1rraga", "Luis", ""], ["Hose", "Katja", ""], ["Schenkel", "Ralf", ""]]}, {"id": "1212.6051", "submitter": "Mouez Ali", "authors": "Wided Bakari, Mouez Ali, Hanene Ben-Abdallah", "title": "Automatic approach for generating ETL operators", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the generation of the ETL\noperators(Extract-Transform-Load) for supplying a Data Warehouse from a\nrelational data source. As a first step, we add new rules to those proposed by\nthe authors of [1], these rules deal with the combination of ETL operators. In\na second step, we propose an automatic approach based on model transformations\nto generate the ETL operations needed for loading a data warehouse. This\napproach offers the possibility to set some designer requirements for loading.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 14:07:44 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Bakari", "Wided", ""], ["Ali", "Mouez", ""], ["Ben-Abdallah", "Hanene", ""]]}, {"id": "1212.6383", "submitter": "Andrea Burattin", "authors": "Andrea Burattin, Alessandro Sperduti, Wil M. P. van der Aalst", "title": "Heuristics Miners for Streaming Event Data", "comments": null, "journal-ref": null, "doi": "10.1109/CEC.2014.6900341", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more business activities are performed using information systems.\nThese systems produce such huge amounts of event data that existing systems are\nunable to store and process them. Moreover, few processes are in steady-state\nand due to changing circumstances processes evolve and systems need to adapt\ncontinuously. Since conventional process discovery algorithms have been defined\nfor batch processing, it is difficult to apply them in such evolving\nenvironments. Existing algorithms cannot cope with streaming event data and\ntend to generate unreliable and obsolete results.\n  In this paper, we discuss the peculiarities of dealing with streaming event\ndata in the context of process mining. Subsequently, we present a general\nframework for defining process mining algorithms in settings where it is\nimpossible to store all events over an extended period or where processes\nevolve while being analyzed. We show how the Heuristics Miner, one of the most\neffective process discovery algorithms for practical applications, can be\nmodified using this framework. Different stream-aware versions of the\nHeuristics Miner are defined and implemented in ProM. Moreover, experimental\nresults on artificial and real logs are reported.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2012 15:15:48 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Burattin", "Andrea", ""], ["Sperduti", "Alessandro", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1212.6636", "submitter": "Paraschos Koutris", "authors": "Paraschos Koutris and Dan Suciu", "title": "A Dichotomy on the Complexity of Consistent Query Answering for Atoms\n  with Simple Keys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of consistent query answering under primary key\nviolations. In this setting, the relations in a database violate the key\nconstraints and we are interested in maximal subsets of the database that\nsatisfy the constraints, which we call repairs. For a boolean query Q, the\nproblem CERTAINTY(Q) asks whether every such repair satisfies the query or not;\nthe problem is known to be always in coNP for conjunctive queries. However,\nthere are queries for which it can be solved in polynomial time. It has been\nconjectured that there exists a dichotomy on the complexity of CERTAINTY(Q) for\nconjunctive queries: it is either in PTIME or coNP-complete. In this paper, we\nprove that the conjecture is indeed true for the case of conjunctive queries\nwithout self-joins, where each atom has as a key either a single attribute\n(simple key) or all attributes of the atom.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2012 15:07:50 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2014 21:19:52 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Koutris", "Paraschos", ""], ["Suciu", "Dan", ""]]}, {"id": "1212.6640", "submitter": "Andrey Nikolaev", "authors": "Andrey Nikolaev", "title": "Exploring mutexes, the Oracle RDBMS retrial spinlocks", "comments": "Proceedings of International Conference on Informatics MEDIAS2012.\n  Cyprus, Limassol, May 7--14, 2012. ISBN 978-5-88835-023-2. 12 pages, 15\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spinlocks are widely used in database engines for processes synchronization.\nKGX mutexes is new retrial spinlocks appeared in contemporary Oracle versions\nfor submicrosecond synchronization. The mutex contention is frequently observed\nin highly concurrent OLTP environments.\n  This work explores how Oracle mutexes operate, spin, and sleep. It develops\npredictive mathematical model and discusses parameters and statistics related\nto mutex performance tuning, as well as results of contention experiments.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2012 15:29:26 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Nikolaev", "Andrey", ""]]}, {"id": "1212.6857", "submitter": "Angela Bonifati", "authors": "Guillaume Bagan and Angela Bonifati and Benoit Groz", "title": "A Trichotomy for Regular Simple Path Queries on Graphs", "comments": "15 pages, conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular path queries (RPQs) select nodes connected by some path in a graph.\nThe edge labels of such a path have to form a word that matches a given regular\nexpression. We investigate the evaluation of RPQs with an additional constraint\nthat prevents multiple traversals of the same nodes. Those regular simple path\nqueries (RSPQs) find several applications in practice, yet they quickly become\nintractable, even for basic languages such as (aa)* or a*ba*.\n  In this paper, we establish a comprehensive classification of regular\nlanguages with respect to the complexity of the corresponding regular simple\npath query problem. More precisely, we identify the fragment that is maximal in\nthe following sense: regular simple path queries can be evaluated in polynomial\ntime for every regular language L that belongs to this fragment and evaluation\nis NP-complete for languages outside this fragment. We thus fully characterize\nthe frontier between tractability and intractability for RSPQs, and we refine\nour results to show the following trichotomy: Evaluations of RSPQs is either\nAC0, NL-complete or NP-complete in data complexity, depending on the regular\nlanguage L. The fragment identified also admits a simple characterization in\nterms of regular expressions.\n  Finally, we also discuss the complexity of the following decision problem:\ndecide, given a language L, whether finding a regular simple path for L is\ntractable. We consider several alternative representations of L: DFAs, NFAs or\nregular expressions, and prove that this problem is NL-complete for the first\nrepresentation and PSPACE-complete for the other two. As a conclusion we extend\nour results from edge-labeled graphs to vertex-labeled graphs and vertex-edge\nlabeled graphs.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 11:02:16 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Bagan", "Guillaume", ""], ["Bonifati", "Angela", ""], ["Groz", "Benoit", ""]]}]