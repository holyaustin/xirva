[{"id": "1705.00761", "submitter": "Samir Abdelrahman", "authors": "Mahmoud Mahdi, Samir Abdelrahman, Reem Bahgat, and Ismail Ismail", "title": "F-tree: an algorithm for clustering transactional data using frequency\n  tree", "comments": "Appeared at Al-Azhar University Engineering Journal, JAUES, Vol.5,\n  No. 8, Dec 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an important data mining technique that groups similar data\nrecords, recently categorical transaction clustering is received more\nattention. In this research, we study the problem of categorical data\nclustering for transactional data characterized with high dimensionality and\nlarge volume. We propose a novel algorithm for clustering transactional data\ncalled F-Tree, which is based on the idea of the frequent pattern algorithm\nFP-tree; the fastest approaches to the frequent item set mining. And the simple\nidea behind the F-Tree is to generate small high pure clusters, and then merge\nthem. That makes it fast, and dynamic in clustering large transactional\ndatasets with high dimensions. We also present a new solution to solve the\noverlapping problem between clusters, by defining a new criterion function,\nwhich is based on the probability of overlapping between weighted items. Our\nexperimental evaluation on real datasets shows that: Firstly, F-Tree is\neffective in finding interesting clusters. Secondly, the usage of the tree\nstructure reduces the clustering process time of the large data set with high\nattributes. Thirdly, the proposed evaluation metric used efficiently to solve\nthe overlapping of transaction items generates high-quality clustering results.\nFinally, we have concluded that the process of merging pure and small clusters\nincreases the purity of resulted clusters as well as it reduces the time of\nclustering better than the process of generating clusters directly from dataset\nthen refine clusters.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 01:55:44 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Mahdi", "Mahmoud", ""], ["Abdelrahman", "Samir", ""], ["Bahgat", "Reem", ""], ["Ismail", "Ismail", ""]]}, {"id": "1705.02245", "submitter": "Neil Lawrence", "authors": "Neil D. Lawrence", "title": "Data Readiness Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of models to data is fraught. Data-generating collaborators often\nonly have a very basic understanding of the complications of collating,\nprocessing and curating data. Challenges include: poor data collection\npractices, missing values, inconvenient storage mechanisms, intellectual\nproperty, security and privacy. All these aspects obstruct the sharing and\ninterconnection of data, and the eventual interpretation of data through\nmachine learning or other approaches. In project reporting, a major challenge\nis in encapsulating these problems and enabling goals to be built around the\nprocessing of data. Project overruns can occur due to failure to account for\nthe amount of time required to curate and collate. But to understand these\nfailures we need to have a common language for assessing the readiness of a\nparticular data set. This position paper proposes the use of data readiness\nlevels: it gives a rough outline of three stages of data preparedness and\nspeculates on how formalisation of these levels into a common language for data\nreadiness could facilitate project management.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 14:53:56 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Lawrence", "Neil D.", ""]]}, {"id": "1705.02844", "submitter": "G\\'abor Sz\\'arnyas", "authors": "J\\'ozsef Marton, G\\'abor Sz\\'arnyas and D\\'aniel Varr\\'o", "title": "Formalising opencypher Graph Queries in Relational Algebra", "comments": "ADBIS conference (21st European Conference on Advances in Databases\n  and Information Systems) The final publication is available at Springer via\n  https://doi.org/10.1007/978-3-319-66917-5_13", "journal-ref": null, "doi": "10.1007/978-3-319-66917-5_13", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph database systems are increasingly adapted for storing and processing\nheterogeneous network-like datasets. However, due to the novelty of such\nsystems, no standard data model or query language has yet emerged.\nConsequently, migrating datasets or applications even between related\ntechnologies often requires a large amount of manual work or ad-hoc solutions,\nthus subjecting the users to the possibility of vendor lock-in. To avoid this\nthreat, vendors are working on supporting existing standard languages (e.g.\nSQL) or creating standardised languages.\n  In this paper, we present a formal specification for openCypher, a high-level\ndeclarative graph query language with an ongoing standardisation effort. We\nintroduce relational graph algebra, which extends relational operators by\nadapting graph-specific operators and define a mapping from core openCypher\nconstructs to this algebra. We propose an algorithm that allows systematic\ncompilation of openCypher queries.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 12:19:37 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 14:32:29 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Marton", "J\u00f3zsef", ""], ["Sz\u00e1rnyas", "G\u00e1bor", ""], ["Varr\u00f3", "D\u00e1niel", ""]]}, {"id": "1705.02936", "submitter": "Manuel Mazzara", "authors": "Andrei Lebedev, JooYoung Lee, Victor Rivera, Manuel Mazzara", "title": "Link Prediction using Top-$k$ Shortest Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply an efficient top-$k$ shortest distance routing\nalgorithm to the link prediction problem and test its efficacy. We compare the\nresults with other base line and state-of-the-art methods as well as with the\nshortest path. Our results show that using top-$k$ distances as a similarity\nmeasure outperforms classical similarity measures such as Jaccard and\nAdamic/Adar.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 14:24:56 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Lebedev", "Andrei", ""], ["Lee", "JooYoung", ""], ["Rivera", "Victor", ""], ["Mazzara", "Manuel", ""]]}, {"id": "1705.02982", "submitter": "Rachana Nget Ms.", "authors": "Rachana Nget, Yang Cao, Masatoshi Yoshikawa", "title": "How to Balance Privacy and Money through Pricing Mechanism in Personal\n  Data Market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A personal data market is a platform including three participants: data\nowners (individuals), data buyers and market maker. Data owners who provide\npersonal data are compensated according to their privacy loss. Data buyers can\nsubmit a query and pay for the result according to their desired accuracy.\nMarket maker coordinates between data owner and buyer. This framework has been\npreviously studied based on differential privacy. However, the previous study\nassumes data owners can accept any level of privacy loss and data buyers can\nconduct the transaction without regard to the financial budget. In this paper,\nwe propose a practical personal data trading framework that is able to strike a\nbalance between money and privacy. In order to gain insights on user\npreferences, we first conducted an online survey on human attitude to- ward\nprivacy and interest in personal data trading. Second, we identify the 5 key\nprinciples of personal data market, which is important for designing a\nreasonable trading frame- work and pricing mechanism. Third, we propose a\nreason- able trading framework for personal data which provides an overview of\nhow the data is traded. Fourth, we propose a balanced pricing mechanism which\ncomputes the query price for data buyers and compensation for data owners\n(whose data are utilized) as a function of their privacy loss. The main goal is\nto ensure a fair trading for both parties. Finally, we will conduct an\nexperiment to evaluate the output of our proposed pricing mechanism in\ncomparison with other previously proposed mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 17:28:01 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 10:55:30 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Nget", "Rachana", ""], ["Cao", "Yang", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "1705.03028", "submitter": "Abolfazl Asudeh", "authors": "Abolfazl Asudeh, Azade Nazi, Nick Koudas and Gautam Das", "title": "Assisting Service Providers In Peer-to-peer Marketplaces: Maximizing\n  Gain Over Flexible Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer to peer marketplaces such as AirBnB enable transactional exchange of\nservices directly between people. In such platforms, those providing a service\n(hosts in AirBnB) are faced with various choices. For example in AirBnB,\nalthough some amenities in a property (attributes of the property) are fixed,\nothers are relatively flexible and can be provided without significant effort.\nProviding an amenity is usually associated with a cost. Naturally different\nsets of amenities may have a different \"gains\" for a host. Consequently, given\na limited budget, deciding which amenities (attributes) to offer is\nchallenging.\n  In this paper, we formally introduce and define the problem of Gain\nMaximization over Flexible Attributes (GMFA). We first prove that the problem\nis NP-hard and show that identifying an approximate algorithm with a constant\napproximate ratio is unlikely. We then provide a practically efficient exact\nalgorithm to the GMFA problem for the general class of monotonic gain\nfunctions, which quantify the benefit of sets of attributes. As the next part\nof our contribution, we focus on the design of a practical gain function for\nGMFA. We introduce the notion of frequent-item based count (FBC), which\nutilizes the existing tuples in the database to define the notion of gain, and\npropose an efficient algorithm for computing it. We present the results of a\ncomprehensive experimental evaluation of the proposed techniques on real\ndataset from AirBnB and demonstrate the practical relevance and utility of our\nproposal.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 18:09:00 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 04:34:59 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Asudeh", "Abolfazl", ""], ["Nazi", "Azade", ""], ["Koudas", "Nick", ""], ["Das", "Gautam", ""]]}, {"id": "1705.03303", "submitter": "Niek Tax", "authors": "Niek Tax, Xixi Lu, Natalia Sidorova, Dirk Fahland, Wil M. P. van der\n  Aalst", "title": "The Imprecisions of Precision Measures in Process Mining", "comments": null, "journal-ref": "Information Processing Letters, 135 (2018), 1-8", "doi": "10.1016/j.ipl.2018.01.013", "report-no": null, "categories": "cs.DB cs.AI cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In process mining, precision measures are used to quantify how much a process\nmodel overapproximates the behavior seen in an event log. Although several\nmeasures have been proposed throughout the years, no research has been done to\nvalidate whether these measures achieve the intended aim of quantifying\nover-approximation in a consistent way for all models and logs. This paper\nfills this gap by postulating a number of axioms for quantifying precision\nconsistently for any log and any model. Further, we show through\ncounter-examples that none of the existing measures consistently quantifies\nprecision.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 11:50:45 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 18:44:59 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Tax", "Niek", ""], ["Lu", "Xixi", ""], ["Sidorova", "Natalia", ""], ["Fahland", "Dirk", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1705.04380", "submitter": "Tommaso Soru", "authors": "Tommaso Soru, Edgard Marx, Axel-Cyrille Ngonga Ngomo", "title": "ROCKER: A Refinement Operator for Key Discovery", "comments": "WWW 2015", "journal-ref": null, "doi": "10.1145/2736277.2741642", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Linked Data principles provide a decentral approach for publishing\nstructured data in the RDF format on the Web. In contrast to structured data\npublished in relational databases where a key is often provided explicitly,\nfinding a set of properties that allows identifying a resource uniquely is a\nnon-trivial task. Still, finding keys is of central importance for manifold\napplications such as resource deduplication, link discovery, logical data\ncompression and data integration. In this paper, we address this research gap\nby specifying a refinement operator, dubbed ROCKER, which we prove to be\nfinite, proper and non-redundant. We combine the theoretical characteristics of\nthis operator with two monotonicities of keys to obtain a time-efficient\napproach for detecting keys, i.e., sets of properties that describe resources\nuniquely. We then utilize a hash index to compute the discriminability score\nefficiently. Therewith, we ensure that our approach can scale to very large\nknowledge bases. Results show that ROCKER yields more accurate results, has a\ncomparable runtime, and consumes less memory w.r.t. existing state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 21:26:06 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Soru", "Tommaso", ""], ["Marx", "Edgard", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1705.04915", "submitter": "Furong Li", "authors": "Furong Li, Xin Luna Dong, Anno Langen, Yang Li", "title": "Discovering Multiple Truths with a Hybrid Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Many data management applications require integrating information from\nmultiple sources. The sources may not be accurate and provide erroneous values.\nWe thus have to identify the true values from conflicting observations made by\nthe sources. The problem is further complicated when there may exist multiple\ntruths (e.g., a book written by several authors). In this paper we propose a\nmodel called Hybrid that jointly makes two decisions: how many truths there\nare, and what they are. It considers the conflicts between values as important\nevidence for ruling out wrong values, while keeps the flexibility of allowing\nmultiple truths. In this way, Hybrid is able to achieve both high precision and\nhigh recall.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 04:03:15 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Li", "Furong", ""], ["Dong", "Xin Luna", ""], ["Langen", "Anno", ""], ["Li", "Yang", ""]]}, {"id": "1705.04928", "submitter": "Dhanya Jothimani", "authors": "Abhay Bhadani, Dhanya Jothimani", "title": "Big Data: Challenges, Opportunities and Realities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of Internet of Things (IoT) and Web 2.0 technologies, there\nhas been a tremendous growth in the amount of data generated. This chapter\nemphasizes on the need for big data, technological advancements, tools and\ntechniques being used to process big data are discussed. Technological\nimprovements and limitations of existing storage techniques are also presented.\nSince, the traditional technologies like Relational Database Management System\n(RDBMS) have their own limitations to handle big data, new technologies have\nbeen developed to handle them and to derive useful insights. This chapter\npresents an overview of big data analytics, its application, advantages, and\nlimitations. Few research issues and future directions are presented in this\nchapter.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 08:44:28 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Bhadani", "Abhay", ""], ["Jothimani", "Dhanya", ""]]}, {"id": "1705.05688", "submitter": "Olivier Cur\\'e", "authors": "Xiangnan Ren and Olivier Cur\\'e", "title": "Strider: A Hybrid Adaptive Distributed RDF Stream Processing Engine", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time processing of data streams emanating from sensors is becoming a\ncommon task in Internet of Things scenarios. The key implementation goal\nconsists in efficiently handling massive incoming data streams and supporting\nadvanced data analytics services like anomaly detection. In an on-going,\nindustrial project, we found out that a 24/7 available stream processing engine\nusually faces dynamically changing data and workload characteristics. These\nchanges impact the engine's performance and reliability. We propose Strider, a\nhybrid adaptive distributed RDF Stream Processing engine that optimizes logical\nquery plan according to the state of data streams. Strider has been designed to\nguarantee important industrial properties such as scalability, high\navailability, fault-tolerant, high throughput and acceptable latency. These\nguarantees are obtained by designing the engine's architecture with\nstate-of-the-art Apache components such as Spark and Kafka. We highlight the\nefficiency (e.g., on a single machine machine, up to 60x gain on throughput\ncompared to state-of-the-art systems, a throughput of 3.1 million\ntriples/second on a 9 machines cluster, a major breakthrough in this system's\ncategory) of Strider on real-world and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 12:54:23 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Ren", "Xiangnan", ""], ["Cur\u00e9", "Olivier", ""]]}, {"id": "1705.05720", "submitter": "Rui Meng", "authors": "Rui Meng, Hao Xin, Lei Chen, Yangqiu Song", "title": "Subjective Knowledge Acquisition and Enrichment Powered By Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases (KBs) have attracted increasing attention due to its great\nsuccess in various areas, such as Web and mobile search.Existing KBs are\nrestricted to objective factual knowledge, such as city population or fruit\nshape, whereas,subjective knowledge, such as big city, which is commonly\nmentioned in Web and mobile queries, has been neglected. Subjective knowledge\ndiffers from objective knowledge in that it has no documented or observed\nground truth. Instead, the truth relies on people's dominant opinion. Thus, we\ncan use the crowdsourcing technique to get opinion from the crowd. In our work,\nwe propose a system, called crowdsourced subjective knowledge acquisition\n(CoSKA),for subjective knowledge acquisition powered by crowdsourcing and\nexisting KBs. The acquired knowledge can be used to enrich existing KBs in the\nsubjective dimension which bridges the gap between existing objective knowledge\nand subjective queries.The main challenge of CoSKA is the conflict between\nlarge scale knowledge facts and limited crowdsourcing resource. To address this\nchallenge, in this work, we define knowledge inference rules and then select\nthe seed knowledge judiciously for crowdsourcing to maximize the inference\npower under the resource constraint. Our experimental results on real knowledge\nbase and crowdsourcing platform verify the effectiveness of CoSKA system.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 14:25:02 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Meng", "Rui", ""], ["Xin", "Hao", ""], ["Chen", "Lei", ""], ["Song", "Yangqiu", ""]]}, {"id": "1705.06135", "submitter": "Gabriela Montoya", "authors": "Gabriela Montoya and Hala Skaf-Molli and Katja Hose", "title": "The Odyssey Approach for Optimizing Federated SPARQL Queries", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": "10.1007/978-3-319-68288-4_28", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering queries over a federation of SPARQL endpoints requires combining\ndata from more than one data source. Optimizing queries in such scenarios is\nparticularly challenging not only because of (i) the large variety of possible\nquery execution plans that correctly answer the query but also because (ii)\nthere is only limited access to statistics about schema and instance data of\nremote sources. To overcome these challenges, most federated query engines rely\non heuristics to reduce the space of possible query execution plans or on\ndynamic programming strategies to produce optimal plans. Nevertheless, these\nplans may still exhibit a high number of intermediate results or high execution\ntimes because of heuristics and inaccurate cost estimations. In this paper, we\npresent Odyssey, an approach that uses statistics that allow for a more\naccurate cost estimation for federated queries and therefore enables Odyssey to\nproduce better query execution plans. Our experimental results show that\nOdyssey produces query execution plans that are better in terms of data\ntransfer and execution time than state-of-the-art optimizers. Our experiments\nusing the FedBench benchmark show execution time gains of at least 25 times on\naverage.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 13:10:59 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 08:32:59 GMT"}, {"version": "v3", "created": "Thu, 2 Nov 2017 13:36:37 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Montoya", "Gabriela", ""], ["Skaf-Molli", "Hala", ""], ["Hose", "Katja", ""]]}, {"id": "1705.07538", "submitter": "Peter Bailis", "authors": "Peter Bailis, Kunle Olukotun, Christopher Re, Matei Zaharia", "title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 02:28:19 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 02:13:09 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Bailis", "Peter", ""], ["Olukotun", "Kunle", ""], ["Re", "Christopher", ""], ["Zaharia", "Matei", ""]]}, {"id": "1705.08317", "submitter": "Qusay Mahmoud", "authors": "Omar Almootassem, Syed Hamza Husain, Denesh Parthipan, Qusay H.\n  Mahmoud", "title": "A Cloud-based Service for Real-Time Performance Evaluation of NoSQL\n  Databases", "comments": "5 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have created a cloud-based service that allows the end users to run tests\non multiple different databases to find which databases are most suitable for\ntheir project. From our research, we could not find another application that\nenables the user to test several databases to gauge the difference between\nthem. This application allows the user to choose which type of test to perform\nand which databases to target. The application also displays the results of\ndifferent tests that were run by other users previously. There is also a map to\nshow the location where all the tests are run to give the user an estimate of\nthe location. Unlike the orthodox static tests and reports conducted to\nevaluate NoSQL databases, we have created a web application to run and analyze\nthese tests in real time. This web application evaluates the performance of\nseveral NoSQL databases. The databases covered are MongoDB, DynamoDB, CouchDB,\nand Firebase. The web service is accessible from: nosqldb.nextproject.ca.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:32:12 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Almootassem", "Omar", ""], ["Husain", "Syed Hamza", ""], ["Parthipan", "Denesh", ""], ["Mahmoud", "Qusay H.", ""]]}, {"id": "1705.09276", "submitter": "Yue Wang", "authors": "Yue Wang and Yeye He", "title": "Synthesizing Mapping Relationships Using Table Corpus", "comments": "The long version of a paper published at SIGMOD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping relationships, such as (country, country-code) or (company,\nstock-ticker), are versatile data assets for an array of applications in data\ncleaning and data integration like auto-correction and auto-join. However,\ntoday there are no good repositories of mapping tables that can enable these\nintelligent applications.\n  Given a corpus of tables such as web tables or spreadsheet tables, we observe\nthat values of these mappings often exist in pairs of columns in same tables.\nMotivated by their broad applicability, we study the problem of synthesizing\nmapping relationships using a large table corpus. Our synthesis process\nleverages compatibility of tables based on co-occurrence statistics, as well as\nconstraints such as functional dependency. Experiment results using web tables\nand enterprise spreadsheets suggest that the proposed approach can produce high\nquality mappings.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:46:55 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 17:52:50 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Wang", "Yue", ""], ["He", "Yeye", ""]]}, {"id": "1705.09359", "submitter": "Niek Tax", "authors": "Niek Tax, Emin Alasgarov, Natalia Sidorova, Wil M.P. van der Aalst,\n  Reinder Haakma", "title": "Generating Time-Based Label Refinements to Discover More Precise Process\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a research field focused on the analysis of event data with\nthe aim of extracting insights related to dynamic behavior. Applying process\nmining techniques on data from smart home environments has the potential to\nprovide valuable insights in (un)healthy habits and to contribute to ambient\nassisted living solutions. Finding the right event labels to enable the\napplication of process mining techniques is however far from trivial, as simply\nusing the triggering sensor as the label for sensor events results in\nuninformative models that allow for too much behavior (overgeneralizing).\nRefinements of sensor level event labels suggested by domain experts have been\nshown to enable discovery of more precise and insightful process models.\nHowever, there exists no automated approach to generate refinements of event\nlabels in the context of process mining. In this paper we propose a framework\nfor the automated generation of label refinements based on the time attribute\nof events, allowing us to distinguish behaviourally different instances of the\nsame event type based on their time attribute. We show on a case study with\nreal life smart home event data that using automatically generated refined\nlabels in process discovery, we can find more specific, and therefore more\ninsightful, process models. We observe that one label refinement could have an\neffect on the usefulness of other label refinements when used together.\nTherefore, we explore four strategies to generate useful combinations of\nmultiple label refinements and evaluate those on three real life smart home\nevent logs.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 21:01:20 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 16:22:43 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Tax", "Niek", ""], ["Alasgarov", "Emin", ""], ["Sidorova", "Natalia", ""], ["van der Aalst", "Wil M. P.", ""], ["Haakma", "Reinder", ""]]}, {"id": "1705.09391", "submitter": "Panagiotis Mandros", "authors": "Panagiotis Mandros, Mario Boley, Jilles Vreeken", "title": "Discovering Reliable Approximate Functional Dependencies", "comments": "Accepted: In Proceedings of the ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD), August 13-17, 2017, Halifax, NS, Canada", "journal-ref": null, "doi": "10.1145/3097983.3098062", "report-no": null, "categories": "cs.DB cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a database and a target attribute of interest, how can we tell whether\nthere exists a functional, or approximately functional dependence of the target\non any set of other attributes in the data? How can we reliably, without bias\nto sample size or dimensionality, measure the strength of such a dependence?\nAnd, how can we efficiently discover the optimal or $\\alpha$-approximate\ntop-$k$ dependencies? These are exactly the questions we answer in this paper.\n  As we want to be agnostic on the form of the dependence, we adopt an\ninformation-theoretic approach, and construct a reliable, bias correcting score\nthat can be efficiently computed. Moreover, we give an effective optimistic\nestimator of this score, by which for the first time we can mine the\napproximate functional dependencies from data with guarantees of optimality.\nEmpirical evaluation shows that the derived score achieves a good bias for\nvariance trade-off, can be used within an efficient discovery algorithm, and\nindeed discovers meaningful dependencies. Most important, it remains reliable\nin the face of data sparsity.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 23:00:46 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 18:18:55 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Mandros", "Panagiotis", ""], ["Boley", "Mario", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1705.09427", "submitter": "Yuliang Li", "authors": "Yuliang Li, Alin Deutsch, and Victor Vianu", "title": "SpinArt: A Spin-based Verifier for Artifact Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven workflows, of which IBM's Business Artifacts are a prime\nexponent, have been successfully deployed in practice, adopted in industrial\nstandards, and have spawned a rich body of research in academia, focused\nprimarily on static analysis. In previous work, we obtained theoretical results\non the verification of a rich model incorporating core elements of IBM's\nsuccessful Guard-Stage-Milestone (GSM) artifact model. The results showed\ndecidability of verification of temporal properties of a large class of GSM\nworkflows and established its complexity. Following up on these results, the\npresent paper reports on the implementation of SpinArt, a practical verifier\nbased on the classical model-checking tool Spin. The implementation includes\nnontrivial optimizations and achieves good performance on real-world business\nprocess examples. Our results shed light on the capabilities and limitations of\noff-the-shelf verifiers in the context of data-driven workflows.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 04:13:17 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 17:35:37 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 23:21:27 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Li", "Yuliang", ""], ["Deutsch", "Alin", ""], ["Vianu", "Victor", ""]]}, {"id": "1705.10007", "submitter": "Yuliang Li", "authors": "Yuliang Li, Alin Deutsch and Victor Vianu", "title": "VERIFAS: A Practical Verifier for Artifact Systems", "comments": "arXiv admin note: text overlap with arXiv:1705.09427", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven workflows, of which IBM's Business Artifacts are a prime\nexponent, have been successfully deployed in practice, adopted in industrial\nstandards, and have spawned a rich body of research in academia, focused\nprimarily on static analysis. The present research bridges the gap between the\ntheory and practice of artifact verification with VERIFAS, the first\nimplementation of practical significance of an artifact verifier with full\nsupport for unbounded data. VERIFAS verifies within seconds linear-time\ntemporal properties over real-world and synthetic workflows of complexity in\nthe range recommended by software engineering practice. Compared to our\nprevious implementation based on the widely-used Spin model checker, VERIFAS\nnot only supports a model with richer data manipulations but also outperforms\nit by over an order of magnitude. VERIFAS' good performance is due to a novel\nsymbolic representation approach and a family of specialized optimizations.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 00:45:18 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 15:27:49 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 13:05:03 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Li", "Yuliang", ""], ["Deutsch", "Alin", ""], ["Vianu", "Victor", ""]]}, {"id": "1705.10202", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Reinder Haakma, Wil M.P. van der Aalst", "title": "Mining Process Model Descriptions of Daily Life through Event\n  Abstraction", "comments": "arXiv admin note: substantial text overlap with arXiv:1606.07283", "journal-ref": "Studies in Computational Intelligence, 751 (2017) 83-104", "doi": "10.1007/978-3-319-69266-1_5", "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining techniques focus on extracting insight in processes from event\nlogs. Process mining has the potential to provide valuable insights in\n(un)healthy habits and to contribute to ambient assisted living solutions when\napplied on data from smart home environments. However, events recorded in smart\nhome environments are on the level of sensor triggers, at which process\ndiscovery algorithms produce overgeneralizing process models that allow for too\nmuch behavior and that are difficult to interpret for human experts. We show\nthat abstracting the events to a higher-level interpretation can enable\ndiscovery of more precise and more comprehensible models. We present a\nframework for the extraction of features that can be used for abstraction with\nsupervised learning methods that is based on the XES IEEE standard for event\nlogs. This framework can automatically abstract sensor-level events to their\ninterpretation at the human activity level, after training it on training data\nfor which both the sensor and human activity events are known. We demonstrate\nour abstraction framework on three real-life smart home event logs and show\nthat the process models that can be discovered after abstraction are more\nprecise indeed.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 20:32:56 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["Haakma", "Reinder", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1705.10480", "submitter": "Gianluca Cima", "authors": "Gianluca Cima", "title": "Preliminary results on Ontology-based Open Data Publishing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the current interest in Open Data publishing, a formal and\ncomprehensive methodology supporting an organization in deciding which data to\npublish and carrying out precise procedures for publishing high-quality data,\nis still missing. In this paper we argue that the Ontology-based Data\nManagement paradigm can provide a formal basis for a principled approach to\npublish high quality, semantically annotated Open Data. We describe two main\napproaches to using an ontology for this endeavor, and then we present some\ntechnical results on one of the approaches, called bottom-up, where the\nspecification of the data to be published is given in terms of the sources, and\nspecific techniques allow deriving suitable annotations for interpreting the\npublished data under the light of the ontology.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 07:16:45 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 13:40:52 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Cima", "Gianluca", ""]]}, {"id": "1705.10977", "submitter": "Hui Li", "authors": "Hui Li, Sourav S Bhowmick, Jiangtao Cui and Jianfeng Ma", "title": "Time is What Prevents Everything from Happening at Once: Propagation\n  Time-conscious Influence Maximization", "comments": "14 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The influence maximization (IM) problem as defined in the seminal paper by\nKempe et al. has received widespread attention from various research\ncommunities, leading to the design of a wide variety of solutions.\nUnfortunately, this classical IM problem ignores the fact that time taken for\ninfluence propagation to reach the largest scope can be significant in\nrealworld social networks, during which the underlying network itself may have\nevolved. This phenomenon may have considerable adverse impact on the quality of\nselected seeds and as a result all existing techniques that use this classical\ndefinition as their building block generate seeds with suboptimal influence\nspread. In this paper, we revisit the classical IM problem and propose a more\nrealistic version called PROTEUS-IM (Propagation Time conscious Influence\nMaximization) to replace it by addressing the aforementioned limitation.\nSpecifically, as influence propagation may take time, we assume that the\nunderlying social network may evolve during influence propagation.\nConsequently, PROTEUSIM aims to select seeds in the current network to maximize\ninfluence spread in the future instance of the network at the end of influence\npropagation process without assuming complete topological knowledge of the\nfuture network. We propose a greedy and a Reverse Reachable (RR) set-based\nalgorithms called PROTEUS-GENIE and PROTEUS-SEER, respectively, to address this\nproblem. Our algorithms utilize the state-of-the-art Forest Fire Model for\nmodeling network evolution during influence propagation to find superior\nquality seeds. Experimental study on real and synthetic social networks shows\nthat our proposed algorithms consistently outperform state-of-the-art classical\nIM algorithms with respect to seed set quality.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 08:19:49 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 04:05:28 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Li", "Hui", ""], ["Bhowmick", "Sourav S", ""], ["Cui", "Jiangtao", ""], ["Ma", "Jianfeng", ""]]}]