[{"id": "1702.00093", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Qin Zhang", "title": "EmbedJoin: Efficient Edit Similarity Joins via Embeddings", "comments": "A preliminary version of this paper has been published in KDD 2017.\n  In this version we have extended the basic EmbedJoin algorithm to EmbedJoin+\n  which has better accuracy on some tested datasets. 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of edit similarity joins, where given a set of strings\nand a threshold value $K$, we want to output all pairs of strings whose edit\ndistances are at most $K$. Edit similarity join is a fundamental problem in\ndata cleaning/integration, bioinformatics, collaborative filtering and natural\nlanguage processing, and has been identified as a primitive operator for\ndatabase systems. This problem has been studied extensively in the literature.\nHowever, we have observed that all the existing algorithms fall short on long\nstrings and large distance thresholds.\n  In this paper we propose an algorithm named EmbedJoin which scales very well\nwith string length and distance threshold. Our algorithm is built on the recent\nadvance of metric embeddings for edit distance, and is very different from all\nof the previous approaches. We demonstrate via an extensive set of experiments\nthat EmbedJoin significantly outperforms the previous best algorithms on long\nstrings and large distance thresholds.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 00:39:18 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 23:05:25 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 19:14:20 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Zhang", "Haoyu", ""], ["Zhang", "Qin", ""]]}, {"id": "1702.00358", "submitter": "Florin Rusu", "authors": "Yu Cheng, Weijie Zhao, Florin Rusu", "title": "OLA-RAW: Scalable Exploration over Raw Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-situ processing has been proposed as a novel data exploration solution in\nmany domains generating massive amounts of raw data, e.g., astronomy, since it\nprovides immediate SQL querying over raw files. The performance of in-situ\nprocessing across a query workload is, however, limited by the speed of full\nscan, tokenizing, and parsing of the entire data. Online aggregation (OLA) has\nbeen introduced as an efficient method for data exploration that identifies\nuninteresting patterns faster by continuously estimating the result of a\ncomputation during the actual processing---the computation can be stopped as\nearly as the estimate is accurate enough to be deemed uninteresting. However,\nexisting OLA solutions have a high upfront cost of randomly shuffling and/or\nsampling the data. In this paper, we present OLA-RAW, a bi-level sampling\nscheme for parallel online aggregation over raw data. Sampling in OLA-RAW is\nquery-driven and performed exclusively in-situ during the runtime query\nexecution, without data reorganization. This is realized by a novel\nresource-aware bi-level sampling algorithm that processes data in random chunks\nconcurrently and determines adaptively the number of sampled tuples inside a\nchunk. In order to avoid the cost of repetitive conversion from raw data,\nOLA-RAW builds and maintains a memory-resident bi-level sample synopsis\nincrementally. We implement OLA-RAW inside a modern in-situ data processing\nsystem and evaluate its performance across several real and synthetic datasets\nand file formats. Our results show that OLA-RAW chooses the sampling plan that\nminimizes the execution time and guarantees the required accuracy for each\nquery in a given workload. The end result is a focused data exploration process\nthat avoids unnecessary work and discards uninteresting data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 17:07:56 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Cheng", "Yu", ""], ["Zhao", "Weijie", ""], ["Rusu", "Florin", ""]]}, {"id": "1702.00535", "submitter": "Xi He", "authors": "Xi He, Ashwin Machanavajjhala, Cheryl Flynn and Divesh Srivastava", "title": "Composing Differential Privacy and Secure Computation: A case study on\n  scaling private record linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private record linkage (PRL) is the problem of identifying pairs of records\nthat are similar as per an input matching rule from databases held by two\nparties that do not trust one another. We identify three key desiderata that a\nPRL solution must ensure: 1) perfect precision and high recall of matching\npairs, 2) a proof of end-to-end privacy, and 3) communication and computational\ncosts that scale subquadratically in the number of input records. We show that\nall of the existing solutions for PRL - including secure 2-party computation\n(S2PC), and their variants that use non-private or differentially private (DP)\nblocking to ensure subquadratic cost - violate at least one of the three\ndesiderata. In particular, S2PC techniques guarantee end-to-end privacy but\nhave either low recall or quadratic cost. In contrast, no end-to-end privacy\nguarantee has been formalized for solutions that achieve subquadratic cost.\nThis is true even for solutions that compose DP and S2PC: DP does not permit\nthe release of any exact information about the databases, while S2PC algorithms\nfor PRL allow the release of matching records.\n  In light of this deficiency, we propose a novel privacy model, called output\nconstrained differential privacy, that shares the strong privacy protection of\nDP, but allows for the truthful release of the output of a certain function\napplied to the data. We apply this to PRL, and show that protocols satisfying\nthis privacy model permit the disclosure of the true matching records, but\ntheir execution is insensitive to the presence or absence of a single\nnon-matching record. We find that prior work that combine DP and S2PC\ntechniques even fail to satisfy this end-to-end privacy model. Hence, we\ndevelop novel protocols that provably achieve this end-to-end privacy\nguarantee, together with the other two desiderata of PRL.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 03:57:52 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 17:39:20 GMT"}, {"version": "v3", "created": "Mon, 28 Aug 2017 17:47:56 GMT"}, {"version": "v4", "created": "Fri, 1 Sep 2017 13:42:56 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["He", "Xi", ""], ["Machanavajjhala", "Ashwin", ""], ["Flynn", "Cheryl", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1702.00567", "submitter": "Yunfan Chen", "authors": "Yunfan Chen, Lei Chen, Chen Jason Zhang", "title": "CrowdFusion: A Crowdsourced Approach on Data Fusion Refinement", "comments": "A short version of this paper will be published as ICDE'2017 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data fusion has played an important role in data mining because high-quality\ndata is required in a lot of applications. As on-line data may be out-of-date\nand errors in the data may propagate with copying and referring between\nsources, it is hard to achieve satisfying results with merely applying existing\ndata fusion methods to fuse Web data. In this paper, we make use of the crowd\nto achieve high-quality data fusion result. We design a framework selecting a\nset of tasks to ask crowds in order to improve the confidence of data. Since\ndata are correlated and crowds may provide incorrect answers, how to select a\nproper set of tasks to ask the crowd is a very challenging problem. In this\npaper, we design an approximation solution to address this challenge since we\nprove that the problem is at NP-hard. To further improve the efficiency, we\ndesign a pruning strategy and a preprocessing method, which effectively improve\nthe performance of the proposed approximation solution. Furthermore, we find\nthat under certain scenarios, we are not interested in all the facts, but only\na specific set of facts. Thus, for these specific scenarios, we also develop\nanother approximation solution which is much faster than the general\napproximation solution. We verify the solutions with extensive experiments on a\nreal crowdsourcing platform.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 07:59:25 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Chen", "Yunfan", ""], ["Chen", "Lei", ""], ["Zhang", "Chen Jason", ""]]}, {"id": "1702.00820", "submitter": "Theodoros Rekatsinas", "authors": "Theodoros Rekatsinas, Xu Chu, Ihab F. Ilyas, Christopher R\\'e", "title": "HoloClean: Holistic Data Repairs with Probabilistic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HoloClean, a framework for holistic data repairing driven by\nprobabilistic inference. HoloClean unifies existing qualitative data repairing\napproaches, which rely on integrity constraints or external data sources, with\nquantitative data repairing methods, which leverage statistical properties of\nthe input data. Given an inconsistent dataset as input, HoloClean automatically\ngenerates a probabilistic program that performs data repairing. Inspired by\nrecent theoretical advances in probabilistic inference, we introduce a series\nof optimizations which ensure that inference over HoloClean's probabilistic\nmodel scales to instances with millions of tuples. We show that HoloClean\nscales to instances with millions of tuples and find data repairs with an\naverage precision of ~90% and an average recall of above ~76% across a diverse\narray of datasets exhibiting different types of errors. This yields an average\nF1 improvement of more than 2x against state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 20:25:41 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Rekatsinas", "Theodoros", ""], ["Chu", "Xu", ""], ["Ilyas", "Ihab F.", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1702.00921", "submitter": "C Ravindranath Chowdary", "authors": "Shubham Varma, Neyshith Sameer and C. Ravindranath Chowdary", "title": "ReLiC: Entity Profiling by using Random Forest and Trustworthiness of a\n  Source - Technical Report", "comments": null, "journal-ref": null, "doi": "10.1007/s12046-019-1178-x", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The digital revolution has brought most of the world on the world wide web.\nThe data available on WWW has increased many folds in the past decade. Social\nnetworks, online clubs and organisations have come into existence. Information\nis extracted from these venues about a real world entity like a person,\norganisation, event, etc. However, this information may change over time, and\nthere is a need for the sources to be up-to-date. Therefore, it is desirable to\nhave a model to extract relevant data items from different sources and merge\nthem to build a complete profile of an entity (entity profiling). Further, this\nmodel should be able to handle incorrect or obsolete data items. In this paper,\nwe propose a novel method for completing a profile. We have developed a two\nphase method-1) The first phase (resolution phase) links records to the\nqueries. We have proposed and observed that the use of random forest for entity\nresolution increases the performance of the system as this has resulted in more\nrecords getting linked to the correct entity. Also, we used trustworthiness of\na source as a feature to the random forest. 2) The second phase selects the\nappropriate values from records to complete a profile based on our proposed\nselection criteria. We have used various metrics for measuring the performance\nof the resolution phase as well as for the overall ReLiC framework. It is\nestablished through our results that the use of biased sources has\nsignificantly improved the performance of the ReLiC framework. Experimental\nresults show that our proposed system, ReLiC outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 07:04:13 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Varma", "Shubham", ""], ["Sameer", "Neyshith", ""], ["Chowdary", "C. Ravindranath", ""]]}, {"id": "1702.01015", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Vinay Goel, Avishek Anand", "title": "ArchiveSpark: Efficient Web Archive Access, Extraction and Derivation", "comments": "JCDL 2016, Newark, NJ, USA", "journal-ref": null, "doi": "10.1145/2910896.2910902", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archives are a valuable resource for researchers of various disciplines.\nHowever, to use them as a scholarly source, researchers require a tool that\nprovides efficient access to Web archive data for extraction and derivation of\nsmaller datasets. Besides efficient access we identify five other objectives\nbased on practical researcher needs such as ease of use, extensibility and\nreusability.\n  Towards these objectives we propose ArchiveSpark, a framework for efficient,\ndistributed Web archive processing that builds a research corpus by working on\nexisting and standardized data formats commonly held by Web archiving\ninstitutions. Performance optimizations in ArchiveSpark, facilitated by the use\nof a widely available metadata index, result in significant speed-ups of data\nprocessing. Our benchmarks show that ArchiveSpark is faster than alternative\napproaches without depending on any additional data stores while improving\nusability by seamlessly integrating queries and derivations with external\ntools.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 14:17:02 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Holzmann", "Helge", ""], ["Goel", "Vinay", ""], ["Anand", "Avishek", ""]]}, {"id": "1702.01168", "submitter": "Navid Yaghmazadeh", "authors": "Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, Thomas Dillig", "title": "Type- and Content-Driven Synthesis of SQL Queries from Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new technique for automatically synthesizing SQL\nqueries from natural language. Our technique is fully automated, works for any\ndatabase without requiring additional customization, and does not require users\nto know the underlying database schema. Our method achieves these goals by\ncombining natural language processing, program synthesis, and automated program\nrepair. Given the user's English description, our technique first uses semantic\nparsing to generate a query sketch, which is subsequently completed using\ntype-directed program synthesis and assigned a confidence score using database\ncontents. However, since the user's description may not accurately reflect the\nactual database schema, our approach also performs fault localization and\nrepairs the erroneous part of the sketch. This synthesize-repair loop is\nrepeated until the algorithm infers a query with a sufficiently high confidence\nscore. We have implemented the proposed technique in a tool called Sqlizer and\nevaluate it on three different databases. Our experiments show that the desired\nquery is ranked within the top 5 candidates in close to 90% of the cases.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:26:43 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Yaghmazadeh", "Navid", ""], ["Wang", "Yuepeng", ""], ["Dillig", "Isil", ""], ["Dillig", "Thomas", ""]]}, {"id": "1702.01208", "submitter": "Arya Mazumdar", "authors": "Arya Mazumdar, Barna Saha", "title": "A Theoretical Analysis of First Heuristics of Crowdsourced Entity\n  Resolution", "comments": "Appears in AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is the task of identifying all records in a database\nthat refer to the same underlying entity, and are therefore duplicates of each\nother. Due to inherent ambiguity of data representation and poor data quality,\nER is a challenging task for any automated process. As a remedy, human-powered\nER via crowdsourcing has become popular in recent years. Using crowd to answer\nqueries is costly and time consuming. Furthermore, crowd-answers can often be\nfaulty. Therefore, crowd-based ER methods aim to minimize human participation\nwithout sacrificing the quality and use a computer generated similarity matrix\nactively. While, some of these methods perform well in practice, no theoretical\nanalysis exists for them, and further their worst case performances do not\nreflect the experimental findings. This creates a disparity in the\nunderstanding of the popular heuristics for this problem. In this paper, we\nmake the first attempt to close this gap. We provide a thorough analysis of the\nprominent heuristic algorithms for crowd-based ER. We justify experimental\nobservations with our analysis and information theoretic lower bounds.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 23:56:58 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Mazumdar", "Arya", ""], ["Saha", "Barna", ""]]}, {"id": "1702.01393", "submitter": "Ji\\v{r}\\'i N\\'advorn\\'ik", "authors": "Jiri Nadvornik, Petr Skoda, Dave Morris, Pavel Tvrdik", "title": "Time Series Cube Data Model", "comments": "30 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this document is to create a data model and its serialization\nfor expressing generic time series data. Already existing IVOA data models are\nreused as much as possible. The model is also made as generic as possible to be\nopen to new extensions but at the same time closed for modifications. This\nenables maintaining interoperability throughout different versions of the data\nmodel. We define the necessary building blocks for metadata discovery,\nserialization of time series data and understanding it by clients. We present\nseveral categories of time series science cases with examples of\nimplementation. We also take into account the most pressing topics for time\nseries providers like tracking original images for every individual point of a\nlight curve or time-derived axes like frequency for gravitational wave\nanalysis. The main motivation for the creation of a new model is to provide a\nunified time series data publishing standard - not only for light curves but\nalso more generic time series data, e.g., radial velocity curves, power\nspectra, hardness ratio, provenance linkage, etc. The flexibility is the most\ncrucial part of our model - we are not dependent on any physical domain or\nframe models. While images or spectra are already stable and standardized\nproducts, the time series related domains are still not completely evolved and\nnew ones will likely emerge in near future. That is why we need to keep models\nlike Time Series Cube DM independent of any underlying physical models. In our\nopinion, this is the only correct and sustainable way for future development of\nIVOA standards.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 12:20:52 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 15:00:42 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Nadvornik", "Jiri", ""], ["Skoda", "Petr", ""], ["Morris", "Dave", ""], ["Tvrdik", "Pavel", ""]]}, {"id": "1702.01446", "submitter": "Nirman Kumar", "authors": "Pankaj K. Agarwal and Nirman Kumar and Stavros Sintos and Subhash Suri", "title": "Efficient Algorithms for k-Regret Minimizing Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regret minimizing set Q is a small size representation of a much larger\ndatabase P so that user queries executed on Q return answers whose scores are\nnot much worse than those on the full dataset. In particular, a k-regret\nminimizing set has the property that the regret ratio between the score of the\ntop-1 item in Q and the score of the top-k item in P is minimized, where the\nscore of an item is the inner product of the item's attributes with a user's\nweight (preference) vector. The problem is challenging because we want to find\na single representative set Q whose regret ratio is small with respect to all\npossible user weight vectors.\n  We show that k-regret minimization is NP-Complete for all dimensions d >= 3.\nThis settles an open problem from Chester et al. [VLDB 2014], and resolves the\ncomplexity status of the problem for all d: the problem is known to have\npolynomial-time solution for d <= 2. In addition, we propose two new\napproximation schemes for regret minimization, both with provable guarantees,\none based on coresets and another based on hitting sets. We also carry out\nextensive experimental evaluation, and show that our schemes compute\nregret-minimizing sets comparable in size to the greedy algorithm proposed in\n[VLDB 14] but our schemes are significantly faster and scalable to large data\nsets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 19:30:44 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 01:46:20 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Kumar", "Nirman", ""], ["Sintos", "Stavros", ""], ["Suri", "Subhash", ""]]}, {"id": "1702.01596", "submitter": "Quoc-Cuong To", "authors": "Quoc-Cuong To, Juan Soto, Volker Markl", "title": "A Survey of State Management in Big Data Processing Systems", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State management and its use in diverse applications varies widely across big\ndata processing systems. This is evident in both the research literature and\nexisting systems, such as Apache Flink, Apache Samza, Apache Spark, and Apache\nStorm. Given the pivotal role that state management plays in various use cases,\nin this survey, we present some of the most important uses of state as an\nenabler, discuss the alternative approaches used to handle and implement state,\npropose a taxonomy to capture the many facets of state management, and\nhighlight new research directions. Our aim is to provide insight into disparate\nstate management techniques, motivate others to pursue research in this area,\nand draw attention to some open problems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 12:41:28 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 07:09:13 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 11:29:21 GMT"}, {"version": "v4", "created": "Wed, 1 Aug 2018 12:43:11 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["To", "Quoc-Cuong", ""], ["Soto", "Juan", ""], ["Markl", "Volker", ""]]}, {"id": "1702.01786", "submitter": "Manuel Bravo", "authors": "Chathuri Gunawardhana, Manuel Bravo, Lu\\'is Rodrigues", "title": "Unobtrusive Deferred Update Stabilization for Efficient Geo-Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach to manage the throughput vs latency\ntradeoff that emerges when managing updates in geo-replicated systems. Our\napproach consists in allowing full concurrency when processing local updates\nand using a deferred local serialisation procedure before shipping updates to\nremote datacenters. This strategy allows to implement inexpensive mechanisms to\nensure system consistency requirements while avoiding intrusive effects on\nupdate operations, a major performance limitation of previous systems. We have\nimplemented our approach as a variant of Riak KV. Our extensive evaluation\nshows that we outperform sequencer-based approaches by almost an order of\nmagnitude in the maximum achievable throughput. Furthermore, unlike previous\nsequencer-free solutions, our approach reaches nearly optimal remote update\nvisibility latencies without limiting throughput.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 20:21:32 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Gunawardhana", "Chathuri", ""], ["Bravo", "Manuel", ""], ["Rodrigues", "Lu\u00eds", ""]]}, {"id": "1702.01975", "submitter": "Vladimir Dzyuba", "authors": "Vladimir Dzyuba, Matthijs van Leeuwen", "title": "Learning what matters - Sampling interesting patterns", "comments": "PAKDD 2017, extended version", "journal-ref": "Advances in Knowledge Discovery and Data Mining. PAKDD 2017.\n  Lecture Notes in Computer Science, vol.10234, 2017, pp.534-546", "doi": "10.1007/978-3-319-57454-7_42", "report-no": null, "categories": "stat.ML cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of exploratory data mining, local structure in data can be\ndescribed by patterns and discovered by mining algorithms. Although many\nsolutions have been proposed to address the redundancy problems in pattern\nmining, most of them either provide succinct pattern sets or take the interests\nof the user into account-but not both. Consequently, the analyst has to invest\nsubstantial effort in identifying those patterns that are relevant to her\nspecific interests and goals. To address this problem, we propose a novel\napproach that combines pattern sampling with interactive data mining. In\nparticular, we introduce the LetSIP algorithm, which builds upon recent\nadvances in 1) weighted sampling in SAT and 2) learning to rank in interactive\npattern mining. Specifically, it exploits user feedback to directly learn the\nparameters of the sampling distribution that represents the user's interests.\nWe compare the performance of the proposed algorithm to the state-of-the-art in\ninteractive pattern mining by emulating the interests of a user. The resulting\nsystem allows efficient and interleaved learning and sampling, thus\nuser-specific anytime data exploration. Finally, LetSIP demonstrates favourable\ntrade-offs concerning both quality-diversity and exploitation-exploration when\ncompared to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 12:01:08 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 16:22:18 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Dzyuba", "Vladimir", ""], ["van Leeuwen", "Matthijs", ""]]}, {"id": "1702.02721", "submitter": "Genqiang Wu", "authors": "Genqiang Wu and Xianyao Xia and Yeping He", "title": "Analytic Theory to Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB math.GM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to develop a mathematical analysis theory to\nsolve differential privacy problems. The heart of our approaches is to use\nanalytic tools to characterize the correlations among the outputs of different\ndatasets, which makes it feasible to represent a differentially private\nmechanism with minimal number of parameters. These results are then used to\nconstruct differentially private mechanisms analytically. Furthermore, our\napproaches are universal to almost all query functions. We believe that the\napproaches and results of this paper are indispensable complements to the\ncurrent studies of differential privacy that are ruled by the ad hoc and\nalgorithmic approaches.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 06:34:19 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 13:41:08 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 18:12:37 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Wu", "Genqiang", ""], ["Xia", "Xianyao", ""], ["He", "Yeping", ""]]}, {"id": "1702.02799", "submitter": "Sheng Wang", "authors": "Anh Dinh, Ji Wang, Sheng Wang, Gang Chen, Wei-Ngan Chin, Qian Lin,\n  Beng Chin Ooi, Pingcheng Ruan, Kian-Lee Tan, Zhongle Xie, Hao Zhang, and\n  Meihui Zhang", "title": "UStore: A Distributed Storage With Rich Semantics", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's storage systems expose abstractions which are either too low-level\n(e.g., key-value store, raw-block store) that they require developers to\nre-invent the wheels, or too high-level (e.g., relational databases, Git) that\nthey lack generality to support many classes of applications. In this work, we\npropose and implement a general distributed data storage system, called UStore,\nwhich has rich semantics. UStore delivers three key properties, namely\nimmutability, sharing and security, which unify and add values to many classes\nof today's applications, and which also open the door for new applications. By\nkeeping the core properties within the storage, UStore helps reduce application\ndevelopment efforts while offering high performance at hand. The storage\nembraces current hardware trends as key enablers. It is built around a\ndata-structure similar to that of Git, a popular source code versioning system,\nbut it also synthesizes many designs from distributed systems and databases.\nOur current implementation of UStore has better performance than general\nin-memory key-value storage systems, especially for version scan operations. We\nport and evaluate four applications on top of UStore: a Git-like application, a\ncollaborative data science application, a transaction management application,\nand a blockchain application. We demonstrate that UStore enables faster\ndevelopment and the UStore-backed applications can have better performance than\nthe existing implementations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 12:06:37 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Dinh", "Anh", ""], ["Wang", "Ji", ""], ["Wang", "Sheng", ""], ["Chen", "Gang", ""], ["Chin", "Wei-Ngan", ""], ["Lin", "Qian", ""], ["Ooi", "Beng Chin", ""], ["Ruan", "Pingcheng", ""], ["Tan", "Kian-Lee", ""], ["Xie", "Zhongle", ""], ["Zhang", "Hao", ""], ["Zhang", "Meihui", ""]]}, {"id": "1702.02809", "submitter": "Arnab Bhattacharya", "authors": "Shubhadip Mitra, Priya Saraf, Richa Sharma, Arnab Bhattacharya, Harsh\n  Bhandari, Sayan Ranu", "title": "NetClus: A Scalable Framework for Locating Top-K Sites for Placement of\n  Trajectory-Aware Services", "comments": "ICDE 2017 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facility location queries identify the best locations to set up new\nfacilities for providing service to its users. Majority of the existing works\nin this space assume that the user locations are static. Such limitations are\ntoo restrictive for planning many modern real-life services such as fuel\nstations, ATMs, convenience stores, cellphone base-stations, etc. that are\nwidely accessed by mobile users. The placement of such services should,\ntherefore, factor in the mobility patterns or trajectories of the users rather\nthan simply their static locations. In this work, we introduce the TOPS\n(Trajectory-Aware Optimal Placement of Services) query that locates the best k\nsites on a road network. The aim is to optimize a wide class of objective\nfunctions defined over the user trajectories. We show that the problem is\nNP-hard and even the greedy heuristic with an approximation bound of (1-1/e)\nfails to scale on urban-scale datasets. To overcome this challenge, we develop\na multi-resolution clustering based indexing framework called NetClus.\nEmpirical studies on real road network trajectory datasets show that NetClus\noffers solutions that are comparable in terms of quality with those of the\ngreedy heuristic, while having practical response times and low memory\nfootprints. Additionally, the NetClus framework can absorb dynamic updates in\nmobility patterns, handle constraints such as site-costs and capacity, and\nexisting services, thereby providing an effective solution for modern\nurban-scale scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 12:42:14 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 10:00:29 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Mitra", "Shubhadip", ""], ["Saraf", "Priya", ""], ["Sharma", "Richa", ""], ["Bhattacharya", "Arnab", ""], ["Bhandari", "Harsh", ""], ["Ranu", "Sayan", ""]]}, {"id": "1702.03253", "submitter": "Vijay Gadepally", "authors": "Lauren Milechin, Alexander Chen, Vijay Gadepally, Dylan Hutchison,\n  Siddharth Samsi, Jeremy Kepner", "title": "D4M 3.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The D4M tool is used by hundreds of researchers to perform complex analytics\non unstructured data. Over the past few years, the D4M toolbox has evolved to\nsupport connectivity with a variety of database engines, graph analytics in the\nApache Accumulo database, and an implementation using the Julia programming\nlanguage. In this article, we describe some of our latest additions to the D4M\ntoolbox and our upcoming D4M 3.0 release.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 00:37:12 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Milechin", "Lauren", ""], ["Chen", "Alexander", ""], ["Gadepally", "Vijay", ""], ["Hutchison", "Dylan", ""], ["Samsi", "Siddharth", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1702.03358", "submitter": "Vladislav Ryzhikov Dr", "authors": "Meghyn Bienvenu, Stanislav Kikot, Roman Kontchakov, Vladimir V.\n  Podolskii, Vladislav Ryzhikov, Michael Zakharyaschev", "title": "The Complexity of Ontology-Based Data Access with OWL 2 QL and Bounded\n  Treewidth Queries", "comments": "PODS 2017 long version. arXiv admin note: text overlap with\n  arXiv:1604.05258", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our concern is the overhead of answering OWL 2 QL ontology-mediated queries\n(OMQs) in ontology-based data access compared to evaluating their underlying\ntree-shaped and bounded treewidth conjunctive queries (CQs). We show that OMQs\nwith bounded-depth ontologies have nonrecursive datalog (NDL) rewritings that\ncan be constructed and evaluated in LOGCFL for combined complexity, even in NL\nif their CQs are tree-shaped with a bounded number of leaves, and so incur no\noverhead in complexity-theoretic terms. For OMQs with arbitrary ontologies and\nbounded-leaf CQs, NDL-rewritings are constructed and evaluated in LOGCFL. We\nshow experimentally feasibility and scalability of our rewritings compared to\npreviously proposed NDL-rewritings. On the negative side, we prove that\nanswering OMQs with tree-shaped CQs is not fixed-parameter tractable if the\nontology depth or the number of leaves in the CQs is regarded as the parameter,\nand that answering OMQs with a fixed ontology (of infinite depth) is\nNP-complete for tree-shaped and LOGCFL for bounded-leaf CQs.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 00:10:30 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 10:23:35 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Bienvenu", "Meghyn", ""], ["Kikot", "Stanislav", ""], ["Kontchakov", "Roman", ""], ["Podolskii", "Vladimir V.", ""], ["Ryzhikov", "Vladislav", ""], ["Zakharyaschev", "Michael", ""]]}, {"id": "1702.03390", "submitter": "Arnab Bhattacharya", "authors": "Anuradha Awasthi, Arnab Bhattacharya, Sanchit Gupta, Ujjwal Kumar\n  Singh", "title": "K-Dominant Skyline Join Queries: Extending the Join Paradigm to\n  K-Dominant Skylines", "comments": "Appeared as a short paper in ICDE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skyline queries enable multi-criteria optimization by filtering objects that\nare worse in all the attributes of interest than another object. To handle the\nlarge answer set of skyline queries in high-dimensional datasets, the concept\nof k-dominance was proposed where an object is said to dominate another object\nif it is better (or equal) in at least k attributes. This relaxes the full\ndomination criterion of normal skyline queries and, therefore, produces lesser\nnumber of skyline objects. This is called the k-dominant skyline set. Many\npractical applications, however, require that the preferences are applied on a\njoined relation. Common examples include flights having one or multiple stops,\na combination of product price and shipping costs, etc. In this paper, we\nextend the k-dominant skyline queries to the join paradigm by enabling such\nqueries to be asked on joined relations. We call such queries KSJQ (k-dominant\nskyline join queries). The number of skyline attributes, k, that an object must\ndominate is from the combined set of skyline attributes of the joined relation.\nWe show how pre-processing the base relations helps in reducing the time of\nanswering such queries over the naive method of joining the relations first and\nthen running the k-dominant skyline computation. We also extend the query to\nhandle cases where the skyline preference is on aggregated values in the joined\nrelation (such as total cost of the multiple legs of the flight) which are\navailable only after the join is performed. In addition to these problems, we\ndevise efficient algorithms to choose the value of k based on the desired\ncardinality of the final skyline set. Experiments on both real and synthetic\ndatasets demonstrate the efficiency, scalability and practicality of our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 06:51:21 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Awasthi", "Anuradha", ""], ["Bhattacharya", "Arnab", ""], ["Gupta", "Sanchit", ""], ["Singh", "Ujjwal Kumar", ""]]}, {"id": "1702.03447", "submitter": "Alex Memory", "authors": "Angelika Kimmig, Alex Memory, Renee J. Miller, Lise Getoor", "title": "A Collective, Probabilistic Approach to Schema Mapping: Appendix", "comments": "This is the appendix to the paper \"A Collective, Probabilistic\n  Approach to Schema Mapping\" accepted to ICDE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this appendix we provide additional supplementary material to \"A\nCollective, Probabilistic Approach to Schema Mapping.\" We include an additional\nextended example, supplementary experiment details, and proof for the\ncomplexity result stated in the main paper.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 19:18:41 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kimmig", "Angelika", ""], ["Memory", "Alex", ""], ["Miller", "Renee J.", ""], ["Getoor", "Lise", ""]]}, {"id": "1702.03484", "submitter": "Xiaowang Zhang", "authors": "Jiaying Feng and Xiaowang Zhang and Zhiyong Feng", "title": "MapSQ: A MapReduce-based Framework for SPARQL Queries on GPU", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a MapReduce-based framework for evaluating SPARQL\nqueries on GPU (named MapSQ) to large-scale RDF datesets efficiently by\napplying both high performance. Firstly, we develop a MapReduce-based Join\nalgorithm to handle SPARQL queries in a parallel way. Secondly, we present a\ncoprocessing strategy to manage the process of evaluating queries where CPU is\nused to assigns subqueries and GPU is used to compute the join of subqueries.\nFinally, we implement our proposed framework and evaluate our proposal by\ncomparing with two popular and latest SPARQL query engines gStore and gStoreD\non the LUBM benchmark. The experiments demonstrate that our proposal MapSQ is\nhighly efficient and effective (up to 50% speedup).\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 03:06:25 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Feng", "Jiaying", ""], ["Zhang", "Xiaowang", ""], ["Feng", "Zhiyong", ""]]}, {"id": "1702.03519", "submitter": "Zeyi Wen", "authors": "Zeyi Wen, Dong Deng, Rui Zhang, Kotagiri Ramamohanarao", "title": "A Technical Report: Entity Extraction using Both Character-based and\n  Token-based Similarity", "comments": "12 pages, 6 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity extraction is fundamental to many text mining tasks such as\norganisation name recognition. A popular approach to entity extraction is based\non matching sub-string candidates in a document against a dictionary of\nentities. To handle spelling errors and name variations of entities, usually\nthe matching is approximate and edit or Jaccard distance is used to measure\ndissimilarity between sub-string candidates and the entities. For approximate\nentity extraction from free text, existing work considers solely\ncharacter-based or solely token-based similarity and hence cannot\nsimultaneously deal with minor variations at token level and typos. In this\npaper, we address this problem by considering both character-based similarity\nand token-based similarity (i.e. two-level similarity). Measuring one-level\n(e.g. character-based) similarity is computationally expensive, and measuring\ntwo-level similarity is dramatically more expensive. By exploiting the\nproperties of the two-level similarity and the weights of tokens, we develop\nnovel techniques to significantly reduce the number of sub-string candidates\nthat require computation of two-level similarity against the dictionary of\nentities. A comprehensive experimental study on real world datasets show that\nour algorithm can efficiently extract entities from documents and produce a\nhigh F1 score in the range of [0.91, 0.97].\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 12:46:40 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Wen", "Zeyi", ""], ["Deng", "Dong", ""], ["Zhang", "Rui", ""], ["Ramamohanarao", "Kotagiri", ""]]}, {"id": "1702.03825", "submitter": "Yang Zhang", "authors": "Yang Zhang, Yusu Wang, Srinivasan Parthasarathy", "title": "Analyzing and Visualizing Scalar Fields on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The value proposition of a dataset often resides in the implicit\ninterconnections or explicit relationships (patterns) among individual\nentities, and is often modeled as a graph. Effective visualization of such\ngraphs can lead to key insights uncovering such value. In this article we\npropose a visualization method to explore graphs with numerical attributes\nassociated with nodes (or edges) -- referred to as scalar graphs. Such\nnumerical attributes can represent raw content information, similarities, or\nderived information reflecting important network measures such as triangle\ndensity and centrality. The proposed visualization strategy seeks to\nsimultaneously uncover the relationship between attribute values and graph\ntopology, and relies on transforming the network to generate a terrain map. A\nkey objective here is to ensure that the terrain map reveals the overall\ndistribution of components-of-interest (e.g. dense subgraphs, k-cores) and the\nrelationships among them while being sensitive to the attribute values over the\ngraph. We also design extensions that can capture the relationship across\nmultiple numerical attributes (scalars). We demonstrate the efficacy of our\nmethod on several real-world data science tasks while scaling to large graphs\nwith millions of nodes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 07:47:48 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Zhang", "Yang", ""], ["Wang", "Yusu", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1702.04242", "submitter": "Ezra N. Hoch", "authors": "Ezra N. Hoch, Yaniv Ben-Yehuda, Noam Lewis, Avi Vigder", "title": "Bizur: A Key-value Consensus Algorithm for Scalable File-systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bizur is a consensus algorithm exposing a key-value interface. It is used by\na distributed file-system that scales to 100s of servers, delivering millions\nof IOPS, both data and metadata, with consistent low-latency.\n  Bizur is aimed for services that require strongly consistent state, but do\nnot require a distributed log; for example, a distributed lock manager or a\ndistributed service locator. By avoiding a distributed log scheme, Bizur\noutperforms distributed log based consensus algorithms, producing more IOPS and\nguaranteeing lower latencies during normal operation and especially during\nfailures.\n  Paxos-like algorithms (e.g., Zab and Raft) which are used by existing\ndistributed file-systems, can have artificial contention points due to their\ndependence on a distributed log. The distributed log is needed when replicating\na general service, but when the desired service is key-value based, the\ncontention points created by the distributed log can be avoided.\n  Bizur does exactly that, by reaching consensus independently on independent\nkeys. This independence allows Bizur to handle failures more efficiently and to\nscale much better than other consensus algorithms, allowing the file-system\nthat utilizes Bizur to scale with it.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 14:51:45 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Hoch", "Ezra N.", ""], ["Ben-Yehuda", "Yaniv", ""], ["Lewis", "Noam", ""], ["Vigder", "Avi", ""]]}, {"id": "1702.04420", "submitter": "Shenggang Ying", "authors": "Shenggang Ying, Mingsheng Ying, Yuan Feng", "title": "Quantum Privacy-Preserving Data Analytics", "comments": "50 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics (such as association rule mining and decision tree mining) can\ndiscover useful statistical knowledge from a big data set. But protecting the\nprivacy of the data provider and the data user in the process of analytics is a\nserious issue. Usually, the privacy of both parties cannot be fully protected\nsimultaneously by a classical algorithm. In this paper, we present a quantum\nprotocol for data mining that can much better protect privacy than the known\nclassical algorithms: (1) if both the data provider and the data user are\nhonest, the data user can know nothing about the database except the\nstatistical results, and the data provider can get nearly no information about\nthe results mined by the data user; (2) if the data user is dishonest and tries\nto disclose private information of the other, she/he will be detected with a\nhigh probability; (3) if the data provider tries to disclose the privacy of the\ndata user, she/he cannot get any useful information since the data user hides\nhis privacy among noises.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 23:23:49 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Ying", "Shenggang", ""], ["Ying", "Mingsheng", ""], ["Feng", "Yuan", ""]]}, {"id": "1702.04746", "submitter": "Arlei Lopes Da Silva", "authors": "Arlei Silva, Ambuj Singh, Ananthram Swami", "title": "Spectral Algorithms for Temporal Graph Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparsest cut problem consists of identifying a small set of edges that\nbreaks the graph into balanced sets of vertices. The normalized cut problem\nbalances the total degree, instead of the size, of the resulting sets.\nApplications of graph cuts include community detection and computer vision.\nHowever, cut problems were originally proposed for static graphs, an assumption\nthat does not hold in many modern applications where graphs are highly dynamic.\nIn this paper, we introduce the sparsest and normalized cut problems in\ntemporal graphs, which generalize their standard definitions by enforcing the\nsmoothness of cuts over time. We propose novel formulations and algorithms for\ncomputing temporal cuts using spectral graph theory, multiplex graphs,\ndivide-and-conquer and low-rank matrix approximation. Furthermore, we extend\nour formulation to dynamic graph signals, where cuts also capture node values,\nas graph wavelets. Experiments show that our solutions are accurate and\nscalable, enabling the discovery of dynamic communities and the analysis of\ndynamic graph processes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 19:37:22 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Silva", "Arlei", ""], ["Singh", "Ambuj", ""], ["Swami", "Ananthram", ""]]}, {"id": "1702.05200", "submitter": "Abdullah Alfarrarjeh", "authors": "Abdullah Alfarrarjeh, Cyrus Shahabi", "title": "Hybrid Indexes to Expedite Spatial-Visual Search", "comments": "12 Pages, 19 Figures, 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the growth of geo-tagged images, recent web and mobile applications\nprovide search capabilities for images that are similar to a given query image\nand simultaneously within a given geographical area. In this paper, we focus on\ndesigning index structures to expedite these spatial-visual searches. We start\nby baseline indexes that are straightforward extensions of the current popular\nspatial (R*-tree) and visual (LSH) index structures. Subsequently, we propose\nhybrid index structures that evaluate both spatial and visual features in\ntandem. The unique challenge of this type of query is that there are\ninaccuracies in both spatial and visual features. Therefore, different\ntraversals of the index structures may produce different images as output, some\nof which more relevant to the query than the others. We compare our hybrid\nstructures with a set of baseline indexes in both performance and result\naccuracy using three real world datasets from Flickr, Google Street View, and\nGeoUGV.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 01:16:25 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Alfarrarjeh", "Abdullah", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1702.05425", "submitter": "Jonathan A. Marshall", "authors": "Jonathan A. Marshall, Lawrence C. Rafsky", "title": "Exact clustering in linear time", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time complexity of data clustering has been viewed as fundamentally\nquadratic, slowing with the number of data items, as each item is compared for\nsimilarity to preceding items. Clustering of large data sets has been\ninfeasible without resorting to probabilistic methods or to capping the number\nof clusters. Here we introduce MIMOSA, a novel class of algorithms which\nachieve linear time computational complexity on clustering tasks. MIMOSA\nalgorithms mark and match partial-signature keys in a hash table to obtain\nexact, error-free cluster retrieval. Benchmark measurements, on clustering a\ndata set of 10,000,000 news articles by news topic, found that a MIMOSA\nimplementation finished more than four orders of magnitude faster than a\nstandard centroid implementation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 16:44:21 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 18:04:49 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Marshall", "Jonathan A.", ""], ["Rafsky", "Lawrence C.", ""]]}, {"id": "1702.05597", "submitter": "Shuai Ma", "authors": "Xuelian Lin, Shuai Ma, Han Zhang, Tianyu Wo, Jinpeng Huai", "title": "One-Pass Error Bounded Trajectory Simplification", "comments": "published at the 43rd International Conference on Very Large Data\n  Bases (VLDB), Munich, Germany, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, various sensors are collecting, storing and transmitting tremendous\ntrajectory data, and it is known that raw trajectory data seriously wastes the\nstorage, network band and computing resource. Line simplification (LS)\nalgorithms are an effective approach to attacking this issue by compressing\ndata points in a trajectory to a set of continuous line segments, and are\ncommonly used in practice. However, existing LS algorithms are not sufficient\nfor the needs of sensors in mobile devices. In this study, we first develop a\none-pass error bounded trajectory simplification algorithm (OPERB), which scans\neach data point in a trajectory once and only once. We then propose an\naggressive one-pass error bounded trajectory simplification algorithm\n(OPERB-A), which allows interpolating new data points into a trajectory under\ncertain conditions. Finally, we experimentally verify that our approaches\n(OPERB and OPERB-A) are both efficient and effective, using four real-life\ntrajectory datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 10:47:20 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Lin", "Xuelian", ""], ["Ma", "Shuai", ""], ["Zhang", "Han", ""], ["Wo", "Tianyu", ""], ["Huai", "Jinpeng", ""]]}, {"id": "1702.05607", "submitter": "Benjamin Rubinstein", "authors": "Maryam Fanaeepour, Benjamin I. P. Rubinstein", "title": "End-to-End Differentially-Private Parameter Tuning in Spatial Histograms", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially-private histograms have emerged as a key tool for location\nprivacy. While past mechanisms have included theoretical & experimental\nanalysis, it has recently been observed that much of the existing literature\ndoes not fully provide differential privacy. The missing component, private\nparameter tuning, is necessary for rigorous evaluation of these mechanisms.\nInstead works frequently tune on training data to optimise parameters without\nconsideration of privacy; in other cases selection is performed arbitrarily and\nindependent of data, degrading utility. We address this open problem by\nderiving a principled tuning mechanism that privately optimises data-dependent\nerror bounds. Theoretical results establish privacy and utility while extensive\nexperimentation demonstrates that we can practically achieve true end-to-end\nprivacy.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 12:43:46 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Fanaeepour", "Maryam", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "1702.06284", "submitter": "Sudhakar Singh", "authors": "Sudhakar Singh, Rakhi Garg, and P. K. Mishra", "title": "Review of Apriori Based Algorithms on MapReduce Framework", "comments": "12 pages, 3 figures, ICC-2014", "journal-ref": null, "doi": null, "report-no": "115", "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Apriori algorithm that mines frequent itemsets is one of the most popular\nand widely used data mining algorithms. Now days many algorithms have been\nproposed on parallel and distributed platforms to enhance the performance of\nApriori algorithm. They differ from each other on the basis of load balancing\ntechnique, memory system, data decomposition technique and data layout used to\nimplement them. The problems with most of the distributed framework are\noverheads of managing distributed system and lack of high level parallel\nprogramming language. Also with grid computing there is always potential\nchances of node failures which cause multiple re-executions of tasks. These\nproblems can be overcome by the MapReduce framework introduced by Google.\nMapReduce is an efficient, scalable and simplified programming model for large\nscale distributed data processing on a large cluster of commodity computers and\nalso used in cloud computing. In this paper, we present the overview of\nparallel Apriori algorithm implemented on MapReduce framework. They are\ncategorized on the basis of Map and Reduce functions used to implement them\ne.g. 1-phase vs. k-phase, I/O of Mapper, Combiner and Reducer, using\nfunctionality of Combiner inside Mapper etc. This survey discusses and analyzes\nthe various implementations of Apriori on MapReduce framework on the basis of\ntheir distinguishing characteristics. Moreover, it also includes the advantages\nand limitations of MapReduce framework.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 07:34:06 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Singh", "Sudhakar", ""], ["Garg", "Rakhi", ""], ["Mishra", "P. K.", ""]]}, {"id": "1702.06298", "submitter": "Md Saiful Islam", "authors": "Md. Saiful Islam, Wenny Rahayu, Chengfei Liu, Tarique Anwar and Bela\n  Stantic", "title": "Computing Influence of a Product through Uncertain Reverse Skyline", "comments": "12 pages, 3 tables, 12 figures, submitted to SSDBM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the influence of a product is crucially important for making\ninformed business decisions. This paper introduces a new type of skyline\nqueries, called uncertain reverse skyline, for measuring the influence of a\nprobabilistic product in uncertain data settings. More specifically, given a\ndataset of probabilistic products P and a set of customers C, an uncertain\nreverse skyline of a probabilistic product q retrieves all customers c in C\nwhich include q as one of their preferred products. We present efficient\npruning ideas and techniques for processing the uncertain reverse skyline query\nof a probabilistic product using R-Tree data index. We also present an\nefficient parallel approach to compute the uncertain reverse skyline and\ninfluence score of a probabilistic product. Our approach significantly\noutperforms the baseline approach derived from the existing literature. The\nefficiency of our approach is demonstrated by conducting extensive experiments\nwith both real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 09:06:04 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Islam", "Md. Saiful", ""], ["Rahayu", "Wenny", ""], ["Liu", "Chengfei", ""], ["Anwar", "Tarique", ""], ["Stantic", "Bela", ""]]}, {"id": "1702.06370", "submitter": "Christoph Berkholz", "authors": "Christoph Berkholz, Jens Keppeler, Nicole Schweikardt", "title": "Answering Conjunctive Queries under Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of enumerating and counting answers to $k$-ary\nconjunctive queries against relational databases that may be updated by\ninserting or deleting tuples. We exhibit a new notion of q-hierarchical\nconjunctive queries and show that these can be maintained efficiently in the\nfollowing sense. During a linear time preprocessing phase, we can build a data\nstructure that enables constant delay enumeration of the query results; and\nwhen the database is updated, we can update the data structure and restart the\nenumeration phase within constant time. For the special case of self-join free\nconjunctive queries we obtain a dichotomy: if a query is not q-hierarchical,\nthen query enumeration with sublinear$^\\ast$ delay and sublinear update time\n(and arbitrary preprocessing time) is impossible.\n  For answering Boolean conjunctive queries and for the more general problem of\ncounting the number of solutions of k-ary queries we obtain complete\ndichotomies: if the query's homomorphic core is q-hierarchical, then size of\nthe the query result can be computed in linear time and maintained with\nconstant update time. Otherwise, the size of the query result cannot be\nmaintained with sublinear update time. All our lower bounds rely on the\nOMv-conjecture, a conjecture on the hardness of online matrix-vector\nmultiplication that has recently emerged in the field of fine-grained\ncomplexity to characterise the hardness of dynamic problems. The lower bound\nfor the counting problem additionally relies on the orthogonal vectors\nconjecture, which in turn is implied by the strong exponential time hypothesis.\n  $^\\ast)$ By sublinear we mean $O(n^{1-\\varepsilon})$ for some\n$\\varepsilon>0$, where $n$ is the size of the active domain of the current\ndatabase.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:15:27 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Berkholz", "Christoph", ""], ["Keppeler", "Jens", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "1702.06379", "submitter": "Elias Alevizos", "authors": "Elias Alevizos, Anastasios Skarlatidis, Alexander Artikis, George\n  Paliouras", "title": "Probabilistic Complex Event Recognition: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex Event Recognition applications exhibit various types of uncertainty,\nranging from incomplete and erroneous data streams to imperfect complex event\npatterns. We review Complex Event Recognition techniques that handle, to some\nextent, uncertainty. We examine techniques based on automata, probabilistic\ngraphical models and first-order logic, which are the most common ones, and\napproaches based on Petri Nets and Grammars, which are less frequently used. A\nnumber of limitations are identified with respect to the employed languages,\ntheir probabilistic models and their performance, as compared to the purely\ndeterministic cases. Based on those limitations, we highlight promising\ndirections for future work.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:41:35 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Alevizos", "Elias", ""], ["Skarlatidis", "Anastasios", ""], ["Artikis", "Alexander", ""], ["Paliouras", "George", ""]]}, {"id": "1702.06943", "submitter": "Fengan Li", "authors": "Fengan Li, Lingjiao Chen, Yijing Zeng, Arun Kumar, Jeffrey F.\n  Naughton, Jignesh M. Patel, Xi Wu", "title": "Tuple-oriented Compression for Large-scale Mini-batch Stochastic\n  Gradient Descent", "comments": "Accepted to Sigmod 2019", "journal-ref": null, "doi": "10.1145/3299869.3300070", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data compression is a popular technique for improving the efficiency of data\nprocessing workloads such as SQL queries and more recently, machine learning\n(ML) with classical batch gradient methods. But the efficacy of such ideas for\nmini-batch stochastic gradient descent (MGD), arguably the workhorse algorithm\nof modern ML, is an open question. MGD's unique data access pattern renders\nprior art, including those designed for batch gradient methods, less effective.\nWe fill this crucial research gap by proposing a new lossless compression\nscheme we call tuple-oriented compression (TOC) that is inspired by an unlikely\nsource, the string/text compression scheme Lempel-Ziv-Welch, but tailored to\nMGD in a way that preserves tuple boundaries within mini-batches. We then\npresent a suite of novel compressed matrix operation execution techniques\ntailored to the TOC compression scheme that operate directly over the\ncompressed data representation and avoid decompression overheads. An extensive\nempirical evaluation with real-world datasets shows that TOC consistently\nachieves substantial compression ratios by up to 51x and reduces runtimes for\nMGD workloads by up to 10.2x in popular ML systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 18:58:25 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 05:43:41 GMT"}, {"version": "v3", "created": "Sun, 20 Jan 2019 05:13:18 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Li", "Fengan", ""], ["Chen", "Lingjiao", ""], ["Zeng", "Yijing", ""], ["Kumar", "Arun", ""], ["Naughton", "Jeffrey F.", ""], ["Patel", "Jignesh M.", ""], ["Wu", "Xi", ""]]}, {"id": "1702.07158", "submitter": "Riccardo Guidotti", "authors": "Riccardo Guidotti, Giulio Rossetti, Luca Pappalardo, Fosca Giannotti,\n  Dino Pedreschi", "title": "Next Basket Prediction using Recurring Sequential Patterns", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2017.111", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, a hot challenge for supermarket chains is to offer personalized\nservices for their customers. Next basket prediction, i.e., supplying the\ncustomer a shopping list for the next purchase according to her current needs,\nis one of these services. Current approaches are not capable to capture at the\nsame time the different factors influencing the customer's decision process:\nco-occurrency, sequentuality, periodicity and recurrency of the purchased\nitems. To this aim, we define a pattern Temporal Annotated Recurring Sequence\n(TARS) able to capture simultaneously and adaptively all these factors. We\ndefine the method to extract TARS and develop a predictor for next basket named\nTBP (TARS Based Predictor) that, on top of TARS, is able to to understand the\nlevel of the customer's stocks and recommend the set of most necessary items.\nBy adopting the TBP the supermarket chains could crop tailored suggestions for\neach individual customer which in turn could effectively speed up their\nshopping sessions. A deep experimentation shows that TARS are able to explain\nthe customer purchase behavior, and that TBP outperforms the state-of-the-art\ncompetitors.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 10:12:40 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Guidotti", "Riccardo", ""], ["Rossetti", "Giulio", ""], ["Pappalardo", "Luca", ""], ["Giannotti", "Fosca", ""], ["Pedreschi", "Dino", ""]]}, {"id": "1702.08042", "submitter": "Caetano Sauer", "authors": "Caetano Sauer, Goetz Graefe, Theo H\\\"arder", "title": "Instant restore after a media failure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media failures usually leave database systems unavailable for several hours\nuntil recovery is complete, especially in applications with large devices and\nhigh transaction volume. Previous work introduced a technique called\nsingle-pass restore, which increases restore bandwidth and thus substantially\ndecreases time to repair. Instant restore goes further as it permits read/write\naccess to any data on a device undergoing restore--even data not yet\nrestored--by restoring individual data segments on demand. Thus, the restore\nprocess is guided primarily by the needs of applications, and the observed mean\ntime to repair is effectively reduced from several hours to a few seconds.\n  This paper presents an implementation and evaluation of instant restore. The\ntechnique is incrementally implemented on a system starting with the\ntraditional ARIES design for logging and recovery. Experiments show that the\ntransaction latency perceived after a media failure can be cut down to less\nthan a second and that the overhead imposed by the technique on normal\nprocessing is minimal. The net effect is that a few \"nines\" of availability are\nadded to the system using simple and low-overhead software techniques.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 14:58:04 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Sauer", "Caetano", ""], ["Graefe", "Goetz", ""], ["H\u00e4rder", "Theo", ""]]}, {"id": "1702.08051", "submitter": "Sylvain Hall\\'e", "authors": "Sylvain Hall\\'e", "title": "From Complex Event Processing to Simple Event Processing", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in Computer Science can be framed as the computation of queries\nover sequences, or \"streams\" of data units called events. The field of Complex\nEvent Processing (CEP) relates to the techniques and tools developed to\nefficiently process these queries. However, most CEP systems developed so far\nhave concentrated on relatively narrow types of queries, which consist of\nsliding windows, aggregation functions, and simple sequential patterns computed\nover events that have a fixed tuple structure. Many of them boast throughput,\nbut in counterpart, they are difficult to setup and cumbersome to extend with\nuser-defined elements.\n  This paper describes a variety of use cases taken from real-world scenarios\nthat present features seldom considered in classical CEP problems. It also\nprovides a broad review of current solutions, that includes tools and\ntechniques going beyond typical surveys on CEP. From a critical analysis of\nthese solutions, design principles for a new type of event stream processing\nsystem are exposed. The paper proposes a simple, generic and extensible\nframework for the processing of event streams of diverse types; it describes in\ndetail a stream processing engine, called BeepBeep, that implements these\nprinciples. BeepBeep's modular architecture, which borrows concepts from many\nother systems, is complemented with an extensible query language, called eSQL.\nThe end result is an open, versatile, and reasonably efficient query engine\nthat can be used in situations that go beyond the capabilities of existing\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 16:51:28 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Hall\u00e9", "Sylvain", ""]]}, {"id": "1702.08327", "submitter": "Haoyuan Xing", "authors": "Haoyuan Xing (1), Sofoklis Floratos (1), Spyros Blanas (1), Suren Byna\n  (2), Prabhat (2), Kesheng Wu (2) and Paul Brown (3) ((1) The Ohio State\n  University, (2) Lawrence Berkeley National Laboratory, (3) Paradigm4, Inc)", "title": "ArrayBridge: Interweaving declarative array processing with\n  high-performance computing", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists are increasingly turning to datacenter-scale computers to produce\nand analyze massive arrays. Despite decades of database research that extols\nthe virtues of declarative query processing, scientists still write, debug and\nparallelize imperative HPC kernels even for the most mundane queries. This\nimpedance mismatch has been partly attributed to the cumbersome data loading\nprocess; in response, the database community has proposed in situ mechanisms to\naccess data in scientific file formats. Scientists, however, desire more than a\npassive access method that reads arrays from files.\n  This paper describes ArrayBridge, a bi-directional array view mechanism for\nscientific file formats, that aims to make declarative array manipulations\ninteroperable with imperative file-centric analyses. Our prototype\nimplementation of ArrayBridge uses HDF5 as the underlying array storage library\nand seamlessly integrates into the SciDB open-source array database system. In\naddition to fast querying over external array objects, ArrayBridge produces\narrays in the HDF5 file format just as easily as it can read from it.\nArrayBridge also supports time travel queries from imperative kernels through\nthe unmodified HDF5 API, and automatically deduplicates between array versions\nfor space efficiency. Our extensive performance evaluation in NERSC, a\nlarge-scale scientific computing facility, shows that ArrayBridge exhibits\nstatistically indistinguishable performance and I/O scalability to the native\nSciDB storage engine.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 15:21:45 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Xing", "Haoyuan", ""], ["Floratos", "Sofoklis", ""], ["Blanas", "Spyros", ""], ["Byna", "Suren", ""], ["Prabhat", "", ""], ["Wu", "Kesheng", ""], ["Brown", "Paul", ""]]}, {"id": "1702.08409", "submitter": "Kyrylo Simonov", "authors": "Clark C. Evans and Kyrylo Simonov", "title": "Query Combinators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Rabbit, a combinator-based query language. Rabbit is designed to\nlet data analysts and other accidental programmers query complex structured\ndata.\n  We combine the functional data model and the categorical semantics of\ncomputations to develop denotational semantics of database queries. In Rabbit,\na query is modeled as a Kleisli arrow for a monadic container determined by the\nquery cardinality. In this model, monadic composition can be used to navigate\nthe database, while other query combinators can aggregate, filter, sort and\npaginate data; construct compound data; connect self-referential data; and\nreorganize data with grouping and data cube operations. A context-aware query\nmodel, with the input context represented as a comonadic container, can express\nquery parameters and window functions. Rabbit semantics enables pipeline\nnotation, encouraging its users to construct database queries as a series of\ndistinct steps, each individually crafted and tested. We believe that Rabbit\ncan serve as a practical tool for data analytics.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 18:11:07 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Evans", "Clark C.", ""], ["Simonov", "Kyrylo", ""]]}, {"id": "1702.08513", "submitter": "Nizar Massouh", "authors": "Nizar Massouh, Francesca Babiloni, Tatiana Tommasi, Jay Young, Nick\n  Hawes and Barbara Caputo", "title": "Learning Deep Visual Object Models From Noisy Web Data: How to Make it\n  Work", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": "2017 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "doi": "10.1109/IROS.2017.8206444", "report-no": null, "categories": "cs.CV cs.DB cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks thrive when trained on large scale data collections. This has\ngiven ImageNet a central role in the development of deep architectures for\nvisual object classification. However, ImageNet was created during a specific\nperiod in time, and as such it is prone to aging, as well as dataset bias\nissues. Moving beyond fixed training datasets will lead to more robust visual\nsystems, especially when deployed on robots in new environments which must\ntrain on the objects they encounter there. To make this possible, it is\nimportant to break free from the need for manual annotators. Recent work has\nbegun to investigate how to use the massive amount of images available on the\nWeb in place of manual image annotations. We contribute to this research thread\nwith two findings: (1) a study correlating a given level of noisily labels to\nthe expected drop in accuracy, for two deep architectures, on two different\ntypes of noise, that clearly identifies GoogLeNet as a suitable architecture\nfor learning from Web data; (2) a recipe for the creation of Web datasets with\nminimal noise and maximum visual variability, based on a visual and natural\nlanguage processing concept expansion strategy. By combining these two results,\nwe obtain a method for learning powerful deep object models automatically from\nthe Web. We confirm the effectiveness of our approach through object\ncategorization experiments using our Web-derived version of ImageNet on a\npopular robot vision benchmark database, and on a lifelong object discovery\ntask on a mobile robot.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 10:02:36 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Massouh", "Nizar", ""], ["Babiloni", "Francesca", ""], ["Tommasi", "Tatiana", ""], ["Young", "Jay", ""], ["Hawes", "Nick", ""], ["Caputo", "Barbara", ""]]}, {"id": "1702.08734", "submitter": "Matthijs Douze", "authors": "Jeff Johnson and Matthijs Douze and Herv\\'e J\\'egou", "title": "Billion-scale similarity search with GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search finds application in specialized database systems handling\ncomplex data such as images or videos, which are typically represented by\nhigh-dimensional features and require specific indexing structures. This paper\ntackles the problem of better utilizing GPUs for this task. While GPUs excel at\ndata-parallel tasks, prior approaches are bottlenecked by algorithms that\nexpose less parallelism, such as k-min selection, or make poor use of the\nmemory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical\npeak performance, enabling a nearest neighbor implementation that is 8.5x\nfaster than prior GPU state of the art. We apply it in different similarity\nsearch scenarios, by proposing optimized design for brute-force, approximate\nand compressed-domain search based on product quantization. In all these\nsetups, we outperform the state of the art by large margins. Our implementation\nenables the construction of a high accuracy k-NN graph on 95 million images\nfrom the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion\nvectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced\nour approach for the sake of comparison and reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 10:42:31 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Johnson", "Jeff", ""], ["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1702.08745", "submitter": "Paulo Adeodato Prof.", "authors": "Paulo J. L. Adeodato, F\\'abio C. Pereira and Rosalvo F. Oliveira Neto", "title": "Optimal Categorical Attribute Transformation for Granularity Change in\n  Relational Databases for Binary Decision Problems in Educational Data Mining", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for transforming data granularity in\nhierarchical databases for binary decision problems by applying regression to\ncategorical attributes at the lower grain levels. Attributes from a lower\nhierarchy entity in the relational database have their information content\noptimized through regression on the categories histogram trained on a small\nexclusive labelled sample, instead of the usual mode category of the\ndistribution. The paper validates the approach on a binary decision task for\nassessing the quality of secondary schools focusing on how logistic regression\ntransforms the students and teachers attributes into school attributes.\nExperiments were carried out on Brazilian schools public datasets via 10-fold\ncross-validation comparison of the ranking score produced also by logistic\nregression. The proposed approach achieved higher performance than the usual\ndistribution mode transformation and equal to the expert weighing approach\nmeasured by the maximum Kolmogorov-Smirnov distance and the area under the ROC\ncurve at 0.01 significance level.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 11:13:17 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Adeodato", "Paulo J. L.", ""], ["Pereira", "F\u00e1bio C.", ""], ["Neto", "Rosalvo F. Oliveira", ""]]}, {"id": "1702.08764", "submitter": "Christoph Berkholz", "authors": "Christoph Berkholz, Jens Keppeler, Nicole Schweikardt", "title": "Answering FO+MOD queries under updates on bounded degree databases", "comments": "This is the full version of a paper with the same title that will be\n  published in the Proceedings of the 20th International Conference on Database\n  Theory (ICDT 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the query evaluation problem for fixed queries over fully\ndynamic databases, where tuples can be inserted or deleted. The task is to\ndesign a dynamic algorithm that immediately reports the new result of a fixed\nquery after every database update. We consider queries in first-order logic\n(FO) and its extension with modulo-counting quantifiers (FO+MOD), and show that\nthey can be efficiently evaluated under updates, provided that the dynamic\ndatabase does not exceed a certain degree bound.\n  In particular, we construct a data structure that allows to answer a Boolean\nFO+MOD query and to compute the size of the result of a non-Boolean query\nwithin constant time after every database update. Furthermore, after every\nupdate we are able to immediately enumerate the new query result with constant\ndelay between the output tuples. The time needed to build the data structure is\nlinear in the size of the database. Our results extend earlier work on the\nevaluation of first-order queries on static databases of bounded degree and\nrely on an effective Hanf normal form for FO+MOD recently obtained by Heimberg,\nKuske, and Schweikardt (LICS 2016).\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 12:46:03 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Berkholz", "Christoph", ""], ["Keppeler", "Jens", ""], ["Schweikardt", "Nicole", ""]]}]