[{"id": "1805.00680", "submitter": "Evangelos Psomakelis Mr", "authors": "Evangelos Psomakelis, Konstantinos Tserpes, Dimosthenis\n  Anagnostopoulos and Theodora Varvarigou", "title": "BUDAMAF: Data Management in Cloud Federations", "comments": "11 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data management has always been a multi-domain problem even in the simplest\ncases. It involves, quality of service, security, resource management, cost\nmanagement, incident identification, disaster avoidance and/or recovery, as\nwell as many other concerns. In our case, this situation gets ever more\ncomplicated because of the divergent nature of a cloud federation like BASMATI.\nIn this federation, the BASMATI Unified Data Management Framework (BUDaMaF),\ntries to create an automated uniform way of managing all the data transactions,\nas well as the data stores themselves, in a polyglot multi-cloud, consisting of\na plethora of different machines and data store systems.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 08:53:35 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Psomakelis", "Evangelos", ""], ["Tserpes", "Konstantinos", ""], ["Anagnostopoulos", "Dimosthenis", ""], ["Varvarigou", "Theodora", ""]]}, {"id": "1805.00819", "submitter": "Bin Jiang", "authors": "Bin Jiang", "title": "Spatial Heterogeneity, Scale, Data Character, and Sustainable Transport\n  in the Big Data Era", "comments": "5 pages, 2 figures, International Journal of Geo-Information, 2018", "journal-ref": "ISPRS International Journal of Geo-Information, 7(5), 167, 2018", "doi": "10.3390/ijgi7050167", "report-no": null, "categories": "cs.DB physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of the emergence of big data, I have advocated and argued for a\nparadigm shift from Tobler's law to scaling law, from Euclidean geometry to\nfractal geometry, from Gaussian statistics to Paretian statistics, and - more\nimportantly - from Descartes' mechanistic thinking to Alexander's organic\nthinking. Fractal geometry falls under the third definition of fractal - that\nis, a set or pattern is fractal if the scaling of far more small things than\nlarge ones recurs multiple times (Jiang and Yin 2014) - rather than under the\nsecond definition of fractal, which requires a power law between scales and\ndetails (Mandelbrot 1982). The new fractal geometry is more towards living\ngeometry that \"follows the rules, constraints, and contingent conditions that\nare, inevitably, encountered in the real world\" (Alexander et al. 2012, p.\n395), not only for understanding complexity, but also for creating complex or\nliving structure (Alexander 2002-2005). This editorial attempts to clarify why\nthe paradigm shift is essential and to elaborate on several concepts, including\nspatial heterogeneity (scaling law), scale (or the fourth meaning of scale),\ndata character (in contrast to data quality), and sustainable transport in the\nbig data era.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 08:42:30 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 21:36:40 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Jiang", "Bin", ""]]}, {"id": "1805.01046", "submitter": "Daniel Kang", "authors": "Daniel Kang, Peter Bailis, Matei Zaharia", "title": "BlazeIt: Optimizing Declarative Aggregation and Limit Queries for Neural\n  Network-Based Video Analytics", "comments": null, "journal-ref": "PVLDB 2020", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural networks (NNs) have enabled automatic querying of\nlarge volumes of video data with high accuracy. While these deep NNs can\nproduce accurate annotations of an object's position and type in video, they\nare computationally expensive and require complex, imperative deployment code\nto answer queries. Prior work uses approximate filtering to reduce the cost of\nvideo analytics, but does not handle two important classes of queries,\naggregation and limit queries; moreover, these approaches still require complex\ncode to deploy. To address the computational and usability challenges of\nquerying video at scale, we introduce BlazeIt, a system that optimizes queries\nof spatiotemporal information of objects in video. BlazeIt accepts queries via\nFrameQL, a declarative extension of SQL for video analytics that enables\nvideo-specific query optimization. We introduce two new query optimization\ntechniques in BlazeIt that are not supported by prior work. First, we develop\nmethods of using NNs as control variates to quickly answer approximate\naggregation queries with error bounds. Second, we present a novel search\nalgorithm for cardinality-limited video queries. Through these these\noptimizations, BlazeIt can deliver up to 83x speedups over the recent\nliterature on video processing.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 22:30:22 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 08:32:43 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Kang", "Daniel", ""], ["Bailis", "Peter", ""], ["Zaharia", "Matei", ""]]}, {"id": "1805.01083", "submitter": "Xiaolan Wang", "authors": "Xiaolan Wang, Aaron Feng, Behzad Golshan, Alon Halevy, George Mihaila,\n  Hidekazu Oiwa, Wang-Chiew Tan", "title": "Scalable Semantic Querying of Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the KOKO system that takes declarative information extraction to a\nnew level by incorporating advances in natural language processing techniques\nin its extraction language. KOKO is novel in that its extraction language\nsimultaneously supports conditions on the surface of the text and on the\nstructure of the dependency parse tree of sentences, thereby allowing for more\nrefined extractions. KOKO also supports conditions that are forgiving to\nlinguistic variation of expressing concepts and allows to aggregate evidence\nfrom the entire document in order to filter extractions.\n  To scale up, KOKO exploits a multi-indexing scheme and heuristics for\nefficient extractions. We extensively evaluate KOKO over publicly available\ntext corpora. We show that KOKO indices take up the smallest amount of space,\nare notably faster and more effective than a number of prior indexing schemes.\nFinally, we demonstrate KOKO's scale up on a corpus of 5 million Wikipedia\narticles.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 01:57:31 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Wang", "Xiaolan", ""], ["Feng", "Aaron", ""], ["Golshan", "Behzad", ""], ["Halevy", "Alon", ""], ["Mihaila", "George", ""], ["Oiwa", "Hidekazu", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "1805.01275", "submitter": "Diyah Puspitaningrum", "authors": "Diyah Puspitaningrum", "title": "cSELENE: Privacy Preserving Query Retrieval System on Heterogeneous\n  Cloud Data", "comments": "The First International Workshop on Learning From Limited or Noisy\n  Data for Information Retrieval (LND4IR), Ann Arbor, Michigan, USA, July 2018\n  (SIGIR 2018), 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While working in collaborative team elsewhere sometimes the federated (huge)\ndata are from heterogeneous cloud vendors. It is not only about the data\nprivacy concern but also about how can those federated data can be querying\nfrom cloud directly in fast and securely way. Previous solution offered hybrid\ncloud between public and trusted private cloud. Another previous solution used\nencryption on MapReduce framework. But the challenge is we are working on\nheterogeneous clouds. In this paper, we present a novel technique for querying\nwith privacy concern.\n  Since we take execution time into account, our basic idea is to use the data\nmining model by partitioning the federated databases in order to reduce the\nsearch and query time. By using model of the database it means we use only the\nsummary or the very characteristic patterns of the database. Modeling is the\nPreserving Privacy Stage I, since by modeling the data is being symbolized. We\nimplement encryption on the database as preserving privacy Stage II. Our\nsystem, called \"cSELENE\" (stands for \"cloud SELENE\"), is designed to handle\nfederated data on heterogeneous clouds: AWS, Microsoft Azure, and Google Cloud\nPlatform with MapReduce technique.\n  In this paper we discuss preserving-privacy system and threat model, the\nformat of federated data, the parallel programming (GPU programming and\nshared/memory systems), the parallel and secure algorithm for data mining model\nin distributed cloud, the cloud infrastructure/architecture, and the UIX design\nof the cSELENE system. Other issues such as incremental method and the secure\ndesign of cloud architecture system (Virtual Machines across platform design)\nare still open to discuss. Our experiments should demonstrate the validity and\npracticality of the proposed high performance computing scheme.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 07:29:34 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Puspitaningrum", "Diyah", ""]]}, {"id": "1805.01825", "submitter": "Markus Schr\\\"oder", "authors": "Markus Schr\\\"oder and J\\\"orn Hees and Ansgar Bernardi and Daniel Ewert\n  and Peter Klotz and Steffen Stadtm\\\"uller", "title": "Simplified SPARQL REST API - CRUD on JSON Object Graphs via URI Paths", "comments": "5 pages, 2 figures, ESWC 2018 demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the Semantic Web community, SPARQL is one of the predominant languages\nto query and update RDF knowledge. However, the complexity of SPARQL, the\nunderlying graph structure and various encodings are common sources of\nconfusion for Semantic Web novices.\n  In this paper we present a general purpose approach to convert any given\nSPARQL endpoint into a simple to use REST API. To lower the initial hurdle, we\nrepresent the underlying graph as an interlinked view of nested JSON objects\nthat can be traversed by the API path.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 09:57:13 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Schr\u00f6der", "Markus", ""], ["Hees", "J\u00f6rn", ""], ["Bernardi", "Ansgar", ""], ["Ewert", "Daniel", ""], ["Klotz", "Peter", ""], ["Stadtm\u00fcller", "Steffen", ""]]}, {"id": "1805.02009", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Lei Zhu, Weiren Yu, Jun Long, Fang Huang, Hongbo Zhao", "title": "Efficient Top K Temporal Spatial Keyword Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive amount of data that are geo-tagged and associated with text\ninformation are being generated at an unprecedented scale in many emerging\napplications such as location based services and social networks. Due to their\nimportance, a large body of work has focused on efficiently computing various\nspatial keyword queries. In this paper,we study the top-$k$ temporal spatial\nkeyword query which considers three important constraints during the search\nincluding time, spatial proximity and textual relevance. A novel index\nstructure, namely SSG-tree, to efficiently insert/delete spatio-temporal web\nobjects with high rates. Base on SSG-tree an efficient algorithm is developed\nto support top-k temporal spatial keyword query. We show via extensive\nexperimentation with real spatial databases that our method has increased\nperformance over alternate techniques\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 05:43:49 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Zhu", "Lei", ""], ["Yu", "Weiren", ""], ["Long", "Jun", ""], ["Huang", "Fang", ""], ["Zhao", "Hongbo", ""]]}, {"id": "1805.02200", "submitter": "Xingbo Wu", "authors": "Xingbo Wu, Fan Ni, Song Jiang", "title": "Wormhole: A Fast Ordered Index for In-memory Data Management", "comments": "15 pages; 18 figures; 1 table", "journal-ref": null, "doi": "10.1145/1810479.1810540", "report-no": null, "categories": "cs.DB cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-memory data management systems, such as key-value stores, have become an\nessential infrastructure in today's big-data processing and cloud computing.\nThey rely on efficient index structures to access data. While unordered\nindexes, such as hash tables, can perform point search with O(1) time, they\ncannot be used in many scenarios where range queries must be supported. Many\nordered indexes, such as B+ tree and skip list, have a O(log N) lookup cost,\nwhere N is number of keys in an index. For an ordered index hosting billions of\nkeys, it may take more than 30 key-comparisons in a lookup, which is an order\nof magnitude more expensive than that on a hash table. With availability of\nlarge memory and fast network in today's data centers, this O(log N) time is\ntaking a heavy toll on applications that rely on ordered indexes.\n  In this paper we introduce a new ordered index structure, named Wormhole,\nthat takes O(log L) worst-case time for looking up a key with a length of L.\nThe low cost is achieved by simultaneously leveraging strengths of three\nindexing structures, namely hash table, prefix tree, and B+ tree, to\norchestrate a single fast ordered index. Wormhole's range operations can be\nperformed by a linear scan of a list after an initial lookup. This improvement\nof access efficiency does not come at a price of compromised space efficiency.\nInstead, Wormhole's index space is comparable to those of B+ tree and skip\nlist. Experiment results show that Wormhole outperforms skip list, B+ tree,\nART, and Masstree by up to 8.4x, 4.9x, 4.3x, and 6.6x in terms of key lookup\nthroughput, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 12:31:28 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 03:49:13 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Wu", "Xingbo", ""], ["Ni", "Fan", ""], ["Jiang", "Song", ""]]}, {"id": "1805.02622", "submitter": "Fotis Psallidas", "authors": "Fotis Psallidas, Eugene Wu", "title": "Provenance for Interactive Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We highlight the connections between data provenance and interactive\nvisualizations. To do so, we first incrementally add interactions to a\nvisualization and show how these interactions are readily expressible in terms\nof provenance. We then describe how an interactive visualization system that\nnatively supports provenance can be easily extended with novel interactions.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 17:11:39 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Psallidas", "Fotis", ""], ["Wu", "Eugene", ""]]}, {"id": "1805.03141", "submitter": "Ji Liu", "authors": "Ji Liu and Noel Moreno Lemus and Esther Pacitti and Fabio Porto and\n  Patrick Valduriez", "title": "Parallel Computation of PDFs on Big Spatial Data Using Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider big spatial data, which is typically produced in scientific areas\nsuch as geological or seismic interpretation. The spatial data can be produced\nby observation (e.g. using sensors or soil instrument) or numerical simulation\nprograms and correspond to points that represent a 3D soil cube area. However,\nerrors in signal processing and modeling create some uncertainty, and thus a\nlack of accuracy in identifying geological or seismic phenomenons. Such\nuncertainty must be carefully analyzed. To analyze uncertainty, the main\nsolution is to compute a Probability Density Function (PDF) of each point in\nthe spatial cube area. However, computing PDFs on big spatial data can be very\ntime consuming (from several hours to even months on a parallel computer). In\nthis paper, we propose a new solution to efficiently compute such PDFs in\nparallel using Spark, with three methods: data grouping, machine learning\nprediction and sampling. We evaluate our solution by extensive experiments on\ndifferent computer clusters using big data ranging from hundreds of GB to\nseveral TB. The experimental results show that our solution scales up very well\nand can reduce the execution time by a factor of 33 (in the order of seconds or\nminutes) compared with a baseline method.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 16:22:25 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Liu", "Ji", ""], ["Lemus", "Noel Moreno", ""], ["Pacitti", "Esther", ""], ["Porto", "Fabio", ""], ["Valduriez", "Patrick", ""]]}, {"id": "1805.03320", "submitter": "Mingtao Lei", "authors": "Mingtao Lei, Lingyang Chu, Zhefeng Wang", "title": "Mining Top-k Sequential Patterns in Database Graphs:A New Challenging\n  Problem and a Sampling-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real world networks, a vertex is usually associated with a\ntransaction database that comprehensively describes the behaviour of the\nvertex. A typical example is the social network, where the behaviour of every\nuser is depicted by a transaction database that stores his daily posted\ncontents. A transaction database is a set of transactions, where a transaction\nis a set of items. Every path of the network is a sequence of vertices that\ninduces multiple sequences of transactions. The sequences of transactions\ninduced by all of the paths in the network forms an extremely large sequence\ndatabase. Finding frequent sequential patterns from such sequence database\ndiscovers interesting subsequences that frequently appear in many paths of the\nnetwork. However, it is a challenging task, since the sequence database induced\nby a database graph is too large to be explicitly induced and stored. In this\npaper, we propose the novel notion of database graph, which naturally models a\nwide spectrum of real world networks by associating each vertex with a\ntransaction database. Our goal is to find the top-k frequent sequential\npatterns in the sequence database induced from a database graph. We prove that\nthis problem is #P-hard. To tackle this problem, we propose an efficient\ntwo-step sampling algorithm that approximates the top-k frequent sequential\npatterns with provable quality guarantee. Extensive experimental results on\nsynthetic and real-world data sets demonstrate the effectiveness and efficiency\nof our method.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 23:52:34 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Lei", "Mingtao", ""], ["Chu", "Lingyang", ""], ["Wang", "Zhefeng", ""]]}, {"id": "1805.03533", "submitter": "Zoi Kaoudi", "authors": "Sebastian Kruse, Zoi Kaoudi, Bertty Contreras, Sanjay Chawla, Felix\n  Naumann, Jorge-Arnulfo Quian\\'e-Ruiz", "title": "RHEEMix in the Data Jungle: A Cost-based Optimizer for Cross-platform\n  Systems", "comments": null, "journal-ref": "VLDB Journal 2020", "doi": "10.1007/s00778-020-00612-x", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pursuit of efficient and scalable data analytics, the insight that \"one\nsize does not fit all\" has given rise to a plethora of specialized data\nprocessing platforms and today's complex data analytics are moving beyond the\nlimits of a single platform. In this paper, we present the cost-based optimizer\nof Rheem, an open-source cross-platform system that copes with these new\nrequirements. The optimizer allocates the subtasks of data analytic tasks to\nthe most suitable platforms. Our main contributions are: (i)~a mechanism based\non graph transformations to explore alternative execution strategies; (ii)~a\nnovel graph-based approach to efficiently plan data movement among subtasks and\nplatforms; and (iii)~an efficient plan enumeration algorithm, based on a novel\nenumeration algebra. We extensively evaluate our optimizer under diverse real\ntasks. The results show that our optimizer is capable of selecting the most\nefficient platform combination for a given task, freeing data analysts from the\nneed to choose and orchestrate platforms. In addition, our optimizer allows\ntasks to run more than one order of magnitude faster by using multiple\nplatforms instead of a single platform.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 13:56:32 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 08:20:28 GMT"}, {"version": "v3", "created": "Sat, 5 Sep 2020 08:52:43 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kruse", "Sebastian", ""], ["Kaoudi", "Zoi", ""], ["Contreras", "Bertty", ""], ["Chawla", "Sanjay", ""], ["Naumann", "Felix", ""], ["Quian\u00e9-Ruiz", "Jorge-Arnulfo", ""]]}, {"id": "1805.03677", "submitter": "Ahmed Hosny", "authors": "Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, Kasia\n  Chmielinski", "title": "The Dataset Nutrition Label: A Framework To Drive Higher Data Quality\n  Standards", "comments": "First Draft May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) systems built on incomplete or biased data will\noften exhibit problematic outcomes. Current methods of data analysis,\nparticularly before model development, are costly and not standardized. The\nDataset Nutrition Label (the Label) is a diagnostic framework that lowers the\nbarrier to standardized data analysis by providing a distilled yet\ncomprehensive overview of dataset \"ingredients\" before AI model development.\nBuilding a Label that can be applied across domains and data types requires\nthat the framework itself be flexible and adaptable; as such, the Label is\ncomprised of diverse qualitative and quantitative modules generated through\nmultiple statistical and probabilistic modelling backends, but displayed in a\nstandardized format. To demonstrate and advance this concept, we generated and\npublished an open source prototype with seven sample modules on the ProPublica\nDollars for Docs dataset. The benefits of the Label are manyfold. For data\nspecialists, the Label will drive more robust data analysis practices, provide\nan efficient way to select the best dataset for their purposes, and increase\nthe overall quality of AI models as a result of more robust training datasets\nand the ability to check for issues at the time of model development. For those\nbuilding and publishing datasets, the Label creates an expectation of\nexplanation, which will drive better data collection practices. We also explore\nthe limitations of the Label, including the challenges of generalizing across\ndiverse datasets, and the risk of using \"ground truth\" data as a comparison\ndataset. We discuss ways to move forward given the limitations identified.\nLastly, we lay out future directions for the Dataset Nutrition Label project,\nincluding research and public policy agendas to further advance consideration\nof the concept.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 18:10:02 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Holland", "Sarah", ""], ["Hosny", "Ahmed", ""], ["Newman", "Sarah", ""], ["Joseph", "Joshua", ""], ["Chmielinski", "Kasia", ""]]}, {"id": "1805.03721", "submitter": "Natanael Arndt", "authors": "Natanael Arndt, Patrick Naumann, Norman Radtke, Michael Martin, Edgard\n  Marx", "title": "Decentralized Collaborative Knowledge Management using Git", "comments": "Special Issue on Managing the Evolution and Preservation of the Data\n  Web", "journal-ref": null, "doi": "10.1016/j.websem.2018.08.002", "report-no": null, "categories": "cs.DB cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web and the Semantic Web are designed as a network of\ndistributed services and datasets. The distributed character of the Web brings\nmanifold collaborative possibilities to interchange data. The commonly adopted\ncollaborative solutions for RDF data are centralized (e.g. SPARQL endpoints and\nwiki systems). But to support distributed collaboration, a system is needed,\nthat supports divergence of datasets, brings the possibility to conflate\ndiverged states, and allows distributed datasets to be synchronized. In this\npaper, we present Quit Store, it was inspired by and it builds upon the\nsuccessful Git system. The approach is based on a formal expression of\nevolution and consolidation of distributed datasets. During the collaborative\ncuration process, the system automatically versions the RDF dataset and tracks\nprovenance information. It also provides support to branch, merge, and\nsynchronize distributed RDF datasets. The merging process is guarded by\nspecific merge strategies for RDF data. Finally, we use our reference\nimplementation to show overall good performance and demonstrate the practical\nusability of the system.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 20:24:20 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 11:48:38 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 07:53:05 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Arndt", "Natanael", ""], ["Naumann", "Patrick", ""], ["Radtke", "Norman", ""], ["Martin", "Michael", ""], ["Marx", "Edgard", ""]]}, {"id": "1805.04104", "submitter": "Mohamed Attia", "authors": "Mohamed Adel Attia, Deepak Kumar, Ravi Tandon", "title": "The Capacity of Private Information Retrieval from Uncoded Storage\n  Constrained Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private information retrieval (PIR) allows a user to retrieve a desired\nmessage from a set of databases without revealing the identity of the desired\nmessage. The replicated databases scenario was considered by Sun and Jafar,\n2016, where $N$ databases can store the same $K$ messages completely. A PIR\nscheme was developed to achieve the optimal download cost given by $\\left(1+\n\\frac{1}{N}+ \\frac{1}{N^{2}}+ \\cdots + \\frac{1}{N^{K-1}}\\right)$. In this work,\nwe consider the problem of PIR from storage constrained databases. Each\ndatabase has a storage capacity of $\\mu KL$ bits, where $L$ is the size of each\nmessage in bits, and $\\mu \\in [1/N, 1]$ is the normalized storage. On one\nextreme, $\\mu=1$ is the replicated databases case. On the other hand, when\n$\\mu= 1/N$, then in order to retrieve a message privately, the user has to\ndownload all the messages from the databases achieving a download cost of\n$1/K$. We aim to characterize the optimal download cost versus storage\ntrade-off for any storage capacity in the range $\\mu \\in [1/N, 1]$. For any\n$(N,K)$, we show that the optimal trade-off between storage, $\\mu$, and the\ndownload cost, $D(\\mu)$, is given by the lower convex hull of the $N$ pairs\n$\\left(\\mu= \\frac{t}{N},D(\\mu) = \\left(1+ \\frac{1}{t}+ \\frac{1}{t^{2}}+ \\cdots\n+ \\frac{1}{t^{K-1}}\\right)\\right)$ for $t=1,2,\\ldots, N$. To prove this result,\nwe first present the storage constrained PIR scheme for any $(N,K)$. We next\nobtain a general lower bound on the download cost for PIR, which is valid for\nthe following storage scenarios: replicated or storage constrained, coded or\nuncoded, and fixed or optimized. We then specialize this bound using the\nuncoded storage assumption to obtain lower bounds matching the achievable\ndownload cost of the storage constrained PIR scheme for any value of the\navailable storage.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 17:59:39 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 18:49:17 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Attia", "Mohamed Adel", ""], ["Kumar", "Deepak", ""], ["Tandon", "Ravi", ""]]}, {"id": "1805.04156", "submitter": "Julia Stoyanovich", "authors": "Benny Kimelfeld, Phokion G. Kolaitis, Julia Stoyanovich", "title": "Computational Social Choice Meets Databases", "comments": "This is an extended version of \"Computational Social Choice Meets\n  Databases\" by Kimelfeld, Kolaitis and Stoyanovich, to appear in IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel framework that aims to create bridges between the\ncomputational social choice and the database management communities. This\nframework enriches the tasks currently supported in computational social choice\nwith relational database context, thus making it possible to formulate\nsophisticated queries about voting rules, candidates, voters, issues, and\npositions. At the conceptual level, we give rigorous semantics to queries in\nthis framework by introducing the notions of necessary answers and possible\nanswers to queries. At the technical level, we embark on an investigation of\nthe computational complexity of the necessary answers. We establish a number of\nresults about the complexity of the necessary answers of conjunctive queries\ninvolving positional scoring rules that contrast sharply with earlier results\nabout the complexity of the necessary winners.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 20:05:59 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Kimelfeld", "Benny", ""], ["Kolaitis", "Phokion G.", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "1805.04265", "submitter": "Feng Tian", "authors": "Feng Tian", "title": "Scripting Relational Database Engine Using Transducer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We allow database user to script a parallel relational database engine with a\nprocedural language. Procedural language code is executed as a user defined\nrelational query operator called transducer. Transducer is tightly integrated\nwith relation engine, including query optimizer, query executor and can be\nexecuted in parallel like other query operators. With transducer, we can\nefficiently execute queries that are very difficult to express in SQL. As\nexample, we show how to run time series and graph queries, etc, within a\nparallel relational database.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 07:52:23 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Tian", "Feng", ""]]}, {"id": "1805.04642", "submitter": "Chengyuan Zhang", "authors": "Jun Long, Lei Zhu, Chengyuan Zhang, Shuangqiao Lin, Zhan Yang, Xinpan\n  Yuan", "title": "HOC-Tree: A Novel Index for efficient Spatio-temporal Range Search", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of mobile computing and Web services, a huge\namount of data with spatial and temporal information have been collected\neveryday by smart mobile terminals, in which an object is described by its\nspatial information and temporal information. Motivated by the significance of\nspatio-temporal range search and the lack of efficient search algorithm, in\nthis paper, we study the problem of spatio-temporal range search (STRS), a\nnovel index structure is proposed, called HOC-Tree, which is based on Hilbert\ncurve and OC-Tree, and takes both spatial and temporal information into\nconsideration. Based on HOC-Tree, we develop an efficient algorithm to solve\nthe problem of spatio-temporal range search. Comprehensive experiments on real\nand synthetic data demonstrate that our method is more efficient than the\nstate-of-the-art technique.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 03:08:53 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Long", "Jun", ""], ["Zhu", "Lei", ""], ["Zhang", "Chengyuan", ""], ["Lin", "Shuangqiao", ""], ["Yang", "Zhan", ""], ["Yuan", "Xinpan", ""]]}, {"id": "1805.05235", "submitter": "Jos\\'e L Balc\\'azar", "authors": "Jos\\'e Luis Balc\\'azar and Marie Ely Piceno and Laura\n  Rodr\\'iguez-Navas", "title": "Decomposition of quantitative Gaifman graphs as a data analysis tool", "comments": "Accepted for presentation at: Intelligent Data Analysis 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We argue the usefulness of Gaifman graphs of first-order relational\nstructures as an exploratory data analysis tool. We illustrate our approach\nwith cases where the modular decompositions of these graphs reveal interesting\nfacts about the data. Then, we introduce generalized notions of Gaifman graphs,\nenhanced with quantitative information, to which we can apply more general,\nexisting decomposition notions via 2-structures; thus enlarging the analytical\ncapabilities of the scheme. The very essence of Gaifman graphs makes this\napproach immediately appropriate for the multirelational data framework.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 15:37:02 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 14:06:40 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Balc\u00e1zar", "Jos\u00e9 Luis", ""], ["Piceno", "Marie Ely", ""], ["Rodr\u00edguez-Navas", "Laura", ""]]}, {"id": "1805.05514", "submitter": "EPTCS", "authors": "Ahmed Al-Brashdi (University of Southampton), Michael Butler\n  (University of Southampton), Abdolbaghi Rezazadeh (University of Southampton)", "title": "Incremental Database Design using UML-B and Event-B", "comments": "In Proceedings IMPEX 2017 and FM&MDD 2017, arXiv:1805.04636", "journal-ref": "EPTCS 271, 2018, pp. 34-47", "doi": "10.4204/EPTCS.271.3", "report-no": null, "categories": "cs.DB cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correct operation of many critical systems is dependent on the data\nconsistency and integrity properties of underlying databases. Therefore, a\nverifiable and rigorous database design process is highly desirable. This\nresearch aims to investigate and deliver a comprehensive and practical approach\nfor modelling databases in formal methods through layered refinements. The\nmethodology is being guided by a number of case studies, using abstraction and\nrefinement in UML-B and verification with the Rodin tool. UML-B is a graphical\nrepresentation of the Event-B formalism and the Rodin tool supports\nverification for Event-B and UML-B. Our method guides developers to model\nrelational databases in UML-B through layered refinement and to specify the\nnecessary constraints and operations on the database.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 01:18:51 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Al-Brashdi", "Ahmed", "", "University of Southampton"], ["Butler", "Michael", "", "University of Southampton"], ["Rezazadeh", "Abdolbaghi", "", "University of Southampton"]]}, {"id": "1805.05670", "submitter": "Siyuan Liu", "authors": "Siyuan Liu, Sourav S Bhowmick, Wanlu Zhang, Shu Wang, Wanyi Huang,\n  Shafiq Joty", "title": "NEURON: Query Optimization Meets Natural Language Processing For\n  Augmenting Database Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational database management system (RDBMS) is a major undergraduate course\ntaught in many universities worldwide as part of their computer science\nprogram. A core component of such course is the design and implementation of\nthe query optimizer in a RDBMS. The goal of the query optimizer is to\nautomatically identify the most efficient execution strategies for executing\nthe declarative SQL queries submitted by users. The query optimization process\nproduces a query execution plan (QEP) which represents an execution strategy\nfor the query. Due to the complexity of the underlying query optimizer,\ncomprehension of a QEP demands that a student is knowledgeable of\nimplementation-specific issues related to the RDBMS. In practice, this is an\nunrealistic assumption to make as most students are learning database\ntechnology for the first time. Hence, it is often difficult for them to\ncomprehend the query execution strategy undertaken by a DBMS by perusing the\nQEP, hindering their learning process. In this demonstration, we present a\nnovel system called NEURON that facilitates natural language interaction with\nQEPs to enhance its understanding. NEURON accepts a SQL query (which may\ninclude joins, aggregation, nesting, among other things) as input, executes it,\nand generates a simplified natural language-based description (both in text and\nvoice form) of the execution strategy deployed by the underlying RDBMS.\nFurthermore, it facilitates understanding of various features related to the\nQEP through a natural language-based question answering framework. We advocate\nthat such tool, world's first of its kind, can greatly enhance students'\nlearning of the query optimization topic.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 09:43:52 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 01:54:36 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 02:03:52 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Liu", "Siyuan", ""], ["Bhowmick", "Sourav S", ""], ["Zhang", "Wanlu", ""], ["Wang", "Shu", ""], ["Huang", "Wanyi", ""], ["Joty", "Shafiq", ""]]}, {"id": "1805.05874", "submitter": "Do Le Quoc", "authors": "Do Le Quoc, Istemi Ekin Akkus, Pramod Bhatotia, Spyros Blanas,\n  Ruichuan Chen, Christof Fetzer, Thorsten Strufe", "title": "Approximate Distributed Joins in Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The join operation is a fundamental building block of parallel data\nprocessing. Unfortunately, it is very resource-intensive to compute an\nequi-join across massive datasets. The approximate computing paradigm allows\nusers to trade accuracy and latency for expensive data processing operations.\nThe equi-join operator is thus a natural candidate for optimization using\napproximation techniques. Although sampling-based approaches are widely used\nfor approximation, sampling over joins is a compelling but challenging task\nregarding the output quality. Naive approaches, which perform joins over\ndataset samples, would not preserve statistical properties of the join output.\n  To realize this potential, we interweave Bloom filter sketching and\nstratified sampling with the join computation in a new operator, ApproxJoin,\nthat preserves the statistical properties of the join output. ApproxJoin\nleverages a Bloom filter to avoid shuffling non-joinable data items around the\nnetwork and then applies stratified sampling to obtain a representative sample\nof the join output.\n  Our analysis shows that ApproxJoin scales well and significantly reduces data\nmovement, without sacrificing tight error bounds on the accuracy of the final\nresults. We implemented ApproxJoin in Apache Spark and evaluated ApproxJoin\nusing microbenchmarks and real-world case studies. The evaluation shows that\nApproxJoin achieves a speedup of 6-9x over unmodified Spark-based joins with\nthe same sampling rate. Furthermore, the speedup is accompanied by a\nsignificant reduction in the shuffled data volume, which is 5-82x less than\nunmodified Spark-based joins.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 16:07:53 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Quoc", "Do Le", ""], ["Akkus", "Istemi Ekin", ""], ["Bhatotia", "Pramod", ""], ["Blanas", "Spyros", ""], ["Chen", "Ruichuan", ""], ["Fetzer", "Christof", ""], ["Strufe", "Thorsten", ""]]}, {"id": "1805.05995", "submitter": "Jianxin Zhao", "authors": "Jianxin Zhao, Tudor Tiplea, Richard Mortier, Jon Crowcroft, Liang Wang", "title": "Data Analytics Service Composition and Deployment on Edge Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics on edge devices has gained rapid growth in research, industry,\nand different aspects of our daily life. This topic still faces many challenges\nsuch as limited computation resource on edge devices. In this paper, we further\nidentify two main challenges: the composition and deployment of data analytics\nservices on edge devices. We present the Zoo system to address these two\nchallenge: on one hand, it provides simple and concise domain-specific language\nto enable easy and and type-safe composition of different data analytics\nservices; on the other, it utilises multiple deployment backends, including\nDocker container, JavaScript, and MirageOS, to accommodate the heterogeneous\nedge deployment environment. We show the expressiveness of Zoo with a use case,\nand thoroughly compare the performance of different deployment backends in\nevaluation.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 00:41:39 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Zhao", "Jianxin", ""], ["Tiplea", "Tudor", ""], ["Mortier", "Richard", ""], ["Crowcroft", "Jon", ""], ["Wang", "Liang", ""]]}, {"id": "1805.06757", "submitter": "Rong Kang", "authors": "Rong Kang, Chen Wang, Peng Wang, Yuting Ding and Jianmin Wang", "title": "Matching Consecutive Subpatterns Over Streaming Time Series", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching of streaming time series with lower latency under limited\ncomputing resource comes to a critical problem, especially as the growth of\nIndustry 4.0 and Industry Internet of Things. However, against traditional\nsingle pattern matching model, a pattern may contain multiple subpatterns\nrepresenting different physical meanings in the real world. Hence, we formulate\na new problem, called \"consecutive subpatterns matching\", which allows users to\nspecify a pattern containing several consecutive subpatterns with various\nspecified thresholds. We propose a novel representation Equal-Length Block\n(ELB) together with two efficient implementations, which work very well under\nall Lp-Norms without false dismissals. Extensive experiments are performed on\nsynthetic and real-world datasets to illustrate that our approach outperforms\nthe brute-force method and MSM, a multi-step filter mechanism over the\nmulti-scaled representation by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 13:34:42 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Kang", "Rong", ""], ["Wang", "Chen", ""], ["Wang", "Peng", ""], ["Ding", "Yuting", ""], ["Wang", "Jianmin", ""]]}, {"id": "1805.07505", "submitter": "Yang Liu", "authors": "Xiang Ao, Yang Liu, Zhen Huang, Luo Zuo, Qing He", "title": "Free-rider Episode Screening via Dual Partition Model", "comments": "The 23rd International Conference on Database Systems for Advanced\n  Applications(DASFAA 2018), 16 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the drawbacks of frequent episode mining is that overwhelmingly many\nof the discovered patterns are redundant. Free-rider episode, as a typical\nexample, consists of a real pattern doped with some additional noise events.\nBecause of the possible high support of the inside noise events, such\nfree-rider episodes may have abnormally high support that they cannot be\nfiltered by frequency based framework. An effective technique for filtering\nfree-rider episodes is using a partition model to divide an episode into two\nconsecutive subepisodes and comparing the observed support of such episode with\nits expected support under the assumption that these two subepisodes occur\nindependently. In this paper, we take more complex subepisodes into\nconsideration and develop a novel partition model named EDP for free-rider\nepisode filtering from a given set of episodes. It combines (1) a dual\npartition strategy which divides an episode to an underlying real pattern and\npotential noises; (2) a novel definition of the expected support of a\nfree-rider episode based on the proposed partition strategy. We can deem the\nepisode interesting if the observed support is substantially higher than the\nexpected support estimated by our model. The experiments on synthetic and\nreal-world datasets demonstrate EDP can effectively filter free-rider episodes\ncompared with existing state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 03:34:09 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Ao", "Xiang", ""], ["Liu", "Yang", ""], ["Huang", "Zhen", ""], ["Zuo", "Luo", ""], ["He", "Qing", ""]]}, {"id": "1805.07599", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhangy, Lei Zhuy, Jun Longy, Shuangqiao Liny, Zhan Yangy,\n  Wenti Huang", "title": "A hybrid index model for efficient spatio-temporal search in HBase", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in geo-positioning technologies and geo-location services,\nthere are a rapidly growing massive amount of spatio-temporal data collected in\nmany applications such as location-aware devices and wireless communication, in\nwhich an object is described by its spatial location and its timestamp.\nConsequently, the study of spatio-temporal search which explores both\ngeo-location information and temporal information of the data has attracted\nsignificant concern from research organizations and commercial communities.\nThis work study the problem of spatio-temporal \\emph{k}-nearest neighbors\nsearch (ST$k$NNS), which is fundamental in the spatial temporal queries. Based\non HBase, a novel index structure is proposed, called \\textbf{H}ybrid\n\\textbf{S}patio-\\textbf{T}emporal HBase \\textbf{I}ndex (\\textbf{HSTI} for\nshort), which is carefully designed and takes both spatial and temporal\ninformation into consideration to effectively reduce the search space. Based on\nHSTI, an efficient algorithm is developed to deal with spatio-temporal\n\\emph{k}-nearest neighbors search. Comprehensive experiments on real and\nsynthetic data clearly show that HSTI is three to five times faster than the\nstate-of-the-art technique.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 14:26:14 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Zhangy", "Chengyuan", ""], ["Zhuy", "Lei", ""], ["Longy", "Jun", ""], ["Liny", "Shuangqiao", ""], ["Yangy", "Zhan", ""], ["Huang", "Wenti", ""]]}, {"id": "1805.08037", "submitter": "Medha Atre", "authors": "Medha Atre", "title": "Algorithms and Analysis for the SPARQL Constructs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Resource Description Framework (RDF) is becoming a popular data modelling\nstandard, the challenges of efficient processing of Basic Graph Pattern (BGP)\nSPARQL queries (a.k.a. SQL inner-joins) have been a focus of the research\ncommunity over the past several years. In our recently published work we\nbrought community's attention to another equally important component of SPARQL,\ni.e., OPTIONAL pattern queries (a.k.a. SQL left-outer-joins). We proposed novel\noptimization techniques -- first of a kind -- and showed experimentally that\nour techniques perform better for the low-selectivity queries, and give at par\nperformance for the highly selective queries, compared to the state-of-the-art\nmethods.\n  BGPs and OPTIONALs (BGP-OPT) make the basic building blocks of the SPARQL\nquery language. Thus, in this paper, treating our BGP-OPT query optimization\ntechniques as the primitives, we extend them to handle other broader components\nof SPARQL such as such as UNION, FILTER, and DISTINCT. We mainly focus on the\nprocedural (algorithmic) aspects of these extensions. We also make several\nimportant observations about the structural aspects of complex SPARQL queries\nwith any intermix of these clauses, and relax some of the constraints regarding\nthe cyclic properties of the queries proposed earlier. We do so without\naffecting the correctness of the results, thus providing more flexibility in\nusing the BGP-OPT optimization techniques.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 13:22:46 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 11:01:07 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 06:12:37 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Atre", "Medha", ""]]}, {"id": "1805.08169", "submitter": "Haochao Huang", "authors": "Haochao Huang", "title": "Cancer Research UK Drug Discovery Process Mining", "comments": "113 pages, 84 figures/tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. The Drug Discovery Unit (DDU) of Cancer Research UK (CRUK) is\nusing the software Dotmatics for storage and analysis of scientific data during\ndrug discovery process. Whilst the data include event logs, time stamps,\nactivities, and user information are mostly sitting in the database without\nfully utilising their potential value. Aims. This dissertation aims at\nextracting knowledge from event logs data which recorded during drug discovery\nprocess, to capture the operational business process of the DDU of Cancer\nResearch UK (CRUK) as it was being executed. It provides the evaluations and\nmethodologies of drawing the process mining panoramic models for the drug\ndiscovery process. Thus by enabling the DDU to maximise its efficiency in\nreviewing its resources and works allocations, patients will benefit from more\nnew treatments faster. Conclusion. Management of organisations can be benefit\nfrom the process mining methodologies. Disco is excellent for non-experts on\nmanagement purposes. ProM is great for expert on research purposes. However,\nthe process mining is not once and for all but is a regular operation\nmanagement process. Indeed, event logs needs to be understand more on the\ntarget organisational behaviours and organisational business process. The\nresearchers have to be aware that event logs data are the most important and\npriority elements in process mining.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 17:53:17 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Huang", "Haochao", ""]]}, {"id": "1805.08520", "submitter": "Hannes M\\\"uhleisen", "authors": "Mark Raasveldt, Hannes M\\\"uhleisen", "title": "MonetDBLite: An Embedded Analytical Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While traditional RDBMSes offer a lot of advantages, they require significant\neffort to setup and to use. Because of these challenges, many data scientists\nand analysts have switched to using alternative data management solutions.\nThese alternatives, however, lack features that are standard for RDBMSes, e.g.\nout-of-core query execution. In this paper, we introduce the embedded\nanalytical database MonetDBLite. MonetDBLite is designed to be both highly\nefficient and easy to use in conjunction with standard analytical tools. It can\nbe installed using standard package managers, and requires no configuration or\nserver management. It is designed for OLAP scenarios, and offers\nnear-instantaneous data transfer between the database and analytical tools, all\nthe while maintaining the transactional guarantees and ACID properties of a\nstandard relational system. These properties make MonetDBLite highly suitable\nas a storage engine for data used in analytics, machine learning and\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 11:50:35 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Raasveldt", "Mark", ""], ["M\u00fchleisen", "Hannes", ""]]}, {"id": "1805.08650", "submitter": "Damiano Carra", "authors": "Pietro Michiardi, Damiano Carra, Sara Migliorini", "title": "Cache-based Multi-query Optimization for Data-intensive Scalable\n  Computing Frameworks", "comments": "12 pages + references, extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern large-scale distributed systems, analytics jobs submitted by\nvarious users often share similar work, for example scanning and processing the\nsame subset of data. Instead of optimizing jobs independently, which may result\nin redundant and wasteful processing, multi-query optimization techniques can\nbe employed to save a considerable amount of cluster resources. In this work,\nwe introduce a novel method combining in-memory cache primitives and\nmulti-query optimization, to improve the efficiency of data-intensive, scalable\ncomputing frameworks. By careful selection and exploitation of common\n(sub)expressions, while satisfying memory constraints, our method transforms a\nbatch of queries into a new, more efficient one which avoids unnecessary\nrecomputations. To find feasible and efficient execution plans, our method uses\na cost-based optimization formulation akin to the multiple-choice knapsack\nproblem. Extensive experiments on a prototype implementation of our system show\nsignificant benefits of worksharing for both TPC-DS workloads and detailed\nmicro-benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 14:59:02 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Michiardi", "Pietro", ""], ["Carra", "Damiano", ""], ["Migliorini", "Sara", ""]]}, {"id": "1805.09149", "submitter": "Steve Berberat", "authors": "Pierre-Andr\\'e Sunier (HES-SO), Steve Berberat (HES-SO)", "title": "Agilit{\\'e} de d{\\'e}veloppement des SI informatis{\\'e}s et outils MDE :\n  d{\\'e}marche p{\\'e}dagogique dans un cours de conception de syst{\\`e}mes\n  d'information informatis{\\'e}s", "comments": "in French", "journal-ref": "20{\\`e}me Conf{\\'e}rence de l'AIM ''Tendances en Management des\n  technologies de l'information : Vers de Nouvelles Cultures et Pratiques\n  Num{\\'e}rique'', May 2015, Rabat, Maroc", "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In software development, business rules implemented by hand using programming\ncode hinder agility of companies. Are our students in information systems aware\nof that? Do our lessons promote this realization ? We use model driven concepts\n(MDA, MDE) in order to demonstrate, with practical examples, that source code\ncan be automatically generated as far as formal specification are sufficient\nand accurate. We have extended usual representation conventions of conceptual\ndata modeling and developed a transformer tool. This make our students\nobserving themselves that no source code need necessarily to be rewrite when a\nchange of business rule occurs, but just an adaptation of the model and a new\nrun of generation of code. In this way, they finally understand more easily the\nbenefits of MDE tools.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 13:37:31 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Sunier", "Pierre-Andr\u00e9", "", "HES-SO"], ["Berberat", "Steve", "", "HES-SO"]]}, {"id": "1805.09157", "submitter": "Vernon Asuncion Va", "authors": "Vernon Asuncion and Yan Zhang", "title": "A New Finitely Controllable Class of Tuple Generating Dependencies: The\n  Triangularly-Guarded Class", "comments": "Submitted for review. arXiv admin note: substantial text overlap with\n  arXiv:1804.05997", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new class of tuple-generating dependencies\n(TGDs) called triangularly-guarded (TG) TGDs. We show that conjunctive query\nanswering under this new class of TGDs is decidable since this new class of\nTGDs also satisfies the finite controllability (FC) property. We further show\nthat this new class strictly contains some other decidable classes such as\nweak-acyclic, guarded, sticky and shy. In this sense, the class TG provides a\nunified representation of all these aforementioned classes of TGDs.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 23:29:09 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Asuncion", "Vernon", ""], ["Zhang", "Yan", ""]]}, {"id": "1805.10511", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Philippe Fournier-Viger, Han-Chieh\n  Chao, Vincent S. Tseng, and Philip S. Yu", "title": "A Survey of Utility-Oriented Pattern Mining", "comments": "Survey paper, accepted by IEEE TKDE, 20 pages", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 2021", "doi": "10.1109/TKDE.2019.2942594", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of data mining and analytics is to find novel, potentially\nuseful patterns that can be utilized in real-world applications to derive\nbeneficial knowledge. For identifying and evaluating the usefulness of\ndifferent kinds of patterns, many techniques and constraints have been\nproposed, such as support, confidence, sequence order, and utility parameters\n(e.g., weight, price, profit, quantity, satisfaction, etc.). In recent years,\nthere has been an increasing demand for utility-oriented pattern mining (UPM,\nor called utility mining). UPM is a vital task, with numerous high-impact\napplications, including cross-marketing, e-commerce, finance, medical, and\nbiomedical applications. This survey aims to provide a general, comprehensive,\nand structured overview of the state-of-the-art methods of UPM. First, we\nintroduce an in-depth understanding of UPM, including concepts, examples, and\ncomparisons with related concepts. A taxonomy of the most common and\nstate-of-the-art approaches for mining different kinds of high-utility patterns\nis presented in detail, including Apriori-based, tree-based, projection-based,\nvertical-/horizontal-data-format-based, and other hybrid approaches. A\ncomprehensive review of advanced topics of existing high-utility pattern mining\ntechniques is offered, with a discussion of their pros and cons. Finally, we\npresent several well-known open-source software packages for UPM. We conclude\nour survey with a discussion on open and practical challenges in this field.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 18:04:16 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 16:35:44 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Fournier-Viger", "Philippe", ""], ["Chao", "Han-Chieh", ""], ["Tseng", "Vincent S.", ""], ["Yu", "Philip S.", ""]]}, {"id": "1805.10515", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Philippe Fournier-Viger, Han-Chieh\n  Chao and Philip S. Yu", "title": "A Survey of Parallel Sequential Pattern Mining", "comments": "Accepted by ACM Trans. on Knowl. Discov. Data, 33 pages", "journal-ref": "ACM Transactions on Knowledge Discovery from Data, 2019", "doi": "10.1145/3314107", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing popularity of shared resources, large volumes of complex\ndata of different types are collected automatically. Traditional data mining\nalgorithms generally have problems and challenges including huge memory cost,\nlow processing speed, and inadequate hard disk space. As a fundamental task of\ndata mining, sequential pattern mining (SPM) is used in a wide variety of\nreal-life applications. However, it is more complex and challenging than other\npattern mining tasks, i.e., frequent itemset mining and association rule\nmining, and also suffers from the above challenges when handling the\nlarge-scale data. To solve these problems, mining sequential patterns in a\nparallel or distributed computing environment has emerged as an important issue\nwith many applications. In this paper, an in-depth survey of the current status\nof parallel sequential pattern mining (PSPM) is investigated and provided,\nincluding detailed categorization of traditional serial SPM approaches, and\nstate of the art parallel SPM. We review the related work of parallel\nsequential pattern mining in detail, including partition-based algorithms for\nPSPM, Apriori-based PSPM, pattern growth based PSPM, and hybrid algorithms for\nPSPM, and provide deep description (i.e., characteristics, advantages,\ndisadvantages and summarization) of these parallel approaches of PSPM. Some\nadvanced topics for PSPM, including parallel quantitative / weighted / utility\nsequential pattern mining, PSPM from uncertain data and stream data, hardware\nacceleration for PSPM, are further reviewed in details. Besides, we review and\nprovide some well-known open-source software of PSPM. Finally, we summarize\nsome challenges and opportunities of PSPM in the big data era.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 18:44:12 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 02:16:58 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Fournier-Viger", "Philippe", ""], ["Chao", "Han-Chieh", ""], ["Yu", "Philip S.", ""]]}, {"id": "1805.10942", "submitter": "Laurent Amsaleg", "authors": "Herwig Lejsek, Bj\\\"orn {\\TH}\\'or J\\'onsson, Laurent Amsaleg,\n  Fri{\\dh}rik Hei{\\dh}ar \\'Asmundsson", "title": "Dynamicity and Durability in Scalable Visual Instance Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual instance search involves retrieving from a collection of images the\nones that contain an instance of a visual query. Systems designed for visual\ninstance search face the major challenge of scalability: a collection of a few\nmillion images used for instance search typically creates a few billion\nfeatures that must be indexed. Furthermore, as real image collections grow\nrapidly, systems must also provide dynamicity, i.e., be able to handle on-line\ninsertions while concurrently serving retrieval operations. Durability, which\nis the ability to recover correctly from software and hardware crashes, is the\nnatural complement of dynamicity. Durability, however, has rarely been\nintegrated within scalable and dynamic high-dimensional indexing solutions.\nThis article addresses the issue of dynamicity and durability for scalable\nindexing of very large and rapidly growing collections of local features for\ninstance retrieval. By extending the NV-tree, a scalable disk-based\nhigh-dimensional index, we show how to implement the ACID properties of\ntransactions which ensure both dynamicity and durability. We present a detailed\nperformance evaluation of the transactional NV-tree: (i) We show that the\ninsertion throughput is excellent despite the overhead for enforcing the ACID\nproperties; (ii) We also show that this transactional index is truly scalable\nusing a standard image benchmark embedded in collections of up to 28.5 billion\nhigh-dimensional vectors; the largest single-server evaluations reported in the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 07:39:55 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 11:21:40 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Lejsek", "Herwig", ""], ["J\u00f3nsson", "Bj\u00f6rn \u00de\u00f3r", ""], ["Amsaleg", "Laurent", ""], ["\u00c1smundsson", "Fri\u00f0rik Hei\u00f0ar", ""]]}, {"id": "1805.11254", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Yunwu Lin, Lei Zhu, XinPan Yuan, Jun Long and Fang\n  Huang", "title": "Hierarchical One Permutation Hashing: Efficient Multimedia Near\n  Duplicate Detection", "comments": "Accepted to appear at Multimedia Tools and Applications", "journal-ref": null, "doi": "10.1007/s11042-018-6178-z", "report-no": null, "categories": "cs.MM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in multimedia technologies and the proliferation of smart\nphone, digital cameras, storage devices, there are a rapidly growing massive\namount of multimedia data collected in many applications such as multimedia\nretrieval and management system, in which the data element is composed of text,\nimage, video and audio. Consequently, the study of multimedia near duplicate\ndetection has attracted significant concern from research organizations and\ncommercial communities. Traditional solution minwish hashing (\\minwise) faces\ntwo challenges: expensive preprocessing time and lower comparison speed. Thus,\nthis work first introduce a hashing method called one permutation hashing\n(\\oph) to shun the costly preprocessing time. Based on \\oph, a more efficient\nstrategy group based one permutation hashing (\\goph) is developed to deal with\nthe high comparison time. Based on the fact that the similarity of most\nmultimedia data is not very high, this work design an new hashing method namely\nhierarchical one permutation hashing (\\hoph) to further improve the\nperformance. Comprehensive experiments on real multimedia datasets clearly show\nthat with similar accuracy \\hoph is five to seven times faster than\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 05:48:14 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 07:42:15 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Lin", "Yunwu", ""], ["Zhu", "Lei", ""], ["Yuan", "XinPan", ""], ["Long", "Jun", ""], ["Huang", "Fang", ""]]}, {"id": "1805.11450", "submitter": "Lingjiao Chen", "authors": "Lingjiao Chen and Paraschos Koutris and Arun Kumar", "title": "Model-based Pricing for Machine Learning in a Data Marketplace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.GT cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics using machine learning (ML) has become ubiquitous in science,\nbusiness intelligence, journalism and many other domains. While a lot of work\nfocuses on reducing the training cost, inference runtime and storage cost of ML\nmodels, little work studies how to reduce the cost of data acquisition, which\npotentially leads to a loss of sellers' revenue and buyers' affordability and\nefficiency.\n  In this paper, we propose a model-based pricing (MBP) framework, which\ninstead of pricing the data, directly prices ML model instances. We first\nformally describe the desired properties of the MBP framework, with a focus on\navoiding arbitrage. Next, we show a concrete realization of the MBP framework\nvia a noise injection approach, which provably satisfies the desired formal\nproperties. Based on the proposed framework, we then provide algorithmic\nsolutions on how the seller can assign prices to models under different market\nscenarios (such as to maximize revenue). Finally, we conduct extensive\nexperiments, which validate that the MBP framework can provide high revenue to\nthe seller, high affordability to the buyer, and also operate on low runtime\ncost.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 06:02:40 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Chen", "Lingjiao", ""], ["Koutris", "Paraschos", ""], ["Kumar", "Arun", ""]]}, {"id": "1805.11517", "submitter": "Torsten Grust", "authors": "Tobias M\\\"uller, Benjamin Dietrich, Torsten Grust", "title": "You Say 'What', I Hear 'Where' and 'Why': (Mis-)Interpreting SQL to\n  Derive Fine-Grained Provenance", "comments": "Extended version of an article published in the Proceedings of the\n  VLDB Endowment (PVLDB, 11(11), August 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SQL declaratively specifies what the desired output of a query is. This work\nshows that a non-standard interpretation of the SQL semantics can, instead,\ndisclose where a piece of the output originated in the input and why that piece\nfound its way into the result. We derive such data provenance for very rich SQL\ndialects (including recursion, windowed aggregates, and user-defined functions)\nat the fine-grained level of individual table cells. The approach is\nnon-invasive and implemented as a compositional source-level SQL rewrite: an\ninput SQL query is transformed into its own interpreter that wields data\ndependencies instead of regular values. We deliberately design this\ntransformation to preserve the shape of both data and query, which allows\nprovenance derivation to scale to complex queries without overwhelming the\nunderlying database system.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 14:50:24 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 11:50:05 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 14:36:37 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["M\u00fcller", "Tobias", ""], ["Dietrich", "Benjamin", ""], ["Grust", "Torsten", ""]]}, {"id": "1805.11723", "submitter": "Zoi Kaoudi", "authors": "Sanjay Chawla, Bertty Contreras-Rojas, Zoi Kaoudi, Sebastian Kruse,\n  Jorge-Arnulfo Quian\\'e-Ruiz", "title": "Building your Cross-Platform Application with RHEEM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, organizations typically perform tedious and costly tasks to juggle\ntheir code and data across different data processing platforms. Addressing this\npain and achieving automatic cross-platform data processing is quite\nchallenging because it requires quite good expertise for all the available data\nprocessing platforms. In this report, we present Rheem, a general-purpose\ncross-platform data processing system that alleviates users from the pain of\nfinding the most efficient data processing platform for a given task. It also\nsplits a task into subtasks and assigns each subtask to a specific platform to\nminimize the overall cost (e.g., runtime or monetary cost). To offer\ncross-platform functionality, it features (i) a robust interface to easily\ncompose data analytic tasks; (ii) a novel cost-based optimizer able to find the\nmost efficient platform in almost all cases; and (iii) an executor to\nefficiently orchestrate tasks over different platforms. As a result, it allows\nusers to focus on the business logic of their applications rather than on the\nmechanics of how to compose and execute them. Rheem is released under an open\nsource license.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 21:46:50 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Chawla", "Sanjay", ""], ["Contreras-Rojas", "Bertty", ""], ["Kaoudi", "Zoi", ""], ["Kruse", "Sebastian", ""], ["Quian\u00e9-Ruiz", "Jorge-Arnulfo", ""]]}, {"id": "1805.11728", "submitter": "Ahmed El-Roby", "authors": "Ahmed El-Roby, Khaled Ammar, Ashraf Aboulnaga, Jimmy Lin", "title": "Sapphire: Querying RDF Data Made Simple", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDF data in the linked open data (LOD) cloud is very valuable for many\ndifferent applications. In order to unlock the full value of this data, users\nshould be able to issue complex queries on the RDF datasets in the LOD cloud.\nSPARQL can express such complex queries, but constructing SPARQL queries can be\na challenge to users since it requires knowing the structure and vocabulary of\nthe datasets being queried. In this paper, we introduce Sapphire, a tool that\nhelps users write syntactically and semantically correct SPARQL queries without\nprior knowledge of the queried datasets. Sapphire interactively helps the user\nwhile typing the query by providing auto-complete suggestions based on the\nqueried data. After a query is issued, Sapphire provides suggestions on ways to\nchange the query to better match the needs of the user. We evaluated Sapphire\nbased on performance experiments and a user study and showed it to be superior\nto competing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 22:22:21 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 13:29:33 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["El-Roby", "Ahmed", ""], ["Ammar", "Khaled", ""], ["Aboulnaga", "Ashraf", ""], ["Lin", "Jimmy", ""]]}, {"id": "1805.11780", "submitter": "Gokhan Kul", "authors": "Gokhan Kul, Shambhu Upadhyaya, Varun Chandola", "title": "Detecting Data Leakage from Databases on Android Apps with Concept Drift", "comments": "This paper is accepted to be published in the proceedings of IEEE\n  TrustCom 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile databases are the statutory backbones of many applications on\nsmartphones, and they store a lot of sensitive information. However,\nvulnerabilities in the operating system or the app logic can lead to sensitive\ndata leakage by giving the adversaries unauthorized access to the app's\ndatabase. In this paper, we study such vulnerabilities to define a threat\nmodel, and we propose an OS-version independent protection mechanism that app\ndevelopers can utilize to detect such attacks. To do so, we model the user\nbehavior with the database query workload created by the original apps. Here,\nwe model the drift in behavior by comparing probability distributions of the\nquery workload features over time. We then use this model to determine if the\napp behavior drift is anomalous. We evaluate our framework on real-world\nworkloads of three different popular Android apps, and we show that our system\nwas able to detect more than 90% of such attacks.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 02:37:13 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Kul", "Gokhan", ""], ["Upadhyaya", "Shambhu", ""], ["Chandola", "Varun", ""]]}, {"id": "1805.11800", "submitter": "Kai Rothauge", "authors": "Alex Gittens, Kai Rothauge, Shusen Wang, Michael W. Mahoney, Lisa\n  Gerhardt, Prabhat, Jey Kottalam, Michael Ringenburg, Kristyn Maschhoff", "title": "Accelerating Large-Scale Data Analysis by Offloading to High-Performance\n  Computing Libraries using Alchemist", "comments": "Accepted for publication in Proceedings of the 24th ACM SIGKDD\n  International Conference on Knowledge Discovery and Data Mining, London, UK,\n  2018", "journal-ref": null, "doi": "10.1145/3219819.3219927", "report-no": null, "categories": "cs.DC cs.DB physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Spark is a popular system aimed at the analysis of large data sets,\nbut recent studies have shown that certain computations---in particular, many\nlinear algebra computations that are the basis for solving common machine\nlearning problems---are significantly slower in Spark than when done using\nlibraries written in a high-performance computing framework such as the\nMessage-Passing Interface (MPI).\n  To remedy this, we introduce Alchemist, a system designed to call MPI-based\nlibraries from Apache Spark. Using Alchemist with Spark helps accelerate linear\nalgebra, machine learning, and related computations, while still retaining the\nbenefits of working within the Spark environment. We discuss the motivation\nbehind the development of Alchemist, and we provide a brief overview of its\ndesign and implementation.\n  We also compare the performances of pure Spark implementations with those of\nSpark implementations that leverage MPI-based codes via Alchemist. To do so, we\nuse data science case studies: a large-scale application of the conjugate\ngradient method to solve very large linear systems arising in a speech\nclassification problem, where we see an improvement of an order of magnitude;\nand the truncated singular value decomposition (SVD) of a 400GB\nthree-dimensional ocean temperature data set, where we see a speedup of up to\n7.9x. We also illustrate that the truncated SVD computation is easily scalable\nto terabyte-sized data by applying it to data sets of sizes up to 17.6TB.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 04:23:41 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Gittens", "Alex", ""], ["Rothauge", "Kai", ""], ["Wang", "Shusen", ""], ["Mahoney", "Michael W.", ""], ["Gerhardt", "Lisa", ""], ["Prabhat", "", ""], ["Kottalam", "Jey", ""], ["Ringenburg", "Michael", ""], ["Maschhoff", "Kristyn", ""]]}, {"id": "1805.11900", "submitter": "Christian Mayer", "authors": "Christian Mayer, Ruben Mayer, Jonas Grunert, Kurt Rothermel, and\n  Muhammad Adnan Tariq", "title": "Q-Graph: Preserving Query Locality in Multi-Query Graph Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arising user-centric graph applications such as route planning and\npersonalized social network analysis have initiated a shift of paradigms in\nmodern graph processing systems towards multi-query analysis, i.e., processing\nmultiple graph queries in parallel on a shared graph. These applications\ngenerate a dynamic number of localized queries around query hotspots such as\npopular urban areas. However, existing graph processing systems are not yet\ntailored towards these properties: The employed methods for graph partitioning\nand synchronization management disregard query locality and dynamism which\nleads to high query latency. To this end, we propose the system Q-Graph for\nmulti-query graph analysis that considers query locality on three levels. (i)\nThe query-aware graph partitioning algorithm Q-cut maximizes query locality to\nreduce communication overhead. (ii) The method for synchronization management,\ncalled hybrid barrier synchronization, allows for full exploitation of local\nqueries spanning only a subset of partitions. (iii) Both methods adapt at\nruntime to changing query workloads in order to maintain and exploit locality.\nOur experiments show that Q-cut reduces average query latency by up to 57\npercent compared to static query-agnostic partitioning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 11:14:30 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Mayer", "Christian", ""], ["Mayer", "Ruben", ""], ["Grunert", "Jonas", ""], ["Rothermel", "Kurt", ""], ["Tariq", "Muhammad Adnan", ""]]}, {"id": "1805.12033", "submitter": "Dhrubajyoti Ghosh", "authors": "Dhrubajyoti Ghosh, Roberto Yus, Yasser Altowim, Sharad Mehrotra", "title": "PIQUE: Progressive Integrated QUery Operator with Pay-As-You-Go\n  Enrichment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data today in the form of text, images, video, and sensor data needs to\nbe enriched (i.e., annotated with tags) prior to be effectively queried or\nanalyzed. Data enrichment (that, depending upon the application could be\ncompiled code, declarative queries, or expensive machine learning and/or signal\nprocessing techniques) often cannot be performed in its entirety as a\npre-processing step at the time of data ingestion. Enriching data as a separate\noffline step after ingestion makes it unavailable for analysis during the\nperiod between the ingestion and enrichment. To bridge such a gap, this paper\nexplores a novel approach that supports progressive data enrichment during\nquery processing in order to support interactive exploratory analysis. Our\napproach is based on integrating an operator, entitled PIQUE, to support a\nprioritized execution of the enrichment functions during query processing.\nQuery processing with the PIQUE operator significantly outperforms the\nbaselines in terms of rate at which answer quality improves during query\nprocessing.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 15:22:33 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 03:30:11 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 22:34:25 GMT"}, {"version": "v4", "created": "Fri, 7 Jun 2019 06:05:11 GMT"}, {"version": "v5", "created": "Fri, 18 Oct 2019 05:40:45 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Ghosh", "Dhrubajyoti", ""], ["Yus", "Roberto", ""], ["Altowim", "Yasser", ""], ["Mehrotra", "Sharad", ""]]}, {"id": "1805.12319", "submitter": "Jingyu Shao Mr.", "authors": "Jingyu Shao, Qing Wang and Yu Lin", "title": "Skyblocking for Entity Resolution", "comments": "14 pages, submit to IS under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, for the first time, we introduce the concept of skyblocking,\nwhich aims to efficiently identify the \"most preferred\" blocking scheme in\nterms of a given set of selection criteria for entity resolution blocking. To\ncapture all possible preferred blocking schemes, scheme skyline (i.e. blocking\nschemes on the skyline) has been studied in a multi-dimensional scheme space\nwith dimensions corresponding to selection criteria for blocking (e.g. PC and\nPQ). However, applying traditional skyline techniques to learn scheme skylines\nis a non-trivial task. Due to the unique characteristics of blocking schemes,\nwe face several challenges, such as: how to find a balanced number of match and\nnon-match labels to effectively approximate a block scheme in a scheme space,\nand how to design efficient skyline algorithms to explore a scheme space for\nfinding scheme skylines. To overcome these challenges, we propose a scheme\nskyline learning approach, which incorporates skyline techniques into an active\nlearning process of scheme skylines. We have conducted experiments over four\nreal-world datasets. The experimental results show that our approach is able to\nefficiently identify scheme skylines in a large scheme space only using a\nlimited number of labels. Our approach also outperforms the state-of-the-art\napproaches for learning blocking schemes in several aspects, including: label\nefficiency, blocking quality and learning efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 05:14:32 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 03:03:12 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 04:16:32 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Shao", "Jingyu", ""], ["Wang", "Qing", ""], ["Lin", "Yu", ""]]}, {"id": "1805.12320", "submitter": "Rong Zhu", "authors": "Rong Zhu, Zhaonian Zou, Yue Han, Sheng Yang, Jianzhong Li", "title": "QuickIM: Efficient, Accurate and Robust Influence Maximization Algorithm\n  on Billion-Scale Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Influence Maximization (IM) problem aims at finding k seed vertices in a\nnetwork, starting from which influence can be spread in the network to the\nmaximum extent. In this paper, we propose QuickIM, the first versatile IM\nalgorithm that attains all the desirable properties of a practically applicable\nIM algorithm at the same time, namely high time efficiency, good result\nquality, low memory footprint, and high robustness. On real-world social\nnetworks, QuickIM achieves the $\\Omega(n + m)$ lower bound on time complexity\nand $\\Omega(n)$ space complexity, where $n$ and $m$ are the number of vertices\nand edges in the network, respectively. Our experimental evaluation verifies\nthe superiority of QuickIM. Firstly, QuickIM runs 1-3 orders of magnitude\nfaster than the state-of-the-art IM algorithms. Secondly, except EasyIM,\nQuickIM requires 1-2 orders of magnitude less memory than the state-of-the-art\nalgorithms. Thirdly, QuickIM always produces as good quality results as the\nstate-of-the-art algorithms. Lastly, the time and the memory performance of\nQuickIM is independent of influence probabilities. On the largest network used\nin the experiments that contains more than 3.6 billion edges, QuickIM is able\nto find hundreds of influential seeds in less than 4 minutes, while all the\nstate-of-the-art algorithms fail to terminate in an hour.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 05:24:16 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Zhu", "Rong", ""], ["Zou", "Zhaonian", ""], ["Han", "Yue", ""], ["Yang", "Sheng", ""], ["Li", "Jianzhong", ""]]}, {"id": "1805.12502", "submitter": "Zhaoqiang Chen", "authors": "Zhaoqiang Chen, Qun Chen, Boyi Hou, Murtadha Ahmed, Zhanhuai Li", "title": "Improving Machine-based Entity Resolution with Limited Human Effort: A\n  Risk Perspective", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": "10.1145/3242153.3242156", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pure machine-based solutions usually struggle in the challenging\nclassification tasks such as entity resolution (ER). To alleviate this problem,\na recent trend is to involve the human in the resolution process, most notably\nthe crowdsourcing approach. However, it remains very challenging to effectively\nimprove machine-based entity resolution with limited human effort. In this\npaper, we investigate the problem of human and machine cooperation for ER from\na risk perspective. We propose to select the machine-labeled instances at high\nrisk of being mislabeled for manual verification. For this task, we present a\nrisk model that takes into consideration the human-labeled instances as well as\nthe output of machine resolution. Finally, we evaluate the performance of the\nproposed risk model on real data. Our experiments demonstrate that it can pick\nup the mislabeled instances with considerably higher accuracy than the existing\nalternatives. Provided with the same amount of human cost budget, it can also\nachieve better resolution quality than the state-of-the-art approach based on\nactive learning.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 14:54:55 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 09:12:46 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Chen", "Zhaoqiang", ""], ["Chen", "Qun", ""], ["Hou", "Boyi", ""], ["Ahmed", "Murtadha", ""], ["Li", "Zhanhuai", ""]]}, {"id": "1805.12503", "submitter": "Xiaoying Mou", "authors": "Yeting Li and Xinyu Chu and Xiaoying Mou and Chunmei Dong and Haiming\n  Chen", "title": "Practical Study of Deterministic Regular Expressions from Large-scale\n  XML and Schema Data", "comments": "9 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular expressions are a fundamental concept in computer science and widely\nused in various applications. In this paper we focused on deterministic regular\nexpressions (DREs). Considering that researchers didn't have large datasets as\nevidence before, we first harvested a large corpus of real data from the Web\nthen conducted a practical study to investigate the usage of DREs. One feature\nof our work is that the data set is sufficiently large compared with previous\nwork, which is obtained using several data collection strategies we proposed.\nThe results show more than 98\\% of expressions in Relax NG are DRE, and more\nthan 56\\% of expressions from RegExLib are DRE, while both Relax NG and\nRegExLib do not have the determinism constraint. These observations indicate\nthat DREs are commonly used in practice. The results also show further study of\nsubclasses of DREs is necessary. As far as we know, we are the first to analyze\nthe determinism and the subclasses of DREs of Relax NG and RegExLib, and give\nthese results. Furthermore, we give some discussions and applications of the\ndata set. We obtain a DRE data set from the original data, which will be useful\nin practice and it has value in its own right. We find current research in new\nsubclasses of DREs is insufficient, therefore it is necessary to do further\nstudy. We also analyze the referencing relationships among XSDs and define\nSchemaRank, which can be used in XML Schema design.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 14:56:02 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Li", "Yeting", ""], ["Chu", "Xinyu", ""], ["Mou", "Xiaoying", ""], ["Dong", "Chunmei", ""], ["Chen", "Haiming", ""]]}]