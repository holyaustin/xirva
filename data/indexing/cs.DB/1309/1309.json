[{"id": "1309.0373", "submitter": "Sebastiaan van Schaik", "authors": "Sebastiaan J. van Schaik and Dan Olteanu and Robert Fink", "title": "ENFrame: A Platform for Processing Probabilistic Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces ENFrame, a unified data processing platform for\nquerying and mining probabilistic data. Using ENFrame, users can write programs\nin a fragment of Python with constructs such as bounded-range loops, list\ncomprehension, aggregate operations on lists, and calls to external database\nengines. The program is then interpreted probabilistically by ENFrame.\n  The realisation of ENFrame required novel contributions along several\ndirections. We propose an event language that is expressive enough to\nsuccinctly encode arbitrary correlations, trace the computation of user\nprograms, and allow for computation of discrete probability distributions of\nprogram variables. We exemplify ENFrame on three clustering algorithms:\nk-means, k-medoids, and Markov Clustering. We introduce sequential and\ndistributed algorithms for computing the probability of interconnected events\nexactly or approximately with error guarantees. Experiments with k-medoids\nclustering of sensor readings from energy networks show orders-of-magnitude\nimprovements of exact clustering using ENFrame over na\\\"ive clustering in each\npossible world, of approximate over exact, and of distributed over sequential\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 12:02:34 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["van Schaik", "Sebastiaan J.", ""], ["Olteanu", "Dan", ""], ["Fink", "Robert", ""]]}, {"id": "1309.0634", "submitter": "Anastasios Gounaris", "authors": "Georgios Koutsoumpakis, Iakovos Koutsoumpakis and Anastasios Gounaris", "title": "Skew Handling in Aggregate Streaming Queries on GPUs", "comments": "11 pages, 11 Postscript figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the data to be processed by database systems has grown so large\nthat any conventional, centralized technique is inadequate. At the same time,\ngeneral purpose computation on GPU (GPGPU) recently has successfully drawn\nattention from the data management community due to its ability to achieve\nsignificant speed-ups at a small cost. Efficient skew handling is a well-known\nproblem in parallel queries, independently of the execution environment. In\nthis work, we investigate solutions to the problem of load imbalances in\nparallel aggregate queries on GPUs that are caused by skewed data. We present a\ngeneric load-balancing framework along with several instantiations, which we\nexperimentally evaluate. To the best of our knowledge, this is the first\nattempt to present runtime load-balancing techniques for database operations on\nGPUs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 10:25:20 GMT"}], "update_date": "2013-09-04", "authors_parsed": [["Koutsoumpakis", "Georgios", ""], ["Koutsoumpakis", "Iakovos", ""], ["Gounaris", "Anastasios", ""]]}, {"id": "1309.1334", "submitter": "Alan Schmitt", "authors": "Todd J. Green and Alan Schmitt", "title": "Proceedings of the 14th International Symposium on Database Programming\n  Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento, Italy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers presented at the 14th Symposium on Database\nProgramming Languages (DBPL 2013) held on August 30th, 2013, in Riva del Garda,\nco-located with the 39th International Conference on Very Large Databases (VLDB\n2013). They cover a wide range of topics including the application of\nprogramming language techniques to further the expressiveness of database\nlanguages, schema management, and the practical use of XPath. To complement\nthis technical program, DBPL 2013 featured three invited talks by Serge\nAbiteboul (Inria), J\\'er\\^ome Sim\\'eon (IBM), and Soren Lassen (Facebook).\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 12:48:03 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 09:48:21 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Green", "Todd J.", ""], ["Schmitt", "Alan", ""]]}, {"id": "1309.1556", "submitter": "Xiaoyan Guo", "authors": "Yu cao, Xiaoyan Guo, Stephen Todd", "title": "Hyper-Graph Based Database Partitioning for Transactional Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A common approach to scaling transactional databases in practice is\nhorizontal partitioning, which increases system scalability, high availability\nand self-manageability. Usu- ally it is very challenging to choose or design an\noptimal partitioning scheme for a given workload and database. In this\ntechnical report, we propose a fine-grained hyper-graph based database\npartitioning system for transactional work- loads. The partitioning system\ntakes a database, a workload, a node cluster and partitioning constraints as\ninput and out- puts a lookup-table encoding the final database partitioning\ndecision. The database partitioning problem is modeled as a multi-constraints\nhyper-graph partitioning problem. By deriving a min-cut of the hyper-graph, our\nsystem can min- imize the total number of distributed transactions in the\nworkload, balance the sizes and workload accesses of the partitions and satisfy\nall the partition constraints imposed. Our system is highly interactive as it\nallows users to im- pose partition constraints, watch visualized partitioning\nef- fects, and provide feedback based on human expertise and indirect domain\nknowledge for generating better partition- ing schemes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 07:35:53 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["cao", "Yu", ""], ["Guo", "Xiaoyan", ""], ["Todd", "Stephen", ""]]}, {"id": "1309.1784", "submitter": "David Koop", "authors": "David Koop, Juliana Freire, and Claudio T. Silva", "title": "Enabling Reproducible Science with VisTrails", "comments": "Accepted for WSSSPE 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing amount of data and use of computation in science,\nsoftware has become an important component in many different domains. Computing\nis now being used more often and in more aspects of scientific work including\ndata acquisition, simulation, analysis, and visualization. To ensure\nreproducibility, it is important to capture the different computational\nprocesses used as well as their executions. VisTrails is an open-source\nscientific workflow system for data analysis and visualization that seeks to\naddress the problem of integrating varied tools as well as automatically\ndocumenting the methods and parameters employed. Growing from a specific\nproject need to supporting a wide array of users required close collaborations\nin addition to new research ideas to design a usable and efficient system. The\nVisTrails project now includes standard software processes like unit testing\nand developer documentation while serving as a base for further research. In\nthis paper, we describe how VisTrails has developed and how our efforts in\nstructuring and advertising the system have contributed to its adoption in many\ndomains.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 21:57:09 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2013 22:07:10 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Koop", "David", ""], ["Freire", "Juliana", ""], ["Silva", "Claudio T.", ""]]}, {"id": "1309.1807", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "Aggregate-Max Nearest Neighbor Searching in the Plane", "comments": "17 pages, 14 figures; preliminary results appeared in CCCG 2013, and\n  in this new version we extend the top-1 queries to top-k queries for the L_1\n  case", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the aggregate/group nearest neighbor searching for the MAX operator\nin the plane. For a set $P$ of $n$ points and a query set $Q$ of $m$ points,\nthe query asks for a point of $P$ whose maximum distance to the points in $Q$\nis minimized. We present data structures for answering such queries for both\n$L_1$ and $L_2$ distance measures. Previously, only heuristic and approximation\nalgorithms were given for both versions. For the $L_1$ version, we build a data\nstructure of O(n) size in $O(n\\log n)$ time, such that each query can be\nanswered in $O(m+\\log n)$ time. For the $L_2$ version, we build a data\nstructure in $O(n\\log n)$ time and $O(n\\log \\log n)$ space, such that each\nquery can be answered in $O(m\\sqrt{n}\\log^{O(1)} n)$ time, and alternatively,\nwe build a data structure in $O(n^{2+\\epsilon})$ time and space for any\n$\\epsilon>0$, such that each query can be answered in $O(m\\log n)$ time.\nFurther, we extend our result for the $L_1$ version to the top-$k$ queries\nwhere each query asks for the $k$ points of $P$ whose maximum distances to $Q$\nare the smallest for any $k$ with $1\\leq k\\leq n$: We build a data structure of\nO(n) size in $O(n\\log n)$ time, such that each top-$k$ query can be answered in\n$O(m+k\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 02:35:40 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "1309.1884", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Jaffer Gardezi", "title": "Tractable vs. Intractable Cases of Matching Dependencies for Query\n  Answering under Entity Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching Dependencies (MDs) are a relatively recent proposal for declarative\nentity resolution. They are rules that specify, on the basis of similarities\nsatisfied by values in a database, what values should be considered duplicates,\nand have to be matched. On the basis of a chase-like procedure for MD\nenforcement, we can obtain clean (duplicate-free) instances; actually possibly\nseveral of them. The resolved answers to queries are those that are invariant\nunder the resulting class of resolved instances. Previous work identified\ncertain classes of queries and sets of MDs for which resolved query answering\nis tractable. Special emphasis was placed on cyclic sets of MDs. In this work\nwe further investigate the complexity of this problem, identifying intractable\ncases, and exploring the frontier between tractability and intractability. We\nconcentrate mostly on acyclic sets of MDs. For a special case we obtain a\ndichotomy result relative to NP-hardness.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 17:12:36 GMT"}, {"version": "v2", "created": "Sun, 6 Apr 2014 21:40:10 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Gardezi", "Jaffer", ""]]}, {"id": "1309.2371", "submitter": "Arpna Shrivastava", "authors": "Arpna Shrivastava, R.C. Jain", "title": "Performance analysis of modified algorithm for finding multilevel\n  association rules", "comments": null, "journal-ref": "Computer Science & Engineering: An International Journal (CSEIJ),\n  Vol. 3, No. 4, August 2013", "doi": "10.5121/cseij.2013.3401", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel association rules explore the concept hierarchy at multiple levels\nwhich provides more specific information. Apriori algorithm explores the single\nlevel association rules. Many implementations are available of Apriori\nalgorithm. Fast Apriori implementation is modified to develop new algorithm for\nfinding multilevel association rules. In this study the performance of this new\nalgorithm is analyzed in terms of running time in seconds.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 04:52:58 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Shrivastava", "Arpna", ""], ["Jain", "R. C.", ""]]}, {"id": "1309.2517", "submitter": "Srikantaiah K C", "authors": "R. H. Vishwanath, S. Leena, K. C. Srikantaiah, K. Shreekrishna Kumar,\n  P. Deepa Shenoy, K. R. Venugopal, S. S. Iyengar and L. M. Patnaik", "title": "Forecasting Stock Time-Series using Data Approximation and Pattern\n  Sequence Similarity", "comments": "11 pages", "journal-ref": "International Journal of Information Processing, 7(2), 90-100,\n  2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series analysis is the process of building a model using statistical\ntechniques to represent characteristics of time series data. Processing and\nforecasting huge time series data is a challenging task. This paper presents\nApproximation and Prediction of Stock Time-series data (APST), which is a two\nstep approach to predict the direction of change of stock price indices. First,\nperforms data approximation by using the technique called Multilevel Segment\nMean (MSM). In second phase, prediction is performed for the approximated data\nusing Euclidian distance and Nearest-Neighbour technique. The computational\ncost of data approximation is O(n ni) and computational cost of prediction task\nis O(m |NN|). Thus, the accuracy and the time required for prediction in the\nproposed method is comparatively efficient than the existing Label Based\nForecasting (LBF) method [1].\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 14:05:09 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Vishwanath", "R. H.", ""], ["Leena", "S.", ""], ["Srikantaiah", "K. C.", ""], ["Kumar", "K. Shreekrishna", ""], ["Shenoy", "P. Deepa", ""], ["Venugopal", "K. R.", ""], ["Iyengar", "S. S.", ""], ["Patnaik", "L. M.", ""]]}, {"id": "1309.2597", "submitter": "Bondu Venkateswarlu", "authors": "Bondu Venkateswarlu and Prof G.S.V.Prasad Raju", "title": "Mine Blood Donors Information through Improved K-Means Clustering", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of accidents and health diseases which are increasing at an\nalarming rate are resulting in a huge increase in the demand for blood. There\nis a necessity for the organized analysis of the blood donor database or blood\nbanks repositories. Clustering analysis is one of the data mining applications\nand K-means clustering algorithm is the fundamental algorithm for modern\nclustering techniques. K-means clustering algorithm is traditional approach and\niterative algorithm. At every iteration, it attempts to find the distance from\nthe centroid of each cluster to each and every data point. This paper gives the\nimprovement to the original k-means algorithm by improving the initial\ncentroids with distribution of data. Results and discussions show that improved\nK-means algorithm produces accurate clusters in less computation time to find\nthe donors information.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 18:07:58 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Venkateswarlu", "Bondu", ""], ["Raju", "Prof G. S. V. Prasad", ""]]}, {"id": "1309.2655", "submitter": "Daniel Zinn", "authors": "Sven K\\\"ohler and Bertram Lud\\\"ascher and Daniel Zinn", "title": "First-Order Provenance Games", "comments": null, "journal-ref": "Peter Buneman Festschrift, LNCS 8000, 2013", "doi": "10.1007/978-3-642-41660-6_20", "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model of provenance, based on a game-theoretic approach to\nquery evaluation. First, we study games G in their own right, and ask how to\nexplain that a position x in G is won, lost, or drawn. The resulting notion of\ngame provenance is closely related to winning strategies, and excludes from\nprovenance all \"bad moves\", i.e., those which unnecessarily allow the opponent\nto improve the outcome of a play. In this way, the value of a position is\ndetermined by its game provenance. We then define provenance games by viewing\nthe evaluation of a first-order query as a game between two players who argue\nwhether a tuple is in the query answer. For RA+ queries, we show that game\nprovenance is equivalent to the most general semiring of provenance polynomials\nN[X]. Variants of our game yield other known semirings. However, unlike\nsemiring provenance, game provenance also provides a \"built-in\" way to handle\nnegation and thus to answer why-not questions: In (provenance) games, the\nreason why x is not won, is the same as why x is lost or drawn (the latter is\npossible for games with draws). Since first-order provenance games are\ndraw-free, they yield a new provenance model that combines how- and why-not\nprovenance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 20:19:37 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["K\u00f6hler", "Sven", ""], ["Lud\u00e4scher", "Bertram", ""], ["Zinn", "Daniel", ""]]}, {"id": "1309.2675", "submitter": "Robert McColl", "authors": "Rob McColl, David Ediger, Jason Poovey, Dan Campbell, David Bader", "title": "A Brief Study of Open Source Graph Databases", "comments": "WSSSPE13, 4 Pages, 18 Pages with Appendix, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.SE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  With the proliferation of large irregular sparse relational datasets, new\nstorage and analysis platforms have arisen to fill gaps in performance and\ncapability left by conventional approaches built on traditional database\ntechnologies and query languages. Many of these platforms apply graph\nstructures and analysis techniques to enable users to ingest, update, query and\ncompute on the topological structure of these relationships represented as\nset(s) of edges between set(s) of vertices. To store and process Facebook-scale\ndatasets, they must be able to support data sources with billions of edges,\nupdate rates of millions of updates per second, and complex analysis kernels.\nThese platforms must provide intuitive interfaces that enable graph experts and\nnovice programmers to write implementations of common graph algorithms. In this\npaper, we explore a variety of graph analysis and storage platforms. We compare\ntheir capabil- ities, interfaces, and performance by implementing and computing\na set of real-world graph algorithms on synthetic graphs with up to 256 million\nedges. In the spirit of full disclosure, several authors are affiliated with\nthe development of STINGER.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 18:36:33 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["McColl", "Rob", ""], ["Ediger", "David", ""], ["Poovey", "Jason", ""], ["Campbell", "Dan", ""], ["Bader", "David", ""]]}, {"id": "1309.2687", "submitter": "Han Su", "authors": "Han Su", "title": "CrowdPlanner: A Crowd-Based Route Recommendation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CrowdPlanner -- a novel crowd-based route recommendation system has been\ndeveloped, which requests human workers to evaluate candidates routes\nrecommended by different sources and methods, and determine the best route\nbased on the feedbacks of these workers. Our system addresses two critical\nissues in its core components: a) task generation component generates a series\nof informative and concise questions with optimized ordering for a given\ncandidate route set so that workers feel comfortable and easy to answer; and b)\nworker selection component utilizes a set of selection criteria and an\nefficient algorithm to find the most eligible workers to answer the questions\nwith high accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 23:06:57 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Su", "Han", ""]]}, {"id": "1309.3439", "submitter": "Reza Malekian Ph.D.", "authors": "Zhong-qin Wang, Ning Ye, Malekian Reza, Ting-ting Zhao, Ru-chuan Wang", "title": "Measuring the similarity of PML documents with RFID-based sensors", "comments": "International Journal of Ad Hoc and Ubiquitous Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Electronic Product Code (EPC) Network is an important part of the\nInternet of Things. The Physical Mark-Up Language (PML) is to represent and\nde-scribe data related to objects in EPC Network. The PML documents of each\ncomponent to exchange data in EPC Network system are XML documents based on PML\nCore schema. For managing theses huge amount of PML documents of tags captured\nby Radio frequency identification (RFID) readers, it is inevitable to develop\nthe high-performance technol-ogy, such as filtering and integrating these tag\ndata. So in this paper, we propose an approach for meas-uring the similarity of\nPML documents based on Bayesian Network of several sensors. With respect to the\nfeatures of PML, while measuring the similarity, we firstly reduce the\nredundancy data except information of EPC. On the basis of this, the Bayesian\nNetwork model derived from the structure of the PML documents being compared is\nconstructed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 18:00:09 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Wang", "Zhong-qin", ""], ["Ye", "Ning", ""], ["Reza", "Malekian", ""], ["Zhao", "Ting-ting", ""], ["Wang", "Ru-chuan", ""]]}, {"id": "1309.3733", "submitter": "Jixue Liu", "authors": "Jixue Liu and Selasi Kwashie and Jiuyong Li and Feiyue Ye and Millist\n  Vincent", "title": "Discovery of Approximate Differential Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential dependencies (DDs) capture the relationships between data\ncolumns of relations. They are more general than functional dependencies (FDs)\nand and the difference is that DDs are defined on the distances between values\nof two tuples, not directly on the values. Because of this difference, the\nalgorithms for discovering FDs from data find only special DDs, not all DDs and\ntherefore are not applicable to DD discovery. In this paper, we propose an\nalgorithm to discover DDs from data following the way of fixing the left hand\nside of a candidate DD to determine the right hand side. We also show some\nproperties of DDs and conduct a comprehensive analysis on how sampling affects\nthe DDs discovered from data.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2013 06:21:30 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Liu", "Jixue", ""], ["Kwashie", "Selasi", ""], ["Li", "Jiuyong", ""], ["Ye", "Feiyue", ""], ["Vincent", "Millist", ""]]}, {"id": "1309.3964", "submitter": "Kato Mivule", "authors": "Kato Mivule and Claude Turner", "title": "An Investigation of Data Privacy and Utility Preservation using KNN\n  Classification as a Gauge", "comments": "2 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is obligatory that organizations by law safeguard the privacy of\nindividuals when handling data sets containing personal identifiable\ninformation (PII). Nevertheless, during the process of data privatization, the\nutility or usefulness of the privatized data diminishes. Yet achieving the\noptimal balance between data privacy and utility needs has been documented as\nan NP-hard challenge. In this study, we investigate data privacy and utility\npreservation using KNN machine learning classification as a gauge.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 14:02:09 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Mivule", "Kato", ""], ["Turner", "Claude", ""]]}, {"id": "1309.4396", "submitter": "Panagiotis Bouros Panagiotis Bouros", "authors": "Dimitris Sacharidis, Panagiotis Bouros", "title": "Routing Directions: Keeping it Fast and Simple", "comments": "Full version of the SIGSPATIAL'13 paper", "journal-ref": "21st ACM SIGSPATIAL International Conference on Advances in\n  Geographic Information Systems (ACM SIGSPATIAL GIS 2013), Orlando, Florida,\n  USA, November 5-8, 2013", "doi": "10.1145/2525314.2525362", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of providing meaningful routing directions over road networks is\nof great importance. In many real-life cases, the fastest route may not be the\nideal choice for providing directions in written, spoken text, or for an\nunfamiliar neighborhood, or in cases of emergency. Rather, it is often more\npreferable to offer \"simple\" directions that are easy to memorize, explain,\nunderstand or follow. However, there exist cases where the simplest route is\nconsiderably longer than the fastest. This paper tries to address this issue,\nby finding near-simplest routes which are as short as possible and near-fastest\nroutes which are as simple as possible. Particularly, we focus on efficiency,\nand propose novel algorithms, which are theoretically and experimentally shown\nto be significantly faster than existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 17:14:49 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Sacharidis", "Dimitris", ""], ["Bouros", "Panagiotis", ""]]}, {"id": "1309.4927", "submitter": "Miika Hannula", "authors": "Miika Hannula and Juha Kontinen", "title": "A finite axiomatization of conditional independence and inclusion\n  dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete finite axiomatization of the unrestricted implication\nproblem for inclusion and conditional independence atoms in the context of\ndependence logic. For databases, our result implies a finite axiomatization of\nthe unrestricted implication problem for inclusion, functional, and embedded\nmultivalued dependencies in the unirelational case.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 10:59:05 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2013 07:58:30 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Hannula", "Miika", ""], ["Kontinen", "Juha", ""]]}, {"id": "1309.5821", "submitter": "Adam Barker", "authors": "Jonathan Stuart Ward and Adam Barker", "title": "Undefined By Data: A Survey of Big Data Definitions", "comments": "Big data definition paper, 2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term big data has become ubiquitous. Owing to a shared origin between\nacademia, industry and the media there is no single unified definition, and\nvarious stakeholders provide diverse and often contradictory definitions. The\nlack of a consistent definition introduces ambiguity and hampers discourse\nrelating to big data. This short paper attempts to collate the various\ndefinitions which have gained some degree of traction and to furnish a clear\nand concise definition of an otherwise ambiguous term.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 13:51:18 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Ward", "Jonathan Stuart", ""], ["Barker", "Adam", ""]]}, {"id": "1309.5822", "submitter": "Vince B", "authors": "Vince B\\'ar\\'any (University of Warsaw), Georg Gottlob (Oxford\n  University Computing Laboratory), Martin Otto (Technische Universit\\\"at\n  Darmstadt)", "title": "Querying the Guarded Fragment", "comments": "This is an improved and extended version of the paper of the same\n  title presented at LICS 2010", "journal-ref": "Logical Methods in Computer Science, Volume 10, Issue 2 (May 21,\n  2014) lmcs:675", "doi": "10.2168/LMCS-10(2:3)2014", "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating a Boolean conjunctive query Q against a guarded first-order theory\nF is equivalent to checking whether \"F and not Q\" is unsatisfiable. This\nproblem is relevant to the areas of database theory and description logic.\nSince Q may not be guarded, well known results about the decidability,\ncomplexity, and finite-model property of the guarded fragment do not obviously\ncarry over to conjunctive query answering over guarded theories, and had been\nleft open in general. By investigating finite guarded bisimilar covers of\nhypergraphs and relational structures, and by substantially generalising\nRosati's finite chase, we prove for guarded theories F and (unions of)\nconjunctive queries Q that (i) Q is true in each model of F iff Q is true in\neach finite model of F and (ii) determining whether F implies Q is\n2EXPTIME-complete. We further show the following results: (iii) the existence\nof polynomial-size conformal covers of arbitrary hypergraphs; (iv) a new proof\nof the finite model property of the clique-guarded fragment; (v) the small\nmodel property of the guarded fragment with optimal bounds; (vi) a\npolynomial-time solution to the canonisation problem modulo guarded\nbisimulation, which yields (vii) a capturing result for guarded bisimulation\ninvariant PTIME.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 14:39:46 GMT"}, {"version": "v2", "created": "Sat, 29 Mar 2014 17:41:57 GMT"}, {"version": "v3", "created": "Sat, 26 Apr 2014 19:42:47 GMT"}, {"version": "v4", "created": "Tue, 20 May 2014 07:30:41 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["B\u00e1r\u00e1ny", "Vince", "", "University of Warsaw"], ["Gottlob", "Georg", "", "Oxford\n  University Computing Laboratory"], ["Otto", "Martin", "", "Technische Universit\u00e4t\n  Darmstadt"]]}, {"id": "1309.5927", "submitter": "Markus Lohrey", "authors": "Mireille Bousquet-Melou, Markus Lohrey, Sebastian Maneth, and Eric\n  Noeth", "title": "XML Compression via DAGs", "comments": "A short version of this paper appeared in the Proceedings of ICDT\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unranked trees can be represented using their minimal dag (directed acyclic\ngraph). For XML this achieves high compression ratios due to their repetitive\nmark up. Unranked trees are often represented through first child/next sibling\n(fcns) encoded binary trees. We study the difference in size (= number of\nedges) of minimal dag versus minimal dag of the fcns encoded binary tree. One\nmain finding is that the size of the dag of the binary tree can never be\nsmaller than the square root of the size of the minimal dag, and that there are\nexamples that match this bound. We introduce a new combined structure, the\nhybrid dag, which is guaranteed to be smaller than (or equal in size to) both\ndags. Interestingly, we find through experiments that last child/previous\nsibling encodings are much better for XML compression via dags, than fcns\nencodings. We determine the average sizes of unranked and binary dags over a\ngiven set of labels (under uniform distribution) in terms of their exact\ngenerating functions, and in terms of their asymptotical behavior.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 19:26:16 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Bousquet-Melou", "Mireille", ""], ["Lohrey", "Markus", ""], ["Maneth", "Sebastian", ""], ["Noeth", "Eric", ""]]}, {"id": "1309.6608", "submitter": "sukhjit Singh Sehra Er.", "authors": "Sukhjit Singh Sehra, Jaiteg Singh and Hardeep Singh Rai", "title": "Assessment of OpenStreetMap Data - A Review", "comments": "Review paper", "journal-ref": "International Journal of Computer Applications, 76(16):17-20,\n  August 2013", "doi": "10.5120/13331-0888 10.5120/13331-0888", "report-no": null, "categories": "cs.CY cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The meaning and purposes of web has been changing and evolving day by day.\nWeb 2. 0 encouraged more contribution by the end users. This movement provided\nrevolutionary methods of sharing and computing data by crowdsourcing such as\nOpenStreetmap, also called \"the wikification of maps\" by some researchers. When\ncrowdsourcing collects huge data with help of general public with varying level\nof mapping experience, the focus of researcher should be on analysing the data\nrather than collecting it. Researchers have assessed the quality of\nOpenStreetMap data by comparing it with proprietary data or data of\ngovernmental map agencies. This study reviews the research work for assessment\nof Open- StreetMap Data and also discusses about the future directions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 18:52:48 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Sehra", "Sukhjit Singh", ""], ["Singh", "Jaiteg", ""], ["Rai", "Hardeep Singh", ""]]}, {"id": "1309.6815", "submitter": "Paul Beame", "authors": "Paul Beame, Jerry Li, Sudeepa Roy, Dan Suciu", "title": "Lower Bounds for Exact Model Counting and Applications in Probabilistic\n  Databases", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-52-61", "categories": "cs.DB cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best current methods for exactly computing the number of satisfying\nassignments, or the satisfying probability, of Boolean formulas can be seen,\neither directly or indirectly, as building 'decision-DNNF' (decision\ndecomposable negation normal form) representations of the input Boolean\nformulas. Decision-DNNFs are a special case of 'd-DNNF's where 'd' stands for\n'deterministic'. We show that any decision-DNNF can be converted into an\nequivalent 'FBDD' (free binary decision diagram) -- also known as a 'read-once\nbranching program' (ROBP or 1-BP) -- with only a quasipolynomial increase in\nrepresentation size in general, and with only a polynomial increase in size in\nthe special case of monotone k-DNF formulas. Leveraging known exponential lower\nbounds for FBDDs, we then obtain similar exponential lower bounds for\ndecision-DNNFs which provide lower bounds for the recent algorithms. We also\nseparate the power of decision-DNNFs from d-DNNFs and a generalization of\ndecision-DNNFs known as AND-FBDDs. Finally we show how these imply exponential\nlower bounds for natural problems associated with probabilistic databases.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:29:56 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Beame", "Paul", ""], ["Li", "Jerry", ""], ["Roy", "Sudeepa", ""], ["Suciu", "Dan", ""]]}]