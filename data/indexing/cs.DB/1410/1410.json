[{"id": "1410.0205", "submitter": "Michael Shekelyan", "authors": "Michael Shekelyan, Gregor Joss\\'e, Matthias Schubert", "title": "ParetoPrep: Fast computation of Path Skylines Queries", "comments": "12 pages, 9 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing cost optimal paths in network data is a very important task in many\napplication areas like transportation networks, computer networks or social\ngraphs. In many cases, the cost of an edge can be described by various cost\ncriteria. For example, in a road network possible cost criteria are distance,\ntime, ascent, energy consumption or toll fees. In such a multicriteria network,\na route or path skyline query computes the set of all paths having pareto\noptimal costs, i.e. each result path is optimal for different user preferences.\nIn this paper, we propose a new method for computing route skylines which\nsignificantly decreases processing time and memory consumption. Furthermore,\nour method does not rely on any precomputation or indexing method and thus, it\nis suitable for dynamically changing edge costs. Our experiments demonstrate\nthat our method outperforms state of the art approaches and allows highly\nefficient path skyline computation without any preprocessing.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 12:45:16 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Shekelyan", "Michael", ""], ["Joss\u00e9", "Gregor", ""], ["Schubert", "Matthias", ""]]}, {"id": "1410.0265", "submitter": "Chao Li", "authors": "Chao Li, Michael Hay, Gerome Miklau, Yue Wang", "title": "A Data- and Workload-Aware Algorithm for Range Queries Under\n  Differential Privacy", "comments": "VLDB 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new algorithm for answering a given set of range queries under\n$\\epsilon$-differential privacy which often achieves substantially lower error\nthan competing methods. Our algorithm satisfies differential privacy by adding\nnoise that is adapted to the input data and to the given query set. We first\nprivately learn a partitioning of the domain into buckets that suit the input\ndata well. Then we privately estimate counts for each bucket, doing so in a\nmanner well-suited for the given query set. Since the performance of the\nalgorithm depends on the input database, we evaluate it on a wide range of real\ndatasets, showing that we can achieve the benefits of data-dependence on both\n\"easy\" and \"hard\" databases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 15:56:42 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Li", "Chao", ""], ["Hay", "Michael", ""], ["Miklau", "Gerome", ""], ["Wang", "Yue", ""]]}, {"id": "1410.0600", "submitter": "Ghislain Fourny", "authors": "Ghislain Fourny", "title": "Cell Stores", "comments": "Technical report - 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell stores provide a relational-like, tabular level of abstraction to\nbusiness users while leveraging recent database technologies, such as key-value\nstores and document stores. This allows to scale up and out the efficient\nstorage and retrieval of highly dimensional data. Cells are the primary\ncitizens and exist in different forms, which can be explained with an analogy\nto the state of matter: as a gas for efficient storage, as a solid for\nefficient retrieval, and as a liquid for efficient interaction with the\nbusiness users. Cell stores were abstracted from, and are compatible with the\nXBRL standard for importing and exporting data. The first cell store repository\ncontains roughly 200GB of SEC filings data, and proves that retrieving data\ncubes can be performed in real time (the threshold acceptable by a human user\nbeing at most a few seconds).\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 16:14:22 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 15:51:58 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Fourny", "Ghislain", ""]]}, {"id": "1410.0709", "submitter": "L\\'aszl\\'o Dobos", "authors": "D\\'aniel Kondor, L\\'aszl\\'o Dobos, Istv\\'an Csabai, Andr\\'as Bodor,\n  G\\'abor Vattay, Tam\\'as Budav\\'ari and Alexander S. Szalay", "title": "Efficient classification of billions of points into complex geographic\n  regions using hierarchical triangular mesh", "comments": "appears in Proceedings of the 26th International Conference on\n  Scientific and Statistical Database Management (2014)", "journal-ref": null, "doi": "10.1145/2618243.2618245", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case study about the spatial indexing and regional\nclassification of billions of geographic coordinates from geo-tagged social\nnetwork data using Hierarchical Triangular Mesh (HTM) implemented for Microsoft\nSQL Server. Due to the lack of certain features of the HTM library, we use it\nin conjunction with the GIS functions of SQL Server to significantly increase\nthe efficiency of pre-filtering of spatial filter and join queries. For\nexample, we implemented a new algorithm to compute the HTM tessellation of\ncomplex geographic regions and precomputed the intersections of HTM triangles\nand geographic regions for faster false-positive filtering. With full control\nover the index structure, HTM-based pre-filtering of simple containment\nsearches outperforms SQL Server spatial indices by a factor of ten and\nHTM-based spatial joins run about a hundred times faster.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 20:50:14 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Kondor", "D\u00e1niel", ""], ["Dobos", "L\u00e1szl\u00f3", ""], ["Csabai", "Istv\u00e1n", ""], ["Bodor", "Andr\u00e1s", ""], ["Vattay", "G\u00e1bor", ""], ["Budav\u00e1ri", "Tam\u00e1s", ""], ["Szalay", "Alexander S.", ""]]}, {"id": "1410.0717", "submitter": "George Ovchinnikov", "authors": "I.V. Oseledets and G.V. Ovchinnikov", "title": "Fast, memory efficient low-rank approximation of SimRank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SimRank is a well-known similarity measure between graph vertices.\n  In this paper novel low-rank approximation of SimRank is proposed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 21:31:12 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Oseledets", "I. V.", ""], ["Ovchinnikov", "G. V.", ""]]}, {"id": "1410.1343", "submitter": "Ahmed Yousef Y", "authors": "Walaa Medhat, Ahmed Hassan Yousef, Hoda Korashy Mohamed", "title": "Combined Algorithm for Data Mining using Association rules", "comments": "Ain Shams Journal of Electrical Engineering, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association Rule mining is one of the most important fields in data mining\nand knowledge discovery. This paper proposes an algorithm that combines the\nsimple association rules derived from basic Apriori Algorithm with the multiple\nminimum support using maximum constraints. The algorithm is implemented, and is\ncompared to its predecessor algorithms using a novel proposed comparison\nalgorithm. Results of applying the proposed algorithm show faster performance\nthan other algorithms without scarifying the accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 12:28:22 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Medhat", "Walaa", ""], ["Yousef", "Ahmed Hassan", ""], ["Mohamed", "Hoda Korashy", ""]]}, {"id": "1410.2698", "submitter": "Michael Gowanlock", "authors": "Michael Gowanlock and Henri Casanova", "title": "Technical Report: Towards Efficient Indexing of Spatiotemporal\n  Trajectories on the GPU for Distance Threshold Similarity Searches", "comments": "30 pages, 18 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in many domains require processing moving object trajectories.\nIn this work, we focus on a trajectory similarity search that finds all\ntrajectories within a given distance of a query trajectory over a time\ninterval, which we call the distance threshold similarity search. We develop\nthree indexing strategies with spatial, temporal and spatiotemporal selectivity\nfor the GPU that differ significantly from indexes suitable for the CPU, and\nshow the conditions under which each index achieves good performance.\nFurthermore, we show that the GPU implementations outperform multithreaded CPU\nimplementations in a range of experimental scenarios, making the GPU an\nattractive technology for processing moving object trajectories. We test our\nimplementations on two synthetic and one real-world dataset of a galaxy merger.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 07:44:05 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Gowanlock", "Michael", ""], ["Casanova", "Henri", ""]]}, {"id": "1410.2803", "submitter": "Ali Shoker", "authors": "Paulo S\\'ergio Almeida, Ali Shoker, and Carlos Baquero", "title": "Efficient State-based CRDTs by Delta-Mutation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CRDTs are distributed data types that make eventual consistency of a\ndistributed object possible and non ad-hoc. Specifically, state-based CRDTs\nensure convergence through disseminating the en- tire state, that may be large,\nand merging it to other replicas; whereas operation-based CRDTs disseminate\noperations (i.e., small states) assuming an exactly-once reliable dissemination\nlayer. We introduce Delta State Conflict-Free Replicated Datatypes\n({\\delta}-CRDT) that can achieve the best of both worlds: small messages with\nan incremental nature, as in operation-based CRDTs, disseminated over\nunreliable communication channels, as in traditional state-based CRDTs. This is\nachieved by defining {\\delta}-mutators to return a delta-state, typically with\na much smaller size than the full state, that is joined to both: local and\nremote states. We introduce the {\\delta}-CRDT framework, and we explain it\nthrough establishing a correspondence to current state-based CRDTs. In\naddition, we present an anti-entropy algorithm that ensures causal consistency,\nand we introduce two {\\delta}-CRDT specifications of well-known replicated\ndatatypes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 15:16:23 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 12:32:20 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Almeida", "Paulo S\u00e9rgio", ""], ["Shoker", "Ali", ""], ["Baquero", "Carlos", ""]]}, {"id": "1410.2988", "submitter": "Jayakrushna Sahoo", "authors": "Jayakrushna Sahoo, Ashok Kumar Das, A. Goswami", "title": "An Algorithm for Mining High Utility Closed Itemsets and Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Traditional association rule mining based on the support-confidence framework\nprovides the objective measure of the rules that are of interest to users.\nHowever, it does not reflect the utility of the rules. To extract non-redundant\nassociation rules in support-confidence framework frequent closed itemsets and\ntheir generators play an important role. To extract non-redundant association\nrules among high utility itemsets, high utility closed itemsets (HUCI) and\ntheir generators should be extracted in order to apply traditional\nsupport-confidence framework. However, no efficient method exists at present\nfor mining HUCIs with their generators. This paper addresses this issue. A\npost-processing algorithm, called the HUCI-Miner, is proposed to mine HUCIs\nwith their generators. The proposed algorithm is implemented using both\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Oct 2014 11:30:14 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Sahoo", "Jayakrushna", ""], ["Das", "Ashok Kumar", ""], ["Goswami", "A.", ""]]}, {"id": "1410.4156", "submitter": "Manas Joglekar", "authors": "Foto Afrati, Manas Joglekar, Christopher R\\'e, Semih Salihoglu,\n  Jeffrey D. Ullman", "title": "GYM: A Multiround Join Algorithm In MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiround algorithms are now commonly used in distributed data processing\nsystems, yet the extent to which algorithms can benefit from running more\nrounds is not well understood. This paper answers this question for a spectrum\nof rounds for the problem of computing the equijoin of $n$ relations.\nSpecifically, given any query $Q$ with width $\\w$, {\\em intersection width}\n$\\iw$, input size $\\mathrm{IN}$, output size $\\mathrm{OUT}$, and a cluster of\nmachines with $M$ memory available per machine, we show that:\n  (1) $Q$ can be computed in $O(n)$ rounds with $O(n\\frac{(\\mathrm{IN}^{\\w} +\n\\mathrm{OUT})^2}{M})$ communication cost.\n  (2) $Q$ can be computed in $O(\\log(n))$ rounds with\n$O(n\\frac{(\\mathrm{IN}^{\\max(\\w, 3\\iw)} + \\mathrm{OUT})^2}{M})$ communication\ncost. \\end{itemize} Intersection width is a new notion of queries and\ngeneralized hypertree decompositions (GHDs) of queries we introduce to capture\nhow connected the adjacent cyclic components of the GHDs are.\n  We achieve our first result by introducing a distributed and generalized\nversion of Yannakakis's algorithm, called GYM. GYM takes as input any GHD of\n$Q$ with width $\\w$ and depth $d$, and computes $Q$ in $O(d + \\log(n))$ rounds\nand $O(n\\frac{(\\mathrm{IN}^{\\w} + \\mathrm{OUT})^2}{M})$ communication cost. We\nachieve our second result by showing how to construct GHDs of $Q$ with width\n$\\max(\\w, 3\\iw)$ and depth $O(\\log(n))$. We describe another technique to\nconstruct GHDs with longer widths and shorter depths, demonstrating a spectrum\nof tradeoffs one can make between communication and the number of rounds.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 18:25:22 GMT"}, {"version": "v2", "created": "Sun, 1 Feb 2015 07:37:45 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2015 21:02:33 GMT"}, {"version": "v4", "created": "Sun, 6 Dec 2015 22:46:12 GMT"}, {"version": "v5", "created": "Sat, 30 Jul 2016 04:48:02 GMT"}, {"version": "v6", "created": "Wed, 3 Aug 2016 03:35:45 GMT"}, {"version": "v7", "created": "Sat, 21 Jan 2017 04:39:07 GMT"}, {"version": "v8", "created": "Thu, 26 Jan 2017 04:51:19 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Afrati", "Foto", ""], ["Joglekar", "Manas", ""], ["R\u00e9", "Christopher", ""], ["Salihoglu", "Semih", ""], ["Ullman", "Jeffrey D.", ""]]}, {"id": "1410.5077", "submitter": "William Waites", "authors": "William Waites", "title": "On the Provenance of Linked Data Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  As the amount of linked data published on the web grows, attempts are being\nmade to describe and measure it. However even basic statistics about a graph,\nsuch as its size, are difficult to express in a uniform and predictable way. In\norder to be able to sensibly interpret a statistic it is necessary to know how\nit was calculate. In this paper we survey the nature of the problem and outline\na strategy for addressing it.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 14:10:50 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Waites", "William", ""]]}, {"id": "1410.5209", "submitter": "Kijung Shin", "authors": "Kijung Shin, U. Kang", "title": "Distributed Methods for High-dimensional and Large-scale Tensor\n  Factorization", "comments": null, "journal-ref": "Data Mining (ICDM), 2014 IEEE International Conference on, pp.\n  989-994. IEEE, 2014", "doi": "10.1109/ICDM.2014.78", "report-no": null, "categories": "cs.NA cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a high-dimensional large-scale tensor, how can we decompose it into\nlatent factors? Can we process it on commodity computers with limited memory?\nThese questions are closely related to recommender systems, which have modeled\nrating data not as a matrix but as a tensor to utilize contextual information\nsuch as time and location. This increase in the dimension requires tensor\nfactorization methods scalable with both the dimension and size of a tensor. In\nthis paper, we propose two distributed tensor factorization methods, SALS and\nCDTF. Both methods are scalable with all aspects of data, and they show an\ninteresting trade-off between convergence speed and memory requirements. SALS\nupdates a subset of the columns of a factor matrix at a time, and CDTF, a\nspecial case of SALS, updates one column at a time. In our experiments, only\nour methods factorize a 5-dimensional tensor with 1 billion observable entries,\n10M mode length, and 1K rank, while all other state-of-the-art methods fail.\nMoreover, our methods require several orders of magnitude less memory than our\ncompetitors. We implement our methods on MapReduce with two widely-applicable\noptimization techniques: local disk caching and greedy row assignment. They\nspeed up our methods up to 98.2X and also the competitors up to 5.9X.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 09:49:46 GMT"}, {"version": "v2", "created": "Thu, 19 Feb 2015 09:52:23 GMT"}, {"version": "v3", "created": "Sat, 11 Jul 2015 07:28:07 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Shin", "Kijung", ""], ["Kang", "U.", ""]]}, {"id": "1410.5290", "submitter": "Robert Soul\\'e", "authors": "Robert Soul\\'e and B\\\"ugra Gedik", "title": "Optimized Disk Layouts for Adaptive Storage of Interaction Graphs", "comments": "Universit\\`a della Svizzera italiana Technical Report", "journal-ref": null, "doi": null, "report-no": "USI-INF-TR-2014-04", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are living in an ever more connected world, where data recording the\ninteractions between people, software systems, and the physical world is\nbecoming increasingly prevalent. This data often takes the form of a temporally\nevolving graph, where entities are the vertices and the interactions between\nthem are the edges. We call such graphs interaction graphs. Various application\ndomains, including telecommunications, transportation, and social media, depend\non analytics performed on interaction graphs. The ability to efficiently\nsupport historical analysis over interaction graphs require effective solutions\nfor the problem of data layout on disk. This paper presents an adaptive disk\nlayout called the railway layout for optimizing disk block storage for\ninteraction graphs. The key idea is to divide blocks into one or more\nsub-blocks, where each sub-block contains a subset of the attributes, but the\nentire graph structure is replicated within each sub-block. This improves query\nI/O, at the cost of increased storage overhead. We introduce optimal ILP\nformulations for partitioning disk blocks into sub-blocks with overlapping and\nnon-overlapping attributes. Additionally, we present greedy heuristic\napproaches that can scale better compared to the ILP alternatives, yet achieve\nclose to optimal query I/O. To demonstrate the benefits of the railway layout,\nwe provide an extensive experimental study comparing our approach to a few\nbaseline alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 14:24:03 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Soul\u00e9", "Robert", ""], ["Gedik", "B\u00fcgra", ""]]}, {"id": "1410.5919", "submitter": "Yonghui Xiao", "authors": "Yonghui Xiao and Li Xiong", "title": "Protecting Locations with Differential Privacy under Temporal\n  Correlations", "comments": "Final version Nov-04-2015", "journal-ref": null, "doi": "10.1145/2810103.2813640", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concerns on location privacy frequently arise with the rapid development of\nGPS enabled devices and location-based applications. While spatial\ntransformation techniques such as location perturbation or generalization have\nbeen studied extensively, most techniques rely on syntactic privacy models\nwithout rigorous privacy guarantee. Many of them only consider static scenarios\nor perturb the location at single timestamps without considering temporal\ncorrelations of a moving user's locations, and hence are vulnerable to various\ninference attacks. While differential privacy has been accepted as a standard\nfor privacy protection, applying differential privacy in location based\napplications presents new challenges, as the protection needs to be enforced on\nthe fly for a single user and needs to incorporate temporal correlations\nbetween a user's locations.\n  In this paper, we propose a systematic solution to preserve location privacy\nwith rigorous privacy guarantee. First, we propose a new definition,\n\"$\\delta$-location set\" based differential privacy, to account for the temporal\ncorrelations in location data. Second, we show that the well known\n$\\ell_1$-norm sensitivity fails to capture the geometric sensitivity in\nmultidimensional space and propose a new notion, sensitivity hull, based on\nwhich the error of differential privacy is bounded. Third, to obtain the\noptimal utility we present a planar isotropic mechanism (PIM) for location\nperturbation, which is the first mechanism achieving the lower bound of\ndifferential privacy. Experiments on real-world datasets also demonstrate that\nPIM significantly outperforms baseline approaches in data utility.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 05:23:04 GMT"}, {"version": "v2", "created": "Sun, 1 Feb 2015 17:24:37 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2015 20:11:43 GMT"}, {"version": "v4", "created": "Wed, 21 Oct 2015 18:57:05 GMT"}, {"version": "v5", "created": "Wed, 4 Nov 2015 16:36:51 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Xiao", "Yonghui", ""], ["Xiong", "Li", ""]]}, {"id": "1410.7990", "submitter": "Jan Michelfeit", "authors": "Jan Michelfeit, Tom\\'a\\v{s} Knap, Martin Ne\\v{c}ask\\'y", "title": "Linked Data Integration with Conflicts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linked Data have emerged as a successful publication format and one of its\nmain strengths is its fitness for integration of data from multiple sources.\nThis gives them a great potential both for semantic applications and the\nenterprise environment where data integration is crucial. Linked Data\nintegration poses new challenges, however, and new algorithms and tools\ncovering all steps of the integration process need to be developed. This paper\nexplores Linked Data integration and its specifics. We focus on data fusion and\nconflict resolution: two novel algorithms for Linked Data fusion with\nprovenance tracking and quality assessment of fused data are proposed. The\nalgorithms are implemented as part of the ODCleanStore framework and evaluated\non real Linked Open Data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 22:17:19 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Michelfeit", "Jan", ""], ["Knap", "Tom\u00e1\u0161", ""], ["Ne\u010dask\u00fd", "Martin", ""]]}, {"id": "1410.8536", "submitter": "Yannis Tzitzikas", "authors": "Christina Lantzaki and Yannis Tzitzikas", "title": "Tasks that Require, or can Benefit from, Matching Blank Nodes", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various domains and cases, we observe the creation and usage of\ninformation elements which are unnamed. Such elements do not have a name, or\nmay have a name that is not externally referable (usually meaningless and not\npersistent over time). This paper discusses why we will never `escape' from the\nproblem of having to construct mappings between such unnamed elements in\ninformation systems. Since unnamed elements nowadays occur very often in the\nframework of the Semantic Web and Linked Data as blank nodes, the paper\ndescribes scenarios that can benefit from methods that compute mappings between\nthe unnamed elements. For each scenario, the corresponding bnode matching\nproblem is formally defined. Based on this analysis, we try to reach to more a\ngeneral formulation of the problem, which can be useful for guiding the\nrequired technological advances. To this end, the paper finally discusses\nmethods to realize blank node matching, the implementations that exist, and\nidentifies open issues and challenges.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 20:04:07 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Lantzaki", "Christina", ""], ["Tzitzikas", "Yannis", ""]]}]