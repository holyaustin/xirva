[{"id": "1708.00183", "submitter": "Sahaana Suri", "authors": "Sahaana Suri and Peter Bailis", "title": "DROP: Dimensionality Reduction Optimization for Time Series", "comments": null, "journal-ref": "DEEM'19: Proceedings of the 3rd International Workshop on Data\n  Management for End-to-End Machine Learning (2019)", "doi": "10.1145/3329486.3329490", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is a critical step in scaling machine learning\npipelines. Principal component analysis (PCA) is a standard tool for\ndimensionality reduction, but performing PCA over a full dataset can be\nprohibitively expensive. As a result, theoretical work has studied the\neffectiveness of iterative, stochastic PCA methods that operate over data\nsamples. However, termination conditions for stochastic PCA either execute for\na predetermined number of iterations, or until convergence of the solution,\nfrequently sampling too many or too few datapoints for end-to-end runtime\nimprovements. We show how accounting for downstream analytics operations during\nDR via PCA allows stochastic methods to efficiently terminate after operating\nover small (e.g., 1%) subsamples of input data, reducing whole workload\nruntime. Leveraging this, we propose DROP, a DR optimizer that enables speedups\nof up to 5x over Singular-Value-Decomposition-based PCA techniques, and exceeds\nconventional approaches like FFT and PAA by up to 16x in end-to-end workloads.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 06:58:15 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 20:08:41 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 22:46:55 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 07:28:23 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Suri", "Sahaana", ""], ["Bailis", "Peter", ""]]}, {"id": "1708.00192", "submitter": "Jorge Lloret-Gazo", "authors": "Jorge Lloret-Gazo", "title": "A Survey on Visual Query Systems in the Web Era (extended version)", "comments": "34 pages. 32 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more and more collections of data are becoming available on the web to\neveryone, non expert users demand easy ways to retrieve data from these\ncollections. One solution is the so called Visual Query Systems (VQS) where\nqueries are represented visually and users do not have to understand query\nlanguages such as SQL or XQuery. In 1996, a paper by Catarci reviewed the\nVisual Query Systems available until that year. In this paper, we review VQSs\nfrom 1997 until now and try to determine whether they have been the solution\nfor non expert users. The short answer is no because very few systems have in\nfact been used in real environments or as commercial tools. We have also\ngathered basic features of VQSs such as the visual representation adopted to\npresent the reality of interest or the visual representation adopted to express\nqueries.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 07:37:16 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Lloret-Gazo", "Jorge", ""]]}, {"id": "1708.00335", "submitter": "Gangli Liu", "authors": "Gangli Liu", "title": "Understanding tree: a tool for estimating an individual's understanding\n  of conceptual knowledge", "comments": "arXiv admin note: substantial text overlap with arXiv:1612.07714", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People learn whenever and wherever possible, and whatever they like or\nencounter--Mathematics, Drama, Art, Languages, Physics, Philosophy, and so on.\nWith the bursting of knowledge, evaluation of one's understanding of conceptual\nknowledge becomes increasingly difficult. There are a lot of demands for\nevaluating one's understanding of a piece of knowledge, e.g., facilitating\npersonalized recommendations; discovering one's expertises and deficiencies in\na field; recommending topics for a conversation between people with different\neducational or cultural backgrounds in their first encounter; recommending a\nlearning material to practice a meaningful learning etc. Assessment of\nunderstanding of knowledge is conventionally practiced through tests or\ninterviews, but they have some limitations such as low-efficiency and\nnot-comprehensive. We propose a method to estimate one's understanding of\nconceptual knowledge, by keeping track of his/her learning activities. It\novercomes some limitations of traditional methods, hence complements\ntraditional methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 10:30:37 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 17:30:08 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Liu", "Gangli", ""]]}, {"id": "1708.00363", "submitter": "Thomas Timm", "authors": "Angela Bonifati, Wim Martens, Thomas Timm", "title": "An Analytical Study of Large SPARQL Query Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the adoption of RDF as the data model for Linked Data and the Semantic\nWeb, query specification from end- users has become more and more common in\nSPARQL end- points. In this paper, we conduct an in-depth analytical study of\nthe queries formulated by end-users and harvested from large and up-to-date\nquery logs from a wide variety of RDF data sources. As opposed to previous\nstudies, ours is the first assessment on a voluminous query corpus, span- ning\nover several years and covering many representative SPARQL endpoints. Apart\nfrom the syntactical structure of the queries, that exhibits already\ninteresting results on this generalized corpus, we drill deeper in the\nstructural char- acteristics related to the graph- and hypergraph represen-\ntation of queries. We outline the most common shapes of queries when visually\ndisplayed as pseudographs, and char- acterize their (hyper-)tree width.\nMoreover, we analyze the evolution of queries over time, by introducing the\nnovel con- cept of a streak, i.e., a sequence of queries that appear as\nsubsequent modifications of a seed query. Our study offers several fresh\ninsights on the already rich query features of real SPARQL queries formulated\nby real users, and brings us to draw a number of conclusions and pinpoint\nfuture di- rections for SPARQL query evaluation, query optimization, tuning,\nand benchmarking.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 14:36:55 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Bonifati", "Angela", ""], ["Martens", "Wim", ""], ["Timm", "Thomas", ""]]}, {"id": "1708.00497", "submitter": "Chiara Boldrini", "authors": "Chiara Boldrini, Raffaele Bruno and Haitam Laarabi", "title": "Car sharing through the data analysis lens", "comments": "Accepted for KNOWMe: 1st International Workshop on Knowledge\n  Discovery from Mobility and Transportation Systems (colocated with PKDD 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Car sharing is one the pillars of a smart transportation infrastructure, as\nit is expected to reduce traffic congestion, parking demands and pollution in\nour cities. From the point of view of demand modelling, car sharing is a weak\nsignal in the city landscape: only a small percentage of the population uses\nit, and thus it is difficult to study reliably with traditional techniques such\nas households travel diaries. In this work, we depart from these traditional\napproaches and we rely on web-based, digital records about vehicle availability\nin 10 European cities for one of the major active car sharing operators. We\ndiscuss how vehicles are used, what are the main characteristics of car sharing\ntrips, whether events happening in certain areas are predictable or not, and\nhow the spatio-temporal information about vehicle availability can be used to\ninfer how different zones in a city are used by customers. We conclude the\npaper by presenting a direct application of the analysis of the dataset, aimed\nat identifying where to locate maintenance facilities within the car sharing\noperational area.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 13:07:47 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Boldrini", "Chiara", ""], ["Bruno", "Raffaele", ""], ["Laarabi", "Haitam", ""]]}, {"id": "1708.01286", "submitter": "Rafael S. Gon\\c{c}alves", "authors": "Rafael S. Gon\\c{c}alves, Martin J. O'Connor, Marcos Mart\\'inez-Romero,\n  John Graybeal and Mark A. Musen", "title": "Metadata in the BioSample Online Repository are Impaired by Numerous\n  Anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The metadata about scientific experiments are crucial for finding,\nreproducing, and reusing the data that the metadata describe. We present a\nstudy of the quality of the metadata stored in BioSample--a repository of\nmetadata about samples used in biomedical experiments managed by the U.S.\nNational Center for Biomedical Technology Information (NCBI). We tested whether\n6.6 million BioSample metadata records are populated with values that fulfill\nthe stated requirements for such values. Our study revealed multiple anomalies\nin the analyzed metadata. The BioSample metadata field names and their values\nare not standardized or controlled--15% of the metadata fields use field names\nnot specified in the BioSample data dictionary. Only 9 out of 452\nBioSample-specified fields ordinarily require ontology terms as values, and the\nquality of these controlled fields is better than that of uncontrolled ones, as\neven simple binary or numeric fields are often populated with inadequate values\nof different data types (e.g., only 27% of Boolean values are valid). Overall,\nthe metadata in BioSample reveal that there is a lack of principled mechanisms\nto enforce and validate metadata requirements. The aberrancies in the metadata\nare likely to impede search and secondary use of the associated datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 19:27:06 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Gon\u00e7alves", "Rafael S.", ""], ["O'Connor", "Martin J.", ""], ["Mart\u00ednez-Romero", "Marcos", ""], ["Graybeal", "John", ""], ["Musen", "Mark A.", ""]]}, {"id": "1708.01402", "submitter": "Yuhang Zhang", "authors": "Yuhang Zhang, Tania Churchill and Kee Siong Ng", "title": "Exploiting Redundancy, Recurrence and Parallelism: How to Link Millions\n  of Addresses with Ten Lines of Code in Ten Minutes", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient record linkage is an open challenge of particular\nrelevance to Australian Government Agencies, who recognise that so-called\nwicked social problems are best tackled by forming partnerships founded on\nlarge-scale data fusion. Names and addresses are the most common attributes on\nwhich data from different government agencies can be linked. In this paper, we\nfocus on the problem of address linking. Linkage is particularly problematic\nwhen the data has significant quality issues. The most common approach for\ndealing with quality issues is to standardise raw data prior to linking. If a\nmistake is made in standardisation, however, it is usually impossible to\nrecover from it to perform linkage correctly. This paper proposes a novel\nalgorithm for address linking that is particularly practical for linking large\ndisparate sets of addresses, being highly scalable, robust to data quality\nissues and simple to implement. It obviates the need for labour intensive and\nproblematic address standardisation. We demonstrate the efficacy of the\nalgorithm by matching two large address datasets from two government agencies\nwith good accuracy and computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 07:26:20 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 02:59:29 GMT"}, {"version": "v3", "created": "Tue, 19 Dec 2017 00:07:44 GMT"}, {"version": "v4", "created": "Wed, 31 Jan 2018 05:57:54 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Zhang", "Yuhang", ""], ["Churchill", "Tania", ""], ["Ng", "Kee Siong", ""]]}, {"id": "1708.02018", "submitter": "Xiu Fang", "authors": "Xiu Susie Fang and Quan Z. Sheng and Xianzhi Wang and Anne H.H. Ngu", "title": "SmartMTD: A Graph-Based Approach for Effective Multi-Truth Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Big Data era features a huge amount of data that are contributed by\nnumerous sources and used by many critical data-driven applications. Due to the\nvarying reliability of sources, it is common to see conflicts among the\nmulti-source data, making it difficult to determine which data sources to\ntrust. Recently, truth discovery has emerged as a means of addressing this\nchallenging issue by determining data veracity jointly with estimating the\nreliability of data sources. A fundamental issue with current truth discovery\nmethods is that they generally assume only one true value for each object,\nwhile in reality, objects may have multiple true values. In this paper, we\npropose a graph-based approach, called SmartMTD, to unravel the truth discovery\nproblem beyond the single-truth assumption, or the multi-truth discovery\nproblem. SmartMTD models and quantifies two types of source relations to\nestimate source reliability precisely and to detect malicious agreement among\nsources for effective multi-truth discovery. In particular, two graphs are\nconstructed based on the modeled source relations. They are further used to\nderive the two aspects of source reliability (i.e., positive precision and\nnegative precision) via random walk computation. Empirical studies on two large\nreal-world datasets demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 07:36:33 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Fang", "Xiu Susie", ""], ["Sheng", "Quan Z.", ""], ["Wang", "Xianzhi", ""], ["Ngu", "Anne H. H.", ""]]}, {"id": "1708.02029", "submitter": "Xiu Fang", "authors": "Xiu Susie Fang and Quan Z. Sheng and Xianzhi Wang and Wei Emma Zhang\n  and Anne H.H. Ngu", "title": "From Appearance to Essence: Comparing Truth Discovery Methods without\n  Using Ground Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truth discovery has been widely studied in recent years as a fundamental\nmeans for resolving the conflicts in multi-source data. Although many truth\ndiscovery methods have been proposed based on different considerations and\nintuitions, investigations show that no single method consistently outperforms\nthe others. To select the right truth discovery method for a specific\napplication scenario, it becomes essential to evaluate and compare the\nperformance of different methods. A drawback of current research efforts is\nthat they commonly assume the availability of certain ground truth for the\nevaluation of methods. However, the ground truth may be very limited or even\nout-of-reach in practice, rendering the evaluation biased by the small ground\ntruth or even unfeasible. In this paper, we present CompTruthHyp, a general\napproach for comparing the performance of truth discovery methods without using\nground truth. In particular, our approach calculates the probability of\nobservations in a dataset based on the output of different methods. The\nprobability is then ranked to reflect the performance of these methods. We\nreview and compare twelve existing truth discovery methods and consider both\nsingle-valued and multi-valued objects. Empirical studies on both real-world\nand synthetic datasets demonstrate the effectiveness of our approach for\ncomparing truth discovery methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 08:12:22 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Fang", "Xiu Susie", ""], ["Sheng", "Quan Z.", ""], ["Wang", "Xianzhi", ""], ["Zhang", "Wei Emma", ""], ["Ngu", "Anne H. H.", ""]]}, {"id": "1708.02125", "submitter": "Caihua Shan", "authors": "Caihua Shan, Nikos Mamoulis, Guoliang Li, Reynold Cheng, Zhipeng\n  Huang, Yudian Zheng", "title": "T-Crowd: Effective Crowdsourcing for Tabular Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing employs human workers to solve computer-hard problems, such as\ndata cleaning, entity resolution, and sentiment analysis. When crowdsourcing\ntabular data, e.g., the attribute values of an entity set, a worker's answers\non the different attributes (e.g., the nationality and age of a celebrity star)\nare often treated independently. This assumption is not always true and can\nlead to suboptimal crowdsourcing performance. In this paper, we present the\nT-Crowd system, which takes into consideration the intricate relationships\namong tasks, in order to converge faster to their true values. Particularly,\nT-Crowd integrates each worker's answers on different attributes to effectively\nlearn his/her trustworthiness and the true data values. The attribute\nrelationship information is also used to guide task allocation to workers.\nFinally, T-Crowd seamlessly supports categorical and continuous attributes,\nwhich are the two main datatypes found in typical databases. Our extensive\nexperiments on real and synthetic datasets show that T-Crowd outperforms\nstate-of-the-art methods in terms of truth inference and reducing the cost of\ncrowdsourcing.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 14:03:17 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Shan", "Caihua", ""], ["Mamoulis", "Nikos", ""], ["Li", "Guoliang", ""], ["Cheng", "Reynold", ""], ["Huang", "Zhipeng", ""], ["Zheng", "Yudian", ""]]}, {"id": "1708.02536", "submitter": "Sudeepa Roy", "authors": "Sudeepa Roy and Babak Salimi", "title": "A Framework for Inferring Causality from Multi-Relational Observational\n  Data using Conditional Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of causality or causal inference - how much a given treatment\ncausally affects a given outcome in a population - goes way beyond correlation\nor association analysis of variables, and is critical in making sound data\ndriven decisions and policies in a multitude of applications. The gold standard\nin causal inference is performing \"controlled experiments\", which often is not\npossible due to logistical or ethical reasons. As an alternative, inferring\ncausality on \"observational data\" based on the \"Neyman-Rubin potential outcome\nmodel\" has been extensively used in statistics, economics, and social sciences\nover several decades. In this paper, we present a formal framework for sound\ncausal analysis on observational datasets that are given as multiple relations\nand where the population under study is obtained by joining these base\nrelations. We study a crucial condition for inferring causality from\nobservational data, called the \"strong ignorability assumption\" (the treatment\nand outcome variables should be independent in the joined relation given the\nobserved covariates), using known conditional independences that hold in the\nbase relations. We also discuss how the structure of the conditional\nindependences in base relations given as graphical models help infer new\nconditional independences in the joined relation. The proposed framework\ncombines concepts from databases, statistics, and graphical models, and aims to\ninitiate new research directions spanning these fields to facilitate powerful\ndata-driven decisions in today's big data world.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 15:56:18 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Roy", "Sudeepa", ""], ["Salimi", "Babak", ""]]}, {"id": "1708.02621", "submitter": "Arun  Kejariwal", "authors": "Arun Kejariwal, Sanjeev Kulkarni and Karthik Ramasamy", "title": "Real Time Analytics: Algorithms and Systems", "comments": "Extended version of VLDB'15 tutorial proposal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Velocity is one of the 4 Vs commonly used to characterize Big Data. In this\nregard, Forrester remarked the following in Q3 2014: \"The high velocity,\nwhite-water flow of data from innumerable real-time data sources such as market\ndata, Internet of Things, mobile, sensors, click-stream, and even transactions\nremain largely unnavigated by most firms. The opportunity to leverage streaming\nanalytics has never been greater.\" Example use cases of streaming analytics\ninclude, but not limited to: (a) visualization of business metrics in real-time\n(b) facilitating highly personalized experiences (c) expediting response during\nemergencies. Streaming analytics is extensively used in a wide variety of\ndomains such as healthcare, e-commerce, financial services, telecommunications,\nenergy and utilities, manufacturing, government and transportation.\n  In this tutorial, we shall present an in-depth overview of streaming\nanalytics - applications, algorithms and platforms - landscape. We shall walk\nthrough how the field has evolved over the last decade and then discuss the\ncurrent challenges - the impact of the other three Vs, viz., Volume, Variety\nand Veracity, on Big Data streaming analytics. The tutorial is intended for\nboth researchers and practitioners in the industry. We shall also present\nstate-of-the-affairs of streaming analytics at Twitter.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 15:59:34 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Kejariwal", "Arun", ""], ["Kulkarni", "Sanjeev", ""], ["Ramasamy", "Karthik", ""]]}, {"id": "1708.02934", "submitter": "Lauren Milechin", "authors": "Lauren Milechin, Vijay Gadepally, Siddharth Samsi, Jeremy Kepner,\n  Alexander Chen, Dylan Hutchison", "title": "D4M 3.0: Extended Database and Language Capabilities", "comments": "IEEE HPEC 2017", "journal-ref": null, "doi": "10.1109/HPEC.2017.8091083", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The D4M tool was developed to address many of today's data needs. This tool\nis used by hundreds of researchers to perform complex analytics on unstructured\ndata. Over the past few years, the D4M toolbox has evolved to support\nconnectivity with a variety of new database engines, including SciDB.\nD4M-Graphulo provides the ability to do graph analytics in the Apache Accumulo\ndatabase. Finally, an implementation using the Julia programming language is\nalso now available. In this article, we describe some of our latest additions\nto the D4M toolbox and our upcoming D4M 3.0 release. We show through\nbenchmarking and scaling results that we can achieve fast SciDB ingest using\nthe D4M-SciDB connector, that using Graphulo can enable graph algorithms on\nscales that can be memory limited, and that the Julia implementation of D4M\nachieves comparable performance or exceeds that of the existing MATLAB(R)\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 17:59:42 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Milechin", "Lauren", ""], ["Gadepally", "Vijay", ""], ["Samsi", "Siddharth", ""], ["Kepner", "Jeremy", ""], ["Chen", "Alexander", ""], ["Hutchison", "Dylan", ""]]}, {"id": "1708.03264", "submitter": "Jason Morton", "authors": "Jason Morton", "title": "Contextuality from missing and versioned data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally categorical data analysis (e.g. generalized linear models)\nworks with simple, flat datasets akin to a single table in a database with no\nnotion of missing data or conflicting versions. In contrast, modern data\nanalysis must deal with distributed databases with many partial local tables\nthat need not always agree. The computational agents tabulating these tables\nare spatially separated, with binding speed-of-light constraints and data\narriving too rapidly for these distributed views ever to be fully informed and\nglobally consistent. Contextuality is a mathematical property which describes a\nkind of inconsistency arising in quantum mechanics (e.g. in Bell's theorem). In\nthis paper we show how contextuality can arise in common data collection\nscenarios, including missing data and versioning (as in low-latency distributed\ndatabases employing snapshot isolation). In the companion paper, we develop\nstatistical models adapted to this regime.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:37:53 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Morton", "Jason", ""]]}, {"id": "1708.03462", "submitter": "Yanhong Wu", "authors": "Xun Zhao, Yanhong Wu, Weiwei Cui, Xinnan Du, Yuan Chen, Yong Wang, Dik\n  Lun Lee, Huamin Qu", "title": "SkyLens: Visual Analysis of Skyline on Multi-dimensional Data", "comments": "10 pages. Accepted for publication at IEEE VIS 2017 (in proceedings\n  of VAST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skyline queries have wide-ranging applications in fields that involve\nmulti-criteria decision making, including tourism, retail industry, and human\nresources. By automatically removing incompetent candidates, skyline queries\nallow users to focus on a subset of superior data items (i.e., the skyline),\nthus reducing the decision-making overhead. However, users are still required\nto interpret and compare these superior items manually before making a\nsuccessful choice. This task is challenging because of two issues. First,\npeople usually have fuzzy, unstable, and inconsistent preferences when\npresented with multiple candidates. Second, skyline queries do not reveal the\nreasons for the superiority of certain skyline points in a multi-dimensional\nspace. To address these issues, we propose SkyLens, a visual analytic system\naiming at revealing the superiority of skyline points from different\nperspectives and at different scales to aid users in their decision making. Two\nscenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of\nattributes. A qualitative study is also conducted to show that users can\nefficiently accomplish skyline understanding and comparison tasks with SkyLens.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 08:04:58 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 01:32:05 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhao", "Xun", ""], ["Wu", "Yanhong", ""], ["Cui", "Weiwei", ""], ["Du", "Xinnan", ""], ["Chen", "Yuan", ""], ["Wang", "Yong", ""], ["Lee", "Dik Lun", ""], ["Qu", "Huamin", ""]]}, {"id": "1708.03734", "submitter": "Pedro Almagro Blanco", "authors": "Pedro Almagro-Blanco, Fernando Sancho-Caparrini", "title": "Generalized Graph Pattern Matching", "comments": "Multi-lingual Paper Main language: English Additional Language:\n  Spanish 23 Figures Engish: 26 pages Spanish: 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the machine learning algorithms are limited to learn from flat data:\na recordset with prefixed structure. When learning from a record, these types\nof algorithms don't take into account other objects even though they are\ndirectly connected to it and can provide valuable information for the learning\ntask. In this paper we present the concept of Generalized Graph Query, a query\ntool over graphs or multi-relational data structures. They are built using the\nsame graph structure as generalized graphs and allow to express powerful\nrelational and non-relational restrictions on this type of data. Also, this\npaper shows mechanisms to build this kind of queries dynamically and how they\ncan be used to perform bottom-up discovery processes through machine laerning\ntechniques.\n  -----\n  La mayor\\'ia de los algoritmos que aprenden a partir de datos est\\'an\nlimitados ya que s\\'olo son capaces de aprender a partir de datos estructurados\nen forma de tabla en la que cada fila representa un registro y cada columna una\npropiedad asociada. Estos algoritmos, no tienen en cuenta los atributos de las\nestructuras con las que un registro dado puede estar relacionado, a pesar de\nque \\'estos pueden aportar informaci\\'on \\'util a la hora de llevar a cabo la\ntarea de aprendizaje. En este art\\'iculo presentamos el concepto de Generalized\nGraph Query, una herramienta de consulta de patrones en grafos generalizados.\nDicha herramienta ha sido construida utilizando la estructura de Grafo\nGeneralizado y permite expresar restricciones relacionales y no relacionales\nsobre este tipo de estructuras. Adem\\'as, en este art\\'iculo se presentan\nmecanismos para la construcci\\'on autom\\'atica de este tipo de consultas y se\nmuestra c\\'omo \\'estas pueden ser utilizadas en procesos de descubrimiento tipo\nbottom-up a trav\\'es de t\\'ecnicas relacionadas con el Aprendizaje\nAutom\\'atico.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 00:00:01 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Almagro-Blanco", "Pedro", ""], ["Sancho-Caparrini", "Fernando", ""]]}, {"id": "1708.03878", "submitter": "Cihan Kucukkececi", "authors": "Cihan K\\\"u\\c{c}\\\"ukke\\c{c}eci, Adnan Yaz{\\i}c{\\i}", "title": "Big Data Model Simulation on a Graph Database for Surveillance in\n  Wireless Multimedia Sensor Networks", "comments": null, "journal-ref": "https://doi.org/10.1016/j.bdr.2017.09.003", "doi": "10.1016/j.bdr.2017.09.003", "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensors are present in various forms all around the world such as mobile\nphones, surveillance cameras, smart televisions, intelligent refrigerators and\nblood pressure monitors. Usually, most of the sensors are a part of some other\nsystem with similar sensors that compose a network. One of such networks is\ncomposed of millions of sensors connect to the Internet which is called\nInternet of things (IoT). With the advances in wireless communication\ntechnologies, multimedia sensors and their networks are expected to be major\ncomponents in IoT. Many studies have already been done on wireless multimedia\nsensor networks in diverse domains like fire detection, city surveillance,\nearly warning systems, etc. All those applications position sensor nodes and\ncollect their data for a long time period with real-time data flow, which is\nconsidered as big data. Big data may be structured or unstructured and needs to\nbe stored for further processing and analyzing. Analyzing multimedia big data\nis a challenging task requiring a high-level modeling to efficiently extract\nvaluable information/knowledge from data. In this study, we propose a big\ndatabase model based on graph database model for handling data generated by\nwireless multimedia sensor networks. We introduce a simulator to generate\nsynthetic data and store and query big data using graph model as a big\ndatabase. For this purpose, we evaluate the well-known graph-based NoSQL\ndatabases, Neo4j and OrientDB, and a relational database, MySQL.We have run a\nnumber of query experiments on our implemented simulator to show that which\ndatabase system(s) for surveillance in wireless multimedia sensor networks is\nefficient and scalable.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 09:05:58 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["K\u00fc\u00e7\u00fckke\u00e7eci", "Cihan", ""], ["Yaz\u0131c\u0131", "Adnan", ""]]}, {"id": "1708.04517", "submitter": "Th\\^ong Nguy\\^en", "authors": "Th\\^ong T. Nguy\\^en and Siu Cheung Hui", "title": "Privacy-Preserving Mechanisms for Parametric Survival Analysis with\n  Weibull Distribution", "comments": "8 pages, Trustcom17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis studies the statistical properties of the time until an\nevent of interest occurs. It has been commonly used to study the effectiveness\nof medical treatments or the lifespan of a population. However, survival\nanalysis can potentially leak confidential information of individuals in the\ndataset. The state-of-the-art techniques apply ad-hoc privacy-preserving\nmechanisms on publishing results to protect the privacy. These techniques\nusually publish sanitized and randomized answers which promise to protect the\nprivacy of individuals in the dataset but without providing any formal\nmechanism on privacy protection. In this paper, we propose private mechanisms\nfor parametric survival analysis with Weibull distribution. We prove that our\nproposed mechanisms achieve differential privacy, a robust and rigorous\ndefinition of privacy-preservation. Our mechanisms exploit the property of\nlocal sensitivity to carefully design a utility function which enables us to\npublish parameters of Weibull distribution with high precision. Our\nexperimental studies show that our mechanisms can publish useful answers and\noutperform other differentially private techniques on real datasets.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 03:29:00 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 01:45:39 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Nguy\u00ean", "Th\u00f4ng T.", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1708.05045", "submitter": "Wei Hu", "authors": "Zequn Sun, Wei Hu, Chengkai Li", "title": "Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment is the task of finding entities in two knowledge bases (KBs)\nthat represent the same real-world object. When facing KBs in different natural\nlanguages, conventional cross-lingual entity alignment methods rely on machine\ntranslation to eliminate the language barriers. These approaches often suffer\nfrom the uneven quality of translations between languages. While recent\nembedding-based techniques encode entities and relationships in KBs and do not\nneed machine translation for cross-lingual entity alignment, a significant\nnumber of attributes remain largely unexplored. In this paper, we propose a\njoint attribute-preserving embedding model for cross-lingual entity alignment.\nIt jointly embeds the structures of two KBs into a unified vector space and\nfurther refines it by leveraging attribute correlations in the KBs. Our\nexperimental results on real-world datasets show that this approach\nsignificantly outperforms the state-of-the-art embedding approaches for\ncross-lingual entity alignment and could be complemented with methods based on\nmachine translation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 19:30:17 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 02:06:08 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Sun", "Zequn", ""], ["Hu", "Wei", ""], ["Li", "Chengkai", ""]]}, {"id": "1708.05072", "submitter": "Getachew Demisse Dr.", "authors": "Getachew B. Demisse, Tsegaye Tadesse, Yared Bayissa", "title": "Data Mining Attribute Selection Approach for Drought Modeling: A Case\n  Study for Greater Horn of Africa", "comments": null, "journal-ref": null, "doi": "10.5121/ijdkp.2017.7401", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The objectives of this paper were to 1) develop an empirical method for\nselecting relevant attributes for modelling drought, and 2) select the most\nrelevant attribute for drought modelling and predictions in the Greater Horn of\nAfrica (GHA). Twenty four attributes from different domain areas were used for\nthis experimental analysis. Two attribute selection algorithms were used for\nthe current study: Principal Component Analysis (PCA) and correlation-based\nattribute selection (CAS). Using the PCA and CAS algorithms, the 24 attributes\nwere ranked by their merit value. Accordingly, 15 attributes were selected for\nmodelling drought in GHA. The average merit values for the selected attributes\nranged from 0.5 to 0.9. Future research may evaluate the developed methodology\nusing relevant classification techniques and quantify the actual information\ngain from the developed approach.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 17:01:14 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Demisse", "Getachew B.", ""], ["Tadesse", "Tsegaye", ""], ["Bayissa", "Yared", ""]]}, {"id": "1708.05665", "submitter": "Anh Dinh", "authors": "Tien Tuan Anh Dinh, Rui Liu, Meihui Zhang, Gang Chen, Beng Chin Ooi,\n  Ji Wang", "title": "Untangling Blockchain: A Data Processing View of Blockchain Systems", "comments": "arXiv admin note: text overlap with arXiv:1703.04057", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technologies are gaining massive momentum in the last few years.\nBlockchains are distributed ledgers that enable parties who do not fully trust\neach other to maintain a set of global states. The parties agree on the\nexistence, values and histories of the states. As the technology landscape is\nexpanding rapidly, it is both important and challenging to have a firm grasp of\nwhat the core technologies have to offer, especially with respect to their data\nprocessing capabilities. In this paper, we first survey the state of the art,\nfocusing on private blockchains (in which parties are authenticated). We\nanalyze both in-production and research systems in four dimensions: distributed\nledger, cryptography, consensus protocol and smart contract. We then present\nBLOCKBENCH, a benchmarking framework for understanding performance of private\nblockchains against data processing workloads. We conduct a comprehensive\nevaluation of three major blockchain systems based on BLOCKBENCH, namely\nEthereum, Parity and Hyperledger Fabric. The results demonstrate several\ntrade-offs in the design space, as well as big performance gaps between\nblockchain and database systems. Drawing from design principles of database\nsystems, we discuss several research directions for bringing blockchain\nperformance closer to the realm of databases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 08:41:23 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Dinh", "Tien Tuan Anh", ""], ["Liu", "Rui", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""], ["Wang", "Ji", ""]]}, {"id": "1708.05918", "submitter": "Stephen Macke", "authors": "Stephen Macke, Yiming Zhang, Silu Huang, Aditya Parameswaran", "title": "Adaptive Sampling for Rapidly Matching Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exploratory data analysis, analysts often have a need to identify\nhistograms that possess a specific distribution, among a large class of\ncandidate histograms, e.g., find countries whose income distribution is most\nsimilar to that of Greece. This distribution could be a new one that the user\nis curious about, or a known distribution from an existing histogram\nvisualization. At present, this process of identification is brute-force,\nrequiring the manual generation and evaluation of a large number of histograms.\nWe present FastMatch: an end-to-end approach for interactively retrieving the\nhistogram visualizations most similar to a user-specified target, from a large\ncollection of histograms. The primary technical contribution underlying\nFastMatch is a probabilistic algorithm, HistSim, a theoretically sound\nsampling-based approach to identify the top-$k$ closest histograms under\n$\\ell_1$ distance. While HistSim can be used independently, within FastMatch we\ncouple HistSim with a novel system architecture that is aware of practical\nconsiderations, employing asynchronous block-based sampling policies, building\non lightweight sampling engines developed in recent work. FastMatch obtains\nnear-perfect accuracy with up to $35\\times$ speedup over approaches that do not\nuse sampling on several real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 01:08:27 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 05:52:12 GMT"}, {"version": "v3", "created": "Tue, 8 May 2018 06:01:53 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Macke", "Stephen", ""], ["Zhang", "Yiming", ""], ["Huang", "Silu", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1708.05926", "submitter": "Muaz Niazi", "authors": "Komal Batool, Muaz A. Niazi", "title": "Tamper-Evident Complex Genomic Networks", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are important storage data structures now used to store personal\ninformation of individuals around the globe. With the advent of personal genome\nsequencing, networks are going to be used to store personal genomic sequencing\nof people. In contrast to social media networks, the importance of\nrelationships in this genomic network is extremely significant. Losing\nconnections between individuals thus implies losing relationship information\n(E.g. father or son etc.). There currently exists a considerably serious\nproblem in the current approach to storing network data. Simply stated, network\ndata is not tamper-evident. In other words, if some links or nodes were\nchanged/removed/added by a malicious attacker, it would be impossible for the\nadministrator to detect such changes. While, in the current age of social media\nnetworks, change in node characteristics and links can be bad in terms of\nrelationships, in the case of networks for storing personal genomes, the\nresults could be truly devastating. Here we present a scheme for building\ntamper-evident networks using a combination of Cryptographic and Ego-based\nNetwork analytic methods. Using actual published data-sets, we also demonstrate\nthe utility and validity of the scheme besides demonstrating its working in\nvarious possible scenarios of usage. Results from the extensive experiments\ndemonstrate the validity of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 03:26:40 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Batool", "Komal", ""], ["Niazi", "Muaz A.", ""]]}, {"id": "1708.06521", "submitter": "Olivier Cur\\'e", "authors": "Xiangnan Ren and Olivier Cur\\'e and Hubert Naacke and Li Ke", "title": "Strider-lsa: Massive RDF Stream Reasoning in the Cloud", "comments": "10 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning over semantically annotated data is an emerging trend in stream\nprocessing aiming to produce sound and complete answers to a set of continuous\nqueries. It usually comes at the cost of finding a trade-off between data\nthroughput and the cost of expressive inferences. Strider-lsa proposes such a\ntrade-off and combines a scalable RDF stream processing engine with an\nefficient reasoning system. The main reasoning tasks are based on a query\nrewriting approach for SPARQL that benefits from an intelligent encoding of\nRDFS+ (RDFS + owl:sameAs) ontology elements. Strider-lsa runs in production at\na major international water management company to detect anomalies from sensor\nstreams. The system is evaluated along different dimensions and over multiple\ndatasets to emphasize its performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 07:45:30 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Ren", "Xiangnan", ""], ["Cur\u00e9", "Olivier", ""], ["Naacke", "Hubert", ""], ["Ke", "Li", ""]]}, {"id": "1708.06574", "submitter": "Jerome Darmont", "authors": "Somayeh Moghadam (ERIC), J\\'er\\^ome Darmont (ERIC), G\\'erald Gavin\n  (ERIC)", "title": "S4: A New Secure Scheme for Enforcing Privacy in Cloud Data Warehouses", "comments": null, "journal-ref": "7th International Conference on Information Systems and\n  Technologies (ICIST 2017), Mar 2017, Dubai, United Arab Emirates. pp.9-16,\n  2017, Proceedings of the 7th International Conference on Information Systems\n  and Technologies (ICIST 2017)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outsourcing data into the cloud becomes popular thanks to the pay-as-you-go\nparadigm. However, such practice raises privacy concerns. The conventional way\nto achieve data privacy is to encrypt sensitive data before outsourcing. When\ndata are encrypted, a trade-off must be achieved between security and efficient\nquery processing. Existing solutions that adopt multiple encryption schemes\ninduce a heavy overhead in terms of data storage and query performance, and are\nnot suited for cloud data warehouses. In this paper, we propose an efficient\nadditive encryption scheme (S4) based on Shamir's secret sharing for securing\ndata warehouses in the cloud. S4 addresses the shortcomings of existing\napproaches by reducing overhead while still enforcing good data privacy.\nExperimental results show the efficiency of S4 in terms of computation and\nstorage overhead with respect to existing solutions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 12:08:37 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Moghadam", "Somayeh", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Gavin", "G\u00e9rald", "", "ERIC"]]}, {"id": "1708.06712", "submitter": "Mangesh Bendre", "authors": "Mangesh Bendre, Vipul Venkataraman, Xinyan Zhou, Kevin Chang, Aditya\n  Parameswaran", "title": "Towards a Holistic Integration of Spreadsheets with Databases: A\n  Scalable Storage Engine for Presentational Data Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreadsheet software is the tool of choice for interactive ad-hoc data\nmanagement, with adoption by billions of users. However, spreadsheets are not\nscalable, unlike database systems. On the other hand, database systems, while\nhighly scalable, do not support interactivity as a first-class primitive. We\nare developing DataSpread, to holistically integrate spreadsheets as a\nfront-end interface with databases as a back-end datastore, providing\nscalability to spreadsheets, and interactivity to databases, an integration we\nterm presentational data management (PDM). In this paper, we make a first step\ntowards this vision: developing a storage engine for PDM, studying how to\nflexibly represent spreadsheet data within a database and how to support and\nmaintain access by position. We first conduct an extensive survey of\nspreadsheet use to motivate our functional requirements for a storage engine\nfor PDM. We develop a natural set of mechanisms for flexibly representing\nspreadsheet data and demonstrate that identifying the optimal representation is\nNP-Hard; however, we develop an efficient approach to identify the optimal\nrepresentation from an important and intuitive subclass of representations. We\nextend our mechanisms with positional access mechanisms that don't suffer from\ncascading update issues, leading to constant time access and modification\nperformance. We evaluate these representations on a workload of typical\nspreadsheets and spreadsheet operations, providing up to 20% reduction in\nstorage, and up to 50% reduction in formula evaluation time.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 16:37:22 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 02:12:59 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Bendre", "Mangesh", ""], ["Venkataraman", "Vipul", ""], ["Zhou", "Xinyan", ""], ["Chang", "Kevin", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1708.06884", "submitter": "Saurabh Hukerikar", "authors": "Byung H. Park, Saurabh Hukerikar, Ryan Adamson, Christian Engelmann", "title": "Big Data Meets HPC Log Analytics: Scalable Approach to Understanding\n  Systems at Extreme Scale", "comments": "IEEE Cluster 2017 at Workshop on Monitoring and Analysis for High\n  Performance Computing Systems Plus Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's high-performance computing (HPC) systems are heavily instrumented,\ngenerating logs containing information about abnormal events, such as critical\nconditions, faults, errors and failures, system resource utilization, and about\nthe resource usage of user applications. These logs, once fully analyzed and\ncorrelated, can produce detailed information about the system health, root\ncauses of failures, and analyze an application's interactions with the system,\nproviding valuable insights to domain scientists and system administrators.\nHowever, processing HPC logs requires a deep understanding of hardware and\nsoftware components at multiple layers of the system stack. Moreover, most log\ndata is unstructured and voluminous, making it more difficult for system users\nand administrators to manually inspect the data. With rapid increases in the\nscale and complexity of HPC systems, log data processing is becoming a big data\nchallenge. This paper introduces a HPC log data analytics framework that is\nbased on a distributed NoSQL database technology, which provides scalability\nand high availability, and the Apache Spark framework for rapid in-memory\nprocessing of the log data. The analytics framework enables the extraction of a\nrange of information about the system so that system administrators and end\nusers alike can obtain necessary insights for their specific needs. We describe\nour experience with using this framework to glean insights from the log data\nabout system behavior from the Titan supercomputer at the Oak Ridge National\nLaboratory.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 04:41:47 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Park", "Byung H.", ""], ["Hukerikar", "Saurabh", ""], ["Adamson", "Ryan", ""], ["Engelmann", "Christian", ""]]}, {"id": "1708.07308", "submitter": "Ce Zhang", "authors": "Tian Li, Jie Zhong, Ji Liu, Wentao Wu, Ce Zhang", "title": "Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning\n  Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ease.ml, a declarative machine learning service platform we built\nto support more than ten research groups outside the computer science\ndepartments at ETH Zurich for their machine learning needs. With ease.ml, a\nuser defines the high-level schema of a machine learning application and\nsubmits the task via a Web interface. The system automatically deals with the\nrest, such as model selection and data movement. In this paper, we describe the\nease.ml architecture and focus on a novel technical problem introduced by\nease.ml regarding resource allocation. We ask, as a \"service provider\" that\nmanages a shared cluster of machines among all our users running machine\nlearning workloads, what is the resource allocation strategy that maximizes the\nglobal satisfaction of all our users?\n  Resource allocation is a critical yet subtle issue in this multi-tenant\nscenario, as we have to balance between efficiency and fairness. We first\nformalize the problem that we call multi-tenant model selection, aiming for\nminimizing the total regret of all users running automatic model selection\ntasks. We then develop a novel algorithm that combines multi-armed bandits with\nBayesian optimization and prove a regret bound under the multi-tenant setting.\nFinally, we report our evaluation of ease.ml on synthetic data and on one\nservice we are providing to our users, namely, image classification with deep\nneural networks. Our experimental evaluation results show that our proposed\nsolution can be up to 9.8x faster in achieving the same global quality for all\nusers as the two popular heuristics used by our users before ease.ml.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 08:21:28 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Li", "Tian", ""], ["Zhong", "Jie", ""], ["Liu", "Ji", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""]]}, {"id": "1708.07436", "submitter": "Th\\^ong Nguy\\^en", "authors": "Th\\^ong T. Nguy\\^en and Siu Cheung Hui", "title": "Differentially Private Regression for Discrete-Time Survival Analysis", "comments": "19 pages, CIKM17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survival analysis, regression models are used to understand the effects of\nexplanatory variables (e.g., age, sex, weight, etc.) to the survival\nprobability. However, for sensitive survival data such as medical data, there\nare serious concerns about the privacy of individuals in the data set when\nmedical data is used to fit the regression models. The closest work addressing\nsuch privacy concerns is the work on Cox regression which linearly projects the\noriginal data to a lower dimensional space. However, the weakness of this\napproach is that there is no formal privacy guarantee for such projection. In\nthis work, we aim to propose solutions for the regression problem in survival\nanalysis with the protection of differential privacy which is a golden standard\nof privacy protection in data privacy research. To this end, we extend the\nOutput Perturbation and Objective Perturbation approaches which are originally\nproposed to protect differential privacy for the Empirical Risk Minimization\n(ERM) problems. In addition, we also propose a novel sampling approach based on\nthe Markov Chain Monte Carlo (MCMC) method to practically guarantee\ndifferential privacy with better accuracy. We show that our proposed approaches\nachieve good accuracy as compared to the non-private results while guaranteeing\ndifferential privacy for individuals in the private data set.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 14:25:44 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 01:42:35 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Nguy\u00ean", "Th\u00f4ng T.", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1708.07624", "submitter": "Tommaso Soru", "authors": "Tommaso Soru, Edgard Marx, Diego Moussallem, Gustavo Publio, Andr\\'e\n  Valdestilhas, Diego Esteves, Ciro Baron Neto", "title": "SPARQL as a Foreign Language", "comments": "SEMANTiCS 2017; 13th International Conference on Semantic Systems,\n  2017", "journal-ref": "SEMANTiCS CEUR Workshop Proceedings 2044 (2017) Paper 14", "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, the Linked Data Cloud has achieved a size of more than 100\nbillion facts pertaining to a multitude of domains. However, accessing this\ninformation has been significantly challenging for lay users. Approaches to\nproblems such as Question Answering on Linked Data and Link Discovery have\nnotably played a role in increasing information access. These approaches are\noften based on handcrafted and/or statistical models derived from data\nobservation. Recently, Deep Learning architectures based on Neural Networks\ncalled seq2seq have shown to achieve state-of-the-art results at translating\nsequences into sequences. In this direction, we propose Neural SPARQL Machines,\nend-to-end deep architectures to translate any natural language expression into\nsentences encoding SPARQL queries. Our preliminary results, restricted on\nselected DBpedia classes, show that Neural SPARQL Machines are a promising\napproach for Question Answering on Linked Data, as they can deal with known\nproblems such as vocabulary mismatch and perform graph pattern composition.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 06:41:55 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 18:13:19 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Soru", "Tommaso", ""], ["Marx", "Edgard", ""], ["Moussallem", "Diego", ""], ["Publio", "Gustavo", ""], ["Valdestilhas", "Andr\u00e9", ""], ["Esteves", "Diego", ""], ["Neto", "Ciro Baron", ""]]}, {"id": "1708.07859", "submitter": "Christopher Aberger", "authors": "Christopher R. Aberger, Andrew Lamb, Kunle Olukotun, and Christopher\n  R\\'e", "title": "LevelHeaded: Making Worst-Case Optimal Joins Work in the Common Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pipelines combining SQL-style business intelligence (BI) queries and linear\nalgebra (LA) are becoming increasingly common in industry. As a result, there\nis a growing need to unify these workloads in a single framework.\nUnfortunately, existing solutions either sacrifice the inherent benefits of\nexclusively using a relational database (e.g. logical and physical\nindependence) or incur orders of magnitude performance gaps compared to\nspecialized engines (or both). In this work we study applying a new type of\nquery processing architecture to standard BI and LA benchmarks. To do this we\npresent a new in-memory query processing engine called LevelHeaded. LevelHeaded\nuses worst-case optimal joins as its core execution mechanism for both BI and\nLA queries. With LevelHeaded, we show how crucial optimizations for BI and LA\nqueries can be captured in a worst-case optimal query architecture. Using these\noptimizations, LevelHeaded outperforms other relational database engines\n(LogicBlox, MonetDB, and HyPer) by orders of magnitude on standard LA\nbenchmarks, while performing on average within 31% of the best-of-breed BI\n(HyPer) and LA (Intel MKL) solutions on their own benchmarks. Our results show\nthat such a single query processing architecture is capable of delivering\ncompetitive performance on both BI and LA queries.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 18:49:30 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Aberger", "Christopher R.", ""], ["Lamb", "Andrew", ""], ["Olukotun", "Kunle", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1708.07975", "submitter": "Vincent Bindschaedler", "authors": "Vincent Bindschaedler, Reza Shokri, Carl A. Gunter", "title": "Plausible Deniability for Privacy-Preserving Data Synthesis", "comments": "In PVLDB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Releasing full data records is one of the most challenging problems in data\nprivacy. On the one hand, many of the popular techniques such as data\nde-identification are problematic because of their dependence on the background\nknowledge of adversaries. On the other hand, rigorous methods such as the\nexponential mechanism for differential privacy are often computationally\nimpractical to use for releasing high dimensional data or cannot preserve high\nutility of original data due to their extensive data perturbation.\n  This paper presents a criterion called plausible deniability that provides a\nformal privacy guarantee, notably for releasing sensitive datasets: an output\nrecord can be released only if a certain amount of input records are\nindistinguishable, up to a privacy parameter. This notion does not depend on\nthe background knowledge of an adversary. Also, it can efficiently be checked\nby privacy tests. We present mechanisms to generate synthetic datasets with\nsimilar statistical properties to the input data and the same format. We study\nthis technique both theoretically and experimentally. A key theoretical result\nshows that, with proper randomization, the plausible deniability mechanism\ngenerates differentially private synthetic data. We demonstrate the efficiency\nof this generative technique on a large dataset; it is shown to preserve the\nutility of original data with respect to various statistical analysis and\nmachine learning measures.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 14:13:28 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Bindschaedler", "Vincent", ""], ["Shokri", "Reza", ""], ["Gunter", "Carl A.", ""]]}, {"id": "1708.08191", "submitter": "Xiaofei Wang", "authors": "Xiaofei Wang (1), Qianhong Wu (2), Yuqing Zhang (1) ((1) National\n  Computer Network Intrusion Protection Center, University of Chinese Academy\n  of Sciences, Beijing, P.R. China, (2) School of Electronic and Information\n  Engineering, Beihang University, Beijing, P.R. China)", "title": "T-DB: Toward Fully Functional Transparent Encrypted Databases in DBaaS\n  Framework", "comments": "37 pages. 12 figures. 6 tables. Source codes and syntax rules are\n  available at http://www.nipc.org.cn/tdbsources.aspx", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals and organizations tend to migrate their data to clouds,\nespecially in a DataBase as a Service (DBaaS) pattern. The major obstacle is\nthe conflict between secrecy and utilization of the relational database to be\noutsourced. We address this obstacle with a Transparent DataBase (T-DB) system\nstrictly following the unmodified DBaaS framework. A database owner outsources\nan encrypted database to a cloud platform, needing only to store the secret\nkeys for encryption and an empty table header for the database; the database\nusers can make almost all types of queries on the encrypted database as usual;\nand the cloud can process ciphertext queries as if the database were not\nencrypted. Experimentations in realistic cloud environments demonstrate that\nT-DB has perfect query answer precision and outstanding performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 05:18:11 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Wang", "Xiaofei", ""], ["Wu", "Qianhong", ""], ["Zhang", "Yuqing", ""]]}, {"id": "1708.08197", "submitter": "Tianyue Zheng", "authors": "Tianyue Zheng, Weihong Deng and Jiani Hu", "title": "Cross-Age LFW: A Database for Studying Cross-Age Face Recognition in\n  Unconstrained Environments", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeled Faces in the Wild (LFW) database has been widely utilized as the\nbenchmark of unconstrained face verification and due to big data driven machine\nlearning methods, the performance on the database approaches nearly 100%.\nHowever, we argue that this accuracy may be too optimistic because of some\nlimiting factors. Besides different poses, illuminations, occlusions and\nexpressions, cross-age face is another challenge in face recognition. Different\nages of the same person result in large intra-class variations and aging\nprocess is unavoidable in real world face verification. However, LFW does not\npay much attention on it. Thereby we construct a Cross-Age LFW (CALFW) which\ndeliberately searches and selects 3,000 positive face pairs with age gaps to\nadd aging process intra-class variance. Negative pairs with same gender and\nrace are also selected to reduce the influence of attribute difference between\npositive/negative pairs and achieve face verification instead of attributes\nclassification. We evaluate several metric learning and deep learning methods\non the new database. Compared to the accuracy on LFW, the accuracy drops about\n10%-17% on CALFW.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 06:07:27 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Zheng", "Tianyue", ""], ["Deng", "Weihong", ""], ["Hu", "Jiani", ""]]}, {"id": "1708.08319", "submitter": "Jim Pivarski", "authors": "Jim Pivarski, Peter Elmer, Brian Bockelman, Zhe Zhang", "title": "Fast Access to Columnar, Hierarchically Nested Data via Code\n  Transformation", "comments": "10 pages, 2 figures, submitted to IEEE Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data query systems represent data in a columnar format for fast,\nselective access, and in some cases (e.g. Apache Drill), perform calculations\ndirectly on the columnar data without row materialization, avoiding runtime\ncosts.\n  However, many analysis procedures cannot be easily or efficiently expressed\nas SQL. In High Energy Physics, the majority of data processing requires nested\nloops with complex dependencies. When faced with tasks like these, the\nconventional approach is to convert the columnar data back into an object form,\nusually with a performance price.\n  This paper describes a new technique to transform procedural code so that it\noperates on hierarchically nested, columnar data natively, without row\nmaterialization. It can be viewed as a compiler pass on the typed abstract\nsyntax tree, rewriting references to objects as columnar array lookups.\n  We will also present performance comparisons between transformed code and\nconventional object-oriented code in a High Energy Physics context.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 23:41:13 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 16:02:33 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Pivarski", "Jim", ""], ["Elmer", "Peter", ""], ["Bockelman", "Brian", ""], ["Zhang", "Zhe", ""]]}, {"id": "1708.08435", "submitter": "Prajakta Kalmegh", "authors": "Prajakta Kalmegh, Shivnath Babu, Sudeepa Roy", "title": "Analyzing Query Performance and Attributing Blame for Contentions in a\n  Cluster Computing Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many approaches is use today to either prevent or minimize the\nimpact of inter-query interactions on a shared cluster. Despite these measures,\nperformance issues due to concurrent executions of mixed workloads still\nprevail causing undue waiting times for queries. Analyzing these resource\ninterferences is thus critical in order to answer time sensitive questions like\n'who is causing my query to slowdown' in a multi-tenant environment. More\nimportantly, dignosing whether the slowdown of a query is a result of resource\ncontentions caused by other queries or some other external factor can help an\nadmin narrow down the many possibilities of performance degradation. This\nprocess of investigating the symptoms of resource contentions and attributing\nblame to concurrent queries is non-trivial and tedious, and involves hours of\nmanually debugging through a cycle of query interactions.\n  In this paper, we present ProtoXplore - a Proto or first system to eXplore\ncontentions, that helps administrators determine whether the blame for resource\nbottlenecks can be attributed to concurrent queries, and uses a methodology\ncalled Resource Acquire Time Penalty (RATP) to quantify this blame towards\ncontentious sources accurately. Further, ProtoXplore builds on the theory of\nexplanations and enables a step-wise deep exploration of various levels of\nperformance bottlenecks faced by a query during its execution using a\nmulti-level directed acyclic graph called ProtoGraph. Our experimental\nevaluation uses ProtoXplore to analyze the interactions between TPC-DS queries\non Apache Spark to show how ProtoXplore provides explanations that help in\ndiagnosing contention related issues and better managing a changing mixed\nworkload in a shared cluster.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 17:44:44 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 22:56:59 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Kalmegh", "Prajakta", ""], ["Babu", "Shivnath", ""], ["Roy", "Sudeepa", ""]]}, {"id": "1708.08674", "submitter": "Piotr S. Maci\u00c4\u0085g", "authors": "Piotr S. Maci\\k{a}g", "title": "Discovering Sequential Patterns in Event-Based Spatio-Temporal Data by\n  Means of Microclustering - Extended Report", "comments": "Comparing to the prior version, experimental results have been\n  revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we consider the problem of discovering sequential patterns from\nevent-based spatio-temporal data. The problem is defined as follows: for a set\nof event types $F$ and for a dataset of events instances $D$ (where each\ninstance in $D$ denotes an occurrence of a particular event type in considered\nspatio-temporal space), discover all sequential patterns defining the following\nrelation between any event types participating in a particular pattern. The\nfollowing relation $\\rightarrow$ between any two event types, denotes the fact\nthat instances of the first event type attract in their spatial proximity and\nin considered temporal window afterwards occurrences of instances of the second\nevent type. In the article, the notion of sequential pattern in event-based\nspatio-temporal data has been defined and the already proposed approach to\ndiscovering sequential pattern has been reformulated. We show, it is possible\nto efficiently and effectively discover sequential patterns in event-based\nspatio-temporal data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 10:00:37 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 23:22:23 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Maci\u0105g", "Piotr S.", ""]]}, {"id": "1708.08905", "submitter": "Yihan Gao", "authors": "Yihan Gao, Silu Huang, Aditya Parameswaran", "title": "Navigating the Data Lake with Datamaran: Automatically Extracting\n  Structure from Log Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations routinely accumulate semi-structured log datasets generated as\nthe output of code; these datasets remain unused and uninterpreted, and occupy\nwasted space - this phenomenon has been colloquially referred to as \"data lake\"\nproblem. One approach to leverage these semi-structured datasets is to convert\nthem into a structured relational format, following which they can be analyzed\nin conjunction with other datasets. We present Datamaran, an tool that extracts\nstructure from semi-structured log datasets with no human supervision.\nDatamaran automatically identifies field and record endpoints, separates the\nstructured parts from the unstructured noise or formatting, and can tease apart\nmultiple structures from within a dataset, in order to efficiently extract\nstructured relational datasets from semi-structured log datasets, at scale with\nhigh accuracy. Compared to other unsupervised log dataset extraction tools\ndeveloped in prior work, Datamaran does not require the record boundaries to be\nknown beforehand, making it much more applicable to the noisy log files that\nare ubiquitous in data lakes. Datamaran can successfully extract structured\ninformation from all datasets used in prior work, and can achieve 95%\nextraction accuracy on automatically collected log datasets from GitHub - a\nsubstantial 66% increase of accuracy compared to unsupervised schemes from\nprior work. Our user study further demonstrates that the extraction results of\nDatamaran are closer to the desired structure than competing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 17:47:08 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 03:47:07 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 01:09:20 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Gao", "Yihan", ""], ["Huang", "Silu", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1708.09140", "submitter": "Ester Livshits", "authors": "Ester Livshits and Benny Kimelfeld", "title": "The Complexity of Computing a Cardinality Repair for Functional\n  Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a relation that violates a set of functional dependencies, we consider\nthe task of finding a maximum number of pairwise-consistent tuples, or what is\nknown as a \"cardinality repair.\" We present a polynomial-time algorithm that,\nfor certain fixed relation schemas (with functional dependencies), computes a\ncardinality repair. Moreover, we prove that on any of the schemas not covered\nby the algorithm, finding a cardinality repair is, in fact, an NP-hard problem.\nIn particular, we establish a dichotomy in the complexity of computing a\ncardinality repair, and we present an efficient algorithm to determine whether\na given schema belongs to the positive side or the negative side of the\ndichotomy.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 07:03:58 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Livshits", "Ester", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "1708.09171", "submitter": "Jerome Darmont", "authors": "Somayeh Sobati Moghadam (1), J\\'er\\^ome Darmont (1), G\\'erald Gavin\n  (1) ((1) ERIC)", "title": "Enforcing Privacy in Cloud Databases", "comments": null, "journal-ref": "19th International Conference on Big Data Analytics and Knowledge\n  Discovery (DaWaK 2017), Aug 2017, Lyon, France. Springer, Lecture Notes in\n  Computer Science, 10440, pp.53-73, 2017", "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outsourcing databases, i.e., resorting to Database-as-a-Service (DBaaS), is\nnowadays a popular choice due to the elasticity, availability, scalability and\npay-as-you-go features of cloud computing. However, most data are sensitive to\nsome extent, and data privacy remains one of the top concerns to DBaaS users,\nfor obvious legal and competitive reasons.In this paper, we survey the\nmechanisms that aim at making databases secure in a cloud environment, and\ndiscuss current pitfalls and related research challenges.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 08:47:20 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Moghadam", "Somayeh Sobati", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Gavin", "G\u00e9rald", "", "ERIC"]]}, {"id": "1708.09299", "submitter": "Markus Nentwig", "authors": "Markus Nentwig, Anika Gro{\\ss}, Maximilian M\\\"oller, Erhard Rahm", "title": "Distributed Holistic Clustering on Linked Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link discovery is an active field of research to support data integration in\nthe Web of Data. Due to the huge size and number of available data sources,\nefficient and effective link discovery is a very challenging task. Common\npairwise link discovery approaches do not scale to many sources with very large\nentity sets. We here propose a distributed holistic approach to link many data\nsources based on a clustering of entities that represent the same real-world\nobject. Our clustering approach provides a compact and fused representation of\nentities, and can identify errors in existing links as well as many new links.\nWe support a distributed execution of the clustering approach to achieve faster\nexecution times and scalability for large real-world data sets. We provide a\nnovel gold standard for multi-source clustering, and evaluate our methods with\nrespect to effectiveness and efficiency for large data sets from the geographic\nand music domains.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 14:36:19 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Nentwig", "Markus", ""], ["Gro\u00df", "Anika", ""], ["M\u00f6ller", "Maximilian", ""], ["Rahm", "Erhard", ""]]}, {"id": "1708.09332", "submitter": "Doina Cosovan", "authors": "Sinica Alboaie, Doina Cosovan", "title": "Private Data System Enabling Self-Sovereign Storage Managed by\n  Executable Choreographies", "comments": "DAIS 2017", "journal-ref": "IFIP International Conference on Distributed Applications and\n  Interoperable Systems, DAIS 2017: Distributed Applications and Interoperable\n  Systems pp 83-98", "doi": "10.1007/978-3-319-59665-5_6", "report-no": null, "categories": "cs.DC cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased use of Internet, governments and large companies store and\nshare massive amounts of personal data in such a way that leaves no space for\ntransparency. When a user needs to achieve a simple task like applying for\ncollege or a driving license, he needs to visit a lot of institutions and\norganizations, thus leaving a lot of private data in many places. The same\nhappens when using the Internet. These privacy issues raised by the centralized\narchitectures along with the recent developments in the area of serverless\napplications demand a decentralized private data layer under user control. We\nintroduce the Private Data System (PDS), a distributed approach which enables\nself-sovereign storage and sharing of private data. The system is composed of\nnodes spread across the entire Internet managing local key-value databases. The\ncommunication between nodes is achieved through executable choreographies,\nwhich are capable of preventing information leakage when executing across\ndifferent organizations with different regulations in place. The user has full\ncontrol over his private data and is able to share and revoke access to\norganizations at any time. Even more, the updates are propagated instantly to\nall the parties which have access to the data thanks to the system design.\nSpecifically, the processing organizations may retrieve and process the shared\ninformation, but are not allowed under any circumstances to store it on long\nterm. PDS offers an alternative to systems that aim to ensure self-sovereignty\nof specific types of data through blockchain inspired techniques but face\nvarious problems, such as low performance. Both approaches propose a\ndistributed database, but with different characteristics. While the\nblockchain-based systems are built to solve consensus problems, PDS's purpose\nis to solve the self-sovereignty aspects raised by the privacy laws, rules and\nprinciples.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 15:00:05 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Alboaie", "Sinica", ""], ["Cosovan", "Doina", ""]]}]