[{"id": "2106.00388", "submitter": "Felix Mannhardt", "authors": "Gamal Elkoumy, Stephan A. Fahrenkrog-Petersen, Mohammadreza Fani Sani,\n  Agnes Koschmider, Felix Mannhardt, Saskia Nu\\~nez von Voigt, Majid Rafiei,\n  Leopold von Waldthausen", "title": "Privacy and Confidentiality in Process Mining -- Threats and Research\n  Challenges", "comments": "Accepted for publication in ACM Transactions on Management\n  Information Systems", "journal-ref": null, "doi": "10.1145/3468877", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy and confidentiality are very important prerequisites for applying\nprocess mining in order to comply with regulations and keep company secrets.\nThis paper provides a foundation for future research on privacy-preserving and\nconfidential process mining techniques. Main threats are identified and related\nto an motivation application scenario in a hospital context as well as to the\ncurrent body of work on privacy and confidentiality in process mining. A newly\ndeveloped conceptual model structures the discussion that existing techniques\nleave room for improvement. This results in a number of important research\nchallenges that should be addressed by future process mining research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:51:11 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Elkoumy", "Gamal", ""], ["Fahrenkrog-Petersen", "Stephan A.", ""], ["Sani", "Mohammadreza Fani", ""], ["Koschmider", "Agnes", ""], ["Mannhardt", "Felix", ""], ["von Voigt", "Saskia Nu\u00f1ez", ""], ["Rafiei", "Majid", ""], ["von Waldthausen", "Leopold", ""]]}, {"id": "2106.00412", "submitter": "Vashti Galpin", "authors": "Vashti Galpin, James Cheney", "title": "Curating Covid-19 data in Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curated scientific databases play an important role in the scientific\nendeavour and support is needed for the significant effort that goes into their\ncreation and maintenance. This demonstration and case study illustrate how\ncuration support has been developed in the Links cross-tier programming\nlanguage, a functional, strongly typed language with language-integrated query\nand support for temporal databases. The chosen case study uses weekly released\nCovid-19 fatality figures from the Scottish government which exhibit updates to\npreviously released data. This data allows the capture and query of update\nprovenance in our prototype. This demonstration will highlight the potential\nfor language-integrated support for curation to simplify and streamline\nprototyping of web-applications in support of scientific databases\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 11:52:59 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Galpin", "Vashti", ""], ["Cheney", "James", ""]]}, {"id": "2106.00932", "submitter": "Khushi Shah", "authors": "Khushi Shah, Aryan Shah, Charmi Shah, Devansh Shah, Mustafa\n  Africawala, Rushabh Shah, Nishant Doshi", "title": "Proposed DBMS for OTT platforms in line with new age requirements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Database management has become an enormous tool for on-demand content\ndistribution services, proffering required information and providing custom\nservices to the user. Also plays a major role for the platforms to manage their\ndata in such a way that data redundancy is minimized. This paper emphasizes\nimproving the user experience for the platform by efficiently managing data.\nKeeping in mind all the new age requirements, especially after COVID-19 the\nsudden surge in subscription has led the stakeholders to try new things to lead\nthe OTT market. Collection of shows being the root of the tree here, this paper\nimprovises the currently existing branches via various tables and suggests some\nnew features on how the data collected can be utilized for introducing new and\nmuch-required query results for the consumer.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 04:42:01 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Shah", "Khushi", ""], ["Shah", "Aryan", ""], ["Shah", "Charmi", ""], ["Shah", "Devansh", ""], ["Africawala", "Mustafa", ""], ["Shah", "Rushabh", ""], ["Doshi", "Nishant", ""]]}, {"id": "2106.01074", "submitter": "James Thorne", "authors": "James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri,\n  Sebastian Riedel, Alon Halevy", "title": "Database Reasoning Over Text", "comments": "To appear at ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models have shown impressive performance gains in answering queries\nfrom natural language text. However, existing works are unable to support\ndatabase queries, such as \"List/Count all female athletes who were born in 20th\ncentury\", which require reasoning over sets of relevant facts with operations\nsuch as join, filtering and aggregation. We show that while state-of-the-art\ntransformer models perform very well for small databases, they exhibit\nlimitations in processing noisy data, numerical operations, and queries that\naggregate facts. We propose a modular architecture to answer these\ndatabase-style queries over multiple spans from text and aggregating these at\nscale. We evaluate the architecture using WikiNLDB, a novel dataset for\nexploring such queries. Our architecture scales to databases containing\nthousands of facts whereas contemporary models are limited by how many facts\ncan be encoded. In direct comparison on small databases, our approach increases\noverall answer accuracy from 85% to 90%. On larger databases, our approach\nretains its accuracy whereas transformer baselines could not encode the\ncontext.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 11:09:40 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Thorne", "James", ""], ["Yazdani", "Majid", ""], ["Saeidi", "Marzieh", ""], ["Silvestri", "Fabrizio", ""], ["Riedel", "Sebastian", ""], ["Halevy", "Alon", ""]]}, {"id": "2106.01501", "submitter": "Sahaana Suri", "authors": "Sahaana Suri, Ihab F. Ilyas, Christopher R\\'e, Theodoros Rekatsinas", "title": "Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured data, or data that adheres to a pre-defined schema, can suffer\nfrom fragmented context: information describing a single entity can be\nscattered across multiple datasets or tables tailored for specific business\nneeds, with no explicit linking keys (e.g., primary key-foreign key\nrelationships or heuristic functions). Context enrichment, or rebuilding\nfragmented context, using keyless joins is an implicit or explicit step in\nmachine learning (ML) pipelines over structured data sources. This process is\ntedious, domain-specific, and lacks support in now-prevalent no-code ML systems\nthat let users create ML pipelines using just input data and high-level\nconfiguration files. In response, we propose Ember, a system that abstracts and\nautomates keyless joins to generalize context enrichment. Our key insight is\nthat Ember can enable a general keyless join operator by constructing an index\npopulated with task-specific embeddings. Ember learns these embeddings by\nleveraging Transformer-based representation learning techniques. We describe\nour core architectural principles and operators when developing Ember, and\nempirically demonstrate that Ember allows users to develop no-code pipelines\nfor five domains, including search, recommendation and question answering, and\ncan exceed alternatives by up to 39% recall, with as little as a single line\nconfiguration change.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 23:02:26 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Suri", "Sahaana", ""], ["Ilyas", "Ihab F.", ""], ["R\u00e9", "Christopher", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "2106.01543", "submitter": "Yue Gong", "authors": "Yue Gong, Zhiru Zhu, Sainyam Galhotra, Raul Castro Fernandez", "title": "Niffler: A Reference Architecture and System Implementation for View\n  Discovery over Pathless Table Collections by Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying a project-join view (PJ-view) over collections of tables is the\nfirst step of many data management projects, e.g., assembling a dataset to feed\ninto a business intelligence tool, creating a training dataset to fit a machine\nlearning model, and more. When the table collections are large and lack join\ninformation--such as when combining databases, or on data lakes--query by\nexample (QBE) systems can help identify relevant data, but they are designed\nunder the assumption that join information is available in the schema, and do\nnot perform well on pathless table collections that do not have join path\ninformation.\n  We present a reference architecture that explicitly divides the end-to-end\nproblem of discovering PJ-views over pathless table collections into a human\nand a technical problem. We then present Niffler, a system built to address the\ntechnical problem. We introduce algorithms for the main components of Niffler,\nincluding a signal generation component that helps reduce the size of the\ncandidate views that may be large due to errors and ambiguity in both the data\nand input queries. We evaluate Niffler on real datasets to demonstrate the\neffectiveness of the new engine in discovering PJ-views over pathless table\ncollections.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:58:24 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 19:52:22 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gong", "Yue", ""], ["Zhu", "Zhiru", ""], ["Galhotra", "Sainyam", ""], ["Fernandez", "Raul Castro", ""]]}, {"id": "2106.02361", "submitter": "Enrico Daga", "authors": "Enrico Daga, Luigi Asprino, Paul Mulholland, Aldo Gangemi", "title": "Facade-X: an opinionated approach to SPARQL anything", "comments": "Version submitted to the SEMANTICS 2021 EU conference (Accepted May\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Semantic Web research community understood since its beginning how\ncrucial it is to equip practitioners with methods to transform non-RDF\nresources into RDF. Proposals focus on either engineering content\ntransformations or accessing non-RDF resources with SPARQL. Existing solutions\nrequire users to learn specific mapping languages (e.g. RML), to know how to\nquery and manipulate a variety of source formats (e.g. XPATH, JSON-Path), or to\ncombine multiple languages (e.g. SPARQL Generate). In this paper, we explore an\nalternative solution and contribute a general-purpose meta-model for converting\nnon-RDF resources into RDF: Facade-X. Our approach can be implemented by\noverriding the SERVICE operator and does not require to extend the SPARQL\nsyntax. We compare our approach with the state of art methods RML and SPARQL\nGenerate and show how our solution has lower learning demands and cognitive\ncomplexity, and it is cheaper to implement and maintain, while having\ncomparable extensibility and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:19:47 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Daga", "Enrico", ""], ["Asprino", "Luigi", ""], ["Mulholland", "Paul", ""], ["Gangemi", "Aldo", ""]]}, {"id": "2106.03085", "submitter": "Soumyabrata Dev", "authors": "Jiantao Wu, Fabrizio Orlandi, Declan O'Sullivan, and Soumyabrata Dev", "title": "An Ontology Model for Climatic Data Analysis", "comments": "Published in IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently ontologies have been exploited in a wide range of research areas for\ndata modeling and data management. They greatly assists in defining the\nsemantic model of the underlying data combined with domain knowledge. In this\npaper, we propose the Climate Analysis (CA) Ontology to model climate datasets\nused by remote sensing analysts. We use the data published by National Oceanic\nand Atmospheric Administration (NOAA) to further explore how ontology modeling\ncan be used to facilitate the field of climatic data processing. The idea of\nthis work is to convert relational climate data to the Resource Description\nFramework (RDF) data model, so that it can be stored in a graph database and\neasily accessed through the Web as Linked Data. Typically, this provides\nclimate researchers, who are interested in datasets such as NOAA, with the\npotential of enriching and interlinking with other databases. As a result, our\napproach facilitates data integration and analysis of diverse climatic data\nsources and allows researchers to interrogate these sources directly on the Web\nusing the standard SPARQL query language.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 10:33:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wu", "Jiantao", ""], ["Orlandi", "Fabrizio", ""], ["O'Sullivan", "Declan", ""], ["Dev", "Soumyabrata", ""]]}, {"id": "2106.03355", "submitter": "Yanchuan Chang", "authors": "Yanchuan Chang, Jianzhong Qi, Egemen Tanin, Xingjun Ma, Hanan Samet", "title": "Sub-trajectory Similarity Join with Obfuscation", "comments": null, "journal-ref": null, "doi": "10.1145/3468791.3468822", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User trajectory data is becoming increasingly accessible due to the\nprevalence of GPS-equipped devices such as smartphones. Many existing studies\nfocus on querying trajectories that are similar to each other in their\nentirety. We observe that trajectories partially similar to each other contain\nuseful information about users' travel patterns which should not be ignored.\nSuch partially similar trajectories are critical in applications such as\nepidemic contact tracing. We thus propose to query trajectories that are within\na given distance range from each other for a given period of time. We formulate\nthis problem as a sub-trajectory similarity join query named as the STS-Join.\nWe further propose a distributed index structure and a query algorithm for\nSTS-Join, where users retain their raw location data and only send obfuscated\ntrajectories to a server for query processing. This helps preserve user\nlocation privacy which is vital when dealing with such data. Theoretical\nanalysis and experiments on real data confirm the effectiveness and the\nefficiency of our proposed index structure and query algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 06:08:06 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chang", "Yanchuan", ""], ["Qi", "Jianzhong", ""], ["Tanin", "Egemen", ""], ["Ma", "Xingjun", ""], ["Samet", "Hanan", ""]]}, {"id": "2106.03840", "submitter": "Giorgos Xanthakis", "authors": "Giorgos Xanthakis, Giorgos Saloustros, Nikos Batsaras, Anastasios\n  Papagiannis, Angelos Bilas", "title": "Balancing Garbage Collection vs I/O Amplification using hybrid Key-Value\n  Placement in LSM-based Key-Value Stores", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key-value (KV) separation is a technique that introduces randomness in the\nI/O access patterns to reduce I/O amplification in LSM-based key-value stores\nfor fast storage devices (NVMe). KV separation has a significant drawback that\nmakes it less attractive: Delete and especially update operations that are\nimportant in modern workloads result in frequent and expensive garbage\ncollection (GC) in the value log. In this paper, we design and implement\nParallax, which proposes hybrid KV placement that reduces GC overhead\nsignificantly and maximizes the benefits of using a log. We first model the\nbenefits of KV separation for different KV pair sizes. We use this model to\nclassify KV pairs in three categories small, medium, and large. Then, Parallax\nuses different approaches for each KV category: It always places large values\nin a log and small values in place. For medium values it uses a mixed strategy\nthat combines the benefits of using a log and eliminates GC overhead as\nfollows: It places medium values in a log for all but the last few (typically\none or two) levels in the LSM structure, where it performs a full compaction,\nmerges values in place, and reclaims log space without the need for GC. We\nevaluate Parallax against RocksDB that places all values in place and BlobDB\nthat always performs KV separation. We find that Parallax increases throughput\nby up to 12.4x and 17.83x, decreases I/O amplification by up to 27.1x and 26x,\nand increases CPU efficiency by up to 18.7x and 28x respectively, for all but\nscan-based YCSB workloads.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:55:32 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 17:10:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xanthakis", "Giorgos", ""], ["Saloustros", "Giorgos", ""], ["Batsaras", "Nikos", ""], ["Papagiannis", "Anastasios", ""], ["Bilas", "Angelos", ""]]}, {"id": "2106.03965", "submitter": "Somalee Datta", "authors": "Sanjay Malunjkar, Susan Weber, Somalee Datta", "title": "A highly scalable repository of waveform and vital signs data from\n  bedside monitoring devices", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The advent of cost effective cloud computing over the past decade and\never-growing accumulation of high-fidelity clinical data in a modern hospital\nsetting is leading to new opportunities for translational medicine. Machine\nlearning is driving the appetite of the research community for various types of\nsignal data such as patient vitals. Health care systems, however, are ill\nsuited for massive processing of large volumes of data. In addition, due to the\nsheer magnitude of the data being collected, it is not feasible to retain all\nof the data in health care systems in perpetuity. This gold mine of information\ngets purged periodically thereby losing invaluable future research\nopportunities. We have developed a highly scalable solution that: a) siphons\noff patient vital data on a nightly basis from on-premises bio-medical systems\nto a cloud storage location as a permanent archive, b) reconstructs the\ndatabase in the cloud, c) generates waveforms, alarms and numeric data in a\nresearch-ready format, and d) uploads the processed data to a storage location\nin the cloud ready for research.\n  The data is de-identified and catalogued such that it can be joined with\nElectronic Medical Records (EMR) and other ancillary data types such as\nelectroencephalogram (EEG), radiology, video monitoring etc. This technique\neliminates the research burden from health care systems. This highly scalable\nsolution is used to process high density patient monitoring data aggregated by\nthe Philips Patient Information Center iX (PIC iX) hospital surveillance system\nfor archival storage in the Philips Data Warehouse Connect enterprise-level\ndatabase. The solution is part of a broader platform that supports a secure\nhigh performance clinical data science platform.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:59:58 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Malunjkar", "Sanjay", ""], ["Weber", "Susan", ""], ["Datta", "Somalee", ""]]}, {"id": "2106.04384", "submitter": "Shuyuan Zheng", "authors": "Shuyuan Zheng, Yang Cao, and Masatoshi Yoshikawa", "title": "Incentive Mechanism for Privacy-Preserving Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is an emerging paradigm for machine learning, in\nwhich data owners can collaboratively train a model by sharing gradients\ninstead of their raw data. Two fundamental research problems in FL are\nincentive mechanism and privacy protection. The former focuses on how to\nincentivize data owners to participate in FL. The latter studies how to protect\ndata owners' privacy while maintaining high utility of trained models. However,\nincentive mechanism and privacy protection in FL have been studied separately\nand no work solves both problems at the same time. In this work, we address the\ntwo problems simultaneously by an FL-Market that incentivizes data owners'\nparticipation by providing appropriate payments and privacy protection.\nFL-Market enables data owners to obtain compensation according to their privacy\nloss quantified by local differential privacy (LDP). Our insight is that, by\nmeeting data owners' personalized privacy preferences and providing appropriate\npayments, we can (1) incentivize privacy risk-tolerant data owners to set\nlarger privacy parameters (i.e., gradients with less noise) and (2) provide\npreferred privacy protection for privacy risk-averse data owners. To achieve\nthis, we design a personalized LDP-based FL framework with a deep\nlearning-empowered auction mechanism for incentivizing trading gradients with\nless noise and optimal aggregation mechanisms for model updates. Our\nexperiments verify the effectiveness of the proposed framework and mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:14:24 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zheng", "Shuyuan", ""], ["Cao", "Yang", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "2106.04703", "submitter": "Owen Lynch", "authors": "Evan Patterson, Owen Lynch, and James Fairbanks", "title": "Categorical Data Structures for Technical Computing", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many mathematical objects can be represented as functors from\nfinitely-presented categories $\\mathsf{C}$ to $\\mathsf{Set}$. For instance,\ngraphs are functors to $\\mathsf{C}$ from the category with two parallel arrows.\nSuch functors are known informally as $\\mathsf{C}$-sets. In this paper, we\ndescribe and implement an extension of $\\mathsf{C}$-sets having data attributes\nwith fixed types, such as graphs with labeled vertices or real-valued edge\nweights. We call such structures \"acsets,\" short for \"attributed\n$\\mathsf{C}$-sets.\" Derived from previous work on algebraic databases, acsets\nare a joint generalization of graphs and data frames. They also encompass more\nelaborate graph-like objects such as wiring diagrams and Petri nets with rate\nconstants. We develop the mathematical theory of acsets and then describe a\ngeneric implementation in the Julia programming language, which uses advanced\nlanguage features to achieve performance comparable with specialized data\nstructures.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 21:44:43 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Patterson", "Evan", ""], ["Lynch", "Owen", ""], ["Fairbanks", "James", ""]]}, {"id": "2106.04727", "submitter": "Shangdi Yu", "authors": "Shangdi Yu, Yiqiu Wang, Yan Gu, Laxman Dhulipala, Julian Shun", "title": "ParChain: A Framework for Parallel Hierarchical Agglomerative Clustering\n  using Nearest-Neighbor Chain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the hierarchical clustering problem, where the goal is to\nproduce a dendrogram that represents clusters at varying scales of a data set.\nWe propose the ParChain framework for designing parallel hierarchical\nagglomerative clustering (HAC) algorithms, and using the framework we obtain\nnovel parallel algorithms for the complete linkage, average linkage, and Ward's\nlinkage criteria. Compared to most previous parallel HAC algorithms, which\nrequire quadratic memory, our new algorithms require only linear memory, and\nare scalable to large data sets. ParChain is based on our parallelization of\nthe nearest-neighbor chain algorithm, and enables multiple clusters to be\nmerged on every round. We introduce two key optimizations that are critical for\nefficiency: a range query optimization that reduces the number of distance\ncomputations required when finding nearest neighbors of clusters, and a caching\noptimization that stores a subset of previously computed distances, which are\nlikely to be reused.\n  Experimentally, we show that our highly-optimized implementations using 48\ncores with two-way hyper-threading achieve 5.8--110.1x speedup over\nstate-of-the-art parallel HAC algorithms and achieve 13.75--54.23x\nself-relative speedup. Compared to state-of-the-art algorithms, our algorithms\nrequire up to 237.3x less space. Our algorithms are able to scale to data set\nsizes with tens of millions of points, which existing algorithms are not able\nto handle.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 23:13:27 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Yu", "Shangdi", ""], ["Wang", "Yiqiu", ""], ["Gu", "Yan", ""], ["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""]]}, {"id": "2106.04771", "submitter": "Henrique Santos", "authors": "Henrique Santos, James P. McCusker, Deborah L. McGuinness", "title": "Geospatial Reasoning with Shapefiles for Supporting Policy Decisions", "comments": "4th International Workshop on Geospatial Linked Data (GeoLD 2021) at\n  ESWC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Policies are authoritative assets that are present in multiple domains to\nsupport decision-making. They describe what actions are allowed or recommended\nwhen domain entities and their attributes satisfy certain criteria. It is\ncommon to find policies that contain geographical rules, including distance and\ncontainment relationships among named locations. These locations' polygons can\noften be found encoded in geospatial datasets. We present an approach to\ntransform data from geospatial datasets into Linked Data using the OWL, PROV-O,\nand GeoSPARQL standards, and to leverage this representation to support\nautomated ontology-based policy decisions. We applied our approach to\nlocation-sensitive radio spectrum policies to identify relationships between\nradio transmitters coordinates and policy-regulated regions in Census.gov\ndatasets. Using a policy evaluation pipeline that mixes OWL reasoning and\nGeoSPARQL, our approach implements the relevant geospatial relationships,\naccording to a set of requirements elicited by radio spectrum domain experts.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 02:19:01 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Santos", "Henrique", ""], ["McCusker", "James P.", ""], ["McGuinness", "Deborah L.", ""]]}, {"id": "2106.04811", "submitter": "Jem Guhit", "authors": "Jem Guhit, Edward Colone, Shawn McKee, Kris Steinhoff, and Katarina\n  Thomas", "title": "Benchmarking NetBASILISK: a Network Security Project for Science", "comments": "12 pages, 4 figures, presented at vCHEP '21 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infrastructures supporting distributed scientific collaborations must address\ncompeting goals in both providing high-performance access to resources while\nsimultaneously securing the infrastructure against security threats. The\nNetBASILISK project is attempting to improve the security of such\ninfrastructures while not adversely impacting their performance. This paper\nwill present our work to create a benchmark and monitoring infrastructure that\nallows us to test for any degradation in transferring data into a NetBASILISK\nprotected site.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 05:08:26 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Guhit", "Jem", ""], ["Colone", "Edward", ""], ["McKee", "Shawn", ""], ["Steinhoff", "Kris", ""], ["Thomas", "Katarina", ""]]}, {"id": "2106.05357", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Kunal Samant, Endrit Memeti, Enamul Karim and Sharma\n  Chakravarthy", "title": "An Extensible Dashboard Architecture For Visualizing Base And Analyzed\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Any data analysis, especially the data sets that may be changing often or in\nreal-time, consists of at least three important synchronized components: i)\nfiguring out what to infer (objectives), ii) analysis or computation of\nobjectives, and iii) understanding of the results which may require drill-down\nand/or visualization. There is a lot of attention paid to the first two of the\nabove components as part of research whereas the understanding as well as\nderiving actionable decisions is quite tricky. Visualization is an important\nstep towards both understanding (even by non-experts) and inferring the actions\nthat need to be taken. As an example, for Covid-19, knowing regions (say, at\nthe county or state level) that have seen a spike or prone to a spike in cases\nin the near future may warrant additional actions with respect to gatherings,\nbusiness opening hours, etc. This paper focuses on an extensible architecture\nfor visualization of base as well as analyzed data. This paper proposes a\nmodular architecture of a dashboard for user-interaction, visualization\nmanagement, and complex analysis of base data. The contributions of this paper\nare: i) extensibility of the architecture providing flexibility to add\nadditional analysis, visualizations, and user interactions without changing the\nworkflow, ii) decoupling of the functional modules to ease and speedup\ndevelopment by different groups, and iii) address efficiency issues for display\nresponse time. This paper uses Multilayer Networks (or MLNs) for analysis. To\nshowcase the above, we present the implementation of a visualization dashboard,\ntermed CoWiz++ (for Covid Wizard), and elaborate on how web-based user\ninteraction and display components are interfaced seamlessly with the back end\nmodules.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 19:45:43 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Santra", "Abhishek", ""], ["Samant", "Kunal", ""], ["Memeti", "Endrit", ""], ["Karim", "Enamul", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "2106.05620", "submitter": "Moonyoung Chung", "authors": "Moonyoung Chung, Soon J. Hyun, and Woong-Kee Loh", "title": "Efficient Exact k-Flexible Aggregate Nearest Neighbor Search in Road\n  Networks Using the M-tree", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes an efficient exact k-flexible aggregate nearest neighbor\n(k-FANN) search algorithm in road networks using the M-tree. The IER-kNN\nalgorithm, which previously showed the highest FANN search performance, used\nthe R-tree and pruned off unnecessary nodes based on the Euclidean coordinates\nof objects in road networks. However, IER-kNN made many unnecessary accesses to\nindex nodes since the Euclidean distance between objects is much different from\nthe actual shortest-path distance between them. In contrast, our algorithm\nproposed in this study can greatly reduce unnecessary accesses to index nodes\ncompared to IER-kNN since the M-tree is constructed based on the actual\nshortest-path distances between objects. To the best of our knowledge, our\nalgorithm is the first exact FANN algorithm using the M-tree. We prove that our\nalgorithm does not cause any false drop. As a result of a series of experiments\nusing various real road network datasets, our algorithm always showed a better\nperformance than IER-kNN and was improved by up to 6.92 times.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 09:56:42 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chung", "Moonyoung", ""], ["Hyun", "Soon J.", ""], ["Loh", "Woong-Kee", ""]]}, {"id": "2106.06739", "submitter": "L Siddharth", "authors": "L Siddharth, Lucienne T.M. Blessing, Kristin L. Wood, Jianxi Luo", "title": "Engineering Knowledge Graph from Patent Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a large, scalable engineering knowledge graph, comprising sets of\n(entity, relationship, entity) triples that are real-world engineering facts\nfound in the patent database. We apply a set of rules based on the syntactic\nand lexical properties of claims in a patent document to extract facts. We\naggregate these facts within each patent document and integrate the aggregated\nsets of facts across the patent database to obtain the engineering knowledge\ngraph. Such a knowledge graph is expected to support inference, reasoning, and\nrecalling in various engineering tasks. The knowledge graph has a greater size\nand coverage in comparison with the previously used knowledge graphs and\nsemantic networks in the engineering literature.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 10:54:31 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Siddharth", "L", ""], ["Blessing", "Lucienne T. M.", ""], ["Wood", "Kristin L.", ""], ["Luo", "Jianxi", ""]]}, {"id": "2106.06889", "submitter": "Feng Zhang", "authors": "Feng Zhang, Zaifeng Pan, Yanliang Zhou, Jidong Zhai, Xipeng Shen, Onur\n  Mutlu, Xiaoyong Du", "title": "G-TADOC: Enabling Efficient GPU-Based Text Analytics without\n  Decompression", "comments": "37th IEEE International Conference on Data Engineering (ICDE 2021)", "journal-ref": null, "doi": "10.1109/ICDE51399.2021.00148", "report-no": null, "categories": "cs.DB cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text analytics directly on compression (TADOC) has proven to be a promising\ntechnology for big data analytics. GPUs are extremely popular accelerators for\ndata analytics systems. Unfortunately, no work so far shows how to utilize GPUs\nto accelerate TADOC. We describe G-TADOC, the first framework that provides\nGPU-based text analytics directly on compression, effectively enabling\nefficient text analytics on GPUs without decompressing the input data. G-TADOC\nsolves three major challenges. First, TADOC involves a large amount of\ndependencies, which makes it difficult to exploit massive parallelism on a GPU.\nWe develop a novel fine-grained thread-level workload scheduling strategy for\nGPU threads, which partitions heavily-dependent loads adaptively in a\nfine-grained manner. Second, in developing G-TADOC, thousands of GPU threads\nwriting to the same result buffer leads to inconsistency while directly using\nlocks and atomic operations lead to large synchronization overheads. We develop\na memory pool with thread-safe data structures on GPUs to handle such\ndifficulties. Third, maintaining the sequence information among words is\nessential for lossless compression. We design a sequence-support strategy,\nwhich maintains high GPU parallelism while ensuring sequence information. Our\nexperimental evaluations show that G-TADOC provides 31.1x average speedup\ncompared to state-of-the-art TADOC.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 00:50:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhang", "Feng", ""], ["Pan", "Zaifeng", ""], ["Zhou", "Yanliang", ""], ["Zhai", "Jidong", ""], ["Shen", "Xipeng", ""], ["Mutlu", "Onur", ""], ["Du", "Xiaoyong", ""]]}, {"id": "2106.07037", "submitter": "Meng Li", "authors": "Rongbiao Xie, Meng Li, Zheyu Miao, Rong Gu, He Huang, Haipeng Dai,\n  Guihai Chen", "title": "Hash Adaptive Bloom Filter", "comments": "11 pages, accepted by ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bloom filter is a compact memory-efficient probabilistic data structure\nsupporting membership testing, i.e., to check whether an element is in a given\nset. However, as Bloom filter maps each element with uniformly random hash\nfunctions, few flexibilities are provided even if the information of negative\nkeys (elements are not in the set) are available. The problem gets worse when\nthe misidentification of negative keys brings different costs. To address the\nabove problems, we propose a new Hash Adaptive Bloom Filter (HABF) that\nsupports the customization of hash functions for keys. The key idea of HABF is\nto customize the hash functions for positive keys (elements are in the set) to\navoid negative keys with high cost, and pack customized hash functions into a\nlightweight data structure named HashExpressor. Then, given an element at query\ntime, HABF follows a two-round pattern to check whether the element is in the\nset. Further, we theoretically analyze the performance of HABF and bound the\nexpected false positive rate. We conduct extensive experiments on\nrepresentative datasets, and the results show that HABF outperforms the\nstandard Bloom filter and its cutting-edge variants on the whole in terms of\naccuracy, construction time, query time, and memory space consumption (Note\nthat source codes are available in [1]).\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 16:33:41 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xie", "Rongbiao", ""], ["Li", "Meng", ""], ["Miao", "Zheyu", ""], ["Gu", "Rong", ""], ["Huang", "He", ""], ["Dai", "Haipeng", ""], ["Chen", "Guihai", ""]]}, {"id": "2106.07102", "submitter": "Dario Korolija", "authors": "Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Konstantin\n  Taranov, Dejan Miloji\\v{c}i\\'c, Gustavo Alonso", "title": "Farview: Disaggregated Memory with Operator Off-loading for Database\n  Engines", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud deployments disaggregate storage from compute, providing more\nflexibility to both the storage and compute layers. In this paper, we explore\ndisaggregation by taking it one step further and applying it to memory (DRAM).\nDisaggregated memory uses network attached DRAM as a way to decouple memory\nfrom CPU. In the context of databases, such a design offers significant\nadvantages in terms of making a larger memory capacity available as a central\npool to a collection of smaller processing nodes. To explore these\npossibilities, we have implemented Farview, a disaggregated memory solution for\ndatabases, operating as a remote buffer cache with operator offloading\ncapabilities. Farview is implemented as an FPGA-based smart NIC making DRAM\navailable as a disaggregated, network attached memory module capable of\nperforming data processing at line rate over data streams to/from disaggregated\nmemory. Farview supports query offloading using operators such as selection,\nprojection, aggregation, regular expression matching and encryption. In this\npaper we focus on analytical queries and demonstrate the viability of the idea\nthrough an extensive experimental evaluation of Farview under different\nworkloads. Farview is competitive with a local buffer cache solution for all\nthe workloads and outperforms it in a number of cases, proving that a smart\ndisaggregated memory can be a viable alternative for databases deployed in\ncloud environments.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 22:35:38 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Korolija", "Dario", ""], ["Koutsoukos", "Dimitrios", ""], ["Keeton", "Kimberly", ""], ["Taranov", "Konstantin", ""], ["Miloji\u010di\u0107", "Dejan", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2106.07258", "submitter": "Madelon Hulsebos", "authors": "Madelon Hulsebos, \\c{C}a\\u{g}atay Demiralp, Paul Groth", "title": "GitTables: A Large-Scale Corpus of Relational Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The practical success of deep learning has sparked interest in improving\nrelational table tasks, like data search, with models trained on large table\ncorpora. Existing corpora primarily contain tables extracted from HTML pages,\nlimiting the capability to represent offline database tables. To train and\nevaluate high-capacity models for applications beyond the Web, we need\nadditional resources with tables that resemble relational database tables.\n  Here we introduce GitTables, a corpus of currently 1.7M relational tables\nextracted from GitHub. Our continuing curation aims at growing the corpus to at\nleast 20M tables. We annotate table columns in GitTables with more than 2K\ndifferent semantic types from Schema.org and DBpedia. Our column annotations\nconsist of semantic types, hierarchical relations, range types and\ndescriptions.\n  The corpus is available at https://gittables.github.io. Our analysis of\nGitTables shows that its structure, content, and topical coverage differ\nsignificantly from existing table corpora. We evaluate our annotation pipeline\non hand-labeled tables from the T2Dv2 benchmark and find that our approach\nprovides results on par with human annotations. We demonstrate a use case of\nGitTables by training a semantic type detection model on it and obtain high\nprediction accuracy. We also show that the same model trained on tables from\ntheWeb generalizes poorly.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 09:22:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hulsebos", "Madelon", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Groth", "Paul", ""]]}, {"id": "2106.07781", "submitter": "Giacomo Bergami", "authors": "Giacomo Bergami", "title": "On Declare MAX-SAT and a finite Herbrand Base for data-aware logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This technical report provides some lightweight introduction motivating the\ndefinition of an alignment of log traces against Data-Aware Declare Models\npotentially containing correlation conditions. This technical report is only\nproviding the intuition of the logical framework as a feasibility study for a\nfuture formalization and experiment section.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 22:26:22 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Bergami", "Giacomo", ""]]}, {"id": "2106.07837", "submitter": "Suman Banerjee", "authors": "Suman Banerjee", "title": "A Survey on Mining and Analysis of Uncertain Graphs", "comments": "46 Pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  \\emph{Uncertain Graph} (also known as \\emph{Probabilistic Graph}) is a\ngeneric model to represent many real\\mbox{-}world networks from social to\nbiological. In recent times analysis and mining of uncertain graphs have drawn\nsignificant attention from the researchers of the data management community.\nSeveral noble problems have been introduced and efficient methodologies have\nbeen developed to solve those problems. Hence, there is a need to summarize the\nexisting results on this topic in a self\\mbox{-}organized way. In this paper,\nwe present a comprehensive survey on uncertain graph mining focusing on mainly\nthree aspects: (i) different problems studied, (ii) computational challenges\nfor solving those problems, and (iii) proposed methodologies. Finally, we list\nout important future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 02:06:34 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Banerjee", "Suman", ""]]}, {"id": "2106.08166", "submitter": "Dimitrios Alivanistos", "authors": "Dimitrios Alivanistos and Max Berrendorf and Michael Cochez and\n  Mikhail Galkin", "title": "Query Embedding on Hyper-relational Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop logical reasoning is an established problem in the field of\nrepresentation learning on knowledge graphs (KGs). It subsumes both one-hop\nlink prediction as well as other more complex types of logical queries.\nExisting algorithms operate only on classical, triple-based graphs, whereas\nmodern KGs often employ a hyper-relational modeling paradigm. In this paradigm,\ntyped edges may have several key-value pairs known as qualifiers that provide\nfine-grained context for facts. In queries, this context modifies the meaning\nof relations, and usually reduces the answer set. Hyper-relational queries are\noften observed in real-world KG applications, and existing approaches for\napproximate query answering cannot make use of qualifier pairs. In this work,\nwe bridge this gap and extend the multi-hop reasoning problem to\nhyper-relational KGs allowing to tackle this new type of complex queries.\nBuilding upon recent advancements in Graph Neural Networks and query embedding\ntechniques, we study how to embed and answer hyper-relational conjunctive\nqueries. Besides that, we propose a method to answer such queries and\ndemonstrate in our experiments that qualifiers improve query answering on a\ndiverse set of query patterns.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:08:50 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 13:53:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Alivanistos", "Dimitrios", ""], ["Berrendorf", "Max", ""], ["Cochez", "Michael", ""], ["Galkin", "Mikhail", ""]]}, {"id": "2106.08455", "submitter": "Yuliang Li", "authors": "Jin Wang, Yuliang Li, Wataru Hirota", "title": "Machamp: A Generalized Entity Matching Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity Matching (EM) refers to the problem of determining whether two\ndifferent data representations refer to the same real-world entity. It has been\na long-standing interest of the data management community and many efforts have\nbeen paid in creating benchmark tasks as well as in developing advanced\nmatching techniques. However, existing benchmark tasks for EM are limited to\nthe case where the two data collections of entities are structured tables with\nthe same schema. Meanwhile, the data collections for matching could be\nstructured, semi-structured, or unstructured in real-world scenarios of data\nscience. In this paper, we come up with a new research problem -- Generalized\nEntity Matching to satisfy this requirement and create a benchmark Machamp for\nit. Machamp consists of seven tasks having diverse characteristics and thus\nprovides good coverage of use cases in real applications. We summarize existing\nEM benchmark tasks for structured tables and conduct a series of processing and\ncleaning efforts to transform them into matching tasks between tables with\ndifferent structures. Based on that, we further conduct comprehensive profiling\nof the proposed benchmark tasks and evaluate popular entity matching approaches\non them. With the help of Machamp, it is the first time that researchers can\nevaluate EM techniques between data collections with different structures.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:02:59 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Jin", ""], ["Li", "Yuliang", ""], ["Hirota", "Wataru", ""]]}, {"id": "2106.09590", "submitter": "Lisa Wenige", "authors": "Lisa Wenige, Claus Stadler, Michael Martin, Richard Figura, Robert\n  Sauter, Christopher W. Frank", "title": "Open Data and the Status Quo -- A Fine-Grained Evaluation Framework for\n  Open Data Quality and an Analysis of Open Data portals in Germany", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a framework for assessing data and metadata quality\nwithin Open Data portals. Although a few benchmark frameworks already exist for\nthis purpose, they are not yet detailed enough in both breadth and depth to\nmake valid statements about the actual discoverability and accessibility of\npublicly available data collections. To address this research gap, we have\ndesigned a quality framework that is able to evaluate data quality in Open Data\nportals on dedicated and fine-grained dimensions, such as interoperability,\nfindability, uniqueness or completeness. Additionally, we propose quality\nmeasures that allow for valid assessments regarding cross-portal findability\nand uniqueness of dataset descriptions. We have validated our novel quality\nframework for the German Open Data landscape and found out that metadata often\nstill lacks meaningful descriptions and is not yet extensively connected to the\nSemantic Web.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:16:36 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wenige", "Lisa", ""], ["Stadler", "Claus", ""], ["Martin", "Michael", ""], ["Figura", "Richard", ""], ["Sauter", "Robert", ""], ["Frank", "Christopher W.", ""]]}, {"id": "2106.09592", "submitter": "Rihan Hai", "authors": "Rihan Hai, Christoph Quix, Matthias Jarke", "title": "Data lake concept and systems: a survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although big data has been discussed for some years, it still has many\nresearch challenges, especially the variety of data. It poses a huge difficulty\nto efficiently integrate, access, and query the large volume of diverse data in\ninformation silos with the traditional 'schema-on-write' approaches such as\ndata warehouses. Data lakes have been proposed as a solution to this problem.\nThey are repositories storing raw data in its original formats and providing a\ncommon access interface. This survey reviews the development, definition, and\narchitectures of data lakes. We provide a comprehensive overview of research\nquestions for designing and building data lakes. We classify the existing data\nlake systems based on their provided functions, which makes this survey a\nuseful technical reference for designing, implementing and applying data lakes.\nWe hope that the thorough comparison of existing solutions and the discussion\nof open research challenges in this survey would motivate the future\ndevelopment of data lake research and practice.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:18:23 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Hai", "Rihan", ""], ["Quix", "Christoph", ""], ["Jarke", "Matthias", ""]]}, {"id": "2106.09764", "submitter": "F.P.J. Nijweide", "authors": "R.R. Mauritz, F.P.J. Nijweide, J. Goseling, M. van Keulen", "title": "Autoencoder-based cleaning in probabilistic databases", "comments": "Submitted to ACM Journal of Data and Information Quality, Special\n  Issue on Deep Learning for Data Quality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of data integration, data quality problems are often encountered\nwhen extracting, combining, and merging data. The probabilistic data\nintegration approach represents information about such problems as\nuncertainties in a probabilistic database. In this paper, we propose a\ndata-cleaning autoencoder capable of near-automatic data quality improvement.\nIt learns the structure and dependencies in the data to identify and correct\ndoubtful values. A theoretical framework is provided, and experiments show that\nit can remove significant amounts of noise from categorical and numeric\nprobabilistic data. Our method does not require clean data. We do, however,\nshow that manually cleaning a small fraction of the data significantly improves\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:46:56 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Mauritz", "R. R.", ""], ["Nijweide", "F. P. J.", ""], ["Goseling", "J.", ""], ["van Keulen", "M.", ""]]}, {"id": "2106.09799", "submitter": "Jesse Weaver", "authors": "Jesse Weaver, Eric Paniagua, Tushar Agarwal, Nicholas Guy and\n  Alexandre Mattos", "title": "Introducing PathQuery, Google's Graph Query Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce PathQuery, a graph query language developed to scale with\nGoogle's query and data volumes as well as its internal developer community.\nPathQuery supports flexible and declarative semantics. We have found that this\nenables query developers to think in a naturally \"graphy\" design space and to\navoid the additional cognitive effort of coordinating numerous joins and\nsubqueries often required to express an equivalent query in a relational space.\nDespite its traversal-oriented syntactic style, PathQuery has a foundation on a\ncustom variant of relational algebra -- the exposition of which we presently\ndefer -- allowing for the application of both common and novel optimizations.\nWe believe that PathQuery has withstood a \"test of time\" at Google, under both\nlarge scale and low latency requirements. We thus share herein a language\ndesign that admits a rigorous declarative semantics, has scaled well in\npractice, and provides a natural syntax for graph traversals while also\nadmitting complex graph patterns.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 20:27:03 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Weaver", "Jesse", ""], ["Paniagua", "Eric", ""], ["Agarwal", "Tushar", ""], ["Guy", "Nicholas", ""], ["Mattos", "Alexandre", ""]]}, {"id": "2106.10515", "submitter": "Qiang Huang", "authors": "Pingyi Luo, Qiang Huang, Anthony K. H. Tung", "title": "A Generic Distributed Clustering Framework for Massive Data", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a novel Generic distributEd clustEring frameworK\n(GEEK) beyond $k$-means clustering to process massive amounts of data. To deal\nwith different data types, GEEK first converts data in the original feature\nspace into a unified format of buckets; then, we design a new Seeding method\nbased on simILar bucKets (SILK) to determine initial seeds. Compared with\nstate-of-the-art seeding methods such as $k$-means++ and its variants, SILK can\nautomatically identify the number of initial seeds based on the closeness of\nshared data objects in similar buckets instead of pre-specifying $k$. Thus, its\ntime complexity is independent of $k$. With these well-selected initial seeds,\nGEEK only needs a one-pass data assignment to get the final clusters. We\nimplement GEEK on a distributed CPU-GPU platform for large-scale clustering. We\nevaluate the performance of GEEK over five large-scale real-life datasets and\nshow that GEEK can deal with massive data of different types and is comparable\nto (or even better than) many state-of-the-art customized GPU-based methods,\nespecially in large $k$ values.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 15:20:21 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Luo", "Pingyi", ""], ["Huang", "Qiang", ""], ["Tung", "Anthony K. H.", ""]]}, {"id": "2106.10562", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi", "title": "Score-Based Explanations in Data Management and Machine Learning: An\n  Answer-Set Programming Approach to Counterfactual Analysis", "comments": "Paper associated to forthcoming short course at Fall School. arXiv\n  admin note: text overlap with arXiv:2007.12799", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe some recent approaches to score-based explanations for query\nanswers in databases and outcomes from classification models in machine\nlearning. The focus is on work done by the author and collaborators. Special\nemphasis is placed on declarative approaches based on answer-set programming to\nthe use of counterfactual reasoning for score specification and computation.\nSeveral examples that illustrate the flexibility of these methods are shown.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 19:21:48 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Bertossi", "Leopoldo", ""]]}, {"id": "2106.10821", "submitter": "Renzhi Wu", "authors": "Renzhi Wu, Prem Sakala, Peng Li, Xu Chu, Yeye He", "title": "Demonstration of Panda: A Weakly Supervised Entity Matching System", "comments": "video can be found at\n  https://chu-data-lab.cc.gatech.edu/ml-for-data-integration/", "journal-ref": "vldb 2021 demo", "doi": "10.14778/3476311.3476332", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity matching (EM) refers to the problem of identifying tuple pairs in one\nor more relations that refer to the same real world entities. Supervised\nmachine learning (ML) approaches, and deep learning based approaches in\nparticular, typically achieve state-of-the-art matching results. However, these\napproaches require many labeled examples, in the form of matching and\nnon-matching pairs, which are expensive and time-consuming to label. In this\npaper, we introduce Panda, a weakly supervised system specifically designed for\nEM. Panda uses the same labeling function abstraction as Snorkel, where\nlabeling functions (LF) are user-provided programs that can generate large\namounts of (somewhat noisy) labels quickly and cheaply, which can then be\ncombined via a labeling model to generate accurate final predictions. To\nsupport users developing LFs for EM, Panda provides an integrated development\nenvironment (IDE) that lives in a modern browser architecture. Panda's IDE\nfacilitates the development, debugging, and life-cycle management of LFs in the\ncontext of EM tasks, similar to how IDEs such as Visual Studio or Eclipse excel\nin general-purpose programming. Panda's IDE includes many novel features\npurpose-built for EM, such as smart data sampling, a builtin library of EM\nutility functions, automatically generated LFs, visual debugging of LFs, and\nfinally, an EM-specific labeling model. We show in this demo that Panda IDE can\ngreatly accelerate the development of high-quality EM solutions using weak\nsupervision.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 03:08:48 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 15:42:38 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wu", "Renzhi", ""], ["Sakala", "Prem", ""], ["Li", "Peng", ""], ["Chu", "Xu", ""], ["He", "Yeye", ""]]}, {"id": "2106.11445", "submitter": "Yiwen Zhu", "authors": "Yiwen Zhu, Subru Krishnan, Konstantinos Karanasos, Isha Tarte, Conor\n  Power, Abhishek Modi, Manoj Kumar, Deli Zhang, Kartheek Muthyala, Nick\n  Jurgens, Sarvesh Sakalanaga, Sudhir Darbha, Minu Iyer, Ankita Agarwal, Carlo\n  Curino", "title": "KEA: Tuning an Exabyte-Scale Data Infrastructure", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3457569", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Microsoft's internal big-data infrastructure is one of the largest in the\nworld -- with over 300k machines running billions of tasks from over 0.6M daily\njobs. Operating this infrastructure is a costly and complex endeavor, and\nefficiency is paramount. In fact, for over 15 years, a dedicated engineering\nteam has tuned almost every aspect of this infrastructure, achieving\nstate-of-the-art efficiency (>60% average CPU utilization across all clusters).\nDespite rich telemetry and strong expertise, faced with evolving\nhardware/software/workloads this manual tuning approach had reached its limit\n-- we had plateaued.\n  In this paper, we present KEA, a multi-year effort to automate our tuning\nprocesses to be fully data/model-driven. KEA leverages a mix of domain\nknowledge and principled data science to capture the essence of our cluster\ndynamic behavior in a set of machine learning (ML) models based on collected\nsystem data. These models power automated optimization procedures for parameter\ntuning, and inform our leadership in critical decisions around engineering and\ncapacity management (such as hardware and data center design, software\ninvestments, etc.). We combine \"observational\" tuning (i.e., using models to\npredict system behavior without direct experimentation) with judicious use of\n\"flighting\" (i.e., conservative testing in production). This allows us to\nsupport a broad range of applications that we discuss in this paper.\n  KEA continuously tunes our cluster configurations and is on track to save\nMicrosoft tens of millions of dollars per year. At the best of our knowledge,\nthis paper is the first to discuss research challenges and practical learnings\nthat emerge when tuning an exabyte-scale data infrastructure.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 23:08:40 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zhu", "Yiwen", ""], ["Krishnan", "Subru", ""], ["Karanasos", "Konstantinos", ""], ["Tarte", "Isha", ""], ["Power", "Conor", ""], ["Modi", "Abhishek", ""], ["Kumar", "Manoj", ""], ["Zhang", "Deli", ""], ["Muthyala", "Kartheek", ""], ["Jurgens", "Nick", ""], ["Sakalanaga", "Sarvesh", ""], ["Darbha", "Sudhir", ""], ["Iyer", "Minu", ""], ["Agarwal", "Ankita", ""], ["Curino", "Carlo", ""]]}, {"id": "2106.11455", "submitter": "Chia-Hsuan Lee", "authors": "Chia-Hsuan Lee, Oleksandr Polozov, Matthew Richardson", "title": "KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers", "comments": "Published as a conference paper at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of database question answering is to enable natural language\nquerying of real-life relational databases in diverse application domains.\nRecently, large-scale datasets such as Spider and WikiSQL facilitated novel\nmodeling techniques for text-to-SQL parsing, improving zero-shot generalization\nto unseen databases. In this work, we examine the challenges that still prevent\nthese techniques from practical deployment. First, we present KaggleDBQA, a new\ncross-domain evaluation dataset of real Web databases, with domain-specific\ndata types, original formatting, and unrestricted questions. Second, we\nre-examine the choice of evaluation tasks for text-to-SQL parsers as applied in\nreal-life settings. Finally, we augment our in-domain evaluation task with\ndatabase documentation, a naturally occurring source of implicit domain\nknowledge. We show that KaggleDBQA presents a challenge to state-of-the-art\nzero-shot parsers but a more realistic evaluation setting and creative use of\nassociated database documentation boosts their accuracy by over 13.2%, doubling\ntheir performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 00:08:03 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Lee", "Chia-Hsuan", ""], ["Polozov", "Oleksandr", ""], ["Richardson", "Matthew", ""]]}, {"id": "2106.11456", "submitter": "Marcelo Arenas", "authors": "Marcelo Arenas and Claudio Gutierrez and Juan F. Sequeda", "title": "Querying in the Age of Graph Databases and Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs have become the best way we know of representing knowledge. The\ncomputing community has investigated and developed the support for managing\ngraphs by means of digital technology. Graph databases and knowledge graphs\nsurface as the most successful solutions to this program. The goal of this\ndocument is to provide a conceptual map of the data management tasks underlying\nthese developments, paying particular attention to data models and query\nlanguages for graphs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 00:17:06 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 03:18:36 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Arenas", "Marcelo", ""], ["Gutierrez", "Claudio", ""], ["Sequeda", "Juan F.", ""]]}, {"id": "2106.12118", "submitter": "Ryan McKenna", "authors": "Ryan McKenna, Gerome Miklau, Michael Hay, Ashwin Machanavajjhala", "title": "HDMM: Optimizing error of high-dimensional statistical queries under\n  differential privacy", "comments": "arXiv admin note: text overlap with arXiv:1808.03537", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we describe the High-Dimensional Matrix Mechanism (HDMM), a\ndifferentially private algorithm for answering a workload of predicate counting\nqueries. HDMM represents query workloads using a compact implicit matrix\nrepresentation and exploits this representation to efficiently optimize over (a\nsubset of) the space of differentially private algorithms for one that is\nunbiased and answers the input query workload with low expected error. HDMM can\nbe deployed for both $\\epsilon$-differential privacy (with Laplace noise) and\n$(\\epsilon, \\delta)$-differential privacy (with Gaussian noise), although the\ncore techniques are slightly different for each. We demonstrate empirically\nthat HDMM can efficiently answer queries with lower expected error than\nstate-of-the-art techniques, and in some cases, it nearly matches existing\nlower bounds for the particular class of mechanisms we consider.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 01:19:18 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["McKenna", "Ryan", ""], ["Miklau", "Gerome", ""], ["Hay", "Michael", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "2106.12189", "submitter": "Anes Abdennebi", "authors": "Anes Abdennebi and Kamer Kaya", "title": "A Bloom Filter Survey: Variants for Different Domain Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There is a plethora of data structures, algorithms, and frameworks dealing\nwith major data-stream problems like estimating the frequency of items,\nanswering set membership, association and multiplicity queries, and several\nother statistics that can be extracted from voluminous data streams. In this\nsurvey, we are focusing on exploring randomized data structures called Bloom\nFilters. This data structure answers whether an item exists or not in a data\nstream with a false positive probability fpp. In this survey, many variants of\nthe Bloom filter will be covered by showing the strengths of each structure and\nits drawbacks i.e. some Bloom filters deal with insertion and deletions and\nothers don't, some variants use the memory efficiently but increase the fpp\nwhere others pay the trade-off in the reversed way. Furthermore, in each Bloom\nfilter structure, the false positive probability will be highlighted alongside\nthe most important technical details showing the improvement it is presenting,\nwhile the main aim of this work is to provide an overall comparison between the\nvariants of the Bloom filter structure according to the application domain that\nit fits in.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 06:30:00 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Abdennebi", "Anes", ""], ["Kaya", "Kamer", ""]]}, {"id": "2106.12240", "submitter": "Frederic Prost", "authors": "Dominique Duval (LJK), Rachid Echahed (LIG), Fr\\'ed\\'eric Prost (LIG)", "title": "Querying RDF Databases with Sub-CONSTRUCTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph query languages feature mainly two kinds of queries when applied to a\ngraph database: those inspired by relational databases which return tables such\nas SELECT queries and those which return graphs such as CONSTRUCT queries in\nSPARQL. The latter are object of study in the present paper. For this purpose,\na core graph query language GrAL is defined with focus on CONSTRUCT queries.\nQueries in GrAL form the final step of a recursive process involving so-called\nGrAL patterns. By evaluating a query over a graph one gets a graph, while by\nevaluating a pattern over a graph one gets a set of matches which involves both\na graph and a table. CONSTRUCT queries are based on CONSTRUCT patterns, and\nsub-CONSTRUCT patterns come for free from the recursive definition of patterns.\nThe semantics of GrAL is based on RDF graphs with a slight modification which\nconsists in accepting isolated nodes. Such an extension of RDF graphs eases the\ndefinition of the evaluation semantics, which is mainly captured by a unique\noperation called Merge. Besides, we define aggregations as part of GrAL\nexpressions, which leads to an original local processing of aggregations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 08:56:16 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Duval", "Dominique", "", "LJK"], ["Echahed", "Rachid", "", "LIG"], ["Prost", "Fr\u00e9d\u00e9ric", "", "LIG"]]}, {"id": "2106.12429", "submitter": "Manisha Luthra", "authors": "Manisha Luthra, Boris Koldehofe, Niels Danger, Pascal Weisenburger,\n  Guido Salvaneschi, Ioannis Stavrakakis", "title": "TCEP: Transitions in Operator Placement to Adapt to Dynamic Network\n  Environments", "comments": "Accepted for publication in Journal of Computer and System Sciences,\n  Special Issue on Algorithmic Theory of Dynamic Networks and its Application", "journal-ref": "Journal of Computer and System Sciences, May 2021", "doi": "10.1016/j.jcss.2021.05.003", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Distributed Complex Event Processing (DCEP) is a commonly used paradigm to\ndetect and act on situational changes of many applications, including the\nInternet of Things (IoT). DCEP achieves this using a simple specification of\nanalytical tasks on data streams called operators and their distributed\nexecution on a set of infrastructure. The adaptivity of DCEP to the dynamics of\nIoT applications is essential and very challenging in the face of changing\ndemands concerning Quality of Service. In our previous work, we addressed this\nissue by enabling transitions, which allow for the adaptive use of multiple\noperator placement mechanisms. In this article, we extend the transition\nmethodology by optimizing the costs of transition and analyzing the behaviour\nusing multiple operator placement mechanisms. Furthermore, we provide an\nextensive evaluation on the costs of transition imposed by operator migrations\nand learning, as it can inflict overhead on the performance if operated\nuncoordinatedly.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 14:31:39 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Luthra", "Manisha", ""], ["Koldehofe", "Boris", ""], ["Danger", "Niels", ""], ["Weisenburger", "Pascal", ""], ["Salvaneschi", "Guido", ""], ["Stavrakakis", "Ioannis", ""]]}, {"id": "2106.12505", "submitter": "Sam Kumar", "authors": "Sam Kumar, Michael P Andersen, David E. Culler", "title": "Mr. Plotter: Unifying Data Reduction Techniques in Storage and\n  Visualization Systems", "comments": "14 pages; Originally published in May 2018 as a technical report in\n  the UC Berkeley EECS Technical Report Series (see\n  https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-85.html)", "journal-ref": null, "doi": null, "report-no": "Technical Report No. UCB/EECS-2018-85", "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the rate of data collection continues to grow rapidly, developing\nvisualization tools that scale to immense data sets is a serious and\never-increasing challenge. Existing approaches generally seek to decouple\nstorage and visualization systems, performing just-in-time data reduction to\ntransparently avoid overloading the visualizer. We present a new architecture\nin which the visualizer and data store are tightly coupled. Unlike systems that\nread raw data from storage, the performance of our system scales linearly with\nthe size of the final visualization, essentially independent of the size of the\ndata. Thus, it scales to massive data sets while supporting interactive\nperformance (sub-100 ms query latency). This enables a new class of\nvisualization clients that automatically manage data, quickly and transparently\nrequesting data from the underlying database without requiring the user to\nexplicitly initiate queries. It lays a groundwork for supporting truly\ninteractive exploration of big data and opens new directions for research on\nscalable information visualization systems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:23:18 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kumar", "Sam", ""], ["Andersen", "Michael P", ""], ["Culler", "David E.", ""]]}, {"id": "2106.12765", "submitter": "Yang Lu", "authors": "Yang Lu, Qifan Chen and Simon Poon", "title": "A Novel Approach to Discover Switch Behaviours in Process Mining", "comments": "ICPM Workshop 2020", "journal-ref": "Process Mining Workshops. ICPM 2020. Lecture Notes in Business\n  Information Processing, vol 406. Springer, Cham", "doi": "10.1007/978-3-030-72693-5_5", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a relatively new subject which builds a bridge between\nprocess modelling and data mining. An exclusive choice in a process model\nusually splits the process into different branches. However, in some processes,\nit is possible to switch from one branch to another. The inductive miner\nguarantees to return sound process models, but fails to return a precise model\nwhen there are switch behaviours between different exclusive choice branches\ndue to the limitation of process trees. In this paper, we present a novel\nextension to the process tree model to support switch behaviours between\ndifferent branches of the exclusive choice operator and propose a novel\nextension to the inductive miner to discover sound process models with switch\nbehaviours. The proposed discovery technique utilizes the theory of a previous\nstudy to detect possible switch behaviours. We apply both artificial and\npublicly-available datasets to evaluate our approach. Our results show that our\napproach can improve the precision of discovered models by 36% while\nmaintaining high fitness values compared to the original inductive miner.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 04:25:28 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lu", "Yang", ""], ["Chen", "Qifan", ""], ["Poon", "Simon", ""]]}, {"id": "2106.12767", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Dongjin Choi and Sara Evensen and \\c{C}a\\u{g}atay Demiralp and Estevam\n  Hruschka", "title": "TagRuler: Interactive Tool for Span-Level Data Programming by\n  Demonstration", "comments": "WWW'21 Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite rapid developments in the field of machine learning research,\ncollecting high-quality labels for supervised learning remains a bottleneck for\nmany applications. This difficulty is exacerbated by the fact that\nstate-of-the-art models for NLP tasks are becoming deeper and more complex,\noften increasing the amount of training data required even for fine-tuning.\nWeak supervision methods, including data programming, address this problem and\nreduce the cost of label collection by using noisy label sources for\nsupervision. However, until recently, data programming was only accessible to\nusers who knew how to program. To bridge this gap, the Data Programming by\nDemonstration framework was proposed to facilitate the automatic creation of\nlabeling functions based on a few examples labeled by a domain expert. This\nframework has proven successful for generating high-accuracy labeling models\nfor document classification. In this work, we extend the DPBD framework to\nspan-level annotation tasks, arguably one of the most time-consuming NLP\nlabeling tasks. We built a novel tool, TagRuler, that makes it easy for\nannotators to build span-level labeling functions without programming and\nencourages them to explore trade-offs between different labeling models and\nactive learning strategies. We empirically demonstrated that an annotator could\nachieve a higher F1 score using the proposed tool compared to manual labeling\nfor different span-level annotation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 04:49:42 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Choi", "Dongjin", ""], ["Evensen", "Sara", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Hruschka", "Estevam", ""]]}, {"id": "2106.12857", "submitter": "Luigi Asprino", "authors": "Luigi Asprino, Christian Colonna, Misael Mongiov\\`i, Margherita\n  Porena, Valentina Presutti", "title": "Pattern-based Visualization of Knowledge Graphs", "comments": "16 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach to knowledge graph visualization based on\nontology design patterns. This approach relies on OPLa (Ontology Pattern\nLanguage) annotations and on a catalogue of visual frames, which are associated\nwith foundational ontology design patterns. We demonstrate that this approach\nsignificantly reduces the cognitive load required to users for visualizing and\ninterpreting a knowledge graph and guides the user in exploring it through\nmeaningful thematic paths provided by ontology patterns.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:43:15 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Asprino", "Luigi", ""], ["Colonna", "Christian", ""], ["Mongiov\u00ec", "Misael", ""], ["Porena", "Margherita", ""], ["Presutti", "Valentina", ""]]}, {"id": "2106.13342", "submitter": "Antonia Kormpa", "authors": "Mahmoud Abo Khamis, George Chichirim, Antonia Kormpa, Dan Olteanu", "title": "The Complexity of Boolean Conjunctive Queries with Intersection Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersection joins over interval data are relevant in spatial and temporal\ndata settings. A set of intervals join if their intersection is non-empty. In\ncase of point intervals, the intersection join becomes the standard equality\njoin.\n  We establish the complexity of Boolean conjunctive queries with intersection\njoins by a many-one equivalence to disjunctions of Boolean conjunctive queries\nwith equality joins. The complexity of any query with intersection joins is\nthat of the hardest query with equality joins in the disjunction exhibited by\nour equivalence. This is captured by a new width measure called the IJ-width.\n  We also introduce a new syntactic notion of acyclicity called iota-acyclicity\nto characterise the class of Boolean queries with intersection joins that admit\nlinear time computation modulo a poly-logarithmic factor in the data size.\nIota-acyclicity is for intersection joins what alpha-acyclicity is for equality\njoins. It strictly sits between gamma-acyclicity and Berge-acyclicity. The\nintersection join queries that are not iota-acyclic are at least as hard as the\nBoolean triangle query with equality joins, which is widely considered not\ncomputable in linear time.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 22:44:30 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Chichirim", "George", ""], ["Kormpa", "Antonia", ""], ["Olteanu", "Dan", ""]]}, {"id": "2106.13367", "submitter": "Qianru Zhou", "authors": "Qianru Zhou and Alasdair J.G. Gray and Stephen McLaughlin", "title": "SeaNet -- Towards A Knowledge Graph Based Autonomic Management of\n  Software Defined Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatic network management driven by Artificial Intelligent technologies\nhas been heatedly discussed over decades. However, current reports mainly focus\non theoretic proposals and architecture designs, works on practical\nimplementations on real-life networks are yet to appear. This paper proposes\nour effort toward the implementation of knowledge graph driven approach for\nautonomic network management in software defined networks (SDNs), termed as\nSeaNet. Driven by the ToCo ontology, SeaNet is reprogrammed based on Mininet (a\nSDN emulator). It consists three core components, a knowledge graph generator,\na SPARQL engine, and a network management API. The knowledge graph generator\nrepresents the knowledge in the telecommunication network management tasks into\nformally represented ontology driven model. Expert experience and network\nmanagement rules can be formalized into knowledge graph and by automatically\ninferenced by SPARQL engine, Network management API is able to packet\ntechnology-specific details and expose technology-independent interfaces to\nusers. The Experiments are carried out to evaluate proposed work by comparing\nwith a commercial SDN controller Ryu implemented by the same language Python.\nThe evaluation results show that SeaNet is considerably faster in most\ncircumstances than Ryu and the SeaNet code is significantly more compact.\nBenefit from RDF reasoning, SeaNet is able to achieve O(1) time complexity on\ndifferent scales of the knowledge graph while the traditional database can\nachieve O(nlogn) at its best. With the developed network management API, SeaNet\nenables researchers to develop semantic-intelligent applications on their own\nSDNs.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 00:33:42 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 07:31:16 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhou", "Qianru", ""], ["Gray", "Alasdair J. G.", ""], ["McLaughlin", "Stephen", ""]]}, {"id": "2106.13861", "submitter": "Yeye He", "authors": "Junwen Yang, Yeye He, Surajit Chaudhuri", "title": "AutoPipeline: Synthesize Data Pipelines By-Target Using Reinforcement\n  Learning and Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has made significant progress in helping users to automate single\ndata preparation steps, such as string-transformations and table-manipulation\noperators (e.g., Join, GroupBy, Pivot, etc.). We in this work propose to\nautomate multiple such steps end-to-end, by synthesizing complex data pipelines\nwith both string transformations and table-manipulation operators. We propose a\nnovel \"by-target\" paradigm that allows users to easily specify the desired\npipeline, which is a significant departure from the traditional by-example\nparadigm. Using by-target, users would provide input tables (e.g., csv or json\nfiles), and point us to a \"target table\" (e.g., an existing database table or\nBI dashboard) to demonstrate how the output from the desired pipeline would\nschematically \"look like\". While the problem is seemingly underspecified, our\nunique insight is that implicit table constraints such as FDs and keys can be\nexploited to significantly constrain the space to make the problem tractable.\nWe develop an Auto-Pipeline system that learns to synthesize pipelines using\nreinforcement learning and search. Experiments on large numbers of real\npipelines crawled from GitHub suggest that Auto-Pipeline can successfully\nsynthesize 60-70% of these complex pipelines (up to 10 steps) in 10-20 seconds\non average.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 19:44:01 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yang", "Junwen", ""], ["He", "Yeye", ""], ["Chaudhuri", "Surajit", ""]]}, {"id": "2106.14038", "submitter": "Yuedan Chen", "authors": "Yuedan Chen, M. Tamer \\\"Ozsu, Guoqing Xiao, Zhuo Tang, Kenli Li", "title": "GSmart: An Efficient SPARQL Query Engine Using Sparse Matrix Algebra --\n  Full Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient execution of SPARQL queries over large RDF datasets is a topic of\nconsiderable interest due to increased use of RDF to encode data. Most of this\nwork has followed either relational or graph-based approaches. In this paper,\nwe propose an alternative query engine, called gSmart, based on matrix algebra.\nThis approach can potentially better exploit the computing power of\nhigh-performance heterogeneous architectures that we target. gSmart\nincorporates: (1) grouped incident edge-based SPARQL query evaluation, in which\nall unevaluated edges of a vertex are evaluated together using a series of\nmatrix operations to fully utilize query constraints and narrow down the\nsolution space; (2) a graph query planner that determines the order in which\nvertices in query graphs should be evaluated; (3) memory- and\ncomputation-efficient data structures including the light-weight sparse matrix\n(LSpM) storage for RDF data and the tree-based representation for evaluation\nresults; (4) a multi-stage data partitioner to map the incident edge-based\nquery evaluation into heterogeneous HPC architectures and develop multi-level\nparallelism; and (5) a parallel executor that uses the fine-grained processing\nscheme, pre-pruning technique, and tree-pruning technique to lower inter-node\ncommunication and enable high throughput. Evaluations of gSmart on a CPU+GPU\nHPC architecture show execution time speedups of up to 46920.00x compared to\nthe existing SPARQL query engines on a single node machine. Additionally,\ngSmart on the Tianhe-1A supercomputer achieves a maximum speedup of 6.90x\nscaling from 2 to 16 CPU+GPU nodes.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 15:03:34 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chen", "Yuedan", ""], ["\u00d6zsu", "M. Tamer", ""], ["Xiao", "Guoqing", ""], ["Tang", "Zhuo", ""], ["Li", "Kenli", ""]]}, {"id": "2106.14052", "submitter": "Medina Andresel", "authors": "Medina Andresel, Csaba Domokos, Daria Stepanova, Trung-Kien Tran", "title": "A Neural-symbolic Approach for Ontology-mediated Query Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, low-dimensional vector space representations of knowledge graphs\n(KGs) have been applied to find answers to conjunctive queries (CQs) over\nincomplete KGs. However, the current methods only focus on inductive reasoning,\ni.e. answering CQs by predicting facts based on patterns learned from the data,\nand lack the ability of deductive reasoning by applying external domain\nknowledge. Such (expert or commonsense) domain knowledge is an invaluable\nresource which can be used to advance machine intelligence. To address this\nshortcoming, we introduce a neural-symbolic method for ontology-mediated CQ\nanswering over incomplete KGs that operates in the embedding space. More\nspecifically, we propose various data augmentation strategies to generate\ntraining queries using query-rewriting based methods and then exploit a novel\nloss function for training the model. The experimental results demonstrate the\neffectiveness of our training strategies and the new loss function, i.e., our\nmethod significantly outperforms the baseline in the settings that require both\ninductive and deductive reasoning.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 16:05:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Andresel", "Medina", ""], ["Domokos", "Csaba", ""], ["Stepanova", "Daria", ""], ["Tran", "Trung-Kien", ""]]}, {"id": "2106.14174", "submitter": "Saeid Hosseini", "authors": "Sana Rahmani, Saeid Hosseini, Raziyeh Zall, Mohammad Reza Kangavari,\n  Sara Kamran, Wen Hua", "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on\n  user latent aspects", "comments": "Under Review on IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 09:10:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Rahmani", "Sana", ""], ["Hosseini", "Saeid", ""], ["Zall", "Raziyeh", ""], ["Kangavari", "Mohammad Reza", ""], ["Kamran", "Sara", ""], ["Hua", "Wen", ""]]}, {"id": "2106.14417", "submitter": "Dickson Owuor Dr.", "authors": "Dickson Odhiambo Owuor", "title": "Capturing the temporal constraints of gradual patterns", "comments": "155 pagesm Doctoral thesis, Montpellier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gradual pattern mining allows for extraction of attribute correlations\nthrough gradual rules such as: \"the more X, the more Y\". Such correlations are\nuseful in identifying and isolating relationships among the attributes that may\nnot be obvious through quick scans on a data set. For instance, a researcher\nmay apply gradual pattern mining to determine which attributes of a data set\nexhibit unfamiliar correlations in order to isolate them for deeper exploration\nor analysis. In this work, we propose an ant colony optimization technique\nwhich uses a popular probabilistic approach that mimics the behavior biological\nants as they search for the shortest path to find food in order to solve\ncombinatorial problems. In our second contribution, we extend an existing\ngradual pattern mining technique to allow for extraction of gradual patterns\ntogether with an approximated temporal lag between the affected gradual item\nsets. Such a pattern is referred to as a fuzzy-temporal gradual pattern and it\nmay take the form: \"the more X, the more Y, almost 3 months later\". In our\nthird contribution, we propose a data crossing model that allows for\nintegration of mostly gradual pattern mining algorithm implementations into a\nCloud platform. This contribution is motivated by the proliferation of IoT\napplications in almost every area of our society and this comes with provision\nof large-scale time-series data from different sources.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 06:45:48 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Owuor", "Dickson Odhiambo", ""]]}, {"id": "2106.14766", "submitter": "Giacomo Bergami", "authors": "Giacomo Bergami", "title": "A Logical Model for joining Property Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The present paper upgrades the logical model required to exploit materialized\nviews over property graphs as intended in the seminal paper \"A Join Operator\nfor Property Graphs\". Furthermore, we provide some computational complexity\nproofs strengthening the contribution of a forthcoming graph equi-join\nalgorithm proposed in a recently accepted paper.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 09:24:27 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bergami", "Giacomo", ""]]}, {"id": "2106.14811", "submitter": "Wensheng Gan", "authors": "Jiahui Chen, Shicheng Wan, Wensheng Gan, Guoting Chen, and Hamido\n  Fujita", "title": "TOPIC: Top-k High-Utility Itemset Discovering", "comments": "Preprint. 5 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utility-driven itemset mining is widely applied in many real-world scenarios.\nHowever, most algorithms do not work for itemsets with negative utilities.\nSeveral efficient algorithms for high-utility itemset (HUI) mining with\nnegative utilities have been proposed. These algorithms can find complete HUIs\nwith or without negative utilities. However, the major problem with these\nalgorithms is how to select an appropriate minimum utility (minUtil) threshold.\nTo address this issue, some efficient algorithms for extracting top-k HUIs have\nbeen proposed, where parameter k is the quantity of HUIs to be discovered.\nHowever, all of these algorithms can solve only one part of the above problem.\nIn this paper, we present a method for TOP-k high-utility Itemset disCovering\n(TOPIC) with positive and negative utility values, which utilizes the\nadvantages of the above algorithms. TOPIC adopts transaction merging and\ndatabase projection techniques to reduce the database scanning cost, and\nutilizes minUtil threshold raising strategies. It also uses an array-based\nutility technique, which calculates the utility of itemsets and upper bounds in\nlinear time. We conducted extensive experiments on several real and synthetic\ndatasets, and the results showed that TOPIC outperforms state-of-the-art\nalgorithm in terms of runtime, memory costs, and scalability.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 15:34:03 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chen", "Jiahui", ""], ["Wan", "Shicheng", ""], ["Gan", "Wensheng", ""], ["Chen", "Guoting", ""], ["Fujita", "Hamido", ""]]}, {"id": "2106.14830", "submitter": "Wensheng Gan", "authors": "Shicheng Wan, Jiahui Chen, Wensheng Gan, Guoting Chen, and Vikram\n  Goyal", "title": "THUE: Discovering Top-K High Utility Episodes", "comments": "Preprint. 6 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episode discovery from an event is a popular framework for data mining tasks\nand has many real-world applications. An episode is a partially ordered set of\nobjects (e.g., item, node), and each object is associated with an event type.\nThis episode can also be considered as a complex event sub-sequence.\nHigh-utility episode mining is an interesting utility-driven mining task in the\nreal world. Traditional episode mining algorithms, by setting a threshold,\nusually return a huge episode that is neither intuitive nor saves time. In\ngeneral, finding a suitable threshold in a pattern-mining algorithm is a\ntrivial and time-consuming task. In this paper, we propose a novel algorithm,\ncalled Top-K High Utility Episode (THUE) mining within the complex event\nsequence, which redefines the previous mining task by obtaining the K highest\nepisodes. We introduce several threshold-raising strategies and optimize the\nepisode-weighted utilization upper bounds to speed up the mining process and\neffectively reduce the memory cost. Finally, the experimental results on both\nreal-life and synthetic datasets reveal that the THUE algorithm can offer six\nto eight orders of magnitude running time performance improvement over the\nstate-of-the-art algorithm and has low memory consumption.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 16:11:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wan", "Shicheng", ""], ["Chen", "Jiahui", ""], ["Gan", "Wensheng", ""], ["Chen", "Guoting", ""], ["Goyal", "Vikram", ""]]}, {"id": "2106.15093", "submitter": "Ananth Mahadevan", "authors": "Ananth Mahadevan and Michael Mathioudakis", "title": "Certifiable Machine Unlearning for Linear Models", "comments": "Updated Figures, VLDB Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine unlearning is the task of updating machine learning (ML) models after\na subset of the training data they were trained on is deleted. Methods for the\ntask are desired to combine effectiveness and efficiency, i.e., they should\neffectively \"unlearn\" deleted data, but in a way that does not require\nexcessive computation effort (e.g., a full retraining) for a small amount of\ndeletions. Such a combination is typically achieved by tolerating some amount\nof approximation in the unlearning. In addition, laws and regulations in the\nspirit of \"the right to be forgotten\" have given rise to requirements for\ncertifiability, i.e., the ability to demonstrate that the deleted data has\nindeed been unlearned by the ML model.\n  In this paper, we present an experimental study of the three state-of-the-art\napproximate unlearning methods for linear models and demonstrate the trade-offs\nbetween efficiency, effectiveness and certifiability offered by each method. In\nimplementing the study, we extend some of the existing works and describe a\ncommon ML pipeline to compare and evaluate the unlearning methods on six\nreal-world datasets and a variety of settings. We provide insights into the\neffect of the quantity and distribution of the deleted data on ML models and\nthe performance of each unlearning method in different settings. We also\npropose a practical online strategy to determine when the accumulated error\nfrom approximate unlearning is large enough to warrant a full retrain of the ML\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 05:05:58 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 13:14:55 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Mahadevan", "Ananth", ""], ["Mathioudakis", "Michael", ""]]}, {"id": "2106.15315", "submitter": "Neil Agarwal", "authors": "Neil Agarwal, Ravi Netravali", "title": "Boggart: Accelerating Retrospective Video Analytics via Model-Agnostic\n  Ingest Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delivering fast responses to retrospective queries on video datasets is\ndifficult due to the large number of frames to consider and the high costs of\nrunning convolutional neural networks (CNNs) on each one. A natural solution is\nto perform a subset of the necessary computations ahead of time, as video is\ningested. However, existing ingest-time systems require knowledge of the\nspecific CNN that will be used in future queries -- a challenging requisite\ngiven the evergrowing space of CNN architectures and training\ndatasets/methodologies.\n  This paper presents Boggart, a retrospective video analytics system that\ndelivers ingest-time speedups in a model-agnostic manner. Our underlying\ninsight is that traditional computer vision (CV) algorithms are capable of\nperforming computations that can be used to accelerate diverse queries with\nwide-ranging CNNs. Building on this, at ingest-time, Boggart carefully employs\na variety of motion tracking algorithms to identify potential objects and their\ntrajectories across frames. Then, at query-time, Boggart uses several novel\ntechniques to collect the smallest sample of CNN results required to meet the\ntarget accuracy: (1) a clustering strategy to efficiently unearth the\ninevitable discrepancies between CV- and CNN-generated outputs, and (2) a set\nof accuracy-preserving propagation techniques to safely extend sampled results\nalong each trajectory. Across many videos, CNNs, and queries Boggart\nconsistently meets accuracy targets while using CNNs sparingly (on 3-54% of\nframes).\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:21:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Agarwal", "Neil", ""], ["Netravali", "Ravi", ""]]}, {"id": "2106.15664", "submitter": "Ariel Sapir", "authors": "Amir Sapir, Ariel Sapir", "title": "Is 2NF a Stable Normal Form?", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditionally, it was accepted that a relational database can be normalized\nstep-by-step, from a set of un-normalized tables to tables in $1NF$, then to\n$2NF$, then to $3NF$, then (possibly) to $BCNF$. The rule applied to a table in\n$1NF$ in order to transform it to a set of tables in $2NF$ seems to be too\nstraightforward to pose any difficulty.\n  However, we show that, depending on the set of functional dependencies, it is\nimpossible to reach $2NF$ and stop there; one must, in these cases, perform the\nnormalization from $1NF$ to $3NF$ as an indecomposable move. The minimal setup\nto exhibit the phenomena requires a single composite key, and two partially\noverlapping chains of transitive dependencies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 18:14:26 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Sapir", "Amir", ""], ["Sapir", "Ariel", ""]]}, {"id": "2106.15703", "submitter": "Matthias Hofer", "authors": "Angela Bonifati and Stefania Dumbrava and George Fletcher and Jan\n  Hidders and Matthias Hofer and Wim Martens and Filip Murlak and Joshua\n  Shinavier and S{\\l}awek Staworko and Dominik Tomaszuk", "title": "Threshold Queries in Theory and in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Threshold queries are an important class of queries that only require\ncomputing or counting answers up to a specified threshold value. To the best of\nour knowledge, threshold queries have been largely disregarded in the research\nliterature, which is surprising considering how common they are in practice. In\nthis paper, we present a deep theoretical analysis of threshold query\nevaluation and show that thresholds can be used to significantly improve the\nasymptotic bounds of state-of-the-art query evaluation algorithms. We also\nempirically show that threshold queries are significant in practice. In\nsurprising contrast to conventional wisdom, we found important scenarios in\nreal-world data sets in which users are interested in computing the results of\nqueries up to a certain threshold, independent of a ranking function that\norders the query results by importance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 20:14:35 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bonifati", "Angela", ""], ["Dumbrava", "Stefania", ""], ["Fletcher", "George", ""], ["Hidders", "Jan", ""], ["Hofer", "Matthias", ""], ["Martens", "Wim", ""], ["Murlak", "Filip", ""], ["Shinavier", "Joshua", ""], ["Staworko", "S\u0142awek", ""], ["Tomaszuk", "Dominik", ""]]}, {"id": "2106.16166", "submitter": "Marcel Maltry", "authors": "Marcel Maltry, Jens Dittrich", "title": "A Critical Analysis of Recursive Model Indexes", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learned indexes like the recursive model index (RMI) have recently been\nintroduced as a machine-learned replacement for traditional indexes with\npossibly game-changing results for how database indexes are constructed and\nused. Has the time come to discard our good old hand-crafted index structures\nthat have been invented over the past decades? We believe that such a bold\nclaim -- with substantial impact on the database world -- is worth a deep\nexamination that clarifies when RMIs have benefits and when not. We present the\nfirst inventor-independent study critically examining RMIs. To do so, we\nrevisit the original paper and carefully reimplemented RMIs. We proceed by\nreproducing the most important experiments from the original paper and\nfollow-up papers all involving the inventors. We extend the original\nexperiments by adding more baselines and considering more configurations.\nFurther, we give insight on why and when RMIs perform well. Our results show\nthat while the general observation of the original work that \"any index is a\nmodel of the underlying data\" is truly inspiring, some conclusions drawn in the\noriginal work may mislead database architects to take unfortunate and too\nradical design decisions. In particular, we show that other types of indexes\noutperform RMIs in some situations. In addition, we will show that the\nperformance of RMIs is surprisingly sensitive to different data distributions.\nWe conclude by giving a clear guideline for database architects when to use\nRMIs, other learned indexes, or traditional indexes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:00:47 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Maltry", "Marcel", ""], ["Dittrich", "Jens", ""]]}]