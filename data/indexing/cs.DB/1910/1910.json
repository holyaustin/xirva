[{"id": "1910.00246", "submitter": "Phuc Nguyen Tri", "authors": "Phuc Nguyen and Natthawut Kertkeidkachorn and Ryutaro Ichise and\n  Hideaki Takeda", "title": "MTab: Matching Tabular Data to Knowledge Graph using Probability Models", "comments": "SemTab 2019. MTab", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the design of our system, namely MTab, for Semantic Web\nChallenge on Tabular Data to Knowledge Graph Matching (SemTab 2019). MTab\ncombines the voting algorithm and the probability models to solve critical\nproblems of the matching tasks. Results on SemTab 2019 show that MTab obtains\npromising performance for the three matching tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 08:20:56 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 02:55:29 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Nguyen", "Phuc", ""], ["Kertkeidkachorn", "Natthawut", ""], ["Ichise", "Ryutaro", ""], ["Takeda", "Hideaki", ""]]}, {"id": "1910.00293", "submitter": "Bruno Yun", "authors": "C\\'esar Prout\\'e, Bruno Yun, Madalina Croitoru", "title": "Distance-Based Approaches to Repair Semantics in Ontology-based Data\n  Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of inconsistencies, repair techniques thrive to restore\nconsistency by reasoning with several repairs. However, since the number of\nrepairs can be large, standard inconsistent tolerant semantics usually yield\nfew answers. In this paper, we use the notion of syntactic distance between\nrepairs following the intuition that it can allow us to cluster some repairs\n\"close\" to each other. In this way, we propose a generic framework to answer\nqueries in a more personalise fashion.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 10:28:32 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Prout\u00e9", "C\u00e9sar", ""], ["Yun", "Bruno", ""], ["Croitoru", "Madalina", ""]]}, {"id": "1910.00409", "submitter": "Jinyang Liu", "authors": "Jinyang Liu, Jieming Zhu, Shilin He, Pinjia He, Zibin Zheng, Michael\n  R. Lyu", "title": "Logzip: Extracting Hidden Structures via Iterative Clustering for Log\n  Compression", "comments": "This paper is accepted in the 34th IEEE/ACM International Conference\n  on Automated Software Engineering (ASE 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System logs record detailed runtime information of software systems and are\nused as the main data source for many tasks around software engineering. As\nmodern software systems are evolving into large scale and complex structures,\nlogs have become one type of fast-growing big data in industry. In particular,\nsuch logs often need to be stored for a long time in practice (e.g., a year),\nin order to analyze recurrent problems or track security issues. However,\narchiving logs consumes a large amount of storage space and computing\nresources, which in turn incurs high operational cost. Data compression is\nessential to reduce the cost of log storage. Traditional compression tools\n(e.g., gzip) work well for general texts, but are not tailed for system logs.\nIn this paper, we propose a novel and effective log compression method, namely\nlogzip. Logzip is capable of extracting hidden structures from raw logs via\nfast iterative clustering and further generating coherent intermediate\nrepresentations that allow for more effective compression. We evaluate logzip\non five large log datasets of different system types, with a total of 63.6 GB\nin size. The results show that logzip can save about half of the storage space\non average over traditional compression tools. Meanwhile, the design of logzip\nis highly parallel and only incurs negligible overhead. In addition, we share\nour industrial experience of applying logzip to Huawei's real products.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 01:00:40 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Liu", "Jinyang", ""], ["Zhu", "Jieming", ""], ["He", "Shilin", ""], ["He", "Pinjia", ""], ["Zheng", "Zibin", ""], ["Lyu", "Michael R.", ""]]}, {"id": "1910.00474", "submitter": "Martin Theobald", "authors": "Katerina Papaioannou, Martin Theobald, Michael H. B\\\"ohlen", "title": "Lineage-Aware Temporal Windows: Supporting Set Operations in\n  Temporal-Probabilistic Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In temporal-probabilistic (TP) databases, the combination of the temporal and\nthe probabilistic dimension adds significant overhead to the computation of set\noperations. Although set queries are guaranteed to yield linearly sized output\nrelations, existing solutions exhibit quadratic runtime complexity. They suffer\nfrom redundant interval comparisons and additional joins for the formation of\nlineage expressions. In this paper, we formally define the semantics of set\noperations in TP databases and study their properties. For their efficient\ncomputation, we introduce the lineage-aware temporal window, a mechanism that\ndirectly binds intervals with lineage expressions. We suggest the lineage-aware\nwindow advancer (LAWA) for producing the windows of two TP relations in\nlinearithmic time, and we implement all TP set operations based on LAWA. By\nexploiting the flexibility of lineage-aware temporal windows, we perform direct\nfiltering of irrelevant intervals and finalization of output lineage\nexpressions and thus guarantee that no additional computational cost or buffer\nspace is needed. A series of experiments over both synthetic and real-world\ndatasets show that (a) our approach has predictable performance, depending only\non the input size and not on the number of time intervals per fact or their\noverlap, and that (b) it outperforms state-of-the-art approaches in both\ntemporal and probabilistic databases.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:17:21 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Papaioannou", "Katerina", ""], ["Theobald", "Martin", ""], ["B\u00f6hlen", "Michael H.", ""]]}, {"id": "1910.00505", "submitter": "G\\\"okberk Ko\\c{c}ak", "authors": "G\\\"okberk Ko\\c{c}ak, \\\"Ozg\\\"ur Akg\\\"un, Tias Guns, Ian Miguel", "title": "Towards Improving Solution Dominance with Incomparability Conditions: A\n  case-study using Generator Itemset Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding interesting patterns is a challenging task in data mining. Constraint\nbased mining is a well-known approach to this, and one for which constraint\nprogramming has been shown to be a well-suited and generic framework. Dominance\nprogramming has been proposed as an extension that can capture an even wider\nclass of constraint-based mining problems, by allowing to compare relations\nbetween patterns. In this paper, in addition to specifying a dominance\nrelation, we introduce the ability to specify an incomparability condition.\nUsing these two concepts we devise a generic framework that can do a batch-wise\nsearch that avoids checking incomparable solutions. We extend the ESSENCE\nlanguage and underlying modelling pipeline to support this. We use generator\nitemset mining problem as a test case and give a declarative specification for\nthat. We also present preliminary experimental results on this specific problem\nclass with a CP solver backend to show that using the incomparability condition\nduring search can improve the efficiency of dominance programming and reduces\nthe need for post-processing to filter dominated solutions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:58:13 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Ko\u00e7ak", "G\u00f6kberk", ""], ["Akg\u00fcn", "\u00d6zg\u00fcr", ""], ["Guns", "Tias", ""], ["Miguel", "Ian", ""]]}, {"id": "1910.00728", "submitter": "Supreeth Shastri", "authors": "Supreeth Shastri, Vinay Banakar, Melissa Wasserman, Arun Kumar, Vijay\n  Chidambaram", "title": "Understanding and Benchmarking the Impact of GDPR on Database Systems", "comments": null, "journal-ref": "PVLDB, 13(7): 1064-1077, 2020", "doi": "10.14778/3384345.3384354", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The General Data Protection Regulation (GDPR) provides new rights and\nprotections to European people concerning their personal data. We analyze GDPR\nfrom a systems perspective, translating its legal articles into a set of\ncapabilities and characteristics that compliant systems must support. Our\nanalysis reveals the phenomenon of metadata explosion, wherein large quantities\nof metadata needs to be stored along with the personal data to satisfy the GDPR\nrequirements. Our analysis also helps us identify new workloads that must be\nsupported under GDPR. We design and implement an open-source benchmark called\nGDPRbench that consists of workloads and metrics needed to understand and\nassess personal-data processing database systems. To gauge the readiness of\nmodern database systems for GDPR, we follow best practices and developer\nrecommendations to modify Redis, PostgreSQL, and a commercial database system\nto be GDPR compliant. Our experiments demonstrate that the resulting GDPR\ncompliant systems achieve poor performance on GPDR workloads, and that\nperformance scales poorly as the volume of personal data increases. We discuss\nthe real-world implications of these findings, and identify research challenges\ntowards making GDPR compliance efficient in production environments. We release\nall of our software artifacts and datasets at http://www.gdprbench.org\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 00:49:44 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 02:54:07 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 03:38:56 GMT"}, {"version": "v4", "created": "Tue, 17 Mar 2020 03:59:10 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Shastri", "Supreeth", ""], ["Banakar", "Vinay", ""], ["Wasserman", "Melissa", ""], ["Kumar", "Arun", ""], ["Chidambaram", "Vijay", ""]]}, {"id": "1910.00765", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Divyakant Agrawal, Amr El Abbadi", "title": "SharPer: Sharding Permissioned Blockchains Over Network Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalability is one of the main roadblocks to business adoption of blockchain\nsystems. Despite recent intensive research on using sharding techniques to\nenhance the scalability of blockchain systems, existing solutions do not\nefficiently address cross-shard transactions. In this paper, we introduce\nSharPer, a permissioned blockchain system that improves scalability by\nclustering (partitioning) the nodes and assigning different data shards to\ndifferent clusters where each data shard is replicated on the nodes of a\ncluster. SharPer supports both intra-shard and cross-shard transactions and\nprocesses intra-shard transactions of different clusters as well as cross-shard\ntransactions with non-overlapping clusters simultaneously. In SharPer, the\nblockchain ledger is formed as a directed acyclic graph where each cluster\nmaintains only a view of the ledger. SharPer also incorporates a flattened\nprotocol to establish consensus among clusters on the order of cross-shard\ntransactions. The experimental results reveal the efficiency of SharPer in\nterms of performance and scalability especially in workloads with a low\npercentage of cross-shard transactions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:51:37 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 23:11:37 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1910.00985", "submitter": "Anh Dinh", "authors": "Tien Tuan Anh Dinh, Anwitaman Datta, Beng Chin Ooi", "title": "A Blueprint for Interoperable Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in blockchain systems has mainly focused on improving security and\nbridging the performance gaps between blockchains and databases. Despite many\npromising results, we observe a worrying trend that the blockchain landscape is\nfragmented in which many systems exist in silos. Apart from a handful of\ngeneral-purpose blockchains, such as Ethereum or Hyperledger Fabric, there are\nhundreds of others designed for specific applications and typically do not talk\nto each other. In this paper, we describe our vision of interoperable\nblockchains. We argue that supporting interaction among different blockchains\nrequires overcoming challenges that go beyond data standardization. The\nunderlying problem is to allow smart contracts running in different blockchains\nto communicate. We discuss three open problems: access control, general\ncross-chain transactions, and cross-chain communication. We describe partial\nsolutions to some of these problems in the literature. Finally, we propose a\nnovel design to overcome these challenges.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:44:47 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 02:56:25 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 12:46:10 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Dinh", "Tien Tuan Anh", ""], ["Datta", "Anwitaman", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1910.01310", "submitter": "Pingcheng Ruan", "authors": "Pingcheng Ruan, Tien Tuan Anh Dinh, Dumitrel Loghin, Meihui Zhang,\n  Gang Chen, Qian Lin, Beng Chin Ooi", "title": "Blockchains vs. Distributed Databases: Dichotomy and Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has come a long way: a system that was initially proposed\nspecifically for cryptocurrencies is now being adapted and adopted as a\ngeneral-purpose transactional system. As blockchain evolves into another data\nmanagement system, the natural question is how it compares against distributed\ndatabase systems. Existing works on this comparison focus on high-level\nproperties, such as security and throughput. They stop short of showing how the\nunderlying design choices contribute to the overall differences. Our work fills\nthis important gap and provides a principled framework for analyzing the\nemerging trend of blockchain-database fusion.\n  We perform a twin study of blockchains and distributed database systems as\ntwo types of transactional systems. We propose a taxonomy that illustrates the\ndichotomy across four dimensions, namely replication, concurrency, storage, and\nsharding. Within each dimension, we discuss how the design choices are driven\nby two goals: security for blockchains, and performance for distributed\ndatabases. To expose the impact of different design choices on the overall\nperformance, we conduct an in-depth performance analysis of two blockchains,\nnamely Quorum and Hyperledger Fabric, and two distributed databases, namely\nTiDB, and etcd. Lastly, we propose a framework for back-of-the-envelope\nperformance forecast of blockchain-database hybrids.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 05:43:41 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 12:40:12 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Ruan", "Pingcheng", ""], ["Dinh", "Tien Tuan Anh", ""], ["Loghin", "Dumitrel", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Lin", "Qian", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1910.01443", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Sanjukta Bhowmick and Sharma Chakravarthy", "title": "Efficient Community Detection in Boolean Composed Multiplex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks (or graphs) are used to model the dyadic relations between entities\nin a complex system. In cases where there exists multiple relations between the\nentities, the complex system can be represented as a multilayer network, where\nthe network in each layer represents one particular relation (or feature). The\nanalysis of multilayer networks involves combining edges from specific layers\nand then computing a network property.\n  Different subsets of the layers can be combined. For any Boolean combination\noperation (e.g. AND, OR), the number of possible subsets is exponential to the\nnumber of layers. Thus recomputing for each subset from scratch is an expensive\nprocess. In this paper, we propose to efficiently analyze multilayer networks\nusing a method that we term network decomposition.\n  Network decomposition is based on analyzing each network layer individually\nand then aggregating the analysis results. We demonstrate the effectiveness of\nusing network decomposition for detecting communities on different combinations\nof network layers. Our results on multilayer networks obtained from real-world\nand synthetic datasets show that our proposed network decomposition method\nrequires significantly lower computation time while producing results of high\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 18:52:54 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Santra", "Abhishek", ""], ["Bhowmick", "Sanjukta", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "1910.01737", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Kanthi Sannappa Komar, Sanjukta Bhowmick and Sharma\n  Chakravarthy", "title": "An Efficient Framework for Computing Structure- And Semantics-Preserving\n  Community in a Heterogeneous Multilayer Network", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.02641", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer networks or MLNs (also called multiplexes or network of networks)\nare being used extensively for modeling and analysis of data sets with multiple\nentity and feature types and associated relationships. Although the concept of\ncommunity is widely-used for aggregate analysis, a structure- and semantics\npreserving definition for it is lacking for MLNs. Retention of original MLN\nstructure and entity relationships is important for detailed drill-down\nanalysis. In addition, efficient computation is also critical for large number\nof analysis.\n  In this paper, we introduce a structure-preserving community definition for\nMLNs as well as a framework for its efficient computation using the decoupling\napproach. The proposed decoupling approach combines communities from individual\nlayers to form a serial k-community for connected k layers in a MLN. We propose\na new algorithm for pairing communities across layers and introduce several\nweight metrics for composing communities from two layers using participating\ncommunity characteristics. In addition to the definition, our proposed approach\nhas a number of desired characteristics. It: i) leverages extant single graph\ncommunity detection algorithms, ii) introduces several weight metrics that are\ncustomized for the community concept, iii) is a new algorithm for pairing\ncommunities using bipartite graphs, and iv) experimentally validates the\ncommunity computation and its efficiency on widely-used IMDb and DBLP data\nsets.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 19:20:55 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Santra", "Abhishek", ""], ["Komar", "Kanthi Sannappa", ""], ["Bhowmick", "Sanjukta", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "1910.02048", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, \\.Ismail \\.Ilkan Ceylan", "title": "The Dichotomy of Evaluating Homomorphism-Closed Queries on Probabilistic\n  Graphs", "comments": "30 pages. Journal version of the ICDT'20 paper\n  https://drops.dagstuhl.de/opus/volltexte/2020/11939/. Submitted to LMCS. The\n  previous version (version 2) was the same as the ICDT'20 paper with some\n  minor formatting tweaks and 7 extra pages of technical appendix", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2020.5", "report-no": null, "categories": "cs.DB cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of probabilistic query evaluation on probabilistic\ngraphs, namely, tuple-independent probabilistic databases on signatures of\narity two. Our focus is the class of queries that is closed under\nhomomorphisms, or equivalently, the infinite unions of conjunctive queries. Our\nmain result states that all unbounded queries from this class are #P-hard for\nprobabilistic query evaluation. As bounded queries from this class are\nequivalent to a union of conjunctive queries, they are already classified by\nthe dichotomy of Dalvi and Suciu (2012). Hence, our result and theirs imply a\ncomplete data complexity dichotomy, between polynomial time and #P-hardness,\nfor evaluating infinite unions of conjunctive queries over probabilistic\ngraphs. This dichotomy covers in particular all fragments of infinite unions of\nconjunctive queries such as negation-free (disjunctive) Datalog, regular path\nqueries, and a large class of ontology-mediated queries on arity-two\nsignatures. Our result is shown by reducing from counting the valuations of\npositive partitioned 2-DNF formulae for some queries, or from the\nsource-to-target reliability problem in an undirected graph for other queries,\ndepending on properties of minimal models. The presented dichotomy result\napplies to even a special case of probabilistic query evaluation called\ngeneralized model counting, where fact probabilities must be 0, 0.5, or 1.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:19:35 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 11:37:45 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 10:32:42 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Amarilli", "Antoine", ""], ["Ceylan", "\u0130smail \u0130lkan", ""]]}, {"id": "1910.02575", "submitter": "Yuening Li", "authors": "Yuening Li, Daochen Zha, Na Zou, Xia Hu", "title": "PyODDS: An End-to-End Outlier Detection System", "comments": "6 Pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PyODDS is an end-to end Python system for outlier detection with database\nsupport. PyODDS provides outlier detection algorithms which meet the demands\nfor users in different fields, w/wo data science or machine learning\nbackground. PyODDS gives the ability to execute machine learning algorithms\nin-database without moving data out of the database server or over the network.\nIt also provides access to a wide range of outlier detection algorithms,\nincluding statistical analysis and more recent deep learning based approaches.\nPyODDS is released under the MIT open-source license, and currently available\nat (https://github.com/datamllab/pyodds) with official documentations at\n(https://pyodds.github.io/).\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 02:01:34 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 04:49:16 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Li", "Yuening", ""], ["Zha", "Daochen", ""], ["Zou", "Na", ""], ["Hu", "Xia", ""]]}, {"id": "1910.02851", "submitter": "Giovanni Schmid PhD.", "authors": "Ferdinando Montecuollo and Giovannni Schmid", "title": "ER-index: a referential index for encrypted genomic databases", "comments": "27 pages with detailed pseudocodes", "journal-ref": "Information Systems 96 (2021) 101668", "doi": "10.1016/j.is.2020.101668", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge DBMSs storing genomic information are being created and engineerized for\ndoing large-scale, comprehensive and in-depth analysis of human beings and\ntheir diseases. However, recent regulations like the GDPR require that\nsensitive data are stored and elaborated thanks to privacy-by-design methods\nand software. We designed and implemented ER-index, a new full-text index in\nminute space which was optimized for compressing and encrypting collections of\ngenomic sequences, and for performing on them fast pattern-search queries. Our\nnew index complements the E2FM-index, which was introduced to compress and\nencrypt collections of nucleotide sequences without relying on a reference\nsequence. When used on collections of highly similar sequences, the ER-index\nallows to obtain compression ratios which are an order of magnitude smaller\nthan those achieved with the E2FM-index, but maintaining its very good search\nperformance. Moreover, thanks to the ER-index multi-user and multiple-keys\nencryption model, a single index can store the sequences related to a\npopulation of individuals so that users may perform search operations only on\nthe sequences to which they were granted access. The ER-index C++ source code\nplus scripts and data to assess the tool performance are available at:\nhttps://github.com/EncryptedIndexes/erindex.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:19:44 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 01:34:41 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Montecuollo", "Ferdinando", ""], ["Schmid", "Giovannni", ""]]}, {"id": "1910.02993", "submitter": "Daniel Y. Fu", "authors": "Daniel Y. Fu, Will Crichton, James Hong, Xinwei Yao, Haotian Zhang,\n  Anh Truong, Avanika Narayan, Maneesh Agrawala, Christopher R\\'e, Kayvon\n  Fatahalian", "title": "Rekall: Specifying Video Events using Compositions of Spatiotemporal\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world video analysis applications require the ability to identify\ndomain-specific events in video, such as interviews and commercials in TV news\nbroadcasts, or action sequences in film. Unfortunately, pre-trained models to\ndetect all the events of interest in video may not exist, and training new\nmodels from scratch can be costly and labor-intensive. In this paper, we\nexplore the utility of specifying new events in video in a more traditional\nmanner: by writing queries that compose outputs of existing, pre-trained\nmodels. To write these queries, we have developed Rekall, a library that\nexposes a data model and programming model for compositional video event\nspecification. Rekall represents video annotations from different sources\n(object detectors, transcripts, etc.) as spatiotemporal labels associated with\ncontinuous volumes of spacetime in a video, and provides operators for\ncomposing labels into queries that model new video events. We demonstrate the\nuse of Rekall in analyzing video from cable TV news broadcasts, films,\nstatic-camera vehicular video streams, and commercial autonomous vehicle logs.\nIn these efforts, domain experts were able to quickly (in a few hours to a day)\nauthor queries that enabled the accurate detection of new events (on par with,\nand in some cases much more accurate than, learned approaches) and to rapidly\nretrieve video clips for human-in-the-loop tasks such as video content curation\nand training data curation. Finally, in a user study, novice users of Rekall\nwere able to author queries to retrieve new events in video given just one hour\nof query development time.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:18:37 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Fu", "Daniel Y.", ""], ["Crichton", "Will", ""], ["Hong", "James", ""], ["Yao", "Xinwei", ""], ["Zhang", "Haotian", ""], ["Truong", "Anh", ""], ["Narayan", "Avanika", ""], ["Agrawala", "Maneesh", ""], ["R\u00e9", "Christopher", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "1910.03033", "submitter": "Erik Altman", "authors": "Erik R. Altman", "title": "Synthesizing Credit Card Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two elements have been essential to AI's recent boom: (1) deep neural nets\nand the theory and practice behind them; and (2) cloud computing with its\nabundant labeled data and large computing resources.\n  Abundant labeled data is available for key domains such as images, speech,\nnatural language processing, and recommendation engines. However, there are\nmany other domains where such data is not available, or access to it is highly\nrestricted for privacy reasons, as with health and financial data. Even when\nabundant data is available, it is often not labeled. Doing such labeling is\nlabor-intensive and non-scalable.\n  As a result, to the best of our knowledge, key domains still lack labeled\ndata or have at most toy data; or the synthetic data must have access to real\ndata from which it can mimic new data. This paper outlines work to generate\nrealistic synthetic data for an important domain: credit card transactions.\n  Some challenges: there are many patterns and correlations in real purchases.\nThere are millions of merchants and innumerable locations. Those merchants\noffer a wide variety of goods. Who shops where and when? How much do people\npay? What is a realistic fraudulent transaction?\n  We use a mixture of technical approaches and domain knowledge including\nmechanics of credit card processing, a broad set of consumer domains:\nelectronics, clothing, hair styling, etc. Connecting everything is a virtual\nworld. This paper outlines some of our key techniques and provides evidence\nthat the data generated is indeed realistic.\n  Beyond the scope of this paper: (1) use of our data to develop and train\nmodels to predict fraud; (2) coupling models and the synthetic dataset to\nassess performance in designing accelerators such as GPUs and TPUs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 01:18:57 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Altman", "Erik R.", ""]]}, {"id": "1910.03118", "submitter": "Mohamed Nadjib Mami", "authors": "Mohamed Nadjib Mami, Damien Graux, Harsh Thakkar, Simon Scerri,\n  S\\\"oren Auer, Jens Lehmann", "title": "The Query Translation Landscape: a Survey", "comments": "25 pages, 5 tables, 92 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Whereas the availability of data has seen a manyfold increase in past years,\nits value can be only shown if the data variety is effectively tackled ---one\nof the prominent Big Data challenges. The lack of data interoperability limits\nthe potential of its collective use for novel applications. Achieving\ninteroperability through the full transformation and integration of diverse\ndata structures remains an ideal that is hard, if not impossible, to achieve.\nInstead, methods that can simultaneously interpret different types of data\navailable in different data structures and formats have been explored. On the\nother hand, many query languages have been designed to enable users to interact\nwith the data, from relational, to object-oriented, to hierarchical, to the\nmultitude emerging NoSQL languages. Therefore, the interoperability issue could\nbe solved not by enforcing physical data transformation, but by looking at\ntechniques that are able to query heterogeneous sources using one uniform\nlanguage. Both industry and research communities have been keen to develop such\ntechniques, which require the translation of a chosen 'universal' query\nlanguage to the various data model specific query languages that make the\nunderlying data accessible. In this article, we survey more than forty query\ntranslation methods and tools for popular query languages, and classify them\naccording to eight criteria. In particular, we study which query language is a\nmost suitable candidate for that 'universal' query language. Further, the\nresults enable us to discover the weakly addressed and unexplored translation\npaths, to discover gaps and to learn lessons that can benefit future research\nin the area.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 22:37:33 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Mami", "Mohamed Nadjib", ""], ["Graux", "Damien", ""], ["Thakkar", "Harsh", ""], ["Scerri", "Simon", ""], ["Auer", "S\u00f6ren", ""], ["Lehmann", "Jens", ""]]}, {"id": "1910.03498", "submitter": "Dominique Mercier", "authors": "Dominique Mercier, Akansha Bhardwaj, Andreas Dengel, Sheraz Ahmed", "title": "SentiCite: An Approach for Publication Sentiment Analysis", "comments": "Preprint, 8 pages, 2 figures, 10th International Conference on Agents\n  and Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth in the number of scientific publications, year after\nyear, it is becoming increasingly difficult to identify quality authoritative\nwork on a single topic. Though there is an availability of scientometric\nmeasures which promise to offer a solution to this problem, these measures are\nmostly quantitative and rely, for instance, only on the number of times an\narticle is cited. With this approach, it becomes irrelevant if an article is\ncited 10 times in a positive, negative or neutral way. In this context, it is\nquite important to study the qualitative aspect of a citation to understand its\nsignificance. This paper presents a novel system for sentiment analysis of\ncitations in scientific documents (SentiCite) and is also capable of detecting\nnature of citations by targeting the motivation behind a citation, e.g.,\nreference to a dataset, reading reference. Furthermore, the paper also presents\ntwo datasets (SentiCiteDB and IntentCiteDB) containing about 2,600 citations\nwith their ground truth for sentiment and nature of citation. SentiCite along\nwith other state-of-the-art methods for sentiment analysis are evaluated on the\npresented datasets. Evaluation results reveal that SentiCite outperforms\nstate-of-the-art methods for sentiment analysis in scientific publications by\nachieving a F1-measure of 0.71.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 06:49:52 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Mercier", "Dominique", ""], ["Bhardwaj", "Akansha", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "1910.04223", "submitter": "Renan Souza", "authors": "Renan Souza, Leonardo Azevedo, V\\'itor Louren\\c{c}o, Elton Soares,\n  Raphael Thiago, Rafael Brand\\~ao, Daniel Civitarese, Emilio Vital Brazil,\n  Marcio Moreno, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, Marco A.\n  S. Netto", "title": "Provenance Data in the Machine Learning Lifecycle in Computational\n  Science and Engineering", "comments": "10 pages, 7 figures, Accepted at Workflows in Support of Large-scale\n  Science (WORKS) co-located with the ACM/IEEE International Conference for\n  High Performance Computing, Networking, Storage, and Analysis (SC) 2019,\n  Denver, Colorado", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has become essential in several industries. In\nComputational Science and Engineering (CSE), the complexity of the ML lifecycle\ncomes from the large variety of data, scientists' expertise, tools, and\nworkflows. If data are not tracked properly during the lifecycle, it becomes\nunfeasible to recreate a ML model from scratch or to explain to stakeholders\nhow it was created. The main limitation of provenance tracking solutions is\nthat they cannot cope with provenance capture and integration of domain and ML\ndata processed in the multiple workflows in the lifecycle while keeping the\nprovenance capture overhead low. To handle this problem, in this paper we\ncontribute with a detailed characterization of provenance data in the ML\nlifecycle in CSE; a new provenance data representation, called PROV-ML, built\non top of W3C PROV and ML Schema; and extensions to a system that tracks\nprovenance from multiple workflows to address the characteristics of ML and\nCSE, and to allow for provenance queries with a standard vocabulary. We show a\npractical use in a real case in the Oil and Gas industry, along with its\nevaluation using 48 GPUs in parallel.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 19:52:40 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 15:19:18 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Souza", "Renan", ""], ["Azevedo", "Leonardo", ""], ["Louren\u00e7o", "V\u00edtor", ""], ["Soares", "Elton", ""], ["Thiago", "Raphael", ""], ["Brand\u00e3o", "Rafael", ""], ["Civitarese", "Daniel", ""], ["Brazil", "Emilio Vital", ""], ["Moreno", "Marcio", ""], ["Valduriez", "Patrick", ""], ["Mattoso", "Marta", ""], ["Cerqueira", "Renato", ""], ["Netto", "Marco A. S.", ""]]}, {"id": "1910.04640", "submitter": "Giovanni Schmid PhD.", "authors": "Ferdinando Montecuollo, Giovannni Schmid, Roberto Tagliaferri", "title": "E2FM: an encrypted and compressed full-text index for collections of\n  genomic sequences", "comments": "23 pages with pseudo-code and experimental results", "journal-ref": "Bioinformatics, 33(18), 2017, 2808-2817", "doi": "10.1093/bioinformatics/btx313", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next Generation Sequencing (NGS) platforms and, more generally,\nhigh-throughput technologies are giving rise to an exponential growth in the\nsize of nucleotide sequence databases. Moreover, many emerging applications of\nnucleotide datasets -- as those related to personalized medicine -- require the\ncompliance with regulations about the storage and processing of sensitive data.\nWe have designed and carefully engineered E2FM-index, a new full-text index in\nminute space which was optimized for compressing and encrypting nucleotide\nsequence collections in FASTA format and for performing fast pattern-search\nqueries. E2FM-index allows to build self-indexes which occupy till to 1/20 of\nthe storage required by the input FASTA file, thus permitting to save about 95%\nof storage when indexing collections of highly similar sequences; moreover, it\ncan exactly search the built indexes for patterns in times ranging from few\nmilliseconds to a few hundreds milliseconds, depending on pattern length.\nSupplementary material and supporting datasets are available through\nBioinformatics Online and https://figshare.com/s/6246ee9c1bd730a8bf6e.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:19:19 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Montecuollo", "Ferdinando", ""], ["Schmid", "Giovannni", ""], ["Tagliaferri", "Roberto", ""]]}, {"id": "1910.04728", "submitter": "Jialin Ding", "authors": "Darryl Ho, Jialin Ding, Sanchit Misra, Nesime Tatbul, Vikram Nathan,\n  Vasimuddin Md, Tim Kraska", "title": "LISA: Towards Learned DNA Sequence Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation sequencing (NGS) technologies have enabled affordable\nsequencing of billions of short DNA fragments at high throughput, paving the\nway for population-scale genomics. Genomics data analytics at this scale\nrequires overcoming performance bottlenecks, such as searching for short DNA\nsequences over long reference sequences. In this paper, we introduce LISA\n(Learned Indexes for Sequence Analysis), a novel learning-based approach to DNA\nsequence search. As a first proof of concept, we focus on accelerating one of\nthe most essential flavors of the problem, called exact search. LISA builds on\nand extends FM-index, which is the state-of-the-art technique widely deployed\nin genomics tool-chains. Initial experiments with human genome datasets\nindicate that LISA achieves up to a factor of 4X performance speedup against\nits traditional counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:41:53 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Ho", "Darryl", ""], ["Ding", "Jialin", ""], ["Misra", "Sanchit", ""], ["Tatbul", "Nesime", ""], ["Nathan", "Vikram", ""], ["Md", "Vasimuddin", ""], ["Kraska", "Tim", ""]]}, {"id": "1910.04786", "submitter": "Ahmed Al-Baghdadi Mr.", "authors": "Ahmed Al-Baghdadi, Xiang Lian, and En Cheng", "title": "Efficient Path Routing Over Road Networks in the Presence of Ad-Hoc\n  Obstacles (Technical Report)", "comments": "Accepted for Information Systems (IS) Journal, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the path routing over road networks has become increasingly\nimportant, yet challenging, in many real-world applications such as\nlocation-based services (LBS), logistics and supply chain management,\ntransportation systems, map utilities, and so on. While many prior works aimed\nto find a path between a source and a destination with the smallest traveling\ndistance/time, they do not take into account the quality constraints (e.g.,\nobstacles) of the returned paths, such as uneven roads, roads under\nconstruction, and weather conditions on roads. Inspired by this, in this paper,\nwe consider two types of ad-hoc obstacles, keyword-based and weather-based\nobstacles, on road networks, which can be used for modeling roads that the\nreturned paths should not pass through. In the presence of such ad-hoc\nobstacles on roads, we formulate a path routing query over road networks with\nad-hoc obstacles (PRAO), which retrieves paths from source to destination on\nroad networks that do not pass ad-hoc keyword and weather obstacles and have\nthe smallest traveling time. In order to efficiently answer PRAO queries, we\ndesign effective pruning methods and indexing mechanism to facilitate efficient\nPRAO query answering. Extensive experiments have demonstrated the efficiency\nand effectiveness of our approaches over real/synthetic data sets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 18:05:45 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Al-Baghdadi", "Ahmed", ""], ["Lian", "Xiang", ""], ["Cheng", "En", ""]]}, {"id": "1910.04927", "submitter": "Gong Cheng", "authors": "Tianshuo Zhou, Ziyang Li, Gong Cheng, Jun Wang, Yu'Ang Wei", "title": "GREASE: A Generative Model for Relevance Search over Knowledge Graphs", "comments": "9 pages, accepted to WSDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relevance search is to find top-ranked entities in a knowledge graph (KG)\nthat are relevant to a query entity. Relevance is ambiguous, particularly over\na schema-rich KG like DBpedia which supports a wide range of different\nsemantics of relevance based on numerous types of relations and attributes. As\nusers may lack the expertise to formalize the desired semantics, supervised\nmethods have emerged to learn the hidden user-defined relevance from\nuser-provided examples. Along this line, in this paper we propose a novel\ngenerative model over KGs for relevance search, named GREASE. The model applies\nto meta-path based relevance where a meta-path characterizes a particular type\nof semantics of relating the query entity to answer entities. It is also\nextended to support properties that constrain answer entities. Extensive\nexperiments on two large-scale KGs demonstrate that GREASE has advanced the\nstate of the art in effectiveness, expressiveness, and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 01:13:51 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Zhou", "Tianshuo", ""], ["Li", "Ziyang", ""], ["Cheng", "Gong", ""], ["Wang", "Jun", ""], ["Wei", "Yu'Ang", ""]]}, {"id": "1910.04939", "submitter": "Hung Ngo", "authors": "Ryan Curtin, Ben Moseley, Hung Q. Ngo, XuanLong Nguyen, Dan Olteanu,\n  Maximilian Schleich", "title": "Rk-means: Fast Clustering for Relational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional machine learning algorithms cannot be applied until a data\nmatrix is available to process. When the data matrix needs to be obtained from\na relational database via a feature extraction query, the computation cost can\nbe prohibitive, as the data matrix may be (much) larger than the total input\nrelation size. This paper introduces Rk-means, or relational k -means\nalgorithm, for clustering relational data tuples without having to access the\nfull data matrix. As such, we avoid having to run the expensive feature\nextraction query and storing its output. Our algorithm leverages the underlying\nstructures in relational data. It involves construction of a small {\\it grid\ncoreset} of the data matrix for subsequent cluster construction. This gives a\nconstant approximation for the k -means objective, while having asymptotic\nruntime improvements over standard approaches of first running the database\nquery and then clustering. Empirical results show orders-of-magnitude speedup,\nand Rk-means can run faster on the database than even just computing the data\nmatrix.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 02:03:08 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Curtin", "Ryan", ""], ["Moseley", "Ben", ""], ["Ngo", "Hung Q.", ""], ["Nguyen", "XuanLong", ""], ["Olteanu", "Dan", ""], ["Schleich", "Maximilian", ""]]}, {"id": "1910.04991", "submitter": "Santhilata Kuppili Venkata", "authors": "Santhilata Kuppili Venkata and Katarzyna Musial", "title": "Sub-query Fragmentation for Query Analysis and Data Caching in the\n  Distributed Environment", "comments": "29 pages, 18 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data stores and users are distributed geographically, it is essential to\norganize distributed data cache points at ideal locations to minimize data\ntransfers. To answer this, we are developing an adaptive distributed data\ncaching framework that can identify suitable data chunks to cache and move\nacross a network of community cache locations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 06:41:09 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Venkata", "Santhilata Kuppili", ""], ["Musial", "Katarzyna", ""]]}, {"id": "1910.05146", "submitter": "Jos\\'e L Balc\\'azar", "authors": "Marie Ely Piceno and Jos\\'e Luis Balc\\'azar", "title": "Analysis of Co-Occurrence Patterns in Data through Modular and Clan\n  Decompositions of Gaifman Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We argue that the existing knowledge about modular decomposition of graphs\nand clan decomposition of 2-structures can be put to use advantageously in a\ncontext of data analysis. We show how to obtain visual descriptions of\nco-occurrence patterns by employing these decompositions on possibly\ngeneralized Gaifman graphs associated to datasets. We provide both theoretical\nadvances that connect the proposed process to other data mining aspects\n(namely, closed set mining), as well as implemented algorithmics leading to an\nopen-source tool that demonstrates our approach.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:57:17 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 11:10:20 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Piceno", "Marie Ely", ""], ["Balc\u00e1zar", "Jos\u00e9 Luis", ""]]}, {"id": "1910.05773", "submitter": "Xiaowei Zhu", "authors": "Xiaowei Zhu, Guanyu Feng, Marco Serafini, Xiaosong Ma, Jiping Yu, Lei\n  Xie, Ashraf Aboulnaga, Wenguang Chen", "title": "LiveGraph: A Transactional Graph Storage System with Purely Sequential\n  Adjacency List Scans", "comments": null, "journal-ref": null, "doi": "10.14778/3384345.3384351", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The specific characteristics of graph workloads make it hard to design a\none-size-fits-all graph storage system. Systems that support transactional\nupdates use data structures with poor data locality, which limits the\nefficiency of analytical workloads or even simple edge scans. Other systems run\ngraph analytics workloads efficiently, but cannot properly support\ntransactions.\n  This paper presents LiveGraph, a graph storage system that outperforms both\nthe best graph transactional systems and the best systems for real-time graph\nanalytics on fresh data. LiveGraph does that by ensuring that adjacency list\nscans, a key operation in graph workloads, are purely sequential: they never\nrequire random accesses even in presence of concurrent transactions. This is\nachieved by combining a novel graph-aware data structure, the Transactional\nEdge Log (TEL), together with a concurrency control mechanism that leverages\nTEL's data layout. Our evaluation shows that LiveGraph significantly\noutperforms state-of-the-art (graph) database solutions on both transactional\nand real-time analytical workloads.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 15:57:13 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 15:30:15 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Zhu", "Xiaowei", ""], ["Feng", "Guanyu", ""], ["Serafini", "Marco", ""], ["Ma", "Xiaosong", ""], ["Yu", "Jiping", ""], ["Xie", "Lei", ""], ["Aboulnaga", "Ashraf", ""], ["Chen", "Wenguang", ""]]}, {"id": "1910.06169", "submitter": "Giorgio Vinciguerra", "authors": "Paolo Ferragina and Giorgio Vinciguerra", "title": "The PGM-index: a multicriteria, compressed and learned approach to data\n  indexing", "comments": "We remark to the reader that this paper is an extended and improved\n  version of our previous paper titled \"Superseding traditional indexes by\n  orchestrating learning and geometry\" (arXiv:1903.00507)", "journal-ref": "PVLDB, 13(8): 1162-1175, 2020", "doi": "10.14778/3389133.3389135", "report-no": null, "categories": "cs.DS cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent introduction of learned indexes has shaken the foundations of the\ndecades-old field of indexing data structures. Combining, or even replacing,\nclassic design elements such as B-tree nodes with machine learning models has\nproven to give outstanding improvements in the space footprint and time\nefficiency of data systems. However, these novel approaches are based on\nheuristics, thus they lack any guarantees both in their time and space\nrequirements. We propose the Piecewise Geometric Model index (shortly,\nPGM-index), which achieves guaranteed I/O-optimality in query operations,\nlearns an optimal number of linear models, and its peculiar recursive\nconstruction makes it a purely learned data structure, rather than a hybrid of\ntraditional and learned indexes (such as RMI and FITing-tree). We show that the\nPGM-index improves the space of the FITing-tree by 63.3% and of the B-tree by\nmore than four orders of magnitude, while achieving their same or even better\nquery time efficiency. We complement this result by proposing three variants of\nthe PGM-index. First, we design a compressed PGM-index that further reduces its\nspace footprint by exploiting the repetitiveness at the level of the learned\nlinear models it is composed of. Second, we design a PGM-index that adapts\nitself to the distribution of the queries, thus resulting in the first known\ndistribution-aware learned index to date. Finally, given its flexibility in the\noffered space-time trade-offs, we propose the multicriteria PGM-index that\nefficiently auto-tune itself in a few seconds over hundreds of millions of keys\nto the possibly evolving space-time constraints imposed by the application of\nuse.\n  We remark to the reader that this paper is an extended and improved version\nof our previous paper titled \"Superseding traditional indexes by orchestrating\nlearning and geometry\" (arXiv:1903.00507).\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:25:25 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ferragina", "Paolo", ""], ["Vinciguerra", "Giorgio", ""]]}, {"id": "1910.06584", "submitter": "Yuxiang Wang", "authors": "Yuxiang Wang, Arijit Khan, Tianxing Wu, Jiahui Jin, Haijiang Yan", "title": "Semantic Guided and Response Times Bounded Top-k Similarity Search over\n  Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph query is widely adopted for querying knowledge graphs. Given\na query graph $G_Q$, the graph query finds subgraphs in a knowledge graph $G$\nthat exactly or approximately match $G_Q$. We face two challenges on graph\nquery: (1) the structural gap between $G_Q$ and the predefined schema in $G$\ncauses mismatch with query graph, (2) users cannot view the answers until the\ngraph query terminates, leading to a longer system response time (SRT). In this\npaper, we propose a semantic-guided and response-time-bounded graph query to\nreturn the top-k answers effectively and efficiently. We leverage a knowledge\ngraph embedding model to build the semantic graph $SG_Q$, and we define the\npath semantic similarity ($pss$) over $SG_Q$ as the metric to evaluate the\nanswer's quality. Then, we propose an A* semantic search on $SG_Q$ to find the\ntop-k answers with the greatest $pss$ via a heuristic $pss$ estimation.\nFurthermore, we make an approximate optimization on A* semantic search to allow\nusers to trade off the effectiveness for SRT within a user-specific time bound.\nExtensive experiments over real datasets confirm the effectiveness and\nefficiency of our solution.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 08:18:00 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 14:03:03 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Wang", "Yuxiang", ""], ["Khan", "Arijit", ""], ["Wu", "Tianxing", ""], ["Jin", "Jiahui", ""], ["Yan", "Haijiang", ""]]}, {"id": "1910.06708", "submitter": "Tianxing Wu", "authors": "Tianxing Wu, Arijit Khan, Huan Gao, Cheng Li", "title": "Efficiently Embedding Dynamic Knowledge Graphs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph (KG) embedding encodes the entities and relations from a KG\ninto low-dimensional vector spaces to support various applications such as KG\ncompletion, question answering, and recommender systems. In real world,\nknowledge graphs (KGs) are dynamic and evolve over time with addition or\ndeletion of triples. However, most existing models focus on embedding static\nKGs while neglecting dynamics. To adapt to the changes in a KG, these models\nneed to be re-trained on the whole KG with a high time cost.\n  In this paper, to tackle the aforementioned problem, we propose a new\ncontext-aware Dynamic Knowledge Graph Embedding (DKGE) method which supports\nthe embedding learning in an online fashion. DKGE introduces two different\nrepresentations (i.e., knowledge embedding and contextual element embedding)\nfor each entity and each relation, in the joint modeling of entities and\nrelations as well as their contexts, by employing two attentive graph\nconvolutional networks, a gate strategy, and translation operations. This\neffectively helps limit the impacts of a KG update in certain regions, not in\nthe entire graph, so that DKGE can rapidly acquire the updated KG embedding by\na proposed online learning algorithm. Furthermore, DKGE can also learn KG\nembedding from scratch. Experiments on the tasks of link prediction and\nquestion answering in a dynamic environment demonstrate the effectiveness and\nefficiency of DKGE.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 13:12:59 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Wu", "Tianxing", ""], ["Khan", "Arijit", ""], ["Gao", "Huan", ""], ["Li", "Cheng", ""]]}, {"id": "1910.06788", "submitter": "Udesh Gunarathna", "authors": "Udesh Gunarathna, Hairuo Xie, Egemen Tanin, Shanika Karunasekara,\n  Renata Borovica-Gajic", "title": "Dynamic Graph Configuration with Reinforcement Learning for Connected\n  Autonomous Vehicle Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DB cs.LG cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional traffic optimization solutions assume that the graph structure of\nroad networks is static, missing opportunities for further traffic flow\noptimization. We are interested in optimizing traffic flows as a new type of\ngraph-based problem, where the graph structure of a road network can adapt to\ntraffic conditions in real time. In particular, we focus on the dynamic\nconfiguration of traffic-lane directions, which can help balance the usage of\ntraffic lanes in opposite directions. The rise of connected autonomous vehicles\noffers an opportunity to apply this type of dynamic traffic optimization at a\nlarge scale. The existing techniques for optimizing lane-directions are however\nnot suitable for dynamic traffic environments due to their high computational\ncomplexity and the static nature.\n  In this paper, we propose an efficient traffic optimization solution, called\nCoordinated Learning-based Lane Allocation (CLLA), which is suitable for\ndynamic configuration of lane-directions. CLLA consists of a two-layer\nmulti-agent architecture, where the bottom-layer agents use a machine learning\ntechnique to find a suitable configuration of lane-directions around individual\nroad intersections. The lane-direction changes proposed by the learning agents\nare then coordinated at a higher level to reduce the negative impact of the\nchanges on other parts of the road network. Our experimental results show that\nCLLA can reduce the average travel time significantly in congested road\nnetworks. We believe our method is general enough to be applied to other types\nof networks as well.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:22:01 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Gunarathna", "Udesh", ""], ["Xie", "Hairuo", ""], ["Tanin", "Egemen", ""], ["Karunasekara", "Shanika", ""], ["Borovica-Gajic", "Renata", ""]]}, {"id": "1910.07144", "submitter": "Guangyan Hu", "authors": "Guangyan Hu, Yongfeng Zhang, Sandro Rigo, Thu D. Nguyen", "title": "Similarity Driven Approximation for Text Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text analytics has become an important part of business intelligence as\nenterprises increasingly seek to extract insights for decision making from text\ndata sets. Processing large text data sets can be computationally expensive,\nhowever, especially if it involves sophisticated algorithms. This challenge is\nexacerbated when it is desirable to run different types of queries against a\ndata set, making it expensive to build multiple indices to speed up query\nprocessing. In this paper, we propose and evaluate a framework called EmApprox\nthat uses approximation to speed up the processing of a wide range of queries\nover large text data sets. The key insight is that different types of queries\ncan be approximated by processing subsets of data that are most similar to the\nqueries. EmApprox builds a general index for a data set by learning a natural\nlanguage processing model, producing a set of highly compressed vectors\nrepresenting words and subcollections of documents. Then, at query processing\ntime, EmApprox uses the index to guide sampling of the data set, with the\nprobability of selecting each subcollection of documents being proportional to\nits {\\em similarity} to the query as computed using the vector representations.\nWe have implemented a prototype of EmApprox as an extension of the Apache Spark\nsystem, and used it to approximate three types of queries: aggregation,\ninformation retrieval, and recommendation. Experimental results show that\nEmApprox's similarity-guided sampling achieves much better accuracy than random\nsampling. Further, EmApprox can achieve significant speedups if users can\ntolerate small amounts of inaccuracies. For example, when sampling at 10\\%,\nEmApprox speeds up a set of queries counting phrase occurrences by almost 10x\nwhile achieving estimated relative errors of less than 22\\% for 90\\% of the\nqueries.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 03:13:23 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 19:54:48 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Hu", "Guangyan", ""], ["Zhang", "Yongfeng", ""], ["Rigo", "Sandro", ""], ["Nguyen", "Thu D.", ""]]}, {"id": "1910.07519", "submitter": "Frederic Prost", "authors": "Dominique Duval (CASC), Rachid Echahed (LIG Laboratoire d'Informatique\n  de Grenoble), Frederic Prost (LIG)", "title": "On foundational aspects of RDF and SPARQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recommendations of the World Wide Web Consortium (W3C) about\nthe Resource Description Framework (RDF) and the associated query language\nSPARQL. We propose a new formal framework based on category theory which\nprovides clear and concise formal definitions of the main basic features of RDF\nand SPARQL. We propose to define the notions of RDF graphs as well as SPARQL\nbasic graph patterns as objects of some nested categories. This allows one to\nclarify, in particular, the role of blank nodes. Furthermore, we consider basic\nSPARQL CONSTRUCT and SELECT queries and formalize their operational semantics\nfollowing a novel algebraic graph transformation approach called POIM.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 08:21:57 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 16:55:33 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Duval", "Dominique", "", "CASC"], ["Echahed", "Rachid", "", "LIG Laboratoire d'Informatique\n  de Grenoble"], ["Prost", "Frederic", "", "LIG"]]}, {"id": "1910.07786", "submitter": "Naibo Wang", "authors": "Naibo Wang, Zhiling Luo, Xiya Lyu, Zitong Yang, Jianwei Yin", "title": "Service Wrapper: a system for converting web data into web services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web services are widely used in many areas via callable APIs, however, data\nare not always available in this way. We always need to get some data from web\npages whose structure is not in order. Many developers use web data extraction\nmethods to generate wrappers to get useful contents from websites and convert\nthem into well-structured files. These methods, however, are designed\nspecifically for professional wrapper program developers and not friendly to\nusers without expertise in this domain. In this work, we construct a service\nwrapper system to convert available data in web pages into web services.\nAdditionally, a set of algorithms are introduced to solve problems in the whole\nconversion process. People can use our system to convert web data into web\nservices with fool-style operations and invoke these services by one simple\nstep, which greatly expands the use of web data. Our cases show the ease of\nuse, high availability, and stability of our system.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:29:24 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Wang", "Naibo", ""], ["Luo", "Zhiling", ""], ["Lyu", "Xiya", ""], ["Yang", "Zitong", ""], ["Yin", "Jianwei", ""]]}, {"id": "1910.07910", "submitter": "Matthias Naaf", "authors": "Katrin M. Dannert, Erich Gr\\\"adel, Matthias Naaf, Val Tannen", "title": "Generalized Absorptive Polynomials and Provenance Semantics for\n  Fixed-Point Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB math.LO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Semiring provenance is a successful approach to provide detailed information\non the combinations of atomic facts that are responsible for the result of a\nquery. In particular, interpretations in general provenance semirings of\npolynomials or formal power series give precise descriptions of the successful\nevaluation strategies for the query. While provenance analysis in databases\nhas, for a long time, been largely confined to negation-free query languages, a\nrecent approach extends this to model checking problems for logics with full\nnegation. Algebraically this relies on new quotient semirings of\ndual-indeterminate polynomials or power series. So far, this approach has been\ndeveloped mainly for first-order logic and for the positive fragment of least\nfixed-point logic. What has remained open is an adequate treatment for\nfixed-point calculi that admit arbitrary interleavings of least and greatest\nfixed points. We show that an adequate framework for the provenance analysis of\nfull fixed-point logics is provided by semirings that are (1) fully continuous,\n(2) absorptive, and (3) chain-positive. Full continuity guarantees that\nprovenance values of least and greatest fixed-points are well-defined.\nAbsorptive semirings provide a symmetry between least and greatest fixed-point\ncomputations and make sure that provenance values of greatest fixed points are\ninformative. Finally, chain-positivity is responsible for having\ntruth-preserving interpretations, which give non-zero values to all true\nformulae. We further identify semirings of generalized absorptive polynomials\nand prove universal properties that make them the most general appropriate\nsemirings for LFP. We illustrate the power of provenance interpretations in\nthese semirings by relating them to provenance values of plays and strategies\nin the associated model-checking games.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 13:44:37 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 15:55:33 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 10:11:57 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Dannert", "Katrin M.", ""], ["Gr\u00e4del", "Erich", ""], ["Naaf", "Matthias", ""], ["Tannen", "Val", ""]]}, {"id": "1910.08023", "submitter": "Ilia Petrov", "authors": "Christian Riegger, Tobias Vincon, Robert Gottstein, Ilia Petrov", "title": "MV-PBT: Multi-Version Index for Large Datasets and HTAP Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern mixed (HTAP) workloads execute fast update-transactions and\nlong-running analytical queries on the same dataset and system. In\nmulti-version (MVCC) systems, such workloads result in many short-lived\nversions and long version-chains as well as in increased and frequent\nmaintenance overhead. Consequently, the index pressure increases significantly.\nFirstly, the frequent modifications cause frequent creation of new versions,\nyielding a surge in index maintenance overhead. Secondly and more importantly,\nindex-scans incur extra I/O overhead to determine, which of the resulting\ntuple-versions are visible to the executing transaction (visibility-check) as\ncurrent designs only store version/timestamp information in the base table --\nnot in the index. Such index-only visibility-check is critical for HTAP\nworkloads on large datasets. In this paper we propose the Multi-Version\nPartitioned B-Tree (MV-PBT) as a version-aware index structure, supporting\nindex-only visibility checks and flash-friendly I/O patterns. The experimental\nevaluation indicates a 2x improvement for analytical queries and 15% higher\ntransactional throughput under HTAP workloads (CH-Benchmark). MV-PBT offers 40%\nhigher transactional throughput compared to WiredTiger's LSM-Tree\nimplementation under YCSB.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:51:44 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Riegger", "Christian", ""], ["Vincon", "Tobias", ""], ["Gottstein", "Robert", ""], ["Petrov", "Ilia", ""]]}, {"id": "1910.08185", "submitter": "Wail Alkowaileet", "authors": "Wail Y. Alkowaileet, Sattam Alsubaiee and Michael J. Carey", "title": "An LSM-based Tuple Compaction Framework for Apache AsterixDB (Extended\n  Version)", "comments": "18 pages, 28 figures, to appear in VLDB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document database systems store self-describing semi-structured records, such\nas JSON, \"as-is\" without requiring the users to pre-define a schema. This\nprovides users with the flexibility to change the structure of incoming records\nwithout worrying about taking the system offline or hindering the performance\nof currently running queries. However, the flexibility of such systems does not\nfree. The large amount of redundancy in the records can introduce an\nunnecessary storage overhead and impact query performance.\n  Our focus in this paper is to address the storage overhead issue by\nintroducing a tuple compactor framework that infers and extracts the schema\nfrom self-describing semi-structured records during the data ingestion. As many\nprominent document stores, such as MongoDB and Couchbase, adopt Log Structured\nMerge (LSM) trees in their storage engines, our framework exploits LSM\nlifecycle events to piggyback the schema inference and extraction operations.\nWe have implemented and empirically evaluated our approach to measure its\nimpact on storage, data ingestion, and query performance in the context of\nApache AsterixDB.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 22:13:40 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 05:23:31 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Alkowaileet", "Wail Y.", ""], ["Alsubaiee", "Sattam", ""], ["Carey", "Michael J.", ""]]}, {"id": "1910.08678", "submitter": "Aline Bessa", "authors": "Aline Bessa, Juliana Freire, Divesh Srivastava, Tamraparni Dasu", "title": "Effective Discovery of Meaningful Outlier Relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PODS (Predictable Outliers in Data-trendS), a method that, given a\ncollection of temporal data sets, derives data-driven explanations for outliers\nby identifying meaningful relationships between them. First, we formalize the\nnotion of meaningfulness, which so far has been informally framed in terms of\nexplainability. Next, since outliers are rare and it is difficult to determine\nwhether their relationships are meaningful, we develop a new criterion that\ndoes so by checking if these relationships could have been predicted from\nnon-outliers, i.e., if we could see the outlier relationships coming. Finally,\nsearching for meaningful outlier relationships between every pair of data sets\nin a large data collection is computationally infeasible. To address that, we\npropose an indexing strategy that prunes irrelevant comparisons across data\nsets, making the approach scalable. We present the results of an experimental\nevaluation using real data sets and different baselines, which demonstrates the\neffectiveness, robustness, and scalability of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 02:03:15 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 23:01:17 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Bessa", "Aline", ""], ["Freire", "Juliana", ""], ["Srivastava", "Divesh", ""], ["Dasu", "Tamraparni", ""]]}, {"id": "1910.08888", "submitter": "Carlo Zaniolo", "authors": "Carlo Zaniolo, Ariyam Das, Jiaqi Gu, Youfu Li, Mingda li, Jin Wang", "title": "Monotonic Properties of Completed Aggregates in Recursive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of aggregates in recursion enables efficient and scalable support for\na wide range of BigData algorithms, including those used in graph applications,\nKDD applications, and ML applications, which have proven difficult to be\nexpressed and supported efficiently in BigData systems supporting Datalog or\nSQL. The problem with these languages and systems is that, to avoid the\nsemantic and computational issues created by non-monotonic constructs in\nrecursion, they only allow programs that are stratified with respect to\nnegation and aggregates. Now, while this crippling restriction is\nwell-justified for negation, it is frequently unjustified for aggregates, since\n(i) aggregates are often monotonic in the standard lattice of set-containment,\n(ii) the PreM property guarantees that programs with extrema in recursion are\nequivalent to stratified programs where extrema are used as post-constraints,\nand (iii) any program computing any aggregates on sets of facts of predictable\ncardinality tantamounts to stratified programs where the precomputation of the\ncardinality of the set is followed by a stratum where recursive rules only use\nmonotonic constructs. With (i) and (ii) covered in previous papers, this paper\nfocuses on (iii) using examples of great practical interest. For such examples,\nwe provide a formal semantics that is conducive to efficient and scalable\nimplementations via well-known techniques such as semi-naive fixpoint currently\nsupported by most Datalog and SQL3 systems.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 03:52:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zaniolo", "Carlo", ""], ["Das", "Ariyam", ""], ["Gu", "Jiaqi", ""], ["Li", "Youfu", ""], ["li", "Mingda", ""], ["Wang", "Jin", ""]]}, {"id": "1910.09017", "submitter": "Maciej Besta", "authors": "Maciej Besta, Emanuel Peter, Robert Gerstenberger, Marc Fischer,\n  Micha{\\l} Podstawski, Claude Barthels, Gustavo Alonso, Torsten Hoefler", "title": "Demystifying Graph Databases: Analysis and Taxonomy of Data\n  Organization, System Designs, and Graph Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processing has become an important part of multiple areas of computer\nscience, such as machine learning, computational sciences, medical\napplications, social network analysis, and many others. Numerous graphs such as\nweb or social networks may contain up to trillions of edges. Often, these\ngraphs are also dynamic (their structure changes over time) and have\ndomain-specific rich data associated with vertices and edges. Graph database\nsystems such as Neo4j enable storing, processing, and analyzing such large,\nevolving, and rich datasets. Due to the sheer size of such datasets, combined\nwith the irregular nature of graph processing, these systems face unique design\nchallenges. To facilitate the understanding of this emerging domain, we present\nthe first survey and taxonomy of graph database systems. We focus on\nidentifying and analyzing fundamental categories of these systems (e.g., triple\nstores, tuple stores, native graph database systems, or object-oriented\nsystems), the associated graph models (e.g., RDF or Labeled Property Graph),\ndata organization techniques (e.g., storing graph data in indexing structures\nor dividing data into records), and different aspects of data distribution and\nquery execution (e.g., support for sharding and ACID). 45 graph database\nsystems are presented and compared, including Neo4j, OrientDB, or Virtuoso. We\noutline graph database queries and relationships with associated domains (NoSQL\nstores, graph streaming, and dynamic graph algorithms). Finally, we describe\nresearch and engineering challenges to outline the future of graph databases.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:45:15 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 13:15:02 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 14:19:20 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 09:54:33 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Besta", "Maciej", ""], ["Peter", "Emanuel", ""], ["Gerstenberger", "Robert", ""], ["Fischer", "Marc", ""], ["Podstawski", "Micha\u0142", ""], ["Barthels", "Claude", ""], ["Alonso", "Gustavo", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1910.09374", "submitter": "Pol del Aguila Pla", "authors": "Pol del Aguila Pla and Lissy Pellaco and Satyam Dwivedi and Peter\n  H\\\"andel and Joakim Jald\\'en", "title": "Clock synchronization over networks using sawtooth models", "comments": "5 pages, 2 figures, 2020 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP), 4-8 May 2020, Barcelona. arXiv admin\n  note: text overlap with arXiv:1906.08208", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DB cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clock synchronization and ranging over a wireless network with low\ncommunication overhead is a challenging goal with tremendous impact. In this\npaper, we study the use of time-to-digital converters in wireless sensors,\nwhich provides clock synchronization and ranging at negligible communication\noverhead through a sawtooth signal model for round trip times between two\nnodes. In particular, we derive Cram\\'{e}r-Rao lower bounds for a\nlinearitzation of the sawtooth signal model, and we thoroughly evaluate simple\nestimation techniques by simulation, giving clear and concise performance\nreferences for this technology.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:43:10 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 08:06:16 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Pla", "Pol del Aguila", ""], ["Pellaco", "Lissy", ""], ["Dwivedi", "Satyam", ""], ["H\u00e4ndel", "Peter", ""], ["Jald\u00e9n", "Joakim", ""]]}, {"id": "1910.10063", "submitter": "Wangda Zhang", "authors": "Wangda Zhang, Kenneth A. Ross", "title": "Exploiting Data Skew for Improved Query Performance", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2020.3006446", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic queries enable sophisticated large-scale data analysis within many\ncommercial, scientific and medical domains today. Data skew is a ubiquitous\nfeature of these real-world domains. In a retail database, some products are\ntypically much more popular than others. In a text database, word frequencies\nfollow a Zipf distribution with a small number of very common words, and a long\ntail of infrequent words. In a geographic database, some regions have much\nhigher populations (and data measurements) than others. Current systems do not\nmake the most of caches for exploiting skew. In particular, a whole cache line\nmay remain cache resident even though only a small part of the cache line\ncorresponds to a popular data item. In this paper, we propose a novel index\nstructure for repositioning data items to concentrate popular items into the\nsame cache lines. The net result is better spatial locality, and better\nutilization of limited cache resources. We develop a theoretical model for\nanalyzing the cache behavior, and implement database operators that are\nefficient in the presence of skew. Our experiments on real and synthetic data\nshow that exploiting skew can significantly improve in-memory query\nperformance. In some cases, our techniques can speed up queries by over an\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:05:02 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Zhang", "Wangda", ""], ["Ross", "Kenneth A.", ""]]}, {"id": "1910.10263", "submitter": "Christopher Buss", "authors": "Ben McCamish, Christopher Buss, Arash Termehchy, David Maier", "title": "Integrating Information About Entities Progressively", "comments": "demonstration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users often have to integrate information about entities from multiple data\nsources. This task is challenging as each data source may represent information\nabout the same entity in a distinct form, e.g., each data source may use a\ndifferent name for the same person. Currently, data from different\nrepresentations are translated into a unified one via lengthy and costly expert\nattention and tuning. Such methods cannot scale to the rapidly increasing\nnumber and variety of available data sources. We demonstrate ProgMap, a\nentity-matching framework in which data sources learn to collaborate and\nintegrate information about entities on-demand and with minimal expert\nintervention. The data sources leverage user feedback to improve the accuracy\nof their collaboration and results. ProgMap also has techniques to reduce the\namount of required user feedback to achieve effective matchings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 22:40:24 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["McCamish", "Ben", ""], ["Buss", "Christopher", ""], ["Termehchy", "Arash", ""], ["Maier", "David", ""]]}, {"id": "1910.10350", "submitter": "Thamir Qadah", "authors": "Thamir M. Qadah", "title": "A Queue-oriented Transaction Processing Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transaction processing has been an active area of research for several\ndecades. A fundamental characteristic of classical transaction processing\nprotocols is non-determinism, which causes them to suffer from performance\nissues on modern computing environments such as main-memory databases using\nmany-core, and multi-socket CPUs and distributed environments. Recent proposals\nof deterministic transaction processing techniques have shown great potential\nin addressing these performance issues. In this position paper, I argue for a\nqueue-oriented transaction processing paradigm that leads to better design and\nimplementation of deterministic transaction processing protocols. I support my\napproach with extensive experimental evaluations and demonstrate significant\nperformance gains.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 04:53:16 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Qadah", "Thamir M.", ""]]}, {"id": "1910.10363", "submitter": "Yan Gao", "authors": "Yan Gao, Jian-Guang Lou, Dongmei Zhang", "title": "A Hybrid Semantic Parsing Approach for Tabular Data Analysis", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to translating natural language\nquestions to SQL queries for given tables, which meets three requirements as a\nreal-world data analysis application: cross-domain, multilingualism and\nenabling quick-start. Our proposed approach consists of: (1) a novel data\nabstraction step before the parser to make parsing table-agnosticism; (2) a set\nof semantic rules for parsing abstracted data-analysis questions to\nintermediate logic forms as tree derivations to reduce the search space; (3) a\nneural-based model as a local scoring function on a span-based semantic parser\nfor structured optimization and efficient inference. Experiments show that our\napproach outperforms state-of-the-art algorithms on a large open benchmark\ndataset WikiSQL. We also achieve promising results on a small dataset for more\ncomplex queries in both English and Chinese, which demonstrates our language\nexpansion and quick-start ability.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:41:39 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 05:05:39 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gao", "Yan", ""], ["Lou", "Jian-Guang", ""], ["Zhang", "Dongmei", ""]]}, {"id": "1910.10421", "submitter": "Keisuke Nakano", "authors": "Keisuke Nakano", "title": "Towards a Complete Picture of Lens Laws", "comments": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional transformation, also called lens, has played important roles in\nmaintaining consistency in many fields of applications. A lens is specified by\na pair of forward and backward functions which relate to each other in a\nconsistent manner. The relation is formalized as a set of equations called lens\nlaws. This report investigates precise dependencies among lens laws: which law\nimplies another and which combination of laws implies another. The set of such\nimplications forms a complicated graph structure. It would be helpful to check\na well-definedness of bidirectional transformation in a lightweight way.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 09:10:56 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Nakano", "Keisuke", ""]]}, {"id": "1910.10959", "submitter": "Jumpei Tanaka", "authors": "Jumpei Tanaka, Van-Dang Tran, Hiroyuki kato, Zhenjiang Hu", "title": "Toward Co-existing Database Schemas based on Bidirectional\n  Transformation", "comments": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to strong demands for rapid and reliable software delivery,\nco-existing database schema versions with multiple application versions are\nreality to contribute them. Current database management systems do not support\nco-existing schema versions in one database. Although a design of co-existing\nschema based on updatable view tables was previously proposed, its flexibility\nis limited due to pre-defined several restrictions to achieve data\nsynchronization among schemas and handling independent unsynchronized data in\neach schema. In this preliminary report, we present a new approach for\nco-existing schemas based on bidirectional transformation. We explain the\nrequired properties to realize co-existing schemas, bidirectionality and\ntotality. We show that the co-existing schemas can be implemented\nsystematically by applying putback-based bidirectional transformation to\nsatisfy both the bidirectionality and the totality. While the bidirectionality\ncan be satisfied by applying bidirectional transformation, to satisfy the\ntotality, extra functions need to be introduced. How to derive these extra\nfunctions is presented.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 07:50:56 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 23:36:10 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Tanaka", "Jumpei", ""], ["Tran", "Van-Dang", ""], ["kato", "Hiroyuki", ""], ["Hu", "Zhenjiang", ""]]}, {"id": "1910.11040", "submitter": "Toshiyuki Shimizu", "authors": "Toshiyuki Shimizu, Hiroki Omori, Masatoshi Yoshikawa", "title": "Toward a view-based data cleaning architecture", "comments": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analysis has become an active area of study with the growth of\nmachine learning techniques. To properly analyze data, it is important to\nmaintain high-quality data. Thus, research on data cleaning is also important.\nIt is difficult to automatically detect and correct inconsistent values for\ndata requiring expert knowledge or data created by many contributors, such as\nintegrated data from heterogeneous data sources. An example of such data is\nmetadata for scientific datasets, which should be confirmed by data managers\nwhile handling the data. To support the efficient cleaning of data by data\nmanagers, we propose a data cleaning architecture in which data managers\ninteractively browse and correct portions of data through views. In this paper,\nwe explain our view-based data cleaning architecture and discuss some remaining\nissues.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:52:13 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Shimizu", "Toshiyuki", ""], ["Omori", "Hiroki", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "1910.11370", "submitter": "Caterina Strambio-De-Castillia Ph.D.", "authors": "Maximiliaan Huisman, Mathias Hammer, Alex Rigano, Ulrike Boehm, James\n  J. Chambers, Nathalie Gaudreault, Alison J. North, Jaime A. Pimentel, Damir\n  Sudar, Peter Bajcsy, Claire M. Brown, Alexander D. Corbett, Orestis Faklaris,\n  Judith Lacoste, Alex Laude, Glyn Nelson, Roland Nitschke, David Grunwald, and\n  Caterina Strambio-De-Castillia", "title": "A perspective on Microscopy Metadata: data provenance and quality\n  control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The application of microscopy in biomedical research has come a long way\nsince Antonie van Leeuwenhoek discovered unicellular organisms. Countless\ninnovations have positioned light microscopy as a cornerstone of modern biology\nand a method of choice for connecting omics datasets to their biological and\nclinical correlates. Still, regardless of how convincing published imaging data\nlooks, it does not always convey meaningful information about the conditions in\nwhich it was acquired, processed, and analyzed. Adequate record-keeping,\nreporting, and quality control are therefore essential to ensure experimental\nrigor and data fidelity, allow experiments to be reproducibly repeated, and\npromote the proper evaluation, interpretation, comparison, and re-use. To this\nend, microscopy images should be accompanied by complete descriptions detailing\nexperimental procedures, biological samples, microscope hardware\nspecifications, image acquisition parameters, and image analysis procedures, as\nwell as metrics accounting for instrument performance and calibration. However,\nuniversal, community-accepted Microscopy Metadata standards and reporting\nspecifications that would result in Findable Accessible Interoperable and\nReproducible (FAIR) microscopy data have not yet been established. To\nunderstand this shortcoming and to propose a way forward, here we provide an\noverview of the nature of microscopy metadata and its importance for fostering\ndata quality, reproducibility, scientific rigor, and sharing value in light\nmicroscopy. The proposal for tiered Microscopy Metadata Specifications that\nextend the OME Data Model put forth by the 4D Nucleome Initiative and by\nBioimaging North America [1-3] as well as a suite of three complementary and\ninteroperable tools are being developed to facilitate the process of image data\ndocumentation and are presented in related manuscripts [4-6].\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:33:17 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 21:22:33 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 23:35:45 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 22:56:39 GMT"}, {"version": "v5", "created": "Thu, 29 Apr 2021 19:37:00 GMT"}, {"version": "v6", "created": "Tue, 1 Jun 2021 03:36:07 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Huisman", "Maximiliaan", ""], ["Hammer", "Mathias", ""], ["Rigano", "Alex", ""], ["Boehm", "Ulrike", ""], ["Chambers", "James J.", ""], ["Gaudreault", "Nathalie", ""], ["North", "Alison J.", ""], ["Pimentel", "Jaime A.", ""], ["Sudar", "Damir", ""], ["Bajcsy", "Peter", ""], ["Brown", "Claire M.", ""], ["Corbett", "Alexander D.", ""], ["Faklaris", "Orestis", ""], ["Lacoste", "Judith", ""], ["Laude", "Alex", ""], ["Nelson", "Glyn", ""], ["Nitschke", "Roland", ""], ["Grunwald", "David", ""], ["Strambio-De-Castillia", "Caterina", ""]]}, {"id": "1910.11386", "submitter": "Shahan Ali Memon", "authors": "Shahan Ali Memon, Hira Dhamyal, Oren Wright, Daniel Justice,\n  Vijaykumar Palat, William Boler, Bhiksha Raj, Rita Singh", "title": "Detecting gender differences in perception of emotion in crowdsourced\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do men and women perceive emotions differently? Popular convictions place\nwomen as more emotionally perceptive than men. Empirical findings, however,\nremain inconclusive. Most prior studies focus on visual modalities. In\naddition, almost all of the studies are limited to experiments within\ncontrolled environments. Generalizability and scalability of these studies has\nnot been sufficiently established. In this paper, we study the differences in\nperception of emotion between genders from speech data in the wild, annotated\nthrough crowdsourcing. While we limit ourselves to a single modality (i.e.\nspeech), our framework is applicable to studies of emotion perception from all\nsuch loosely annotated data in general. Our paper addresses multiple serious\nchallenges related to making statistically viable conclusions from crowdsourced\ndata. Overall, the contributions of this paper are two fold: a reliable novel\nframework for perceptual studies from crowdsourced data; and the demonstration\nof statistically significant differences in speech-based emotion perception\nbetween genders.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 19:13:21 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 18:23:50 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 13:28:06 GMT"}, {"version": "v4", "created": "Mon, 4 Nov 2019 14:33:39 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Memon", "Shahan Ali", ""], ["Dhamyal", "Hira", ""], ["Wright", "Oren", ""], ["Justice", "Daniel", ""], ["Palat", "Vijaykumar", ""], ["Boler", "William", ""], ["Raj", "Bhiksha", ""], ["Singh", "Rita", ""]]}, {"id": "1910.11582", "submitter": "Ingo M\\\"uller", "authors": "Ingo M\\\"uller (1), Ghislain Fourny (1), Stefan Irimescu (2), Can\n  Berker Cikis, Gustavo Alonso (1) ((1) ETH Zurich, (2) Beekeeper AG)", "title": "Rumble: Data Independence for Large Messy Data Sets", "comments": "In revision for PVLDB 14, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces Rumble, a query execution engine for large,\nheterogeneous, and nested collections of JSON objects built on top of Apache\nSpark. While data sets of this type are more and more wide-spread, most\nexisting tools are built around a tabular data model, creating an impedance\nmismatch for both the engine and the query interface. In contrast, Rumble uses\nJSONiq, a standardized language specifically designed for querying JSON\ndocuments. The key challenge in the design and implementation of Rumble is\nmapping the recursive structure of JSON documents and JSONiq queries onto\nSpark's execution primitives based on tabular data frames. Our solution is to\ntranslate a JSONiq expression into a tree of iterators that dynamically switch\nbetween local and distributed execution modes depending on the nesting level.\nBy overcoming the impedance mismatch in the engine, Rumble frees the user from\nsolving the same problem for every single query, thus increasing their\nproductivity considerably. As we show in extensive experiments, Rumble is able\nto scale to large and complex data sets in the terabyte range with a similar or\nbetter performance than other engines. The results also illustrate that Codd's\nconcept of data independence makes as much sense for heterogeneous, nested data\nsets as it does on highly structured tables.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 09:04:03 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 08:27:54 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 21:00:48 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["M\u00fcller", "Ingo", "", "ETH Zurich"], ["Fourny", "Ghislain", "", "ETH Zurich"], ["Irimescu", "Stefan", "", "Beekeeper AG"], ["Cikis", "Can Berker", "", "ETH Zurich"], ["Alonso", "Gustavo", "", "ETH Zurich"]]}, {"id": "1910.11754", "submitter": "Diego Pennino", "authors": "Diego Pennino, Maurizio Pizzonia and Alessio Papi", "title": "Overlay Indexes: Efficiently Supporting Aggregate Range Queries and\n  Authenticated Data Structures in Off-the-Shelf Databases", "comments": null, "journal-ref": "IEEE Access, Volume 7, pages 175642 - 175670, year 2019", "doi": "10.1109/ACCESS.2019.2957346", "report-no": null, "categories": "cs.DB cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial off-the-shelf DataBase Management Systems (DBMSes) are highly\noptimized to process a wide range of queries by means of carefully designed\nindexing and query planning. However, many aggregate range queries are usually\nperformed by DBMSes using sequential scans, and certain needs, like storing\nAuthenticated Data Structures (ADS), are not supported at all. Theoretically,\nthese needs could be efficiently fulfilled adopting specific kinds of indexing,\nwhich however are normally ruled-out in DBMSes design.\n  We introduce the concept of overlay index: an index that is meant to be\nstored in a standard database, alongside regular data and managed by regular\nsoftware, to complement DBMS capabilities. We show a data structure, that we\ncall DB-tree, that realizes an overlay index to support a wide range of custom\naggregate range queries as well as ADSes, efficiently. All DB-trees operations\ncan be performed by executing a small number of queries to the DBMS, that can\nbe issued in parallel in one or two query rounds, and involves a logarithmic\namount of data. We experimentally evaluate the efficiency of DB-trees showing\nthat our approach is effective, especially if data updates are limited.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:24:35 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Pennino", "Diego", ""], ["Pizzonia", "Maurizio", ""], ["Papi", "Alessio", ""]]}, {"id": "1910.11927", "submitter": "Ajay Shrestha", "authors": "Ajay Kumar Shrestha, Julita Vassileva", "title": "User Data Sharing Frameworks: A Blockchain-Based Incentive Solution", "comments": "7 pages, 4 figure, 1 table, IEEE IEMCON 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DB cs.GT cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, there is no universal method to track who shared what, with whom,\nwhen and for what purposes in a verifiable way to create an individual\nincentive for data owners. A platform that allows data owners to control,\ndelete, and get rewards from sharing their data would be an important enabler\nof user data-sharing. We propose a usable blockchain- and smart contracts-based\nframework that allows users to store research data locally and share without\nlosing control and ownership of it. We have created smart contracts for\nbuilding automatic verification of the conditions for data access that also\nnaturally supports building up a verifiable record of the provenance,\nincentives for users to share their data and accountability of access. The\npaper presents a review of the existing work of research data sharing, the\nproposed blockchain-based framework and an evaluation of the framework by\nmeasuring the transaction cost for smart contracts deployment. The results show\nthat nodes responded quickly in all tested cases with a befitting transaction\ncost.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 20:39:43 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Shrestha", "Ajay Kumar", ""], ["Vassileva", "Julita", ""]]}, {"id": "1910.12001", "submitter": "Faisal Almutairi", "authors": "Faisal M. Almutairi, Charilaos I. Kanatsoulis, and Nicholas D.\n  Sidiropoulos", "title": "PREMA: Principled Tensor Data Recovery from Multiple Aggregated Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional data have become ubiquitous and are frequently encountered\nin situations where the information is aggregated over multiple data atoms. The\naggregation can be over time or other features, such as geographical location.\nWe often have access to multiple aggregated views of the same data, each\naggregated in one or more dimensions, especially when data are collected or\nmeasured by different agencies. For instance, item sales can be aggregated\ntemporally, and over groups of stores based on their location or affiliation.\nHowever, data mining and machine learning models benefit from detailed data for\npersonalized analysis and prediction. Thus, data disaggregation algorithms are\nbecoming increasingly important in various domains. The goal of this paper is\nto reconstruct finer-scale data from multiple coarse views, aggregated over\ndifferent (subsets of) dimensions. The proposed method, called PREMA, leverages\nlow-rank tensor factorization tools to fuse the multiple views and provide\nrecovery guarantees under certain conditions. PREMA can tackle challenging\nscenarios, such as missing or partially observed data, double aggregation, and\neven blind disaggregation (without knowledge of the aggregation patterns) using\na variant of PREMA called B-PREMA. To showcase the effectiveness of PREMA, the\npaper includes extensive experiments using real data from different domains:\nretail sales, crime counts, and weather observations.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 05:37:30 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 18:38:31 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Almutairi", "Faisal M.", ""], ["Kanatsoulis", "Charilaos I.", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1910.12201", "submitter": "Ruochen Jiang", "authors": "Ruochen Jiang, Changbo Qu, Jiannan Wang, Chi Wang, Yudian Zheng", "title": "Towards Extracting Highlights From Recorded Live Videos: An Implicit\n  Crowdsourcing Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Live streaming platforms need to store a lot of recorded live videos on a\ndaily basis. An important problem is how to automatically extract highlights\n(i.e., attractive short video clips) from these massive, long recorded live\nvideos. One approach is to directly apply a highlight extraction algorithm to\nvideo content. However, algorithmic approaches are either domain-specific,\nwhich require experts to spend a long time to design, or resource-intensive,\nwhich require a lot of training data and/or computing resources. In this paper,\nwe propose Lightor, a novel implicit crowdsourcing approach to overcome these\nlimitations. The key insight is to collect users' natural interactions with a\nlive streaming platform, and then leverage them to detect highlights. Lightor\nconsists of two major components. Highlight Initializer collects time-stamped\nchat messages from a live video and then uses them to predict approximate\nhighlight positions. Highlight Extractor keeps track of how users interact with\nthese approximate highlight positions and then refines these positions\niteratively. We find that the collected user chat and interaction data are very\nnoisy, and propose effective techniques to deal with noise. Lightor can be\neasily deployed into existing live streaming platforms, or be implemented as a\nweb browser extension. We recruit hundreds of users from Amazon Mechanical\nTurk, and evaluate the performance of Lightor using two popular games in\nTwitch. The results show that Lightor can achieve high extraction precision\nwith a small set of training data and low computing resources.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 07:42:04 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Jiang", "Ruochen", ""], ["Qu", "Changbo", ""], ["Wang", "Jiannan", ""], ["Wang", "Chi", ""], ["Zheng", "Yudian", ""]]}, {"id": "1910.12261", "submitter": "Mengxuan Zhang", "authors": "Mengxuan Zhang, Lei Li, Wen Hua, Xiaofang Zhou", "title": "Typical Snapshots Selection for Shortest Path Query in Dynamic Road\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the shortest paths in road network is an important query in our life\nnowadays, and various index structures are constructed to speed up the query\nanswering. However, these indexes can hardly work in real-life scenario because\nthe traffic condition changes dynamically, which makes the pathfinding slower\nthan in the static environment. In order to speed up path query answering in\nthe dynamic road network, we propose a framework to support these indexes.\nFirstly, we view the dynamic graph as a series of static snapshots. After that,\nwe propose two kinds of methods to select the typical snapshots. The first kind\nis time-based and it only considers the temporal information. The second\ncategory is the graph representation-based, which considers more insights:\nedge-based that captures the road continuity, and vertex-based that reflects\nthe region traffic fluctuation. Finally, we propose the snapshot matching to\nfind the most similar typical snapshot for the current traffic condition and\nuse its index to answer the query directly. Extensive experiments on real-life\nroad network and traffic conditions validate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 13:28:13 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhang", "Mengxuan", ""], ["Li", "Lei", ""], ["Hua", "Wen", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1910.12414", "submitter": "Lin Chen", "authors": "Lin Chen, Hossein Esfandiari, Thomas Fu, Vahab S. Mirrokni", "title": "Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss\n  and Beyond", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing approximate nearest neighbors in high dimensional spaces is a\ncentral problem in large-scale data mining with a wide range of applications in\nmachine learning and data science. A popular and effective technique in\ncomputing nearest neighbors approximately is the locality-sensitive hashing\n(LSH) scheme. In this paper, we aim to develop LSH schemes for distance\nfunctions that measure the distance between two probability distributions,\nparticularly for f-divergences as well as a generalization to capture mutual\ninformation loss. First, we provide a general framework to design LHS schemes\nfor f-divergence distance functions and develop LSH schemes for the generalized\nJensen-Shannon divergence and triangular discrimination in this framework. We\nshow a two-sided approximation result for approximation of the generalized\nJensen-Shannon divergence by the Hellinger distance, which may be of\nindependent interest. Next, we show a general method of reducing the problem of\ndesigning an LSH scheme for a Krein kernel (which can be expressed as the\ndifference of two positive definite kernels) to the problem of maximum inner\nproduct search. We exemplify this method by applying it to the mutual\ninformation loss, due to its several important applications such as model\ncompression.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:07:29 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Chen", "Lin", ""], ["Esfandiari", "Hossein", ""], ["Fu", "Thomas", ""], ["Mirrokni", "Vahab S.", ""]]}, {"id": "1910.13065", "submitter": "Pingfu Chao", "authors": "Pingfu Chao, Yehong Xu, Wen Hua, Xiaofang Zhou", "title": "A Survey on Map-Matching Algorithms", "comments": "12 pages, 5 figures, submitted to ADC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The map-matching is an essential preprocessing step for most of the\ntrajectory-based applications. Although it has been an active topic for more\nthan two decades and, driven by the emerging applications, is still under\ndevelopment. There is a lack of categorisation of existing solutions recently\nand analysis for future research directions. In this paper, we review the\ncurrent status of the map-matching problem and survey the existing algorithms.\nWe propose a new categorisation of the solutions according to their\nmap-matching models and working scenarios. In addition, we experimentally\ncompare three representative methods from different categories to reveal how\nmatching model affects the performance. Besides, the experiments are conducted\non multiple real datasets with different settings to demonstrate the influence\nof other factors in map-matching problem, like the trajectory quality, data\ncompression and matching latency.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 03:26:36 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Chao", "Pingfu", ""], ["Xu", "Yehong", ""], ["Hua", "Wen", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1910.13784", "submitter": "Abigail Goldsteen", "authors": "Abigail Goldsteen, Tomer Douek, Yaniv Cohen, Igor Gokhman, Ofir\n  Keren-Ackerman, Gadi Katsovich, Grisha Weintraub, and Doron Ben-Ari", "title": "Forgotten @ Scale: A Practical Solution for Implementing the Right To Be\n  Forgotten in Large-Scale Systems", "comments": null, "journal-ref": "1st International Workshop on Security and Privacy in Models and\n  Data (TRIDENT 2019)", "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The European General Data Protection Regulation asserts data subjects' right\nto be forgotten, i.e., their right to request that all their personal data be\ndeleted from an organizations' data stores. However, fulfilling such requests\nin large-scale systems is technically challenging. It requires that\norganizations keep track of all locations in which an individual's data is\nstored, be able to access and delete it in a reasonable time frame, and be able\nto prove that all such data was in fact deleted. In addition, organizations\nmust cope with complexities such as multiple, distributed, and continuously\nevolving systems of record, complex data retention policies and deletion\napproval workflows. We present a first design pattern and practical\nimplementation of the right to be forgotten on a large scale in Big Data and\ncloud environments.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 11:48:47 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Goldsteen", "Abigail", ""], ["Douek", "Tomer", ""], ["Cohen", "Yaniv", ""], ["Gokhman", "Igor", ""], ["Keren-Ackerman", "Ofir", ""], ["Katsovich", "Gadi", ""], ["Weintraub", "Grisha", ""], ["Ben-Ari", "Doron", ""]]}, {"id": "1910.14028", "submitter": "Boel Nelson", "authors": "Boel Nelson and Jenni Reuben", "title": "SoK: Chasing Accuracy and Privacy, and Catching Both in Differentially\n  Private Histogram Publication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histograms and synthetic data are of key importance in data analysis.\nHowever, researchers have shown that even aggregated data such as histograms,\ncontaining no obvious sensitive attributes, can result in privacy leakage. To\nenable data analysis, a strong notion of privacy is required to avoid risking\nunintended privacy violations.\n  Such a strong notion of privacy is differential privacy, a statistical notion\nof privacy that makes privacy leakage quantifiable. The caveat regarding\ndifferential privacy is that while it has strong guarantees for privacy,\nprivacy comes at a cost of accuracy. Despite this trade off being a central and\nimportant issue in the adoption of differential privacy, there exists a gap in\nthe literature regarding providing an understanding of the trade off and how to\naddress it appropriately.\n  Through a systematic literature review (SLR), we investigate the\nstate-of-the-art within accuracy improving differentially private algorithms\nfor histogram and synthetic data publishing. Our contribution is two-fold: 1)\nwe identify trends and connections in the contributions to the field of\ndifferential privacy for histograms and synthetic data and 2) we provide an\nunderstanding of the privacy/accuracy trade off challenge by crystallizing\ndifferent dimensions to accuracy improvement. Accordingly, we position and\nvisualize the ideas in relation to each other and external work, and\ndeconstruct each algorithm to examine the building blocks separately with the\naim of pinpointing which dimension of accuracy improvement each\ntechnique/approach is targeting. Hence, this systematization of knowledge (SoK)\nprovides an understanding of in which dimensions and how accuracy improvement\ncan be pursued without sacrificing privacy.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 10:19:01 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 09:54:32 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Nelson", "Boel", ""], ["Reuben", "Jenni", ""]]}]