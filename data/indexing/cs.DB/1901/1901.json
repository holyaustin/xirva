[{"id": "1901.00113", "submitter": "Jie Song", "authors": "Jie Song, Yichuan Zhang, Yubin Bao, Ge Yu", "title": "Probery: A Probability-based Incomplete Query Optimization for Big Data", "comments": "15 pages under the review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, query optimization has been highly concerned in big data\nmanagement, especially in NoSQL databases. Approximate queries boost query\nperformance by loss of accuracy, for example, sampling approaches trade off\nquery completeness for efficiency. Different from them, we propose an\nuncertainty of query completeness, called Probability of query Completeness (PC\nfor short). PC refers to the possibility that query results contain all\nsatisfied records. For example PC=0.95, it guarantees that there are no more\nthan 5 incomplete queries among 100 ones, but not guarantees how incomplete\nthey are. We trade off PC for query performance, and experiments show that a\nsmall loss of PC doubles query performance. The proposed Probery\n(PROBability-based data quERY) adopts the uncertainty of query completeness to\naccelerate OLTP queries. This paper illustrates the data and probability\nmodels, the probability based data placement and query processing, and the\nApache Drill-based implementation of Probery. In experiments, we first prove\nthat the percentage of complete queries is larger than the given PC confidence\nfor various cases, namely that the PC guarantee is validate. Then Probery is\ncompared with Drill, Impala and Hive in terms of query performance. The results\nindicate that Drill-based Probery performs as fast as Drill with complete\nquery, while averagely 1.8x, 1.3x and 1.6x faster than Drill, Impala and Hive\nwith possible complete query, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 08:09:25 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Song", "Jie", ""], ["Zhang", "Yichuan", ""], ["Bao", "Yubin", ""], ["Yu", "Ge", ""]]}, {"id": "1901.00228", "submitter": "Medha Atre", "authors": "Shubham S. Srivastava, Medha Atre, Shubham Sharma, Rahul Gupta,\n  Sandeep K. Shukla", "title": "Verity: Blockchains to Detect Insider Attacks in DBMS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrity and security of the data in database systems are typically\nmaintained with access control policies and firewalls. However, insider attacks\n-- where someone with an intimate knowledge of the system and administrative\nprivileges tampers with the data -- pose a unique challenge. Measures like\nappend only logging prove to be insufficient because an attacker with\nadministrative privileges can alter logs and login records to eliminate the\ntrace of attack, thus making insider attacks hard to detect.\n  In this paper, we propose Verity -- first of a kind system to the best of our\nknowledge. Verity serves as a dataless framework by which any blockchain\nnetwork can be used to store fixed-length metadata about tuples from any SQL\ndatabase, without complete migration of the database. Verity uses a formalism\nfor parsing SQL queries and query results to check the respective tuples'\nintegrity using blockchains to detect insider attacks. We have implemented our\ntechnique using Hyperledger Fabric, Composer REST API, and SQLite database.\nUsing TPC-H data and SQL queries of varying complexity and types, our\nexperiments demonstrate that any overhead of integrity checking remains\nconstant per tuple in a query's results, and scales linearly.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 01:02:56 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Srivastava", "Shubham S.", ""], ["Atre", "Medha", ""], ["Sharma", "Shubham", ""], ["Gupta", "Rahul", ""], ["Shukla", "Sandeep K.", ""]]}, {"id": "1901.00232", "submitter": "Shuai Ma", "authors": "Shuai Ma and Jinpeng Huai", "title": "Approximate Computation for Big Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past a few years, research and development has made significant\nprogresses on big data analytics. A fundamental issue for big data analytics is\nthe efficiency. If the optimal solution is unable to attain or not required or\nhas a price to high to pay, it is reasonable to sacrifice optimality with a\n`good' feasible solution that can be computed efficiently. Existing\napproximation techniques can be in general classified into approximation\nalgorithms, approximate query processing for aggregate SQL queries and\napproximation computing for multiple layers of the system stack. In this\narticle, we systematically introduce approximate computation, i.e., query\napproximation and data approximation, for efficiency and effectiveness big data\nanalytics. We first explain the idea and rationale of query approximation, and\nshow efficiency can be obtained with high effectiveness in practice with three\nanalytic tasks: graph pattern matching, trajectory compression and dense\nsubgraph computation. We then explain the idea and rationale of data\napproximation, and show efficiency can be obtained even without sacrificing for\neffectiveness in practice with three analytic tasks: shortest paths/distances,\nnetwork anomaly detection and link prediction.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 01:38:30 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Ma", "Shuai", ""], ["Huai", "Jinpeng", ""]]}, {"id": "1901.00671", "submitter": "Leila Ben Othman", "authors": "Leila Ben Othman", "title": "Une nouvelle approche de compl\\'etion des valeurs manquantes dans les\n  bases de donn\\'ees", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  When tackling real-life datasets, it is common to face the existence of\nscrambled missing values within data. Considered as 'dirty data', usually it is\nremoved during a pre-processing step. Starting from the fact that 'making up\nthis missing data is better than throwing out it away', we present a new\napproach trying to complete missing data. The main singularity of the\nintroduced approach is that it sheds light on a fruitful synergy between\ngeneric basis of association rules and the topic of missing values handling. In\nfact, beyond interesting compactness rate, such generic association rules make\nit possible to get a considerable reduction of conflicts during the completion\nstep. A new metric called 'Robustness' is also introduced, and aims to select\nthe robust association rule for the completion of a missing value whenever a\nconflict appears. Carried out experiments on benchmark datasets confirm the\nsoundness of our approach. Thus, it reduces conflict during the completion step\nwhile offering a high percentage of correct completion accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 10:30:14 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Othman", "Leila Ben", ""]]}, {"id": "1901.00716", "submitter": "Marmar Orooji", "authors": "Marmar Orooji, Gerald M. Knapp", "title": "Improving Suppression to Reduce Disclosure Risk and Enhance Data Utility", "comments": "6 pages, conference", "journal-ref": "Institute of Industrial and Systems Engineers (2018)", "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Privacy Preserving Data Publishing, various privacy models have been\ndeveloped for employing anonymization operations on sensitive individual level\ndatasets, in order to publish the data for public access while preserving the\nprivacy of individuals in the dataset. However, there is always a trade-off\nbetween preserving privacy and data utility; the more changes we make on the\nconfidential dataset to reduce disclosure risk, the more information the data\nloses and the less data utility it preserves. The optimum privacy technique is\nthe one that results in a dataset with minimum disclosure risk and maximum data\nutility. In this paper, we propose an improved suppression method, which\nreduces the disclosure risk and enhances the data utility by targeting the\nhighest risk records and keeping other records intact. We have shown the\neffectiveness of our approach through an experiment on a real-world\nconfidential dataset.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 18:48:34 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 01:36:58 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Orooji", "Marmar", ""], ["Knapp", "Gerald M.", ""]]}, {"id": "1901.00735", "submitter": "Paul Groth", "authors": "Adriane Chapman and Elena Simperl and Laura Koesten and George\n  Konstantinidis and Luis-Daniel Ib\\'a\\~nez-Gonzalez and Emilia Kacprzak and\n  Paul Groth", "title": "Dataset search: a survey", "comments": "20 pages, 153 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating value from data requires the ability to find, access and make\nsense of datasets. There are many efforts underway to encourage data sharing\nand reuse, from scientific publishers asking authors to submit data alongside\nmanuscripts to data marketplaces, open data portals and data communities.\nGoogle recently beta released a search service for datasets, which allows users\nto discover data stored in various online repositories via keyword queries.\nThese developments foreshadow an emerging research field around dataset search\nor retrieval that broadly encompasses frameworks, methods and tools that help\nmatch a user data need against a collection of datasets. Here, we survey the\nstate of the art of research and commercial systems in dataset retrieval. We\nidentify what makes dataset search a research field in its own right, with\nunique challenges and methods and highlight open problems. We look at\napproaches and implementations from related areas dataset search is drawing\nupon, including information retrieval, databases, entity-centric and tabular\nsearch in order to identify possible paths to resolve these open problems as\nwell as immediate next steps that will take the field forward.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 14:06:13 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Chapman", "Adriane", ""], ["Simperl", "Elena", ""], ["Koesten", "Laura", ""], ["Konstantinidis", "George", ""], ["Ib\u00e1\u00f1ez-Gonzalez", "Luis-Daniel", ""], ["Kacprzak", "Emilia", ""], ["Groth", "Paul", ""]]}, {"id": "1901.00902", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "A Model for Learned Bloom Filters, and Optimizing by Sandwiching", "comments": "12 pages; the complete version of the paper that appears in NIPS\n  2018, including addendum on learned Bloomier filters. arXiv admin note:\n  substantial text overlap with arXiv:1802.00884, arXiv:1803.01474", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has suggested enhancing Bloom filters by using a pre-filter,\nbased on applying machine learning to determine a function that models the data\nset the Bloom filter is meant to represent. Here we model such learned Bloom\nfilters,, with the following outcomes: (1) we clarify what guarantees can and\ncannot be associated with such a structure; (2) we show how to estimate what\nsize the learning function must obtain in order to obtain improved performance;\n(3) we provide a simple method, sandwiching, for optimizing learned Bloom\nfilters; and (4) we propose a design and analysis approach for a learned\nBloomier filter, based on our modeling approach.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 20:14:12 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "1901.01003", "submitter": "Xiangmin Zhou", "authors": "Xiangmin Zhou, Dong Qin, Xiaolu Lu, Lei Chen, Yanchun Zhang", "title": "Online Social Media Recommendation over Streams", "comments": "This paper appears at 35th IEEE International Conference on Data\n  Engineering (ICDE 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As one of the most popular services over online communities, the social\nrecommendation has attracted increasing research efforts recently. Among all\nthe recommendation tasks, an important one is social item recommendation over\nhigh speed social media streams. Existing streaming recommendation techniques\nare not effective for handling social users with diverse interests. Meanwhile,\napproaches for recommending items to a particular user are not efficient when\napplied to a huge number of users over high speed streams. In this paper, we\npropose a novel framework for the social recommendation over streaming\nenvironments. Specifically, we first propose a novel Bi-Layer Hidden Markov\nModel (BiHMM) that adaptively captures the behaviors of social users and their\ninteractions with influential official accounts to predict their long-term and\nshort-term interests. Then, we design a new probabilistic entity matching\nscheme for effectively identifying the relevance score of a streaming item to a\nuser. Following that, we propose a novel indexing scheme called {\\Tree} for\nimproving the efficiency of our solution. Extensive experiments are conducted\nto prove the high performance of our approach in terms of the recommendation\nquality and time cost.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 07:40:58 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Zhou", "Xiangmin", ""], ["Qin", "Dong", ""], ["Lu", "Xiaolu", ""], ["Chen", "Lei", ""], ["Zhang", "Yanchun", ""]]}, {"id": "1901.01328", "submitter": "Hongyu Miao", "authors": "Hongyu Miao, Myeongjae Jeon, Gennady Pekhimenko, Kathryn S. McKinley,\n  and Felix Xiaozhu Lin", "title": "StreamBox-HBM: Stream Analytics on High Bandwidth Hybrid Memory", "comments": null, "journal-ref": null, "doi": "10.1145/3297858.3304031", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream analytics have an insatiable demand for memory and performance.\nEmerging hybrid memories combine commodity DDR4 DRAM with 3D-stacked High\nBandwidth Memory (HBM) DRAM to meet such demands. However, achieving this\npromise is challenging because (1) HBM is capacity-limited and (2) HBM boosts\nperformance best for sequential access and high parallelism workloads. At first\nglance, stream analytics appear a particularly poor match for HBM because they\nhave high capacity demands and data grouping operations, their most demanding\ncomputations, use random access. This paper presents the design and\nimplementation of StreamBox-HBM, a stream analytics engine that exploits hybrid\nmemories to achieve scalable high performance. StreamBox-HBM performs data\ngrouping with sequential access sorting algorithms in HBM, in contrast to\nrandom access hashing algorithms commonly used in DRAM. StreamBox-HBM solely\nuses HBM to store Key Pointer Array (KPA) data structures that contain only\npartial records (keys and pointers to full records) for grouping operations. It\ndynamically creates and manages prodigious data and pipeline parallelism,\nchoosing when to allocate KPAs in HBM. It dynamically optimizes for both the\nhigh bandwidth and limited capacity of HBM, and the limited bandwidth and high\ncapacity of standard DRAM. StreamBox-HBM achieves 110 million records per\nsecond and 238 GB/s memory bandwidth while effectively utilizing all 64 cores\nof Intel's Knights Landing, a commercial server with hybrid memory. It\noutperforms stream engines with sequential access algorithms without KPAs by 7x\nand stream engines with random access algorithms by an order of magnitude in\nthroughput. To the best of our knowledge, StreamBox-HBM is the first stream\nengine optimized for hybrid memories.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 22:14:14 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 15:50:56 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Miao", "Hongyu", ""], ["Jeon", "Myeongjae", ""], ["Pekhimenko", "Gennady", ""], ["McKinley", "Kathryn S.", ""], ["Lin", "Felix Xiaozhu", ""]]}, {"id": "1901.01488", "submitter": "Florin Rusu", "authors": "Jun Hyung Shin, Florin Rusu, Alex Suhan", "title": "Exact Selectivity Computation for Modern In-Memory Database Query\n  Optimization", "comments": "Long version of the CIDR 2019 lightning talk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selectivity estimation remains a critical task in query optimization even\nafter decades of research and industrial development. Optimizers rely on\naccurate selectivities when generating execution plans. They maintain a large\nrange of statistical synopses for efficiently estimating selectivities.\nNonetheless, small errors -- propagated exponentially -- can lead to severely\nsub-optimal plans---especially, for complex predicates. Database systems for\nmodern computing architectures rely on extensive in-memory processing supported\nby massive multithread parallelism and vectorized instructions. However, they\nmaintain the same synopses approach to query optimization as traditional\ndisk-based databases. We introduce a novel query optimization paradigm for\nin-memory and GPU-accelerated databases based on \\textit{exact selectivity\ncomputation (ESC)}. The central idea in ESC is to compute selectivities exactly\nthrough queries during query optimization. In order to make the process\nefficient, we propose several optimizations targeting the selection and\nmaterialization of tables and predicates to which ESC is applied. We implement\nESC in the MapD open-source database system. Experiments on the TPC-H and SSB\nbenchmarks show that ESC records constant and less than 30 milliseconds\noverhead when running on GPU and generates improved query execution plans that\nare as much as 32X faster.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 02:20:57 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Shin", "Jun Hyung", ""], ["Rusu", "Florin", ""], ["Suhan", "Alex", ""]]}, {"id": "1901.01710", "submitter": "Yoshitaka Yamamoto", "authors": "Yoshitaka Yamamoto, Yasuo Tabei, and Koji Iwanuma", "title": "Approximate-Closed-Itemset Mining for Streaming Data Under Resource\n  Constraint", "comments": "14 pages, 16 figures, submitted to VLDB2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we present a novel algorithm for frequent itemset mining for streaming\ndata (FIM-SD). For the past decade, various FIM-SD methods in one-pass\napproximation settings have been developed to approximate the frequency of each\nitemset. These approaches can be categorized into two approximation types:\nparameter-constrained (PC) mining and resource-constrained (RC) mining. PC\nmethods control the maximum error that can be included in the frequency based\non a pre-defined parameter. In contrast, RC methods limit the maximum memory\nconsumption based on resource constraints. However, the existing PC methods can\nexponentially increase the memory consumption, while the existing RC methods\ncan rapidly increase the maximum error. In this study, we address this problem\nby introducing the notion of a condensed representation, called a\n$\\Delta$-covered set, to the RC approximation. This notion is regarded as an\nextension of the closedness compression and when $\\Delta = 0$, the solution\ncorresponds to an ordinary closed itemset. The algorithm searches for such\napproximate closed itemsets that can restore the frequent itemsets and their\nfrequencies under resource constraint while the maximum error is bounded by an\ninteger, $\\Delta$. We first propose a one-pass approximation algorithm to find\nthe condensed solution. Then, we improve the basic algorithm by introducing a\nunified PC-RC approximation approach. Finally, we empirically demonstrate that\nthe proposed algorithm significantly outperforms the state-of-the-art PC and RC\nmethods for FIM-SD.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 08:58:59 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Yamamoto", "Yoshitaka", ""], ["Tabei", "Yasuo", ""], ["Iwanuma", "Koji", ""]]}, {"id": "1901.01930", "submitter": "Joseph M. Hellerstein", "authors": "Joseph M. Hellerstein and Peter Alvaro", "title": "Keeping CALM: When Distributed Consistency is Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key concern in modern distributed systems is to avoid the cost of\ncoordination while maintaining consistent semantics. Until recently, there was\nno answer to the question of when coordination is actually required. In this\npaper we present an informal introduction to the CALM Theorem, which answers\nthis question precisely by moving up from traditional storage consistency to\nconsider properties of programs.\n  CALM is an acronym for \"consistency as logical monotonicity\". The CALM\nTheorem shows that the programs that have consistent, coordination-free\ndistributed implementations are exactly the programs that can be expressed in\nmonotonic logic. This theoretical result has practical implications for\ndevelopers of distributed applications. We show how CALM provides a\nconstructive application-level counterpart to conventional \"systems\" wisdom,\nsuch as the apparently negative results of the CAP Theorem. We also discuss\nways that monotonic thinking can influence distributed systems design, and how\nnew programming language designs and tools can help developers write\nconsistent, coordination-free code.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 17:20:25 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 00:16:08 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Hellerstein", "Joseph M.", ""], ["Alvaro", "Peter", ""]]}, {"id": "1901.01944", "submitter": "Diego Seco", "authors": "Nataly Cruces, Diego Seco, Gilberto Guti\\'errez", "title": "A Compact Representation of Raster Time Series", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Proceedings of the Data Compression Conference (DCC 2019)", "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The raster model is widely used in Geographic Information Systems to\nrepresent data that vary continuously in space, such as temperatures,\nprecipitations, elevation, among other spatial attributes. In applications like\nweather forecast systems, not just a single raster, but a sequence of rasters\ncovering the same region at different timestamps, known as a raster time\nseries, needs to be stored and queried. Compact data structures have proven\nsuccessful to provide space-efficient representations of rasters with query\ncapabilities. Hence, a naive approach to save space is to use such a\nrepresentation for each raster in a time series. However, in this paper we show\nthat it is possible to take advantage of the temporal locality that exists in a\nraster time series to reduce the space necessary to store it while keeping\ncompetitive query times for several types of queries.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 17:52:23 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Cruces", "Nataly", ""], ["Seco", "Diego", ""], ["Guti\u00e9rrez", "Gilberto", ""]]}, {"id": "1901.01973", "submitter": "Joseph M. Hellerstein", "authors": "Joseph M. Hellerstein", "title": "Looking Back at Postgres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is a recollection of the UC Berkeley Postgres project, which was led by\nMike Stonebraker from the mid-1980's to the mid-1990's. The article was\nsolicited for Stonebraker's Turing Award book, as one of many\npersonal/historical recollections. As a result it focuses on Stonebraker's\ndesign ideas and leadership. But Stonebraker was never a coder, and he stayed\nout of the way of his development team. The Postgres codebase was the work of a\nteam of brilliant students and the occasional university \"staff programmers\"\nwho had little more experience (and only slightly more compensation) than the\nstudents. I was lucky to join that team as a student during the latter years of\nthe project. I got helpful input on this writeup from some of the more senior\nstudents on the project, but any errors or omissions are mine. If you spot any\nsuch, please contact me and I will try to fix them.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 18:59:51 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Hellerstein", "Joseph M.", ""]]}, {"id": "1901.02049", "submitter": "Jaroslaw Szlichta", "authors": "Guilherme Damasio, Vincent Corvinelli, Parke Godfrey, Piotr\n  Mierzejewski, Alexandar Mihaylov, Jaroslaw Szlichta, Calisto Zuzarte", "title": "Guided Automated Learning for query workload re-Optimization", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query optimization is a hallmark of database systems enabling complex SQL\nqueries of today's applications to be run efficiently. The query optimizer\noften fails to find the best plan, when logical subtleties in business queries\nand schemas circumvent it. When a query runs more expensively than is viable or\nwarranted, determination of the performance issues is usually performed\nmanually in consultation with experts through the analysis of query's execution\nplan (QEP). However, this is an excessively time consuming, human error-prone,\nand costly process. GALO is a novel system that automates this process. The\ntool automatically learns recurring problem patterns in query plans over\nworkloads in an offline learning phase, to build a knowledge base of\nplan-rewrite remedies. It then uses the knowledge base online to re-optimize\nqueries queued for execution to improve performance, often quite drastically.\n  GALO's knowledge base is built on RDF and SPARQL, W3C graph database\nstandards, which is well suited for manipulating and querying over SQL query\nplans, which are graphs themselves. GALO acts as a third-tier of\nre-optimization, after query rewrite and cost-based optimization, as a query\nplan rewrite. Since the knowledge base is not tied to the context of supplied\nQEPs, table and column names are matched automatically during the\nre-optimization phase. Thus, problem patterns learned over a particular query\nworkload can be applied in other query workloads. GALO's knowledge base is also\nan invaluable tool for database experts to debug query performance issues by\ntracking to known issues and solutions as well as refining the optimizer with\nnew tuned techniques by the development team. We demonstrate an experimental\nstudy of the effectiveness of our techniques over synthetic TPC-DS and real IBM\nclient query workloads.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 20:21:31 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 16:28:21 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 17:36:02 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Damasio", "Guilherme", ""], ["Corvinelli", "Vincent", ""], ["Godfrey", "Parke", ""], ["Mierzejewski", "Piotr", ""], ["Mihaylov", "Alexandar", ""], ["Szlichta", "Jaroslaw", ""], ["Zuzarte", "Calisto", ""]]}, {"id": "1901.03179", "submitter": "Sourav Mukherjee", "authors": "Sourav Mukherjee", "title": "Popular SQL Server Database Encryption Choices", "comments": "6 pages, 3 figures, Published with International Journal of Computer\n  Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT).\n  Volume 66 Number 1. December 2018. Published by Seventh Sense Research Group", "doi": "10.14445/22312803/IJCTT-V66P103", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives an overview of different database encryption choices in\nSQL Server. Which one works best in which situation. In today's world Data is\nmore crucial than the expensive hardware cost. No one wants their personal data\nto be comprised. Same for business houses as well and they also do not want\ntheir data to be inappropriately handled to go out of the business. To help\nprotect the public rights and safety, recently this year, the European Union\nhad come up with strict rules and regulation of GDPR (General Data Protection\nRegulation).\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 02:53:49 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 22:52:03 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Mukherjee", "Sourav", ""]]}, {"id": "1901.03633", "submitter": "Nicolas Crosetti", "authors": "Florent Capelli, Nicolas Crosetti, Joachim Niehren, Jan Ramon", "title": "Solving linear programs on factorized databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical workflow for solving a linear programming problem is to first write\na linear program parametrized by the data in a language such as Math GNU Prog\nor AMPL then call the solver on this program while providing the data. When the\ndata is extracted using a query on a database, this approach ignores the\nunderlying structure of the answer set which may result in a blow-up of the\nsize of the linear program if the answer set is big. In this paper, we study\nthe problem of solving linear programming problems whose variables are the\nanswers to a conjunctive query. We show that one can exploit the structure of\nthe query to rewrite the linear program so that its size depends only on the\nsize of the database and not on the size of the answer set. More precisely, we\ngive a generic way of rewriting a linear program whose variables are the tuples\nin Q(D) for a conjunctive query Q and a database D into a linear program having\na number of variables that only depends on the size of a factorized\nrepresentation of Q(D), which can be much smaller when the fractional hypertree\nwidth of Q is bounded.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 16:22:26 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 14:05:00 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Capelli", "Florent", ""], ["Crosetti", "Nicolas", ""], ["Niehren", "Joachim", ""], ["Ramon", "Jan", ""]]}, {"id": "1901.03772", "submitter": "Roberto Palmieri", "authors": "Masoomeh Javidi Kishi, Sebastiano Peluso, Hank Korth, Roberto Palmieri", "title": "SSS: Scalable Key-Value Store with External Consistent and Abort-free\n  Read-only Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SSS, a scalable transactional key-value store deploying a novel\ndistributed concurrency control that provides external consistency for all\ntransactions, never aborts read-only transactions due to concurrency, all\nwithout specialized hardware. SSS ensures the above properties without any\ncentralized source of synchronization. SSS's concurrency control uses a\ncombination of vector clocks and a new technique, called snapshot-queuing, to\nestablish a single transaction serialization order that matches the order of\ntransaction completion observed by clients. We compare SSS against high\nperformance key-value stores, Walter, ROCOCO, and a two-phase commit baseline.\nSSS outperforms 2PC-baseline by as much as 7x using 20 nodes; and ROCOCO by as\nmuch as 2.2x with long read-only transactions using 15 nodes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 23:46:05 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Kishi", "Masoomeh Javidi", ""], ["Peluso", "Sebastiano", ""], ["Korth", "Hank", ""], ["Palmieri", "Roberto", ""]]}, {"id": "1901.03897", "submitter": "Andreas Pieris", "authors": "Tomasz Gogacz, Jerzy Marcinkowski, Andreas Pieris", "title": "All-Instances Restricted Chase Termination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chase procedure is a fundamental algorithmic tool in database theory with\na variety of applications. A key problem concerning the chase procedure is\nall-instances termination: for a given set of tuple-generating dependencies\n(TGDs), is it the case that the chase terminates for every input database? In\nview of the fact that this problem is undecidable, it is natural to ask whether\nknown well-behaved classes of TGDs ensure decidability. We consider here the\nmain paradigms that led to robust TGD-based formalisms, that is, guardedness\nand stickiness. Although all-instances termination is well-understood for the\noblivious version of the chase, the more subtle case of the restricted (a.k.a.\nthe standard) chase is rather unexplored. We show that all-instances restricted\nchase termination for guarded and sticky single-head TGDs is decidable.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 20:51:58 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 18:06:06 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Gogacz", "Tomasz", ""], ["Marcinkowski", "Jerzy", ""], ["Pieris", "Andreas", ""]]}, {"id": "1901.04182", "submitter": "Liat Peterfreund", "authors": "Liat Peterfreund, Dominik D. Freydenberger, Benny Kimelfeld, Markus\n  Kr\\\"oll", "title": "Complexity Bounds for Relational Algebra over Document Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of evaluating queries in Relational Algebra\n(RA) over the relations extracted by regex formulas (i.e., regular expressions\nwith capture variables) over text documents. Such queries, also known as the\nregular document spanners, were shown to have an evaluation with polynomial\ndelay for every positive RA expression (i.e., consisting of only natural joins,\nprojections and unions); here, the RA expression is fixed and the input\nconsists of both the regex formulas and the document. In this work, we explore\nthe implication of two fundamental generalizations. The first is adopting the\n\"schemaless\" semantics for spanners, as proposed and studied by Maturana et al.\nThe second is going beyond the positive RA to allowing the difference operator.\nWe show that each of the two generalizations introduces computational hardness:\nit is intractable to compute the natural join of two regex formulas under the\nschemaless semantics, and the difference between two regex formulas under both\nthe ordinary and schemaless semantics. Nevertheless, we propose and analyze\nsyntactic constraints, on the RA expression and the regex formulas at hand,\nsuch that the expressive power is fully preserved and, yet, evaluation can be\ndone with polynomial delay. Unlike the previous work on RA over regex formulas,\nour technique is not (and provably cannot be) based on the static compilation\nof regex formulas, but rather on an ad-hoc compilation into an automaton that\nincorporates both the query and the document. This approach also allows us to\ninclude black-box extractors in the RA expression.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 08:38:24 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 07:37:32 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Peterfreund", "Liat", ""], ["Freydenberger", "Dominik D.", ""], ["Kimelfeld", "Benny", ""], ["Kr\u00f6ll", "Markus", ""]]}, {"id": "1901.04452", "submitter": "Nicholas Schiefer", "authors": "Christos Chrysafis, Ben Collins, Scott Dugas, Jay Dunkelberger, Moussa\n  Ehsan, Scott Gray, Alec Grieser, Ori Herrnstadt, Kfir Lev-Ari, Tao Lin, Mike\n  McMahon, Nicholas Schiefer, Alexander Shraer", "title": "FoundationDB Record Layer: A Multi-Tenant Structured Datastore", "comments": "16 pages; updated to reflect reviewer suggestions and fix typos", "journal-ref": null, "doi": "10.1145/3299869.3314039", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FoundationDB Record Layer is an open source library that provides a\nrecord-oriented data store with semantics similar to a relational database\nimplemented on top of FoundationDB, an ordered, transactional key-value store.\nThe Record Layer provides a lightweight, highly extensible way to store\nstructured data. It offers schema management and a rich set of query and\nindexing facilities, some of which are not usually found in traditional\nrelational databases, such as nested record types, indexes on commit versions,\nand indexes that span multiple record types. The Record Layer is stateless and\nbuilt for massive multi-tenancy, encapsulating and isolating all of a tenant's\nstate, including indexes, into a separate logical database. We demonstrate how\nthe Record Layer is used by CloudKit, Apple's cloud backend service, to provide\npowerful abstractions to applications serving hundreds of millions of users.\nCloudKit uses the Record Layer to host billions of independent databases, many\nwith a common schema. Features provided by the Record Layer enable CloudKit to\nprovide richer APIs and stronger semantics with reduced maintenance overhead\nand improved scalability.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 18:36:25 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 23:52:51 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chrysafis", "Christos", ""], ["Collins", "Ben", ""], ["Dugas", "Scott", ""], ["Dunkelberger", "Jay", ""], ["Ehsan", "Moussa", ""], ["Gray", "Scott", ""], ["Grieser", "Alec", ""], ["Herrnstadt", "Ori", ""], ["Lev-Ari", "Kfir", ""], ["Lin", "Tao", ""], ["McMahon", "Mike", ""], ["Schiefer", "Nicholas", ""], ["Shraer", "Alexander", ""]]}, {"id": "1901.04954", "submitter": "Pavlos Fafalios", "authors": "Pavlos Fafalios, Yannis Tzitzikas", "title": "How Many and What Types of SPARQL Queries can be Answered through\n  Zero-Knowledge Link Traversal?", "comments": "Preprint of paper accepted for publication in the 34th ACM/SIGAPP\n  Symposium On Applied Computing (SAC 2019)", "journal-ref": null, "doi": "10.1145/3297280.3297505", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current de-facto way to query the Web of Data is through the SPARQL\nprotocol, where a client sends queries to a server through a SPARQL endpoint.\nContrary to an HTTP server, providing and maintaining a robust and reliable\nendpoint requires a significant effort that not all publishers are willing or\nable to make. An alternative query evaluation method is through link traversal,\nwhere a query is answered by dereferencing online web resources (URIs) at real\ntime. While several approaches for such a lookup-based query evaluation method\nhave been proposed, there exists no analysis of the types (patterns) of queries\nthat can be directly answered on the live Web, without accessing local or\nremote endpoints and without a-priori knowledge of available data sources. In\nthis paper, we first provide a method for checking if a SPARQL query (to be\nevaluated on a SPARQL endpoint) can be answered through zero-knowledge link\ntraversal (without accessing the endpoint), and analyse a large corpus of real\nSPARQL query logs for finding the frequency and distribution of answerable and\nnon-answerable query patterns. Subsequently, we provide an algorithm for\ntransforming answerable queries to SPARQL-LD queries that bypass the endpoints.\nWe report experimental results about the efficiency of the transformed queries\nand discuss the benefits and the limitations of this query evaluation method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 12:09:37 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fafalios", "Pavlos", ""], ["Tzitzikas", "Yannis", ""]]}, {"id": "1901.05152", "submitter": "Immanuel Trummer Mr.", "authors": "Immanuel Trummer, Junxiong Wang, Deepak Maram, Samuel Moseley, Saehan\n  Jo, Joseph Antonakakis", "title": "SkinnerDB: Regret-Bounded Query Evaluation via Reinforcement Learning", "comments": null, "journal-ref": "SIGMOD 2019", "doi": "10.1145/3299869.3300088", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SkinnerDB is designed from the ground up for reliable join ordering. It\nmaintains no data statistics and uses no cost or cardinality models. Instead,\nit uses reinforcement learning to learn optimal join orders on the fly, during\nthe execution of the current query. To that purpose, we divide the execution of\na query into many small time slices. Different join orders are tried in\ndifferent time slices. We merge result tuples generated according to different\njoin orders until a complete result is obtained. By measuring execution\nprogress per time slice, we identify promising join orders as execution\nproceeds.\n  Along with SkinnerDB, we introduce a new quality criterion for query\nexecution strategies. We compare expected execution cost against execution cost\nfor an optimal join order. SkinnerDB features multiple execution strategies\nthat are optimized for that criterion. Some of them can be executed on top of\nexisting database systems. For maximal performance, we introduce a customized\nexecution engine, facilitating fast join order switching via specialized\nmulti-way join algorithms and tuple representations.\n  We experimentally compare SkinnerDB's performance against various baselines,\nincluding MonetDB, Postgres, and adaptive processing methods. We consider\nvarious benchmarks, including the join order benchmark and TPC-H variants with\nuser-defined functions. Overall, the overheads of reliable join ordering are\nnegligible compared to the performance impact of the occasional, catastrophic\njoin order choice.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 06:38:11 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Trummer", "Immanuel", ""], ["Wang", "Junxiong", ""], ["Maram", "Deepak", ""], ["Moseley", "Samuel", ""], ["Jo", "Saehan", ""], ["Antonakakis", "Joseph", ""]]}, {"id": "1901.05451", "submitter": "Yankai Chen", "authors": "Yankai Chen, Yixiang Fang, Reynold Cheng, Yun Li, Xiaojun Chen, Jie\n  Zhang", "title": "Exploring Communities in Large Profiled Graphs", "comments": null, "journal-ref": "IEEE Transactions on Knowledge and Data Engineering 2018", "doi": "10.1109/TKDE.2018.2882837", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$ and a vertex $q\\in G$, the community search (CS) problem\naims to efficiently find a subgraph of $G$ whose vertices are closely related\nto $q$. Communities are prevalent in social and biological networks, and can be\nused in product advertisement and social event recommendation. In this paper,\nwe study profiled community search (PCS), where CS is performed on a profiled\ngraph. This is a graph in which each vertex has labels arranged in a\nhierarchical manner. Extensive experiments show that PCS can identify\ncommunities with themes that are common to their vertices, and is more\neffective than existing CS approaches. As a naive solution for PCS is highly\nexpensive, we have also developed a tree index, which facilitate efficient and\nonline solutions for PCS.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 13:38:12 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Chen", "Yankai", ""], ["Fang", "Yixiang", ""], ["Cheng", "Reynold", ""], ["Li", "Yun", ""], ["Chen", "Xiaojun", ""], ["Zhang", "Jie", ""]]}, {"id": "1901.06208", "submitter": "Otmane Azeroual", "authors": "Otmane Azeroual, Gunter Saake and Mohammad Abuosba", "title": "Data Quality Measures and Data Cleansing for Research Information\n  Systems", "comments": "16(1), pp.12-21", "journal-ref": "Journal of Digital Information Management, Digital Information\n  Research Foundation, 2018", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The collection, transfer and integration of research information into\ndifferent research Information systems can result in different data errors that\ncan have a variety of negative effects on data quality. In order to detect\nerrors at an early stage and treat them efficiently, it is necessary to\ndetermine the clean-up measures and the new techniques of data cleansing for\nquality improvement in research institutions. Thereby an adequate and reliable\nbasis for decision-making using an RIS is provided, and confidence in a given\ndataset increased. In this paper, possible measures and the new techniques of\ndata cleansing for improving and increasing the data quality in research\ninformation systems will be presented and how these are to be applied to the\nResearch information.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 12:59:59 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Azeroual", "Otmane", ""], ["Saake", "Gunter", ""], ["Abuosba", "Mohammad", ""]]}, {"id": "1901.06238", "submitter": "Massimiliano Morrelli", "authors": "Michele Gentile, Massimiliano Morrelli", "title": "Integrazione di Apache Hive con Spark", "comments": "in Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  English. This document describes the solutions adopted, which arose from the\nneed to transfer a large amount of information between the most famous\ndistributed SQL and NoSQL storage systems to perform analysis and/or\nmodification operations exploiting the peculiarities of the same. The goal was\nachieved using the Spark engine and studying and using the open source library\n\"Hive Warehouse Connector\" made by Hortonworks. It provides new\ninteroperability features between Hive and Spark. The choice fell on these APIs\nin order to take advantage from Spark's distributed computing through Spark-Sql\nlibraries, to allow a quick reading and writing on the databases chosen by the\nNetwork Contacts Systems Engineering Team and to make the stored information\navailable for consultation outside the Ambari cluster.\n  Italiano. Il presente documento descrive le soluzioni adottate, nate dalla\nnecessit\\`a di trasferire un elevato numero di informazioni tra i pi\\`u famosi\nsistemi distribuiti di archiviazione SQL e NoSQL per effettuare operazioni di\nanalisi e/o modifica sfruttando le peculiarit\\`a degli stessi. L'obiettivo \\`e\nstato raggiunto utilizzando l'engine Spark e studiando e utilizzando la\nlibreria open source \"Hive Warehouse Connector\" messa a disposizione da\nHortonworks che fornisce nuove funzionalit\\`a di interoperabilit\\`a tra Hive e\nSpark. La scelta \\`e ricaduta su queste API per poter avvalersi del calcolo\ndistribuito di Spark mediante le librerie di Spark-Sql, per consentire una\nrapida lettura e scrittura sui database scelti dal team di Ingegneria dei\nSistemi di Network Contacts al fine di rendere consultabili le informazioni\narchiviate all'esterno del cluster Ambari.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 13:41:39 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Gentile", "Michele", ""], ["Morrelli", "Massimiliano", ""]]}, {"id": "1901.06491", "submitter": "Huan Zhou", "authors": "H. Zhou, J. W. Guo, H. Q. Hu, W. N. Qian, X. Zhou, A. Y. Zhou", "title": "Guaranteeing Recoverability via Partially Constrained Transaction Logs", "comments": "13 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transaction logging is an essential constituent to guarantee the atomicity\nand durability in online transaction processing (OLTP) systems. It always has a\nconsiderable impact on performance, especially in an in-memory database system.\nConventional implementations of logging rely heavily on a centralized design,\nwhich guarantees the correctness of recovery by enforcing a total order of all\noperations such as log sequence number (LSN) allocation, log persistence,\ntransaction committing and recovering. This strict sequential constraint\nseriously limits the scalability and parallelism of transaction logging and\nrecovery, especially in the multi-core hardware environment.\n  In this paper, we define recoverability for transaction logging and\ndemonstrate its correctness for crash recovery. Based on recoverability, we\npropose a recoverable logging scheme named Poplar, which enables scalable and\nparallel log processing by easing the restrictions. Its main advantages are\nthat (1) Poplar enables the parallel log persistence on multiple storage\ndevices; (2) it replaces the centralized LSN allocation by calculating a\npartially ordered sequence number in a distributed manner, which allows log\nrecords to only track RAW and WAW dependencies among transactions; (3) it only\ndemands transactions with RAW dependencies to be committed in serial order; (4)\nPoplar can concurrently restore a consistent database state based on the\npartially constrained logs after a crash. Experimental results show that Poplar\nscales well with the increase of IO devices and outperforms other logging\napproaches on both SSDs and emulated non-volatile memory.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 09:42:30 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Zhou", "H.", ""], ["Guo", "J. W.", ""], ["Hu", "H. Q.", ""], ["Qian", "W. N.", ""], ["Zhou", "X.", ""], ["Zhou", "A. Y.", ""]]}, {"id": "1901.06862", "submitter": "Pierre Senellart", "authors": "Silviu Maniu, Pierre Senellart, Suraj Jog", "title": "An Experimental Study of the Treewidth of Real-World Graph Data\n  (Extended Version)", "comments": "Extended version of an article published in the proceedings of ICDT\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Treewidth is a parameter that measures how tree-like a relational instance\nis, and whether it can reasonably be decomposed into a tree. Many computation\ntasks are known to be tractable on databases of small treewidth, but computing\nthe treewidth of a given instance is intractable. This article is the first\nlarge-scale experimental study of treewidth and tree decompositions of\nreal-world database instances (25 datasets from 8 different domains, with sizes\nranging from a few thousand to a few million vertices). The goal is to\ndetermine which data, if any, can benefit of the wealth of algorithms for\ndatabases of small treewidth. For each dataset, we obtain upper and lower bound\nestimations of their treewidth, and study the properties of their tree\ndecompositions. We show in particular that, even when treewidth is high, using\npartial tree decompositions can result in data structures that can assist\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 10:35:53 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Maniu", "Silviu", ""], ["Senellart", "Pierre", ""], ["Jog", "Suraj", ""]]}, {"id": "1901.07064", "submitter": "Joy Arulraj", "authors": "Joy Arulraj, Ran Xian, Lin Ma, Andrew Pavlo", "title": "Predictive Indexing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable research on automated index tuning in database\nmanagement systems (DBMSs). But the majority of these solutions tune the index\nconfiguration by retrospectively making computationally expensive physical\ndesign changes all at once. Such changes degrade the DBMS's performance during\nthe process, and have reduced utility during subsequent query processing due to\nthe delay between a workload shift and the associated change. A better approach\nis to generate small changes that tune the physical design over time, forecast\nthe utility of these changes, and apply them ahead of time to maximize their\nimpact.\n  This paper presents predictive indexing that continuously improves a\ndatabase's physical design using lightweight physical design changes. It uses a\nmachine learning model to forecast the utility of these changes, and\ncontinuously refines the index configuration of the database to handle evolving\nworkloads. We introduce a lightweight hybrid scan operator with which a DBMS\ncan make use of partially-built indexes for query processing. Our evaluation\nshows that predictive indexing improves the throughput of a DBMS by 3.5--5.2x\ncompared to other state-of-the-art indexing approaches. We demonstrate that\npredictive indexing works seamlessly with other lightweight automated physical\ndesign tuning methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 20:16:47 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Arulraj", "Joy", ""], ["Xian", "Ran", ""], ["Ma", "Lin", ""], ["Pavlo", "Andrew", ""]]}, {"id": "1901.07388", "submitter": "Otmane Azeroual", "authors": "Otmane Azeroual and Mohammad Abuosba", "title": "Improving the data quality in the research information systems", "comments": "15(11), pp. 82-86. arXiv admin note: substantial text overlap with\n  arXiv:1901.06208", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, 2017", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In order to introduce an integrated research information system, this will\nprovide scientific institutions with the necessary information on research\nactivities and research results in assured quality. Since data collection,\nduplication, missing values, incorrect formatting, inconsistencies, etc. can\narise in the collection of research data in different research information\nsystems, which can have a wide range of negative effects on data quality, the\nsubject of data quality should be treated with better results. This paper\nexamines the data quality problems in research information systems and presents\nthe new techniques that enable organizations to improve their quality of\nresearch information.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 13:34:26 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Azeroual", "Otmane", ""], ["Abuosba", "Mohammad", ""]]}, {"id": "1901.07627", "submitter": "Oliver Kennedy", "authors": "Darshana Balakrishnan, Lukasz Ziarek, Oliver Kennedy", "title": "Just-in-Time Index Compilation", "comments": "Work Supported by NSF Award #IIS-1617586", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating or modifying a primary index is a time-consuming process, as the\nindex typically needs to be rebuilt from scratch. In this paper, we explore a\nmore graceful \"just-in-time\" approach to index reorganization, where small\nchanges are dynamically applied in the background. To enable this type of\nreorganization, we formalize a composable organizational grammar, expressive\nenough to capture instances of not only existing index structures, but\narbitrary hybrids as well. We introduce an algebra of rewrite rules for such\nstructures, and a framework for defining and optimizing policies for\njust-in-time rewriting. Our experimental analysis shows that the resulting\nindex structure is flexible enough to adapt to a variety of performance goals,\nwhile also remaining competitive with existing structures like the C++ standard\ntemplate library map.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 22:19:17 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Balakrishnan", "Darshana", ""], ["Ziarek", "Lukasz", ""], ["Kennedy", "Oliver", ""]]}, {"id": "1901.07655", "submitter": "Farhad Shirani Chaharsooghi", "authors": "Farhad Shirani, Siddharth Garg and Elza Erkip", "title": "A Concentration of Measure Approach to Database De-anonymization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, matching of correlated high-dimensional databases is\ninvestigated. A stochastic database model is considered where the correlation\namong the database entries is governed by an arbitrary joint distribution.\nConcentration of measure theorems such as typicality and laws of large numbers\nare used to develop a database matching scheme and derive necessary conditions\nfor successful matching. Furthermore, it is shown that these conditions are\ntight through a converse result which characterizes a set of distributions on\nthe database entries for which reliable matching is not possible. The necessary\nand sufficient conditions for reliable matching are evaluated in the cases when\nthe database entries are independent and identically distributed as well as\nunder Markovian database models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 00:21:48 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 23:24:24 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 01:47:58 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Shirani", "Farhad", ""], ["Garg", "Siddharth", ""], ["Erkip", "Elza", ""]]}, {"id": "1901.07747", "submitter": "Xuguang Ren", "authors": "Xuguang Ren, Junhu Wang, Wook-Shin Han, Jeffrey Xu Yu", "title": "Fast and Robust Distributed Subgraph Enumeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic subgraph enumeration problem under distributed settings.\nExisting solutions either suffer from severe memory crisis or rely on large\nindexes, which makes them impractical for very large graphs. Most of them\nfollow a synchronous model where the performance is often bottlenecked by the\nmachine with the worst performance. Motivated by this, in this paper, we\npropose RADS, a Robust Asynchronous Distributed Subgraph enumeration system.\nRADS first identifies results that can be found using single-machine\nalgorithms. This strategy not only improves the overall performance but also\nreduces network communication and memory cost. Moreover, RADS employs a novel\nregion-grouped multi-round expand verify & filter framework which does not need\nto shuffle and exchange the intermediate results, nor does it need to replicate\na large part of the data graph in each machine. This feature not only reduces\nnetwork communication cost and memory usage, but also allows us to adopt simple\nstrategies for memory control and load balancing, making it more robust.\nSeveral heuristics are also used in RADS to further improve the performance.\nOur experiments verified the superiority of RADS to state-of-the-art subgraph\nenumeration approaches.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 06:49:02 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Ren", "Xuguang", ""], ["Wang", "Junhu", ""], ["Han", "Wook-Shin", ""], ["Yu", "Jeffrey Xu", ""]]}, {"id": "1901.07773", "submitter": "Huu-Hiep Nguyen", "authors": "Huu Hiep Nguyen", "title": "Boosting Frequent Itemset Mining via Early Stopping Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent itemsets from a transaction database has emerged as a\nfundamental problem in data mining and committed itself as a building block for\nmany pattern mining tasks. In this paper, we present a general technique to\nreduce support checking time in existing depth-first search generate-and-test\nschemes such as Eclat/dEclat and PrePost+. Our technique allows infrequent\ncandidate itemsets to be detected early. The technique is based on an\nearly-stopping criterion and is general enough to be applicable in many\nfrequent itemset mining algorithms. We have applied the technique to two\nTID-list based schemes (Eclat/dEclat) and one N-list based scheme (PrePost+).\nOur technique has been tested over a variety of datasets and confirmed its\neffectiveness in runtime reduction.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 08:43:50 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Nguyen", "Huu Hiep", ""]]}, {"id": "1901.08248", "submitter": "Alin Deutsch", "authors": "Alin Deutsch, Yu Xu, Mingxi Wu, Victor Lee", "title": "TigerGraph: A Native MPP Graph Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present TigerGraph, a graph database system built from the ground up to\nsupport massively parallel computation of queries and analytics.\n  TigerGraph's high-level query language, GSQL, is designed for compatibility\nwith SQL, while simultaneously allowing NoSQL programmers to continue thinking\nin Bulk-Synchronous Processing (BSP) terms and reap the benefits of high-level\nspecification.\n  GSQL is sufficiently high-level to allow declarative SQL-style programming,\nyet sufficiently expressive to concisely specify the sophisticated iterative\nalgorithms required by modern graph analytics and traditionally coded in\ngeneral-purpose programming languages like C++ and Java.\n  We report very strong scale-up and scale-out performance over a benchmark we\npublished on GitHub for full reproducibility.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 06:34:15 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Deutsch", "Alin", ""], ["Xu", "Yu", ""], ["Wu", "Mingxi", ""], ["Lee", "Victor", ""]]}, {"id": "1901.08304", "submitter": "Rui Liu", "authors": "Rui Liu, Jun Yuan", "title": "Benchmarking Time Series Databases with IoTDB-Benchmark for IoT\n  Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide application of time series databases (TSDBs) in big data fields\nlike cluster monitoring and industrial IoT, there have been developed a number\nof TSDBs for time series data management. Different TSDBs have test reports\ncomparing themselves with other databases to show their advantages, but the\ncomparisons are typically based on their own tools without using a common\nwell-recognized test framework. To the best of our knowledge, there is no\nmature TSDB benchmark either. With the goal of establishing a standard of\nevaluating TSDB systems, we present the IoTDB-Benchmark framework, specifically\ndesigned for TSDB and IoT application scenarios. We pay close attention to some\nspecial data ingestion scenarios and summarize 10 basic queries types. We use\nthis benchmark to compare four TSDB systems: InfluxDB, OpenTSDB, KairosDB and\nTimescaleDB. Our benchmark framework/tool not only measures performance metrics\nbut also takes system resource consumption into consideration.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 09:40:28 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 09:16:04 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 02:53:38 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Liu", "Rui", ""], ["Yuan", "Jun", ""]]}, {"id": "1901.08666", "submitter": "Boris Glavic", "authors": "Jason Arnold and Boris Glavic and Ioan Raicu", "title": "HRDBMS: Combining the Best of Modern and Traditional Relational\n  Databases", "comments": "Oral Ph.D. Qualifier Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HRDBMS is a novel distributed relational database that uses a hybrid model\ncombining the best of traditional distributed relational databases and Big Data\nanalytics platforms such as Hive. This allows HRDBMS to leverage years worth of\nresearch regarding query optimization, while also taking advantage of the\nscalability of Big Data platforms. The system uses an execution framework that\nis tailored for relational processing, thus addressing some of the performance\nchallenges of running SQL on top of platforms such as MapReduce and Spark.\nThese include excessive materialization of intermediate results, lack of a\nglobal cost-based optimization, unnecessary sorting, lack of index support, no\nstatistics, no support for DML and ACID, and excessive communication caused by\nthe rigid communication patterns enforced by these platforms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 22:20:46 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Arnold", "Jason", ""], ["Glavic", "Boris", ""], ["Raicu", "Ioan", ""]]}, {"id": "1901.09090", "submitter": "Ryan Marcus", "authors": "Ryan Marcus, Olga Papaemmanouil", "title": "Flexible Operator Embeddings via Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating machine learning into the internals of database management\nsystems requires significant feature engineering, a human effort-intensive\nprocess to determine the best way to represent the pieces of information that\nare relevant to a task. In addition to being labor intensive, the process of\nhand-engineering features must generally be repeated for each data management\ntask, and may make assumptions about the underlying database that are not\nuniversally true. We introduce flexible operator embeddings, a deep learning\ntechnique for automatically transforming query operators into feature vectors\nthat are useful for a multiple data management tasks and is custom-tailored to\nthe underlying database. Our approach works by taking advantage of an\noperator's context, resulting in a neural network that quickly transforms\nsparse representations of query operators into dense, information-rich feature\nvectors. Experimentally, we show that our flexible operator embeddings perform\nwell across a number of data management tasks, using both synthetic and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 21:33:50 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 23:30:19 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Marcus", "Ryan", ""], ["Papaemmanouil", "Olga", ""]]}, {"id": "1901.09353", "submitter": "Egor V. Kostylev", "authors": "Mark Kaminski and Egor V. Kostylev", "title": "Subsumption of Weakly Well-Designed SPARQL Patterns is Undecidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly well-designed SPARQL patterns is a recent generalisation of\nwell-designed patterns, which preserve good computational properties but also\ncapture almost all patterns that appear in practice. Subsumption is one of\nstatic analysis problems for SPARQL, along with equivalence and containment. In\nthis paper we show that subsumption is undecidable for weakly well-designed\npatterns, which is in stark contrast to well-designed patterns, and to\nequivalence and containment.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 11:18:06 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Kaminski", "Mark", ""], ["Kostylev", "Egor V.", ""]]}, {"id": "1901.10109", "submitter": "Yanhao Wang", "authors": "Yanhao Wang, Yuchen Li, Kian-Lee Tan", "title": "Semantic and Influence aware k-Representative Queries over Social\n  Streams", "comments": "27 pages, 14 figures, to appear in the 22nd International Conference\n  on Extending Database Technology (EDBT 2019)", "journal-ref": null, "doi": "10.5441/002/edbt.2019.17", "report-no": null, "categories": "cs.SI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Massive volumes of data continuously generated on social platforms have\nbecome an important information source for users. A primary method to obtain\nfresh and valuable information from social streams is \\emph{social search}.\nAlthough there have been extensive studies on social search, existing methods\nonly focus on the \\emph{relevance} of query results but ignore the\n\\emph{representativeness}. In this paper, we propose a novel Semantic and\nInfluence aware $k$-Representative ($k$-SIR) query for social streams based on\ntopic modeling. Specifically, we consider that both user queries and elements\nare represented as vectors in the topic space. A $k$-SIR query retrieves a set\nof $k$ elements with the maximum \\emph{representativeness} over the sliding\nwindow at query time w.r.t. the query vector. The representativeness of an\nelement set comprises both semantic and influence scores computed by the topic\nmodel. Subsequently, we design two approximation algorithms, namely\n\\textsc{Multi-Topic ThresholdStream} (MTTS) and \\textsc{Multi-Topic\nThresholdDescend} (MTTD), to process $k$-SIR queries in real-time. Both\nalgorithms leverage the ranked lists maintained on each topic for $k$-SIR\nprocessing with theoretical guarantees. Extensive experiments on real-world\ndatasets demonstrate the effectiveness of $k$-SIR query compared with existing\nmethods as well as the efficiency and scalability of our proposed algorithms\nfor $k$-SIR processing.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 05:25:33 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Wang", "Yanhao", ""], ["Li", "Yuchen", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1901.10615", "submitter": "Shale Xiong", "authors": "Shale Xiong, Andrea Cerone, Azalea Raad, Philippa Gardner", "title": "Data Consistency in Transactional Storage Systems: a Centralised\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an interleaving operational semantics for describing the\nclient-observable behaviour of atomic transactions on distributed key-value\nstores. Our semantics builds on abstract states comprising centralised, global\nkey-value stores and partial client views. We provide operational definitions\nof consistency models for our key-value stores which are shown to be equivalent\nto the well-known declarative definitions of consistency model for execution\ngraphs. We explore two immediate applications of our semantics: specific\nprotocols of geo-replicated databases (e.g. COPS) and partitioned databases\n(e.g. Clock-SI) can be shown to be correct for a specific consistency model by\nembedding them in our centralised semantics; programs can be directly shown to\nhave invariant properties such as robustness results against a weak consistency\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 23:47:02 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 14:48:40 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Xiong", "Shale", ""], ["Cerone", "Andrea", ""], ["Raad", "Azalea", ""], ["Gardner", "Philippa", ""]]}, {"id": "1901.10938", "submitter": "Joy Arulraj", "authors": "Joy Arulraj, Andy Pavlo, Krishna Teja Malladi", "title": "Multi-Tier Buffer Management and Storage System Design for Non-Volatile\n  Memory", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of the buffer manager in database management systems (DBMSs) is\ninfluenced by the performance characteristics of volatile memory (DRAM) and\nnon-volatile storage (e.g., SSD). The key design assumptions have been that the\ndata must be migrated to DRAM for the DBMS to operate on it and that storage is\norders of magnitude slower than DRAM. But the arrival of new non-volatile\nmemory (NVM) technologies that are nearly as fast as DRAM invalidates these\nprevious assumptions. This paper presents techniques for managing and designing\na multi-tier storage hierarchy comprising of DRAM, NVM, and SSD. Our main\ntechnical contributions are a multi-tier buffer manager and a storage system\ndesigner that leverage the characteristics of NVM. We propose a set of\noptimizations for maximizing the utility of data migration between different\ndevices in the storage hierarchy. We demonstrate that these optimizations have\nto be tailored based on device and workload characteristics. Given this, we\npresent a technique for adapting these optimizations to achieve a near-optimal\nbuffer management policy for an arbitrary workload and storage hierarchy\nwithout requiring any manual tuning. We finally present a recommendation system\nfor designing a multi-tier storage hierarchy for a target workload and system\ncost budget. Our results show that the NVM-aware buffer manager and storage\nsystem designer improve throughput and reduce system cost across different\ntransaction and analytical processing workloads.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 16:38:13 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Arulraj", "Joy", ""], ["Pavlo", "Andy", ""], ["Malladi", "Krishna Teja", ""]]}, {"id": "1901.11376", "submitter": "Reynaldo John Tristan  Mahinay Jr.", "authors": "Franz Stewart V. Dizon, Stephen Kyle R. Farinas, Reynaldo John Tristan\n  H. Mahinay Jr., Harry S. Pardo and Cecil Jose A. Delfinado", "title": "Learning of High Dengue Incidence with Clustering and FP-Growth\n  Algorithm using WHO Historical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies FP-Growth algorithm in mining fuzzy association rules for\na prediction system of dengue. The system mines its rules through input of\nhistoric predictor variables for dengue. The rules will be used to build a\nrule-based classifier to predict the dengue incidence for the next month for\nthe years 2001-2006 in the Philippines. The FP-Growth Algorithm was compared to\nApriori Algorithm by Sensitivity, Specificity, PPV, NPV, execution time and\nmemory usage. The results showed that FP-Growth Algorithm is significantly\nbetter in execution time, numerically better in memory and comparable in\nSensitivity, Specificity, PPV and NPV to Apriori Algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 03:36:01 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Dizon", "Franz Stewart V.", ""], ["Farinas", "Stephen Kyle R.", ""], ["Mahinay", "Reynaldo John Tristan H.", "Jr."], ["Pardo", "Harry S.", ""], ["Delfinado", "Cecil Jose A.", ""]]}]