[{"id": "1808.00024", "submitter": "Hongzhi Wang", "authors": "Xiaoou Ding and Hongzhi Wang and Jiaxuan Su and Jianzhong Li and Hong\n  Gao", "title": "Improve3C: Data Cleaning on Consistency and Completeness with Currency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quality plays a key role in big data management today. With the\nexplosive growth of data from a variety of sources, the quality of data is\nfaced with multiple problems. Motivated by this, we study the multiple data\nquality improvement on completeness, consistency and currency in this paper.\nFor the proposed problem, we introduce a 4-step framework, named Improve3C, for\ndetection and quality improvement on incomplete and inconsistent data without\ntimestamps. We compute and achieve a relative currency order among records\nderived from given currency constraints, according to which inconsistent and\nincomplete data can be repaired effectively considering the temporal impact.\nFor both effectiveness and efficiency consideration, we carry out inconsistent\nrepair ahead of incomplete repair. Currency-related consistency distance is\ndefined to measure the similarity between dirty records and clean ones more\naccurately. In addition, currency orders are treated as an important feature in\nthe training process of incompleteness repair. The solution algorithms are\nintroduced in detail with examples. A thorough experiment on one real-life data\nand a synthetic one verifies that the proposed method can improve the\nperformance of dirty data cleaning with multiple quality problems which are\nhard to be cleaned by the existing approaches effectively.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 18:48:32 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Ding", "Xiaoou", ""], ["Wang", "Hongzhi", ""], ["Su", "Jiaxuan", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}, {"id": "1808.00197", "submitter": "Jerome Darmont", "authors": "Ayb\\\"uk\\\"e Ozt\\\"urk (ERIC, ArAr), St\\'ephane Lallich (ERIC),\n  J\\'er\\^ome Darmont (ERIC), Sylvie Yona Waksman (ArAr)", "title": "MaxMin Linear Initialization for Fuzzy C-Means", "comments": null, "journal-ref": "IBaI. 14th International Conference on Machine Learning and Data\n  Mining (MLDM 2018), Jul 2018, New York, United States. Springer, Lecture\n  Notes in Artificial Intelligence, 10934-10935, 2018, Machine Learning and\n  Data Mining in Pattern Recognition. http://www.mldm.de", "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an extensive research area in data science. The aim of\nclustering is to discover groups and to identify interesting patterns in\ndatasets. Crisp (hard) clustering considers that each data point belongs to one\nand only one cluster. However, it is inadequate as some data points may belong\nto several clusters, as is the case in text categorization. Thus, we need more\nflexible clustering. Fuzzy clustering methods, where each data point can belong\nto several clusters, are an interesting alternative. Yet, seeding iterative\nfuzzy algorithms to achieve high quality clustering is an issue. In this paper,\nwe propose a new linear and efficient initialization algorithm MaxMin Linear to\ndeal with this problem. Then, we validate our theoretical results through\nextensive experiments on a variety of numerical real-world and artificial\ndatasets. We also test several validity indices, including a new validity index\nthat we propose, Transformed Standardized Fuzzy Difference (TSFD).\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 07:07:15 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Ozt\u00fcrk", "Ayb\u00fck\u00eb", "", "ERIC, ArAr"], ["Lallich", "St\u00e9phane", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Waksman", "Sylvie Yona", "", "ArAr"]]}, {"id": "1808.00554", "submitter": "Chiara Renso", "authors": "Andrea Esuli, Lucas May Petry, Chiara Renso, Vania Bogorny", "title": "Traj2User: exploiting embeddings for computing similarity of users\n  mobile behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic trajectories are high level representations of user movements where\nseveral aspects related to the movement context are represented as\nheterogeneous textual labels. With the objective of finding a meaningful\nsimilarity measure for semantically enriched trajectories, we propose\nTraj2User, a Word2Vec-inspired method for the generation of a vector\nrepresentation of user movements as user embeddings. Traj2User uses simple\nrepresentations of trajectories and delegates the definition of the similarity\nmodel to the learning process of the network. Preliminary results show that\nTraj2User is able to generate effective user embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 14:33:13 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 08:19:48 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Esuli", "Andrea", ""], ["Petry", "Lucas May", ""], ["Renso", "Chiara", ""], ["Bogorny", "Vania", ""]]}, {"id": "1808.00986", "submitter": "Hongzhi Wang", "authors": "Meifan Zhang and Hongzhi Wang and Jianzhong Li and Hong Gao", "title": "Diversification on Big Data in Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, in the area of big data, some popular applications such as web\nsearch engines and recommendation systems, face the problem to diversify\nresults during query processing. In this sense, it is both significant and\nessential to propose methods to deal with big data in order to increase the\ndiversity of the result set. In this paper, we firstly define a set's diversity\nand an element's ability to improve the set's overall diversity. Based on these\ndefinitions, we propose a diversification framework which has good performance\nin terms of effectiveness and efficiency. Also, this framework has theoretical\nguarantee on probability of success. Secondly, we design implementation\nalgorithms based on this framework for both numerical and string data. Thirdly,\nfor numerical and string data respectively, we carry out extensive experiments\non real data to verify the performance of our proposed framework, and also\nperform scalability experiments on synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 18:47:27 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Zhang", "Meifan", ""], ["Wang", "Hongzhi", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}, {"id": "1808.01095", "submitter": "Doris Xin", "authors": "Doris Xin, Litian Ma, Jialin Liu, Stephen Macke, Shuchen Song, Aditya\n  Parameswaran", "title": "Helix: Accelerating Human-in-the-loop Machine Learning", "comments": null, "journal-ref": null, "doi": "10.14778/3229863.3236234", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data application developers and data scientists spend an inordinate amount of\ntime iterating on machine learning (ML) workflows -- by modifying the data\npre-processing, model training, and post-processing steps -- via\ntrial-and-error to achieve the desired model performance. Existing work on\naccelerating machine learning focuses on speeding up one-shot execution of\nworkflows, failing to address the incremental and dynamic nature of typical ML\ndevelopment. We propose Helix, a declarative machine learning system that\naccelerates iterative development by optimizing workflow execution end-to-end\nand across iterations. Helix minimizes the runtime per iteration via program\nanalysis and intelligent reuse of previous results, which are selectively\nmaterialized -- trading off the cost of materialization for potential future\nbenefits -- to speed up future iterations. Additionally, Helix offers a\ngraphical interface to visualize workflow DAGs and compare versions to\nfacilitate iterative development. Through two ML applications, in\nclassification and in structured prediction, attendees will experience the\nsuccinctness of Helix programming interface and the speed and ease of iterative\ndevelopment using Helix. In our evaluations, Helix achieved up to an order of\nmagnitude reduction in cumulative run time compared to state-of-the-art machine\nlearning tools.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 06:02:46 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Xin", "Doris", ""], ["Ma", "Litian", ""], ["Liu", "Jialin", ""], ["Macke", "Stephen", ""], ["Song", "Shuchen", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1808.01620", "submitter": "Hongzhi Wang", "authors": "Tianbao Lia and Hongzhi Wang and Jianzhong Li and Hong Gao", "title": "Schema Integration on Massive Data Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the fundamental phrase of collecting and analyzing data, data integration\nis used in many applications, such as data cleaning, bioinformatics and pattern\nrecognition. In big data era, one of the major problems of data integration is\nto obtain the global schema of data sources since the global schema could be\nhardly derived from massive data sources directly. In this paper, we attempt to\nsolve such schema integration problem. For different scenarios, we develop\nbatch and incremental schema integration algorithms. We consider the\nrepresentation difference of attribute names in various data sources and\npropose ED Join and Semantic Join algorithms to integrate attributes with\ndifferent representations. Extensive experimental results demonstrate that the\nproposed algorithms could integrate schemas efficiently and effectively.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 14:10:14 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Lia", "Tianbao", ""], ["Wang", "Hongzhi", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}, {"id": "1808.01621", "submitter": "Hongzhi Wang", "authors": "Hongzhi Wang and Mingda Li and Jiawei Zhao and Jianzhong Li and Hong\n  Gao", "title": "Mining CFD Rules on Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current conditional functional dependencies (CFDs) discovery algorithms\nalways need a well-prepared training data set. This makes them difficult to be\napplied on large datasets which are always in low-quality. To handle the volume\nissue of big data, we develop the sampling algorithms to obtain a small\nrepresentative training set. For the low-quality issue of big data, we then\ndesign the fault-tolerant rule discovery algorithm and the conflict resolution\nalgorithm. We also propose parameter selection strategy for CFD discovery\nalgorithm to ensure its effectiveness. Experimental results demonstrate that\nour method could discover effective CFD rules on billion-tuple data within\nreasonable time.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 14:11:14 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Wang", "Hongzhi", ""], ["Li", "Mingda", ""], ["Zhao", "Jiawei", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}, {"id": "1808.01624", "submitter": "Hongzhi Wang", "authors": "Dan Zhang and Hongzhi Wang and Xiaoou Ding and Yice Zhang and\n  Jianzhong Li and Hong Gao", "title": "On the Fairness of Quality-based Data Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For data pricing, data quality is a factor that must be considered. To keep\nthe fairness of data market from the aspect of data quality, we proposed a fair\ndata market that considers data quality while pricing. To ensure fairness, we\nfirst design a quality-driven data pricing strategy. Then based on the\nstrategy, a fairness assurance mechanism for quality-driven data marketplace is\nproposed. In this mechanism, we ensure that savvy consumers cannot cheat the\nsystem and users can verify each consumption with Trusted Third Party (TTP)\nthat they are charged properly. Based on this mechanism, we develop a fair\nquality-driven data market system. Extensive experiments are performed to\nverify the effectiveness of proposed techniques. Experimental results show that\nour quality-driven data pricing strategy could assign a reasonable price to the\ndata according to data quality and the fairness assurance mechanism could\neffectively protect quality-driven data pricing from potential cheating.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 14:13:29 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Zhang", "Dan", ""], ["Wang", "Hongzhi", ""], ["Ding", "Xiaoou", ""], ["Zhang", "Yice", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}, {"id": "1808.01703", "submitter": "Kira Adaricheva V", "authors": "Oren Segal, Justin Cabot-Miller, Kira Adaricheva, J.B.Nation, Anuar\n  Sharafudinov", "title": "The Bases of Association Rules of High Confidence", "comments": "Presented at DTMN, Sydney, Australia, July 28, 2018", "journal-ref": "David C.Wyld et al. (Eds) : CSITY, DTMN, NWCOM, SIGPRO - 2018, pp.\n  39-51, 2018", "doi": "10.5121/csit.2018.81104", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach for distributed computing of the association rules\nof high confidence in a binary table. It is derived from the D-basis algorithm\nin K. Adaricheva and J.B. Nation (TCS 2017), which is performed on multiple\nsub-tables of a table given by removing several rows at a time. The set of\nrules is then aggregated using the same approach as the D-basis is retrieved\nfrom a larger set of implications. This allows to obtain a basis of association\nrules of high confidence, which can be used for ranking all attributes of the\ntable with respect to a given fixed attribute using the relevance parameter\nintroduced in K. Adaricheva et al. (Proceedings of ICFCA-2015). This paper\nfocuses on the technical implementation of the new algorithm. Some testing\nresults are performed on transaction data and medical data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 01:01:50 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Segal", "Oren", ""], ["Cabot-Miller", "Justin", ""], ["Adaricheva", "Kira", ""], ["Nation", "J. B.", ""], ["Sharafudinov", "Anuar", ""]]}, {"id": "1808.01949", "submitter": "Ferdinando Fioretto", "authors": "Ferdinando Fioretto and Pascal Van Hentenryck", "title": "OptStream: Releasing Time Series Privately", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research (JAIR) Vol. 65 (2019)", "doi": "10.1613/jair.1.11583", "report-no": null, "categories": "cs.CR cs.AI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of machine learning and optimization operate on data\nstreams. While these datasets are fundamental to fuel decision-making\nalgorithms, often they contain sensitive information about individuals and\ntheir usage poses significant privacy risks. Motivated by an application in\nenergy systems, this paper presents OPTSTREAM, a novel algorithm for releasing\ndifferentially private data streams under the w-event model of privacy.\nOPTSTREAM is a 4-step procedure consisting of sampling, perturbation,\nreconstruction, and post-processing modules. First, the sampling module selects\na small set of points to access in each period of interest. Then, the\nperturbation module adds noise to the sampled data points to guarantee privacy.\nNext, the reconstruction module reassembles non-sampled data points from the\nperturbed sample points. Finally, the post-processing module uses convex\noptimization over the private output of the previous modules, as well as the\nprivate answers of additional queries on the data stream, to improve accuracy\nby redistributing the added noise. OPTSTREAM is evaluated on a test case\ninvolving the release of a real data stream from the largest European\ntransmission operator. Experimental results show that OPTSTREAM may not only\nimprove the accuracy of state-of-the-art methods by at least one order of\nmagnitude but also supports accurate load forecasting on the private data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 14:54:42 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 13:24:03 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Fioretto", "Ferdinando", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1808.02066", "submitter": "Stratos Idreos", "authors": "Stratos Idreos, Kostas Zoumpatianos, Brian Hentschel, Michael S.\n  Kester, Demi Guo", "title": "The Internals of the Data Calculator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data structures are critical in any data-driven scenario, but they are\nnotoriously hard to design due to a massive design space and the dependence of\nperformance on workload and hardware which evolve continuously. We present a\ndesign engine, the Data Calculator, which enables interactive and\nsemi-automated design of data structures. It brings two innovations. First, it\noffers a set of fine-grained design primitives that capture the first\nprinciples of data layout design: how data structure nodes lay data out, and\nhow they are positioned relative to each other. This allows for a structured\ndescription of the universe of possible data structure designs that can be\nsynthesized as combinations of those primitives. The second innovation is\ncomputation of performance using learned cost models. These models are trained\non diverse hardware and data profiles and capture the cost properties of\nfundamental data access primitives (e.g., random access). With these models, we\nsynthesize the performance cost of complex operations on arbitrary data\nstructure designs without having to: 1) implement the data structure, 2) run\nthe workload, or even 3) access the target hardware. We demonstrate that the\nData Calculator can assist data structure designers and researchers by\naccurately answering rich what-if design questions on the order of a few\nseconds or minutes, i.e., computing how the performance (response time) of a\ngiven data structure design is impacted by variations in the: 1) design, 2)\nhardware, 3) data, and 4) query workloads. This makes it effortless to test\nnumerous designs and ideas before embarking on lengthy implementation,\ndeployment, and hardware acquisition steps. We also demonstrate that the Data\nCalculator can synthesize entirely new designs, auto-complete partial designs,\nand detect suboptimal design choices.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 18:46:18 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Idreos", "Stratos", ""], ["Zoumpatianos", "Kostas", ""], ["Hentschel", "Brian", ""], ["Kester", "Michael S.", ""], ["Guo", "Demi", ""]]}, {"id": "1808.02291", "submitter": "Alessandro Ronca", "authors": "Alessandro Ronca, Mark Kaminski, Bernardo Cuenca Grau, Ian Horrocks", "title": "The Window Validity Problem in Rule-Based Stream Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule-based temporal query languages provide the expressive power and\nflexibility required to capture in a natural way complex analysis tasks over\nstreaming data. Stream processing applications, however, typically require near\nreal-time response using limited resources. In particular, it becomes essential\nthat the underpinning query language has favourable computational properties\nand that stream processing algorithms are able to keep only a small number of\npreviously received facts in memory at any point in time without sacrificing\ncorrectness. In this paper, we propose a recursive fragment of temporal Datalog\nwith tractable data complexity and study the properties of a generic stream\nreasoning algorithm for this fragment. We focus on the window validity problem\nas a way to minimise the number of time points for which the stream reasoning\nalgorithm needs to keep data in memory at any point in time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 10:17:11 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 12:04:03 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 17:53:44 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Ronca", "Alessandro", ""], ["Kaminski", "Mark", ""], ["Grau", "Bernardo Cuenca", ""], ["Horrocks", "Ian", ""]]}, {"id": "1808.02793", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Kesheng Cheng, Lei Zhu, Ruipeng Chen, Zuping Zhang\n  and Fang Huang", "title": "Efficient Continuous Top-$k$ Geo-Image Search on Road Network", "comments": "Multimedia Tools and Applications, Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of mobile Internet and cloud computing technology,\nlarge-scale multimedia data, e.g., texts, images, audio and videos have been\ngenerated, collected, stored and shared. In this paper, we propose a novel\nquery problem named continuous top-$k$ geo-image query on road network which\naims to search out a set of geo-visual objects based on road network distance\nproximity and visual content similarity. Existing approaches for spatial\ntextual query and geo-image query cannot address this problem effectively\nbecause they do not consider both of visual content similarity and road network\ndistance proximity on road network. In order to address this challenge\neffectively and efficiently, firstly we propose the definition of geo-visual\nobjects and continuous top-$k$ geo-visual objects query on road network, then\ndevelop a score function for search. To improve the query efficiency in a\nlarge-scale road network, we propose the search algorithm named geo-visual\nsearch on road network based on a novel hybrid indexing framework called\nVIG-Tree, which combines G-Tree and visual inverted index technique. In\naddition, an important notion named safe interval and results updating rule are\nproposed, and based on them we develop an efficient algorithm named moving\nmonitor algorithm to solve continuous query. Experimental evaluation on real\nmultimedia dataset and road network dataset illustrates that our solution\noutperforms state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 14:25:16 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Cheng", "Kesheng", ""], ["Zhu", "Lei", ""], ["Chen", "Ruipeng", ""], ["Zhang", "Zuping", ""], ["Huang", "Fang", ""]]}, {"id": "1808.02850", "submitter": "Medina Andresel", "authors": "Medina Andre\\c{s}el, Yazmin Ib\\'a\\~nez-Garc\\'ia, Magdalena Ortiz and\n  Mantas \\v{S}imkus", "title": "Relaxing and Restraining Queries for OBDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ontology-based data access (OBDA), ontologies have been successfully\nemployed for querying possibly unstructured and incomplete data. In this paper,\nwe advocate using ontologies not only to formulate queries and compute their\nanswers, but also for modifying queries by relaxing or restraining them, so\nthat they can retrieve either more or less answers over a given dataset.\nTowards this goal, we first illustrate that some domain knowledge that could be\nnaturally leveraged in OBDA can be expressed using complex role inclusions\n(CRI). Queries over ontologies with CRI are not first-order (FO) rewritable in\ngeneral. We propose an extension of DL-Lite with CRI, and show that conjunctive\nqueries over ontologies in this extension are FO rewritable. Our main\ncontribution is a set of rules to relax and restrain conjunctive queries (CQs).\nFirstly, we define rules that use the ontology to produce CQs that are\nrelaxations/restrictions over any dataset. Secondly, we introduce a set of\ndata-driven rules, that leverage patterns in the current dataset, to obtain\nmore fine-grained relaxations and restrictions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 16:27:52 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Andre\u015fel", "Medina", ""], ["Ib\u00e1\u00f1ez-Garc\u00eda", "Yazmin", ""], ["Ortiz", "Magdalena", ""], ["\u0160imkus", "Mantas", ""]]}, {"id": "1808.03196", "submitter": "Sanjay Krishnan", "authors": "Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, Ion\n  Stoica", "title": "Learning to Optimize Join Queries With Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exhaustive enumeration of all possible join orders is often avoided, and most\noptimizers leverage heuristics to prune the search space. The design and\nimplementation of heuristics are well-understood when the cost model is roughly\nlinear, and we find that these heuristics can be significantly suboptimal when\nthere are non-linearities in cost. Ideally, instead of a fixed heuristic, we\nwould want a strategy to guide the search space in a more data-driven\nway---tailoring the search to a specific dataset and query workload.\nRecognizing the link between classical Dynamic Programming enumeration methods\nand recent results in Reinforcement Learning (RL), we propose a new method for\nlearning optimized join search strategies. We present our RL-based DQ\noptimizer, which currently optimizes select-project-join blocks. We implement\nthree versions of DQ to illustrate the ease of integration into existing\nDBMSes: (1) A version built on top of Apache Calcite, (2) a version integrated\ninto PostgreSQL, and (3) a version integrated into SparkSQL. Our extensive\nevaluation shows that DQ achieves plans with optimization costs and query\nexecution times competitive with the native query optimizer in each system, but\ncan execute significantly faster after learning (often by orders of magnitude).\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 15:30:06 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 20:33:48 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Krishnan", "Sanjay", ""], ["Yang", "Zongheng", ""], ["Goldberg", "Ken", ""], ["Hellerstein", "Joseph", ""], ["Stoica", "Ion", ""]]}, {"id": "1808.03537", "submitter": "Ryan McKenna", "authors": "Ryan McKenna, Gerome Miklau, Michael Hay, Ashwin Machanavajjhala", "title": "Optimizing error of high-dimensional statistical queries under\n  differential privacy", "comments": null, "journal-ref": "PVLDB, 11 (10): 1206-1219, 2018", "doi": "10.14778/3231751.3231769", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Differentially private algorithms for answering sets of predicate counting\nqueries on a sensitive database have many applications. Organizations that\ncollect individual-level data, such as statistical agencies and medical\ninstitutions, use them to safely release summary tabulations. However, existing\ntechniques are accurate only on a narrow class of query workloads, or are\nextremely slow, especially when analyzing more than one or two dimensions of\nthe data. In this work we propose HDMM, a new differentially private algorithm\nfor answering a workload of predicate counting queries, that is especially\neffective for higher-dimensional datasets. HDMM represents query workloads\nusing an implicit matrix representation and exploits this compact\nrepresentation to efficiently search (a subset of) the space of differentially\nprivate algorithms for one that answers the input query workload with high\naccuracy. We empirically show that HDMM can efficiently answer queries with\nlower error than state-of-the-art techniques on a variety of low and high\ndimensional datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 13:44:26 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["McKenna", "Ryan", ""], ["Miklau", "Gerome", ""], ["Hay", "Michael", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "1808.03555", "submitter": "Dan Zhang", "authors": "Dan Zhang, Ryan McKenna, Ios Kotsogiannis, George Bissias, Michael\n  Hay, Ashwin Machanavajjhala, Gerome Miklau", "title": "Ektelo: A Framework for Defining Differentially-Private Computations", "comments": "Journal version under submission", "journal-ref": null, "doi": "10.1145/3183713.3196921", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of differential privacy is growing but the complexity of\ndesigning private, efficient and accurate algorithms is still high. We propose\na novel programming framework and system, Ektelo, for implementing both\nexisting and new privacy algorithms. For the task of answering linear counting\nqueries, we show that nearly all existing algorithms can be composed from\noperators, each conforming to one of a small number of operator classes. While\npast programming frameworks have helped to ensure the privacy of programs, the\nnovelty of our framework is its significant support for authoring accurate and\nefficient (as well as private) programs.\n  After describing the design and architecture of the Ektelo system, we show\nthat Ektelo is expressive, allows for safer implementations through code reuse,\nand that it allows both privacy novices and experts to easily design\nalgorithms. We demonstrate the use of Ektelo by designing several new\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 14:18:40 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 16:00:22 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 15:27:04 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Zhang", "Dan", ""], ["McKenna", "Ryan", ""], ["Kotsogiannis", "Ios", ""], ["Bissias", "George", ""], ["Hay", "Michael", ""], ["Machanavajjhala", "Ashwin", ""], ["Miklau", "Gerome", ""]]}, {"id": "1808.04088", "submitter": "Jalpesh Vasa", "authors": "Jalpesh Vasa, Panthini Modi", "title": "Review of Different Privacy Preserving Techniques in PPDP", "comments": "05 Pages, \"Published with International Journal of Engineering Trends\n  and Technology (IJETT)\"", "journal-ref": "IJETT, V59(5),223-227 May 2018, ISSN:2231-5381", "doi": "10.14445/22315381/IJETT-V59P242", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data is a term used for a very large data sets that have many\ndifficulties in storing and processing the data. Analysis this much amount of\ndata will lead to information loss. The main goal of this paper is to share\ndata in a way that privacy is preserved while information loss is kept at\nleast. Data that include Government agencies, University details and Medical\nhistory etc., are very necessary for an organization to do analysis and predict\ntrends and patterns, but it may prevent the data owner from sharing the data\nbecause of privacy regulations [1]. By doing an analysis of several algorithms\nof Anonymization such as k-anonymity, l-diversity and tcloseness, one can\nachieve privacy at minimum loss. Admitting these techniques has some\nlimitations. We need to maintain trade-off between privacy and information\nloss. We introduce a novel approach called Differential Privacy.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 07:39:04 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Vasa", "Jalpesh", ""], ["Modi", "Panthini", ""]]}, {"id": "1808.04486", "submitter": "Thibault Sellam", "authors": "Thibault Sellam, Kevin Lin, Ian Yiran Huang, Yiru Chen, Michelle Yang,\n  Carl Vondrick, Eugene Wu", "title": "DeepBase: Deep Inspection of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models perform remarkably well across a range of tasks\nsuch as language translation and object recognition, it remains unclear what\nhigh-level logic, if any, they follow. Understanding this logic may lead to\nmore transparency, better model design, and faster experimentation. Recent\nmachine learning research has leveraged statistical methods to identify hidden\nunits that behave (e.g., activate) similarly to human understandable logic, but\nthose analyses require considerable manual effort. Our insight is that many of\nthose studies follow a common analysis pattern, which we term Deep Neural\nInspection. There is opportunity to provide a declarative abstraction to easily\nexpress, execute, and optimize them.\n  This paper describes DeepBase, a system to inspect neural network behaviors\nthrough a unified interface. We model logic with user-provided hypothesis\nfunctions that annotate the data with high-level labels (e.g., part-of-speech\ntags, image captions). DeepBase lets users quickly identify individual or\ngroups of units that have strong statistical dependencies with desired\nhypotheses. We discuss how DeepBase can express existing analyses, propose a\nset of simple and effective optimizations to speed up a standard Python\nimplementation by up to 72x, and reproduce recent studies from the NLP\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 22:56:55 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 16:00:49 GMT"}, {"version": "v3", "created": "Sun, 30 Dec 2018 23:26:32 GMT"}, {"version": "v4", "created": "Mon, 7 Jan 2019 09:45:24 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Sellam", "Thibault", ""], ["Lin", "Kevin", ""], ["Huang", "Ian Yiran", ""], ["Chen", "Yiru", ""], ["Yang", "Michelle", ""], ["Vondrick", "Carl", ""], ["Wu", "Eugene", ""]]}, {"id": "1808.04663", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Pierre Bourhis, Mika\\\"el Monet, Pierre Senellart", "title": "Evaluating Datalog via Tree Automata and Cycluits", "comments": "56 pages, 63 references. Journal version of \"Combined Tractability of\n  Query Evaluation via Tree Automata and Cycluits (Extended Version)\" at\n  arXiv:1612.04203. Up to the stylesheet, page/environment numbering, and\n  possible minor publisher-induced changes, this is the exact content of the\n  journal paper that will appear in Theory of Computing Systems. Update wrt\n  version 1: latest reviewer feedback", "journal-ref": null, "doi": "10.1007/s00224-018-9901-2", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate parameterizations of both database instances and queries that\nmake query evaluation fixed-parameter tractable in combined complexity. We show\nthat clique-frontier-guarded Datalog with stratified negation (CFG-Datalog)\nenjoys bilinear-time evaluation on structures of bounded treewidth for programs\nof bounded rule size. Such programs capture in particular conjunctive queries\nwith simplicial decompositions of bounded width, guarded negation fragment\nqueries of bounded CQ-rank, or two-way regular path queries. Our result is\nshown by translating to alternating two-way automata, whose semantics is\ndefined via cyclic provenance circuits (cycluits) that can be tractably\nevaluated.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 12:48:15 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 15:06:06 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Monet", "Mika\u00ebl", ""], ["Senellart", "Pierre", ""]]}, {"id": "1808.04876", "submitter": "Chunbin Lin", "authors": "Chunbin Lin, Etienne Boursier, Yannis Papakonstantinou", "title": "Plato: Approximate Analytics over Compressed Time Series with Tight\n  Deterministic Error Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plato provides fast approximate analytics on time series, by precomputing and\nstoring compressed time series. Plato's key novelty is the delivery of tight\ndeterministic error guarantees for time series analytics. Plato evaluates any\ntime series expression composed by the linear algebra operators over vectors,\nalong with arithmetic operators. This large scope of possible expressions\nincludes common use cases such as correlation and cross-correlation\nexpressions. Each time series is segmented either by fixed-length segmentation\nor by (a usually more effective) variable-length segmentation. Each segment is\ncompressed by an estimation/compression function that approximates the actual\nvalues and is coming from a user-chosen function family, as taught by many\nprior works. The novelty is that Plato associates to each segment 1 to 3\n(depending on the case) precomputed error measures and, using them, Plato\ncomputes tight deterministic error guarantees for analytics over the\ncompressions. Importantly, some compression families lead to much better\ndeterministic error guarantees. This work identifies two broad estimation\nfunction family groups (Vector Space (VS) and Linear Scalable Family (LSF)),\nwhich lead to theoretically and practically high-quality guarantees, even for\nexpressions (eg correlation) that combine multiple time series that have been\nindependently compressed and may, thus, use misaligned segmentations. The\ntheoretical aspect of \"high quality\" is crisply captured by the Amplitude\nIndependence (AI) property: An AI guarantee does not depend on the amplitude of\nthe involved time series, even when we combine multiple time series. The\nexperiments on four real-life datasets showed that when the novel AI guarantees\nwere applicable, the approximate query results were certified to be very close\n(typically 1%) to the true results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 19:55:42 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 23:59:47 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Lin", "Chunbin", ""], ["Boursier", "Etienne", ""], ["Papakonstantinou", "Yannis", ""]]}, {"id": "1808.05138", "submitter": "Lauren Milechin", "authors": "Lauren Milechin, Vijay Gadepally, Jeremy Kepner", "title": "Database Operations in D4M.jl", "comments": "IEEE HPEC 2018. arXiv admin note: text overlap with arXiv:1708.02934", "journal-ref": null, "doi": "10.1109/HPEC.2018.8547567", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each step in the data analytics pipeline is important, including database\ningest and query. The D4M-Accumulo database connector has allowed analysts to\nquickly and easily ingest to and query from Apache Accumulo using MATLAB(R)/GNU\nOctave syntax. D4M.jl, a Julia implementation of D4M, provides much of the\nfunctionality of the original D4M implementation to the Julia community. In\nthis work, we extend D4M.jl to include many of the same database capabilities\nthat the MATLAB(R)/GNU Octave implementation provides. Here we will describe\nthe D4M.jl database connector, demonstrate how it can be used, and show that it\nhas comparable or better performance to the original implementation in\nMATLAB(R)/GNU Octave.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 20:19:56 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Milechin", "Lauren", ""], ["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1808.05199", "submitter": "Qiang Qu", "authors": "Muhammad Muzammal, Qiang Qu, Bulat Nasrulin, Anders Skovsgaard", "title": "A Blockchain Database Application Platform", "comments": "the draft will be reshaped", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A blockchain is a decentralised linked data structure that is characterised\nby its inherent resistance to data modification, but it is deficient in search\nqueries, primarily due to its inferior data formatting. A distributed database\nis also a decentralised data structure which features quick query processing\nand well-designed data formatting but suffers from data reliability. In this\ndemonstration, we showcase a blockchain database application platform developed\nby integrating the blockchain with the database, i.e. we demonstrate a system\nthat has the decentralised, distributed and audibility features of the\nblockchain and quick query processing and well-designed data structure of the\ndistributed databases. The system features a tamper-resistant, consistent and\ncost-effective multi-active database and an effective and reliable data-level\ndisaster recovery backup. The system is demonstrated in practice as a\nmulti-active database along with the data-level disaster recovery backup\nfeature.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 17:44:25 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 03:46:13 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 08:47:32 GMT"}, {"version": "v4", "created": "Wed, 7 Nov 2018 08:27:55 GMT"}, {"version": "v5", "created": "Thu, 10 Jan 2019 06:54:39 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Muzammal", "Muhammad", ""], ["Qu", "Qiang", ""], ["Nasrulin", "Bulat", ""], ["Skovsgaard", "Anders", ""]]}, {"id": "1808.05215", "submitter": "Jo\\~ao Marcelo Borovina Josko", "authors": "Jo\\~ao Marcelo Borovina Josko, Jo\\~ao Eduardo Ferreira", "title": "Vis4DD: A visualization system that supports Data Quality Visual\n  Assessment", "comments": "6 pages, 3 figures, Proceedings of the satellite events on 32nd.\n  Brazilian Symposium on Databases", "journal-ref": "32th Brazilian Symposium on Databases - Demo and Applications\n  (2017) pp. 46-51", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quality assessment process is essential to ensure reliable analytical\noutcomes. This process depends on human supervision-driven approaches since it\nis impossible to determine a defect based only on data. Visualization systems\nbelong to a class of supervised tools that can make data defect pattern\nvisible. However, their considerable design knowledge encodings and imple-\nmentations provide little support design to data quality visual assessment. To\ncover this gap, this work reports the design approach of V is4DD visualization\nsystem based on patterns of data defects structures and assessment tasks. An\nexploratory case study used this web-based system to explore which and how\nvisual-interactive properties facilitate visual detection of data defect.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 13:47:09 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Josko", "Jo\u00e3o Marcelo Borovina", ""], ["Ferreira", "Jo\u00e3o Eduardo", ""]]}, {"id": "1808.05448", "submitter": "Hannes M\\\"uhleisen", "authors": "Aleksei Kashuba and Hannes M\\\"uhleisen", "title": "Automatic Generation of a Hybrid Query Execution Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing need for fast data processing demands new methods for\nefficient query execution. Just-in-time query compilation techniques have been\ndemonstrated to improve performance in a set of analytical tasks significantly.\nIn this work, we investigate the possibility of adding this approach to\nexisting database solutions and the benefits it provides. To that end, we\ncreate a set of automated tools to create a runtime code generation engine and\nintegrate such an engine into SQLite which is one of the most popular\nrelational databases in the world and is used in a large variety of contexts.\nSpeedups of up to 1.7x were observed in microbenchmarks with queries involving\na large number of operations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 12:42:29 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Kashuba", "Aleksei", ""], ["M\u00fchleisen", "Hannes", ""]]}, {"id": "1808.05698", "submitter": "Mohammad Roohitavaf", "authors": "Mohammad Roohitavaf, Jung-Sang Ahn, Woon-Hak Kang, Kun Ren, Gene\n  Zhang, Sami Ben-Romdhane, Sandeep S. Kulkarni", "title": "Session Guarantees with Raft and Hybrid Logical Clocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eventual consistency is a popular consistency model for geo-replicated data\nstores. Although eventual consistency provides high performance and\navailability, it can cause anomalies that make programming complex for\napplication developers. Session guarantees can remove some of these anomalies\nwhile causing much lower overhead compared with stronger consistency models. In\nthis paper, we provide a protocol for providing session guarantees for NuKV, a\nkey-value store developed for services with very high availability and\nperformance requirements at eBay. NuKV relies on the Raft protocol for\nreplication inside datacenters, and uses eventual consistency for replication\namong datacenters. We provide modified versions of conventional session\nguarantees to avoid the problem of slowdown cascades in systems with large\nnumbers of partitions. We also use Hybrid Logical Clocks to eliminate the need\nfor delaying write operations to satisfy session guarantees. Our experiments\nshow that our protocol provides session guarantees with a negligible overhead\nwhen compared with eventual consistency.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 22:54:25 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Roohitavaf", "Mohammad", ""], ["Ahn", "Jung-Sang", ""], ["Kang", "Woon-Hak", ""], ["Ren", "Kun", ""], ["Zhang", "Gene", ""], ["Ben-Romdhane", "Sami", ""], ["Kulkarni", "Sandeep S.", ""]]}, {"id": "1808.05752", "submitter": "Boris Glavic", "authors": "Seokki Lee, Bertram Ludaescher, Boris Glavic", "title": "PUG: A Framework and Practical Implementation for Why & Why-Not\n  Provenance (extended version)", "comments": "Extended version of VLDB journal article of the same name. arXiv\n  admin note: text overlap with arXiv:1701.05699", "journal-ref": null, "doi": null, "report-no": "IIT/CS-DB-2018-02", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining why an answer is (or is not) returned by a query is important for\nmany applications including auditing, debugging data and queries, and answering\nhypothetical questions about data. In this work, we present the first practical\napproach for answering such questions for queries with negation (first- order\nqueries). Specifically, we introduce a graph-based provenance model that, while\nsyntactic in nature, supports reverse reasoning and is proven to encode a wide\nrange of provenance models from the literature. The implementation of this\nmodel in our PUG (Provenance Unification through Graphs) system takes a\nprovenance question and Datalog query as an input and generates a Datalog\nprogram that computes an explanation, i.e., the part of the provenance that is\nrelevant to answer the question. Furthermore, we demonstrate how a desirable\nfactorization of provenance can be achieved by rewriting an input query. We\nexperimentally evaluate our approach demonstrating its efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 02:01:11 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Lee", "Seokki", ""], ["Ludaescher", "Bertram", ""], ["Glavic", "Boris", ""]]}, {"id": "1808.05988", "submitter": "Hal Cooper", "authors": "Hal Cooper, Garud Iyengar, Ching-Yung Lin", "title": "Attainment Ratings for Graph-Query Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video game industry is larger than both the film and music industries\ncombined. Recommender systems for video games have received relatively scant\nacademic attention, despite the uniqueness of the medium and its data. In this\npaper, we introduce a graph-based recommender system that makes use of\ninteractivity, arguably the most significant feature of video gaming. We show\nthat the use of implicit data that tracks user-game interactions and levels of\nattainment (e.g. Sony Playstation Trophies, Microsoft Xbox Achievements) has\nhigh predictive value when making recommendations. Furthermore, we argue that\nthe characteristics of the video gaming hobby (low cost, high duration,\nsocially relevant) make clear the necessity of personalized, individual\nrecommendations that can incorporate social networking information. We\ndemonstrate the natural suitability of graph-query based recommendation for\nthis purpose.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 20:35:56 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Cooper", "Hal", ""], ["Iyengar", "Garud", ""], ["Lin", "Ching-Yung", ""]]}, {"id": "1808.06298", "submitter": "Tobias Grubenmann", "authors": "Tobias Grubenmann, Abraham Bernstein, Dmitry Moor, Sven Seuken", "title": "FedMark: A Marketplace for Federated Data on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web of Data (WoD) has experienced a phenomenal growth in the past. This\ngrowth is mainly fueled by tireless volunteers, government subsidies, and open\ndata legislations. The majority of commercial data has not made the transition\nto the WoD, yet. The problem is that it is not clear how publishers of\ncommercial data can monetize their data in this new setting. Advertisement,\nwhich is one of the main financial engines of the World Wide Web, cannot be\napplied to the Web of Data as such unwanted data can easily be filtered out,\nautomatically. This raises the question how the WoD can (i) maintain its grow\nwhen subsidies disappear and (ii) give commercial data providers financial\nincentives to share their wealth of data. In this paper, we propose a\nmarketplace for the WoD as a solution for this data monetization problem. Our\napproach allows a customer to transparently buy data from a combination of\ndifferent providers. To that end, we introduce two different approaches for\ndeciding which data elements to buy and compare their performance. We also\nintroduce FedMark, a prototypical implementation of our marketplace that\nrepresents a first step towards an economically viable WoD beyond subsidies.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 03:57:16 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Grubenmann", "Tobias", ""], ["Bernstein", "Abraham", ""], ["Moor", "Dmitry", ""], ["Seuken", "Sven", ""]]}, {"id": "1808.06800", "submitter": "Arijit Khan", "authors": "Arijit Khan and Sixing Yan", "title": "Composite Hashing for Data Stream Sketches", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In rapid and massive data streams, it is often not possible to estimate the\nfrequency of items with complete accuracy. To perform the operation in a\nreasonable amount of space and with sufficiently low latency, approximated\nmethods are used. The most common ones are variations of the Count-Min sketch.\nBy using multiple hash functions, they summarize massive streams in sub-linear\nspace. In reality, data item ids or keys can be modular, e.g., a graph edge is\nrepresented by source and target node ids, a 32-bit IP address is composed of\nfour 8-bit words, a web address consists of domain name, domain extension,\npath, and filename, among many others. In this paper, we investigate the\nmodularity property of item keys, and systematically develop more accurate,\ncomposite hashing strategies, such as employing multiple independent hash\nfunctions that hash different modules in a key and their combinations\nseparately, instead of hashing the entire key directly into the sketch.\n  However, our problem of finding the best hashing strategy is non-trivial,\nsince there are exponential number of ways to combine the modules of a key\nbefore they can be hashed into the sketch. Moreover, given a fixed size\nallocated for the entire sketch, it is hard to find the optimal range of all\nhash functions that correspond to different modules and their combinations. We\nsolve both these problems with extensive theoretical analysis, and perform\nthorough experiments with real-world datasets to demonstrate the accuracy and\nefficiency of our proposed method, MOD-Sketch.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 08:43:20 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 02:42:41 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Khan", "Arijit", ""], ["Yan", "Sixing", ""]]}, {"id": "1808.06907", "submitter": "Rafael S. Gon\\c{c}alves", "authors": "Rafael S. Gon\\c{c}alves and Mark A. Musen", "title": "The variable quality of metadata about biological samples used in\n  biomedical experiments", "comments": "arXiv admin note: text overlap with arXiv:1708.01286", "journal-ref": null, "doi": "10.1038/sdata.2019.21", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an analytical study of the quality of metadata about samples used\nin biomedical experiments. The metadata under analysis are stored in two\nwell-known databases: BioSample---a repository managed by the National Center\nfor Biotechnology Information (NCBI), and BioSamples---a repository managed by\nthe European Bioinformatics Institute (EBI). We tested whether 11.4M sample\nmetadata records in the two repositories are populated with values that fulfill\nthe stated requirements for such values. Our study revealed multiple anomalies\nin the metadata. Most metadata field names and their values are not\nstandardized or controlled. Even simple binary or numeric fields are often\npopulated with inadequate values of different data types. By clustering\nmetadata field names, we discovered there are often many distinct ways to\nrepresent the same aspect of a sample. Overall, the metadata we analyzed reveal\nthat there is a lack of principled mechanisms to enforce and validate metadata\nrequirements. The significant aberrancies that we found in the metadata are\nlikely to impede search and secondary use of the associated datasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 18:03:26 GMT"}, {"version": "v2", "created": "Sat, 19 Jan 2019 03:46:41 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Gon\u00e7alves", "Rafael S.", ""], ["Musen", "Mark A.", ""]]}, {"id": "1808.07151", "submitter": "Luke Rodriguez", "authors": "Luke Rodriguez, Babak Salimi, Haoyue Ping, Julia Stoyanovich, Bill\n  Howe", "title": "MobilityMirror: Bias-Adjusted Transportation Datasets", "comments": "Presented at BIDU 2018 workshop and published in Springer\n  Communications in Computer and Information Science vol 926", "journal-ref": "Big Social Data and Urban Computing. BiDU 2018. Communications in\n  Computer and Information Science, vol 926. Springer, Cham", "doi": "10.1007/978-3-030-11238-7_2", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe customized synthetic datasets for publishing mobility data.\nPrivate companies are providing new transportation modalities, and their data\nis of high value for integrative transportation research, policy enforcement,\nand public accountability. However, these companies are disincentivized from\nsharing data not only to protect the privacy of individuals (drivers and/or\npassengers), but also to protect their own competitive advantage. Moreover,\ndemographic biases arising from how the services are delivered may be amplified\nif released data is used in other contexts.\n  We describe a model and algorithm for releasing origin-destination histograms\nthat removes selected biases in the data using causality-based methods. We\ncompute the origin-destination histogram of the original dataset then adjust\nthe counts to remove undesirable causal relationships that can lead to\ndiscrimination or violate contractual obligations with data owners. We evaluate\nthe utility of the algorithm on real data from a dockless bike share program in\nSeattle and taxi data in New York, and show that these adjusted transportation\ndatasets can retain utility while removing bias in the underlying data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 22:19:48 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 01:28:03 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 00:27:21 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Rodriguez", "Luke", ""], ["Salimi", "Babak", ""], ["Ping", "Haoyue", ""], ["Stoyanovich", "Julia", ""], ["Howe", "Bill", ""]]}, {"id": "1808.07603", "submitter": "Luke Rodriguez", "authors": "Luke Rodriguez, Bill Howe", "title": "Privacy-Preserving Synthetic Datasets Over Weakly Constrained Domains", "comments": "Submitted to TDPD18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques to deliver privacy-preserving synthetic datasets take a sensitive\ndataset as input and produce a similar dataset as output while maintaining\ndifferential privacy. These approaches have the potential to improve data\nsharing and reuse, but they must be accessible to non-experts and tolerant of\nrealistic data. Existing approaches make an implicit assumption that the active\ndomain of the dataset is similar to the global domain, potentially violating\ndifferential privacy.\n  In this paper, we present an algorithm for generating differentially private\nsynthetic data over the large, weakly constrained domains we find in realistic\nopen data situations. Our algorithm models the unrepresented domain\nanalytically as a probability distribution to adjust the output and compute\nnoise, avoiding the need to compute the full domain explicitly. We formulate\nthe tradeoff between privacy and utility in terms of a \"tolerance for\nrandomness\" parameter that does not require users to inspect the data to set.\nFinally, we show that the algorithm produces sensible results on real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 01:52:45 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Rodriguez", "Luke", ""], ["Howe", "Bill", ""]]}, {"id": "1808.07767", "submitter": "Piotr Ostropolski-Nalewaja", "authors": "Grzegorz G{\\l}uch, Jerzy Marcinkowski, Piotr Ostropolski-Nalewaja", "title": "The First Order Truth behind Undecidability of Regular Path Queries\n  Determinacy", "comments": "arXiv admin note: text overlap with arXiv:1802.01554", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our paper [G{\\l}uch, Marcinkowski, Ostropolski-Nalewaja, LICS ACM, 2018]\nwe have solved an old problem stated in [Calvanese, De Giacomo, Lenzerini,\nVardi, SPDS ACM, 2000] showing that query determinacy is undecidable for\nRegular Path Queries. Here a strong generalisation of this result is shown, and\n-- we think -- a very unexpected one. We prove that no regularity is needed:\ndeterminacy remains undecidable even for finite unions of conjunctive path\nqueries.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 15:12:34 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 20:11:34 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["G\u0142uch", "Grzegorz", ""], ["Marcinkowski", "Jerzy", ""], ["Ostropolski-Nalewaja", "Piotr", ""]]}, {"id": "1808.08181", "submitter": "Boxiang Dong", "authors": "Haipei Sun, Boxiang Dong, Hui (Wendy) Wang, Ting Yu, Zhan Qin", "title": "Truth Inference on Sparse Crowdsourcing Data with Local Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has arisen as a new problem-solving paradigm for tasks that are\ndifficult for computers but easy for humans. However, since the answers\ncollected from the recruited participants (workers) may contain sensitive\ninformation, crowdsourcing raises serious privacy concerns. In this paper, we\ninvestigate the problem of protecting answer privacy under local differential\nprivacy (LDP), by which individual workers randomize their answers\nindependently and send the perturbed answers to the task requester. The utility\ngoal is to enable to infer the true answer (i.e., truth) from the perturbed\ndata with high accuracy. One of the challenges of LDP perturbation is the\nsparsity of worker answers (i.e., each worker only answers a small number of\ntasks). Simple extension of the existing approaches (e.g., Laplace perturbation\nand randomized response) may incur large error of truth inference on sparse\ndata. Thus we design an efficient new matrix factorization (MF) algorithm under\nLDP. We prove that our MF algorithm can provide both LDP guarantee and small\nerror of truth inference, regardless of the sparsity of worker answers. We\nperform extensive experiments on real-world and synthetic datasets, and\ndemonstrate that the MF algorithm performs better than the existing LDP\nalgorithms on sparse crowdsourcing data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 15:48:06 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Sun", "Haipei", "", "Wendy"], ["Dong", "Boxiang", "", "Wendy"], ["Hui", "", "", "Wendy"], ["Wang", "", ""], ["Yu", "Ting", ""], ["Qin", "Zhan", ""]]}, {"id": "1808.08355", "submitter": "Shrainik Jain", "authors": "Shrainik Jain, Jiaqi Yan, Thierry Cruane, Bill Howe", "title": "Database-Agnostic Workload Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system to support generalized SQL workload analysis and\nmanagement for multi-tenant and multi-database platforms. Workload analysis\napplications are becoming more sophisticated to support database\nadministration, model user behavior, audit security, and route queries, but the\nmethods rely on specialized feature engineering, and therefore must be\ncarefully implemented and reimplemented for each SQL dialect, database system,\nand application. Meanwhile, the size and complexity of workloads are increasing\nas systems centralize in the cloud. We model workload analysis and management\ntasks as variations on query labeling, and propose a system design that can\nsupport general query labeling routines across multiple applications and\ndatabase backends. The design relies on the use of learned vector embeddings\nfor SQL queries as a replacement for application-specific syntactic features,\nreducing custom code and allowing the use of off-the-shelf machine learning\nalgorithms for labeling. The key hypothesis, for which we provide evidence in\nthis paper, is that these learned features can outperform conventional feature\nengineering on representative machine learning tasks. We present the design of\na database-agnostic workload management and analytics service, describe\npotential applications, and show that separating workload representation from\nlabeling tasks affords new capabilities and can outperform existing solutions\nfor representative tasks, including workload sampling for index recommendation\nand user labeling for security audits.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 05:14:31 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Jain", "Shrainik", ""], ["Yan", "Jiaqi", ""], ["Cruane", "Thierry", ""], ["Howe", "Bill", ""]]}, {"id": "1808.08634", "submitter": "Emanuel Sallinger", "authors": "Felix Burgstaller, Bernd Neumayr, Emanuel Sallinger, Michael Schrefl", "title": "Rule Module Inheritance with Modification Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapting rule sets to different settings, yet avoiding uncontrolled\nproliferation of variations, is a key challenge of rule management. One\nfundamental concept to foster reuse and simplify adaptation is inheritance.\nBuilding on rule modules, i.e., rule sets with input and output schema, we\nformally define inheritance of rule modules by incremental modification in\nsingle inheritance hierarchies. To avoid uncontrolled proliferation of\nmodifications, we introduce formal modification restrictions which flexibly\nregulate the degree to which a child module may be modified in comparison to\nits parent. As concrete rule language, we employ Datalog+/- which can be\nregarded a common logical core of many rule languages. We evaluate the approach\nby a proof-of-concept prototype.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 22:28:36 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 13:09:31 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Burgstaller", "Felix", ""], ["Neumayr", "Bernd", ""], ["Sallinger", "Emanuel", ""], ["Schrefl", "Michael", ""]]}, {"id": "1808.08822", "submitter": "Dimitri Surinx", "authors": "Dimitri Surinx and Jan Van den Bussche", "title": "A Monotone Preservation Result for Boolean Queries Expressed as a\n  Containment of Conjunctive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a relational database is queried, the result is normally a relation.\nSome queries, however, only require a yes/no answer; such queries are often\ncalled boolean queries. It is customary in database theory to express boolean\nqueries by testing nonemptiness of query expressions. Another interesting way\nfor expressing boolean queries are containment statements of the form $Q_1\n\\subseteq Q_2$ where $Q_1$ and $Q_2$ are query expressions. Here, for any input\ninstance $I$, the boolean query result is $\\mathit{true}$ if $Q_1(I)$ is a\nsubset of $Q_2(I)$ and $\\mathit{false}$ otherwise.\n  In the present paper we will focus on nonemptiness and containment statements\nabout conjunctive queries. The main goal is to investigate the monotone\nfragment of the containments of conjunctive queries. In particular, we show a\npreservation like result for this monotone fragment. That is, we show that, in\nexpressive power, the monotone containments of conjunctive queries are exactly\nequal to conjunctive queries under nonemptiness.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 12:42:38 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 14:28:54 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Surinx", "Dimitri", ""], ["Bussche", "Jan Van den", ""]]}, {"id": "1808.08896", "submitter": "Chen Luo", "authors": "Chen Luo, Michael J. Carey", "title": "Efficient Data Ingestion and Query Processing for LSM-Based Storage\n  Systems", "comments": "15 pages, 23 figures, to appear in VLDB 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the Log Structured Merge (LSM) tree has been widely adopted\nby NoSQL and NewSQL systems for its superior write performance. Despite its\npopularity, however, most existing work has focused on LSM-based key-value\nstores with only a primary LSM-tree index; auxiliary structures, which are\ncritical for supporting ad-hoc queries, have received much less attention. In\nthis paper, we focus on efficient data ingestion and query processing for\ngeneral-purpose LSM-based storage systems. We first propose and evaluate a\nseries of optimizations for efficient batched point lookups, significantly\nimproving the range of applicability of LSM-based secondary indexes. We then\npresent several new and efficient maintenance strategies for LSM-based storage\nsystems. Finally, we have implemented and experimentally evaluated the proposed\ntechniques in the context of the Apache AsterixDB system, and we present the\nresults here.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 15:56:00 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 18:14:04 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Luo", "Chen", ""], ["Carey", "Michael J.", ""]]}, {"id": "1808.08983", "submitter": "Zhe Wang", "authors": "Zhe Wang, Dylan Cashman, Mingwei Li, Jixian Li, Matthew Berger, Joshua\n  A. Levine, Remco Chang, and Carlos Scheidegger", "title": "NeuralCubes: Deep Representations for Visual Data Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual exploration of large multidimensional datasets has seen tremendous\nprogress in recent years, allowing users to express rich data queries that\nproduce informative visual summaries, all in real time. Techniques based on\ndata cubes are some of the most promising approaches. However, these techniques\nusually require a large memory footprint for large datasets. To tackle this\nproblem, we present NeuralCubes: neural networks that predict results for\naggregate queries, similar to data cubes. NeuralCubes learns a function that\ntakes as input a given query, for instance, a geographic region and temporal\ninterval, and outputs the result of the query. The learned function serves as a\nreal-time, low-memory approximator for aggregation queries. NeuralCubes models\nare small enough to be sent to the client side (e.g. the web browser for a\nweb-based application) for evaluation, enabling data exploration of large\ndatasets without database/network connection. We demonstrate the effectiveness\nof NeuralCubes through extensive experiments on a variety of datasets and\ndiscuss how NeuralCubes opens up opportunities for new types of visualization\nand interaction.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 18:11:16 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 16:21:32 GMT"}, {"version": "v3", "created": "Wed, 10 Jul 2019 16:22:02 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Wang", "Zhe", ""], ["Cashman", "Dylan", ""], ["Li", "Mingwei", ""], ["Li", "Jixian", ""], ["Berger", "Matthew", ""], ["Levine", "Joshua A.", ""], ["Chang", "Remco", ""], ["Scheidegger", "Carlos", ""]]}, {"id": "1808.09267", "submitter": "Kristopher Fair Mr", "authors": "Kristopher M. Fair, Cameron Zachreson and Mikhail Prokopenko", "title": "Creating a surrogate commuter network from Australian Bureau of\n  Statistics census data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Between the 2011 and 2016 national censuses, the Australian Bureau of\nStatistics changed its anonymity policy compliance system for the distribution\nof census data. The new method has resulted in dramatic inconsistencies when\ncomparing low-resolution data to aggregated high-resolution data. Hence,\naggregated totals do not match true totals, and the mismatch gets worse as the\ndata resolution gets finer. Here, we address several aspects of this\ninconsistency with respect to the 2016 usual-residence to place-of-work travel\ndata. We introduce a re-sampling system that rectifies many of the artifacts\nintroduced by the new ABS protocol, ensuring a higher level of consistency\nacross partition sizes. We offer a surrogate high-resolution 2016 commuter\ndataset that reduces the difference between aggregated and true commuter totals\nfrom ~34% to only ~7%, which is on the order of the discrepancy across\npartition resolutions in data from earlier years.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 06:02:49 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 04:09:44 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Fair", "Kristopher M.", ""], ["Zachreson", "Cameron", ""], ["Prokopenko", "Mikhail", ""]]}, {"id": "1808.09545", "submitter": "Boxiang Dong", "authors": "Yanying Li, Haipei Sun, Boxiang Dong, Hui (Wendy) Wang", "title": "Cost-efficient Data Acquisition on Online Data Marketplaces for\n  Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incentivized by the enormous economic profits, the data marketplace platform\nhas been proliferated recently. In this paper, we consider the data marketplace\nsetting where a data shopper would like to buy data instances from the data\nmarketplace for correlation analysis of certain attributes. We assume that the\ndata in the marketplace is dirty and not free. The goal is to find the data\ninstances from a large number of datasets in the marketplace whose join result\nnot only is of high-quality and rich join informativeness, but also delivers\nthe best correlation between the requested attributes. To achieve this goal, we\ndesign DANCE, a middleware that provides the desired data acquisition service.\nDANCE consists of two phases: (1) In the off-line phase, it constructs a\ntwo-layer join graph from samples. The join graph consists of the information\nof the datasets in the marketplace at both schema and instance levels; (2) In\nthe online phase, it searches for the data instances that satisfy the\nconstraints of data quality, budget, and join informativeness, while maximize\nthe correlation of source and target attribute sets. We prove that the\ncomplexity of the search problem is NP-hard, and design a heuristic algorithm\nbased on Markov chain Monte Carlo (MCMC). Experiment results on two benchmark\ndatasets demonstrate the efficiency and effectiveness of our heuristic data\nacquisition algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 21:05:02 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Li", "Yanying", "", "Wendy"], ["Sun", "Haipei", "", "Wendy"], ["Dong", "Boxiang", "", "Wendy"], ["Hui", "", "", "Wendy"], ["Wang", "", ""]]}, {"id": "1808.10105", "submitter": "Md Kamruzzaman Sarker", "authors": "Md. Kamruzzaman Sarker, Adila A. Krisnadhi, Pascal Hitzler", "title": "OWLAx: A Protege Plugin to Support Ontology Axiomatization through\n  Diagramming", "comments": "Poster in ISWC 2016", "journal-ref": "The 15th International Semantic Web Conference (ISWC 2016) Kobe,\n  Japan", "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Once the conceptual overview, in terms of a somewhat informal class diagram,\nhas been designed in the course of engineering an ontology, the process of\nadding many of the appropriate logical axioms is mostly a routine task. We\nprovide a Protege plugin which supports this task, together with a visual user\ninterface, based on established methods for ontology design pattern modeling.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 03:57:58 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Sarker", "Md. Kamruzzaman", ""], ["Krisnadhi", "Adila A.", ""], ["Hitzler", "Pascal", ""]]}, {"id": "1808.10108", "submitter": "Md Kamruzzaman Sarker", "authors": "Md. Kamruzzaman Sarker, Adila Krisnadhi, David Carral, Pascal Hitzler", "title": "Rule-based OWL Modeling with ROWLTab Protege Plugin", "comments": "Accepted at ESWC 2017", "journal-ref": "14th ESWC 2017, Portoroz, Slovenia", "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been argued that it is much easier to convey logical statements using\nrules rather than OWL (or description logic (DL)) axioms. Based on recent\ntheoretical developments on transformations between rules and DLs, we have\ndeveloped ROWLTab, a Protege plugin that allows users to enter OWL axioms by\nway of rules; the plugin then automatically converts these rules into OWL 2 DL\naxioms if possible, and prompts the user in case such a conversion is not\npossible without weakening the semantics of the rule. In this paper, we present\nROWLTab, together with a user evaluation of its effectiveness compared to\nentering axioms using the standard Protege interface. Our evaluation shows that\nmodeling with ROWLTab is much quicker than the standard interface, while at the\nsame time, also less prone to errors for hard modeling tasks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 04:05:35 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Sarker", "Md. Kamruzzaman", ""], ["Krisnadhi", "Adila", ""], ["Carral", "David", ""], ["Hitzler", "Pascal", ""]]}]