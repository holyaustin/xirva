[{"id": "1703.00080", "submitter": "Md Farhadur Rahman", "authors": "Md Farhadur Rahman, Abolfazl Asudeh, Nick Koudas, Gautam Das", "title": "Efficient Computation of Subspace Skyline over Categorical Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Platforms such as AirBnB, Zillow, Yelp, and related sites have transformed\nthe way we search for accommodation, restaurants, etc. The underlying datasets\nin such applications have numerous attributes that are mostly Boolean or\nCategorical. Discovering the skyline of such datasets over a subset of\nattributes would identify entries that stand out while enabling numerous\napplications. There are only a few algorithms designed to compute the skyline\nover categorical attributes, yet are applicable only when the number of\nattributes is small.\n  In this paper, we place the problem of skyline discovery over categorical\nattributes into perspective and design efficient algorithms for two cases. (i)\nIn the absence of indices, we propose two algorithms, ST-S and ST-P, that\nexploits the categorical characteristics of the datasets, organizing tuples in\na tree data structure, supporting efficient dominance tests over the candidate\nset. (ii) We then consider the existence of widely used precomputed sorted\nlists. After discussing several approaches, and studying their limitations, we\npropose TA-SKY, a novel threshold style algorithm that utilizes sorted lists.\nMoreover, we further optimize TA-SKY and explore its progressive nature, making\nit suitable for applications with strict interactive requirements. In addition\nto the extensive theoretical analysis of the proposed algorithms, we conduct a\ncomprehensive experimental evaluation of the combination of real (including the\nentire AirBnB data collection) and synthetic datasets to study the practicality\nof the proposed algorithms. The results showcase the superior performance of\nour techniques, outperforming applicable approaches by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 22:37:27 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 22:01:13 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 09:18:24 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Rahman", "Md Farhadur", ""], ["Asudeh", "Abolfazl", ""], ["Koudas", "Nick", ""], ["Das", "Gautam", ""]]}, {"id": "1703.00123", "submitter": "Jian Dai", "authors": "Jian Dai, Fei He, Wang-Chien Lee, Gang Chen, Beng Chin Ooi", "title": "DTNC: A New Server-side Data Cleansing Framework for Cellular Trajectory\n  Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is essential for the cellular network operators to provide cellular\nlocation services to meet the needs of their users and mobile applications.\nHowever, cellular locations, estimated by network-based methods at the\nserver-side, bear with {\\it high spatial errors} and {\\it arbitrary missing\nlocations}. Moreover, auxiliary sensor data at the client-side are not\navailable to the operators. In this paper, we study the {\\em cellular\ntrajectory cleansing problem} and propose an innovative data cleansing\nframework, namely \\underline{D}ynamic \\underline{T}ransportation\n\\underline{N}etwork based \\underline{C}leansing (DTNC) to improve the quality\nof cellular locations delivered in online cellular trajectory services. We\nmaintain a dynamic transportation network (DTN), which associates a network\nedge with a probabilistic distribution of travel times updated continuously. In\naddition, we devise an object motion model, namely, {\\em travel-time-aware\nhidden semi-Markov model} ({\\em TT-HsMM}), which is used to infer the most\nprobable traveled edge sequences on DTN. To validate our ideas, we conduct a\ncomprehensive evaluation using real-world cellular data provided by a major\ncellular network operator and a GPS dataset collected by smartphones as the\nground truth. In the experiments, DTNC displays significant advantages over six\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 03:41:40 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 07:42:42 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Dai", "Jian", ""], ["He", "Fei", ""], ["Lee", "Wang-Chien", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1703.00391", "submitter": "Ilias Tachmazidis", "authors": "Ilias Tachmazidis, Sotiris Batsakis, John Davies, Alistair Duke, Mauro\n  Vallati, Grigoris Antoniou, Sandra Stincic Clarke", "title": "A Hypercat-enabled Semantic Internet of Things Data Hub: Technical\n  Report", "comments": "Technical report of an accepted ESWC-2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of information is generated from the rapidly increasing\nnumber of sensor networks and smart devices. A wide variety of sources generate\nand publish information in different formats, thus highlighting\ninteroperability as one of the key prerequisites for the success of Internet of\nThings (IoT). The BT Hypercat Data Hub provides a focal point for the sharing\nand consumption of available datasets from a wide range of sources. In this\nwork, we propose a semantic enrichment of the BT Hypercat Data Hub, using\nwell-accepted Semantic Web standards and tools. We propose an ontology that\ncaptures the semantics of the imported data and present the BT SPARQL Endpoint\nby means of a mapping between SPARQL and SQL queries. Furthermore, federated\nSPARQL queries allow queries over multiple hub-based and external data sources.\nFinally, we provide two use cases in order to illustrate the advantages\nafforded by our semantic approach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 17:10:27 GMT"}, {"version": "v2", "created": "Sun, 12 Mar 2017 13:18:29 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Tachmazidis", "Ilias", ""], ["Batsakis", "Sotiris", ""], ["Davies", "John", ""], ["Duke", "Alistair", ""], ["Vallati", "Mauro", ""], ["Antoniou", "Grigoris", ""], ["Clarke", "Sandra Stincic", ""]]}, {"id": "1703.00617", "submitter": "Benjamin Rubinstein", "authors": "Neil G. Marchant and Benjamin I. P. Rubinstein", "title": "In Search of an Entity Resolution OASIS: Optimal Asymptotic Sequential\n  Importance Sampling", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) presents unique challenges for evaluation methodology.\nWhile crowdsourcing platforms acquire ground truth, sound approaches to\nsampling must drive labelling efforts. In ER, extreme class imbalance between\nmatching and non-matching records can lead to enormous labelling requirements\nwhen seeking statistically consistent estimates for rigorous evaluation. This\npaper addresses this important challenge with the OASIS algorithm: a sampler\nand F-measure estimator for ER evaluation. OASIS draws samples from a (biased)\ninstrumental distribution, chosen to ensure estimators with optimal asymptotic\nvariance. As new labels are collected OASIS updates this instrumental\ndistribution via a Bayesian latent variable model of the annotator oracle, to\nquickly focus on unlabelled items providing more information. We prove that\nresulting estimates of F-measure, precision, recall converge to the true\npopulation values. Thorough comparisons of sampling methods on a variety of ER\ndatasets demonstrate significant labelling reductions of up to 83% without loss\nto estimate accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 04:49:22 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 07:34:10 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 01:28:50 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Marchant", "Neil G.", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "1703.00983", "submitter": "Kexin Rong", "authors": "Kexin Rong, Peter Bailis", "title": "ASAP: Prioritizing Attention via Time Series Smoothing", "comments": null, "journal-ref": "Proc. VLDB Endow. Vol. 10, No. 11 pages 1358-1369, 2017", "doi": "10.14778/3137628.3137645", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series visualization of streaming telemetry (i.e., charting of key\nmetrics such as server load over time) is increasingly prevalent in modern data\nplatforms and applications. However, many existing systems simply plot the raw\ndata streams as they arrive, often obscuring large-scale trends due to\nsmall-scale noise. We propose an alternative: to better prioritize end users'\nattention, smooth time series visualizations as much as possible to remove\nnoise, while retaining large-scale structure to highlight significant\ndeviations. We develop a new analytics operator called ASAP that automatically\nsmooths streaming time series by adaptively optimizing the trade-off between\nnoise reduction (i.e., variance) and trend retention (i.e., kurtosis). We\nintroduce metrics to quantitatively assess the quality of smoothed plots and\nprovide an efficient search strategy for optimizing these metrics that combines\ntechniques from stream processing, user interface design, and signal processing\nvia autocorrelation-based pruning, pixel-aware preaggregation, and on-demand\nrefresh. We demonstrate that ASAP can improve users' accuracy in identifying\nlong-term deviations in time series by up to 38.4% while reducing response\ntimes by up to 44.3%. Moreover, ASAP delivers these results several orders of\nmagnitude faster than alternative search strategies.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 23:09:48 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 08:49:09 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Rong", "Kexin", ""], ["Bailis", "Peter", ""]]}, {"id": "1703.01148", "submitter": "Bikash Chandra", "authors": "Bikash Chandra, S. Sudarshan", "title": "Runtime Optimization of Join Location in Parallel Data Management\n  Systems", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications running on parallel systems often need to join a streaming\nrelation or a stored relation with data indexed in a parallel data storage\nsystem. Some applications also compute UDFs on the joined tuples. The join can\nbe done at the data storage nodes, corresponding to reduce side joins, or by\nfetching data from the storage system to compute nodes, corresponding to map\nside join. Both may be suboptimal: reduce side joins may cause skew, while map\nside joins may lead to a lot of data being transferred and replicated.\n  In this paper, we present techniques to make runtime decisions between the\ntwo options on a per key basis, in order to improve the throughput of the join,\naccounting for UDF computation if any. Our techniques are based on an extended\nski-rental algorithm and provide worst-case performance guarantees with respect\nto the optimal point in the space considered by us. Our techniques use load\nbalancing taking into account the CPU, network and I/O costs as well as the\nload on compute and storage nodes. We have implemented our techniques on\nHadoop, Spark and the Muppet stream processing engine. Our experiments show\nthat our optimization techniques provide a significant improvement in\nthroughput over existing techniques.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 13:21:25 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 06:01:10 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 09:26:37 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chandra", "Bikash", ""], ["Sudarshan", "S.", ""]]}, {"id": "1703.01298", "submitter": "William Agresti", "authors": "William W. Agresti", "title": "Defining Domain-Independent Discovery Informatics", "comments": "14 pages; no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a personal account of the early legacy of discovery\ninformatics, especially surrounding the first published definition of\ndomain-independent DI. The state of DI is traced across various reference\nsources and the literature on the fourth paradigm of the scientific method.\nObservations are offered on DI, concluding that it will retain its appeal as a\nhighly apt descriptor for research and practice activities that are inherent in\nour human nature.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 18:17:53 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Agresti", "William W.", ""]]}, {"id": "1703.01727", "submitter": "Md Saiful Islam", "authors": "Charles H. Goonetilleke, J. Wenny Rahayu and Md. Saiful Islam", "title": "Frequent Query Matching in Dynamic Data Warehousing", "comments": "2 Tables, 29 Figures, submitted to the Elsevier Journal of Systems\n  and Software for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the need for flexible and on-demand decision support, Dynamic Data\nWarehouses (DDW) provide benefits over traditional data warehouses due to their\ndynamic characteristics in structuring and access mechanism. A DDW is a data\nframework that accommodates data source changes easily to allow seamless\nquerying to users. Materialized Views (MV) are proven to be an effective\nmethodology to enhance the process of retrieving data from a DDW as results are\npre-computed and stored in it. However, due to the static nature of\nmaterialized views, the level of dynamicity that can be provided at the MV\naccess layer is restricted. As a result, the collection of materialized views\nis not compatible with ever-changing reporting requirements. It is important\nthat the MV collection is consistent with current and upcoming queries. The\nsolution to the above problem must consider the following aspects: (a) MV must\nbe matched against an OLAP query in order to recognize whether the MV can\nanswer the query, (b) enable scalability in the MV collection, an intuitive\nmechanism to prune it and retrieve closely matching MVs must be incorporated,\n(c) MV collection must be able to evolve in correspondence to the regularly\nchanging user query patterns. Therefore, the primary objective of this paper is\nto explore these aspects and provide a well-rounded solution for the MV access\nlayer to remove the mismatch between the MV collection and reporting\nrequirements. Our contribution to solve the problem includes a Query Matching\nTechnique, a Domain Matching Technique and Maintenance of the MV collection. We\ndeveloped an experimental platform using real data-sets to evaluate the\neffectiveness in terms of performance and precision of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 05:14:07 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Goonetilleke", "Charles H.", ""], ["Rahayu", "J. Wenny", ""], ["Islam", "Md. Saiful", ""]]}, {"id": "1703.01910", "submitter": "Andr\\'e Petermann", "authors": "Andr\\'e Petermann, Martin Junghanns and Erhard Rahm", "title": "DIMSpan - Transactional Frequent Subgraph Mining with Distributed\n  In-Memory Dataflow Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional frequent subgraph mining identifies frequent subgraphs in a\ncollection of graphs. This research problem has wide applicability and\nincreasingly requires higher scalability over single machine solutions to\naddress the needs of Big Data use cases. We introduce DIMSpan, an advanced\napproach to frequent subgraph mining that utilizes the features provided by\ndistributed in-memory dataflow systems such as Apache Spark or Apache Flink. It\ndetermines the complete set of frequent subgraphs from arbitrary string-labeled\ndirected multigraphs as they occur in social, business and knowledge networks.\nDIMSpan is optimized to runtime and minimal network traffic but memory-aware.\nAn extensive performance evaluation on large graph collections shows the\nscalability of DIMSpan and the effectiveness of its pruning and optimization\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 14:57:03 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Petermann", "Andr\u00e9", ""], ["Junghanns", "Martin", ""], ["Rahm", "Erhard", ""]]}, {"id": "1703.02212", "submitter": "Md Saiful Islam", "authors": "Mehdi Naseriparsa, Md. Saiful Islam, Chengfei Liu and Irene Moser", "title": "No-But-Semantic-Match: Computing Semantically Matched XML Keyword Search\n  Results", "comments": "24 pages, 21 figures, 6 tables, submitted to The VLDB Journal for\n  possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users are rarely familiar with the content of a data source they are\nquerying, and therefore cannot avoid using keywords that do not exist in the\ndata source. Traditional systems may respond with an empty result, causing\ndissatisfaction, while the data source in effect holds semantically related\ncontent. In this paper we study this no-but-semantic-match problem on XML\nkeyword search and propose a solution which enables us to present the top-k\nsemantically related results to the user. Our solution involves two steps: (a)\nextracting semantically related candidate queries from the original query and\n(b) processing candidate queries and retrieving the top-k semantically related\nresults. Candidate queries are generated by replacement of non-mapped keywords\nwith candidate keywords obtained from an ontological knowledge base. Candidate\nresults are scored using their cohesiveness and their similarity to the\noriginal query. Since the number of queries to process can be large, with each\nresult having to be analyzed, we propose pruning techniques to retrieve the\ntop-$k$ results efficiently. We develop two query processing algorithms based\non our pruning techniques. Further, we exploit a property of the candidate\nqueries to propose a technique for processing multiple queries in batch, which\nimproves the performance substantially. Extensive experiments on two real\ndatasets verify the effectiveness and efficiency of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 04:54:44 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Naseriparsa", "Mehdi", ""], ["Islam", "Md. Saiful", ""], ["Liu", "Chengfei", ""], ["Moser", "Irene", ""]]}, {"id": "1703.02475", "submitter": "Silu Huang", "authors": "Silu Huang, Liqi Xu, Jialin Liu, Aaron Elmore, Aditya Parameswaran", "title": "OrpheusDB: Bolt-on Versioning for Relational Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science teams often collaboratively analyze datasets, generating dataset\nversions at each stage of iterative exploration and analysis. There is a\npressing need for a system that can support dataset versioning, enabling such\nteams to efficiently store, track, and query across dataset versions. We\nintroduce OrpheusDB, a dataset version control system that \"bolts on\"\nversioning capabilities to a traditional relational database system, thereby\ngaining the analytics capabilities of the database \"for free\". We develop and\nevaluate multiple data models for representing versioned data, as well as a\nlight-weight partitioning scheme, LyreSplit, to further optimize the models for\nreduced query latencies. With LyreSplit, OrpheusDB is on average 1000x faster\nin finding effective (and better) partitionings than competing approaches,\nwhile also reducing the latency of version retrieval by up to 20x relative to\nschemes without partitioning. LyreSplit can be applied in an online fashion as\nnew versions are added, alongside an intelligent migration scheme that reduces\nmigration time by 10x on average.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 17:09:13 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Huang", "Silu", ""], ["Xu", "Liqi", ""], ["Liu", "Jialin", ""], ["Elmore", "Aaron", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1703.02529", "submitter": "Daniel Kang", "authors": "Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis, Matei Zaharia", "title": "NoScope: Optimizing Neural Network Queries over Video at Scale", "comments": "PVLDB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision-in the form of deep neural networks-have\nmade it possible to query increasing volumes of video data with high accuracy.\nHowever, neural network inference is computationally expensive at scale:\napplying a state-of-the-art object detector in real time (i.e., 30+ frames per\nsecond) to a single video requires a $4000 GPU. In response, we present\nNoScope, a system for querying videos that can reduce the cost of neural\nnetwork video analysis by up to three orders of magnitude via\ninference-optimized model search. Given a target video, object to detect, and\nreference neural network, NoScope automatically searches for and trains a\nsequence, or cascade, of models that preserves the accuracy of the reference\nnetwork but is specialized to the target video and are therefore far less\ncomputationally expensive. NoScope cascades two types of models: specialized\nmodels that forego the full generality of the reference model but faithfully\nmimic its behavior for the target video and object; and difference detectors\nthat highlight temporal differences across frames. We show that the optimal\ncascade architecture differs across videos and objects, so NoScope uses an\nefficient cost-based optimizer to search across models and cascades. With this\napproach, NoScope achieves two to three order of magnitude speed-ups\n(265-15,500x real-time) on binary classification tasks over fixed-angle webcam\nand surveillance video while maintaining accuracy within 1-5% of\nstate-of-the-art neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:54:28 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 07:08:53 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 21:34:30 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Kang", "Daniel", ""], ["Emmons", "John", ""], ["Abuzaid", "Firas", ""], ["Bailis", "Peter", ""], ["Zaharia", "Matei", ""]]}, {"id": "1703.02591", "submitter": "Peyman Behzadnia", "authors": "Peyman Behzadnia, Yi-Cheng Tu, Bo Zeng and Wei Yuan", "title": "Energy-Aware Disk Storage Management: Online Approach with Application\n  in DBMS", "comments": "22 pages, 9 figures", "journal-ref": "International Journal of Database Management Systems (IJDMS)\n  Vol.9, No.1, Pages 1-22, February 2017", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption has become a first-class optimization goal in design and\nimplementation of data-intensive computing systems. This is particularly true\nin the design of database management systems (DBMS), which was found to be the\nmajor consumer of energy in the software stack of modern data centers. Among\nall database components, the storage system is one of the most power-hungry\nelements. In previous work, dynamic power management (DPM) techniques that make\nreal-time decisions to transition the disks to low-power modes are normally\nused to save energy in storage systems. In this paper, we tackle the\nlimitations of DPM proposals in previous contributions. We introduced a DPM\noptimization model integrated with model predictive control (MPC) strategy to\nminimize power consumption of the disk-based storage system while satisfying\ngiven performance requirements. It dynamically determines the state of disks\nand plans for inter-disk data fragment migration to achieve desirable balance\nbetween power consumption and query response time. Via analyzing our\noptimization model to identify structural properties of optimal solutions, we\npropose a fast-solution heuristic DPM algorithm that can be integrated in\nlarge-scale disk storage systems for efficient state configuration and data\nmigration. We evaluate our proposed ideas by running simulations using\nextensive set of synthetic workloads based on popular TPC benchmarks. Our\nresults show that our solution significantly outperforms the best existing\nalgorithm in both energy savings and response time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 20:50:46 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Behzadnia", "Peyman", ""], ["Tu", "Yi-Cheng", ""], ["Zeng", "Bo", ""], ["Yuan", "Wei", ""]]}, {"id": "1703.02602", "submitter": "Carlos Vega", "authors": "Carlos Vega, Paula Roquero, Rafael Leira, Ivan Gonzalez, Javier Aracil", "title": "Loginson: a transform and load system for very large scale log analysis\n  in large IT infrastructures", "comments": "23 pages, Figure 12", "journal-ref": "The Journal of Supercomputing; Volume 73-2017; pp 1-22", "doi": "10.1007/s11227-017-1990-1", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, most systems and applications produce log records that are useful\nfor security and monitoring purposes such as debugging programming errors,\nchecking system status, and detecting configuration problems or even attacks.\nTo this end, a log repository becomes necessary whereby logs can be accessed\nand visualized in a timely manner. This paper presents Loginson, a\nhigh-performance log centralization system for large-scale log collection and\nprocessing in large IT infrastructures. Besides log collection, Loginson\nprovides high-level analytics through a visual interface for the purpose of\ntroubleshooting critical incidents. We note that Loginson outperforms all of\nthe other log centralization solutions by taking full advantage of the vertical\nscalability, and therefore decreasing Capital Expenditure (CAPEX) and Operating\nExpense (OPEX) costs for deployment scenarios with a huge volume of log data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 21:30:17 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Vega", "Carlos", ""], ["Roquero", "Paula", ""], ["Leira", "Rafael", ""], ["Gonzalez", "Ivan", ""], ["Aracil", "Javier", ""]]}, {"id": "1703.02638", "submitter": "Fabio Porto", "authors": "Fabio Porto, Amir Khatibi, Jo\\~ao R. Nobre, Eduardo Ogasawara, Patrick\n  Valduriez, Dennis Shasha", "title": "Constellation Queries over Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A geometrical pattern is a set of points with all pairwise distances (or,\nmore generally, relative distances) specified. Finding matches to such patterns\nhas applications to spatial data in seismic, astronomical, and transportation\ncontexts. For example, a particularly interesting geometric pattern in\nastronomy is the Einstein cross, which is an astronomical phenomenon in which a\nsingle quasar is observed as four distinct sky objects (due to gravitational\nlensing) when captured by earth telescopes. Finding such crosses, as well as\nother geometric patterns, is a challenging problem as the potential number of\nsets of elements that compose shapes is exponentially large in the size of the\ndataset and the pattern. In this paper, we denote geometric patterns as\nconstellation queries and propose algorithms to find them in large data\napplications. Our methods combine quadtrees, matrix multiplication, and\nunindexed join processing to discover sets of points that match a geometric\npattern within some additive factor on the pairwise distances. Our distributed\nexperiments show that the choice of composition algorithm (matrix\nmultiplication or nested loops) depends on the freedom introduced in the query\ngeometry through the distance additive factor. Three clearly identified blocks\nof threshold values guide the choice of the best composition algorithm.\nFinally, solving the problem for relative distances requires a novel\ncontinuous-to-discrete transformation. To the best of our knowledge this paper\nis the first to investigate constellation queries at scale.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 23:45:46 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Porto", "Fabio", ""], ["Khatibi", "Amir", ""], ["Nobre", "Jo\u00e3o R.", ""], ["Ogasawara", "Eduardo", ""], ["Valduriez", "Patrick", ""], ["Shasha", "Dennis", ""]]}, {"id": "1703.02722", "submitter": "Chang Yao", "authors": "Chang Yao, Meihui Zhang, Qian Lin, Beng Chin Ooi, Jiatao Xu", "title": "Scaling Distributed Transaction Processing and Recovery based on\n  Dependency Logging", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DGCC protocol has been shown to achieve good performance on multi-core\nin-memory system. However, distributed transactions complicate the dependency\nresolution, and therefore, an effective transaction partitioning strategy is\nessential to reduce expensive multi-node distributed transactions. During\nfailure recovery, log must be examined from the last checkpoint onwards and the\naffected transactions are re-executed based on the way they are partitioned and\nexecuted. Existing approaches treat both transaction management and recovery as\ntwo separate problems, even though recovery is dependent on the sequence in\nwhich transactions are executed.\n  In this paper, we propose to treat the transaction management and recovery\nproblems as one. We first propose an efficient Distributed Dependency Graph\nbased Concurrency Control (DistDGCC) protocol for handling transactions\nspanning multiple nodes, and propose a new novel and efficient logging protocol\ncalled Dependency Logging that also makes use of dependency graphs for\nefficient logging and recovery. DistDGCC optimizes the average cost for each\ndistributed transaction by processing transactions in batch. Moreover, it also\nreduces the effects of thread blocking caused by distributed transactions and\nconsequently improves the runtime performance. Further, dependency logging\nexploits the same data structure that is used by DistDGCC to reduce the logging\noverhead, as well as the logical dependency information to improve the recovery\nparallelism. Extensive experiments are conducted to evaluate the performance of\nour proposed technique against state-of-the-art techniques. Experimental\nresults show that DistDGCC is efficient and scalable, and dependency logging\nsupports fast recovery with marginal runtime overhead. Hence, the overall\nsystem performance is significantly improved as a result.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 06:21:07 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Yao", "Chang", ""], ["Zhang", "Meihui", ""], ["Lin", "Qian", ""], ["Ooi", "Beng Chin", ""], ["Xu", "Jiatao", ""]]}, {"id": "1703.03147", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Atri Rudra", "title": "Juggling Functions Inside a Database", "comments": "arXiv admin note: text overlap with arXiv:1504.04044", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study the Functional Aggregate Query (FAQ) problem, which\ncaptures common computational tasks across a very wide range of domains\nincluding relational databases, logic, matrix and tensor computation,\nprobabilistic graphical models, constraint satisfaction, and signal processing.\nSimply put, an FAQ is a declarative way of defining a new function from a\ndatabase of input functions.\n  We present \"InsideOut\", a dynamic programming algorithm, to evaluate an FAQ.\nThe algorithm rewrites the input query into a set of easier-to-compute FAQ\nsub-queries. Each sub-query is then evaluated using a worst-case optimal\nrelational join algorithm. The topic of designing algorithms to optimally\nevaluate the classic multiway join problem has seen exciting developments in\nthe past few years. Our framework tightly connects these new ideas in database\ntheory with a vast number of application areas in a coherent manner, showing\npotentially that a good database engine can be a general-purpose constraint\nsolver, relational data store, graphical model inference engine, and\nmatrix/tensor computation processor all at once.\n  The InsideOut algorithm is very simple, as shall be described in this paper.\nYet, in spite of solving an extremely general problem, its runtime either is as\ngood as or improves upon the best known algorithm for the applications that FAQ\nspecializes to. These corollaries include computational tasks in graphical\nmodel inference, matrix/tensor operations, relational joins, and logic. Better\nyet, InsideOut can be used within any database engine, because it is basically\na principled way of rewriting queries. Indeed, it is already part of the\nLogicBlox database engine, helping efficiently answer traditional database\nqueries, graphical model inference queries, and train a large class of machine\nlearning models inside the database itself.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 06:08:01 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Rudra", "Atri", ""]]}, {"id": "1703.03201", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Mika\\\"el Monet, Pierre Senellart", "title": "Conjunctive Queries on Probabilistic Graphs: Combined Complexity", "comments": "36 pages including 4 appendix sections. This is the PODS'17 article\n  with all proofs and all reviewer feedback. Relative to the previous version\n  and to the PODS version, this version adds details about a subtle point in\n  Appendix D, and fixes some minor formatting issues", "journal-ref": null, "doi": "10.1145/3034786.3056121", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query evaluation over probabilistic databases is known to be intractable in\nmany cases, even in data complexity, i.e., when the query is fixed. Although\nsome restrictions of the queries [19] and instances [4] have been proposed to\nlower the complexity, these known tractable cases usually do not apply to\ncombined complexity, i.e., when the query is not fixed. This leaves open the\nquestion of which query and instance languages ensure the tractability of\nprobabilistic query evaluation in combined complexity.\n  This paper proposes the first general study of the combined complexity of\nconjunctive query evaluation on probabilistic instances over binary signatures,\nwhich we can alternatively phrase as a probabilistic version of the graph\nhomomorphism problem, or of a constraint satisfaction problem (CSP) variant. We\nstudy the complexity of this problem depending on whether instances and queries\ncan use features such as edge labels, disconnectedness, branching, and edges in\nboth directions. We show that the complexity landscape is surprisingly rich,\nusing a variety of technical tools: automata-based compilation to d-DNNF\nlineages as in [4], \\b{eta}-acyclic lineages using [10], the X-property for\ntractable CSP from [24], graded DAGs [27] and various coding techniques for\nhardness proofs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 09:50:57 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 17:19:26 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Monet", "Mika\u00ebl", ""], ["Senellart", "Pierre", ""]]}, {"id": "1703.03524", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Mostafa Milani", "title": "The Ontological Multidimensional Data Model", "comments": "Extended abstract. This version with minor revisions and slightly\n  extended. To appear in Proc. AMW'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this extended abstract we describe, mainly by examples, the main elements\nof the Ontological Multidimensional Data Model, which considerably extends a\nrelational reconstruction of the multidimensional data model proposed by\nHurtado and Mendelzon by means of tuple-generating dependencies,\nequality-generating dependencies, and negative constraints as found in\nDatalog+-. We briefly mention some good computational properties of the model.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 02:48:29 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 01:01:48 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Milani", "Mostafa", ""]]}, {"id": "1703.03856", "submitter": "Laurel Orr", "authors": "Laurel Orr, Magda Balazinska, and Dan Suciu", "title": "Probabilistic Database Summarization for Interactive Data Exploration", "comments": "To appear VLDB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic approach to generate a small, query-able summary\nof a dataset for interactive data exploration. Departing from traditional\nsummarization techniques, we use the Principle of Maximum Entropy to generate a\nprobabilistic representation of the data that can be used to give approximate\nquery answers. We develop the theoretical framework and formulation of our\nprobabilistic representation and show how to use it to answer queries. We then\npresent solving techniques and give three critical optimizations to improve\npreprocessing time and query accuracy. Lastly, we experimentally evaluate our\nwork using a 5 GB dataset of flights within the United States and a 210 GB\ndataset from an astronomy particle simulation. While our current work only\nsupports linear queries, we show that our technique can successfully answer\nqueries faster than sampling while introducing, on average, no more error than\nsampling and can better distinguish between rare and nonexistent values.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 22:17:22 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 20:44:53 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Orr", "Laurel", ""], ["Balazinska", "Magda", ""], ["Suciu", "Dan", ""]]}, {"id": "1703.04057", "submitter": "Anh Dinh", "authors": "Tien Tuan Anh Dinh, Ji Wang, Gang Chen, Rui Liu, Beng Chin Ooi,\n  Kian-Lee Tan", "title": "BLOCKBENCH: A Framework for Analyzing Private Blockchains", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technologies are taking the world by storm. Public blockchains,\nsuch as Bitcoin and Ethereum, enable secure peer-to-peer applications like\ncrypto-currency or smart contracts. Their security and performance are well\nstudied. This paper concerns recent private blockchain systems designed with\nstronger security (trust) assumption and performance requirement. These systems\ntarget and aim to disrupt applications which have so far been implemented on\ntop of database systems, for example banking, finance applications. Multiple\nplatforms for private blockchains are being actively developed and fine tuned.\nHowever, there is a clear lack of a systematic framework with which different\nsystems can be analyzed and compared against each other. Such a framework can\nbe used to assess blockchains' viability as another distributed data processing\nplatform, while helping developers to identify bottlenecks and accordingly\nimprove their platforms.\n  In this paper, we first describe BlockBench, the first evaluation framework\nfor analyzing private blockchains. It serves as a fair means of comparison for\ndifferent platforms and enables deeper understanding of different system design\nchoices. Any private blockchain can be integrated to BlockBench via simple APIs\nand benchmarked against workloads that are based on real and synthetic smart\ncontracts. BlockBench measures overall and component-wise performance in terms\nof throughput, latency, scalability and fault-tolerance. Next, we use\nBlockBench to conduct comprehensive evaluation of three major private\nblockchains: Ethereum, Parity and Hyperledger Fabric. The results demonstrate\nthat these systems are still far from displacing current database systems in\ntraditional data processing workloads. Furthermore, there are gaps in\nperformance among the three systems which are attributed to the design choices\nat different layers of the software stack.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 02:10:06 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Dinh", "Tien Tuan Anh", ""], ["Wang", "Ji", ""], ["Chen", "Gang", ""], ["Liu", "Rui", ""], ["Ooi", "Beng Chin", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1703.04206", "submitter": "S. Matthew English", "authors": "S. Matthew English and Ehsan Nezhadian", "title": "Application of Bitcoin Data-Structures & Design Principles to Supply\n  Chain Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heretofore the concept of \"blockchain\" has not been precisely defined.\nAccordingly the potential useful applications of this technology have been\nlargely inflated. This work sidesteps the question of what constitutes a\nblockchain as such and focuses on the architectural components of the Bitcoin\ncryptocurrency, insofar as possible, in isolation. We consider common problems\ninherent in the design of effective supply chain management systems. With each\nidentified problem we propose a solution that utilizes one or more component\naspects of Bitcoin. This culminates in five design principles for increased\nefficiency in supply chain management systems through the application of\nincentive mechanisms and data structures native to the Bitcoin cryptocurrency\nprotocol.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 00:32:14 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["English", "S. Matthew", ""], ["Nezhadian", "Ehsan", ""]]}, {"id": "1703.04290", "submitter": "Lucas Braun", "authors": "Lucas Braun, Renato Marroquin, Kai-En Tsay, Donald Kossmann", "title": "MTBase: Optimizing Cross-Tenant Database Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, many business applications have moved into the cloud. In\nparticular, the \"database-as-a-service\" paradigm has become mainstream. While\nexisting multi-tenant data management systems focus on single-tenant query\nprocessing, we believe that it is time to rethink how queries can be processed\nacross multiple tenants in such a way that we do not only gain more valuable\ninsights, but also at minimal cost. As we will argue in this paper, standard\nSQL semantics are insufficient to process cross-tenant queries in an\nunambiguous way, which is why existing systems use other, expensive means like\nETL or data integration. We first propose MTSQL, a set of extensions to\nstandard SQL, which fixes the ambiguity problem. Next, we present MTBase, a\nquery processing middleware that efficiently processes MTSQL on top of SQL. As\nwe will see, there is a canonical, provably correct, rewrite algorithm from\nMTSQL to SQL, which may however result in poor query execution performance,\neven on high-performance database products. We further show that with\ncarefully-designed optimizations, execution times can be reduced in such ways\nthat the difference to single-tenant queries becomes marginal.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 08:35:39 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Braun", "Lucas", ""], ["Marroquin", "Renato", ""], ["Tsay", "Kai-En", ""], ["Kossmann", "Donald", ""]]}, {"id": "1703.04780", "submitter": "Maximilian Schleich", "authors": "Mahmoud Abo Khamis and Hung Q. Ngo and XuanLong Nguyen and Dan Olteanu\n  and Maximilian Schleich", "title": "Learning Models over Relational Data using Sparse Tensors and Functional\n  Dependencies", "comments": "61 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated solutions for analytics over relational databases are of great\npractical importance as they avoid the costly repeated loop data scientists\nhave to deal with on a daily basis: select features from data residing in\nrelational databases using feature extraction queries involving joins,\nprojections, and aggregations; export the training dataset defined by such\nqueries; convert this dataset into the format of an external learning tool; and\ntrain the desired model using this tool. These integrated solutions are also a\nfertile ground of theoretically fundamental and challenging problems at the\nintersection of relational and statistical data models.\n  This article introduces a unified framework for training and evaluating a\nclass of statistical learning models over relational databases. This class\nincludes ridge linear regression, polynomial regression, factorization\nmachines, and principal component analysis. We show that, by synergizing key\ntools from database theory such as schema information, query structure,\nfunctional dependencies, recent advances in query evaluation algorithms, and\nfrom linear algebra such as tensor and matrix operations, one can formulate\nrelational analytics problems and design efficient (query and data)\nstructure-aware algorithms to solve them.\n  This theoretical development informed the design and implementation of the\nAC/DC system for structure-aware learning. We benchmark the performance of\nAC/DC against R, MADlib, libFM, and TensorFlow. For typical retail forecasting\nand advertisement planning applications, AC/DC can learn polynomial regression\nmodels and factorization machines with at least the same accuracy as its\ncompetitors and up to three orders of magnitude faster than its competitors\nwhenever they do not run out of memory, exceed 24-hour timeout, or encounter\ninternal design limitations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:27:09 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 21:08:38 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 19:48:12 GMT"}, {"version": "v4", "created": "Sun, 18 Nov 2018 12:23:53 GMT"}, {"version": "v5", "created": "Thu, 6 Feb 2020 21:16:32 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Nguyen", "XuanLong", ""], ["Olteanu", "Dan", ""], ["Schleich", "Maximilian", ""]]}, {"id": "1703.05028", "submitter": "Sen Wu", "authors": "Sen Wu, Luke Hsiao, Xiao Cheng, Braden Hancock, Theodoros Rekatsinas,\n  Philip Levis, Christopher R\\'e", "title": "Fonduer: Knowledge Base Construction from Richly Formatted Data", "comments": null, "journal-ref": "SIGMOD 2018", "doi": "10.1145/3183713.3183729", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on knowledge base construction (KBC) from richly formatted data. In\ncontrast to KBC from text or tabular data, KBC from richly formatted data aims\nto extract relations conveyed jointly via textual, structural, tabular, and\nvisual expressions. We introduce Fonduer, a machine-learning-based KBC system\nfor richly formatted data. Fonduer presents a new data model that accounts for\nthree challenging characteristics of richly formatted data: (1) prevalent\ndocument-level relations, (2) multimodality, and (3) data variety. Fonduer uses\na new deep-learning model to automatically capture the representation (i.e.,\nfeatures) needed to learn how to extract relations from richly formatted data.\nFinally, Fonduer provides a new programming model that enables users to convert\ndomain expertise, based on multiple modalities of information, to meaningful\nsignals of supervision for training a KBC system. Fonduer-based KBC systems are\nin production for a range of use cases, including at a major online retailer.\nWe compare Fonduer against state-of-the-art KBC approaches in four different\ndomains. We show that Fonduer achieves an average improvement of 41 F1 points\non the quality of the output knowledge base---and in some cases produces up to\n1.87x the number of correct entries---compared to expert-curated public\nknowledge bases. We also conduct a user study to assess the usability of\nFonduer's new programming model. We show that after using Fonduer for only 30\nminutes, non-domain experts are able to design KBC systems that achieve on\naverage 23 F1 points higher quality than traditional machine-learning-based KBC\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 09:12:29 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 01:06:12 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Wu", "Sen", ""], ["Hsiao", "Luke", ""], ["Cheng", "Xiao", ""], ["Hancock", "Braden", ""], ["Rekatsinas", "Theodoros", ""], ["Levis", "Philip", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1703.05160", "submitter": "Ryan Spring", "authors": "Ryan Spring, Anshumali Shrivastava", "title": "A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators\n  for Partition Function Computation in Log-Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-linear models are arguably the most successful class of graphical models\nfor large-scale applications because of their simplicity and tractability.\nLearning and inference with these models require calculating the partition\nfunction, which is a major bottleneck and intractable for large state spaces.\nImportance Sampling (IS) and MCMC-based approaches are lucrative. However, the\ncondition of having a \"good\" proposal distribution is often not satisfied in\npractice.\n  In this paper, we add a new dimension to efficient estimation via sampling.\nWe propose a new sampling scheme and an unbiased estimator that estimates the\npartition function accurately in sub-linear time. Our samples are generated in\nnear-constant time using locality sensitive hashing (LSH), and so are\ncorrelated and unnormalized. We demonstrate the effectiveness of our proposed\napproach by comparing the accuracy and speed of estimating the partition\nfunction against other state-of-the-art estimation techniques including IS and\nthe efficient variant of Gumbel-Max sampling. With our efficient sampling\nscheme, we accurately train real-world language models using only 1-2% of\ncomputations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 14:01:21 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Spring", "Ryan", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1703.05468", "submitter": "Yongjoo Park", "authors": "Yongjoo Park, Ahmad Shahab Tajik, Michael Cafarella, Barzan Mozafari", "title": "Database Learning: Toward a Database that Becomes Smarter Every Time", "comments": "This manuscript is an extended report of the work published in ACM\n  SIGMOD conference 2017", "journal-ref": null, "doi": "10.1145/3035918.3064013", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's databases, previous query answers rarely benefit answering future\nqueries. For the first time, to the best of our knowledge, we change this\nparadigm in an approximate query processing (AQP) context. We make the\nfollowing observation: the answer to each query reveals some degree of\nknowledge about the answer to another query because their answers stem from the\nsame underlying distribution that has produced the entire dataset. Exploiting\nand refining this knowledge should allow us to answer queries more\nanalytically, rather than by reading enormous amounts of raw data. Also,\nprocessing more queries should continuously enhance our knowledge of the\nunderlying distribution, and hence lead to increasingly faster response times\nfor future queries.\n  We call this novel idea---learning from past query answers---Database\nLearning. We exploit the principle of maximum entropy to produce answers, which\nare in expectation guaranteed to be more accurate than existing sample-based\napproximations. Empowered by this idea, we build a query engine on top of Spark\nSQL, called Verdict. We conduct extensive experiments on real-world query\ntraces from a large customer of a major database vendor. Our results\ndemonstrate that Verdict supports 73.7% of these queries, speeding them up by\nup to 23.0x for the same accuracy level compared to existing AQP systems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 03:36:28 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 21:47:25 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Park", "Yongjoo", ""], ["Tajik", "Ahmad Shahab", ""], ["Cafarella", "Michael", ""], ["Mozafari", "Barzan", ""]]}, {"id": "1703.05481", "submitter": "Ashish Sureka", "authors": "Kunal Gupta, Astha Sachdev, Ashish Sureka", "title": "Empirical Analysis on Comparing the Performance of Alpha Miner Algorithm\n  in SQL Query Language and NoSQL Column-Oriented Databases Using Apache\n  Phoenix", "comments": "The work presented in this paper is extension of the work presented\n  in (1) Kunal Gupta et al. in C3S2E 2015 (2) Astha Sachdev et al in Big Data\n  Analytics (BDA 2015) . The study presented in this paper has several more\n  results which are not present in these papers due to limited space in the\n  conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process-Aware Information Systems (PAIS) is an IT system that support\nbusiness processes and generate large amounts of event logs from the execution\nof business processes. An event log is represented as a tuple of CaseID,\nTimestamp, Activity and Actor. Process Mining is a new and emerging field that\naims at analyzing the event logs to discover, enhance and improve business\nprocesses and check conformance between run time and design time business\nprocesses. The large volume of event logs generated are stored in the\ndatabases. Relational databases perform well for a certain class of\napplications. However, there are a certain class of applications for which\nrelational databases are not able to scale. To handle such class of\napplications, NoSQL database systems emerged. Discovering a process model\n(workflow model) from event logs is one of the most challenging and important\nProcess Mining task. The $\\alpha$-miner algorithm is one of the first and most\nwidely used Process Discovery technique. Our objective is to investigate which\nof the databases (Relational or NoSQL) performs better for a Process Discovery\napplication under Process Mining. We implement the $\\alpha$-miner algorithm on\nrelational (row-oriented) and NoSQL (column-oriented) databases in database\nquery languages so that our algorithm is tightly coupled to the database. We\npresent a performance benchmarking and comparison of the $\\alpha$-miner\nalgorithm on row-oriented database and NoSQL column-oriented database so that\nwe can compare which database can efficiently store massive event logs and\nanalyze it in seconds to discover a process model.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 06:32:11 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Gupta", "Kunal", ""], ["Sachdev", "Astha", ""], ["Sureka", "Ashish", ""]]}, {"id": "1703.05547", "submitter": "Hamida Seba", "authors": "C. Nabti, T. Mecharnia, S. E. Boukhetta, H. Seba and K. Amrouche", "title": "Compact Neighborhood Index for Subgraph Queries in Massive Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph queries also known as subgraph isomorphism search is a fundamental\nproblem in querying graph-like structured data. It consists to enumerate the\nsubgraphs of a data graph that match a query graph. This problem arises in many\nreal-world applications related to query processing or pattern recognition such\nas computer vision, social network analysis, bioinformatic and big data\nanalytic. Subgraph isomorphism search knows a lot of investigations and\nsolutions mainly because of its importance and use but also because of its\nNP-completeness. Existing solutions use filtering mechanisms and optimise the\norder within witch the query vertices are matched on the data vertices to\nobtain acceptable processing times. However, existing approaches are iterative\nand generate several intermediate results. They also require that the data\ngraph is loaded in main memory and consequently are not adapted to large graphs\nthat do not fit into memory or are accessed by streams. To tackle this problem,\nwe propose a new approach based on concepts widely different from existing\nworks. Our approach distills the semantic and topological information that\nsurround a vertex into a simple integer. This simple vertex encoding that can\nbe computed and updated incrementally reduces considerably intermediate results\nand avoid to load the entire data graph into main memory. We evaluate our\napproach on several real-word datasets. The experimental results show that our\napproach is efficient and scalable.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 10:20:31 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 08:50:03 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 10:41:50 GMT"}, {"version": "v4", "created": "Tue, 10 Jul 2018 15:39:47 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Nabti", "C.", ""], ["Mecharnia", "T.", ""], ["Boukhetta", "S. E.", ""], ["Seba", "H.", ""], ["Amrouche", "K.", ""]]}, {"id": "1703.05740", "submitter": "Marco Montali", "authors": "Wil M. P. van der Aalst, Guangming Li, Marco Montali", "title": "Object-Centric Behavioral Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's process modeling languages often force the analyst or modeler to\nstraightjacket real-life processes into simplistic or incomplete models that\nfail to capture the essential features of the domain under study. Conventional\nbusiness process models only describe the lifecycles of individual instances\n(cases) in isolation. Although process models may include data elements (cf.\nBPMN), explicit connections to real data models (e.g., an entity relationship\nmodel or a UML class model) are rarely made. Therefore, we propose a novel\napproach that extends data models with a behavioral perspective. Data models\ncan easily deal with many-to-many and one-to-many relationships. This is\nexploited to create process models that can also model complex interactions\nbetween different types of instances. Classical multiple-instance problems are\ncircumvented by using the data model for event correlation. The declarative\nnature of the proposed language makes it possible to model behavioral\nconstraints over activities like cardinality constraints in data models. The\nresulting object-centric behavioral constraint (OCBC) model is able to describe\nprocesses involving interacting instances and complex data dependencies. In\nthis paper, we introduce the OCBC model and notation, providing a number of\nexamples that give a flavour of the approach. We then define a set-theoretic\nsemantics exploiting cardinality constraints within and across time points. We\nfinally formalize conformance checking in our setting, arguing that evaluating\nconformance against OCBC models requires diagnostics that go beyond what is\nprovided by contemporary conformance checking approaches.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:18:36 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["van der Aalst", "Wil M. P.", ""], ["Li", "Guangming", ""], ["Montali", "Marco", ""]]}, {"id": "1703.06103", "submitter": "Thomas Kipf", "authors": "Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den\n  Berg, Ivan Titov, Max Welling", "title": "Modeling Relational Data with Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs enable a wide variety of applications, including question\nanswering and information retrieval. Despite the great effort invested in their\ncreation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata)\nremain incomplete. We introduce Relational Graph Convolutional Networks\n(R-GCNs) and apply them to two standard knowledge base completion tasks: Link\nprediction (recovery of missing facts, i.e. subject-predicate-object triples)\nand entity classification (recovery of missing entity attributes). R-GCNs are\nrelated to a recent class of neural networks operating on graphs, and are\ndeveloped specifically to deal with the highly multi-relational data\ncharacteristic of realistic knowledge bases. We demonstrate the effectiveness\nof R-GCNs as a stand-alone model for entity classification. We further show\nthat factorization models for link prediction such as DistMult can be\nsignificantly improved by enriching them with an encoder model to accumulate\nevidence over multiple inference steps in the relational graph, demonstrating a\nlarge improvement of 29.8% on FB15k-237 over a decoder-only baseline.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 17:09:14 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 13:43:41 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 15:49:12 GMT"}, {"version": "v4", "created": "Thu, 26 Oct 2017 19:53:49 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Schlichtkrull", "Michael", ""], ["Kipf", "Thomas N.", ""], ["Bloem", "Peter", ""], ["Berg", "Rianne van den", ""], ["Titov", "Ivan", ""], ["Welling", "Max", ""]]}, {"id": "1703.06348", "submitter": "Andrea Detti PhD", "authors": "Andrea Detti, Michele Orru, Riccardo Paolillo, Giulio Rossi, Pierpaolo\n  Loreti, Lorenzo Bracciale, Nicola Blefari Melazzi", "title": "Application of Information Centric Networking to NoSQL Databases: the\n  Spatio-Temporal use case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores methodologies, advantages and challenges related to the\nuse of the Information Centric Network technology for developing NoSQL\ndistributed databases, which are expected to play a central role in the\nforthcoming IoT and BigData era. ICN services make possible to simplify the\ndevelopment of the database software, improve performance, and provide\ndata-level access control. We use our findings for devising a NoSQL\nspatio-temporal database, named OpenGeoBase, and evaluate its performance with\na real data set related to Intelligent Transport System applications.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 20:31:37 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Detti", "Andrea", ""], ["Orru", "Michele", ""], ["Paolillo", "Riccardo", ""], ["Rossi", "Giulio", ""], ["Loreti", "Pierpaolo", ""], ["Bracciale", "Lorenzo", ""], ["Melazzi", "Nicola Blefari", ""]]}, {"id": "1703.07116", "submitter": "Niek Tax", "authors": "Niek Tax and Benjamin Dalmas and Natalia Sidorova and Wil M P van der\n  Aalst and Sylvie Norre", "title": "Interest-Driven Discovery of Local Process Models", "comments": "submitted to the International Conference on Business Process\n  Management (BPM) 2017", "journal-ref": "Information Systems (2018), 1-31", "doi": "10.1016/j.is.2018.04.006", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Process Models (LPM) describe structured fragments of process behavior\noccurring in the context of less structured business processes. Traditional LPM\ndiscovery aims to generate a collection of process models that describe highly\nfrequent behavior, but these models do not always provide useful answers for\nquestions posed by process analysts aiming at business process improvement. We\npropose a framework for goal-driven LPM discovery, based on utility functions\nand constraints. We describe four scopes on which these utility functions and\nconstrains can be defined, and show that utility functions and constraints on\ndifferent scopes can be combined to form composite utility\nfunctions/constraints. Finally, we demonstrate the applicability of our\napproach by presenting several actionable business insights discovered with LPM\ndiscovery on two real life data sets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 09:57:59 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Tax", "Niek", ""], ["Dalmas", "Benjamin", ""], ["Sidorova", "Natalia", ""], ["van der Aalst", "Wil M P", ""], ["Norre", "Sylvie", ""]]}, {"id": "1703.07138", "submitter": "R\\'emi Cura", "authors": "R\\'emi Cura, Bertrand Dumenieu, Nathalie Abadie, Benoit Costes, Julien\n  Perret, Maurizio Gribaudi", "title": "Historical collaborative geocoding", "comments": "WORKING PAPER", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest developments in digital have provided large data sets that can\nincreasingly easily be accessed and used. These data sets often contain\nindirect localisation information, such as historical addresses. Historical\ngeocoding is the process of transforming the indirect localisation information\nto direct localisation that can be placed on a map, which enables spatial\nanalysis and cross-referencing. Many efficient geocoders exist for current\naddresses, but they do not deal with the temporal aspect and are based on a\nstrict hierarchy (..., city, street, house number) that is hard or impossible\nto use with historical data. Indeed historical data are full of uncertainties\n(temporal aspect, semantic aspect, spatial precision, confidence in historical\nsource, ...) that can not be resolved, as there is no way to go back in time to\ncheck. We propose an open source, open data, extensible solution for geocoding\nthat is based on the building of gazetteers composed of geohistorical objects\nextracted from historical topographical maps. Once the gazetteers are\navailable, geocoding an historical address is a matter of finding the\ngeohistorical object in the gazetteers that is the best match to the historical\naddress. The matching criteriae are customisable and include several dimensions\n(fuzzy semantic, fuzzy temporal, scale, spatial precision ...). As the goal is\nto facilitate historical work, we also propose web-based user interfaces that\nhelp geocode (one address or batch mode) and display over current or historical\ntopographical maps, so that they can be checked and collaboratively edited. The\nsystem is tested on Paris city for the 19-20th centuries, shows high returns\nrate and is fast enough to be used interactively.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 10:45:09 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 08:06:43 GMT"}, {"version": "v3", "created": "Sun, 13 May 2018 05:10:35 GMT"}, {"version": "v4", "created": "Thu, 31 May 2018 01:17:43 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Cura", "R\u00e9mi", ""], ["Dumenieu", "Bertrand", ""], ["Abadie", "Nathalie", ""], ["Costes", "Benoit", ""], ["Perret", "Julien", ""], ["Gribaudi", "Maurizio", ""]]}, {"id": "1703.07213", "submitter": "Lorena Etcheverry", "authors": "Lorena Etcheverry and Alejandro A. Vaisman", "title": "Efficient Analytical Queries on Semantic Web Data Cubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of multidimensional data published on the semantic web (SW) is\nconstantly increasing, due to initiatives such as Open Data and Open Government\nData, among other ones. Models, languages, and tools, that allow to obtain\nvaluable information efficiently, are thus required. Multidimensional data are\ntypically represented as data cubes, and exploited using Online Analytical\nProcessing (OLAP) techniques. The RDF Data Cube Vocabulary, also denoted QB, is\nthe current W3C standard to represent statistical data on the SW.Since QB does\nnot include key features needed for OLAP analysis, in previous work we have\nproposed an extension, denoted QB4OLAP, to overcome this problem without the\nneed of modifying already published data. Once data cubes are represented on\nthe SW, we need tools to analyze them. However, writing efficient analytical\nqueries over SW cubes demands a deep knowledge of RDF and SPARQL. These skills\nare not common in typical analytical users. Also, OLAP languages like MDX are\nfar from being easily understood by the final user. The lack of friendly tools\nto exploit multidimensional data on the SW is a barrier that needs to be broken\nto promote the publication of such data. We address this problem in this paper.\nOur approach is based on allowing analytical users to write queries using OLAP\noperations over cubes, without dealing with SW standards. For this, we devised\nCQL (standing for Cube Query Language), a simple, high-level query language\nthat operates over cubes. Using the metadata provided by QB4OLAP, we translate\nCQL queries into SPARQL. Then, we propose query improvement strategies to\nproduce efficient SPARQL queries, adapting SPARQL query optimization\ntechniques. We evaluate our approach using the Star-Schema benchmark, showing\nthat our proposal outperforms others. A web application that allows querying SW\ndata cubes using CQL, completes our contributions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 13:41:51 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Etcheverry", "Lorena", ""], ["Vaisman", "Alejandro A.", ""]]}, {"id": "1703.07342", "submitter": "Dylan Hutchison", "authors": "Dylan Hutchison, Bill Howe, Dan Suciu", "title": "LaraDB: A Minimalist Kernel for Linear and Relational Algebra\n  Computation", "comments": "10 pages, to appear in the BeyondMR workshop at the 2017 ACM SIGMOD\n  conference", "journal-ref": null, "doi": "10.1145/3070607.3070608", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytics tasks manipulate structured data with variants of relational\nalgebra (RA) and quantitative data with variants of linear algebra (LA). The\ntwo computational models have overlapping expressiveness, motivating a common\nprogramming model that affords unified reasoning and algorithm design. At the\nlogical level we propose Lara, a lean algebra of three operators, that\nexpresses RA and LA as well as relevant optimization rules. We show a series of\nproofs that position Lara %formal and informal at just the right level of\nexpressiveness for a middleware algebra: more explicit than MapReduce but more\ngeneral than RA or LA. At the physical level we find that the Lara operators\nafford efficient implementations using a single primitive that is available in\na variety of backend engines: range scans over partitioned sorted maps.\n  To evaluate these ideas, we implemented the Lara operators as range iterators\nin Apache Accumulo, a popular implementation of Google's BigTable. First we\nshow how Lara expresses a sensor quality control task, and we measure the\nperformance impact of optimizations Lara admits on this task. Second we show\nthat the LaraDB implementation outperforms Accumulo's native MapReduce\nintegration on a core task involving join and aggregation in the form of matrix\nmultiply, especially at smaller scales that are typically a poor fit for\nscale-out approaches. We find that LaraDB offers a conceptually lean framework\nfor optimizing mixed-abstraction analytics tasks, without giving up fast\nrecord-level updates and scans.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 17:56:47 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 19:29:22 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 22:29:28 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Hutchison", "Dylan", ""], ["Howe", "Bill", ""], ["Suciu", "Dan", ""]]}, {"id": "1703.07371", "submitter": "Vishal Jain", "authors": "Vishal Jain and Mahesh Kumar Madan", "title": "Multi Agent Driven Data Mining For Knowledge Discovery in Cloud\n  Computing", "comments": null, "journal-ref": "International Journal of Computer Science & Information Technology\n  Research Excellence Vol. 2, Issue 1, Jan-Feb 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, huge amount of data is available on the web. Now there is a need to\nconvert that data in knowledge which can be useful for different purposes. This\npaper depicts the use of data mining process, OLAP with the combination of\nmulti agent system to find the knowledge from data in cloud computing. For\nthis, I am also trying to explain one case study of online shopping of one\nBakery Shop. May be we can increase the sale of items by using the model, which\nI am trying to represent.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 18:06:29 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Jain", "Vishal", ""], ["Madan", "Mahesh Kumar", ""]]}, {"id": "1703.07484", "submitter": "Milos Nikolic", "authors": "Milos Nikolic and Dan Olteanu", "title": "Incremental View Maintenance with Triple Lock Factorization Benefits", "comments": "27 pages, 13 figures, a shorter version appeared in SIGMOD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce F-IVM, a unified incremental view maintenance (IVM) approach for\na variety of tasks, including gradient computation for learning linear\nregression models over joins, matrix chain multiplication, and factorized\nevaluation of conjunctive queries.\n  F-IVM is a higher-order IVM algorithm that reduces the maintenance of the\ngiven task to the maintenance of a hierarchy of increasingly simpler views. The\nviews are functions mapping keys, which are tuples of input data values, to\npayloads, which are elements from a task-specific ring. Whereas the computation\nover the keys is the same for all tasks, the computation over the payloads\ndepends on the task. F-IVM achieves efficiency by factorizing the computation\nof the keys, payloads, and updates.\n  We implemented F-IVM as an extension of DBToaster. We show in a range of\nscenarios that it can outperform classical first-order IVM, DBToaster's fully\nrecursive higher-order IVM, and plain recomputation by orders of magnitude\nwhile using less memory.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 01:39:00 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 22:18:35 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Nikolic", "Milos", ""], ["Olteanu", "Dan", ""]]}, {"id": "1703.07555", "submitter": "Pierre De", "authors": "Landy Rajaonarivo (ENIB), Matthieu Courgeon (ENIB), Eric Maisel\n  (ENIB), Pierre De Loor (ENIB)", "title": "Inline Co-Evolution between Users and Information Presentation for Data\n  Exploration", "comments": null, "journal-ref": "22nd International Conference on Intelligent User Interfaces , Mar\n  2017, Limassol, Cyprus. ACM, IUI '17 Proceedings of the 22nd International\n  Conference on Intelligent User Interfaces pp.215 - 219, 2017, IUI '17\n  Proceedings of the 22nd International Conference on Intelligent User\n  Interfaces", "doi": "10.1145/3025171.3025226", "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an intelligent user interface model dedicated to the\nexploration of complex databases. This model is implemented on a 3D metaphor :\na virtual museum. In this metaphor, the database elements are embodied as\nmuseum objects. The objects are grouped in rooms according to their semantic\nproperties and relationships and the rooms organization forms the museum. Rooms\norganization is not predefi-ned but defined incrementally by taking into\naccount not only the relationships between objects, but also the users centers\nof interest. The latter are evaluated in real-time through user interactions\nwithin the virtual museum. This interface allows for a personal reading and\nfavors the discovery of unsuspec-ted links between data. In this paper, we\npresent our model's formalization as well as its application to the context of\ncultural heritage.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 08:05:30 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Rajaonarivo", "Landy", "", "ENIB"], ["Courgeon", "Matthieu", "", "ENIB"], ["Maisel", "Eric", "", "ENIB"], ["De Loor", "Pierre", "", "ENIB"]]}, {"id": "1703.07617", "submitter": "Jun Sun", "authors": "Yuanzhen Ji, Jun Sun, Anisoara Nica, Zbigniew Jerzak, Gregor\n  Hackenbroich, Christof Fetzer", "title": "Quality-Driven Disorder Handling for M-way Sliding Window Stream Joins", "comments": "12 pages, 11 figures, IEEE ICDE 2016", "journal-ref": null, "doi": "10.1109/ICDE.2016.7498265", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding window join is one of the most important operators for stream\napplications. To produce high quality join results, a stream processing system\nmust deal with the ubiquitous disorder within input streams which is caused by\nnetwork delay, asynchronous source clocks, etc. Disorder handling involves an\ninevitable tradeoff between the latency and the quality of produced join\nresults. To meet different requirements of stream applications, it is desirable\nto provide a user-configurable result-latency vs. result-quality tradeoff.\nExisting disorder handling approaches either do not provide such\nconfigurability, or support only user-specified latency constraints.\n  In this work, we advocate the idea of quality-driven disorder handling, and\npropose a buffer-based disorder handling approach for sliding window joins,\nwhich minimizes sizes of input-sorting buffers, thus the result latency, while\nrespecting user-specified result-quality requirements. The core of our approach\nis an analytical model which directly captures the relationship between sizes\nof input buffers and the produced result quality. Our approach is generic. It\nsupports m-way sliding window joins with arbitrary join conditions. Experiments\non real-world and synthetic datasets show that, compared to the state of the\nart, our approach can reduce the result latency incurred by disorder handling\nby up to 95% while providing the same level of result quality.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 12:27:21 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Ji", "Yuanzhen", ""], ["Sun", "Jun", ""], ["Nica", "Anisoara", ""], ["Jerzak", "Zbigniew", ""], ["Hackenbroich", "Gregor", ""], ["Fetzer", "Christof", ""]]}, {"id": "1703.07795", "submitter": "Qiqi Yan", "authors": "Matthias Ruhl, Mukund Sundararajan, Qiqi Yan", "title": "Hierarchical Summarization of Metric Changes", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study changes in metrics that are defined on a cartesian product of trees.\nSuch metrics occur naturally in many practical applications, where a global\nmetric (such as revenue) can be broken down along several hierarchical\ndimensions (such as location, gender, etc).\n  Given a change in such a metric, our goal is to identify a small set of\nnon-overlapping data segments that account for the change. An organization\ninterested in improving the metric can then focus their attention on these data\nsegments.\n  Our key contribution is an algorithm that mimics the operation of a\nhierarchical organization of analysts. The algorithm has been successfully\napplied, for example within Google Adwords to help advertisers triage the\nperformance of their advertising campaigns.\n  We show that the algorithm is optimal for two dimensions, and has an\napproximation ratio $\\log^{d-2}(n+1)$ for $d \\geq 3$ dimensions, where $n$ is\nthe number of input data segments. For the Adwords application, we can show\nthat our algorithm is in fact a $2$-approximation.\n  Mathematically, we identify a certain data pattern called a \\emph{conflict}\nthat both guides the design of the algorithm, and plays a central role in the\nhardness results. We use these conflicts to both derive a lower bound of\n$1.144^{d-2}$ (again $d\\geq3$) for our algorithm, and to show that the problem\nis NP-hard, justifying the focus on approximation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 18:02:29 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Ruhl", "Matthias", ""], ["Sundararajan", "Mukund", ""], ["Yan", "Qiqi", ""]]}, {"id": "1703.07920", "submitter": "Kaori Abe", "authors": "Kaori Abe, Teppei Suzuki, Shunya Ueta, Akio Nakamura, Yutaka Satoh and\n  Hirokatsu Kataoka", "title": "Changing Fashion Cultures", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel concept that analyzes and visualizes worldwide\nfashion trends. Our goal is to reveal cutting-edge fashion trends without\ndisplaying an ordinary fashion style. To achieve the fashion-based analysis, we\ncreated a new fashion culture database (FCDB), which consists of 76 million\ngeo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of\nmixed fashion styles,the paper also proposes an unsupervised fashion trend\ndescriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal\nanalysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD\neffectively emphasizes consecutive features between two different times. In\nexperiments, we clearly show the analysis of fashion trends and fashion-based\ncity similarity. As the result of large-scale data collection and an\nunsupervised analyzer, the proposed approach achieves world-level fashion\nvisualization in a time series. The code, model, and FCDB will be publicly\navailable after the construction of the project page.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 03:48:08 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Abe", "Kaori", ""], ["Suzuki", "Teppei", ""], ["Ueta", "Shunya", ""], ["Nakamura", "Akio", ""], ["Satoh", "Yutaka", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1703.07994", "submitter": "Andreas Pieris", "authors": "Pablo Barcelo, Gerald Berger, Andreas Pieris", "title": "Containment for Rule-Based Ontology-Mediated Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many efforts have been dedicated to identifying restrictions on ontologies\nexpressed as tuple-generating dependencies (tgds), a.k.a. existential rules,\nthat lead to the decidability for the problem of answering ontology-mediated\nqueries (OMQs). This has given rise to three families of formalisms: guarded,\nnon-recursive, and sticky sets of tgds. In this work, we study the containment\nproblem for OMQs expressed in such formalisms, which is a key ingredient for\nsolving static analysis tasks associated with them. Our main contribution is\nthe development of specially tailored techniques for OMQ containment under the\nclasses of tgds stated above. This enables us to obtain sharp complexity bounds\nfor the problems at hand, which in turn allow us to delimitate its practical\napplicability. We also apply our techniques to pinpoint the complexity of\nproblems associated with two emerging applications of OMQ containment:\ndistribution over components and UCQ rewritability of OMQs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 10:44:18 GMT"}, {"version": "v2", "created": "Sun, 2 Apr 2017 16:13:16 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 00:26:02 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Barcelo", "Pablo", ""], ["Berger", "Gerald", ""], ["Pieris", "Andreas", ""]]}, {"id": "1703.08198", "submitter": "Daniel Lemire", "authors": "Antonio Badia and Daniel Lemire", "title": "On Desirable Semantics of Functional Dependencies over Databases with\n  Incomplete Information", "comments": "to appear in Fundamenta Informaticae", "journal-ref": "Fundamenta Informaticae 158 (2018) 327-352", "doi": "10.3233/FI-2018-1651", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Codd's relational model describes just one possible world. To better cope\nwith incomplete information, extended database models allow several possible\nworlds. Vague tables are one such convenient extended model where attributes\naccept sets of possible values (e.g., the manager is either Jill or Bob).\nHowever, conceptual database design in such cases remains an open problem. In\nparticular, there is no canonical definition of functional dependencies (FDs)\nover possible worlds (e.g., each employee has just one manager). We identify\nseveral desirable properties that the semantics of such FDs should meet\nincluding Armstrong's axioms, the independence from irrelevant attributes,\nseamless satisfaction and implied by strong satisfaction. We show that we can\ndefine FDs such that they have all our desirable properties over vague tables.\nHowever, we also show that no notion of FD can satisfy all our desirable\nproperties over a more general model (disjunctive tables). Our work formalizes\na trade-off between having a general model and having well-behaved FDs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 18:29:03 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 20:08:04 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Badia", "Antonio", ""], ["Lemire", "Daniel", ""]]}, {"id": "1703.08219", "submitter": "Tiark Rompf", "authors": "Gr\\'egory M. Essertel, Ruby Y. Tahboub, James M. Decker, Kevin J.\n  Brown, Kunle Olukotun, Tiark Rompf", "title": "Flare: Native Compilation for Heterogeneous Workloads in Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for modern data analytics to combine relational, procedural, and\nmap-reduce-style functional processing is widely recognized. State-of-the-art\nsystems like Spark have added SQL front-ends and relational query optimization,\nwhich promise an increase in expressiveness and performance. But how good are\nthese extensions at extracting high performance from modern hardware platforms?\n  While Spark has made impressive progress, we show that for relational\nworkloads, there is still a significant gap compared with best-of-breed query\nengines. And when stepping outside of the relational world, query optimization\ntechniques are ineffective if large parts of a computation have to be treated\nas user-defined functions (UDFs).\n  We present Flare: a new back-end for Spark that brings performance closer to\nthe best SQL engines, without giving up the added expressiveness of Spark. We\ndemonstrate order of magnitude speedups both for relational workloads such as\nTPC-H, as well as for a range of machine learning kernels that combine\nrelational and iterative functional processing.\n  Flare achieves these results through (1) compilation to native code, (2)\nreplacing parts of the Spark runtime system, and (3) extending the scope of\noptimization and code generation to large classes of UDFs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 20:04:55 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Essertel", "Gr\u00e9gory M.", ""], ["Tahboub", "Ruby Y.", ""], ["Decker", "James M.", ""], ["Brown", "Kevin J.", ""], ["Olukotun", "Kunle", ""], ["Rompf", "Tiark", ""]]}, {"id": "1703.08273", "submitter": "Shiyu Ji", "authors": "Shiyu Ji, Kun Wan", "title": "An Asymptotically Tighter Bound on Sampling for Frequent Itemsets Mining", "comments": "13 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new error bound on sampling algorithms for\nfrequent itemsets mining. We show that the new bound is asymptotically tighter\nthan the state-of-art bounds, i.e., given the chosen samples, for small enough\nerror probability, the new error bound is roughly half of the existing bounds.\nBased on the new bound, we give a new approximation algorithm, which is much\nsimpler compared to the existing approximation algorithms, but can also\nguarantee the worst approximation error with precomputed sample size. We also\ngive an algorithm which can approximate the top-$k$ frequent itemsets with high\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 02:59:51 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Ji", "Shiyu", ""], ["Wan", "Kun", ""]]}, {"id": "1703.08425", "submitter": "Vineet John", "authors": "Vineet John", "title": "Redynis: Traffic-aware dynamic repartitioning for a distributed\n  key-value store", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.12252.39048", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern data stores tend to be distributed, to enable the scaling of the\ndata across multiple instances of commodity hardware. Although this ensures a\nnear unlimited potential for storage, the data itself is not always ideally\npartitioned, and the cost of a network round-trip may cause a degradation of\nend-user experience with respect to response latency. The problem being solved\nis bringing the data objects closer to the frequent sources of requests using a\ndynamic repartitioning algorithm. This is important if the objective is to\nmitigate the overhead of network latency, and especially so if the partitions\nare widely geo-distributed. The intention is to bring these features to an\nexisting distributed key-value store product, Redis.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 14:35:08 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["John", "Vineet", ""]]}, {"id": "1703.08614", "submitter": "Charles Packer", "authors": "Charles A. Packer, Lawrence B. Holder", "title": "GraphZip: Dictionary-based Compression for Mining Graph Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A massive amount of data generated today on platforms such as social\nnetworks, telecommunication networks, and the internet in general can be\nrepresented as graph streams. Activity in a network's underlying graph\ngenerates a sequence of edges in the form of a stream; for example, a social\nnetwork may generate a graph stream based on the interactions (edges) between\ndifferent users (nodes) over time. While many graph mining algorithms have\nalready been developed for analyzing relatively small graphs, graphs that begin\nto approach the size of real-world networks stress the limitations of such\nmethods due to their dynamic nature and the substantial number of nodes and\nconnections involved.\n  In this paper we present GraphZip, a scalable method for mining interesting\npatterns in graph streams. GraphZip is inspired by the Lempel-Ziv (LZ) class of\ncompression algorithms, and uses a novel dictionary-based compression approach\nin conjunction with the minimum description length principle to discover\nmaximally-compressing patterns in a graph stream. We experimentally show that\nGraphZip is able to retrieve complex and insightful patterns from large\nreal-world graphs and artificially-generated graphs with ground truth patterns.\nAdditionally, our results demonstrate that GraphZip is both highly efficient\nand highly effective compared to existing state-of-the-art methods for mining\ngraph streams.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 22:08:00 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Packer", "Charles A.", ""], ["Holder", "Lawrence B.", ""]]}, {"id": "1703.08668", "submitter": "Dong Wen", "authors": "Dong Wen, Lu Qin, Xuemin Lin, Ying Zhang, Lijun Chang", "title": "Enumerating k-Vertex Connected Components in Large Graphs", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cohesive subgraph detection is an important graph problem that is widely\napplied in many application domains, such as social community detection,\nnetwork visualization, and network topology analysis. Most of existing cohesive\nsubgraph metrics can guarantee good structural properties but may cause the\nfree-rider effect. Here, by free-rider effect, we mean that some irrelevant\nsubgraphs are combined as one subgraph if they only share a small number of\nvertices and edges. In this paper, we study k-vertex connected component\n(k-VCC) which can effectively eliminate the free-rider effect but less studied\nin the literature. A k-VCC is a connected subgraph in which the removal of any\nk-1 vertices will not disconnect the subgraph. In addition to eliminating the\nfree-rider effect, k-VCC also has other advantages such as bounded diameter,\nhigh cohesiveness, bounded graph overlapping, and bounded subgraph number. We\npropose a polynomial time algorithm to enumerate all k-VCCs of a graph by\nrecursively partitioning the graph into overlapped subgraphs. We find that the\nkey to improving the algorithm is reducing the number of local connectivity\ntestings. Therefore, we propose two effective optimization strategies, namely\nneighbor sweep and group sweep, to largely reduce the number of local\nconnectivity testings. We conduct extensive performance studies using seven\nlarge real datasets to demonstrate the effectiveness of this model as well as\nthe efficiency of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 09:36:47 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Wen", "Dong", ""], ["Qin", "Lu", ""], ["Lin", "Xuemin", ""], ["Zhang", "Ying", ""], ["Chang", "Lijun", ""]]}, {"id": "1703.08685", "submitter": "Carl Camilleri", "authors": "Carl Camilleri and Joseph Vella and Vitezslav Nezval", "title": "Thespis: Actor-Based Middleware for Causal Consistency", "comments": "need to withdraw for corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a survey of the current state of the art in\nCausally-Consistent data stores. Furthermore, we present the design of Thespis,\na middleware that innovatively leverages the Actor model to implement causal\nconsistency over an industry-standard data store.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 12:48:55 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 07:27:18 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Camilleri", "Carl", ""], ["Vella", "Joseph", ""], ["Nezval", "Vitezslav", ""]]}, {"id": "1703.08732", "submitter": "Thibault Sellam", "authors": "Thibault Sellam, Martin Kersten", "title": "80 New Packages to Mine Database Query Logs", "comments": "Vision Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The query log of a DBMS is a powerful resource. It enables many practical\napplications, including query optimization and user experience enhancement. And\nyet, mining SQL queries is a difficult task. The fundamental problem is that\nqueries are symbolic objects, not vectors of numbers. Therefore, many popular\nstatistical concepts, such as means, regression, or decision trees do not\napply. Most authors limit themselves to ad hoc algorithms or approaches based\non neighborhoods, such as k Nearest Neighbors. Our project is to challenge this\nlimitation. We introduce methods to manipulate SQL queries as if they were\nvectors, thereby unlocking the whole statistical toolbox. We present three\nfamilies of methods: feature maps, kernel methods, and Bayesian models. The\nfirst technique directly encodes queries into vectors. The second one\ntransforms the queries implicitly. The last one exploits probabilistic\ngraphical models as an alternative to vector spaces. We present the benefits\nand drawbacks of each solution, highlight how they relate to each other, and\nmake the case for future investigation.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 19:00:23 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Sellam", "Thibault", ""], ["Kersten", "Martin", ""]]}, {"id": "1703.09141", "submitter": "Rada Chirkova", "authors": "Rada Chirkova, Jon Doyle, and Juan L. Reutter", "title": "A Framework for Assessing Achievability of Data-Quality Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing and improving the quality of data are fundamental challenges for\ndata-intensive systems that have given rise to applications targeting\ntransformation and cleaning of data. However, while schema design, data\ncleaning, and data migration are now reasonably well understood in isolation,\nnot much attention has been given to the interplay between the tools addressing\nissues in these areas. We focus on the problem of determining whether the\navailable data-processing procedures can be used together to bring about the\ndesired quality of the given data. For instance, consider an organization\nintroducing new data-analysis tasks. Depending on the tasks, it may be a\npriority to determine whether the data can be processed and transformed using\nthe available data-processing tools to satisfy certain properties or quality\nassurances needed for the success of the task. Here, while the organization may\ncontrol some of its tools, some other tools may be external or proprietary,\nwith only basic information available on how they process data. The problem is\nthen, how to decide which tools to apply, and in which order, to make the data\nready for the new tasks?\n  Toward addressing this problem, we develop a new framework that abstracts\ndata-processing tools as black-box procedures with only some of the properties\nexposed, such as the applicability requirements, the parts of the data that the\nprocedure modifies, and the conditions that the data satisfy once the procedure\nhas been applied. We show how common tasks such as data cleaning and data\nmigration are encapsulated into our framework and, as a proof of concept, we\nstudy basic properties of the framework for the case of procedures described by\nstandard relational constraints. While reasoning in this framework may be\ncomputationally infeasible in general, we show that there exist well-behaved\nspecial cases with potential practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 15:25:01 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Chirkova", "Rada", ""], ["Doyle", "Jon", ""], ["Reutter", "Juan L.", ""]]}, {"id": "1703.09193", "submitter": "Saravanan Thirumuruganathan", "authors": "Zoi Kaoudi, Jorge-Arnulfo Quian\\'e-Ruiz, Saravanan Thirumuruganathan,\n  Sanjay Chawla, Divy Agrawal", "title": "A Cost-based Optimizer for Gradient Descent Optimization", "comments": "Accepted at SIGMOD 2017", "journal-ref": null, "doi": "10.1145/3035918.3064042", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the use of machine learning (ML) permeates into diverse application\ndomains, there is an urgent need to support a declarative framework for ML.\nIdeally, a user will specify an ML task in a high-level and easy-to-use\nlanguage and the framework will invoke the appropriate algorithms and system\nconfigurations to execute it. An important observation towards designing such a\nframework is that many ML tasks can be expressed as mathematical optimization\nproblems, which take a specific form. Furthermore, these optimization problems\ncan be efficiently solved using variations of the gradient descent (GD)\nalgorithm. Thus, to decouple a user specification of an ML task from its\nexecution, a key component is a GD optimizer. We propose a cost-based GD\noptimizer that selects the best GD plan for a given ML task. To build our\noptimizer, we introduce a set of abstract operators for expressing GD\nalgorithms and propose a novel approach to estimate the number of iterations a\nGD algorithm requires to converge. Extensive experiments on real and synthetic\ndatasets show that our optimizer not only chooses the best GD plan but also\nallows for optimizations that achieve orders of magnitude performance speed-up.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 17:24:54 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Kaoudi", "Zoi", ""], ["Quian\u00e9-Ruiz", "Jorge-Arnulfo", ""], ["Thirumuruganathan", "Saravanan", ""], ["Chawla", "Sanjay", ""], ["Agrawal", "Divy", ""]]}, {"id": "1703.09218", "submitter": "Rada Chirkova", "authors": "Farid Alborzi, Surajit Chaudhuri, Rada Chirkova, Pallavi Deo,\n  Christopher Healey, Gargi Pingale, Juan Reutter, and Vaira Selvakani", "title": "DataSlicer: Task-Based Data Selection for Visual Data Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual exploration and analysis of data, determining how to select and\ntransform the data for visualization is a challenge for data-unfamiliar or\ninexperienced users. Our main hypothesis is that for many data sets and common\nanalysis tasks, there are relatively few \"data slices\" that result in effective\nvisualizations. By focusing human users on appropriate and suitably transformed\nparts of the underlying data sets, these data slices can help the users carry\ntheir task to correct completion.\n  To verify this hypothesis, we develop a framework that permits us to capture\nexemplary data slices for a user task, and to explore and parse\nvisual-exploration sequences into a format that makes them distinct and easy to\ncompare. We develop a recommendation system, DataSlicer, that matches a\n\"currently viewed\" data slice with the most promising \"next effective\" data\nslices for the given exploration task. We report the results of controlled\nexperiments with an implementation of the DataSlicer system, using four common\nanalytical task types. The experiments demonstrate statistically significant\nimprovements in accuracy and exploration speed versus users without access to\nour system.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 15:01:42 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Alborzi", "Farid", ""], ["Chaudhuri", "Surajit", ""], ["Chirkova", "Rada", ""], ["Deo", "Pallavi", ""], ["Healey", "Christopher", ""], ["Pingale", "Gargi", ""], ["Reutter", "Juan", ""], ["Selvakani", "Vaira", ""]]}, {"id": "1703.09539", "submitter": "Radim Ba\\v{c}a Ing.", "authors": "Petr Luk\\'a\\v{s} and Radim Ba\\v{c}a and Michal Kr\\'atk\\'y and Tok Wang\n  Ling", "title": "Demythization of Structural XML Query Processing: Comparison of Holistic\n  and Binary Approaches, Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML query can be modeled by twig pattern query (TPQ) specifying predicates on\nXML nodes and XPath relationships satisfied between them. A lot of TPQ types\nhave been proposed; this paper takes into account a TPQ model extended by a\nspecification of output and non-output query nodes since it complies with the\nXQuery semantics and, in many cases, it leads to a more efficient query\nprocessing. In general, there are two approaches to process the TPQ: holistic\njoins and binary joins. Whereas the binary join approach builds a query plan as\na tree of interconnected binary operators, the holistic join approach evaluates\na whole query using one operator (i.e., using one complex algorithm).\nSurprisingly, a thorough analytical and experimental comparison is still\nmissing despite an enormous research effort in this area. In this paper, we try\nto fill this gap; we analytically and experimentally show that the binary joins\nused in a fully-pipelined plan (i.e., the plan where each join operation does\nnot wait for the complete result of the previous operation and no explicit\nsorting is used) can often outperform the holistic joins, especially for TPQs\nwith a higher ratio of non-output query nodes. The main contributions of this\npaper can be summarized as follows: (i) we introduce several improvements of\nexisting binary join approaches allowing to build a fully-pipelined plan for a\nTPQ considering non-output query nodes, (ii) we prove that for a certain class\nof TPQs such a plan has the linear time complexity with respect to the size of\nthe input and output as well as the linear space complexity with respect to the\nXML document depth (i.e., the same complexity as the holistic join approaches),\n(iii) we show that our improved binary join approach outperforms the holistic\njoin approaches in many situations, and (iv) we propose a simple combined\napproach that uses advantages of both types of approaches.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 12:33:45 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 07:08:38 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 13:44:33 GMT"}, {"version": "v4", "created": "Tue, 23 Oct 2018 08:44:36 GMT"}, {"version": "v5", "created": "Fri, 26 Jul 2019 08:44:16 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Luk\u00e1\u0161", "Petr", ""], ["Ba\u010da", "Radim", ""], ["Kr\u00e1tk\u00fd", "Michal", ""], ["Ling", "Tok Wang", ""]]}, {"id": "1703.09574", "submitter": "Witold Litwin", "authors": "Witold Litwin", "title": "Stored and Inherited Relations", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The universally applied Codd's relational model has two constructs: a stored\nrelation, with stored attributes only and a view, only with the inherited ones.\nIn 1992, we have proposed third construct, mixing both types of attributes.\nExamples showed the idea attractive. No one followed however. We now revisit\nour proposal. We show that a relational database scheme using also our\nconstruct may be more faithful to reality. It may spare the logical navigation\nor complex value expressions to queries. It may also avoid auxiliary views,\noften necessary in practice at present. Better late than never, existing DBSs\nshould easily accommodate our proposal, with almost no storage and processing\noverhead.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 13:52:03 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Litwin", "Witold", ""]]}, {"id": "1703.09749", "submitter": "Kouakou Ive Koffi", "authors": "Kouakou Ive Arsene Koffi, Konan Marcellin Brou, Souleymane Oumtanaga", "title": "Developpement de Methodes Automatiques pour la Reutilisation des\n  Composants Logiciels", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large amount of information and the increasing complexity of applications\nconstrain developers to have stand-alone and reusable components from libraries\nand component markets.Our approach consists in developing methods to evaluate\nthe quality of the software component of these libraries, on the one hand and\nmoreover to optimize the financial cost and the adaptation's time of these\nselected components. Our objective function defines a metric that maximizes the\nvalue of the software component quality by minimizing the financial cost and\nmaintenance time. This model should make it possible to classify the components\nand order them in order to choose the most optimized.\n  MOTS-CLES : d{\\'e}veloppement de m{\\'e}thode, r{\\'e}utilisation, composants\nlogiciels, qualit{\\'e} de composant\n  KEYWORDS:method development, reuse, software components, component quality .\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 15:34:28 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Koffi", "Kouakou Ive Arsene", ""], ["Brou", "Konan Marcellin", ""], ["Oumtanaga", "Souleymane", ""]]}, {"id": "1703.09807", "submitter": "Nhien-An Le-Khac", "authors": "Lamine M. Aouad, Nhien-An Le-Khac, Tahar Kechadi", "title": "Grid-based Approaches for Distributed Data Mining Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data mining field is an important source of large-scale applications and\ndatasets which are getting more and more common. In this paper, we present\ngrid-based approaches for two basic data mining applications, and a performance\nevaluation on an experimental grid environment that provides interesting\nmonitoring capabilities and configuration tools. We propose a new distributed\nclustering approach and a distributed frequent itemsets generation well-adapted\nfor grid environments. Performance evaluation is done using the Condor system\nand its workflow manager DAGMan. We also compare this performance analysis to a\nsimple analytical model to evaluate the overheads related to the workflow\nengine and the underlying grid system. This will specifically show that\nrealistic performance expectations are currently difficult to achieve on the\ngrid.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 21:19:24 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Aouad", "Lamine M.", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "Tahar", ""]]}, {"id": "1703.09823", "submitter": "Nhien-An Le-Khac", "authors": "Lamine M. Aouad, Nhien-An Le-Khac, Tahar Kechadi", "title": "Variance-based Clustering Technique for Distributed Data Mining\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, huge amounts of data are naturally collected in distributed sites\ndue to different facts and moving these data through the network for extracting\nuseful knowledge is almost unfeasible for either technical reasons or policies.\nFurthermore, classical par- allel algorithms cannot be applied, specially in\nloosely coupled environments. This requires to develop scalable distributed\nalgorithms able to return the global knowledge by aggregating local results in\nan effective way. In this paper we propose a distributed algorithm based on\nindependent local clustering processes and a global merging based on minimum\nvariance increases and requires a limited communication overhead. We also\nintroduce the notion of distributed sub-clusters perturbation to improve the\nglobal generated distribution. We show that this algorithm improves the quality\nof clustering compared to classical local centralized ones and is able to find\nreal global data nature or distribution.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 21:59:33 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Aouad", "Lamine M.", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "Tahar", ""]]}, {"id": "1703.10350", "submitter": "Liat Peterfreund", "authors": "Dominik D. Freydenberger, Benny Kimelfeld and Liat Peterfreund", "title": "Joining Extractions of Regular Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular expressions with capture variables, also known as \"regex formulas,\"\nextract relations of spans (interval positions) from text. These relations can\nbe further manipulated via Relational Algebra as studied in the context of\ndocument spanners, Fagin et al.'s formal framework for information extraction.\nWe investigate the complexity of querying text by Conjunctive Queries (CQs) and\nUnions of CQs (UCQs) on top of regex formulas. We show that the lower bounds\n(NP-completeness and W[1]-hardness) from the relational world also hold in our\nsetting; in particular, hardness hits already single-character text! Yet, the\nupper bounds from the relational world do not carry over. Unlike the relational\nworld, acyclic CQs, and even gamma-acyclic CQs, are hard to compute. The source\nof hardness is that it may be intractable to instantiate the relation defined\nby a regex formula, simply because it has an exponential number of tuples. Yet,\nwe are able to establish general upper bounds. In particular, UCQs can be\nevaluated with polynomial delay, provided that every CQ has a bounded number of\natoms (while unions and projection can be arbitrary). Furthermore, UCQ\nevaluation is solvable with FPT (Fixed-Parameter Tractable) delay when the\nparameter is the size of the UCQ.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 08:27:11 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Freydenberger", "Dominik D.", ""], ["Kimelfeld", "Benny", ""], ["Peterfreund", "Liat", ""]]}, {"id": "1703.10692", "submitter": "Hasan Jamil", "authors": "Hasan M. Jamil", "title": "Knowledge Rich Natural Language Queries over Structured Biological\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly, keyword, natural language and NoSQL queries are being used for\ninformation retrieval from traditional as well as non-traditional databases\nsuch as web, document, image, GIS, legal, and health databases. While their\npopularity are undeniable for obvious reasons, their engineering is far from\nsimple. In most part, semantics and intent preserving mapping of a well\nunderstood natural language query expressed over a structured database schema\nto a structured query language is still a difficult task, and research to tame\nthe complexity is intense. In this paper, we propose a multi-level\nknowledge-based middleware to facilitate such mappings that separate the\nconceptual level from the physical level. We augment these multi-level\nabstractions with a concept reasoner and a query strategy engine to dynamically\nlink arbitrary natural language querying to well defined structured queries. We\ndemonstrate the feasibility of our approach by presenting a Datalog based\nprototype system, called BioSmart, that can compute responses to arbitrary\nnatural language queries over arbitrary databases once a syntactic\nclassification of the natural language query is made.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 21:37:14 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Jamil", "Hasan M.", ""]]}]