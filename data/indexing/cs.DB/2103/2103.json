[{"id": "2103.00170", "submitter": "Rodrigo Laigner Mr", "authors": "Rodrigo Laigner and Yongluan Zhou and Marcos Antonio Vaz Salles and\n  Yijian Liu and Marcos Kalinowski", "title": "Data Management in Microservices: State of the Practice, Challenges, and\n  Research Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We are recently witnessing an increased adoption of microservice\narchitectures by the industry for achieving scalability by functional\ndecomposition, fault-tolerance by deployment of small and independent services,\nand polyglot persistence by the adoption of different database technologies\nspecific to the needs of each service. Despite the accelerating industrial\nadoption and the extensive research on microservices, there is a lack of\nthorough investigation on the state of the practice and the major challenges\nfaced by practitioners with regard to data management. To bridge this gap, this\npaper presents a detailed investigation of data management in microservices.\nOur exploratory study is based on the following methodology: we conducted a\nsystematic literature review of articles reporting the adoption of\nmicroservices in industry, where more than 300 articles were filtered down to\n11 representative studies; we analyzed a set of 9 popular open-source\nmicroservice-based applications, selected out of more than 20 open-source\nprojects; furthermore, to strengthen our evidence, we conducted an online\nsurvey that we then used to cross-validate the findings of the previous steps\nwith the perceptions and experiences of over 120 practitioners and researchers.\nThrough this process, we were able to categorize the state of practice and\nreveal several principled challenges that cannot be solved by software\nengineering practices, but rather need system-level support to alleviate the\nburden of practitioners. Based on the observations we also identified a series\nof research directions to achieve this goal. Fundamentally, novel database\nsystems and data management tools that support isolation for microservices,\nwhich include fault isolation, performance isolation, data ownership, and\nindependent schema evolution across microservices must be built to address the\nneeds of this growing architectural style.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 10:08:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Laigner", "Rodrigo", ""], ["Zhou", "Yongluan", ""], ["Salles", "Marcos Antonio Vaz", ""], ["Liu", "Yijian", ""], ["Kalinowski", "Marcos", ""]]}, {"id": "2103.00288", "submitter": "Amir Gilad", "authors": "Daniel Deutch, Ariel Frankenthal, Amir Gilad, Yuval Moskovitch", "title": "On Optimizing the Trade-off between Privacy and Utility in Data\n  Provenance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organizations that collect and analyze data may wish or be mandated by\nregulation to justify and explain their analysis results. At the same time, the\nlogic that they have followed to analyze the data, i.e., their queries, may be\nproprietary and confidential. Data provenance, a record of the transformations\nthat data underwent, was extensively studied as means of explanations. In\ncontrast, only a few works have studied the tension between disclosing\nprovenance and hiding the underlying query.\n  This tension is the focus of the present paper, where we formalize and\nexplore for the first time the tradeoff between the utility of presenting\nprovenance information and the breach of privacy it poses with respect to the\nunderlying query. Intuitively, our formalization is based on the notion of\nprovenance abstraction, where the representation of some tuples in the\nprovenance expressions is abstracted in a way that makes multiple tuples\nindistinguishable. The privacy of a chosen abstraction is then measured based\non how many queries match the obfuscated provenance, in the same vein as\nk-anonymity. The utility is measured based on the entropy of the abstraction,\nintuitively how much information is lost with respect to the actual tuples\nparticipating in the provenance. Our formalization yields a novel optimization\nproblem of choosing the best abstraction in terms of this tradeoff. We show\nthat the problem is intractable in general, but design greedy heuristics that\nexploit the provenance structure towards a practically efficient exploration of\nthe search space. We experimentally prove the effectiveness of our solution\nusing the TPC-H benchmark and the IMDB dataset.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 18:24:25 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Deutch", "Daniel", ""], ["Frankenthal", "Ariel", ""], ["Gilad", "Amir", ""], ["Moskovitch", "Yuval", ""]]}, {"id": "2103.00418", "submitter": "Francois Luus", "authors": "Francois Luus, Prithviraj Sen, Pavan Kapanipathi, Ryan Riegel,\n  Ndivhuwo Makondo, Thabang Lebese, Alexander Gray", "title": "Logic Embeddings for Complex Query Answering", "comments": "IBM Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering logical queries over incomplete knowledge bases is challenging\nbecause: 1) it calls for implicit link prediction, and 2) brute force answering\nof existential first-order logic queries is exponential in the number of\nexistential variables. Recent work of query embeddings provides fast querying,\nbut most approaches model set logic with closed regions, so lack negation.\nQuery embeddings that do support negation use densities that suffer drawbacks:\n1) only improvise logic, 2) use expensive distributions, and 3) poorly model\nanswer uncertainty. In this paper, we propose Logic Embeddings, a new approach\nto embedding complex queries that uses Skolemisation to eliminate existential\nvariables for efficient querying. It supports negation, but improves on density\napproaches: 1) integrates well-studied t-norm logic and directly evaluates\nsatisfiability, 2) simplifies modeling with truth values, and 3) models\nuncertainty with truth bounds. Logic Embeddings are competitively fast and\naccurate in query answering over large, incomplete knowledge graphs, outperform\non negation queries, and in particular, provide improved modeling of answer\nuncertainty as evidenced by a superior correlation between answer set size and\nembedding entropy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 07:52:37 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Luus", "Francois", ""], ["Sen", "Prithviraj", ""], ["Kapanipathi", "Pavan", ""], ["Riegel", "Ryan", ""], ["Makondo", "Ndivhuwo", ""], ["Lebese", "Thabang", ""], ["Gray", "Alexander", ""]]}, {"id": "2103.00558", "submitter": "Jiawei Huang", "authors": "Hu Ding and Jiawei Huang", "title": "Is Simple Uniform Sampling Efficient for Center-Based Clustering With\n  Outliers: When and Why?", "comments": "arXiv admin note: text overlap with arXiv:1905.10143", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering has many important applications in computer science, but\nreal-world datasets often contain outliers. The presence of outliers can make\nthe clustering problems to be much more challenging. In this paper, we propose\na framework for solving three representative center-based clustering with\noutliers problems: $k$-center/median/means clustering with outliers. The\nframework actually is very simple, where we just need to take a small uniform\nsample from the input and run an existing approximation algorithm on the\nsample. However, our analysis is fundamentally different from the previous\n(uniform and non-uniform) sampling based ideas. To explain the effectiveness of\nuniform sampling in theory, we introduce a \"significance\" criterion and prove\nthat the performance of our framework depends on the significance degree of the\ngiven instance. In particular, the sample size can be independent of the input\ndata size $n$ and the dimensionality $d$, if we assume the given instance is\nsufficiently \"significant\", which is in fact a fairly appropriate assumption in\npractice. Due to its simplicity, the uniform sampling approach also enjoys\nseveral significant advantages over the non-uniform sampling approaches. The\nexperiments suggest that our framework can achieve comparable clustering\nresults with existing methods, but is much easier to implement and can greatly\nreduce the running times. To the best of our knowledge, this is the first work\nthat systematically studies the effectiveness of uniform sampling from both\ntheoretical and experimental aspects.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 16:43:37 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 05:40:40 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ding", "Hu", ""], ["Huang", "Jiawei", ""]]}, {"id": "2103.00740", "submitter": "Hui Li", "authors": "Weiguo Wang and Sourav S Bhowmick and Hui Li and Shafiq R Joty and\n  Siyuan Liu and Peng Chen", "title": "Towards Enhancing Database Education: Natural Language Generation Meets\n  Query Execution Plans", "comments": "16 pages, 10 figures, SIGMOD2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The database systems course is offered as part of an undergraduate computer\nscience degree program in many major universities. A key learning goal of\nlearners taking such a course is to understand how SQL queries are processed in\na RDBMS in practice. Since a query execution plan (QEP) describes the execution\nsteps of a query, learners can acquire the understanding by perusing the QEPs\ngenerated by a RDBMS. Unfortunately, in practice, it is often daunting for a\nlearner to comprehend these QEPs containing vendor-specific implementation\ndetails, hindering her learning process. In this paper, we present a novel,\nend-to-end, generic system called lantern that generates a natural language\ndescription of a qep to facilitate understanding of the query execution steps.\nIt takes as input an SQL query and its QEP, and generates a natural language\ndescription of the execution strategy deployed by the underlying RDBMS.\nSpecifically, it deploys a declarative framework called pool that enables\nsubject matter experts to efficiently create and maintain natural language\ndescriptions of physical operators used in QEPs. A rule-based framework called\nRULE-LANTERN is proposed that exploits pool to generate natural language\ndescriptions of QEPs. Despite the high accuracy of RULE-LANTERN, our engagement\nwith learners reveal that, consistent with existing psychology theories,\nperusing such rule-based descriptions lead to boredom due to repetitive\nstatements across different QEPs. To address this issue, we present a novel\ndeep learning-based language generation framework called NEURAL-LANTERN that\ninfuses language variability in the generated description by exploiting a set\nof paraphrasing tools and word embedding. Our experimental study with real\nlearners shows the effectiveness of lantern in facilitating comprehension of\nQEPs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 04:13:21 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 08:17:55 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 03:26:02 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wang", "Weiguo", ""], ["Bhowmick", "Sourav S", ""], ["Li", "Hui", ""], ["Joty", "Shafiq R", ""], ["Liu", "Siyuan", ""], ["Chen", "Peng", ""]]}, {"id": "2103.00798", "submitter": "Saugata Ghose", "authors": "Amirali Boroumand, Saugata Ghose, Geraldo F. Oliveira, Onur Mutlu", "title": "Polynesia: Enabling Effective Hybrid Transactional/Analytical Databases\n  with Specialized Hardware/Software Co-Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exponential growth in data volume, combined with increasing demand for\nreal-time analysis (i.e., using the most recent data), has resulted in the\nemergence of database systems that concurrently support transactions and data\nanalytics. These hybrid transactional and analytical processing (HTAP) database\nsystems can support real-time data analysis without the high costs of\nsynchronizing across separate single-purpose databases. Unfortunately, for many\napplications that perform a high rate of data updates, state-of-the-art HTAP\nsystems incur significant drops in transactional (up to 74.6%) and/or\nanalytical (up to 49.8%) throughput compared to performing only transactions or\nonly analytics in isolation, due to (1) data movement between the CPU and\nmemory, (2) data update propagation, and (3) consistency costs.\n  We propose Polynesia, a hardware-software co-designed system for in-memory\nHTAP databases. Polynesia (1) divides the HTAP system into transactional and\nanalytical processing islands, (2) implements custom algorithms and hardware to\nreduce the costs of update propagation and consistency, and (3) exploits\nprocessing-in-memory for the analytical islands to alleviate data movement. Our\nevaluation shows that Polynesia outperforms three state-of-the-art HTAP\nsystems, with average transactional/analytical throughput improvements of\n1.70X/3.74X, and reduces energy consumption by 48% over the prior lowest-energy\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:41:11 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Boroumand", "Amirali", ""], ["Ghose", "Saugata", ""], ["Oliveira", "Geraldo F.", ""], ["Mutlu", "Onur", ""]]}, {"id": "2103.00858", "submitter": "Jiaoyi Zhang", "authors": "Jiaoyi Zhang and Yihan Gao", "title": "CARMI: A Cache-Aware Learned Index with a Cost-based Construction\n  Algorithm", "comments": "16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned indexes, which use machine learning models to replace traditional\nindex structures, have shown promising results in recent studies. However, our\nunderstanding of this new type of index structure is still at an early stage\nwith many details that need to be carefully examined and improved. In this\npaper, we propose a cache-aware learned index (CARMI) design to improve the\nefficiency of the Recursive Model Index (RMI) framework proposed by Kraska et\nal. and a cost-based construction algorithm to construct the optimal indexes in\na wide variety of application scenarios. We formulate the problem of finding\nthe optimal design of a learned index as an optimization problem and propose a\ndynamic programming algorithm for solving it and a partial greedy step to speed\nup. Experiments show that our index construction strategy can construct indexes\nwith significantly better performance compared to baselines under various data\ndistribution and workload requirements. Among them, CARMI can obtain an average\nof 2.52X speedup compared to B-tree, while using only about 0.56X memory space\nof B-tree on average.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:20:53 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 13:08:52 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhang", "Jiaoyi", ""], ["Gao", "Yihan", ""]]}, {"id": "2103.01986", "submitter": "Vijay Gadepally", "authors": "El Kindi Rezig, Michael Cafarella, Vijay Gadepally", "title": "Technical Report on Data Integration and Preparation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AI application developers typically begin with a dataset of interest and a\nvision of the end analytic or insight they wish to gain from the data at hand.\nAlthough these are two very important components of an AI workflow, one often\nspends the first few weeks (sometimes months) in the phase we refer to as data\nconditioning. This step typically includes tasks such as figuring out how to\nprepare data for analytics, dealing with inconsistencies in the dataset, and\ndetermining which algorithm (or set of algorithms) will be best suited for the\napplication. Larger, faster, and messier datasets such as those from Internet\nof Things sensors, medical devices or autonomous vehicles only amplify these\nissues. These challenges, often referred to as the three Vs (volume, velocity,\nvariety) of Big Data, require low-level tools for data management, preparation\nand integration. In most applications, data can come from structured and/or\nunstructured sources and often includes inconsistencies, formatting\ndifferences, and a lack of ground-truth labels.\n  In this report, we highlight a number of tools that can be used to simplify\ndata integration and preparation steps. Specifically, we focus on data\nintegration tools and techniques, a deep dive into an exemplar data integration\ntool, and a deep-dive in the evolving field of knowledge graphs. Finally, we\nprovide readers with a list of practical steps and considerations that they can\nuse to simplify the data integration challenge. The goal of this report is to\nprovide readers with a view of state-of-the-art as well as practical tips that\ncan be used by data creators that make data integration more seamless.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:12:06 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Rezig", "El Kindi", ""], ["Cafarella", "Michael", ""], ["Gadepally", "Vijay", ""]]}, {"id": "2103.02145", "submitter": "Doris Xin", "authors": "Doris Xin, Devin Petersohn, Dixin Tang, Yifan Wu, Joseph E. Gonzalez,\n  Joseph M. Hellerstein, Anthony D. Joseph, Aditya G. Parameswaran", "title": "Enhancing the Interactivity of Dataframe Queries by Leveraging Think\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose opportunistic evaluation, a framework for accelerating\ninteractions with dataframes. Interactive latency is critical for iterative,\nhuman-in-the-loop dataframe workloads for supporting exploratory data analysis.\nOpportunistic evaluation significantly reduces interactive latency by 1)\nprioritizing computation directly relevant to the interactions and 2)\nleveraging think time for asynchronous background computation for non-critical\noperators that might be relevant to future interactions. We show, through\nempirical analysis, that current user behavior presents ample opportunities for\noptimization, and the solutions we propose effectively harness such\nopportunities.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:56:46 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Xin", "Doris", ""], ["Petersohn", "Devin", ""], ["Tang", "Dixin", ""], ["Wu", "Yifan", ""], ["Gonzalez", "Joseph E.", ""], ["Hellerstein", "Joseph M.", ""], ["Joseph", "Anthony D.", ""], ["Parameswaran", "Aditya G.", ""]]}, {"id": "2103.02211", "submitter": "Yeji Choi", "authors": "Yeji Choi, Hyunjung Park, Gi Pyo Nam, Haksub Kim, Heeseung Choi,\n  Junghyun Cho, Ig-Jae Kim", "title": "K-FACE: A Large-Scale KIST Face Database in Consideration with\n  Unconstrained Environments", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a new large-scale face database from KIST,\ndenoted as K-FACE, and describe a novel capturing device specifically designed\nto obtain the data. The K-FACE database contains more than 1 million\nhigh-quality images of 1,000 subjects selected by considering the ratio of\ngender and age groups. It includes a variety of attributes, including 27 poses,\n35 lighting conditions, three expressions, and occlusions by the combination of\nfive types of accessories. As the K-FACE database is systematically constructed\nthrough a hemispherical capturing system with elaborate lighting control and\nmultiple cameras, it is possible to accurately analyze the effects of factors\nthat cause performance degradation, such as poses, lighting changes, and\naccessories. We consider not only the balance of external environmental\nfactors, such as pose and lighting, but also the balance of personal\ncharacteristics such as gender and age group. The gender ratio is the same,\nwhile the age groups of subjects are uniformly distributed from the 20s to 50s\nfor both genders. The K-FACE database can be extensively utilized in various\nvision tasks, such as face recognition, face frontalization, illumination\nnormalization, face age estimation, and three-dimensional face model\ngeneration. We expect systematic diversity and uniformity of the K-FACE\ndatabase to promote these research fields.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:50:33 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Choi", "Yeji", ""], ["Park", "Hyunjung", ""], ["Nam", "Gi Pyo", ""], ["Kim", "Haksub", ""], ["Choi", "Heeseung", ""], ["Cho", "Junghyun", ""], ["Kim", "Ig-Jae", ""]]}, {"id": "2103.02284", "submitter": "Pranjal Gupta", "authors": "Pranjal Gupta, Amine Mhedhbi, Semih Salihoglu", "title": "Integrating Column-Oriented Storage and Query Processing Techniques Into\n  Graph Database Management Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We revisit column-oriented storage and query processing techniques in the\ncontext of contemporary graph database management systems (GDBMSs). Similar to\ncolumn-oriented RDBMSs, GDBMSs support read-heavy analytical workloads that\nhowever have fundamentally different data access patterns than traditional\nanalytical workloads. We first derive a set of desiderata for optimizing\nstorage and query processors of GDBMS based on their access patterns. We then\npresent the design of columnar storage, compression, and query processing\ntechniques based on these desiderata. In addition to showing direct integration\nof existing techniques from columnar RDBMSs, we also propose novel ones that\nare optimized for GDBMSs. These include a novel list-based query processor,\nwhich avoids expensive data copies of traditional block-based processors under\nmany-to-many joins and avoids materializing adjacency lists in intermediate\ntuples, a new data structure we call single-indexed edge property pages and an\naccompanying edge ID scheme, and a new application of Jacobson's bit vector\nindex for compressing NULL values and empty lists. We integrated our techniques\ninto the GraphflowDB in-memory GDBMS. Through extensive experiments, we\ndemonstrate the scalability and query performance benefits of our techniques.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:55:28 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Gupta", "Pranjal", ""], ["Mhedhbi", "Amine", ""], ["Salihoglu", "Semih", ""]]}, {"id": "2103.02397", "submitter": "Zheng Li", "authors": "Zheng Li", "title": "Long Live The Image: Container-Native Data Persistence in Production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Containerization plays a crucial role in the de facto technology stack for\nimplementing microservices architecture (each microservice has its own database\nin most cases). Nevertheless, there are still fierce debates on containerizing\nproduction databases, mainly due to the data persistence issues and concerns.\nDriven by a project of refactoring an Automated Machine Learning system, this\nresearch proposes the container-native data persistence as a conditional\nsolution to running database containers in production. In essence, the proposed\nsolution distinguishes the stateless data access (i.e. reading) from the\nstateful data processing (i.e. creating, updating, and deleting) in databases.\nA master database handles the stateful data processing and dumps database\ncopies for building container images, while the database containers will keep\nstateless at runtime, based on the preloaded dump in the image. Although there\nare delays in the state/image update propagation, this solution is particularly\nsuitable for the read-only, the eventual consistency, and the asynchronous\nprocessing scenarios. Moreover, with optimal tuning (e.g., disabling locking),\nthe portability and performance gains of a read-only database container would\noutweigh the performance loss in accessing data across the underlying image\nlayers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:44:22 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Li", "Zheng", ""]]}, {"id": "2103.02515", "submitter": "Peter C Dillinger", "authors": "Peter C. Dillinger and Stefan Walzer", "title": "Ribbon filter: practically smaller than Bloom and Xor", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Filter data structures over-approximate a set of hashable keys, i.e. set\nmembership queries may incorrectly come out positive. A filter with false\npositive rate $f \\in (0,1]$ is known to require $\\ge \\log_2(1/f)$ bits per key.\nAt least for larger $f \\ge 2^{-4}$, existing practical filters require a space\noverhead of at least 20% with respect to this information-theoretic bound.\n  We introduce the Ribbon filter: a new filter for static sets with a broad\nrange of configurable space overheads and false positive rates with competitive\nspeed over that range, especially for larger $f \\ge 2^{-7}$. In many cases,\nRibbon is faster than existing filters for the same space overhead, or can\nachieve space overhead below 10% with some additional CPU time. An experimental\nRibbon design with load balancing can even achieve space overheads below 1%.\n  A Ribbon filter resembles an Xor filter modified to maximize locality and is\nconstructed by solving a band-like linear system over Boolean variables. In\nprevious work, Dietzfelbinger and Walzer describe this linear system and an\nefficient Gaussian solver. We present and analyze a faster, more adaptable\nsolving process we call \"Rapid Incremental Boolean Banding ON the fly,\" which\nresembles hash table construction. We also present and analyze an attractive\nRibbon variant based on making the linear system homogeneous, and describe\nseveral more practical enhancements.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:41:36 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 16:14:45 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Dillinger", "Peter C.", ""], ["Walzer", "Stefan", ""]]}, {"id": "2103.02953", "submitter": "Jo\\~ao Silva", "authors": "Filipe Fernandes, Helena Serrano, Maria Alexandra Oliveira, Cristina\n  Branquinho, Jo\\~ao Nuno Silva", "title": "GAPS: Geo Data Portals for Air Pollution Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a wealth of data on air pollution within several users' reach,\nincluding modelled concentrations and depositions as well as observations from\nair quality stations. However, data integration to perceive spatial and\ntemporal trends at the national level is a complex undertaking. The\ndifficulties are mainly related to the data sources (many files with a lot of\ninformation and in different formats). In addition, the processing of this data\nis time-consuming and impractical when the time period under analysis\nincreases. Furthermore,the processing of spatial-temporal data requires the use\nof multiple specific tools, such as geographic information systems software and\nstatistical, which are not readily accessible to non-specialised users. The\nproposed solution is to develop libraries that are responsible for aggregating\ndifferent types of data related with pollution. In addition to allowing the\nintegration of different data, the libraries are also the basis for developing\nweb applications. The libraries allow the collection of data made available by\nthe Portuguese Environment Agency (APA) and official modelling results for\nEurope. The applications developed will extend the libraries to allow data\nprocessing and representation through Dashboards. These Dashboards provide\naccess to non-specialised users, namely regarding the trends of pollutants in\neach region. The implemented libraries allow the development of projects that\nwould need access to the same data sources. These Dashboards allow for\nsimplified access to data and for studies that have so far been beyond the\nreach of researchers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:08:13 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Fernandes", "Filipe", ""], ["Serrano", "Helena", ""], ["Oliveira", "Maria Alexandra", ""], ["Branquinho", "Cristina", ""], ["Silva", "Jo\u00e3o Nuno", ""]]}, {"id": "2103.02958", "submitter": "Yuncheng Wu", "authors": "Yuncheng Wu, Tien Tuan Anh Dinh, Guoyu Hu, Meihui Zhang, Yeow Meng\n  Chee, Beng Chin Ooi", "title": "Serverless Model Serving for Data Science", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) is an important part of modern data science\napplications. Data scientists today have to manage the end-to-end ML life cycle\nthat includes both model training and model serving, the latter of which is\nessential, as it makes their works available to end-users. Systems for model\nserving require high performance, low cost, and ease of management. Cloud\nproviders are already offering model serving options, including managed\nservices and self-rented servers. Recently, serverless computing, whose\nadvantages include high elasticity and fine-grained cost model, brings another\npossibility for model serving.\n  In this paper, we study the viability of serverless as a mainstream model\nserving platform for data science applications. We conduct a comprehensive\nevaluation of the performance and cost of serverless against other model\nserving systems on two clouds: Amazon Web Service (AWS) and Google Cloud\nPlatform (GCP). We find that serverless outperforms many cloud-based\nalternatives with respect to cost and performance. More interestingly, under\nsome circumstances, it can even outperform GPU-based systems for both average\nlatency and cost. These results are different from previous works' claim that\nserverless is not suitable for model serving, and are contrary to the\nconventional wisdom that GPU-based systems are better for ML workloads than\nCPU-based systems. Other findings include a large gap in cold start time\nbetween AWS and GCP serverless functions, and serverless' low sensitivity to\nchanges in workloads or models. Our evaluation results indicate that serverless\nis a viable option for model serving. Finally, we present several practical\nrecommendations for data scientists on how to use serverless for scalable and\ncost-effective model serving.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:23:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wu", "Yuncheng", ""], ["Dinh", "Tien Tuan Anh", ""], ["Hu", "Guoyu", ""], ["Zhang", "Meihui", ""], ["Chee", "Yeow Meng", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2103.03314", "submitter": "Akhil Dixit", "authors": "Akhil A. Dixit and Phokion G. Kolaitis", "title": "Consistent Answers of Aggregation Queries using SAT Solvers", "comments": "16 pages, 10 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of database repairs and consistent answers to queries is a\nprincipled approach to managing inconsistent databases. We describe the first\nsystem able to compute the consistent answers of general aggregation queries\nwith the COUNT(A), COUNT(*), and SUM operators, and with or without grouping\nconstructs. Our system uses reductions to optimization versions of Boolean\nsatisfiability (SAT) and then leverages powerful SAT solvers. We carry out an\nextensive set of experiments on both synthetic and real-world data that\ndemonstrate the usefulness and scalability of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:35:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 00:56:24 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Dixit", "Akhil A.", ""], ["Kolaitis", "Phokion G.", ""]]}, {"id": "2103.03537", "submitter": "Markus Schr\\\"oder", "authors": "Markus Schr\\\"oder, Christian Jilek, Michael Schulze, Andreas Dengel", "title": "Interactively Constructing Knowledge Graphs from Messy User-Generated\n  Spreadsheets", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When spreadsheets are filled freely by knowledge workers, they can contain\nrather unstructured content. For humans and especially machines it becomes\ndifficult to interpret such data properly. Therefore, spreadsheets are often\nconverted to a more explicit, formal and structured form, for example, to a\nknowledge graph. However, if a data maintenance strategy has been missing and\nuser-generated data becomes \"messy\", the construction of knowledge graphs will\nbe a challenging task. In this paper, we catalog several of those challenges\nand propose an interactive approach to solve them. Our approach includes a\ngraphical user interface which enables knowledge engineers to bulk-annotate\nspreadsheet cells with extracted information. Based on the cells' annotations a\nknowledge graph is ultimately formed. Using five spreadsheets from an\nindustrial scenario, we built a 25k-triple graph during our evaluation. We\ncompared our method with the state-of-the-art RDF Mapping Language (RML)\nattempt. The comparison highlights contributions of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 08:33:06 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Schr\u00f6der", "Markus", ""], ["Jilek", "Christian", ""], ["Schulze", "Michael", ""], ["Dengel", "Andreas", ""]]}, {"id": "2103.04489", "submitter": "Yeye He", "authors": "Peng Li, Xiang Cheng, Xu Chu, Yeye He, Surajit Chaudhuri", "title": "Auto-FuzzyJoin: Auto-Program Fuzzy Similarity Joins Without Labeled\n  Examples", "comments": "full version of a paper in SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fuzzy similarity join is an important database operator widely used in\npractice. So far the research community has focused exclusively on optimizing\nfuzzy join \\textit{scalability}. However, practitioners today also struggle to\noptimize fuzzy-join \\textit{quality}, because they face a daunting space of\nparameters (e.g., distance-functions, distance-thresholds,\ntokenization-options, etc.), and often have to resort to a manual\ntrial-and-error approach to program these parameters in order to optimize\nfuzzy-join quality. This key challenge of automatically generating high-quality\nfuzzy-join programs has received surprisingly little attention thus far.\n  In this work, we study the problem of \"auto-program\" fuzzy-joins. Leveraging\na geometric interpretation of distance-functions, we develop an unsupervised\n\\textsc{Auto-FuzzyJoin} framework that can infer suitable fuzzy-join programs\non given input tables, without requiring explicit human input such as labeled\ntraining data. Using \\textsc{Auto-FuzzyJoin}, users only need to provide two\ninput tables $L$ and $R$, and a desired precision target $\\tau$ (say 0.9).\n\\textsc{Auto-FuzzyJoin} leverages the fact that one of the input is a reference\ntable to automatically program fuzzy-joins that meet the precision target\n$\\tau$ in expectation, while maximizing fuzzy-join recall (defined as the\nnumber of correctly joined records).\n  Experiments on both existing benchmarks and a new benchmark with 50\nfuzzy-join tasks created from Wikipedia data suggest that the proposed\n\\textsc{Auto-FuzzyJoin} significantly outperforms existing unsupervised\napproaches, and is surprisingly competitive even against supervised approaches\n(e.g., Magellan and DeepMatcher) when 50\\% of ground-truth labels are used as\ntraining data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 23:47:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Peng", ""], ["Cheng", "Xiang", ""], ["Chu", "Xu", ""], ["He", "Yeye", ""], ["Chaudhuri", "Surajit", ""]]}, {"id": "2103.04541", "submitter": "Tu Gu", "authors": "Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, Sheng Wang", "title": "The RLR-Tree: A Reinforcement Learning Based R-Tree for Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned indices have been proposed to replace classic index structures like\nB-Tree with machine learning (ML) models. They require to replace both the\nindices and query processing algorithms currently deployed by the databases,\nand such a radical departure is likely to encounter challenges and obstacles.\nIn contrast, we propose a fundamentally different way of using ML techniques to\nimprove on the query performance of the classic R-Tree without the need of\nchanging its structure or query processing algorithms. Specifically, we develop\nreinforcement learning (RL) based models to decide how to choose a subtree for\ninsertion and how to split a node, instead of relying on hand-crafted heuristic\nrules as R-Tree and its variants. Experiments on real and synthetic datasets\nwith up to 100 million spatial objects clearly show that our RL based index\noutperforms R-Tree and its variants.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 04:29:58 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gu", "Tu", ""], ["Feng", "Kaiyu", ""], ["Cong", "Gao", ""], ["Long", "Cheng", ""], ["Wang", "Zheng", ""], ["Wang", "Sheng", ""]]}, {"id": "2103.04681", "submitter": "Jeeta Ann Chacko", "authors": "Jeeta Ann Chacko, Ruben Mayer, Hans-Arno Jacobsen", "title": "Why Do My Blockchain Transactions Fail? A Study of Hyperledger Fabric\n  (Extended version)*", "comments": "This is an extended version of an upcoming publication at ACM SIGMOD\n  2021. Please cite the original SIGMOD version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned blockchain systems promise to provide both decentralized trust\nand privacy. Hyperledger Fabric is currently one of the most wide-spread\npermissioned blockchain systems and is heavily promoted both in industry and\nacademia. Due to its optimistic concurrency model, the transaction failure\nrates in Fabric can become a bottleneck. While there is active research to\nreduce failures, there is a lack of understanding on their root cause and,\nconsequently, a lack of guidelines on how to configure Fabric optimally for\ndifferent scenarios. To close this gap, in this paper, we first introduce a\nformal definition of the different types of transaction failures in Fabric.\nThen, we develop a comprehensive testbed and benchmarking system,\nHyperLedgerLab, along with four different chaincodes that represent realistic\nuse cases and a chaincode/workload generator. Using HyperLedgerLab, we conduct\nexhaustive experiments to analyze the impact of different parameters of Fabric\nsuch as block size, endorsement policies, and others, on transaction failures.\nWe further analyze three recently proposed optimizations from the literature,\nFabric++, Streamchain and FabricSharp, and evaluate under which conditions they\nreduce the failure rates. Finally, based on our results, we provide\nrecommendations for Fabric practitioners on how to configure the system and\nalso propose new research directions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 11:42:32 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chacko", "Jeeta Ann", ""], ["Mayer", "Ruben", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "2103.05173", "submitter": "Masoumeh Shafieinejad", "authors": "Masoumeh Shafieinejad (1) and Florian Kerschbaum (1) and Ihab F. Ilyas\n  (1) ((1) University of Waterloo)", "title": "PCOR: Private Contextual Outlier Release via Differentially Private\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Outlier detection plays a significant role in various real world applications\nsuch as intrusion, malfunction, and fraud detection. Traditionally, outlier\ndetection techniques are applied to find outliers in the context of the whole\ndataset. However, this practice neglects contextual outliers, that are not\noutliers in the whole dataset but in some specific neighborhoods. Contextual\noutliers are particularly important in data exploration and targeted anomaly\nexplanation and diagnosis. In these scenarios, the data owner computes the\nfollowing information: i) The attributes that contribute to the abnormality of\nan outlier (metric), ii) Contextual description of the outlier's neighborhoods\n(context), and iii) The utility score of the context, e.g. its strength in\nshowing the outlier's significance, or in relation to a particular explanation\nfor the outlier. However, revealing the outlier's context leaks information\nabout the other individuals in the population as well, violating their privacy.\nWe address the issue of population privacy violations in this paper, and\npropose a solution for the two main challenges. In this setting, the data owner\nis required to release a valid context for the queried record, i.e. a context\nin which the record is an outlier. Hence, the first major challenge is that the\nprivacy technique must preserve the validity of the context for each record. We\npropose techniques to protect the privacy of individuals through a relaxed\nnotion of differential privacy to satisfy this requirement. The second major\nchallenge is applying the proposed techniques efficiently, as they impose\nintensive computation to the base algorithm. To overcome this challenge, we\npropose a graph structure to map the contexts to, and introduce differentially\nprivate graph search algorithms as efficient solutions for the computation\nproblem caused by differential privacy techniques.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 01:55:01 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Shafieinejad", "Masoumeh", "", "University of Waterloo"], ["Kerschbaum", "Florian", "", "University of Waterloo"], ["Ilyas", "Ihab F.", "", "University of Waterloo"]]}, {"id": "2103.05538", "submitter": "Sergey Gorshkov", "authors": "Sergey Gorshkov, Alexander Grebeshkov, Roman Shebalov", "title": "Ontology-based industrial data management platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relational and noSQL storages are developed for the fast processing of the\nlarge data sets having a stable structure, while the ontologies are used to\nrep-resent complex and dynamic sets of information of a limited size. In the\nin-dustrial applications it is often needed to maintain the large warehouses of\ndata consolidated from various sources. The ontologies are useful to repre-sent\nthe structure of that data, but RDF triple stores are not well suitable for\nstoring it. We offer an approach and a system allowing to use the\nopportuni-ties of fast storage engines along with the flexibility of\nontology-based data management tools, including SPARQL queries. The system\nimplements a multi-model data abstraction layer which allows working with the\ndata as if it is situated in RDF triple store, executes SPARQL queries over it\nand ap-plies SHACL constraints and rules.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 16:37:30 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Gorshkov", "Sergey", ""], ["Grebeshkov", "Alexander", ""], ["Shebalov", "Roman", ""]]}, {"id": "2103.05792", "submitter": "Masoumeh Shafieinejad", "authors": "Masoumeh Shafieinejad (1) and Suraj Gupta (1) and Jin Yang Liu (1) and\n  Koray Karabina (2) and Florian Kerschbaum (1) ((1) University of Waterloo,\n  (2) National Research Council of Canada)", "title": "Equi-Joins Over Encrypted Data for Series of Queries", "comments": "13 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Encryption provides a method to protect data outsourced to a DBMS provider,\ne.g., in the cloud. However, performing database operations over encrypted data\nrequires specialized encryption schemes that carefully balance security and\nperformance. In this paper, we present a new encryption scheme that can\nefficiently perform equi-joins over encrypted data with better security than\nthe state-of-the-art. In particular, our encryption scheme reduces the leakage\nto equality of rows that match a selection criterion and only reveals the\ntransitive closure of the sum of the leakages of each query in a series of\nqueries. Our encryption scheme is provable secure. We implemented our\nencryption scheme and evaluated it over a dataset from the TPC-H benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 00:07:33 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Shafieinejad", "Masoumeh", ""], ["Gupta", "Suraj", ""], ["Liu", "Jin Yang", ""], ["Karabina", "Koray", ""], ["Kerschbaum", "Florian", ""]]}, {"id": "2103.05844", "submitter": "Lyle Regenwetter", "authors": "Lyle Regenwetter, Brent Curry, Faez Ahmed", "title": "BIKED: A Dataset and Machine Learning Benchmarks for Data-Driven Bicycle\n  Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present \"BIKED,\" a dataset comprised of 4500 individually\ndesigned bicycle models sourced from hundreds of designers. We expect BIKED to\nenable a variety of data-driven design applications for bicycles and support\nthe development of data-driven design methods. The dataset is comprised of a\nvariety of design information including assembly images, component images,\nnumerical design parameters, and class labels. In this paper, we first discuss\nthe processing of the dataset, then highlight some prominent research questions\nthat BIKED can help address. Of these questions, we further explore the\nfollowing in detail: 1) Are there prominent gaps in the current bicycle market\nand design space? We explore the design space using unsupervised dimensionality\nreduction methods. 2) How does one identify the class of a bicycle and what\nfactors play a key role in defining it? We address the bicycle classification\ntask by training a multitude of classifiers using different forms of design\ndata and identifying parameters of particular significance through\npermutation-based interpretability analysis. 3) How does one synthesize new\nbicycles using different representation methods? We consider numerous machine\nlearning methods to generate new bicycle models as well as interpolate between\nand extrapolate from existing models using Variational Autoencoders. The\ndataset and code are available at http://decode.mit.edu/projects/biked/.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:12:32 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 21:22:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Regenwetter", "Lyle", ""], ["Curry", "Brent", ""], ["Ahmed", "Faez", ""]]}, {"id": "2103.05864", "submitter": "Huayi Wang", "authors": "Huayi Wang, Jingfan Meng, Long Gong, Jun Xu and Mitsunori Ogihara", "title": "MP-RW-LSH: An Efficient Multi-Probe LSH Solution to ANNS in $L_1$\n  Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Nearest Neighbor Search (ANNS) is a fundamental algorithmic\nproblem, with numerous applications in many areas of computer science.\nLocality-sensitive hashing (LSH) is one of the most popular solution approaches\nfor ANNS. A common shortcoming of many LSH schemes is that since they probe\nonly a single bucket in a hash table, they need to use a large number of hash\ntables to achieve a high query accuracy. For ANNS-$L_2$, a multi-probe scheme\nwas proposed to overcome this drawback by strategically probing multiple\nbuckets in a hash table. In this work, we propose MP-RW-LSH, the first and so\nfar only multi-probe LSH solution to ANNS in $L_1$ distance. Another\ncontribution of this work is to explain why a state-of-the-art ANNS-$L_1$\nsolution called Cauchy projection LSH (CP-LSH) is fundamentally not suitable\nfor multi-probe extension. We show that MP-RW-LSH uses 15 to 53 times fewer\nhash tables than CP-LSH for achieving similar query accuracies.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 04:22:34 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Wang", "Huayi", ""], ["Meng", "Jingfan", ""], ["Gong", "Long", ""], ["Xu", "Jun", ""], ["Ogihara", "Mitsunori", ""]]}, {"id": "2103.06253", "submitter": "Tobias Zeimetz", "authors": "Tobias Zeimetz and Ralf Schenkel", "title": "FiLiPo: A Sample Driven Approach for Finding Linkage Points between RDF\n  Data and APIs (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration is an important task in order to create comprehensive RDF\nknowledge bases. Many data sources are used to extend a given dataset or to\ncorrect errors. Since several data providers make their data publicly available\nonly via Web APIs they also must be included in the integration process.\nHowever, APIs often come with limitations in terms of access frequencies and\nspeed due to latencies and other constraints. On the other hand, APIs always\nprovide access to the latest data. So far, integrating APIs has been mainly a\nmanual task due to the heterogeneity of API responses. To tackle this problem\nwe present in this paper the FiLiPo (Finding Linkage Points) system which\nautomatically finds connections (i.e., linkage points) between data provided by\nAPIs and local knowledge bases. FiLiPo is an open source sample-driven schema\nmatching system that models API services as parameterized queries. Furthermore,\nour approach is able to find valid input values for APIs automatically (e.g.\nIDs) and can determine valid alignments between KBs and APIs. Our results on\nten pairs of KBs and APIs show that FiLiPo performs well in terms of precision\nand recall and outperforms the current state-of-the-art system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:35:53 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 16:16:40 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 09:55:36 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zeimetz", "Tobias", ""], ["Schenkel", "Ralf", ""]]}, {"id": "2103.06376", "submitter": "Amir Shaikhha", "authors": "Amir Shaikhha, Mathieu Huot, Jaclyn Smith, Dan Olteanu", "title": "Functional Collection Programming with Semi-Ring Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces semi-ring dictionaries, a powerful class of\ncompositional and purely functional collections that subsume other collection\ntypes such as sets, multisets, arrays, vectors, and matrices. We develop SDQL,\na statically typed language centered around semi-ring dictionaries, that can\nencode expressions in relational algebra with aggregations, functional\ncollections, and linear algebra. Furthermore, thanks to the semi-ring algebraic\nstructures behind these dictionaries, SDQL unifies a wide range of\noptimizations commonly used in databases and linear algebra. As a result, SDQL\nenables efficient processing of hybrid database and linear algebra workloads,\nby putting together optimizations that are otherwise confined to either\ndatabase systems or linear algebra frameworks. Through experimental results, we\nshow that a handful of relational and linear algebra workloads can take\nadvantage of the SDQL language and optimizations. Overall, we observe that SDQL\nachieves competitive performance to Typer and Tectorwise, which are\nstate-of-the-art in-memory systems for (flat, not nested) relational data, and\nachieves an average 2x speedup over SciPy for linear algebra workloads.\nFinally, for hybrid workloads involving linear algebra processing over nested\nbiomedical data, SDQL can give up to one order of magnitude speedup over\nTrance, a state-of-the-art nested relational engine.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:54:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Shaikhha", "Amir", ""], ["Huot", "Mathieu", ""], ["Smith", "Jaclyn", ""], ["Olteanu", "Dan", ""]]}, {"id": "2103.06438", "submitter": "Tianxi Ji", "authors": "Tianxi Ji, Emre Yilmaz, Erman Ayday, Pan Li", "title": "The Curse of Correlations for Robust Fingerprinting of Relational\n  Databases", "comments": "To appear in 24th International Symposium on Research in Attacks,\n  Intrusions and Defenses (RAID'21)", "journal-ref": null, "doi": "10.1145/3471621.3471853", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database fingerprinting have been widely adopted to prevent unauthorized\nsharing of data and identify the source of data leakages. Although existing\nschemes are robust against common attacks, like random bit flipping and subset\nattack, their robustness degrades significantly if attackers utilize the\ninherent correlations among database entries. In this paper, we first\ndemonstrate the vulnerability of existing database fingerprinting schemes by\nidentifying different correlation attacks: column-wise correlation attack,\nrow-wise correlation attack, and the integration of them. To provide robust\nfingerprinting against the identified correlation attacks, we then develop\nmitigation techniques, which can work as post-processing steps for any\noff-the-shelf database fingerprinting schemes. The proposed mitigation\ntechniques also preserve the utility of the fingerprinted database considering\ndifferent utility metrics. We empirically investigate the impact of the\nidentified correlation attacks and the performance of mitigation techniques\nusing real-world relational databases. Our results show (i) high success rates\nof the identified correlation attacks against existing fingerprinting schemes\n(e.g., the integrated correlation attack can distort 64.8\\% fingerprint bits by\njust modifying 14.2\\% entries in a fingerprinted database), and (ii) high\nrobustness of the proposed mitigation techniques (e.g., with the mitigation\ntechniques, the integrated correlation attack can only distort $3\\%$\nfingerprint bits).\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 03:47:51 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 19:37:38 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ji", "Tianxi", ""], ["Yilmaz", "Emre", ""], ["Ayday", "Erman", ""], ["Li", "Pan", ""]]}, {"id": "2103.06531", "submitter": "Matteo Lissandrini", "authors": "Georgia Troullinou, Haridimos Kondylakis, Matteo Lissandrini, Davide\n  Mottin", "title": "SOFOS: Demonstrating the Challenges of Materialized View Selection on\n  Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Analytical queries over RDF data are becoming prominent as a result of the\nproliferation of knowledge graphs. Yet, RDF databases are not optimized to\nperform such queries efficiently, leading to long processing times. A well\nknown technique to improve the performance of analytical queries is to exploit\nmaterialized views. Although popular in relational databases, view\nmaterialization for RDF and SPARQL has not yet transitioned into practice, due\nto the non-trivial application to the RDF graph model. Motivated by a lack of\nunderstanding of the impact of view materialization alternatives for RDF data,\nwe demonstrate SOFOS, a system that implements and compares several cost models\nfor view materialization. SOFOS is, to the best of our knowledge, the first\nattempt to adapt cost models, initially studied in relational data, to the\ngeneric RDF setting, and to propose new ones, analyzing their pitfalls and\nmerits. SOFOS takes an RDF dataset and an analytical query for some facet in\nthe data, and compares and evaluates alternative cost models, displaying\nstatistics and insights about time, memory consumption, and query\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 08:48:11 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Troullinou", "Georgia", ""], ["Kondylakis", "Haridimos", ""], ["Lissandrini", "Matteo", ""], ["Mottin", "Davide", ""]]}, {"id": "2103.06766", "submitter": "Jan Toenshoff", "authors": "Jan Toenshoff, Neta Friedman, Martin Grohe, Benny Kimelfeld", "title": "Dynamic Database Embeddings with FoRWaRD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of computing an embedding of the tuples of a relational\ndatabase in a manner that is extensible to dynamic changes of the database.\nImportantly, the embedding of existing tuples should not change due to the\nembedding of newly inserted tuples (as database applications might rely on\nexisting embeddings), while the embedding of all tuples, old and new, should\nretain high quality. This task is challenging since state-of-the-art embedding\ntechniques for structured data, such as (adaptations of) embeddings on graphs,\nhave inherent inter-dependencies among the embeddings of different entities. We\npresent the FoRWaRD algorithm (Foreign Key Random Walk Embeddings for\nRelational Databases) that draws from embedding techniques for general graphs\nand knowledge graphs, and is inherently utilizing the schema and its key and\nforeign-key constraints. We compare FoRWaRD to an alternative approach that we\ndevise by adapting node embeddings for graphs (Node2Vec) to dynamic databases.\nWe show that FoRWaRD is comparable and sometimes superior to state-of-the-art\nembeddings in the static (traditional) setting, using a collection of\ndownstream tasks of column prediction over geographical and biological domains.\nMore importantly, in the dynamic setting FoRWaRD outperforms the alternatives\nconsistently and often considerably, and features only a mild reduction of\nquality even when the database consists of mostly newly inserted tuples.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:23:03 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Toenshoff", "Jan", ""], ["Friedman", "Neta", ""], ["Grohe", "Martin", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "2103.06784", "submitter": "Matteo Brucato", "authors": "Matteo Brucato and Nishant Yadav and Azza Abouzied and Peter J. Haas\n  and Alexandra Meliou", "title": "Stochastic Package Queries in Probabilistic Databases", "comments": null, "journal-ref": "SIGMOD 2020", "doi": "10.1145/3318464.3389765", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide methods for in-database support of decision making under\nuncertainty. Many important decision problems correspond to selecting a package\n(bag of tuples in a relational database) that jointly satisfy a set of\nconstraints while minimizing some overall cost function; in most real-world\nproblems, the data is uncertain. We provide methods for specifying -- via a SQL\nextension -- and processing stochastic package queries (SPQs), in order to\nsolve optimization problems over uncertain data, right where the data resides.\nPrior work in stochastic programming uses Monte Carlo methods where the\noriginal stochastic optimization problem is approximated by a large\ndeterministic optimization problem that incorporates many scenarios, i.e.,\nsample realizations of the uncertain data values. For large database tables,\nhowever, a huge number of scenarios is required, leading to poor performance\nand, often, failure of the solver software. We therefore provide a novel\nSummarySearch algorithm that, instead of trying to solve a large deterministic\nproblem, seamlessly approximates it via a sequence of smaller problems defined\nover carefully crafted summaries of the scenarios that accelerate convergence\nto a feasible and near-optimal solution. Experimental results on our prototype\nsystem show that SummarySearch can be orders of magnitude faster than prior\nmethods at finding feasible and high-quality packages.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:44:27 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Brucato", "Matteo", ""], ["Yadav", "Nishant", ""], ["Abouzied", "Azza", ""], ["Haas", "Peter J.", ""], ["Meliou", "Alexandra", ""]]}, {"id": "2103.06866", "submitter": "Wensheng Gan", "authors": "Yanling Cui, Wensheng Gan, Hong Lin, and Weimin Zheng", "title": "FRI-Miner: Fuzzy Rare Itemset Mining", "comments": "Preprint. 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is a widely used technology for various real-life applications of\ndata analytics and is important to discover valuable association rules in\ntransaction databases. Interesting itemset mining plays an important role in\nmany real-life applications, such as market, e-commerce, finance, and medical\ntreatment. To date, various data mining algorithms based on frequent patterns\nhave been widely studied, but there are a few algorithms that focus on mining\ninfrequent or rare patterns. In some cases, infrequent or rare itemsets and\nrare association rules also play an important role in real-life applications.\nIn this paper, we introduce a novel fuzzy-based rare itemset mining algorithm\ncalled FRI-Miner, which discovers valuable and interesting fuzzy rare itemsets\nin a quantitative database by applying fuzzy theory with linguistic meaning.\nAdditionally, FRI-Miner utilizes the fuzzy-list structure to store important\ninformation and applies several pruning strategies to reduce the search space.\nThe experimental results show that the proposed FRI-Miner algorithm can\ndiscover fewer and more interesting itemsets by considering the quantitative\nvalue in reality. Moreover, it significantly outperforms state-of-the-art\nalgorithms in terms of effectiveness (w.r.t. different types of derived\npatterns) and efficiency (w.r.t. running time and memory usage).\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:53:17 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Cui", "Yanling", ""], ["Gan", "Wensheng", ""], ["Lin", "Hong", ""], ["Zheng", "Weimin", ""]]}, {"id": "2103.07037", "submitter": "Zezhou Huang", "authors": "Zezhou Huang, Eugene Wu", "title": "Reptile: Aggregation-level Explanations for Hierarchical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent query explanation systems help users understand anomalies in\naggregation results by proposing predicates that describe input records that,\nif deleted, would resolve the anomalies. However, it can be difficult for users\nto understand how a predicate was chosen, and these approaches are limited to\nerrors that can be resolved through deletion. In contrast, data errors may be\ndue to group-wise errors, such as missing records or systematic value errors.\nThis paper presents Reptile, an explanation system for hierarchical data. Given\nan anomalous aggregate query result, Reptile recommends the next drill-down\nattribute,and ranks the drill-down groups based on the extent repairing the\ngroup's statistics to its expected values resolves the anomaly. Reptile\nefficiently trains a multi-level model that leverages the data's hierarchy to\nestimate the expected values, and uses a factorised representation of the\nfeature matrix to remove redundancies due to the data's hierarchical structure.\nWe further extend model training to support factorised data, and develop a\nsuite of optimizations that leverage the data's hierarchical structure. Reptile\nreduces end-to-end runtimes by more than 6 times compared to a Matlab-based\nimplementation, correctly identifies 21/30 data errors in John Hopkin's\nCOVID-19 data, and correctly resolves 20/22 complaints in a user study using\ndata and researchers from Columbia University's Financial Instruments Sector\nTeam.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 01:53:45 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Huang", "Zezhou", ""], ["Wu", "Eugene", ""]]}, {"id": "2103.07184", "submitter": "Anahita Farhang Ghahfarokhi", "authors": "Anahita Farhang Ghahfarokhi, Alessandro Berti, Wil M.P. van der Aalst", "title": "Process Comparison Using Object-Centric Process Cubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process mining provides ways to analyze business processes. Common process\nmining techniques consider the process as a whole. However, in real-life\nbusiness processes, different behaviors exist that make the overall process too\ncomplex to interpret. Process comparison is a branch of process mining that\nisolates different behaviors of the process from each other by using process\ncubes. Process cubes organize event data using different dimensions. Each cell\ncontains a set of events that can be used as an input to apply process mining\ntechniques. Existing work on process cubes assume single case notions. However,\nin real processes, several case notions (e.g., order, item, package, etc.) are\nintertwined. Object-centric process mining is a new branch of process mining\naddressing multiple case notions in a process. To make a bridge between\nobject-centric process mining and process comparison, we propose a process cube\nframework, which supports process cube operations such as slice and dice on\nobject-centric event logs. To facilitate the comparison, the framework is\nintegrated with several object-centric process discovery approaches.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:08:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ghahfarokhi", "Anahita Farhang", ""], ["Berti", "Alessandro", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2103.07532", "submitter": "Pranav Subramaniam", "authors": "Pranav Subramaniam (1), Yintong Ma (1), Chi Li (1), Ipsita Mohanty\n  (2), Raul Castro Fernandez (1) ((1) University of Chicago (2) IIT Kanpur)", "title": "Comprehensive and Comprehensible Data Catalogs: The What, Who, Where,\n  When, Why, and How of Metadata Management", "comments": "10 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable data science requires access to metadata, which is increasingly\nmanaged by databases called data catalogs. With today's data catalogs, users\nchoose between designs that make it easy to store or retrieve metadata, but not\nboth. We find this problem arises because catalogs lack an easy to understand\nmental model. In this paper, we present a new catalog mental model called\n5W1H+R. The new mental model is comprehensive in the metadata it represents,\nand comprehensible in that it permits users to locate metadata easily. We\ndemonstrate these properties via a user study. We then study different schema\ndesigns for the new mental model implementation and evaluate them on different\nbackends to understand their relative merits. We conclude mental models are\nimportant to make data catalogs more useful and to boost metadata management\nefforts that are crucial for data science tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 21:14:36 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Subramaniam", "Pranav", "", "University of Chicago"], ["Ma", "Yintong", "", "University of Chicago"], ["Li", "Chi", "", "University of Chicago"], ["Mohanty", "Ipsita", "", "IIT Kanpur"], ["Fernandez", "Raul Castro", "", "University of Chicago"]]}, {"id": "2103.07561", "submitter": "Seokki Lee", "authors": "Ralf Diestelkaemper, Seokki Lee, Melanie Herschel, Boris Glavic", "title": "To not miss the forest for the trees -- a holistic approach for\n  explaining missing answers over nested data (extended version)", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query-based explanations for missing answers identify which operators of a\nquery are responsible for the failure to return a missing answer of interest.\nThis type of explanations has proven to be useful in a variety of contexts\nincluding debugging of complex analytical queries. Such queries are frequent in\nbig data systems such as Apache Spark. We present a novel approach for\nproducing query-based explanations. Our approach is the first to support nested\ndata and to consider operators that modify the schema and structure of the data\n(e.g., nesting and projections) as potential causes of missing answers. To\nefficiently compute explanations, we propose a heuristic algorithm that applies\ntwo novel techniques: (i) reasoning about multiple schema alternatives for a\nquery and (ii) re-validating at each step whether an intermediate result can\ncontribute to the missing answer. Using an implementation of our approach on\nSpark, we demonstrate that it is the first to scale to large datasets and that\nit often finds explanations that existing techniques fail to identify.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 22:58:28 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Diestelkaemper", "Ralf", ""], ["Lee", "Seokki", ""], ["Herschel", "Melanie", ""], ["Glavic", "Boris", ""]]}, {"id": "2103.07703", "submitter": "Yuanwei Zhao", "authors": "Yuanwei Zhao, Lan Huang, Bo Wang, Dongxu Zhang, Simone Bocca, Fausto\n  Giunchiglia, Rui Zhang", "title": "Is your Schema Good Enough to Answer my Query?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontology-based data integration has been one of the practical methodologies\nfor heterogeneous legacy database integrated service construction. However, it\nis neither efficient nor economical to build the cross-domain ontology on top\nof the schemas of each legacy database for the specific integration application\nthan to reuse the existed ontologies. Then the question lies in whether the\nexisted ontology is compatible with the cross-domain queries and with all the\nlegacy systems. It is highly needed an effective criteria to evaluate the\ncompatibility as it limits the upbound quality of the integrated services. This\npaper studies the semantic similarity of schemas from the aspect of properties.\nIt provides a set of in-depth criteria, namely coverage and flexibility to\nevaluate the compatibility among the queries, the schemas and the existing\nontology. The weights of classes are extended to make precise compatibility\ncomputation. The use of such criteria in the practical project verifies the\napplicability of our method.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 12:15:43 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhao", "Yuanwei", ""], ["Huang", "Lan", ""], ["Wang", "Bo", ""], ["Zhang", "Dongxu", ""], ["Bocca", "Simone", ""], ["Giunchiglia", "Fausto", ""], ["Zhang", "Rui", ""]]}, {"id": "2103.07978", "submitter": "Genoveva Vargas-Solar", "authors": "Ali Akoglu, Genoveva Vargas-Solar", "title": "Putting Data Science Pipelines on the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper proposes a composable \"Just in Time Architecture\" for Data Science\n(DS) Pipelines named JITA-4DS and associated resource management techniques for\nconfiguring disaggregated data centers (DCs). DCs under our approach are\ncomposable based on vertical integration of the application,\nmiddleware/operating system, and hardware layers customized dynamically to meet\napplication Service Level Objectives (SLO - application-aware management).\nThereby, pipelines utilize a set of flexible building blocks that can be\ndynamically and automatically assembled and re-assembled to meet the dynamic\nchanges in the workload's SLOs. To assess disaggregated DC's, we study how to\nmodel and validate their performance in large-scale settings.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 17:21:26 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Akoglu", "Ali", ""], ["Vargas-Solar", "Genoveva", ""]]}, {"id": "2103.08115", "submitter": "Muhao Chen", "authors": "Junheng Hao, Muhao Chen, Wenchao Yu, Yizhou Sun, Wei Wang", "title": "Universal Representation Learning of Knowledge Bases by Jointly\n  Embedding Instances and Ontological Concepts", "comments": "KDD-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale knowledge bases simultaneously represent two views of\nknowledge graphs (KGs): an ontology view for abstract and commonsense concepts,\nand an instance view for specific entities that are instantiated from\nontological concepts. Existing KG embedding models, however, merely focus on\nrepresenting one of the two views alone. In this paper, we propose a novel\ntwo-view KG embedding model, JOIE, with the goal to produce better knowledge\nembedding and enable new applications that rely on multi-view knowledge. JOIE\nemploys both cross-view and intra-view modeling that learn on multiple facets\nof the knowledge base. The cross-view association model is learned to bridge\nthe embeddings of ontological concepts and their corresponding instance-view\nentities. The intra-view models are trained to capture the structured knowledge\nof instance and ontology views in separate embedding spaces, with a\nhierarchy-aware encoding technique enabled for ontologies with hierarchies. We\nexplore multiple representation techniques for the two model components and\ninvestigate with nine variants of JOIE. Our model is trained on large-scale\nknowledge bases that consist of massive instances and their corresponding\nontological concepts connected via a (small) set of cross-view links.\nExperimental results on public datasets show that the best variant of JOIE\nsignificantly outperforms previous models on instance-view triple prediction\ntask as well as ontology population on ontologyview KG. In addition, our model\nsuccessfully extends the use of KG embeddings to entity typing with promising\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 03:24:37 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hao", "Junheng", ""], ["Chen", "Muhao", ""], ["Yu", "Wenchao", ""], ["Sun", "Yizhou", ""], ["Wang", "Wei", ""]]}, {"id": "2103.08588", "submitter": "Teodoro Baldazzi", "authors": "Teodoro Baldazzi (Universit\\`a Roma Tre), Luigi Bellomarini (Banca\n  d'Italia), Emanuel Sallinger (University of Oxford and TU Wien), Paolo Atzeni\n  (Universit\\`a Roma Tre)", "title": "iWarded: A System for Benchmarking Datalog+/- Reasoning (technical\n  report)", "comments": "17 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen increasing popularity of logic-based reasoning\nsystems, with research and industrial interest as well as many flourishing\napplications in the area of Knowledge Graphs. Despite that, one can observe a\nsubstantial lack of specific tools able to generate nontrivial reasoning\nsettings and benchmark scenarios. As a consequence, evaluating, analysing and\ncomparing reasoning systems is a complex task, especially when they embody\nsophisticated optimizations and execution techniques that leverage the\ntheoretical underpinnings of the adopted logic fragment. In this paper, we aim\nat filling this gap by introducing iWarded, a system that can generate very\nlarge, complex, realistic reasoning settings to be used for the benchmarking of\nlogic-based reasoning systems adopting Datalog+/-, a family of extensions of\nDatalog that has seen a resurgence in the last few years. In particular,\niWarded generates reasoning settings for Warded Datalog+/-, a language with a\nvery good tradeoff between computational complexity and expressive power. In\nthe paper, we present the iWarded system and a set of novel theoretical results\nadopted to generate effective scenarios. As Datalog-based languages are of\ngeneral interest and see increasing adoption, we believe that iWarded is a step\nforward in the empirical evaluation of current and future systems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:56:46 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Baldazzi", "Teodoro", "", "Universit\u00e0 Roma Tre"], ["Bellomarini", "Luigi", "", "Banca\n  d'Italia"], ["Sallinger", "Emanuel", "", "University of Oxford and TU Wien"], ["Atzeni", "Paolo", "", "Universit\u00e0 Roma Tre"]]}, {"id": "2103.08720", "submitter": "Weilong Ren", "authors": "Weilong Ren, Xiang Lian, Kambiz Ghazinour", "title": "Online Topic-Aware Entity Resolution Over Incomplete Data Streams\n  (Technical Report)", "comments": "Technical report of the paper entitled \"Online Topic-Aware Entity\n  Resolution Over Incomplete Data Streams\", published on SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real applications such as the data integration, social network\nanalysis, and the Semantic Web, the entity resolution (ER) is an important and\nfundamental problem, which identifies and links the same real-world entities\nfrom various data sources. While prior works usually consider ER over static\nand complete data, in practice, application data are usually collected in a\nstreaming fashion, and often incur missing attributes (due to the inaccuracy of\ndata extraction techniques). Therefore, in this paper, we will formulate and\ntackle a novel problem, topic-aware entity resolution over incomplete data\nstreams (TER-iDS), which online imputes incomplete tuples and detects pairs of\ntopic-related matching entities from incomplete data streams. In order to\neffectively and efficiently tackle the TER-iDS problem, we propose an effective\nimputation strategy, carefully design effective pruning strategies, as well as\nindexes/synopsis, and develop an efficient TER-iDS algorithm via index joins.\nExtensive experiments have been conducted to evaluate the effectiveness and\nefficiency of our proposed TER-iDS approach over real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 21:06:12 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ren", "Weilong", ""], ["Lian", "Xiang", ""], ["Ghazinour", "Kambiz", ""]]}, {"id": "2103.09055", "submitter": "Hantian Zhang", "authors": "Hantian Zhang, Xu Chu, Abolfazl Asudeh, Shamkant B. Navathe", "title": "OmniFair: A Declarative System for Model-Agnostic Group Fairness in\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) is increasingly being used to make decisions in our\nsociety. ML models, however, can be unfair to certain demographic groups (e.g.,\nAfrican Americans or females) according to various fairness metrics. Existing\ntechniques for producing fair ML models either are limited to the type of\nfairness constraints they can handle (e.g., preprocessing) or require\nnontrivial modifications to downstream ML training algorithms (e.g.,\nin-processing).\n  We propose a declarative system OmniFair for supporting group fairness in ML.\nOmniFair features a declarative interface for users to specify desired group\nfairness constraints and supports all commonly used group fairness notions,\nincluding statistical parity, equalized odds, and predictive parity. OmniFair\nis also model-agnostic in the sense that it does not require modifications to a\nchosen ML algorithm. OmniFair also supports enforcing multiple user declared\nfairness constraints simultaneously while most previous techniques cannot. The\nalgorithms in OmniFair maximize model accuracy while meeting the specified\nfairness constraints, and their efficiency is optimized based on the\ntheoretically provable monotonicity property regarding the trade-off between\naccuracy and fairness that is unique to our system.\n  We conduct experiments on commonly used datasets that exhibit bias against\nminority groups in the fairness literature. We show that OmniFair is more\nversatile than existing algorithmic fairness approaches in terms of both\nsupported fairness constraints and downstream ML models. OmniFair reduces the\naccuracy loss by up to $94.8\\%$ compared with the second best method. OmniFair\nalso achieves similar running time to preprocessing methods, and is up to\n$270\\times$ faster than in-processing methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 02:44:10 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhang", "Hantian", ""], ["Chu", "Xu", ""], ["Asudeh", "Abolfazl", ""], ["Navathe", "Shamkant B.", ""]]}, {"id": "2103.09497", "submitter": "Guliu Liu", "authors": "Guliu Liu, Lei Li, Guanfeng Liu, Xindong Wu", "title": "Social Group Query Based on Multi-fuzzy-constrained Strong Simulation", "comments": "23 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional social group analysis mostly uses interaction models, event\nmodels, or other methods to identify and distinguish groups. This type of\nmethod can divide social participants into different groups based on their\ngeographic location, social relationships, and/or related events. However, in\nsome applications, it is necessary to make more specific restrictions on the\nmembers and the interaction between members of the group. Generally, graph\npattern matching (GPM) is used to solve this problem. However, the existing GPM\nmethods rarely consider the rich contextual information of nodes and edges to\nmeasure the credibility between members. In this paper, a social group query\nproblem that needs to consider the trust between members of the group is\nproposed. To solve this problem, we propose a Strong Simulation GPM algorithm\n(NTSS) based on the exploration of pattern Node Topological ordered sequence.\nAiming at the inefficiency of the NTSS algorithm when matching pattern graph\nwith multiple nodes with zero in-degree and the problem of repeated calculation\nof matching edges shared by multiple matching subgraphs, two optimization\nstrategies are proposed. Finally, we conduct verification experiments on the\neffectiveness and efficiency of the NTSS algorithm and the algorithms with the\noptimization strategies on four social network datasets in real applications.\nExperimental results show that the NTSS algorithm is significantly better than\nthe existing multi-constrained GPM algorithm, and the NTSS_Inv_EdgC algorithm,\nwhich combines two optimization strategies, greatly improves the efficiency of\nthe NTSS algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 08:05:06 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Guliu", ""], ["Li", "Lei", ""], ["Liu", "Guanfeng", ""], ["Wu", "Xindong", ""]]}, {"id": "2103.09668", "submitter": "Gagandeep Singh", "authors": "Gagandeep Singh and Akshar Kaul", "title": "Secure Hypersphere Range Query on Encrypted Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial queries like range queries, nearest neighbor, circular range queries\netc. are the most widely used queries in the location-based applications.\nBuilding secure and efficient solutions for these queries in the cloud\ncomputing framework is critical and has been an area of active research. This\npaper focuses on the problem of Secure Circular Range Queries (SCRQ), where\nclient submits an encrypted query (consisting of a center point and radius of\nthe circle) and the cloud (storing encrypted data points) has to return the\npoints lying inside the circle. The existing solutions for this problem suffer\nfrom various disadvantages such as high processing time which is proportional\nto square of the query radius, query generation phase which is directly\nproportional to the number of points covered by the query etc. This paper\npresents solution for the above problem which is much more efficient than the\nexisting solutions. Three protocols are proposed with varying characteristics.\nIt is shown that all the three protocols are secure. The proposed protocols can\nbe extended to multiple dimensions and thus are able to handle Secure\nHypersphere Range Queries (SHRQ) as well. Internally the proposed protocols use\npairing-based cryptography and a concept of lookup table. To enable the\nefficient use of limited size lookup table, a new storage scheme is presented.\nThe proposed storage scheme enables the protocols to handle query with much\nlarger radius values. Using the SHRQ protocols, we also propose a mechanism to\nanswer the Secure range Queries. Extensive performance evaluation has been done\nto evaluate the efficiency of the proposed protocols\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 14:06:35 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Singh", "Gagandeep", ""], ["Kaul", "Akshar", ""]]}, {"id": "2103.09906", "submitter": "Zhihan Guo", "authors": "Zhihan Guo, Kan Wu, Cong Yan, Xiangyao Yu", "title": "Releasing Locks As Early As You Can: Reducing Contention of Hotspots by\n  Violating Two-Phase Locking (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hotspots, a small set of tuples frequently read/written by a large number of\ntransactions, cause contention in a concurrency control protocol. While a\nhotspot may comprise only a small fraction of a transaction's execution time,\nconventional strict two-phase locking allows a transaction to release lock only\nafter the transaction completes, which leaves significant parallelism\nunexploited. Ideally, a concurrency control protocol serializes transactions\nonly for the duration of the hotspots, rather than the duration of\ntransactions. We observe that exploiting such parallelism requires violating\ntwo-phase locking. In this paper, we propose Bamboo, a new concurrency control\nprotocol that can enable such parallelism by modifying the conventional\ntwo-phase locking, while maintaining the same guarantees in correctness. We\nthoroughly analyzed the effect of cascading aborts involved in reading\nuncommitted data and discussed optimizations that can be applied to further\nimprove the performance. Our evaluation on TPC-C shows a performance\nimprovement up to 4x compared to the best of pessimistic and optimistic\nbaseline protocols. On synthetic workloads that contain a single hotspot,\nBamboo achieves a speedup up to 19x over baselines.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 21:05:06 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Guo", "Zhihan", ""], ["Wu", "Kan", ""], ["Yan", "Cong", ""], ["Yu", "Xiangyao", ""]]}, {"id": "2103.09940", "submitter": "Aristotelis Leventidis", "authors": "Aristotelis Leventidis, Laura Di Rocco, Wolfgang Gatterbauer, Ren\\'ee\n  J. Miller, Mirek Riedewald", "title": "DomainNet: Homograph Detection for Data Lake Disambiguation", "comments": "Full version of paper appearing in EDBT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data lakes are deeply heterogeneous in the vocabulary that is used to\ndescribe data. We study a problem of disambiguation in data lakes: how can we\ndetermine if a data value occurring more than once in the lake has different\nmeanings and is therefore a homograph? While word and entity disambiguation\nhave been well studied in computational linguistics, data management and data\nscience, we show that data lakes provide a new opportunity for disambiguation\nof data values since they represent a massive network of interconnected values.\nWe investigate to what extent this network can be used to disambiguate values.\nDomainNet uses network-centrality measures on a bipartite graph whose nodes\nrepresent values and attributes to determine, without supervision, if a value\nis a homograph. A thorough experimental evaluation demonstrates that\nstate-of-the-art techniques in domain discovery cannot be re-purposed to\ncompete with our method. Specifically, using a domain discovery method to\nidentify homographs has a precision and a recall of 38% versus 69% with our\nmethod on a synthetic benchmark. By applying a network-centrality measure to\nour graph representation, DomainNet achieves a good separation between\nhomographs and data values with a unique meaning. On a real data lake our\ntop-200 precision is 89%.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 22:43:56 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 01:32:33 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Leventidis", "Aristotelis", ""], ["Di Rocco", "Laura", ""], ["Gatterbauer", "Wolfgang", ""], ["Miller", "Ren\u00e9e J.", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2103.10169", "submitter": "Asterios Katsifodimos", "authors": "Can Gencer, Marko Topolnik, Viliam \\v{D}urina, Emin Demirci, Ensar B.\n  Kahveci, Ali G\\\"urb\\\"uz Ond\\v{r}ej Luk\\'a\\v{s}, J\\'ozsef Bart\\'ok, Grzegorz\n  Gierlach, Franti\\v{s}ek Hartman, Ufuk Y{\\i}lmaz, Mehmet Do\\u{g}an, Mohamed\n  Mandouh, Marios Fragkoulis and Asterios Katsifodimos", "title": "Hazelcast Jet: Low-latency Stream Processing at the 99.99th Percentile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Jet is an open-source, high-performance, distributed stream processor built\nat Hazelcast during the last five years. Jet was engineered with millisecond\nlatency on the 99.99th percentile as its primary design goal. Originally Jet's\npurpose was to be an execution engine that performs complex business logic on\ntop of streams generated by Hazelcast's In-memory Data Grid (IMDG): a set of\nhigh-performance, in-memory, partitioned and replicated data structures. With\ntime, Jet evolved into a full-fledged, scale-out stream processor that can\nhandle out-of-order streams and exactly-once processing guarantees. Jet's\nend-to-end latency lies in the order of milliseconds, and its throughput in the\norder of millions of events per CPU-core. This paper presents main design\ndecisions we made in order to maximize the performance per CPU-core, alongside\nlessons learned, and an empirical performance evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:06:20 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Gencer", "Can", ""], ["Topolnik", "Marko", ""], ["\u010eurina", "Viliam", ""], ["Demirci", "Emin", ""], ["Kahveci", "Ensar B.", ""], ["Luk\u00e1\u0161", "Ali G\u00fcrb\u00fcz Ond\u0159ej", ""], ["Bart\u00f3k", "J\u00f3zsef", ""], ["Gierlach", "Grzegorz", ""], ["Hartman", "Franti\u0161ek", ""], ["Y\u0131lmaz", "Ufuk", ""], ["Do\u011fan", "Mehmet", ""], ["Mandouh", "Mohamed", ""], ["Fragkoulis", "Marios", ""], ["Katsifodimos", "Asterios", ""]]}, {"id": "2103.10520", "submitter": "Immanuel Trummer Mr.", "authors": "Immanuel Trummer and Connor Anderson", "title": "Optimally Summarizing Data by Small Fact Sets for Concise Answers to\n  Voice Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to find combinations of facts that optimally summarize data sets.\nWe consider this problem in the context of voice query interfaces for simple,\nexploratory data analysis. Here, the system answers voice queries with a short\nsummary of relevant data. Finding optimal voice data summaries is\ncomputationally expensive. Prior work in this domain has exploited sampling and\nincremental processing. Instead, we rely on a pre-processing stage generating\nsummaries of data subsets in a batch operation. This step reduces run time\noverheads by orders of magnitude.\n  We present multiple algorithms for the pre-processing stage, realizing\ndifferent tradeoffs between optimality and data processing overheads. We\nanalyze our algorithms formally and compare them experimentally with prior\nmethods for generating voice data summaries. We report on multiple user studies\nwith a prototype system implementing our approach. Furthermore, we report on\ninsights gained from a public deployment of our system on the Google Assistant\nPlatform.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:57:42 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Trummer", "Immanuel", ""], ["Anderson", "Connor", ""]]}, {"id": "2103.10558", "submitter": "Luigi Quaranta", "authors": "Luigi Quaranta, Fabio Calefato, Filippo Lanubile", "title": "KGTorrent: A Dataset of Python Jupyter Notebooks from Kaggle", "comments": null, "journal-ref": "Proc. of 2021 IEEE/ACM 18th International Conference on Mining\n  Software Repositories (MSR), pp. 550-554", "doi": "10.1109/MSR52588.2021.00072", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational notebooks have become the tool of choice for many data\nscientists and practitioners for performing analyses and disseminating results.\nDespite their increasing popularity, the research community cannot yet count on\na large, curated dataset of computational notebooks. In this paper, we fill\nthis gap by introducing KGTorrent, a dataset of Python Jupyter notebooks with\nrich metadata retrieved from Kaggle, a platform hosting data science\ncompetitions for learners and practitioners with any levels of expertise. We\ndescribe how we built KGTorrent, and provide instructions on how to use it and\nrefresh the collection to keep it up to date. Our vision is that the research\ncommunity will use KGTorrent to study how data scientists, especially\npractitioners, use Jupyter Notebook in the wild and identify potential\nshortcomings to inform the design of its future extensions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 22:57:01 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Quaranta", "Luigi", ""], ["Calefato", "Fabio", ""], ["Lanubile", "Filippo", ""]]}, {"id": "2103.11027", "submitter": "Robert Kent", "authors": "Robert E. Kent", "title": "Relational Operations in FOLE", "comments": "85 pages, 43 figures, 21 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses relational operations in the first-order logical\nenvironment {FOLE}. Here we demonstrate how FOLE expresses the relational\noperations of database theory in a clear and implementable representation. An\nanalysis of the representation of database tables/relations in FOLE reveals a\nprincipled way to express the relational operations. This representation is\nexpressed in terms of a distinction between basic components versus composite\nrelational operations. The 9 basic components fall into three categories:\nreflection (2), Booleans or basic operations (3), and adjoint flow (4). Adjoint\nflow is given for signatures (2) and for type domains (2), which are then\ncombined into full adjoint flow. The basic components are used to express\nvarious composite operations, where we illustrate each of these with a\nflowchart. Implementation of the composite operations is then expressed in an\ninput/output table containing four parts: constraint, construction, input, and\noutput. We explain how limits and colimits are constructed from diagrams of\ntables, and then classify composite relational operations into three\ncategories: limit-like, colimit-like and unorthodox.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 21:23:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kent", "Robert E.", ""]]}, {"id": "2103.11080", "submitter": "Zhenghua Lyu", "authors": "Zhenghua Lyu, Huan Hubert Zhang, Gang Xiong, Haozhou Wang, Gang Guo,\n  Jinbao Chen, Asim Praveen, Yu Yang, Xiaoming Gao, Ashwin Agrawal, Alexandra\n  Wang, Wen Lin, Junfeng Yang, Hao Wu, Xiaoliang Li, Feng Guo, Jiang Wu, Jesse\n  Zhang, Venkatesh Raghavan", "title": "Greenplum: A Hybrid Database for Transactional and Analytical Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand for enterprise data warehouse solutions to support real-time Online\nTransaction Processing (OLTP) queries as well as long-running Online Analytical\nProcessing (OLAP) workloads is growing. Greenplum database is traditionally\nknown as an OLAP data warehouse system with limited ability to process OLTP\nworkloads. In this paper, we augment Greenplum into a hybrid system to serve\nboth OLTP and OLAP workloads. The challenge we address here is to achieve this\ngoal while maintaining the ACID properties with minimal performance overhead.\nIn this effort, we identify the engineering and performance bottlenecks such as\nthe under-performing restrictive locking and the two-phase commit protocol.\nNext we solve the resource contention issues between transactional and\nanalytical queries. We propose a global deadlock detector to increase the\nconcurrency of query processing. When transactions that update data are\nguaranteed to reside on exactly one segment we introduce one-phase commit to\nspeed up query processing. Our resource group model introduces the capability\nto separate OLAP and OLTP workloads into more suitable query processing mode.\nOur experimental evaluation on the TPC-B and CH-benCHmark benchmarks\ndemonstrates the effectiveness of our approach in boosting the OLTP performance\nwithout sacrificing the OLAP performance.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 03:00:14 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 00:48:41 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 03:34:46 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lyu", "Zhenghua", ""], ["Zhang", "Huan Hubert", ""], ["Xiong", "Gang", ""], ["Wang", "Haozhou", ""], ["Guo", "Gang", ""], ["Chen", "Jinbao", ""], ["Praveen", "Asim", ""], ["Yang", "Yu", ""], ["Gao", "Xiaoming", ""], ["Agrawal", "Ashwin", ""], ["Wang", "Alexandra", ""], ["Lin", "Wen", ""], ["Yang", "Junfeng", ""], ["Wu", "Hao", ""], ["Li", "Xiaoliang", ""], ["Guo", "Feng", ""], ["Wu", "Jiang", ""], ["Zhang", "Jesse", ""], ["Raghavan", "Venkatesh", ""]]}, {"id": "2103.11137", "submitter": "Shixuan Sun", "authors": "Shixuan Sun and Yuhang Chen and Bingsheng He and Bryan Hooi", "title": "PathEnum: Towards Real-Time Hop-Constrained s-t Path Enumeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the hop-constrained s-t path enumeration (HcPE) problem, which takes\na graph $G$, two distinct vertices $s,t$ and a hop constraint $k$ as input, and\noutputs all paths from $s$ to $t$ whose length is at most $k$. The\nstate-of-the-art algorithms suffer from severe performance issues caused by the\ncostly pruning operations during enumeration for the workloads with the large\nsearch space. Consequently, these algorithms hardly meet the real-time\nconstraints of many online applications. In this paper, we propose PathEnum, an\nefficient index-based algorithm towards real-time HcPE. For an input query,\nPathEnum first builds a light-weight index aiming to reduce the number of edges\ninvolved in the enumeration, and develops efficient index-based approaches for\nenumeration, one based on depth-first search and the other based on joins. We\nfurther develop a query optimizer based on a join-based cost model to optimize\nthe search order. We conduct experiments with 15 real-world graphs. Our\nexperiment results show that PathEnum outperforms the state-of-the-art\napproaches by orders of magnitude in terms of the query time, throughput and\nresponse time.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 08:52:58 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 07:37:34 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Sun", "Shixuan", ""], ["Chen", "Yuhang", ""], ["He", "Bingsheng", ""], ["Hooi", "Bryan", ""]]}, {"id": "2103.11338", "submitter": "Aparna Varde", "authors": "Anita Pampoore-Thampi, Aparna S. Varde, Danlin Yu", "title": "Mining GIS Data to Predict Urban Sprawl", "comments": "8 Pages, 13 figures, KDD 2014 conference Bloomberg track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper addresses the interesting problem of processing and analyzing data\nin geographic information systems (GIS) to achieve a clear perspective on urban\nsprawl. The term urban sprawl refers to overgrowth and expansion of low-density\nareas with issues such as car dependency and segregation between residential\nversus commercial use. Sprawl has impacts on the environment and public health.\nIn our work, spatiotemporal features related to real GIS data on urban sprawl\nsuch as population growth and demographics are mined to discover knowledge for\ndecision support. We adapt data mining algorithms, Apriori for association rule\nmining and J4.8 for decision tree classification to geospatial analysis,\ndeploying the ArcGIS tool for mapping. Knowledge discovered by mining this\nspatiotemporal data is used to implement a prototype spatial decision support\nsystem (SDSS). This SDSS predicts whether urban sprawl is likely to occur.\nFurther, it estimates the values of pertinent variables to understand how the\nvariables impact each other. The SDSS can help decision-makers identify\nproblems and create solutions for avoiding future sprawl occurrence and\nconducting urban planning where sprawl already occurs, thus aiding sustainable\ndevelopment. This work falls in the broad realm of geospatial intelligence and\nsets the stage for designing a large scale SDSS to process big data in complex\nenvironments, which constitutes part of our future work.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 08:41:35 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pampoore-Thampi", "Anita", ""], ["Varde", "Aparna S.", ""], ["Yu", "Danlin", ""]]}, {"id": "2103.11342", "submitter": "Shuhan Zhang", "authors": "Ruqian Lu, Shuhan Zhang", "title": "BigCarl: Mining frequent subnets from a single large Petri net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there have been lots of work studying frequent subgraph mining, very\nrare publications have discussed frequent subnet mining from more complicated\ndata structures such as Petri nets. This paper studies frequent subnets mining\nfrom a single large Petri net. We follow the idea of transforming a Petri net\nin net graph form and to mine frequent sub-net graphs to avoid high complexity.\nTechnically, we take a minimal traversal approach to produce a canonical label\nof the big net graph. We adapted the maximal independent embedding set approach\nto the net graph representation and proposed an incremental pattern growth\n(independent embedding set reduction) way for discovering frequent sub-net\ngraphs from the single large net graph, which are finally transformed back to\nfrequent subnets. Extensive performance studies made on a single large Petri\nnet, which contains 10K events, 40K conditions and 30 K arcs, showed that our\napproach is correct and the complexity is reasonable.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 08:46:25 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lu", "Ruqian", ""], ["Zhang", "Shuhan", ""]]}, {"id": "2103.11525", "submitter": "Gordon Watts", "authors": "Gordon Watts", "title": "hep_tables: Heterogeneous Array Programming for HEP", "comments": "10 pages, 5 figures, submission for vCHEP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Array operations are one of the most concise ways of expressing common\nfiltering and simple aggregation operations that is the hallmark of the first\nstep of a particle physics analysis: selection, filtering, basic vector\noperations, and filling histograms. The High Luminosity run of the Large Hadron\nCollider (HL-LHC), scheduled to start in 2026, will require physicists to\nregularly skim datasets that are over a PB in size, and repeatedly run over\ndatasets that are 100's of TB's - too big to fit in memory. Declarative\nprogramming techniques are a way of separating the intent of the physicist from\nthe mechanics of finding the data, processing the data, and using distributed\ncomputing to process it efficiently that is required to extract the plot or\ndata desired in a timely fashion. This paper describes a prototype library that\nprovides a framework for different sub-systems to cooperate in producing this\ndata, using an array-programming declarative interface. This prototype has a\nServiceX data-delivery sub-system and an awkward array sub-system cooperating\nto generate requested data. The ServiceX system runs against ATLAS xAOD data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 00:49:45 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Watts", "Gordon", ""]]}, {"id": "2103.11630", "submitter": "Jiping Zheng", "authors": "Jiping Zheng, Qi Dong, Xiaoyang Wang, Ying Zhang, Wei Ma, Yuan Ma", "title": "Efficient Processing of k-regret Minimization Queries with Theoretical\n  Guarantees", "comments": "14 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assisting end users to identify desired results from a large dataset is an\nimportant problem for multi-criteria decision making. To address this problem,\ntop-k and skyline queries have been widely adopted, but they both have inherent\ndrawbacks, i.e., the user either has to provide a specific utility function or\nfaces many results. The k-regret minimization query is proposed, which\nintegrates the merits of top-k and skyline queries. Due to the NP-hardness of\nthe problem, the k-regret minimization query is time consuming and the greedy\nframework is widely adopted. However, formal theoretical analysis of the greedy\napproaches for the quality of the returned results is still lacking. In this\npaper, we first fill this gap by conducting a nontrivial theoretical analysis\nof the approximation ratio of the returned results. To speed up query\nprocessing, a sampling-based method, StocPreGreed,, is developed to reduce the\nevaluation cost. In addition, a theoretical analysis of the required sample\nsize is conducted to bound the quality of the returned results. Finally,\ncomprehensive experiments are conducted on both real and synthetic datasets to\ndemonstrate the efficiency and effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 07:41:07 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zheng", "Jiping", ""], ["Dong", "Qi", ""], ["Wang", "Xiaoyang", ""], ["Zhang", "Ying", ""], ["Ma", "Wei", ""], ["Ma", "Yuan", ""]]}, {"id": "2103.11740", "submitter": "Martin Kabierski", "authors": "Martin Kabierski, Stephan Fahrenkrog-Petersen, Matthias Weidlich", "title": "Privacy-aware Process Performance Indicators: Framework and Release\n  Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Process performance indicators (PPIs) are metrics to quantify the degree with\nwhich organizational goals defined based on business processes are fulfilled.\nThey exploit the event logs recorded by information systems during the\nexecution of business processes, thereby providing a basis for process\nmonitoring and subsequent optimization. However, PPIs are often evaluated on\nprocesses that involve individuals, which implies an inevitable risk of privacy\nintrusion. In this paper, we address the demand for privacy protection in the\ncomputation of PPIs. We first present a framework that enforces control over\nthe data exploited for process monitoring. We then show how PPIs defined based\non the established PPINOT meta-model are instantiated in this framework through\na set of data release mechanisms. These mechanisms are designed to provide\nprovable guarantees in terms of differential privacy. We evaluate our framework\nand the release mechanisms in a series of controlled experiments. We further\nuse a public event log to compare our framework with approaches based on\nprivatization of event logs. The results demonstrate feasibility and shed light\non the trade-offs between data utility and privacy guarantees in the\ncomputation of PPIs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:41:21 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kabierski", "Martin", ""], ["Fahrenkrog-Petersen", "Stephan", ""], ["Weidlich", "Matthias", ""]]}, {"id": "2103.11824", "submitter": "Weiwei Jiang", "authors": "Weiwei Jiang, Jiayun Luo", "title": "Big Data for Traffic Estimation and Prediction: A Survey of Data and\n  Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Big data has been used widely in many areas including the transportation\nindustry. Using various data sources, traffic states can be well estimated and\nfurther predicted for improving the overall operation efficiency. Combined with\nthis trend, this study presents an up-to-date survey of open data and big data\ntools used for traffic estimation and prediction. Different data types are\ncategorized and the off-the-shelf tools are introduced. To further promote the\nuse of big data for traffic estimation and prediction tasks, challenges and\nfuture directions are given for future studies.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 01:46:05 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jiang", "Weiwei", ""], ["Luo", "Jiayun", ""]]}, {"id": "2103.11972", "submitter": "Sainyam Galhotra", "authors": "Sainyam Galhotra, Romila Pradhan, Babak Salimi", "title": "Explaining Black-Box Algorithms Using Probabilistic Contrastive\n  Counterfactuals", "comments": "Proceedings of the 2021 International Conference on Management of\n  Data. ACM, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a recent resurgence of interest in explainable artificial\nintelligence (XAI) that aims to reduce the opaqueness of AI-based\ndecision-making systems, allowing humans to scrutinize and trust them. Prior\nwork in this context has focused on the attribution of responsibility for an\nalgorithm's decisions to its inputs wherein responsibility is typically\napproached as a purely associational concept. In this paper, we propose a\nprincipled causality-based approach for explaining black-box decision-making\nsystems that addresses limitations of existing methods in XAI. At the core of\nour framework lies probabilistic contrastive counterfactuals, a concept that\ncan be traced back to philosophical, cognitive, and social foundations of\ntheories on how humans generate and select explanations. We show how such\ncounterfactuals can quantify the direct and indirect influences of a variable\non decisions made by an algorithm, and provide actionable recourse for\nindividuals negatively affected by the algorithm's decision. Unlike prior work,\nour system, LEWIS: (1)can compute provably effective explanations and recourse\nat local, global and contextual levels (2)is designed to work with users with\nvarying levels of background knowledge of the underlying causal model and\n(3)makes no assumptions about the internals of an algorithmic system except for\nthe availability of its input-output data. We empirically evaluate LEWIS on\nthree real-world datasets and show that it generates human-understandable\nexplanations that improve upon state-of-the-art approaches in XAI, including\nthe popular LIME and SHAP. Experiments on synthetic data further demonstrate\nthe correctness of LEWIS's explanations and the scalability of its recourse\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 16:20:21 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 06:33:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Pradhan", "Romila", ""], ["Salimi", "Babak", ""]]}, {"id": "2103.12317", "submitter": "Rana Alotaibi", "authors": "Rana Alotaibi, Bogdan Cautis, Alin Deutsch, Ioana Manolescu", "title": "HADAD: A Lightweight Approach for Optimizing Hybrid Complex Analytics\n  Queries (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hybrid complex analytics workloads typically include (i) data management\ntasks (joins, selections, etc. ), easily expressed using relational algebra\n(RA)-based languages, and (ii) complex analytics tasks (regressions, matrix\ndecompositions, etc.), mostly expressed in linear algebra (LA) expressions.\nSuch workloads are common in many application areas, including scientific\ncomputing, web analytics, and business recommendation. Existing solutions for\nevaluating hybrid analytical tasks - ranging from LA-oriented systems, to\nrelational systems (extended to handle LA operations), to hybrid systems -\neither optimize data management and complex tasks separately, exploit RA\nproperties only while leaving LA-specific optimization opportunities\nunexploited, or focus heavily on physical optimization, leaving semantic query\noptimization opportunities unexplored. Additionally, they are not able to\nexploit precomputed (materialized) results to avoid recomputing (part of) a\ngiven mixed (RA and/or LA) computation. In this paper, we take a major step\ntowards filling this gap by proposing HADAD, an extensible lightweight approach\nfor optimizing hybrid complex analytics queries, based on a common abstraction\nthat facilitates unified reasoning: a relational model endowed with integrity\nconstraints. Our solution can be naturally and portably applied on top of pure\nLA and hybrid RA-LA platforms without modifying their internals. An extensive\nempirical evaluation shows that HADAD yields significant performance gains on\ndiverse workloads, ranging from LA-centered to hybrid.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:18:24 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Alotaibi", "Rana", ""], ["Cautis", "Bogdan", ""], ["Deutsch", "Alin", ""], ["Manolescu", "Ioana", ""]]}, {"id": "2103.12465", "submitter": "Johan Kok Zhi Kang", "authors": "Johan Kok Zhi Kang, Gaurav, Sien Yi Tan, Feng Cheng, Shixuan Sun,\n  Bingsheng He", "title": "Efficient Deep Learning Pipelines for Accurate Cost Estimations Over\n  Large Scale Query Workload", "comments": "Technical report, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep learning models for forecasting the resource consumption\npatterns of SQL queries have recently been a popular area of study. With many\ncompanies using cloud platforms to power their data lakes for large scale\nanalytic demands, these models form a critical part of the pipeline in managing\ncloud resource provisioning. While these models have demonstrated promising\naccuracy, training them over large scale industry workloads are expensive.\nSpace inefficiencies of encoding techniques over large numbers of queries and\nexcessive padding used to enforce shape consistency across diverse query plans\nimplies 1) longer model training time and 2) the need for expensive, scaled up\ninfrastructure to support batched training. In turn, we developed Prestroid, a\ntree convolution based data science pipeline that accurately predicts resource\nconsumption patterns of query traces, but at a much lower cost.\n  We evaluated our pipeline over 19K Presto OLAP queries from Grab, on a data\nlake of more than 20PB of data. Experimental results imply that our pipeline\noutperforms benchmarks on predictive accuracy, contributing to more precise\nresource prediction for large-scale workloads, yet also reduces per-batch\nmemory footprint by 13.5x and per-epoch training time by 3.45x. We demonstrate\ndirect cost savings of up to 13.2x for large batched model training over\nMicrosoft Azure VMs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:36:10 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Kang", "Johan Kok Zhi", ""], ["Gaurav", "", ""], ["Tan", "Sien Yi", ""], ["Cheng", "Feng", ""], ["Sun", "Shixuan", ""], ["He", "Bingsheng", ""]]}, {"id": "2103.12468", "submitter": "Marc Roth", "authors": "Jacob Focke, Leslie Ann Goldberg, Marc Roth, Stanislav \\v{Z}ivn\\'y", "title": "Approximately Counting Answers to Conjunctive Queries with Disequalities\n  and Negations", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of approximating the number of answers to a small\nquery $\\varphi$ in a large database $\\mathcal{D}$. We establish an exhaustive\nclassification into tractable and intractable cases if $\\varphi$ is a\nconjunctive query with disequalities and negations:\n  $\\bullet$ If there is a constant bound on the arity of $\\varphi$, and if the\nrandomised Exponential Time Hypothesis (rETH) holds, then the problem has a\nfixed-parameter tractable approximation scheme (FPTRAS) if and only if the\ntreewidth of $\\varphi$ is bounded.\n  $\\bullet$ If the arity is unbounded and we allow disequalities only, then the\nproblem has an FPTRAS if and only if the adaptive width of $\\varphi$ (a width\nmeasure strictly more general than treewidth) is bounded; the lower bound\nrelies on the rETH as well.\n  Additionally we show that our results cannot be strengthened to achieve a\nfully polynomial randomised approximation scheme (FPRAS): We observe that,\nunless $\\mathrm{NP} =\\mathrm{RP}$, there is no FPRAS even if the treewidth (and\nthe adaptive width) is $1$. However, if there are neither disequalities nor\nnegations, we prove the existence of an FPRAS for queries of bounded fractional\nhypertreewidth, strictly generalising the recently established FPRAS for\nconjunctive queries with bounded hypertreewidth due to Arenas, Croquevielle,\nJayaram and Riveros (STOC 2021).\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:45:09 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Focke", "Jacob", ""], ["Goldberg", "Leslie Ann", ""], ["Roth", "Marc", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "2103.12797", "submitter": "Binger Chen", "authors": "Binger Chen, Ziawasch Abedjan", "title": "RPT: Effective and Efficient Retrieval of Program Translations from Big\n  Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program translation is a growing demand in software engineering. Manual\nprogram translation requires programming expertise in source and target\nlanguage. One way to automate this process is to make use of the big data of\nprograms, i.e., Big Code. In particular, one can search for program\ntranslations in Big Code. However, existing code retrieval techniques are not\ndesigned for cross-language code retrieval. Other data-driven approaches\nrequire human efforts in constructing cross-language parallel datasets to train\ntranslation models. In this paper, we present RPT, a novel code translation\nretrieval system. We propose a lightweight but informative program\nrepresentation, which can be generalized to all imperative PLs. Furthermore, we\npresent our index structure and hierarchical filtering mechanism for efficient\ncode retrieval from a Big Code database.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 19:01:43 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chen", "Binger", ""], ["Abedjan", "Ziawasch", ""]]}, {"id": "2103.12887", "submitter": "Junyang Gao", "authors": "Junyang Gao, Yifan Xu, Pankaj K. Agarwal, Jun Yang", "title": "Efficiently Answering Durability Prediction Queries", "comments": "in SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of queries called durability prediction queries that\narise commonly in predictive analytics, where we use a given predictive model\nto answer questions about possible futures to inform our decisions. Examples of\ndurability prediction queries include \"what is the probability that this\nfinancial product will keep losing money over the next 12 quarters before\nturning in any profit?\" and \"what is the chance for our proposed server cluster\nto fail the required service-level agreement before its term ends?\" We devise a\ngeneral method called Multi-Level Splitting Sampling (MLSS) that can\nefficiently handle complex queries and complex models -- including those\ninvolving black-box functions -- as long as the models allow us to simulate\npossible futures step by step. Our method addresses the inefficiency of\nstandard Monte Carlo (MC) methods by applying the idea of importance splitting\nto let one \"promising\" sample path prefix generate multiple \"offspring\" paths,\nthereby directing simulation efforts toward more promising paths. We propose\npractical techniques for designing splitting strategies, freeing users from\nmanual tuning. Experiments show that our approach is able to achieve unbiased\nestimates and the same error guarantees as standard MC while offering an\norder-of-magnitude cost reduction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 23:21:35 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 21:44:34 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gao", "Junyang", ""], ["Xu", "Yifan", ""], ["Agarwal", "Pankaj K.", ""], ["Yang", "Jun", ""]]}, {"id": "2103.13155", "submitter": "Jerome Darmont", "authors": "Etienne Scholly (ERIC), Pegdwend\\'e Sawadogo (ERIC), Pengfei Liu\n  (ERIC), Javier Alfonso Espinosa-Oviedo (ERIC), C\\'ecile Favre (ERIC), Sabine\n  Loudcher (ERIC), J\\'er\\^ome Darmont (ERIC), Camille No\\^us", "title": "Coining goldMEDAL: A New Contribution to Data Lake Generic Metadata\n  Modeling", "comments": null, "journal-ref": "23rd International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data (DOLAP@EDBT/ICDT 2021), Mar 2021, Nicosia,\n  Cyprus", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of big data has revolutionized data exploitation practices and led\nto the emergence of new concepts. Among them, data lakes have emerged as large\nheterogeneous data repositories that can be analyzed by various methods. An\nefficient data lake requires a metadata system that addresses the many problems\narising when dealing with big data. In consequence, the study of data lake\nmetadata models is currently an active research topic and many proposals have\nbeen made in this regard. However, existing metadata models are either tailored\nfor a specific use case or insufficiently generic to manage different types of\ndata lakes, including our previous model MEDAL. In this paper, we generalize\nMEDAL's concepts in a new metadata model called goldMEDAL. Moreover, we compare\ngoldMEDAL with the most recent state-of-the-art metadata models aiming at\ngenericity and show that we can reproduce these metadata models with\ngoldMEDAL's concepts. As a proof of concept, we also illustrate that goldMEDAL\nallows the design of various data lakes by presenting three different use\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:56:57 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Scholly", "Etienne", "", "ERIC"], ["Sawadogo", "Pegdwend\u00e9", "", "ERIC"], ["Liu", "Pengfei", "", "ERIC"], ["Espinosa-Oviedo", "Javier Alfonso", "", "ERIC"], ["Favre", "C\u00e9cile", "", "ERIC"], ["Loudcher", "Sabine", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["No\u00fbs", "Camille", ""]]}, {"id": "2103.13681", "submitter": "Kyoungmin Kim", "authors": "Kyoungmin Kim, Hyeonji Kim, George Fletcher, Wook-Shin Han", "title": "[Technical Report] Combining Sampling and Synopses with Worst-Case\n  Optimal Runtime and Quality Guarantees for Graph Pattern Cardinality\n  Estimation", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pattern cardinality estimation is the problem of estimating the number\nof embeddings of a query graph in a data graph. This fundamental problem\narises, for example, during query planning in subgraph matching algorithms.\nThere are two major approaches to solving the problem: sampling and synopsis.\nSynopsis (or summary)-based methods are fast and accurate if synopses capture\ninformation of graphs well. However, these methods suffer from large errors due\nto loss of information during summarization and inherent assumptions.\nSampling-based methods are unbiased but suffer from large estimation variance\ndue to large sample space.\n  To address these limitations, we propose Alley, a hybrid method that combines\nboth sampling and synopses. Alley employs 1) a novel sampling strategy, random\nwalk with intersection, which effectively reduces the sample space, 2)\nbranching to further reduce variance, and 3) a novel mining approach that\nextracts and indexes tangled patterns as synopses which are inherently\ndifficult to estimate by sampling. By using them in the online estimation\nphase, we can effectively reduce the sample space while still ensuring\nunbiasedness. We establish that Alley has worst-case optimal runtime and\napproximation quality guarantees for any given error bound $\\epsilon$ and\nrequired confidence $\\mu$. In addition to the theoretical aspect of Alley, our\nextensive experiments show that Alley outperforms the state-of-the-art methods\nby up to orders of magnitude higher accuracy with similar efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 08:58:47 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 10:54:05 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kim", "Kyoungmin", ""], ["Kim", "Hyeonji", ""], ["Fletcher", "George", ""], ["Han", "Wook-Shin", ""]]}, {"id": "2103.14000", "submitter": "Ke Yang", "authors": "Meike Zehlike, Ke Yang, Julia Stoyanovich", "title": "Fairness in Ranking: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, there has been much work on incorporating fairness\nrequirements into algorithmic rankers, with contributions coming from the data\nmanagement, algorithms, information retrieval, and recommender systems\ncommunities. In this survey we give a systematic overview of this work,\noffering a broad perspective that connects formalizations and algorithmic\napproaches across subfields. An important contribution of our work is in\ndeveloping a common narrative around the value frameworks that motivate\nspecific fairness-enhancing interventions in ranking. This allows us to unify\nthe presentation of mitigation objectives and of algorithmic techniques to help\nmeet those objectives or identify trade-offs.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:38:20 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 23:48:52 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zehlike", "Meike", ""], ["Yang", "Ke", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "2103.14071", "submitter": "Roy Friedman", "authors": "Yamit Barshatz-Schneor and Roy Friedman", "title": "Accelerating Big-Data Sorting Through Programmable Switches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sorting is a fundamental and well studied problem that has been studied\nextensively. Sorting plays an important role in the area of databases, as many\nqueries can be served much faster if the relations are first sorted. One of the\nmost popular sorting algorithm in databases is merge sort.\n  In modern data-centers, data is stored in storage servers, while processing\ntakes place in compute servers. Hence, in order to compute queries on the data,\nit must travel through the network from the storage servers to the compute\nservers. This creates a potential for utilizing programmable switches to\nperform partial sorting in order to accelerate the sorting process at the\nserver side. This is possible because, as mentioned above, data packets pass\nthrough the switch in any case on their way to the server. Alas, programmable\nswitches offer a very restricted and non-intuitive programming model, which is\nwhy realizing this is not-trivial.\n  We devised a novel partial sorting algorithm that fits the programming model\nand restrictions of programmable switches and can expedite merge sort at the\nserver. We also utilize built-in parallelism in the switch to divide the data\ninto sequential ranges. Thus, the server needs to sort each range separately\nand then concatenate them to one sorted stream. This way, the server needs to\nsort smaller sections and each of these sections is already partially sorted.\nHence, the server does less work, and the access pattern becomes more\nvirtual-memory friendly.\n  We evaluated the performance improvements obtained when utilizing our partial\nsorting algorithm over several data stream compositions with various switch\nconfigurations. Our study exhibits an improvement of 20%-75% in the sorting\nrun-time when using our approach compared to plain sorting on the original\nstream.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:46:33 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Barshatz-Schneor", "Yamit", ""], ["Friedman", "Roy", ""]]}, {"id": "2103.14120", "submitter": "Ainur Smagulova", "authors": "Ainur Smagulova, Alin Deutsch", "title": "Vertex-centric Parallel Computation of SQL Queries", "comments": "50 pages, LaTeX; added examples in Section 6.4; corrected figure in\n  Section 8.6", "journal-ref": null, "doi": "10.1145/3448016.3457314", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a scheme for parallel execution of SQL queries on top of any\nvertex-centric BSP graph processing engine. The scheme comprises a graph\nencoding of relational instances and a vertex program specification of our\nalgorithm called TAG-join, which matches the theoretical communication and\ncomputation complexity of state-of-the-art join algorithms. When run on top of\nthe vertex-centric TigerGraph database engine on a single multi-core server,\nTAG-join exploits thread parallelism and is competitive with (and often\noutperforms) reference RDBMSs on the TPC benchmarks they are traditionally\ntuned for. In a distributed cluster, TAG-join outperforms the popular Spark SQL\nengine.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 20:22:33 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 20:52:25 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 23:17:57 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Smagulova", "Ainur", ""], ["Deutsch", "Alin", ""]]}, {"id": "2103.14189", "submitter": "Kurtis Haut", "authors": "Taylan K. Sen, Gazi Naven, Luke Gerstner, Daryl Bagley, Raiyan Abdul\n  Baten, Wasifur Rahman, Kamrul Hasan, Kurtis G. Haut, Abdullah Mamun, Samiha\n  Samrose, Anne Solbu, R. Eric Barnes, Mark G. Frank, Ehsan Hoque", "title": "DBATES: DataBase of Audio features, Text, and visual Expressions in\n  competitive debate Speeches", "comments": "12 pages, 5 figures, 4 tables, under-going major revision for TAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we present a database of multimodal communication features\nextracted from debate speeches in the 2019 North American Universities Debate\nChampionships (NAUDC). Feature sets were extracted from the visual (facial\nexpression, gaze, and head pose), audio (PRAAT), and textual (word sentiment\nand linguistic category) modalities of raw video recordings of competitive\ncollegiate debaters (N=717 6-minute recordings from 140 unique debaters). Each\nspeech has an associated competition debate score (range: 67-96) from expert\njudges as well as competitor demographic and per-round reflection surveys. We\nobserve the fully multimodal model performs best in comparison to models\ntrained on various compositions of modalities. We also find that the weights of\nsome features (such as the expression of joy and the use of the word we) change\nin direction between the aforementioned models. We use these results to\nhighlight the value of a multimodal dataset for studying competitive,\ncollegiate debate.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 00:43:49 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Sen", "Taylan K.", ""], ["Naven", "Gazi", ""], ["Gerstner", "Luke", ""], ["Bagley", "Daryl", ""], ["Baten", "Raiyan Abdul", ""], ["Rahman", "Wasifur", ""], ["Hasan", "Kamrul", ""], ["Haut", "Kurtis G.", ""], ["Mamun", "Abdullah", ""], ["Samrose", "Samiha", ""], ["Solbu", "Anne", ""], ["Barnes", "R. Eric", ""], ["Frank", "Mark G.", ""], ["Hoque", "Ehsan", ""]]}, {"id": "2103.14294", "submitter": "Zhengyi Yang", "authors": "Zhengyi Yang, Longbin Lai, Xuemin Lin, Kongzhang Hao, Wenjie Zhang", "title": "HUGE: An Efficient and Scalable Subgraph Enumeration System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph enumeration is a fundamental problem in graph analytics, which aims\nto find all instances of a given query graph on a large data graph. In this\npaper, we propose a system called HUGE to efficiently process subgraph\nenumeration at scale in the distributed context. HUGE features 1) an optimiser\nto compute an advanced execution plan without the constraints of existing\nworks; 2) a hybrid communication layer that supports both pushing and pulling\ncommunication; 3) a novel two-stage execution mode with a lock-free and\nzero-copy cache design, 4) a BFS/DFS-adaptive scheduler to bound memory\nconsumption, and 5) two-layer intra- and inter-machine load balancing. HUGE is\ngeneric such that all existing distributed subgraph enumeration algorithms can\nbe plugged in to enjoy automatic speed up and bounded-memory execution.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 06:57:23 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 11:29:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yang", "Zhengyi", ""], ["Lai", "Longbin", ""], ["Lin", "Xuemin", ""], ["Hao", "Kongzhang", ""], ["Zhang", "Wenjie", ""]]}, {"id": "2103.14371", "submitter": "Ramya C", "authors": "Dr. Ramya C, Dr. Shreedhara K S", "title": "A PSO Strategy of Finding Relevant Web Documents using a New Similarity\n  Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the world of the Internet and World Wide Web, which offers a tremendous\namount of information, an increasing emphasis is being given to searching\nservices and functionality. Currently, a majority of web portals offer their\nsearching utilities, be it better or worse. These can search for the content\nwithin the sites, mainly text the textual content of documents. In this paper a\nnovel similarity measure called SMDR (Similarity Measure for Documents\nRetrieval) is proposed to help retrieve more similar documents from the\nrepository thus contributing considerably to the effectiveness of Web\nInformation Retrieval (WIR) process. Bio-inspired PSO methodology is used with\nthe intent to reduce the response time of the system and optimizes WIR process,\nhence contributes to the efficiency of the system. This paper also demonstrates\na comparative study of the proposed system with the existing method in terms of\naccuracy, sensitivity, F-measure and specificity. Finally, extensive\nexperiments are conducted on CACM collections. Better precision-recall rates\nare achieved than the existing system. Experimental results demonstrate the\neffectiveness and efficiency of the proposed system.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:19:38 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["C", "Dr. Ramya", ""], ["S", "Dr. Shreedhara K", ""]]}, {"id": "2103.14435", "submitter": "Amir Gilad", "authors": "Amir Gilad, Shweta Patwa, Ashwin Machanavajjhala", "title": "Synthesizing Linked Data Under Cardinality and Integrity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The generation of synthetic data is useful in multiple aspects, from testing\napplications to benchmarking to privacy preservation. Generating the links\nbetween relations, subject to cardinality constraints (CCs) and integrity\nconstraints (ICs) is an important aspect of this problem. Given instances of\ntwo relations, where one has a foreign key dependence on the other and is\nmissing its foreign key ($FK$) values, and two types of constraints: (1) CCs\nthat apply to the join view and (2) ICs that apply to the table with missing\n$FK$ values, our goal is to impute the missing $FK$ values such that the\nconstraints are satisfied. We provide a novel framework for the problem based\non declarative CCs and ICs. We further show that the problem is NP-hard and\npropose a novel two-phase solution that guarantees the satisfaction of the ICs.\nPhase I yields an intermediate solution accounting for the CCs alone, and\nrelies on a hybrid approach based on CC types. For one type, the problem is\nmodeled as an Integer Linear Program. For the others, we describe an efficient\nand accurate solution. We then combine the two solutions. Phase II augments\nthis solution by incorporating the ICs and uses a coloring of the conflict\nhypergraph to infer the values of the $FK$ column. Our extensive experimental\nstudy shows that our solution scales well when the data and number of\nconstraints increases. We further show that our solution maintains low error\nrates for the CCs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:35:32 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Gilad", "Amir", ""], ["Patwa", "Shweta", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "2103.14688", "submitter": "Semyon Grigorev", "authors": "Ekaterina Shemetova, Rustam Azimov, Egor Orachev, Ilya Epelbaum,\n  Semyon Grigorev", "title": "One Algorithm to Evaluate Them All: Unified Linear Algebra Based\n  Approach to Evaluate Both Regular and Context-Free Path Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Kronecker product-based algorithm for context-free path querying (CFPQ)\nwas proposed by Orachev et al. (2020). We reduce this algorithm to operations\nover Boolean matrices and extend it with the mechanism to extract all paths of\ninterest. We also prove $O(n^3/\\log{n})$ time complexity of the proposed\nalgorithm, where n is a number of vertices of the input graph. Thus, we provide\nthe alternative way to construct a slightly subcubic algorithm for CFPQ which\nis based on linear algebra and incremental transitive closure (a classic\ngraph-theoretic problem), as opposed to the algorithm with the same complexity\nproposed by Chaudhuri (2008). Our evaluation shows that our algorithm is a good\ncandidate to be the universal algorithm for both regular and context-free path\nquerying.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:54:22 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Shemetova", "Ekaterina", ""], ["Azimov", "Rustam", ""], ["Orachev", "Egor", ""], ["Epelbaum", "Ilya", ""], ["Grigorev", "Semyon", ""]]}, {"id": "2103.14695", "submitter": "Favyen Bastani", "authors": "Favyen Bastani and Sam Madden", "title": "MultiScope: Efficient Video Pre-processing for Exploratory Video\n  Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performing analytics tasks over large-scale video datasets is increasingly\ncommon in a wide range of applications. These tasks generally involve object\ndetection and tracking operations that require applying expensive machine\nlearning models, and several systems have recently been proposed to optimize\nthe execution of video queries to reduce their cost. However, prior work\ngenerally optimizes execution speed in only one dimension, focusing on one\noptimization technique while ignoring other potential avenues for accelerating\nexecution, thereby delivering an unsatisfactory tradeoff between speed and\naccuracy. We propose MultiScope, a general-purpose video pre-processor for\nobject detection and tracking that explores multiple avenues for optimizing\nvideo queries to extract tracks from video with a superior tradeoff between\nspeed and accuracy over prior work. We compare MultiScope against three recent\nsystems on seven diverse datasets, and find that it provides a 2.9x average\nspeedup over the next best baseline at the same accuracy level.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 19:04:12 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bastani", "Favyen", ""], ["Madden", "Sam", ""]]}, {"id": "2103.14713", "submitter": "Jing Tang", "authors": "Yuming Huang and Jing Tang and Qianhao Cong and Andrew Lim and\n  Jianliang Xu", "title": "Do the Rich Get Richer? Fairness Analysis for Blockchain Incentives", "comments": "A short version of the paper will appear in 2021 International\n  Conference on Management of Data (SIGMOD '21)", "journal-ref": null, "doi": "10.1145/3448016.3457285", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proof-of-Work (PoW) is the most widely adopted incentive model in current\nblockchain systems, which unfortunately is energy inefficient. Proof-of-Stake\n(PoS) is then proposed to tackle the energy issue. The rich-get-richer concern\nof PoS has been heavily debated in the blockchain community. The debate is\ncentered around the argument that whether rich miners possessing more stakes\nwill obtain higher staking rewards and further increase their potential income\nin the future. In this paper, we define two types of fairness, i.e.,\nexpectational fairness and robust fairness, that are useful for answering this\nquestion. In particular, expectational fairness illustrates that the expected\nincome of a miner is proportional to her initial investment, indicating that\nthe expected return on investment is a constant. To better capture the\nuncertainty of mining outcomes, robust fairness is proposed to characterize\nwhether the return on investment concentrates to a constant with high\nprobability as time evolves. Our analysis shows that the classical PoW\nmechanism can always preserve both types of fairness as long as the mining game\nruns for a sufficiently long time. Furthermore, we observe that current PoS\nblockchains implement various incentive models and discuss three\nrepresentatives, namely ML-PoS, SL-PoS and C-PoS. We find that (i) ML-PoS\n(e.g., Qtum and Blackcoin) preserves expectational fairness but may not achieve\nrobust fairness, (ii) SL-PoS (e.g., NXT) does not protect any type of fairness,\nand (iii) C-PoS (e.g., Ethereum 2.0) outperforms ML-PoS in terms of robust\nfairness while still maintaining expectational fairness. Finally, massive\nexperiments on real blockchain systems and extensive numerical simulations are\nperformed to validate our analysis.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 19:53:15 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 07:02:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Huang", "Yuming", ""], ["Tang", "Jing", ""], ["Cong", "Qianhao", ""], ["Lim", "Andrew", ""], ["Xu", "Jianliang", ""]]}, {"id": "2103.14837", "submitter": "Vladimir Ivanov", "authors": "V. K. Ivanov", "title": "Peculiarities of organization of data storage based on intelligent\n  search agent and evolutionary model selection the target information", "comments": "12 pages, in Russian", "journal-ref": "Bulletin of the Tver State Technical University. Series\n  \"Engineering Sciences\". 2019. N 1 (1)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The article presents a systematic review of the results of the development of\nthe theoretical basis and the pilot implementation of data storage technology\nwith automatic replenishment of data from sources belonging to different\nthematic segments. It is expected that the repository will contain information\nabout objects with significant innovative potential. The mechanism of selection\nof such information is based on the determination of its semantic relevance to\nthe generated search queries. At the same time, a quantitative assessment of\nthe innovation of objects, in particular their technological novelty and demand\nis given. The article describes the accepted indicators of innovation,\ndiscusses the application of the theory of evidence for the processing of\nincomplete and fuzzy information, identifies the main ideas of the method of\nprocessing the results of measurements for the calculation of the probabilistic\nvalue of the components of innovation, briefly describes the application of the\nevolutionary approach in the formation of the linguistic model of the archetype\nof the object, provides information about the experimental verification of the\nadequacy of the developed computational model. The research results that are\ndescribed in the article can be used for business planning, forecasting of\ntechnological development, information support of investment projects\nexpertise.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 07:53:12 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ivanov", "V. K.", ""]]}, {"id": "2103.14915", "submitter": "Shengliang Lu", "authors": "Shengliang Lu, Shixuan Sun, Johns Paul, Yuchen Li, Bingsheng He", "title": "Cache-Efficient Fork-Processing Patterns on Large Graphs", "comments": "in SIGMOD 2021", "journal-ref": null, "doi": "10.1145/3448016.3457253", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As large graph processing emerges, we observe a costly fork-processing\npattern (FPP) that is common in many graph algorithms. The unique feature of\nthe FPP is that it launches many independent queries from different source\nvertices on the same graph. For example, an algorithm in analyzing the network\ncommunity profile can execute Personalized PageRanks that start from tens of\nthousands of source vertices at the same time. We study the efficiency of\nhandling FPPs in state-of-the-art graph processing systems on multi-core\narchitectures. We find that those systems suffer from severe cache miss penalty\nbecause of the irregular and uncoordinated memory accesses in processing FPPs.\n  In this paper, we propose ForkGraph, a cache-efficient FPP processing system\non multi-core architectures. To improve the cache reuse, we divide the graph\ninto partitions each sized of LLC capacity, and the queries in an FPP are\nbuffered and executed on the partition basis. We further develop efficient\nintra- and inter-partition execution strategies for efficiency. For\nintra-partition processing, since the graph partition fits into LLC, we propose\nto execute each graph query with efficient sequential algorithms (in contrast\nwith parallel algorithms in existing parallel graph processing systems) and\npresent an atomic-free query processing by consolidating contending operations\nto cache-resident graph partition. For inter-partition processing, we propose\nyielding and priority-based scheduling, to reduce redundant work in processing.\nBesides, we theoretically prove that ForkGraph performs the same amount of\nwork, to within a constant factor, as the fastest known sequential algorithms\nin FPP queries processing, which is work efficient. Our evaluations on\nreal-world graphs show that ForkGraph significantly outperforms\nstate-of-the-art graph processing systems with two orders of magnitude\nspeedups.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 14:29:04 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 01:05:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lu", "Shengliang", ""], ["Sun", "Shixuan", ""], ["Paul", "Johns", ""], ["Li", "Yuchen", ""], ["He", "Bingsheng", ""]]}, {"id": "2103.15203", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Timothy Davis, Vijay Gadepally, Hayden Jananthan,\n  Lauren Milechin", "title": "Mathematics of Digital Hyperspace", "comments": "9 pages, 8 figures, 2 tables, accepted to GrAPL 2021. arXiv admin\n  note: text overlap with arXiv:1807.03165, arXiv:2004.01181, arXiv:1909.05631,\n  arXiv:1708.02937", "journal-ref": null, "doi": "10.1109/IPDPSW52791.2021.00048", "report-no": null, "categories": "cs.MS cs.DB cs.DM cs.NE math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media, e-commerce, streaming video, e-mail, cloud documents, web\npages, traffic flows, and network packets fill vast digital lakes, rivers, and\noceans that we each navigate daily. This digital hyperspace is an amorphous\nflow of data supported by continuous streams that stretch standard concepts of\ntype and dimension. The unstructured data of digital hyperspace can be\nelegantly represented, traversed, and transformed via the mathematics of\nhypergraphs, hypersparse matrices, and associative array algebra. This paper\nexplores a novel mathematical concept, the semilink, that combines pairs of\nsemirings to provide the essential operations for graph analytics, database\noperations, and machine learning. The GraphBLAS standard currently supports\nhypergraphs, hypersparse matrices, the mathematics required for semilinks, and\nseamlessly performs graph, network, and matrix operations. With the addition of\nkey based indices (such as pointers to strings) and semilinks, GraphBLAS can\nbecome a richer associative array algebra and be a plug-in replacement for\nspreadsheets, database tables, and data centric operating systems, enhancing\nthe navigation of unstructured data found in digital hyperspace.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 19:11:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kepner", "Jeremy", ""], ["Davis", "Timothy", ""], ["Gadepally", "Vijay", ""], ["Jananthan", "Hayden", ""], ["Milechin", "Lauren", ""]]}, {"id": "2103.15509", "submitter": "Alexander L\\\"oser", "authors": "Alexander L\\\"oser", "title": "Automatic Clustering in Hyrise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physical data layout is an important performance factor for modern databases.\nClustering, i.e., storing similar values in proximity, can lead to performance\ngains in several ways. We present an automated model to determine beneficial\nclustering columns and a clustering algorithm for the column-oriented,\nmemory-resident database Hyrise. To automatically select clustering columns,\nthe model analyzes the database's workload and provides estimates by how much\ncertain clustering columns would impact the workload's latency. We evaluate the\nprecision of the model's estimates, as well as the overall quality of its\nclustering suggestions. To apply a determined clustering configuration, we\ndeveloped an online clustering algorithm. The clustering algorithm supports an\narbitrary number of clustering dimensions. We show that the algorithm is robust\nagainst concurrently running data modifying queries. We obtain a 5% latency\nreduction for the TPC-H benchmark when clustering the lineitem table and a 4%\nlatency reduction for the TPC-DS benchmark when clustering the store_sales\ntable.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:29:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["L\u00f6ser", "Alexander", ""]]}, {"id": "2103.15797", "submitter": "Chenjie Li", "authors": "Chenjie Li, Zhengjie Miao, Qitian Zeng, Boris Glavic, Sudeepa Roy", "title": "Putting Things into Context: Rich Explanations for Query Answers using\n  Join Graphs (extended version)", "comments": "technical report(32 pages), main paper to appear in SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many data analysis applications, there is a need to explain why a\nsurprising or interesting result was produced by a query. Previous approaches\nto explaining results have directly or indirectly used data provenance (input\ntuples contributing to the result(s) of interest), which is limited by the fact\nthat relevant information for explaining an answer may not be fully contained\nin the provenance. We propose a new approach for explaining query results by\naugmenting provenance with information from other related tables in the\ndatabase. We develop a suite of optimization techniques, and demonstrate\nexperimentally using real datasets and through a user study that our approach\nproduces meaningful results by efficiently navigating the large search space of\npossible explanations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:48:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Chenjie", ""], ["Miao", "Zhengjie", ""], ["Zeng", "Qitian", ""], ["Glavic", "Boris", ""], ["Roy", "Sudeepa", ""]]}, {"id": "2103.15815", "submitter": "Vladimir Ivanov", "authors": "V. K. Ivanov", "title": "Experimental check of model of object innovation evaluation", "comments": "10 pages, in Russian", "journal-ref": "Bulletin of the Tver State Technical University. Series\n  \"Engineering Sciences\". 2020. N 4 (8)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The article discusses the approach for evaluating the innovation index of the\nproducts and technologies. The evaluation results can be used to create a\nwarehouse of the object descriptions with significant innovation potential. The\nmodel of innovation index computation is based on the concepts of novelty,\nrelevance, and implementability of the object. Formal definitions of these\nindicators are given and a methodology for their calculation are described. The\nfuzzy methods to coprocess (incomplete) data from numerous sources and to\nobtain probabilistic innovation assessments are used. The experimental data of\nthe model check including the calculations of local criteria and global\nadditive evaluation criterion are presented. The cyclical nature of dynamic\nchanges in indicators, their interdependence was established, some general\nfeatures of the products promotion were found. The obtained experimental data\nare consistent with expert estimates of the products under study. The analysis\nof the local criteria used in the research gives grounds to assert the correct\nuse of the additive n-dimensional utility function. The adequacy of assumptions\nand formal expressions that are used in computational algorithms for selection\ninformation for data warehouse is confirmed.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 07:29:09 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ivanov", "V. K.", ""]]}, {"id": "2103.15816", "submitter": "Vladimir Ivanov", "authors": "V. K. Ivanov", "title": "Some Results of Experimental Check of The Model of the Object\n  Innovativeness Quantitative Evaluation", "comments": "10 pages, in Russian", "journal-ref": "Information and Innovations. 2020. Vol. 15, N 3", "doi": "10.31432/1994-2443-2020-15-3-27-36", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The paper presents the results of the experiments that were conducted to\nconfirm the main ideas of the proposed approach to determining the objects\ninnovativeness. This approach assumed that the product life cycle of whose\ndescriptions are placed in different data warehouses is adequate. The proposed\nformal model allows us to calculate the quantitative value of the additive\nevaluation criterion of objects innovativeness. The obtained experimental data\nmake it possible to evaluate the adopted approach correctness.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 08:26:45 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ivanov", "V. K.", ""]]}, {"id": "2103.15942", "submitter": "Chenghong Wang", "authors": "Chenghong Wang, Johes Bater, Kartik Nayak, Ashwin Machanavajjhala", "title": "DP-Sync: Hiding Update Patterns in Secure Outsourced Databases with\n  Differential Privacy", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3457306", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we have introduced a new type of leakage associated with\nmodern encrypted databases called update pattern leakage. We formalize the\ndefinition and security model of DP-Sync with DP update patterns. We also\nproposed the framework DP-Sync, which extends existing encrypted database\nschemes to DP-Sync with DP update patterns. DP-Sync guarantees that the entire\ndata update history over the outsourced data structure is protected by\ndifferential privacy. This is achieved by imposing differentially-private\nstrategies that dictate the data owner's synchronization of local~data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:36:59 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 01:56:15 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 02:45:27 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wang", "Chenghong", ""], ["Bater", "Johes", ""], ["Nayak", "Kartik", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "2103.15994", "submitter": "Xi Liang", "authors": "Xi Liang, Stavros Sintos, Zechao Shang, Sanjay Krishnan", "title": "Combining Aggregation and Sampling (Nearly) Optimally for Approximate\n  Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample-based approximate query processing (AQP) suffers from many pitfalls\nsuch as the inability to answer very selective queries and unreliable\nconfidence intervals when sample sizes are small. Recent research presented an\nintriguing solution of combining materialized, pre-computed aggregates with\nsampling for accurate and more reliable AQP. We explore this solution in detail\nin this work and propose an AQP physical design called PASS, or\nPrecomputation-Assisted Stratified Sampling. PASS builds a tree of partial\naggregates that cover different partitions of the dataset. The leaf nodes of\nthis tree form the strata for stratified samples. Aggregate queries whose\npredicates align with the partitions (or unions of partitions) are exactly\nanswered with a depth-first search, and any partial overlaps are approximated\nwith the stratified samples. We propose an algorithm for optimally partitioning\nthe data into such a data structure with various practical approximation\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 23:55:37 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liang", "Xi", ""], ["Sintos", "Stavros", ""], ["Shang", "Zechao", ""], ["Krishnan", "Sanjay", ""]]}, {"id": "2103.16007", "submitter": "Doris Xin", "authors": "Doris Xin, Hui Miao, Aditya Parameswaran, Neoklis Polyzotis", "title": "Production Machine Learning Pipelines: Empirical Analysis and\n  Optimization Opportunities", "comments": null, "journal-ref": "Proceedings of the 2021 International Conference on Management of\n  Data", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) is now commonplace, powering data-driven applications\nin various organizations. Unlike the traditional perception of ML in research,\nML production pipelines are complex, with many interlocking analytical\ncomponents beyond training, whose sub-parts are often run multiple times on\noverlapping subsets of data. However, there is a lack of quantitative evidence\nregarding the lifespan, architecture, frequency, and complexity of these\npipelines to understand how data management research can be used to make them\nmore efficient, effective, robust, and reproducible. To that end, we analyze\nthe provenance graphs of 3000 production ML pipelines at Google, comprising\nover 450,000 models trained, spanning a period of over four months, in an\neffort to understand the complexity and challenges underlying production ML.\nOur analysis reveals the characteristics, components, and topologies of typical\nindustry-strength ML pipelines at various granularities. Along the way, we\nintroduce a specialized data model for representing and reasoning about\nrepeatedly run components in these ML pipelines, which we call model graphlets.\nWe identify several rich opportunities for optimization, leveraging traditional\ndata management ideas. We show how targeting even one of these opportunities,\ni.e., identifying and pruning wasted computation that does not translate to\nmodel deployment, can reduce wasted computation cost by 50% without\ncompromising the model deployment cadence.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:46:29 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Xin", "Doris", ""], ["Miao", "Hui", ""], ["Parameswaran", "Aditya", ""], ["Polyzotis", "Neoklis", ""]]}, {"id": "2103.16037", "submitter": "Zi Chen", "authors": "Zi Chen, Long Yuan, Xuemin Lin, Zhengping Qian, Lu Qin", "title": "Higher-Order Neighborhood Truss Decomposition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  $k$-truss model is a typical cohesive subgraph model and has been received\nconsiderable attention. However, the $k$-truss model lacks the ability to\nreveal fine-grained structure information of the graph. Motivated by this, in\nthis paper, we propose a new model named ($k$,$\\tau$)-truss that considers the\nhigher-order neighborhood ($\\tau$ hop) information of an edge. Based on the\n($k$,$\\tau$)-truss model, we study the higher-order neighborhood truss\ndecomposition problem which computes the ($k$,$\\tau$)-trusses for all possible\n$k$ values regarding a given $\\tau$. To address this problem, we first propose\na bottom-up decomposition algorithm to compute the ($k$,$\\tau$)-truss in the\nincreasing order of $k$ values. Based on the algorithm, we devise three\noptimization strategies to further improve the decomposition efficiency. We\nevaluate our proposed model and algorithms on real datasets, and the\nexperimental results demonstrate the effectiveness of the ($k$,$\\tau$)-truss\nmodel and the efficiency of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 02:43:51 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Zi", ""], ["Yuan", "Long", ""], ["Lin", "Xuemin", ""], ["Qian", "Zhengping", ""], ["Qin", "Lu", ""]]}, {"id": "2103.16061", "submitter": "Qifan Chen", "authors": "Qifan Chen, Yang Lu, Charmaine Tam, Simon Poon", "title": "A Novel Approach to Detect Redundant Activity Labels For More\n  Representative Event Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The insights revealed from process mining heavily rely on the quality of\nevent logs. Activities extracted from healthcare information systems with the\nfree-text nature may lead to inconsistent labels. Such inconsistency would then\nlead to redundancy of activity labels, which refer to labels that have\ndifferent syntax but share the same behaviours. The identifications of these\nlabels from data-driven process discovery are difficult and rely heavily on\nresource-intensive human review. Existing work achieves low accuracy either\nredundant activity labels are in low occurrence frequency or the existence of\nnumerical data values as attributes in event logs. However, these phenomena are\ncommonly observed in healthcare information systems. In this paper, we propose\nan approach to detect redundant activity labels using control-flow relations\nand numerical data values from event logs. Natural Language Processing is also\nintegrated into our method to assess semantic similarity between labels, which\nprovides users with additional insights. We have evaluated our approach through\nsynthetic logs generated from the real-life Sepsis log and a case study using\nthe MIMIC-III data set. The results demonstrate that our approach can\nsuccessfully detect redundant activity labels. This approach can add value to\nthe preprocessing step to generate more representative event logs for process\nmining tasks in the healthcare domain.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 04:18:39 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 03:43:29 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chen", "Qifan", ""], ["Lu", "Yang", ""], ["Tam", "Charmaine", ""], ["Poon", "Simon", ""]]}, {"id": "2103.16084", "submitter": "Sheng Wang", "authors": "Sheng Wang, Yuan Sun, Christopher Musco, Zhifeng Bao", "title": "Public Transport Planning: When Transit Network Connectivity Meets\n  Commuting Demand", "comments": "SIGMOD 2021, 14 pages", "journal-ref": null, "doi": "10.1145/3448016.3457247", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we make a first attempt to incorporate both commuting demand\nand transit network connectivity in bus route planning (CT-Bus), and formulate\nit as a constrained optimization problem: planning a new bus route with k edges\nover an existing transit network without building new bus stops to maximize a\nlinear aggregation of commuting demand and connectivity of the transit network.\nWe prove the NP-hardness of CT-Bus and propose an expansion-based greedy\nalgorithm that iteratively scans potential candidate paths in the network. To\nboost the efficiency of computing the connectivity of new networks with\ncandidate paths, we convert it to a matrix trace estimation problem and employ\na Lanczos method to estimate the natural connectivity of the transit network\nwith a guaranteed error bound. Furthermore, we derive upper bounds on the\nobjective values and use them to greedily select candidates for expansion. Our\nexperiments conducted on real-world transit networks in New York City and\nChicago verify the efficiency, effectiveness, and scalability of our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:32:24 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 13:11:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Sheng", ""], ["Sun", "Yuan", ""], ["Musco", "Christopher", ""], ["Bao", "Zhifeng", ""]]}, {"id": "2103.16196", "submitter": "Can Cui Mr", "authors": "Can Cui, Wei Wang, Meihui Zhang, Gang Chen, Zhaojing Luo, Beng Chin\n  Ooi", "title": "AlphaEvolve: A Learning Framework to Discover Novel Alphas in\n  Quantitative Investment", "comments": "Accepted by SIGMOD 2021 Data Science and Engineering Track", "journal-ref": null, "doi": "10.1145/3448016.3457324", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alphas are stock prediction models capturing trading signals in a stock\nmarket. A set of effective alphas can generate weakly correlated high returns\nto diversify the risk. Existing alphas can be categorized into two classes:\nFormulaic alphas are simple algebraic expressions of scalar features, and thus\ncan generalize well and be mined into a weakly correlated set. Machine learning\nalphas are data-driven models over vector and matrix features. They are more\npredictive than formulaic alphas, but are too complex to mine into a weakly\ncorrelated set. In this paper, we introduce a new class of alphas to model\nscalar, vector, and matrix features which possess the strengths of these two\nexisting classes. The new alphas predict returns with high accuracy and can be\nmined into a weakly correlated set. In addition, we propose a novel alpha\nmining framework based on AutoML, called AlphaEvolve, to generate the new\nalphas. To this end, we first propose operators for generating the new alphas\nand selectively injecting relational domain knowledge to model the relations\nbetween stocks. We then accelerate the alpha mining by proposing a pruning\ntechnique for redundant alphas. Experiments show that AlphaEvolve can evolve\ninitial alphas into the new alphas with high returns and weak correlations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:28:41 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 10:35:19 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Cui", "Can", ""], ["Wang", "Wei", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Luo", "Zhaojing", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2103.16267", "submitter": "Sami Alabed", "authors": "Sami Alabed, Eiko Yoneki", "title": "High-Dimensional Bayesian Optimization with Multi-Task Learning for\n  RocksDB", "comments": null, "journal-ref": "{The 1st Workshop on Machine Learning and Systems (EuroMLSys'21),\n  April 26, 2021, Online, United Kingdom}", "doi": "10.1145/3437984.3458841", "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RocksDB is a general-purpose embedded key-value store used in multiple\ndifferent settings. Its versatility comes at the cost of complex tuning\nconfigurations. This paper investigates maximizing the throughput of RocksDB IO\noperations by auto-tuning ten parameters of varying ranges. Off-the-shelf\noptimizers struggle with high-dimensional problem spaces and require a large\nnumber of training samples. We propose two techniques to tackle this problem:\nmulti-task modeling and dimensionality reduction through a manual grouping of\nparameters. By incorporating adjacent optimization in the model, the model\nconverged faster and found complicated settings that other tuners could not\nfind. This approach had an additional computational complexity overhead, which\nwe mitigated by manually assigning parameters to each sub-goal through our\nknowledge of RocksDB. The model is then incorporated in a standard Bayesian\nOptimization loop to find parameters that maximize RocksDB's IO throughput. Our\nmethod achieved x1.3 improvement when benchmarked against a simulation of\nFacebook's social graph traffic, and converged in ten optimization steps\ncompared to other state-of-the-art methods that required fifty steps.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 11:38:52 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 20:53:12 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Alabed", "Sami", ""], ["Yoneki", "Eiko", ""]]}, {"id": "2103.16519", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Zilin Du, Weiping Ding, Chunkai Zhang, and Han-Chieh\n  Chao", "title": "Explainable Fuzzy Utility Mining on Sequences", "comments": "Preprint. 13 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy systems have good modeling capabilities in several data science\nscenarios, and can provide human-explainable intelligence models with\nexplainability and interpretability. In contrast to transaction data, which\nhave been extensively studied, sequence data are more common in real-life\napplications. To obtain a human-explainable data intelligence model for\ndecision making, in this study, we investigate explainable fuzzy-theoretic\nutility mining on multi-sequences. Meanwhile, a more normative formulation of\nthe problem of fuzzy utility mining on sequences is formulated. By exploring\nfuzzy set theory for utility mining, we propose a novel method termed pattern\ngrowth fuzzy utility mining (PGFUM) for mining fuzzy high-utility sequences\nwith linguistic meaning. In the case of sequence data, PGFUM reflects the fuzzy\nquantity and utility regions of sequences. To improve the efficiency and\nfeasibility of PGFUM, we develop two compressed data structures with\nexplainable fuzziness. Furthermore, one existing and two new upper bounds on\nthe explainable fuzzy utility of candidates are adopted in three proposed\npruning strategies to substantially reduce the search space and thus expedite\nthe mining process. Finally, the proposed PGFUM algorithm is compared with\nPFUS, which is the only currently available method for the same task, through\nextensive experimental evaluation. It is demonstrated that PGFUM achieves not\nonly human-explainable mining results that contain the original nature of\nrevealable intelligibility, but also high efficiency in terms of runtime and\nmemory cost.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:23:23 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Gan", "Wensheng", ""], ["Du", "Zilin", ""], ["Ding", "Weiping", ""], ["Zhang", "Chunkai", ""], ["Chao", "Han-Chieh", ""]]}, {"id": "2103.16604", "submitter": "Brandon Haynes", "authors": "Brandon Haynes, Maureen Daum, Dong He, Amrita Mazumdar, Magdalena\n  Balazinska, Alvin Cheung, Luis Ceze", "title": "VSS: A Storage System for Video Analytics [Technical Report]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new video storage system (VSS) designed to decouple high-level\nvideo operations from the low-level details required to store and efficiently\nretrieve video data. VSS is designed to be the storage subsystem of a video\ndata management system (VDBMS) and is responsible for: (1) transparently and\nautomatically arranging the data on disk in an efficient, granular format; (2)\ncaching frequently-retrieved regions in the most useful formats; and (3)\neliminating redundancies found in videos captured from multiple cameras with\noverlapping fields of view. Our results suggest that VSS can improve VDBMS read\nperformance by up to 54%, reduce storage costs by up to 45%, and enable\ndevelopers to focus on application logic rather than video storage and\nretrieval.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:21:19 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Haynes", "Brandon", ""], ["Daum", "Maureen", ""], ["He", "Dong", ""], ["Mazumdar", "Amrita", ""], ["Balazinska", "Magdalena", ""], ["Cheung", "Alvin", ""], ["Ceze", "Luis", ""]]}, {"id": "2103.16615", "submitter": "Wensheng Gan", "authors": "Chunkai Zhang, Zilin Du, Quanjian Dai, Wensheng Gan, Jian Weng, and\n  Philip S. Yu", "title": "TUSQ: Targeted High-Utility Sequence Querying", "comments": "Preprint. 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant efforts have been expended in the research and development of a\ndatabase management system (DBMS) that has a wide range of applications for\nmanaging an enormous collection of multisource, heterogeneous, complex, or\ngrowing data. Besides the primary function (i.e., create, delete, and update),\na practical and impeccable DBMS can interact with users through information\nselection, that is, querying with their targets. Previous querying algorithms,\nsuch as frequent itemset querying and sequential pattern querying (SPQ) have\nfocused on the measurement of frequency, which does not involve the concept of\nutility, which is helpful for users to discover more informative patterns. To\napply the querying technology for wider applications, we incorporate utility\ninto target-oriented SPQ and formulate the task of targeted utility-oriented\nsequence querying. To address the proposed problem, we develop a novel\nalgorithm, namely targeted high-utility sequence querying (TUSQ), based on two\nnovel upper bounds suffix remain utility and terminated descendants utility as\nwell as a vertical Last Instance Table structure. For further efficiency, TUSQ\nrelies on a projection technology utilizing a compact data structure called the\ntargeted chain. An extensive experimental study conducted on several real and\nsynthetic datasets shows that the proposed algorithm outperformed the designed\nbaseline algorithm in terms of runtime, memory consumption, and candidate\nfiltering.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:39:09 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhang", "Chunkai", ""], ["Du", "Zilin", ""], ["Dai", "Quanjian", ""], ["Gan", "Wensheng", ""], ["Weng", "Jian", ""], ["Yu", "Philip S.", ""]]}, {"id": "2103.16640", "submitter": "Samuel Maddock", "authors": "Graham Cormode, Samuel Maddock, Carsten Maple", "title": "Frequency Estimation under Local Differential Privacy [Experiments,\n  Analysis and Benchmarks]", "comments": "13 pages; Updated figures (7,8,10) and minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Private collection of statistics from a large distributed population is an\nimportant problem, and has led to large scale deployments from several leading\ntechnology companies. The dominant approach requires each user to randomly\nperturb their input, leading to guarantees in the local differential privacy\nmodel. In this paper, we place the various approaches that have been suggested\ninto a common framework, and perform an extensive series of experiments to\nunderstand the tradeoffs between different implementation choices. Our\nconclusion is that for the core problems of frequency estimation and heavy\nhitter identification, careful choice of algorithms can lead to very effective\nsolutions that scale to millions of users\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:23:20 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 16:09:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Cormode", "Graham", ""], ["Maddock", "Samuel", ""], ["Maple", "Carsten", ""]]}, {"id": "2103.17128", "submitter": "Ali Hamdi", "authors": "Ali Hamdi, Khaled Shaban, Abdelkarim Erradi, Amr Mohamed, Shakila Khan\n  Rumi, Flora Salim", "title": "Spatiotemporal Data Mining: A Survey on Challenges and Open Problems", "comments": "Accepted for publication at Artificial Intelligence Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Spatiotemporal data mining (STDM) discovers useful patterns from the dynamic\ninterplay between space and time. Several available surveys capture STDM\nadvances and report a wealth of important progress in this field. However, STDM\nchallenges and problems are not thoroughly discussed and presented in articles\nof their own. We attempt to fill this gap by providing a comprehensive\nliterature survey on state-of-the-art advances in STDM. We describe the\nchallenging issues and their causes and open gaps of multiple STDM directions\nand aspects. Specifically, we investigate the challenging issues in regards to\nspatiotemporal relationships, interdisciplinarity, discretisation, and data\ncharacteristics. Moreover, we discuss the limitations in the literature and\nopen research problems related to spatiotemporal data representations,\nmodelling and visualisation, and comprehensiveness of approaches. We explain\nissues related to STDM tasks of classification, clustering, hotspot detection,\nassociation and pattern mining, outlier detection, visualisation, visual\nanalytics, and computer vision tasks. We also highlight STDM issues related to\nmultiple applications including crime and public safety, traffic and\ntransportation, earth and environment monitoring, epidemiology, social media,\nand Internet of Things.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:48:39 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hamdi", "Ali", ""], ["Shaban", "Khaled", ""], ["Erradi", "Abdelkarim", ""], ["Mohamed", "Amr", ""], ["Rumi", "Shakila Khan", ""], ["Salim", "Flora", ""]]}, {"id": "2103.17178", "submitter": "Pawe{\\l} Guzewicz", "authors": "Yanlei Diao, Pawe{\\l} Guzewicz, Ioana Manolescu, Mirjana Mazuran", "title": "Efficient Exploration of Interesting Aggregates in RDF Graphs", "comments": "Accepted for publication in proceedings of the 2021 International\n  Conference on Management of Data (SIGMOD '21), June 20--25, 2021, Virtual\n  Event, China", "journal-ref": null, "doi": "10.1145/3448016.3457307", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As large Open Data are increasingly shared as RDF graphs today, there is a\ngrowing demand to help users discover the most interesting facets of a graph,\nwhich are often hard to grasp without automatic tools. We consider the problem\nof automatically identifying the k most interesting aggregate queries that can\nbe evaluated on an RDF graph, given an integer k and a user-specified\ninterestingness function. Our problem departs from analytics in relational data\nwarehouses in that (i) in an RDF graph we are not given but we must identify\nthe facts, dimensions, and measures of candidate aggregates; (ii) the classical\napproach to efficiently evaluating multiple aggregates breaks in the face of\nmulti-valued dimensions in RDF data. In this work, we propose an extensible\nend-to-end framework that enables the identification and evaluation of\ninteresting aggregates based on a new RDF-compatible one-pass algorithm for\nefficiently evaluating a lattice of aggregates and a novel early-stop technique\n(with probabilistic guarantees) that can prune uninteresting aggregates.\nExperiments using both real and synthetic graphs demonstrate the ability of our\nframework to find interesting aggregates in a large search space, the\nefficiency of our algorithms (with up to 2.9x speedup over a similar pipeline\nbased on existing algorithms), and scalability as the data size and complexity\ngrow.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:52:18 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Diao", "Yanlei", ""], ["Guzewicz", "Pawe\u0142", ""], ["Manolescu", "Ioana", ""], ["Mazuran", "Mirjana", ""]]}]