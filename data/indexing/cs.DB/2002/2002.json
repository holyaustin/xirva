[{"id": "2002.00084", "submitter": "Seokki Lee", "authors": "Seokki Lee, Bertram Ludaescher, Boris Glavic", "title": "Approximate Summaries for Why and Why-not Provenance (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why and why-not provenance have been studied extensively in recent years.\nHowever, why-not provenance, and to a lesser degree why provenance, can be very\nlarge resulting in severe scalability and usability challenges. In this paper,\nwe introduce a novel approximate summarization technique for provenance which\novercomes these challenges. Our approach uses patterns to encode (why-not)\nprovenance concisely. We develop techniques for efficiently computing\nprovenance summaries balancing informativeness, conciseness, and completeness.\nTo achieve scalability, we integrate sampling techniques into provenance\ncapture and summarization. Our approach is the first to scale to large datasets\nand to generate comprehensive and meaningful summaries.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 22:47:43 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 16:29:09 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lee", "Seokki", ""], ["Ludaescher", "Bertram", ""], ["Glavic", "Boris", ""]]}, {"id": "2002.00160", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Sajjad Rahnama, Jelle Hellings, Mohammad Sadoghi", "title": "ResilientDB: Global Scale Resilient Blockchain Fabric", "comments": null, "journal-ref": "PVLDB 13 (2020) 868-883", "doi": "10.14778/3380750.3380757", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in blockchain technology have inspired innovative new\ndesigns in resilient distributed and database systems. At their core, these\nblockchain applications typically use Byzantine fault-tolerant consensus\nprotocols to maintain a common state across all replicas, even if some replicas\nare faulty or malicious. Unfortunately, existing consensus protocols are not\ndesigned to deal with geo-scale deployments in which many replicas spread\nacross a geographically large area participate in consensus. To address this,\nwe present the Geo-Scale Byzantine FaultTolerant consensus protocol (GeoBFT).\nGeoBFT is designed for excellent scalability by using a topological-aware\ngrouping of replicas in local clusters, by introducing parallelization of\nconsensus at the local level, and by minimizing communication between clusters.\nTo validate our vision of high-performance geo-scale resilient distributed\nsystems, we implement GeoBFT in our efficient ResilientDB permissioned\nblockchain fabric. We show that GeoBFT is not only sound and provides great\nscalability, but also outperforms state-of-the-art consensus protocols by a\nfactor of six in geo-scale deployments.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 07:20:23 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 20:27:56 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Gupta", "Suyash", ""], ["Rahnama", "Sajjad", ""], ["Hellings", "Jelle", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "2002.00215", "submitter": "Sergio Esteves", "authors": "Sergio Esteves, Joao Nuno Silva, and Luis Veiga", "title": "Palpatine: Mining Frequent Sequences for Data Prefetching in NoSQL\n  Distributed Key-Value Stores", "comments": "14 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents PALPATINE, the first in-memory application-level cache\nfor Distributed Key-Value (DKV) data stores, capable of prefetching data that\nis likely to be accessed in an immediate future. To predict data accesses,\nPALPATINE continuously captures frequent access patterns to the back store by\nmeans of data mining techniques. With these patterns, PALPATINE builds a\nstochastic graph of accessed items, and makes prefetching decisions based on\nit.\n  Experimental evaluation indicates that PALPATINE can improve the latency of a\nspecific DKV store by more that an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 14:19:27 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 20:10:22 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Esteves", "Sergio", ""], ["Silva", "Joao Nuno", ""], ["Veiga", "Luis", ""]]}, {"id": "2002.00540", "submitter": "Albert Kim", "authors": "Albert Kim, Atalay Mert Ileri, Sam Madden", "title": "Optimizing Query Predicates with Disjunctions for Column Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its inception, database research has given limited attention to\noptimizing predicates with disjunctions. What little past work there is has\nfocused on optimizations for traditional row-oriented databases. A key\ndifference in predicate evaluation for row stores and column stores is that\nwhile row stores apply predicates to one record at a time, column stores apply\npredicates to sets of records. Not only must the execution engine decide the\norder in which to apply the predicates, but it must also decide how many times\neach predicate should be applied and on which sets of records it should be\napplied to. In our work, we tackle exactly this problem. We formulate, analyze,\nand solve the predicate evaluation problem for column stores. Our results\ninclude proofs about various properties of the problem, and in turn, these\nproperties have allowed us to derive the first polynomial-time (i.e., O(n log\nn)) algorithm ShallowFish which evaluates predicates optimally for all\npredicate expressions with a depth of 2 or less. We capture the exact property\nwhich makes the problem more difficult for predicate expressions of depth 3 or\ngreater and propose an approximate algorithm DeepFish which outperforms\nShallowFish in these situations. Finally, we show that both ShallowFish and\nDeepFish outperform the corresponding state of the art by two orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 02:35:50 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Kim", "Albert", ""], ["Ileri", "Atalay Mert", ""], ["Madden", "Sam", ""]]}, {"id": "2002.00655", "submitter": "Alexander Renz-Wieland", "authors": "Alexander Renz-Wieland, Rainer Gemulla, Steffen Zeuch, Volker Markl", "title": "Dynamic Parameter Allocation in Parameter Servers", "comments": null, "journal-ref": "PVLDB, 13(11): 1877-1890, 2020", "doi": "10.14778/3407790.3407796", "report-no": null, "categories": "cs.LG cs.DB cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To keep up with increasing dataset sizes and model complexity, distributed\ntraining has become a necessity for large machine learning tasks. Parameter\nservers ease the implementation of distributed parameter management---a key\nconcern in distributed training---, but can induce severe communication\noverhead. To reduce communication overhead, distributed machine learning\nalgorithms use techniques to increase parameter access locality (PAL),\nachieving up to linear speed-ups. We found that existing parameter servers\nprovide only limited support for PAL techniques, however, and therefore prevent\nefficient training. In this paper, we explore whether and to what extent PAL\ntechniques can be supported, and whether such support is beneficial. We propose\nto integrate dynamic parameter allocation into parameter servers, describe an\nefficient implementation of such a parameter server called Lapse, and\nexperimentally compare its performance to existing parameter servers across a\nnumber of machine learning tasks. We found that Lapse provides near-linear\nscaling and can be orders of magnitude faster than existing parameter servers.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 11:37:54 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 14:05:58 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 12:52:13 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Renz-Wieland", "Alexander", ""], ["Gemulla", "Rainer", ""], ["Zeuch", "Steffen", ""], ["Markl", "Volker", ""]]}, {"id": "2002.00819", "submitter": "Donatella Firmani", "authors": "Andrea Rossi, Donatella Firmani, Antonio Matinata, Paolo Merialdo,\n  Denilson Barbosa", "title": "Knowledge Graph Embedding for Link Prediction: A Comparative Analysis", "comments": "Andrea Rossi, Donatella Firmani, Antonio Matinata, Paolo Merialdo,\n  Denilson Barbosa. 2020. Knowledge Graph Embedding for Link Prediction: A\n  Comparative Analysis. In ACM Transactions on Knowledge Discovery from Data.\n  January 2021. (TKDD 2021). ACM, New York, NY, USA", "journal-ref": null, "doi": "10.1145/3424672", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs (KGs) have found many applications in industry and academic\nsettings, which in turn, have motivated considerable research efforts towards\nlarge-scale information extraction from a variety of sources. Despite such\nefforts, it is well known that even state-of-the-art KGs suffer from\nincompleteness. Link Prediction (LP), the task of predicting missing facts\namong entities already a KG, is a promising and widely studied task aimed at\naddressing KG incompleteness. Among the recent LP techniques, those based on KG\nembeddings have achieved very promising performances in some benchmarks.\nDespite the fast growing literature in the subject, insufficient attention has\nbeen paid to the effect of the various design choices in those methods.\nMoreover, the standard practice in this area is to report accuracy by\naggregating over a large number of test facts in which some entities are\nover-represented; this allows LP methods to exhibit good performance by just\nattending to structural properties that include such entities, while ignoring\nthe remaining majority of the KG. This analysis provides a comprehensive\ncomparison of embedding-based LP methods, extending the dimensions of analysis\nbeyond what is commonly available in the literature. We experimentally compare\neffectiveness and efficiency of 16 state-of-the-art methods, consider a\nrule-based baseline, and report detailed analysis over the most popular\nbenchmarks in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 15:21:25 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 09:32:38 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 22:13:48 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 21:15:36 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Rossi", "Andrea", ""], ["Firmani", "Donatella", ""], ["Matinata", "Antonio", ""], ["Merialdo", "Paolo", ""], ["Barbosa", "Denilson", ""]]}, {"id": "2002.00866", "submitter": "Harshad Deshmukh", "authors": "Harshad Deshmukh, Bruhathi Sundarmurthy, Jignesh M. Patel", "title": "To pipeline or not to pipeline, that is the question", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In designing query processing primitives, a crucial design choice is the\nmethod for data transfer between two operators in a query plan. As we were\nconsidering this critical design mechanism for an in-memory database system\nthat we are building, we quickly realized that (surprisingly) there isn't a\nclear definition of this concept. Papers are full or ad hoc use of terms like\npipelining and blocking, but as these terms are not crisply defined, it is hard\nto fully understand the results attributed to these concepts. To address this\nlimitation, we introduce a clear terminology for how to think about data\ntransfer between operators in a query pipeline. We show that there isn't a\nclear definition of pipelining and blocking, and that there is a full spectrum\nof techniques based on a simple concept called unit-of-transfer. Next, we\ndevelop an analytical model for inter-operator communication, and highlight the\nkey parameters that impact performance (for in-memory database settings). Armed\nwith this model, we then apply it to the system we are designing and highlight\nthe insights we gathered from this exercise. We find that the gap between\npipelining and non-pipelining query execution, w.r.t. key factors such as\nperformance and memory footprint is quite narrow, and thus system designers\nshould likely rethink the notion of pipelining vs. blocking for in-memory\ndatabase systems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 16:22:17 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Deshmukh", "Harshad", ""], ["Sundarmurthy", "Bruhathi", ""], ["Patel", "Jignesh M.", ""]]}, {"id": "2002.01047", "submitter": "Raul Castro Fernandez", "authors": "Raul Castro Fernandez, Pranav Subramaniam, Michael J. Franklin", "title": "Data Market Platforms: Trading Data Assets to Solve Data Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data only generates value for a few organizations with expertise and\nresources to make data shareable, discoverable, and easy to integrate. Sharing\ndata that is easy to discover and integrate is hard because data owners lack\ninformation (who needs what data) and they do not have incentives to prepare\nthe data in a way that is easy to consume by others.\n  In this paper, we propose data market platforms to address the lack of\ninformation and incentives and tackle the problems of data sharing, discovery,\nand integration. In a data market platform, data owners want to share data\nbecause they will be rewarded if they do so. Consumers are encouraged to share\ntheir data needs because the market will solve the discovery and integration\nproblem for them in exchange for some form of currency.\n  We consider internal markets that operate within organizations to bring down\ndata silos, as well as external markets that operate across organizations to\nincrease the value of data for everybody. We outline a research agenda that\nrevolves around two problems. The problem of market design, or how to design\nrules that lead to the outcomes we want, and the systems problem, how to\nimplement the market and enforce the rules. Treating data as a first-class\nasset is sorely needed to extend the value of data to more organizations, and\nwe propose data market platforms as one mechanism to achieve this goal.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 23:15:29 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 21:02:32 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Fernandez", "Raul Castro", ""], ["Subramaniam", "Pranav", ""], ["Franklin", "Michael J.", ""]]}, {"id": "2002.01164", "submitter": "Ali Cakmak", "authors": "Mehmet Aytimur and Ali Cakmak", "title": "Using Positional Sequence Patterns to Estimate the Selectivity of SQL\n  LIKE Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the dramatic increase in the amount of the text-based data which\ncommonly contains misspellings and other errors, querying such data with\nflexible search patterns becomes more and more commonplace. Relational\ndatabases support the LIKE operator to allow searching with a particular\nwildcard predicate (e.g., LIKE 'Sub%', which matches all strings starting with\n'Sub'). Due to the large size of text data, executing such queries in the most\noptimal way is quite critical for database performance. While building the most\nefficient execution plan for a LIKE query, the query optimizer requires the\nselectivity estimate for the flexible pattern-based query predicate. Recently,\nSPH algorithm is proposed which employs a sequence pattern-based histogram\nstructure to estimate the selectivity of LIKE queries. A drawback of the SPH\napproach is that it often overestimates the selectivity of queries. In order to\nalleviate the overestimation problem, in this paper, we propose a novel\nsequence pattern type, called positional sequence patterns. The proposed\npatterns differentiate between sequence item pairs that appear next to each\nother in all pattern occurrences from those that may have other items between\nthem. Besides, we employ redundant pattern elimination based on pattern\ninformation content during histogram construction. Finally, we propose a\npartitioning-based matching scheme during the selectivity estimation. The\nexperimental results on a real dataset from DBLP show that the proposed\napproach outperforms the state of the art by around 20% improvement in error\nrates.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 08:02:15 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Aytimur", "Mehmet", ""], ["Cakmak", "Ali", ""]]}, {"id": "2002.01501", "submitter": "Milan Lopuha\\\"a-Zwakenberg", "authors": "Milan Lopuha\\\"a-Zwakenberg", "title": "The Privacy Funnel from the viewpoint of Local Differential Privacy", "comments": "To appear at: ICDS 2020, PPODS track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a database $\\vec{X} = (X_1,\\cdots,X_n)$ containing the data of\n$n$ users. The data aggregator wants to publicise the database, but wishes to\nsanitise the dataset to hide sensitive data $S_i$ correlated to $X_i$. This\nsetting is considered in the Privacy Funnel, which uses mutual information as a\nleakage metric. The downsides to this approach are that mutual information does\nnot give worst-case guarantees, and that finding optimal sanitisation protocols\ncan be computationally prohibitive. We tackle these problems by using\ndifferential privacy metrics, and by considering local protocols which operate\non one entry at a time. We show that under both the Local Differential Privacy\nand Local Information Privacy leakage metrics, one can efficiently obtain\noptimal protocols; however, Local Information Privacy is both more closely\naligned to the privacy requirements of the Privacy Funnel scenario, and more\nefficiently computable. We also consider the scenario where each user has\nmultiple attributes (i.e. $X_i = (X^1_i,\\cdots,X^m_i)$), for which we define\n\\emph{Side-channel Resistant Local Information Privacy}, and we give efficient\nmethods to find protocols satisfying this criterion while still offering good\nutility. Exploratory experiments confirm the validity of these methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 19:16:00 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 21:03:10 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Lopuha\u00e4-Zwakenberg", "Milan", ""]]}, {"id": "2002.01531", "submitter": "Harshad Deshmukh", "authors": "Bruhathi Sundarmurthy, Harshad Deshmukh, Paris Koutris, Jeffrey\n  Naughton", "title": "Providing Insights for Queries affected by Failures and Stragglers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive time responses are a crucial requirement for users analyzing\nlarge amounts of data. Such analytical queries are typically run in a\ndistributed setting, with data being sharded across thousands of nodes for high\nthroughput. However, providing real-time analytics is still a very big\nchallenge; with data distributed across thousands of nodes, the probability\nthat some of the required nodes are unavailable or very slow during query\nexecution is very high and unavailability may result in slow execution or even\nfailures. The sheer magnitude of data and users increase resource contention\nand this exacerbates the phenomenon of stragglers and node failures during\nexecution. In this paper, we propose a novel solution to alleviate the\nstraggler/failure problem that exploits existing efficient partitioning\nproperties of the data, particularly, co-hash partitioned data, and provides\napproximate answers along with confidence bounds to queries affected by\nfailed/straggler nodes. We consider aggregate queries that involve joins, group\nbys, having clauses and a subclass of nested subqueries. Finally, we validate\nour approach through extensive experiments on the TPC-H dataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 20:52:55 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Sundarmurthy", "Bruhathi", ""], ["Deshmukh", "Harshad", ""], ["Koutris", "Paris", ""], ["Naughton", "Jeffrey", ""]]}, {"id": "2002.01582", "submitter": "Ryan McKenna", "authors": "Ryan McKenna, Raj Kumar Maity, Arya Mazumdar, Gerome Miklau", "title": "A workload-adaptive mechanism for linear queries under local\n  differential privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new mechanism to accurately answer a user-provided set of linear\ncounting queries under local differential privacy (LDP). Given a set of linear\ncounting queries (the workload) our mechanism automatically adapts to provide\naccuracy on the workload queries. We define a parametric class of mechanisms\nthat produce unbiased estimates of the workload, and formulate a constrained\noptimization problem to select a mechanism from this class that minimizes\nexpected total squared error. We solve this optimization problem numerically\nusing projected gradient descent and provide an efficient implementation that\nscales to large workloads. We demonstrate the effectiveness of our\noptimization-based approach in a wide variety of settings, showing that it\noutperforms many competitors, even outperforming existing mechanisms on the\nworkloads for which they were intended.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 00:10:54 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 16:32:57 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["McKenna", "Ryan", ""], ["Maity", "Raj Kumar", ""], ["Mazumdar", "Arya", ""], ["Miklau", "Gerome", ""]]}, {"id": "2002.01766", "submitter": "Eugenia Oshurko", "authors": "Russ Harmer and Eugenia Oshurko", "title": "Knowledge representation and update in hierarchies of graphs", "comments": "25 pages, 4 figures, submitted to the Journal of Logical and\n  Algebraic Methods in Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mathematical theory is presented for the representation of knowledge in the\nform of a directed acyclic hierarchy of objects in a category where all paths\nbetween any given pair of objects are required to be equal. The conditions\nunder which knowledge update, in the form of the sesqui-pushout rewriting of an\nobject in a hierarchy, can be propagated to the rest of the hierarchy, in order\nto maintain all required path equalities, are analysed: some rewrites must be\npropagated forwards, in the direction of the arrows, while others must be\npropagated backwards, against the direction of the arrows, and, depending on\nthe precise form of the hierarchy, certain composability conditions may also be\nnecessary. The implementation of this theory, in the ReGraph Python library for\n(simple) directed graphs with attributes on nodes and edges, is then discussed\nin the context of two significant use cases.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 13:01:55 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Harmer", "Russ", ""], ["Oshurko", "Eugenia", ""]]}, {"id": "2002.02017", "submitter": "Brian Choi", "authors": "Brian Choi, Parv Saxena, Ryan Huang, Randal Burns", "title": "Observations on Porting In-memory KV stores to Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems that require high-throughput and fault tolerance, such as key-value\nstores and databases, are looking to persistent memory to combine the\nperformance of in-memory systems with the data-consistent fault-tolerance of\nnonvolatile stores. Persistent memory devices provide fast bytea-ddressable\naccess to non-volatile memory. We analyze the design space when integrating\npersistent memory into in-memory key value stores and quantify performance\ntradeoffs between throughput, latency, and and recovery time. Previous works\nhave explored many design choices, but did not quantify the tradeoffs. We\nimplement persistent memory support in Redis and Memcached, adapting the data\nstructures of each to work in two modes: (1) with all data in persistent memory\nand (2) a hybrid mode that uses persistent memory for key/value data and\nnon-volatile memory for indexing and metadata. Our experience reveals three\nactionable design principles that hold in Redis and Memcached, despite their\nvery different implementations. We conclude that the hybrid design increases\nthroughput and decreases latency at a minor cost in recovery time and code\ncomplexity\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 22:11:45 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Choi", "Brian", ""], ["Saxena", "Parv", ""], ["Huang", "Ryan", ""], ["Burns", "Randal", ""]]}, {"id": "2002.02046", "submitter": "Milan Cvitkovic", "authors": "Milan Cvitkovic", "title": "Supervised Learning on Relational Databases with Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of data scientists and machine learning practitioners use\nrelational data in their work [State of ML and Data Science 2017, Kaggle,\nInc.]. But training machine learning models on data stored in relational\ndatabases requires significant data extraction and feature engineering efforts.\nThese efforts are not only costly, but they also destroy potentially important\nrelational structure in the data. We introduce a method that uses Graph Neural\nNetworks to overcome these challenges. Our proposed method outperforms\nstate-of-the-art automatic feature engineering methods on two out of three\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 00:57:39 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Cvitkovic", "Milan", ""]]}, {"id": "2002.02552", "submitter": "Milad Khaki", "authors": "Milad Khaki", "title": "Progressive Cleaning and Mining of Uncertain Smart Water Meter Data", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several municipalities have recently installed wireless 'smart' water meters\nthat allow functionalities such as demand response, leak alerts, identification\nof characteristic demand patterns, and detailed consumption analysis. To\nachieve these benefits, the meter data needs to be error-free, which is not\nnecessarily available in practice, due to 'dirtiness' or 'uncertainty' of data,\nwhich is mostly unavoidable.\n  The focus of this paper is to investigate practical solutions to mine\nuncertain data for reliable results and to evaluate the impact of dirty data on\nfilters. This evaluation would eventually lead to valuable information, which\ncan be used for educated decision making on water planning strategies. We\nperform a systematic study of the errors existing in a large-scale smart water\nmeter deployments, which is helpful to better understand the nature of errors.\n  Identifying customers contributing to a load peak is used as the main filter.\nThe filter outputs are then combined with the domain expert knowledge to\nevaluate their accuracy and validity and also to look for potential errors.\nAfter discovering each error, we analyze its trails in the data and track back\nits source, which would eventually lead to the removal of the error or dealing\nwith it accordingly. This procedure is applied progressively to ensure that all\ndetectable errors are discovered and characterized in the data model.\n  We evaluate the performance of the proposed approach using the smart water\nmeter consumption data obtained from the City of Abbotsford, British Columbia,\nCanada. We present the results of both unprocessed and cleaned data and\nanalyze, in detail, the sensitivity of the selected filter to the errors.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 23:21:15 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Khaki", "Milad", ""]]}, {"id": "2002.03063", "submitter": "Edward Gan", "authors": "Edward Gan, Peter Bailis, Moses Charikar", "title": "Storyboard: Optimizing Precomputed Summaries for Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging class of data systems partition their data and precompute\napproximate summaries (i.e., sketches and samples) for each segment to reduce\nquery costs. They can then aggregate and combine the segment summaries to\nestimate results without scanning the raw data. However, given limited storage\nspace each summary introduces approximation errors that affect query accuracy.\nFor instance, systems that use existing mergeable summaries cannot reduce query\nerror below the error of an individual precomputed summary. We introduce\nStoryboard, a query system that optimizes item frequency and quantile summaries\nfor accuracy when aggregating over multiple segments. Compared to conventional\nmergeable summaries, Storyboard leverages additional memory available for\nsummary construction and aggregation to derive a more precise combined result.\nThis reduces error by up to 25x over interval aggregations and 4.4x over data\ncube aggregations on industrial datasets compared to standard summarization\nmethods, with provable worst-case error guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 01:35:50 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Gan", "Edward", ""], ["Bailis", "Peter", ""], ["Charikar", "Moses", ""]]}, {"id": "2002.03182", "submitter": "Zafaryab Rasool", "authors": "Zafaryab Rasool, Rui Zhou, Lu Chen, Chengfei Liu, Jiajie Xu", "title": "Index-based Solutions for Efficient Density Peak Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density Peak Clustering (DPC), a popular density-based clustering approach,\nhas received considerable attention from the research community primarily due\nto its simplicity and fewer-parameter requirement. However, the resultant\nclusters obtained using DPC are influenced by the sensitive parameter $d_c$,\nwhich depends on data distribution and requirements of different users.\nBesides, the original DPC algorithm requires visiting a large number of\nobjects, making it slow. To this end, this paper investigates index-based\nsolutions for DPC. Specifically, we propose two list-based index methods viz.\n(i) a simple List Index, and (ii) an advanced Cumulative Histogram Index.\nEfficient query algorithms are proposed for these indices which significantly\navoids irrelevant comparisons at the cost of space. For memory-constrained\nsystems, we further introduce an approximate solution to the above indices\nwhich allows substantial reduction in the space cost, provided that slight\ninaccuracies are admissible. Furthermore, owing to considerably lower memory\nrequirements of existing tree-based index structures, we also present effective\npruning techniques and efficient query algorithms to support DPC using the\npopular Quadtree Index and R-tree Index. Finally, we practically evaluate all\nthe above indices and present the findings and results, obtained from a set of\nextensive experiments on six synthetic and real datasets. The experimental\ninsights obtained can help to guide in selecting a befitting index.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 15:22:37 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 02:08:44 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Rasool", "Zafaryab", ""], ["Zhou", "Rui", ""], ["Chen", "Lu", ""], ["Liu", "Chengfei", ""], ["Xu", "Jiajie", ""]]}, {"id": "2002.03274", "submitter": "Shriram Ramesh", "authors": "Shriram Ramesh, Animesh Baranawal and Yogesh Simmhan", "title": "A Distributed Path Query Engine for Temporal Property Graphs", "comments": "An extended version of the paper that appears in IEEE/ACM\n  International Symposium on Cluster, Cloud and Internet Computing (CCGrid),\n  2020", "journal-ref": "IEEE/ACM International Symposium on Cluster, Cloud and Internet\n  Computing (CCGrid), 2020, 499-508", "doi": "10.1109/CCGrid49817.2020.00-43", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property graphs are a common form of linked data, with path queries used to\ntraverse and explore them for enterprise transactions and mining. Temporal\nproperty graphs are a recent variant where time is a first-class entity to be\nqueried over, and their properties and structure vary over time. These are seen\nin social, telecom, transit and epidemic networks. However, current graph\ndatabases and query engines have limited support for temporal relations among\ngraph entities, no support for time-varying entities and/or do not scale on\ndistributed resources. We address this gap by extending a linear path query\nmodel over property graphs to include intuitive temporal predicates and\naggregation operators over temporal graphs. We design a distributed execution\nmodel for these temporal path queries using the interval-centric computing\nmodel, and develop a novel cost model to select an efficient execution plan\nfrom several. We perform detailed experiments of our Granite distributed query\nengine using both static and dynamic temporal property graphs as large as 52M\nvertices, 218M edges and 325M properties, and a 1600-query workload, derived\nfrom the LDBC benchmark. We often offer sub-second query latencies on a\ncommodity cluster, which is 149x-1140x faster compared to industry-leading\nNeo4J shared-memory graph database and the JanusGraph / Spark distributed graph\nquery engine. Granite also completes 100% of the queries for all graphs,\ncompared to only 32-92% workload completion by the baseline systems. Further,\nour cost model selects a query plan that is within 10% of the optimal execution\ntime in 90% of the cases. Despite the irregular nature of graph processing, we\nexhibit a weak-scaling efficiency >= 60% on 8 nodes and >= 40% on 16 nodes, for\nmost query workloads.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 03:41:25 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 14:35:23 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ramesh", "Shriram", ""], ["Baranawal", "Animesh", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "2002.03461", "submitter": "Piotr Koniusz", "authors": "Xianjing Wang, Flora D. Salim, Yongli Ren, Piotr Koniusz", "title": "Relation Embedding for Personalised POI Recommendation", "comments": "12 pages, 3 figures, Accepted in the 24th Pacific-Asia Conference on\n  Knowledge Discovery and Data Mining (PAKDD 2020)", "journal-ref": null, "doi": "10.1007/978-3-030-47426-3_5", "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-of-Interest (POI) recommendation is one of the most important\nlocation-based services helping people discover interesting venues or services.\nHowever, the extreme user-POI matrix sparsity and the varying spatio-temporal\ncontext pose challenges for POI systems, which affects the quality of POI\nrecommendations. To this end, we propose a translation-based relation embedding\nfor POI recommendation. Our approach encodes the temporal and geographic\ninformation, as well as semantic contents effectively in a low-dimensional\nrelation space by using Knowledge Graph Embedding techniques. To further\nalleviate the issue of user-POI matrix sparsity, a combined matrix\nfactorization framework is built on a user-POI graph to enhance the inference\nof dynamic personal interests by exploiting the side-information. Experiments\non two real-world datasets demonstrate the effectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 22:26:52 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:40:48 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Wang", "Xianjing", ""], ["Salim", "Flora D.", ""], ["Ren", "Yongli", ""], ["Koniusz", "Piotr", ""]]}, {"id": "2002.03614", "submitter": "Ghadeer Abuoda", "authors": "Aisha Mohamed, Ghadeer Abuoda, Abdurrahman Ghanem, Zoi Kaoudi, Ashraf\n  Aboulnaga", "title": "RDFFrames: Knowledge Graph Access for Machine Learning Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs represented as RDF datasets are integral to many machine\nlearning applications. RDF is supported by a rich ecosystem of data management\nsystems and tools, most notably RDF database systems that provide a SPARQL\nquery interface. Surprisingly, machine learning tools for knowledge graphs do\nnot use SPARQL, despite the obvious advantages of using a database system. This\nis due to the mismatch between SPARQL and machine learning tools in terms of\ndata model and programming style. Machine learning tools work on data in\ntabular format and process it using an imperative programming style, while\nSPARQL is declarative and has as its basic operation matching graph patterns to\nRDF triples. We posit that a good interface to knowledge graphs from a machine\nlearning software stack should use an imperative, navigational programming\nparadigm based on graph traversal rather than the SPARQL query paradigm based\non graph patterns. In this paper, we present RDFFrames, a framework that\nprovides such an interface. RDFFrames provides an imperative Python API that\ngets internally translated to SPARQL, and it is integrated with the PyData\nmachine learning software stack. RDFFrames enables the user to make a sequence\nof Python calls to define the data to be extracted from a knowledge graph\nstored in an RDF database system, and it translates these calls into a compact\nSPQARL query, executes it on the database system, and returns the results in a\nstandard tabular format. Thus, RDFFrames is a useful tool for data preparation\nthat combines the usability of PyData with the flexibility and performance of\nRDF database systems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 09:39:25 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 22:52:40 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 06:39:37 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Mohamed", "Aisha", ""], ["Abuoda", "Ghadeer", ""], ["Ghanem", "Abdurrahman", ""], ["Kaoudi", "Zoi", ""], ["Aboulnaga", "Ashraf", ""]]}, {"id": "2002.03686", "submitter": "Jens D\\\"orpinghaus", "authors": "Jens D\\\"orpinghaus, Andreas Stefan", "title": "Optimization of Retrieval Algorithms on Large Scale Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs have been shown to play an important role in recent\nknowledge mining and discovery, for example in the field of life sciences or\nbioinformatics. Although a lot of research has been done on the field of query\noptimization, query transformation and of course in storing and retrieving\nlarge scale knowledge graphs the field of algorithmic optimization is still a\nmajor challenge and a vital factor in using graph databases. Few researchers\nhave addressed the problem of optimizing algorithms on large scale labeled\nproperty graphs. Here, we present two optimization approaches and compare them\nwith a naive approach of directly querying the graph database. The aim of our\nwork is to determine limiting factors of graph databases like Neo4j and we\ndescribe a novel solution to tackle these challenges. For this, we suggest a\nclassification schema to differ between the complexity of a problem on a graph\ndatabase. We evaluate our optimization approaches on a test system containing a\nknowledge graph derived biomedical publication data enriched with text mining\ndata. This dense graph has more than 71M nodes and 850M relationships. The\nresults are very encouraging and - depending on the problem - we were able to\nshow a speedup of a factor between 44 and 3839.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 12:37:03 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["D\u00f6rpinghaus", "Jens", ""], ["Stefan", "Andreas", ""]]}, {"id": "2002.04460", "submitter": "Piotr Wieczorek", "authors": "Jakub Michaliszyn and Jan Otop and Piotr Wieczorek", "title": "Modular Path Queries with Arithmetic", "comments": "arXiv admin note: substantial text overlap with arXiv:1710.04419", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to querying graph databases. Our approach balances\ncompeting goals of expressive power, language clarity and computational\ncomplexity. A distinctive feature of our approach is the ability to express\nproperties of minimal (e.g. shortest) and maximal (e.g. most valuable) paths\nsatisfying given criteria. To express complex properties in a modular way, we\nintroduce labelling-generating ontologies. The resulting formalism is\ncomputationally attractive -- queries can be answered in non-deterministic\nlogarithmic space in the size of the database.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 15:14:49 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 17:12:46 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 16:58:59 GMT"}, {"version": "v4", "created": "Wed, 28 Apr 2021 17:41:20 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Michaliszyn", "Jakub", ""], ["Otop", "Jan", ""], ["Wieczorek", "Piotr", ""]]}, {"id": "2002.04640", "submitter": "Raoni Louren\\c{c}o", "authors": "Raoni Louren\\c{c}o and Juliana Freire and Dennis Shasha", "title": "Debugging Machine Learning Pipelines", "comments": "10 pages", "journal-ref": "Proceedings of the 3rd International Workshop on Data Management\n  for End-to-End Machine Learning, June 2019, Article No.: 3", "doi": "10.1145/3329486.3329489", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning tasks entail the use of complex computational pipelines to\nreach quantitative and qualitative conclusions. If some of the activities in a\npipeline produce erroneous or uninformative outputs, the pipeline may fail or\nproduce incorrect results. Inferring the root cause of failures and unexpected\nbehavior is challenging, usually requiring much human thought, and is both\ntime-consuming and error-prone. We propose a new approach that makes use of\niteration and provenance to automatically infer the root causes and derive\nsuccinct explanations of failures. Through a detailed experimental evaluation,\nwe assess the cost, precision, and recall of our approach compared to the state\nof the art. Our source code and experimental data will be available for\nreproducibility and enhancement.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 19:13:12 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Louren\u00e7o", "Raoni", ""], ["Freire", "Juliana", ""], ["Shasha", "Dennis", ""]]}, {"id": "2002.05589", "submitter": "Sylvain Hall\\'e", "authors": "Sylvain Hall\\'e", "title": "Explainable Queries over Event Logs", "comments": "10 pages, submitted to IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Added value can be extracted from event logs generated by business processes\nin various ways. However, although complex computations can be performed over\nevent logs, the result of such computations is often difficult to explain; in\nparticular, it is hard to determine what parts of an input log actually matters\nin the production of that result. This paper describes how an existing log\nprocessing library, called BeepBeep, can be extended in order to provide a form\nof provenance: individual output events produced by a query can be precisely\ntraced back to the data elements of the log that contribute to (i.e. \"explain\")\nthe result.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:06:39 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Hall\u00e9", "Sylvain", ""]]}, {"id": "2002.05837", "submitter": "Xiangyao Yu", "authors": "Xiangyao Yu, Matt Youill, Matthew Woicik, Abdurrahman Ghanem, Marco\n  Serafini, Ashraf Aboulnaga, Michael Stonebraker", "title": "PushdownDB: Accelerating a DBMS using S3 Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the effectiveness of pushing parts of DBMS analytics\nqueries into the Simple Storage Service (S3) engine of Amazon Web Services\n(AWS), using a recently released capability called S3 Select. We show that some\nDBMS primitives (filter, projection, aggregation) can always be\ncost-effectively moved into S3. Other more complex operations (join, top-K,\ngroup-by) require reimplementation to take advantage of S3 Select and are often\ncandidates for pushdown. We demonstrate these capabilities through\nexperimentation using a new DBMS that we developed, PushdownDB. Experimentation\nwith a collection of queries including TPC-H queries shows that PushdownDB is\non average 30% cheaper and 6.7X faster than a baseline that does not use S3\nSelect.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 01:23:54 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Yu", "Xiangyao", ""], ["Youill", "Matt", ""], ["Woicik", "Matthew", ""], ["Ghanem", "Abdurrahman", ""], ["Serafini", "Marco", ""], ["Aboulnaga", "Ashraf", ""], ["Stonebraker", "Michael", ""]]}, {"id": "2002.05869", "submitter": "Vitor Almeida", "authors": "Vitor Pinheiro de Almeida and Sukanya Bhowmik and Markus Endler and\n  Kurt Rothermel", "title": "DSCEP: An Infrastructure for Distributed Semantic Complex Event\n  Processing", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Today most applications continuously produce information under the form of\nstreams, due to the advent of the means of collecting data. Sensors and social\nnetworks collect an immense variety and volume of data, from different\nreal-life situations and at a considerable velocity. Increasingly, applications\nrequire processing of heterogeneous data streams from different sources\ntogether with large background knowledge. To use only the information on the\ndata stream is not enough for many use cases. Semantic Complex Event Processing\n(CEP) systems have evolved from the classical rule-based CEP systems, by\nintegrating high-level knowledge representation and RDF stream processing using\nboth the data stream and background static knowledge. Additionally, CEP\napproaches lack the capability to semantically interpret and analyze data,\nwhich Semantic CEP (SCEP) attempts to address. SCEP has several limitations;\none of them is related to their high processing time. This paper provides a\nconceptual model and an implementation of an infrastructure for distributed\nSCEP, where each SCEP operator can process part of the data and send it to\nother SCEP operators in order to achieves some answer. We show that by\nsplitting the RDF stream processing and the background knowledge using the\nconcept of SCEP operators, it's possible to considerably reduce processing\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 04:36:51 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["de Almeida", "Vitor Pinheiro", ""], ["Bhowmik", "Sukanya", ""], ["Endler", "Markus", ""], ["Rothermel", "Kurt", ""]]}, {"id": "2002.05945", "submitter": "Daniel Schuster", "authors": "Daniel Schuster and Sebastiaan J. van Zelst", "title": "Online Process Monitoring Using Incremental State-Space Expansion: An\n  Exact Algorithm", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58666-9_9", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The execution of (business) processes generates valuable traces of event data\nin the information systems employed within companies. Recently, approaches for\nmonitoring the correctness of the execution of running processes have been\ndeveloped in the area of process mining, i.e., online conformance checking. The\nadvantages of monitoring a process' conformity during its execution are clear,\ni.e., deviations are detected as soon as they occur and countermeasures can\nimmediately be initiated to reduce the possible negative effects caused by\nprocess deviations. Existing work in online conformance checking only allows\nfor obtaining approximations of non-conformity, e.g., overestimating the actual\nseverity of the deviation. In this paper, we present an exact, parameter-free,\nonline conformance checking algorithm that computes conformance checking\nresults on the fly. Our algorithm exploits the fact that the conformance\nchecking problem can be reduced to a shortest path problem, by incrementally\nexpanding the search space and reusing previously computed intermediate\nresults. Our experiments show that our algorithm outperforms comparable\nstate-of-the-art approximation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 10:10:43 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 14:18:16 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 16:27:03 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Schuster", "Daniel", ""], ["van Zelst", "Sebastiaan J.", ""]]}, {"id": "2002.06039", "submitter": "Michael R\\\"oder", "authors": "Michael R\\\"oder and Mohamed Ahmed Sherif and Muhammad Saleem and Felix\n  Conrads and Axel-Cyrille Ngonga Ngomo", "title": "Benchmarking Knowledge Graphs on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing interest in making use of Knowledge Graphs for developing\nexplainable artificial intelligence, there is an increasing need for a\ncomparable and repeatable comparison of the performance of Knowledge\nGraph-based systems. History in computer science has shown that a main driver\nto scientific advances, and in fact a core element of the scientific method as\na whole, is the provision of benchmarks to make progress measurable. This paper\ngives an overview of benchmarks used to evaluate systems that process Knowledge\nGraphs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 14:02:29 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["R\u00f6der", "Michael", ""], ["Sherif", "Mohamed Ahmed", ""], ["Saleem", "Muhammad", ""], ["Conrads", "Felix", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "2002.06075", "submitter": "David Aparicio", "authors": "David Apar\\'icio, Ricardo Barata, Jo\\~ao Bravo, Jo\\~ao Tiago\n  Ascens\\~ao, Pedro Bizarro", "title": "ARMS: Automated rules management system for fraud detection", "comments": "11 pages, 12 figures, submitted to KDD '20 Applied Data Science Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fraud detection is essential in financial services, with the potential of\ngreatly reducing criminal activities and saving considerable resources for\nbusinesses and customers. We address online fraud detection, which consists of\nclassifying incoming transactions as either legitimate or fraudulent in\nreal-time. Modern fraud detection systems consist of a machine learning model\nand rules defined by human experts. Often, the rules performance degrades over\ntime due to concept drift, especially of adversarial nature. Furthermore, they\ncan be costly to maintain, either because they are computationally expensive or\nbecause they send transactions for manual review. We propose ARMS, an automated\nrules management system that evaluates the contribution of individual rules and\noptimizes the set of active rules using heuristic search and a user-defined\nloss-function. It complies with critical domain-specific requirements, such as\nhandling different actions (e.g., accept, alert, and decline), priorities,\nblacklists, and large datasets (i.e., hundreds of rules and millions of\ntransactions). We use ARMS to optimize the rule-based systems of two real-world\nclients. Results show that it can maintain the original systems' performance\n(e.g., recall, or false-positive rate) using only a fraction of the original\nrules (~ 50% in one case, and ~ 20% in the other).\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 15:29:59 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Apar\u00edcio", "David", ""], ["Barata", "Ricardo", ""], ["Bravo", "Jo\u00e3o", ""], ["Ascens\u00e3o", "Jo\u00e3o Tiago", ""], ["Bizarro", "Pedro", ""]]}, {"id": "2002.06098", "submitter": "Ramy E. Ali", "authors": "Ramy E. Ali", "title": "Consistency Analysis of Replication-Based Probabilistic Key-Value Stores", "comments": null, "journal-ref": "IEEE International Conference on Communications IEEE International\n  Conference on Communications (ICC), 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial quorum systems are widely used in distributed key-value stores due to\ntheir latency benefits at the expense of providing weaker consistency\nguarantees. The probabilistically bounded staleness framework (PBS) studied the\nlatency-consistency trade-off of Dynamo-style partial quorum systems through\nMonte Carlo event-based simulations. In this paper, we study the\nlatency-consistency trade-off for such systems analytically and derive a\nclosed-form expression for the inconsistency probability. Our approach allows\nfine-tuning of latency and consistency guarantees in key-value stores, which is\nintractable using Monte Carlo event-based simulations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:09:47 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 00:48:09 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 19:05:23 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 19:42:03 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ali", "Ramy E.", ""]]}, {"id": "2002.06161", "submitter": "Markus Suhr", "authors": "Markus Suhr, Christoph Lehmann, Christian Robert Bauer, Theresa\n  Bender, Cornelius Knopp, Luca Freckmann, Bj\\\"orn \\\"Ost Hansen, Christian\n  Henke, Georg Aschenbrandt, Lea K\\\"uhlborn, Sophia Rheinl\\\"ander, Linus Weber,\n  Bartlomiej Marzec, Marcel Hellkamp, Philipp Wieder, Harald Kusch, Ulrich Sax,\n  Sara Yasemin Nussbeck", "title": "menoci: Lightweight Extensible Web Portal enabling FAIR Data Management\n  for Biomedical Research Projects", "comments": "Preprint. 19 pages, 2 figures", "journal-ref": "BMC Bioinformatics 21, 582 (2020)", "doi": "10.1186/s12859-020-03928-1", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Background: Biomedical research projects deal with data management\nrequirements from multiple sources like funding agencies' guidelines, publisher\npolicies, discipline best practices, and their own users' needs. We describe\nfunctional and quality requirements based on many years of experience\nimplementing data management for the CRC 1002 and CRC 1190. A fully equipped\ndata management software should improve documentation of experiments and\nmaterials, enable data storage and sharing according to the FAIR Guiding\nPrinciples while maximizing usability, information security, as well as\nsoftware sustainability and reusability. Results: We introduce the modular web\nportal software menoci for data collection, experiment documentation, data\npublication, sharing, and preservation in biomedical research projects. Menoci\nmodules are based on the Drupal content management system which enables\nlightweight deployment and setup, and creates the possibility to combine\nresearch data management with a customisable project home page or collaboration\nplatform. Conclusions: Management of research data and digital research\nartefacts is transforming from individual researcher or groups best practices\ntowards project- or organisation-wide service infrastructures. To enable and\nsupport this structural transformation process, a vital ecosystem of open\nsource software tools is needed. Menoci is a contribution to this ecosystem of\nresearch data management tools that is specifically designed to support\nbiomedical research projects.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 15:36:21 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Suhr", "Markus", ""], ["Lehmann", "Christoph", ""], ["Bauer", "Christian Robert", ""], ["Bender", "Theresa", ""], ["Knopp", "Cornelius", ""], ["Freckmann", "Luca", ""], ["Hansen", "Bj\u00f6rn \u00d6st", ""], ["Henke", "Christian", ""], ["Aschenbrandt", "Georg", ""], ["K\u00fchlborn", "Lea", ""], ["Rheinl\u00e4nder", "Sophia", ""], ["Weber", "Linus", ""], ["Marzec", "Bartlomiej", ""], ["Hellkamp", "Marcel", ""], ["Wieder", "Philipp", ""], ["Kusch", "Harald", ""], ["Sax", "Ulrich", ""], ["Nussbeck", "Sara Yasemin", ""]]}, {"id": "2002.06163", "submitter": "Stella Giannakopoulou", "authors": "Stella Giannakopoulou, Manos Karpathiotakis, Anastasia Ailamaki", "title": "Cleaning Denial Constraint Violations through Relaxation", "comments": "To appear in SIGMOD 2020 proceedings", "journal-ref": null, "doi": "10.1145/3318464.3389775", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data cleaning is a time-consuming process that depends on the data analysis\nthat users perform. Existing solutions treat data cleaning as a separate\noffline process that takes place before analysis begins. Applying data cleaning\nbefore analysis assumes a priori knowledge of the inconsistencies and the query\nworkload, thereby requiring effort on understanding and cleaning the data that\nis unnecessary for the analysis. We propose an approach that performs\nprobabilistic repair of denial constraint violations on-demand, driven by the\nexploratory analysis that users perform. We introduce Daisy, a system that\nseamlessly integrates data cleaning into the analysis by relaxing query\nresults. Daisy executes analytical query-workloads over dirty data by weaving\ncleaning operators into the query plan. Our evaluation shows that Daisy adapts\nto the workload and outperforms traditional offline cleaning on both synthetic\nand real-world workloads.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:27:37 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 23:43:47 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 20:29:10 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2020 17:56:34 GMT"}, {"version": "v5", "created": "Sun, 12 Apr 2020 09:50:48 GMT"}, {"version": "v6", "created": "Wed, 15 Apr 2020 22:47:40 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Giannakopoulou", "Stella", ""], ["Karpathiotakis", "Manos", ""], ["Ailamaki", "Anastasia", ""]]}, {"id": "2002.06442", "submitter": "Chuan Xiao", "authors": "Yaoshu Wang, Chuan Xiao, Jianbin Qin, Xin Cao, Yifang Sun, Wei Wang,\n  and Makoto Onizuka", "title": "Monotonic Cardinality Estimation of Similarity Selection: A Deep\n  Learning Approach", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3380570", "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the outstanding capability of capturing underlying data distributions,\ndeep learning techniques have been recently utilized for a series of\ntraditional database problems. In this paper, we investigate the possibilities\nof utilizing deep learning for cardinality estimation of similarity selection.\nAnswering this problem accurately and efficiently is essential to many data\nmanagement applications, especially for query optimization. Moreover, in some\napplications the estimated cardinality is supposed to be consistent and\ninterpretable. Hence a monotonic estimation w.r.t. the query threshold is\npreferred. We propose a novel and generic method that can be applied to any\ndata type and distance function. Our method consists of a feature extraction\nmodel and a regression model. The feature extraction model transforms original\ndata and threshold to a Hamming space, in which a deep learning-based\nregression model is utilized to exploit the incremental property of cardinality\nw.r.t. the threshold for both accuracy and monotonicity. We develop a training\nstrategy tailored to our model as well as techniques for fast estimation. We\nalso discuss how to handle updates. We demonstrate the accuracy and the\nefficiency of our method through experiments, and show how it improves the\nperformance of a query optimizer.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 20:22:51 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 15:54:07 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 19:04:19 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Wang", "Yaoshu", ""], ["Xiao", "Chuan", ""], ["Qin", "Jianbin", ""], ["Cao", "Xin", ""], ["Sun", "Yifang", ""], ["Wang", "Wei", ""], ["Onizuka", "Makoto", ""]]}, {"id": "2002.06608", "submitter": "Nurefsan G\\\"ur", "authors": "Nurefsan G\\\"ur, Torben Bach Pedersen, Katja Hose, and Mikael Midtgaard", "title": "Multidimensional Enrichment of Spatial RDF Data for SOLAP -- Full\n  Version", "comments": "33 pages, 8 figures, 7 tables, 10 listings, 7 algorithms, under\n  review in Semantic Web Journal, available on\n  http://www.semantic-web-journal.net/content/multidimensional-enrichment-spatial-rdf-data-solap", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large volumes of spatial data and multidimensional data are being published\non the Semantic Web, which has led to new opportunities for advanced analysis,\nsuch as Spatial Online Analytical Processing (SOLAP). The RDF Data Cube (QB)\nand QB4OLAP vocabularies have been widely used for annotating and publishing\nstatistical and multidimensional RDF data. Although such statistical data sets\nmight have spatial information, such as coordinates, the lack of spatial\nsemantics and spatial multidimensional concepts in QB4OLAP and QB prevents\nusers from employing SOLAP queries over spatial data using SPARQL. The QB4SOLAP\nvocabulary, on the other hand, fully supports annotating spatial and\nmultidimensional data on the Semantic Web and enables users to query endpoints\nwith SOLAP operators in SPARQL. To bridge the gap between QB/QB4OLAP and\nQB4SOLAP, we propose an RDF2SOLAP enrichment model that automatically annotates\nspatial multidimensional concepts with QB4SOLAP and in doing so enables SOLAP\non existing QB and QB4OLAP data on the Semantic Web. Furthermore, we present\nand evaluate a wide range of enrichment algorithms and apply them on a\nnon-trivial real-world use case involving governmental open data with complex\ngeometry types.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 16:03:23 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["G\u00fcr", "Nurefsan", ""], ["Pedersen", "Torben Bach", ""], ["Hose", "Katja", ""], ["Midtgaard", "Mikael", ""]]}, {"id": "2002.06920", "submitter": "Thomas Guyet", "authors": "Thomas Guyet (LACODAM), Philippe Besnard (IRIT)", "title": "Semantics of negative sequential patterns", "comments": "proceedings of European Conference on Artificial Intelligence (ECAI),\n  Jun 2020, Santiago de Compostela, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of pattern mining, a negative sequential pattern is specified by\nmeans of a sequence consisting of events to occur and of other events, called\nnegative events, to be absent. For instance, containment of the pattern\n$\\langle a\\ \\neg b\\ c\\rangle$ arises with an occurrence of a and a subsequent\noccurrence of c but no occurrence of b in between. This article is to shed\nlight on the ambiguity of such a seemingly intuitive notation and we identify\neight possible semantics for the containment relation between a pattern and a\nsequence. These semantics are illustrated and formally studied, in particular\nwe propose dominance and equivalence relations between them. Also we prove that\nsupport is anti-monotonic for some of these semantics. Some of the results are\ndiscussed with the aim of developing algorithms to extract efficiently frequent\nnegative patterns.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 12:48:37 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 12:46:41 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Guyet", "Thomas", "", "LACODAM"], ["Besnard", "Philippe", "", "IRIT"]]}, {"id": "2002.06964", "submitter": "Krist\\'of B\\'erczi", "authors": "Krist\\'of B\\'erczi, Endre Boros, Ond\\v{r}ej \\v{C}epek, Petr\n  Ku\\v{c}era, Kazuhisa Makino", "title": "Unique key Horn functions", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a relational database, a key is a set of attributes such that a value\nassignment to this set uniquely determines the values of all other attributes.\nThe database uniquely defines a pure Horn function $h$, representing the\nfunctional dependencies. If the knowledge of the attribute values in set $A$\ndetermines the value for attribute $v$, then $A\\rightarrow v$ is an implicate\nof $h$. If $K$ is a key of the database, then $K\\rightarrow v$ is an implicate\nof $h$ for all attributes $v$.\n  Keys of small sizes play a crucial role in various problems. We present\nstructural and complexity results on the set of minimal keys of pure Horn\nfunctions. We characterize Sperner hypergraphs for which there is a unique pure\nHorn function with the given hypergraph as the set of minimal keys.\nFurthermore, we show that recognizing such hypergraphs is co-NP-complete\nalready when every hyperedge has size two. On the positive side, we identify\nseveral classes of graphs for which the recognition problem can be decided in\npolynomial time.\n  We also present an algorithm that generates the minimal keys of a pure Horn\nfunction with polynomial delay. By establishing a connection between keys and\ntarget sets, our approach can be used to generate all minimal target sets with\npolynomial delay when the thresholds are bounded by a constant. As a byproduct,\nour proof shows that the Minimum Key problem is at least as hard as the Minimum\nTarget Set Selection problem with bounded thresholds.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 14:09:50 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["B\u00e9rczi", "Krist\u00f3f", ""], ["Boros", "Endre", ""], ["\u010cepek", "Ond\u0159ej", ""], ["Ku\u010dera", "Petr", ""], ["Makino", "Kazuhisa", ""]]}, {"id": "2002.07402", "submitter": "Guohao Sun", "authors": "Guohao Sun, Guanfeng Liu, Yan Wang and Xiaofang Zhou", "title": "Updates-Aware Graph Pattern based Node Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Pattern based Node Matching (GPNM) is to find all the matches of the\nnodes in a data graph GD based on a given pattern graph GP. GPNM has become\nincreasingly important in many applications, e.g., group finding and expert\nrecommendation. In real scenarios, both GP and GD are updated frequently.\nHowever, the existing GPNM methods either need to perform a new GPNM procedure\nfrom scratch to deliver the node matching results based on the updated GP and\nGD or incrementally perform the GPNM procedure for each of the updates, leading\nto low efficiency. Therefore, there is a pressing need for a new method to\nefficiently deliver the node matching results on the updated graphs. In this\npaper, we first analyze and detect the elimination relationships between the\nupdates. Then, we construct an Elimination Hierarchy Tree (EH-Tree) to index\nthese elimination relationships. In order to speed up the GPNM process, we\npropose a graph partition method and then propose a new updates-aware GPNM\nmethod, called UA-GPNM, considering the single-graph elimination relationships\namong the updates in a single graph of GP or GD, and also the cross-graph\nelimination relationships between the updates in GP and the updates in GD.\nUA-GPNM first delivers the GPNM result of an initial query, and then delivers\nthe GPNM result of a subsequent query, based on the initial GPNM result and the\nmultiple updates that occur between two queries. The experimental results on\nfive real-world social graphs demonstrate that our proposed UA-GPNM is much\nmore efficient than the state-of-the-art GPNM methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 06:44:20 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Sun", "Guohao", ""], ["Liu", "Guanfeng", ""], ["Wang", "Yan", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "2002.07511", "submitter": "Peng Chen", "authors": "Weiguo Wang, Hui Li, Yanguo Peng, Sourav S Bhowmick, Peng Chen,\n  Xiaofeng Chen, Jiangtao Cui", "title": "An Efficient Secure Dynamic Skyline Query Model", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now cost-effective to outsource large dataset and perform query over\nthe cloud. However, in this scenario, there exist serious security and privacy\nissues that sensitive information contained in the dataset can be leaked. The\nmost effective way to address that is to encrypt the data before outsourcing.\nNevertheless, it remains a grand challenge to process queries in ciphertext\nefficiently. In this work, we shall focus on solving one representative query\ntask, namely dynamic skyline query, in a secure manner over the cloud. However,\nit is difficult to be performed on encrypted data as its dynamic domination\ncriteria require both subtraction and comparison, which cannot be directly\nsupported by a single encryption scheme efficiently. To this end, we present a\nnovel framework called SCALE. It works by transforming traditional dynamic\nskyline domination into pure comparisons. The whole process can be completed in\nsingle-round interaction between user and the cloud. We theoretically prove\nthat the outsourced database, query requests, and returned results are all kept\nsecret under our model. Moreover, we also present an efficient strategy for\ndynamic insertion and deletion of stored records. Empirical study over a series\nof datasets demonstrates that our framework improves the efficiency of query\nprocessing by nearly three orders of magnitude compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 12:09:44 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 07:47:30 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Wang", "Weiguo", ""], ["Li", "Hui", ""], ["Peng", "Yanguo", ""], ["Bhowmick", "Sourav S", ""], ["Chen", "Peng", ""], ["Chen", "Xiaofeng", ""], ["Cui", "Jiangtao", ""]]}, {"id": "2002.07951", "submitter": "Remy Wang", "authors": "Yisu Remy Wang, Shana Hutchison, Jonathan Leang, Bill Howe, Dan Suciu", "title": "SPORES: Sum-Product Optimization via Relational Equality Saturation for\n  Large Scale Linear Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are commonly specified in linear algebra (LA). LA\nexpressions can be rewritten into more efficient forms, by taking advantage of\ninput properties such as sparsity, as well as program properties such as common\nsubexpressions and fusible operators. The complex interaction among these\nproperties' impact on the execution cost poses a challenge to optimizing\ncompilers. Existing compilers resort to intricate heuristics that complicate\nthe codebase and add maintenance cost but fail to search through the large\nspace of equivalent LA expressions to find the cheapest one. We introduce a\ngeneral optimization technique for LA expressions, by converting the LA\nexpressions into Relational Algebra (RA) expressions, optimizing the latter,\nthen converting the result back to (optimized) LA expressions. One major\nadvantage of this method is that it is complete, meaning that any equivalent LA\nexpression can be found using the equivalence rules in RA. The challenge is the\nmajor size of the search space, and we address this by adopting and extending a\ntechnique used in compilers, called equality saturation. We integrate the\noptimizer into SystemML and validate it empirically across a spectrum of\nmachine learning tasks; we show that we can derive all existing hand-coded\noptimizations in SystemML, and perform new optimizations that lead to speedups\nfrom 1.2X to 5X.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 01:15:17 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 19:40:44 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Wang", "Yisu Remy", ""], ["Hutchison", "Shana", ""], ["Leang", "Jonathan", ""], ["Howe", "Bill", ""], ["Suciu", "Dan", ""]]}, {"id": "2002.08082", "submitter": "Jieming Shi", "authors": "Jieming Shi, Tianyuan Jin, Renchi Yang, Xiaokui Xiao, Yin Yang", "title": "Realtime Index-Free Single Source SimRank Processing on Web-Scale Graphs", "comments": "To appear in PVLDB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph G and a node u in G, a single source SimRank query evaluates\nthe similarity between u and every node v in G. Existing approaches to single\nsource SimRank computation incur either long query response time, or expensive\npre-computation, which needs to be performed again whenever the graph G\nchanges. Consequently, to our knowledge none of them is ideal for scenarios in\nwhich (i) query processing must be done in realtime, and (ii) the underlying\ngraph G is massive, with frequent updates.\n  Motivated by this, we propose SimPush, a novel algorithm that answers single\nsource SimRank queries without any pre-computation, and at the same time\nachieves significantly higher query processing speed than even the fastest\nknown index-based solutions. Further, SimPush provides rigorous result quality\nguarantees, and its high performance does not rely on any strong assumption of\nthe underlying graph. Specifically, compared to existing methods, SimPush\nemploys a radically different algorithmic design that focuses on (i)\nidentifying a small number of nodes relevant to the query, and subsequently\n(ii) computing statistics and performing residue push from these nodes only.\n  We prove the correctness of SimPush, analyze its time complexity, and compare\nits asymptotic performance with that of existing methods. Meanwhile, we\nevaluate the practical performance of SimPush through extensive experiments on\n8 real datasets. The results demonstrate that SimPush consistently outperforms\nall existing solutions, often by over an order of magnitude. In particular, on\na commodity machine, SimPush answers a single source SimRank query on a web\ngraph containing over 133 million nodes and 5.4 billion edges in under 62\nmilliseconds, with 0.00035 empirical error, while the fastest index-based\ncompetitor needs 1.18 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 09:38:40 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Shi", "Jieming", ""], ["Jin", "Tianyuan", ""], ["Yang", "Renchi", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Yin", ""]]}, {"id": "2002.08102", "submitter": "Philipp D. Rohde", "authors": "Philipp D. Rohde and Maria-Esther Vidal", "title": "Optimizing Federated Queries Based on the Physical Design of a Data Lake", "comments": "work-in-progress paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The optimization of query execution plans is known to be crucial for reducing\nthe query execution time. In particular, query optimization has been studied\nthoroughly for relational databases over the past decades. Recently, the\nResource Description Framework (RDF) became popular for publishing data on the\nWeb. As a consequence, federations composed of different data models like RDF\nand relational databases evolved. One type of these federations are Semantic\nData Lakes where every data source is kept in its original data model and\nsemantically annotated with ontologies or controlled vocabularies. However,\nstate-of-the-art query engines for federated query processing over Semantic\nData Lakes often rely on optimization techniques tailored for RDF. In this\npaper, we present query optimization techniques guided by heuristics that take\nthe physical design of a Data Lake into account. The heuristics are implemented\non top of Ontario, a SPARQL query engine for Semantic Data Lakes. Using\nsource-specific heuristics, the query engine is able to generate more efficient\nquery execution plans by exploiting the knowledge about indexes and\nnormalization in relational databases. We show that heuristics which take the\nphysical design of the Data Lake into account are able to speed up query\nprocessing.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 10:59:18 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 19:39:40 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Rohde", "Philipp D.", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "2002.08828", "submitter": "Johannes Doleschal", "authors": "Johannes Doleschal, Noa Bratman, Benny Kimelfeld, Wim Martens", "title": "The Complexity of Aggregates over Extractions by Regular Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular expressions with capture variables, also known as \"regex formulas,\"\nextract relations of spans (intervals identified by their start and end\nindices) from text. Based on these Fagin et al. introduced regular document\nspanners which are the closure of regex formulas under Relational Algebra. In\nthis work, we study the computational complexity of querying text by aggregate\nfunctions, like sum, average or quantiles, on top of regular document spanners.\nTo this end, we formally define aggregate functions over regular document\nspanners and analyze the computational complexity of exact and approximative\ncomputation of the aggregates. To be precise, we show that in a restricted case\nall aggregates can be computed in polynomial time. In general, however, even\nthough exact computation is intractable, some aggregates can still be\napproximated with fully polynomial-time randomized approximation schemes\n(FPRAS).\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:07:18 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 13:21:38 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Doleschal", "Johannes", ""], ["Bratman", "Noa", ""], ["Kimelfeld", "Benny", ""], ["Martens", "Wim", ""]]}, {"id": "2002.09091", "submitter": "Zainab Zolaktaf", "authors": "Zainab Zolaktaf, Mostafa Milani, Rachel Pottinger", "title": "Facilitating SQL Query Composition and Analysis", "comments": "Full version of the ACM SIGMOD 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formulating efficient SQL queries requires several cycles of tuning and\nexecution, particularly for inexperienced users. We examine methods that can\naccelerate and improve this interaction by providing insights about SQL queries\nprior to execution. We achieve this by predicting properties such as the query\nanswer size, its run-time, and error class. Unlike existing approaches, our\napproach does not rely on any statistics from the database instance or query\nexecution plans. This is particularly important in settings with limited access\nto the database instance. Our approach is based on using data-driven machine\nlearning techniques that rely on large query workloads to model SQL queries and\ntheir properties. We evaluate the utility of neural network models and\ntraditional machine learning models. We use two real-world query workloads: the\nSloan Digital Sky Survey (SDSS) and the SQLShare query workload. Empirical\nresults show that the neural network models are more accurate in predicting the\nquery error class, achieving a higher F-measure on classes with fewer samples\nas well as performing better on other problems such as run-time and answer size\nprediction. These results are encouraging and confirm that SQL query workloads\nand data-driven machine learning methods can be leveraged to facilitate query\ncomposition and analysis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 02:10:42 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Zolaktaf", "Zainab", ""], ["Milani", "Mostafa", ""], ["Pottinger", "Rachel", ""]]}, {"id": "2002.09172", "submitter": "Christian Aebeloe", "authors": "Christian Aebeloe, Ilkcan Keles, Gabriela Montoya, Katja Hose", "title": "Star Pattern Fragments: Accessing Knowledge Graphs through Star Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web offers access to a vast Web of interlinked information\naccessible via SPARQL endpoints. Such endpoints offer a well-defined interface\nto retrieve results for complex SPARQL queries. The computational load for\nprocessing such queries, however, lies entirely with the server hosting the\nSPARQL endpoint, which can easily become overloaded and in the worst case not\nonly become slow in responding but even crash so that the data becomes\ntemporarily unavailable. Recently proposed interfaces, such as Triple Pattern\nFragments, have therefore shifted the query processing load from the server to\nthe client. For queries involving triple patterns with low selectivity, this\ncan easily result in high network traffic and slow execution times. In this\npaper, we therefore present a novel interface, Star Pattern Fragments (SPF),\nwhich decomposes SPARQL queries into star-shaped subqueries and can combine a\nlower network load with a higher query throughput and a comparatively low\nserver load. Our experimental results show that our approach does not only\nsignificantly reduce network traffic but is also at least an order of magnitude\nfaster in comparison to the state-of-the-art interfaces under high query\nprocessing load.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 07:59:22 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Aebeloe", "Christian", ""], ["Keles", "Ilkcan", ""], ["Montoya", "Gabriela", ""], ["Hose", "Katja", ""]]}, {"id": "2002.09361", "submitter": "Wei Hu", "authors": "Jiacheng Huang and Wei Hu and Zhifeng Bao and Yuzhong Qu", "title": "Crowdsourced Collective Entity Resolution with Relational Match\n  Propagation", "comments": "Accepted by the 36th IEEE International Conference on Data\n  Engineering (ICDE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases (KBs) store rich yet heterogeneous entities and facts. Entity\nresolution (ER) aims to identify entities in KBs which refer to the same\nreal-world object. Recent studies have shown significant benefits of involving\nhumans in the loop of ER. They often resolve entities with pairwise similarity\nmeasures over attribute values and resort to the crowds to label uncertain\nones. However, existing methods still suffer from high labor costs and\ninsufficient labeling to some extent. In this paper, we propose a novel\napproach called crowdsourced collective ER, which leverages the relationships\nbetween entities to infer matches jointly rather than independently.\nSpecifically, it iteratively asks human workers to label picked entity pairs\nand propagates the labeling information to their neighbors in distance. During\nthis process, we address the problems of candidate entity pruning,\nprobabilistic propagation, optimal question selection and error-tolerant truth\ninference. Our experiments on real-world datasets demonstrate that, compared\nwith state-of-the-art methods, our approach achieves superior accuracy with\nmuch less labeling.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 15:33:53 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Huang", "Jiacheng", ""], ["Hu", "Wei", ""], ["Bao", "Zhifeng", ""], ["Qu", "Yuzhong", ""]]}, {"id": "2002.09440", "submitter": "Ibrahim Abdelaziz", "authors": "Ibrahim Abdelaziz, Julian Dolby, James P. McCusker, Kavitha Srinivas", "title": "Graph4Code: A Machine Interpretable Knowledge Graph for Code", "comments": "Preprint, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs have proven extremely useful in powering diverse\napplications in semantic search and natural language understanding. Graph4Code\nis a knowledge graph about program code that can similarly power diverse\napplications such as program search, code understanding, refactoring, bug\ndetection, and code automation. The graph uses generic techniques to capture\nthe semantics of Python code: the key nodes in the graph are classes, functions\nand methods in popular Python modules. Edges indicate function usage (e.g., how\ndata flows through function calls, as derived from program analysis of real\ncode), and documentation about functions (e.g., code documentation, usage\ndocumentation, or forum discussions such as StackOverflow). We make extensive\nuse of named graphs in RDF to make the knowledge graph extensible by the\ncommunity. We describe a set of generic extraction techniques that we applied\nto over 1.3M Python files drawn from GitHub, over 2,300 Python modules, as well\nas 47M forum posts to generate a graph with over 2 billion triples. We also\nprovide a number of initial use cases of the knowledge graph in code\nassistance, enforcing best practices, debugging and type inference. The graph\nand all its artifacts are available to the community for use.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 17:40:20 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 19:46:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Abdelaziz", "Ibrahim", ""], ["Dolby", "Julian", ""], ["McCusker", "James P.", ""], ["Srinivas", "Kavitha", ""]]}, {"id": "2002.09449", "submitter": "Carlos Sarraute PhD", "authors": "Marcelo Mottalli, Alejo Sanchez, Gustavo Ajzenman, Carlos Sarraute", "title": "Snel: SQL Native Execution for LLVM", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Snel is a relational database engine featuring Just-In-Time (JIT) compilation\nof queries and columnar data representation. Snel is designed for fast on-line\nanalytics by leveraging the LLVM compiler infrastructure. It also has custom\nspecial methods like resolving histograms as extensions to the SQL language.\n\"Snel\" means \"SQL Native Execution for LLVM\".\n  Unlike traditional database engines, it does not provide a client-server\ninterface. Instead, it exposes its interface as an extension to SQLite, for a\nsimple interactive usage from command line and for embedding in applications.\nSince Snel tables are read-only, it does not provide features like transactions\nor updates. This allows queries to be very fast since they don't have the\noverhead of table locking or ensuring consistency.\n  At its core, Snel is simply a dynamic library that can be used by client\napplications. It has an SQLite extension for seamless integration with a\ntraditional SQL environment and simple interactive usage from command line.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 18:06:18 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 12:32:08 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Mottalli", "Marcelo", ""], ["Sanchez", "Alejo", ""], ["Ajzenman", "Gustavo", ""], ["Sarraute", "Carlos", ""]]}, {"id": "2002.09754", "submitter": "Parmita Mehta", "authors": "Parmita Mehta, Stephen Portillo, Magdalena Balazinska, Andrew Connolly", "title": "Sampling for Deep Learning Model Diagnosis (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) models have achieved paradigm-changing performance in many\nfields with high dimensional data, such as images, audio, and text. However,\nthe black-box nature of deep neural networks is a barrier not just to adoption\nin applications such as medical diagnosis, where interpretability is essential,\nbut also impedes diagnosis of under performing models. The task of diagnosing\nor explaining DL models requires the computation of additional artifacts, such\nas activation values and gradients. These artifacts are large in volume, and\ntheir computation, storage, and querying raise significant data management\nchallenges.\n  In this paper, we articulate DL diagnosis as a data management problem, and\nwe propose a general, yet representative, set of queries to evaluate systems\nthat strive to support this new workload. We further develop a novel data\nsampling technique that produce approximate but accurate results for these\nmodel debugging queries. Our sampling technique utilizes the lower dimension\nrepresentation learned by the DL model and focuses on model decision boundaries\nfor the data in this lower dimensional space. We evaluate our techniques on one\nstandard computer vision and one scientific data set and demonstrate that our\nsampling technique outperforms a variety of state-of-the-art alternatives in\nterms of query accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 19:24:16 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Mehta", "Parmita", ""], ["Portillo", "Stephen", ""], ["Balazinska", "Magdalena", ""], ["Connolly", "Andrew", ""]]}, {"id": "2002.09755", "submitter": "Xikui Wang", "authors": "Steven Jacobs, Xikui Wang, Michael J. Carey, Vassilis J. Tsotras, Md\n  Yusuf Sarwar Uddin", "title": "BAD to the Bone: Big Active Data at its Core", "comments": "30 pages. Accepted by VLDBJ", "journal-ref": null, "doi": "10.1007/s00778-020-00616-7", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtually all of today's Big Data systems are passive in nature, responding\nto queries posted by their users. Instead, we are working to shift Big Data\nplatforms from passive to active. In our view, a Big Active Data (BAD) system\nshould continuously and reliably capture Big Data while enabling timely and\nautomatic delivery of relevant information to a large pool of interested users,\nas well as supporting retrospective analyses of historical information. While\nvarious scalable streaming query engines have been created, their active\nbehavior is limited to a (relatively) small window of the incoming data. To\nthis end we have created a BAD platform that combines ideas and capabilities\nfrom both Big Data and Active Data (e.g., Publish/Subscribe, Streaming\nEngines). It supports complex subscriptions that consider not only newly\narrived items but also their relationships to past, stored data. Further, it\ncan provide actionable notifications by enriching the subscription results with\nother useful data. Our platform extends an existing open-source Big Data\nManagement System, Apache AsterixDB, with an active toolkit. The toolkit\ncontains features to rapidly ingest semistructured data, share execution\npipelines among users, manage scaled user data subscriptions, and actively\nmonitor the state of the data to produce individualized information for each\nuser. This paper describes the features and design of our current BAD data\nplatform and demonstrates its ability to scale without sacrificing query\ncapabilities or result individualization.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 19:32:51 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 00:09:29 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Jacobs", "Steven", ""], ["Wang", "Xikui", ""], ["Carey", "Michael J.", ""], ["Tsotras", "Vassilis J.", ""], ["Uddin", "Md Yusuf Sarwar", ""]]}, {"id": "2002.09799", "submitter": "Laurel Orr", "authors": "Laurel Orr, Magda Balazinska, and Dan Suciu", "title": "Sample Debiasing in the Themis Open World Database System (Extended\n  Version)", "comments": "SIGMOD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open world database management systems assume tuples not in the database\nstill exist and are becoming an increasingly important area of research. We\npresent Themis, the first open world database that automatically rebalances\narbitrarily biased samples to approximately answer queries as if they were\nissued over the entire population. We leverage apriori population aggregate\ninformation to develop and combine two different approaches for automatic\ndebiasing: sample reweighting and Bayesian network probabilistic modeling. We\nbuild a prototype of Themis and demonstrate that Themis achieves higher query\naccuracy than the default AQP approach, an alternative sample reweighting\ntechnique, and a variety of Bayesian network models while maintaining\ninteractive query response times. We also show that \\name is robust to\ndifferences in the support between the sample and population, a key use case\nwhen using social media samples.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 00:53:04 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 18:38:59 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Orr", "Laurel", ""], ["Balazinska", "Magda", ""], ["Suciu", "Dan", ""]]}, {"id": "2002.10029", "submitter": "Tal Friedman", "authors": "Tal Friedman and Guy Van den Broeck", "title": "Symbolic Querying of Vector Spaces: Probabilistic Databases Meets\n  Relational Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose unifying techniques from probabilistic databases and relational\nembedding models with the goal of performing complex queries on incomplete and\nuncertain data. We formalize a probabilistic database model with respect to\nwhich all queries are done. This allows us to leverage the rich literature of\ntheory and algorithms from probabilistic databases for solving problems. While\nthis formalization can be used with any relational embedding model, the lack of\na well-defined joint probability distribution causes simple query problems to\nbecome provably hard. With this in mind, we introduce \\TO, a relational\nembedding model designed to be a tractable probabilistic database, by\nexploiting typical embedding assumptions within the probabilistic framework.\nUsing a principled, efficient inference algorithm that can be derived from its\ndefinition, we empirically demonstrate that \\TOs is an effective and general\nmodel for these querying tasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 01:17:25 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 20:01:49 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Friedman", "Tal", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "2002.10181", "submitter": "Gong Cheng", "authors": "Shuxin Li, Gong Cheng, Chengkai Li", "title": "Relaxing Relationship Queries on Graph Data", "comments": "16 pages, accepted to JoWS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many domains we have witnessed the need to search a large entity-relation\ngraph for direct and indirect relationships between a set of entities specified\nin a query. A search result, called a semantic association (SA), is typically a\ncompact (e.g., diameter-constrained) connected subgraph containing all the\nquery entities. For this problem of SA search, efficient algorithms exist but\nwill return empty results if some query entities are distant in the graph. To\nreduce the occurrence of failing query and provide alternative results, we\nstudy the problem of query relaxation in the context of SA search. Simply\nrelaxing the compactness constraint will sacrifice the compactness of an SA,\nand more importantly, may lead to performance issues and be impracticable.\nInstead, we focus on removing the smallest number of entities from the original\nfailing query, to form a maximum successful sub-query which minimizes the loss\nof result quality caused by relaxation. We prove that verifying the success of\na sub-query turns into finding an entity (called a certificate) that satisfies\na distance-based condition about the query entities. To efficiently find a\ncertificate of the success of a maximum sub-query, we propose a best-first\nsearch algorithm that leverages distance-based estimation to effectively prune\nthe search space. We further improve its performance by adding two fine-grained\nheuristics: one based on degree and the other based on distance. Extensive\nexperiments over popular RDF datasets demonstrate the efficiency of our\nalgorithm, which is more scalable than baselines.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 11:55:24 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Li", "Shuxin", ""], ["Cheng", "Gong", ""], ["Li", "Chengkai", ""]]}, {"id": "2002.10269", "submitter": "Mojgan Mohajer Dr.", "authors": "Mojgan Mohajer", "title": "A Graph-Based Platform for Customer Behavior Analysis using\n  Applications' Clickstream Data", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clickstream analysis is getting more attention since the increase of usage in\ne-commerce and applications. Beside customers' purchase behavior analysis,\nthere is also attempt to analyze the customer behavior in relation to the\nquality of web or application design. In general, clickstream data can be\nconsidered as a sequence of log events collected at different levels of web/app\nusage. The analysis of clickstream data can be performed directly as sequence\nanalysis or by extracting features from sequences. In this work, we show how\nrepresenting and saving the sequences with their underlying graph structures\ncan induce a platform for customer behavior analysis. Our main idea is that\nclickstream data containing sequences of actions of an application, are walks\nof the corresponding finite state automaton (FSA) of that application. Our\nhypothesis is that the customers of an application normally do not use all\npossible walks through that FSA and the number of actual walks is much smaller\nthan total number of possible walks through the FSA. Sequences of such a walk\nnormally consist of a finite number of cycles on FSA graphs. Identifying and\nmatching these cycles in the classical sequence analysis is not straight\nforward. We show that representing the sequences through their underlying graph\nstructures not only groups the sequences automatically but also provides a\ncompressed data representation of the original sequences.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 13:57:29 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Mohajer", "Mojgan", ""]]}, {"id": "2002.10283", "submitter": "Sven Hertling", "authors": "Sven Hertling, Heiko Paulheim", "title": "The Knowledge Graph Track at OAEI -- Gold Standards, Baselines, and the\n  Golden Hammer Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ontology Alignment Evaluation Initiative (OAEI) is an annual evaluation\nof ontology matching tools. In 2018, we have started the Knowledge Graph track,\nwhose goal is to evaluate the simultaneous matching of entities and schemas of\nlarge-scale knowledge graphs. In this paper, we discuss the design of the track\nand two different strategies of gold standard creation. We analyze results and\nexperiences obtained in first editions of the track, and, by revealing a hidden\ntask, we show that all tools submitted to the track (and probably also to other\ntracks) suffer from a bias which we name the golden hammer bias.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 14:35:02 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Hertling", "Sven", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2002.10537", "submitter": "Ioannis Xarchakos", "authors": "Nick Koudas, Raymond Li, Ioannis Xarchakos", "title": "Video Monitoring Queries", "comments": "12 pages, 14 figures, to be published in International Conference in\n  Data Engineering (ICDE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in video processing utilizing deep learning primitives\nachieved breakthroughs in fundamental problems in video analysis such as frame\nclassification and object detection enabling an array of new applications.\n  In this paper we study the problem of interactive declarative query\nprocessing on video streams. In particular we introduce a set of approximate\nfilters to speed up queries that involve objects of specific type (e.g., cars,\ntrucks, etc.) on video frames with associated spatial relationships among them\n(e.g., car left of truck). The resulting filters are able to assess quickly if\nthe query predicates are true to proceed with further analysis of the frame or\notherwise not consider the frame further avoiding costly object detection\noperations.\n  We propose two classes of filters $IC$ and $OD$, that adapt principles from\ndeep image classification and object detection. The filters utilize extensible\ndeep neural architectures and are easy to deploy and utilize. In addition, we\npropose statistical query processing techniques to process aggregate queries\ninvolving objects with spatial constraints on video streams and demonstrate\nexperimentally the resulting increased accuracy on the resulting aggregate\nestimation.\n  Combined these techniques constitute a robust set of video monitoring query\nprocessing techniques. We demonstrate that the application of the techniques\nproposed in conjunction with declarative queries on video streams can\ndramatically increase the frame processing rate and speed up query processing\nby at least two orders of magnitude. We present the results of a thorough\nexperimental study utilizing benchmark video data sets at scale demonstrating\nthe performance benefits and the practical relevance of our proposals.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:53:35 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Koudas", "Nick", ""], ["Li", "Raymond", ""], ["Xarchakos", "Ioannis", ""]]}, {"id": "2002.10688", "submitter": "Behshid Behkamal", "authors": "Behshid Behkamal, Moshen Kahani, Ebrahim Bagheri, Majid Sazvar", "title": "A metric Suite for Systematic Quality Assessment of Linked Open Data", "comments": "20 pages, 12 tables", "journal-ref": "IJICT Vol. 8, No. 3 (2016) 27-45", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Abstract- The vision of the Linked Open Data (LOD) initiative is to provide a\ndistributed model for publishing and meaningfully interlinking open data. The\nrealization of this goal depends strongly on the quality of the data that is\npublished as a part of the LOD. This paper focuses on the systematic quality\nassessment of datasets prior to publication on the LOD cloud. To this end, we\nidentify important quality deficiencies that need to be avoided and/or resolved\nprior to the publication of a dataset. We then propose a set of metrics to\nmeasure these quality deficiencies in a dataset. This way, we enable the\nassessment and identification of undesirable quality characteristics of a\ndataset through our proposed metrics. This will help publishers to filter out\nlow-quality data based on the quality assessment results, which in turn enables\ndata consumers to make better and more informed decisions when using the open\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 06:02:07 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Behkamal", "Behshid", ""], ["Kahani", "Moshen", ""], ["Bagheri", "Ebrahim", ""], ["Sazvar", "Majid", ""]]}, {"id": "2002.11593", "submitter": "Antonio Fern\\'andez Anta", "authors": "Vicent Cholvi and Antonio Fernandez Anta and Chryssis Georgiou and\n  Nicolas Nicolaou and Michel Raynal", "title": "Appending Atomically in Byzantine Distributed Ledgers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Distributed Ledger Object (DLO) is a concurrent object that maintains a\ntotally ordered sequence of records, and supports two basic operations: append,\nwhich appends a record at the end of the sequence, and get, which returns the\nsequence of records. In this work we provide a proper formalization of a\nByzantine-tolerant Distributed Ledger Object (BDLO), which is a DLO in a\ndistributed system in which processes may deviate arbitrarily from their\nindented behavior, i.e. they may be Byzantine. Our formal definition is\naccompanied by algorithms to implement BDLOs by utilizing an underlying\nByzantine Atomic Broadcast service.\n  We then utilize the BDLO implementations to solve the Atomic Appends problem\nagainst Byzantine processes. The Atomic Appends problem emerges when several\nclients have records to append, the record of each client has to be appended to\na different BDLO, and it must be guaranteed that either all records are\nappended or none. We present distributed algorithms implementing solutions for\nthe Atomic Appends problem when the clients (which are involved in the appends)\nand the servers (which maintain the BDLOs) may be Byzantine.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:22:47 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Cholvi", "Vicent", ""], ["Anta", "Antonio Fernandez", ""], ["Georgiou", "Chryssis", ""], ["Nicolaou", "Nicolas", ""], ["Raynal", "Michel", ""]]}, {"id": "2002.11622", "submitter": "Guillermo de Bernardo", "authors": "Nieves R. Brisaboa, Ana Cerdeira-Pena, Guillermo de Bernardo, Antonio\n  Fari\\~na", "title": "Revisiting compact RDF stores based on k2-trees", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new compact representation to efficiently store and query large\nRDF datasets in main memory. Our proposal, called BMatrix, is based on the\nk2-tree, a data structure devised to represent binary matrices in a compressed\nway, and aims at improving the results of previous state-of-the-art\nalternatives, especially in datasets with a relatively large number of\npredicates. We introduce our technique, together with some improvements on the\nbasic k2-tree that can be applied to our solution in order to boost\ncompression. Experimental results in the flagship RDF dataset DBPedia show that\nour proposal achieves better compression than existing alternatives, while\nyielding competitive query times, particularly in the most frequent triple\npatterns and in queries with unbound predicate, in which we outperform existing\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:03:28 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Cerdeira-Pena", "Ana", ""], ["de Bernardo", "Guillermo", ""], ["Fari\u00f1a", "Antonio", ""]]}, {"id": "2002.11675", "submitter": "Fabrizio Albertetti", "authors": "Fabrizio Albertetti, Hatem Ghorbel", "title": "Workload Prediction of Business Processes -- An Approach Based on\n  Process Mining and Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the interconnectedness and digitization of industrial\nmachines, known as Industry 4.0, pave the way for new analytical techniques.\nIndeed, the availability and the richness of production-related data enables\nnew data-driven methods. In this paper, we propose a process mining approach\naugmented with artificial intelligence that (1) reconstructs the historical\nworkload of a company and (2) predicts the workload using neural networks. Our\nmethod relies on logs, representing the history of business processes related\nto manufacturing. These logs are used to quantify the supply and demand and are\nfed into a recurrent neural network model to predict customer orders. The\ncorresponding activities to fulfill these orders are then sampled from history\nwith a replay mechanism, based on criteria such as trace frequency and\nactivities similarity. An evaluation and illustration of the method is\nperformed on the administrative processes of Heraeus Materials SA. The workload\nprediction on a one-year test set achieves an MAPE score of 19% for a one-week\nforecast. The case study suggests a reasonable accuracy and confirms that a\ngood understanding of the historical workload combined to articulated\npredictions are of great help for supporting management decisions and can\ndecrease costs with better resources planning on a medium-term level.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 08:19:23 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Albertetti", "Fabrizio", ""], ["Ghorbel", "Hatem", ""]]}, {"id": "2002.11771", "submitter": "Dongfang Zhao", "authors": "Dongfang Zhao and Tonglin Li", "title": "Distributed Cross-Blockchain Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interoperability across multiple or many blockchains would play a\ncritical role in the forthcoming blockchain-based data management paradigm. In\nparticular, how to ensure the ACID properties of those transactions across an\narbitrary number of blockchains remains an open problem in both academic and\nindustry: Existing solutions either work for only two blockchains or requires a\ncentralized component, neither of which would meet the scalability requirement\nin practice. This short paper shares our vision and some early results toward\nscalable cross-blockchain transactions. Specifically, we design two distributed\ncommit protocols and, both analytically and experimentally, demonstrate their\neffectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 19:57:59 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Zhao", "Dongfang", ""], ["Li", "Tonglin", ""]]}, {"id": "2002.11780", "submitter": "Mayuresh Kunjir", "authors": "Mayuresh Kunjir and Shivnath Babu", "title": "Black or White? How to Develop an AutoTuner for Memory-based Analytics\n  [Extended Version]", "comments": "Main version in ACM SIGMOD 2020", "journal-ref": null, "doi": "10.1145/3318464.3380591", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a lot of interest today in building autonomous (or, self-driving)\ndata processing systems. An emerging school of thought is to leverage AI-driven\n\"black box\" algorithms for this purpose. In this paper, we present a contrarian\nview. We study the problem of autotuning the memory allocation for applications\nrunning on modern distributed data processing systems. For this problem, we\nshow that an empirically-driven \"white-box\" algorithm, called RelM, that we\nhave developed provides a close-to-optimal tuning at a fraction of the\noverheads compared to state-of-the-art AI-driven \"black box\" algorithms,\nnamely, Bayesian Optimization (BO) and Deep Distributed Policy Gradient (DDPG).\nThe main reason for RelM's superior performance is that the memory management\nin modern memory-based data analytics systems is an interplay of algorithms at\nmultiple levels: (i) at the resource-management level across various containers\nallocated by resource managers like Kubernetes and YARN, (ii) at the container\nlevel among the OS, pods, and processes such as the Java Virtual Machine (JVM),\n(iii) at the application level for caching, aggregation, data shuffles, and\napplication data structures, and (iv) at the JVM level across various pools\nsuch as the Young and Old Generation. RelM understands these interactions and\nuses them in building an analytical solution to autotune the memory management\nknobs. In another contribution, called GBO, we use the RelM's analytical models\nto speed up Bayesian Optimization. Through an evaluation based on Apache Spark,\nwe showcase that RelM's recommendations are significantly better than what\ncommonly-used Spark deployments provide, and are close to the ones obtained by\nbrute-force exploration; while GBO provides optimality guarantees for a higher,\nbut still significantly lower compared to the state-of-the-art AI-driven\npolicies, cost overhead.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 20:34:28 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Kunjir", "Mayuresh", ""], ["Babu", "Shivnath", ""]]}, {"id": "2002.11791", "submitter": "Yinjun Wu", "authors": "Yinjun Wu, Val Tannen, Susan B. Davidson", "title": "PrIU: A Provenance-Based Approach for Incrementally Updating Regression\n  Models", "comments": "28 Pages, published in 2020 ACM SIGMOD International Conference on\n  Management of Data (SIGMOD 2020)", "journal-ref": null, "doi": "10.1145/3318464.3380571", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquitous use of machine learning algorithms brings new challenges to\ntraditional database problems such as incremental view update. Much effort is\nbeing put in better understanding and debugging machine learning models, as\nwell as in identifying and repairing errors in training datasets. Our focus is\non how to assist these activities when they have to retrain the machine\nlearning model after removing problematic training samples in cleaning or\nselecting different subsets of training data for interpretability. This paper\npresents an efficient provenance-based approach, PrIU, and its optimized\nversion, PrIU-opt, for incrementally updating model parameters without\nsacrificing prediction accuracy. We prove the correctness and convergence of\nthe incrementally updated model parameters, and validate it experimentally.\nExperimental results show that up to two orders of magnitude speed-ups can be\nachieved by PrIU-opt compared to simply retraining the model from scratch, yet\nobtaining highly similar models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 21:04:06 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Wu", "Yinjun", ""], ["Tannen", "Val", ""], ["Davidson", "Susan B.", ""]]}, {"id": "2002.11862", "submitter": "Anas Daghistani", "authors": "Anas Daghistani, Walid G. Aref, Arif Ghafoor, and Ahmed R. Mahmood", "title": "SWARM: Adaptive Load Balancing in Distributed Streaming Systems for Big\n  Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of GPS-enabled devices has led to the development of\nnumerous location-based services. These services need to process massive\namounts of spatial data in real-time. The current scale of spatial data cannot\nbe handled using centralized systems. This has led to the development of\ndistributed spatial streaming systems. Existing systems are using static\nspatial partitioning to distribute the workload. In contrast, the real-time\nstreamed spatial data follows non-uniform spatial distributions that are\ncontinuously changing over time. Distributed spatial streaming systems need to\nreact to the changes in the distribution of spatial data and queries. This\npaper introduces SWARM, a light-weight adaptivity protocol that continuously\nmonitors the data and query workloads across the distributed processes of the\nspatial data streaming system, and redistribute and rebalance the workloads\nsoon as performance bottlenecks get detected. SWARM is able to handle multiple\nquery-execution and data-persistence models. A distributed streaming system can\ndirectly use SWARM to adaptively rebalance the system's workload among its\nmachines with minimal changes to the original code of the underlying spatial\napplication. Extensive experimental evaluation using real and synthetic\ndatasets illustrate that, on average, SWARM achieves 200% improvement over a\nstatic grid partitioning that is determined based on observing a limited\nhistory of the data and query workloads. Moreover, SWARM reduces execution\nlatency on average 4x compared with the other technique.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 00:55:22 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Daghistani", "Anas", ""], ["Aref", "Walid G.", ""], ["Ghafoor", "Arif", ""], ["Mahmood", "Ahmed R.", ""]]}, {"id": "2002.12393", "submitter": "Tarique Siddiqui", "authors": "Tarique Siddiqui, Alekh Jindal, Shi Qiao, Hiren Patel, Wangchao le", "title": "Cost Models for Big Data Query Processing: Learning, Retrofitting, and\n  Our Findings", "comments": "To appear at SIGMOD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Query processing over big data is ubiquitous in modern clouds, where the\nsystem takes care of picking both the physical query execution plans and the\nresources needed to run those plans, using a cost-based query optimizer. A good\ncost model, therefore, is akin to better resource efficiency and lower\noperational costs. Unfortunately, the production workloads at Microsoft show\nthat costs are very complex to model for big data systems. In this work, we\ninvestigate two key questions: (i) can we learn accurate cost models for big\ndata systems, and (ii) can we integrate the learned models within the query\noptimizer. To answer these, we make three core contributions. First, we exploit\nworkload patterns to learn a large number of individual cost models and combine\nthem to achieve high accuracy and coverage over a long period. Second, we\npropose extensions to Cascades framework to pick optimal resources, i.e, number\nof containers, during query planning. And third, we integrate the learned cost\nmodels within the Cascade-style query optimizer of SCOPE at Microsoft. We\nevaluate the resulting system, Cleo, in a production environment using both\nproduction and TPC-H workloads. Our results show that the learned cost models\nare 2 to 3 orders of magnitude more accurate, and 20X more correlated with the\nactual runtimes, with a large majority (70%) of the plan changes leading to\nsubstantial improvements in latency as well as resource usage.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 19:09:33 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Siddiqui", "Tarique", ""], ["Jindal", "Alekh", ""], ["Qiao", "Shi", ""], ["Patel", "Hiren", ""], ["le", "Wangchao", ""]]}, {"id": "2002.12459", "submitter": "Shaleen Deep", "authors": "Shaleen Deep, Xiao Hu, Paraschos Koutris", "title": "Fast Join Project Query Evaluation using Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, much effort has been devoted to developing join\nalgorithms in order to achieve worst-case optimality for join queries over\nrelational databases. Towards this end, the database community has had\nconsiderable success in developing succinct algorithms that achieve worst-case\noptimal runtime for full join queries, i.e the join is over all variables\npresent in the input database. However, not much is known about join evaluation\nwith {\\em projections} beyond some simple techniques of pushing down the\nprojection operator in the query execution plan. Such queries have a large\nnumber of applications in entity matching, graph analytics and searching over\ncompressed graphs. In this paper, we study how a class of join queries with\nprojections can be evaluated faster using worst-case optimal algorithms\ntogether with matrix multiplication. Crucially, our algorithms are\nparameterized by the output size of the final result, allowing for choice of\nthe best execution strategy. We implement our algorithms as a subroutine and\ncompare the performance with state-of-the-art techniques to show they can be\nimproved upon by as much as 50x. More importantly, our experiments indicate\nthat matrix multiplication is a useful operation that can help speed up join\nprocessing owing to highly optimized open source libraries that are also highly\nparallelizable.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 21:50:40 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Deep", "Shaleen", ""], ["Hu", "Xiao", ""], ["Koutris", "Paraschos", ""]]}, {"id": "2002.12664", "submitter": "Chao Wang", "authors": "Chao Wang, Kezhao Huang, Xuehai Qian", "title": "Comprehensive Framework of RDMA-enabled Concurrency Control Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop RCC, the first unified and comprehensive\nRDMA-enabled distributed transaction processing framework supporting six\nserializable concurrency control protocols: not only the classical protocols\nNOWAIT, WAITDIE, and OCC, but also more advanced MVCC and SUNDIAL, and even\nCALVIN, the deterministic concurrency control protocol. Our goal is to\nunbiasedly compare the protocols in a common execution environment with the\nconcurrency control protocol being the only changeable component. We focus on\nthe correct and efficient implementation using key techniques, such as\nco-routines, outstanding requests, and doorbell batching, with two-sided and\none-sided communication primitives. Based on RCC, we get the deep insights that\ncannot be obtained by any existing systems. Most importantly, we obtain the\nexecution stage latency breakdowns with one-sided and two-sided primitive for\neach protocol, which are analyzed to develop more efficient hybrid\nimplementations. Our results show that three hybrid designs are indeed better\nthan both one-sided and two-sided implementations by up to 17.8%. We believe\nthat RCC is a significant advance over the state-of-the-art; it can both\nprovide performance insights and be used as the common infrastructure for fast\nprototyping new implementations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 11:47:13 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 00:49:35 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 06:19:26 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 07:55:26 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Wang", "Chao", ""], ["Huang", "Kezhao", ""], ["Qian", "Xuehai", ""]]}]