[{"id": "1810.00224", "submitter": "Luiz Gadelha Jr.", "authors": "Luiz M. R. Gadelha Jr., Pedro C. de Siracusa, Artur Ziviani, Eduardo\n  Couto Dalcin, Helen Michelle Affe, Marinez Ferreira de Siqueira, Lu\\'is\n  Alexandre Estev\\~ao da Silva, Douglas A. Augusto, Eduardo Krempser, Marcia\n  Chame, Raquel Lopes Costa, Pedro Milet Meirelles, Fabiano Thompson", "title": "A survey of biodiversity informatics: Concepts, practices, and\n  challenges", "comments": null, "journal-ref": "WIREs Data Mining and Knowledge Discovery (2020)", "doi": "10.1002/widm.1394", "report-no": null, "categories": "q-bio.PE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented size of the human population, along with its associated\neconomic activities, have an ever increasing impact on global environments.\nAcross the world, countries are concerned about the growing resource\nconsumption and the capacity of ecosystems to provide them. To effectively\nconserve biodiversity, it is essential to make indicators and knowledge openly\navailable to decision-makers in ways that they can effectively use them. The\ndevelopment and deployment of mechanisms to produce these indicators depend on\nhaving access to trustworthy data from field surveys and automated sensors,\nbiological collections, molecular data, and historic academic literature. The\ntransformation of this raw data into synthesized information that is fit for\nuse requires going through many refinement steps. The methodologies and\ntechniques used to manage and analyze this data comprise an area often called\nbiodiversity informatics (or e-Biodiversity). Biodiversity data follows a life\ncycle consisting of planning, collection, certification, description,\npreservation, discovery, integration, and analysis. Researchers, whether\nproducers or consumers of biodiversity data, will likely perform activities\nrelated to at least one of these steps. This article explores each stage of the\nlife cycle of biodiversity data, discussing its methodologies, tools, and\nchallenges.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 15:41:17 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 11:22:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Gadelha", "Luiz M. R.", "Jr."], ["de Siracusa", "Pedro C.", ""], ["Ziviani", "Artur", ""], ["Dalcin", "Eduardo Couto", ""], ["Affe", "Helen Michelle", ""], ["de Siqueira", "Marinez Ferreira", ""], ["da Silva", "Lu\u00eds Alexandre Estev\u00e3o", ""], ["Augusto", "Douglas A.", ""], ["Krempser", "Eduardo", ""], ["Chame", "Marcia", ""], ["Costa", "Raquel Lopes", ""], ["Meirelles", "Pedro Milet", ""], ["Thompson", "Fabiano", ""]]}, {"id": "1810.00326", "submitter": "Wahyudi Wahyudi", "authors": "Wahyudi, Masayu Leylia Khodra, Ary Setijadi Prihatmanto, Carmadi\n  Machbub", "title": "Using Graph-Pattern Association Rules On Yago Knowledge Base", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of Graph-Pattern Association Rules (GPARs) on the Yago\nknowledge base. Extending association rules for itemsets, GPARS can help to\ndiscover regularities between entities in knowledge bases. A rule-generated\ngraph pattern (RGGP) algorithm was used for extracting rules from the Yago\nknowledge base and a graph-pattern association rules algorithm for creating\nassociation rules. Our research resulted in 1114 association rules, where the\nvalue of standard confidence at 50.18% was better than partial completeness\nassumption (PCA) confidence at 49.82%. Besides that the computation time for\nstandard confidence was also better than for PCA confidence\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 06:42:18 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Wahyudi", "", ""], ["Khodra", "Masayu Leylia", ""], ["Prihatmanto", "Ary Setijadi", ""], ["Machbub", "Carmadi", ""]]}, {"id": "1810.00511", "submitter": "Feilong Liu", "authors": "Feilong Liu, Ario Salmasi, Spyros Blanas, Anastasios Sidiropoulos", "title": "Chasing Similarity: Distribution-aware Aggregation Scheduling (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel aggregation is a ubiquitous operation in data analytics that is\nexpressed as GROUP BY in SQL, reduce in Hadoop, or segment in TensorFlow.\nParallel aggregation starts with an optional local pre-aggregation step and\nthen repartitions the intermediate result across the network. While local\npre-aggregation works well for low-cardinality aggregations, the network\ncommunication cost remains significant for high-cardinality aggregations even\nafter local pre-aggregation. The problem is that the repartition-based\nalgorithm for high-cardinality aggregation does not fully utilize the network.\n  In this work, we first formulate a mathematical model that captures the\nperformance of parallel aggregation. We prove that finding optimal aggregation\nplans from a known data distribution is NP-hard, assuming the Small Set\nExpansion conjecture. We propose GRASP, a GReedy Aggregation Scheduling\nProtocol that decomposes parallel aggregation into phases. GRASP is\ndistribution-aware as it aggregates the most similar partitions in each phase\nto reduce the transmitted data size in subsequent phases. In addition, GRASP\ntakes the available network bandwidth into account when scheduling aggregations\nin each phase to maximize network utilization. The experimental evaluation on\nreal data shows that GRASP outperforms repartition-based aggregation by 3.5x\nand LOOM by 2.0x.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 02:58:34 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 16:16:02 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Liu", "Feilong", ""], ["Salmasi", "Ario", ""], ["Blanas", "Spyros", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "1810.01037", "submitter": "Jialin Qiao", "authors": "Jialin Qiao, Xiangdong Huang, Lei Rui and Jianmin Wang", "title": "Heterogeneous Replica for Query on Cassandra", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cassandra is a popular structured storage system with high-performance,\nscalability and high availability, and is usually used to store data that has\nsome sortable attributes. When deploying and configuring Cassandra, it is\nimportant to design a suitable schema of column families for accelerating the\ntarget queries. However, one schema is only suitable for a part of queries, and\nleaves other queries with high latency.\n  In this paper, we propose a new replica mechanism, called heterogeneous\nreplica, to reduce the query latency greatly while ensuring high write\nthroughput and data recovery. With this replica mechanism, different replica\nhas the same dataset while having different serialization on disk. By\nimplementing the heterogeneous replica mechanism on Cassandra, we show that the\nread performance of Cassandra can be improved by two orders of magnitude with\nTPC-H data set.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 02:31:42 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Qiao", "Jialin", ""], ["Huang", "Xiangdong", ""], ["Rui", "Lei", ""], ["Wang", "Jianmin", ""]]}, {"id": "1810.01538", "submitter": "Andee Kaplan", "authors": "Andee Kaplan, Brenda Betancourt, and Rebecca C. Steorts", "title": "Posterior Prototyping: Bridging the Gap between Bayesian Record Linkage\n  and Regression", "comments": "23 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage (entity resolution or de-deduplication) is the process of\nmerging noisy databases to remove duplicate entities. While record linkage\nremoves duplicate entities from the data, many researchers are interested in\nperforming inference, prediction or post-linkage analysis on the linked data,\nwhich we call the downstream task. Depending on the downstream task, one may\nwish to find the most representative record before performing the post-linkage\nanalysis. Motivated by the downstream task, we propose first performing record\nlinkage using a Bayesian model and then choosing representative records through\nprototyping. Given the information about the representative records, we then\nexplore two downstream tasks - linear regression and binary classification via\nlogistic regression. In addition, we explore how error propagation occurs in\nboth of these settings. We provide thorough empirical studies for our proposed\nmethodology, and conclude with a discussion of practical insights into our\nwork.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 22:55:58 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Kaplan", "Andee", ""], ["Betancourt", "Brenda", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1810.01698", "submitter": "Marc Shapiro", "authors": "Alejandro Z. Tomsic (DELYS), Manuel Bravo, Marc Shapiro (DELYS)", "title": "Distributed transactional reads: the strong, the quick, the fresh \\& the\n  impossible", "comments": null, "journal-ref": "ACM. 2018 ACM/IFIP/USENIX International Middleware Conference, Dec\n  2018, Rennes, France. ACM, 2018 ACM/IFIP/USENIX International Middleware\n  Conference, pp.14, 0018, Proceedings of 2018 ACM/IFIP/USENIX International\n  Middleware Conference. http://2018.middleware-conference.org", "doi": "10.1145/3274808.3274818", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the costs and trade-offs of providing transactional\nconsistent reads in a distributed storage system. We identify the following\ndimensions: read consistency, read delay (latency), and data freshness. We show\nthat there is a three-way trade-off between them, which can be summarised as\nfollows: (i) it is not possible to ensure at the same time order-preserving\n(e.g., causally-consistent) or atomic reads, Minimal Delay, and maximal\nfreshness; thus, reading data that is the most fresh without delay is possible\nonly in a weakly-isolated mode; (ii) to ensure atomic or order-preserving reads\nat Minimal Delay imposes to read data from the past (not fresh); (iii) however,\norder-preserving minimal-delay reads can be fresher than atomic; (iv) reading\natomic or order-preserving data at maximal freshness may block reads or writes\nindefinitely. Our impossibility results hold independently of other features of\nthe database, such as update semantics (totally ordered or not) or data model\n(structured or unstructured). Guided by these results, we modify an existing\nprotocol to ensure minimal-delay reads (at the cost of freshness) under\natomic-visibility and causally-consistent semantics. Our experimental\nevaluation supports the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 11:51:44 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Tomsic", "Alejandro Z.", "", "DELYS"], ["Bravo", "Manuel", "", "DELYS"], ["Shapiro", "Marc", "", "DELYS"]]}, {"id": "1810.01794", "submitter": "Tiantu Xu", "authors": "Tiantu Xu, Luis Materon Botelho, Felix Xiaozhu Lin", "title": "VStore: A Data Store for Analytics on Large Videos", "comments": null, "journal-ref": null, "doi": "10.1145/3302424.3303971", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VStore, a data store for supporting fast, resource-efficient\nanalytics over large archival videos. VStore manages video ingestion, storage,\nretrieval, and consumption. It controls video formats along the video data\npath. It is challenged by i) the huge combinatorial space of video format\nknobs; ii) the complex impacts of these knobs and their high profiling cost;\niii) optimizing for multiple resource types. It explores an idea called\nbackward derivation of configuration: in the opposite direction along the video\ndata path, VStore passes the video quantity and quality expected by analytics\nbackward to retrieval, to storage, and to ingestion. In this process, VStore\nderives an optimal set of video formats, optimizing for different resources in\na progressive manner. VStore automatically derives large, complex\nconfigurations consisting of more than one hundred knobs over tens of video\nformats. In response to queries, VStore selects video formats catering to the\nexecuted operators and the target accuracy. It streams video data from disks\nthrough decoder to operators. It runs queries as fast as 362x of video\nrealtime.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 15:31:36 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 21:03:20 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 19:31:41 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Xu", "Tiantu", ""], ["Botelho", "Luis Materon", ""], ["Lin", "Felix Xiaozhu", ""]]}, {"id": "1810.01816", "submitter": "Johes Bater", "authors": "Johes Bater, Xi He, William Ehrich, Ashwin Machanavajjhala, Jennie\n  Rogers", "title": "Shrinkwrap: Differentially-Private Query Processing in Private Data\n  Federations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A private data federation is a set of autonomous databases that share a\nunified query interface offering in-situ evaluation of SQL queries over the\nunion of the sensitive data of its members. Owing to privacy concerns, these\nsystems do not have a trusted data collector that can see all their data and\ntheir member databases cannot learn about individual records of other engines.\nFederations currently achieve this goal by evaluating queries obliviously using\nsecure multiparty computation. This hides the intermediate result cardinality\nof each query operator by exhaustively padding it. With cascades of such\noperators, this padding accumulates to a blow-up in the output size of each\noperator and a proportional loss in query performance. Hence, existing private\ndata federations do not scale well to complex SQL queries over large datasets.\n  We introduce Shrinkwrap, a private data federation that offers data owners a\ndifferentially private view of the data held by others to improve their\nperformance over oblivious query processing. Shrinkwrap uses computational\ndifferential privacy to minimize the padding of intermediate query results,\nachieving up to 35X performance improvement over oblivious query processing.\nWhen the query needs differentially private output, Shrinkwrap provides a\ntrade-off between result accuracy and query evaluation performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 16:09:14 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Bater", "Johes", ""], ["He", "Xi", ""], ["Ehrich", "William", ""], ["Machanavajjhala", "Ashwin", ""], ["Rogers", "Jennie", ""]]}, {"id": "1810.01997", "submitter": "Guna Prasaad", "authors": "Guna Prasaad, Alvin Cheung, Dan Suciu", "title": "Improving High Contention OLTP Performance via Transaction Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in transaction processing has made significant progress in improving\nthe performance of multi-core in-memory transactional systems. However, the\nfocus has mainly been on low-contention workloads. Modern transactional systems\nperform poorly on workloads with transactions accessing a few highly contended\ndata items. We observe that most transactional workloads, including those with\nhigh contention, can be divided into clusters of data conflict-free\ntransactions and a small set of residuals. In this paper, we introduce a new\nconcurrency control protocol called Strife that leverages the above\nobservation. Strife executes transactions in batches, where each batch is\npartitioned into clusters of conflict-free transactions and a small set of\nresidual transactions. The conflict-free clusters are executed in parallel\nwithout any concurrency control, followed by executing the residual cluster\neither serially or with concurrency control. We present a low-overhead\nalgorithm that partitions a batch of transactions into clusters that do not\nhave cross-cluster conflicts and a small residual cluster. We evaluate Strife\nagainst the optimistic concurrency control protocol and several variants of\ntwo-phase locking, where the latter is known to perform better than other\nconcurrency protocols under high contention, and show that Strife can improve\ntransactional throughput by up to 2x. We also perform an in-depth\nmicro-benchmark analysis to empirically characterize the performance and\nquality of our clustering algorithm\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 22:53:29 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Prasaad", "Guna", ""], ["Cheung", "Alvin", ""], ["Suciu", "Dan", ""]]}, {"id": "1810.02270", "submitter": "Yong Tan", "authors": "Yong Tan", "title": "Compound Binary Search Tree and Algorithms", "comments": "8 pages with words into 6300", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Binary Search Tree (BST) is average in computer science which supports a\ncompact data structure in memory and oneself even conducts a row of quick\nalgorithms, by which people often apply it in dynamical circumstance. Besides\nthese edges, it is also with weakness on its own structure specially with poor\nperformance at worst case. In this paper, we will develop this data structure\ninto a synthesis to show a series of novel features residing in. Of that, there\nare new methods invented for raising the performance and efficiency\nnevertheless some existing ones in logarithm or linear time.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 15:11:47 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Tan", "Yong", ""]]}, {"id": "1810.02935", "submitter": "Chris Liu", "authors": "Chris Liu, Pengfei Zhang, Bo Tang, Hang Shen, Lei Zhu, Ziliang Lai,\n  Eric Lo", "title": "Towards Self-Tuning Parameter Servers", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years, many applications have been driven advances by the use of\nMachine Learning (ML). Nowadays, it is common to see industrial-strength\nmachine learning jobs that involve millions of model parameters, terabytes of\ntraining data, and weeks of training. Good efficiency, i.e., fast completion\ntime of running a specific ML job, therefore, is a key feature of a successful\nML system. While the completion time of a long-running ML job is determined by\nthe time required to reach model convergence, practically that is also largely\ninfluenced by the values of various system settings. In this paper, we\ncontribute techniques towards building self-tuning parameter servers. Parameter\nServer (PS) is a popular system architecture for large-scale machine learning\nsystems; and by self-tuning we mean while a long-running ML job is iteratively\ntraining the expert-suggested model, the system is also iteratively learning\nwhich system setting is more efficient for that job and applies it online.\nWhile our techniques are general enough to various PS-style ML systems, we have\nprototyped our techniques on top of TensorFlow. Experiments show that our\ntechniques can reduce the completion times of a variety of long-running\nTensorFlow jobs from 1.4x to 18x.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 05:12:23 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 14:51:00 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Liu", "Chris", ""], ["Zhang", "Pengfei", ""], ["Tang", "Bo", ""], ["Shen", "Hang", ""], ["Zhu", "Lei", ""], ["Lai", "Ziliang", ""], ["Lo", "Eric", ""]]}, {"id": "1810.03367", "submitter": "Johannes Doleschal", "authors": "Johannes Doleschal and Benny Kimelfeld and Wim Martens and Frank Neven\n  and Matthias Niewerth", "title": "Split-Correctness in Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs for extracting structured information from text, namely information\nextractors, often operate separately on document segments obtained from a\ngeneric splitting operation such as sentences, paragraphs, k-grams, HTTP\nrequests, and so on. An automated detection of this behavior of extractors,\nwhich we refer to as split-correctness, would allow text analysis systems to\ndevise query plans with parallel evaluation on segments for accelerating the\nprocessing of large documents. Other applications include the incremental\nevaluation on dynamic content, where re-evaluation of information extractors\ncan be restricted to revised segments, and debugging, where developers of\ninformation extractors are informed about potential boundary crossing of\ndifferent semantic components. We propose a new formal framework for\nsplit-correctness within the formalism of document spanners. Our analysis\nstudies the complexity of split-correctness over regular spanners. We also\ndiscuss different variants of split-correctness, for instance, in the presence\nof black-box extractors with split constraints.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 11:03:40 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 11:54:04 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Doleschal", "Johannes", ""], ["Kimelfeld", "Benny", ""], ["Martens", "Wim", ""], ["Neven", "Frank", ""], ["Niewerth", "Matthias", ""]]}, {"id": "1810.03386", "submitter": "Jef Wijsen", "authors": "Paraschos Koutris and Jef Wijsen", "title": "Consistent Query Answering for Primary Keys in Logspace", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of consistent query answering on databases that may\nviolate primary key constraints. A repair of such a database is any consistent\ndatabase that can be obtained by deleting a minimal set of tuples. For every\nBoolean query q, CERTAINTY(q) is the problem that takes a database as input and\nasks whether q evaluates to true on every repair. In [KW17], the authors show\nthat for every self-join-free Boolean conjunctive query q, the problem\nCERTAINTY(q) is either in P or coNP-complete, and it is decidable which of the\ntwo cases applies. In this paper, we sharpen this result by showing that for\nevery self-join-free Boolean conjunctive query q, the problem CERTAINTY(q) is\neither expressible in symmetric stratified Datalog or coNP-complete. Since\nsymmetric stratified Datalog is in L, we thus obtain a complexity-theoretic\ndichotomy between L and coNP-complete. Another new finding of practical\nimportance is that CERTAINTY(q) is on the logspace side of the dichotomy for\nqueries q where all join conditions express foreign-to-primary key matches,\nwhich is undoubtedly the most common type of join condition.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 11:50:20 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Koutris", "Paraschos", ""], ["Wijsen", "Jef", ""]]}, {"id": "1810.04599", "submitter": "Hui Miao", "authors": "Hui Miao and Amol Deshpande", "title": "Understanding Data Science Lifecycle Provenance via Graph Segmentation\n  and Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly modern data science platforms today have non-intrusive and\nextensible provenance ingestion mechanisms to collect rich provenance and\ncontext information, handle modifications to the same file using\ndistinguishable versions, and use graph data models (e.g., property graphs) and\nquery languages (e.g., Cypher) to represent and manipulate the stored\nprovenance/context information. Due to the schema-later nature of the metadata,\nmultiple versions of the same files, and unfamiliar artifacts introduced by\nteam members, the \"provenance graph\" is verbose and evolving, and hard to\nunderstand; using standard graph query model, it is difficult to compose\nqueries and utilize this valuable information.\n  In this paper, we propose two high-level graph query operators to address the\nverboseness and evolving nature of such provenance graphs. First, we introduce\na graph segmentation operator, which queries the retrospective provenance\nbetween a set of source vertices and a set of destination vertices via flexible\nboundary criteria to help users get insight about the derivation relationships\namong those vertices. We show the semantics of such a query in terms of a\ncontext-free grammar, and develop efficient algorithms that run orders of\nmagnitude faster than state-of-the-art. Second, we propose a graph\nsummarization operator that combines similar segments together to query\nprospective provenance of the underlying project. The operator allows tuning\nthe summary by ignoring vertex details and characterizing local structures, and\nensures the provenance meaning using path constraints. We show the optimal\nsummary problem is PSPACE-complete and develop effective approximation\nalgorithms. The operators are implemented on top of a property graph backend.\nWe evaluate our query methods extensively and show the effectiveness and\nefficiency of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 15:40:27 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 05:05:07 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Miao", "Hui", ""], ["Deshpande", "Amol", ""]]}, {"id": "1810.04604", "submitter": "Vuong M. Ngo", "authors": "Sven Helmer and Vuong M. Ngo", "title": "A Similarity Measure for Weaving Patterns in Textiles", "comments": "10 papes, will be published in SIGIR 2015", "journal-ref": "SIGIR 2015", "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for measuring the similarity between weaving\npatterns that can provide similarity-based search functionality for textile\narchives. We represent textile structures using hypergraphs and extract\nmultisets of k-neighborhoods from these graphs. The resulting multisets are\nthen compared using Jaccard coefficients, Hamming distances, and cosine\nmeasures. We evaluate the different variants of our similarity measure\nexperimentally, showing that it can be implemented efficiently and illustrating\nits quality using it to cluster and query a data set containing more than a\nthousand textile samples.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 15:50:03 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Helmer", "Sven", ""], ["Ngo", "Vuong M.", ""]]}, {"id": "1810.04915", "submitter": "Liang Li", "authors": "Liang Li, Guoren Wang, Gang Wu, Ye Yuan, Lei Chen, and Xiang Lian", "title": "A Comparative Study of Consistent Snapshot Algorithms for Main-Memory\n  Database Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-memory databases (IMDBs) are gaining increasing popularity in big data\napplications, where clients commit updates intensively. Specifically, it is\nnecessary for IMDBs to have efficient snapshot performance to support certain\nspecial applications (e.g., consistent checkpoint, HTAP). Formally, the\nin-memory consistent snapshot problem refers to taking an in-memory consistent\ntime-in-point snapshot with the constraints that 1) clients can read the latest\ndata items and 2) any data item in the snapshot should not be overwritten.\nVarious snapshot algorithms have been proposed in academia to trade off\nthroughput and latency, but industrial IMDBs such as Redis adhere to the simple\nfork algorithm. To understand this phenomenon, we conduct comprehensive\nperformance evaluations on mainstream snapshot algorithms. Surprisingly, we\nobserve that the simple fork algorithm indeed outperforms the state-of-the-arts\nin update-intensive workload scenarios. On this basis, we identify the\ndrawbacks of existing research and propose two lightweight improvements.\nExtensive evaluations on synthetic data and Redis show that our lightweight\nimprovements yield better performance than fork, the current industrial\nstandard, and the representative snapshot algorithms from academia. Finally, we\nhave opensourced the implementation of all the above snapshot algorithms so\nthat practitioners are able to benchmark the performance of each algorithm and\nselect proper methods for different application scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 09:18:13 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Li", "Liang", ""], ["Wang", "Guoren", ""], ["Wu", "Gang", ""], ["Yuan", "Ye", ""], ["Chen", "Lei", ""], ["Lian", "Xiang", ""]]}, {"id": "1810.05357", "submitter": "Ian Davidson", "authors": "Chia-Tung Kuo and Ian Davidson", "title": "On The Equivalence of Tries and Dendrograms - Efficient Hierarchical\n  Clustering of Traffic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread use of GPS-enabled devices generates voluminous and continuous\namounts of traffic data but analyzing such data for interpretable and\nactionable insights poses challenges. A hierarchical clustering of the trips\nhas many uses such as discovering shortest paths, common routes and often\ntraversed areas. However, hierarchical clustering typically has time complexity\nof $O(n^2 \\log n)$ where $n$ is the number of instances, and is difficult to\nscale to large data sets associated with GPS data. Furthermore, incremental\nhierarchical clustering is still a developing area. Prefix trees (also called\ntries) can be efficiently constructed and updated in linear time (in $n$). We\nshow how a specially constructed trie can compactly store the trips and further\nshow this trie is equivalent to a dendrogram that would have been built by\nclassic agglomerative hierarchical algorithms using a specific distance metric.\nThis allows creating hierarchical clusterings of GPS trip data and updating\nthis hierarchy in linear time. %we can extract a meaningful kernel and can also\ninterpret the structure as clusterings of differing granularity as one\nprogresses down the tree. We demonstrate the usefulness of our proposed\napproach on a real world data set of half a million taxis' GPS traces, well\nbeyond the capabilities of agglomerative clustering methods. Our work is not\nlimited to trip data and can be used with other data with a string\nrepresentation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 05:02:29 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Kuo", "Chia-Tung", ""], ["Davidson", "Ian", ""]]}, {"id": "1810.05497", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts and Anshumali Shrivastava", "title": "Probabilistic Blocking with An Application to the Syrian Conflict", "comments": "16 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1510.07714, arXiv:1710.02690", "journal-ref": "Steorts R.C., Shrivastava A. (2018) Probabilistic Blocking with an\n  Application to the Syrian Conflict. PSD (2018)", "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution seeks to merge databases as to remove duplicate entries\nwhere unique identifiers are typically unknown. We review modern blocking\napproaches for entity resolution, focusing on those based upon locality\nsensitive hashing (LSH). First, we introduce $k$-means locality sensitive\nhashing (KLSH), which is based upon the information retrieval literature and\nclusters similar records into blocks using a vector-space representation and\nprojections. Second, we introduce a subquadratic variant of LSH to the\nliterature, known as Densified One Permutation Hashing (DOPH). Third, we\npropose a weighted variant of DOPH. We illustrate each method on an application\nto a subset of the ongoing Syrian conflict, giving a discussion of each method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 01:16:31 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1810.05570", "submitter": "Souad Bouasker", "authors": "Souad Bouasker", "title": "Characterization and extraction of condensed representation of\n  correlated patterns based on formal concept analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated pattern mining has increasingly become an important task in data\nmining since these patterns allow conveying knowledge about meaningful and\nsurprising relations among data. Frequent correlated patterns were thoroughly\nstudied in the literature. In this thesis, we propose to benefit from both\nfrequent correlated as well as rare correlated patterns according to the bond\ncorrelation measure. We propose to extract a subset without information loss of\nthe sets of frequent correlated and of rare correlated patterns, this subset is\ncalled ``Condensed Representation``. In this regard, we are based on the\nnotions derived from the Formal Concept Analysis FCA, specifically the\nequivalence classes associated to a closure operator fbond dedicated to the\nbond measure, to introduce new concise representations of both frequent\ncorrelated and rare correlated patterns.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 15:16:47 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Bouasker", "Souad", ""]]}, {"id": "1810.05972", "submitter": "Xun Jian", "authors": "Xun Jian, Yue Wang, Xiayu Lei, Yanyan Shen, Lei Chen", "title": "DDSL: Efficient Subgraph Listing on Distributed and Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph listing is a fundamental problem in graph theory and has wide\napplications in areas like sociology, chemistry, and social networks. Modern\ngraphs can usually be large-scale as well as highly dynamic, which challenges\nthe efficiency of existing subgraph listing algorithms. Recent works have shown\nthe benefits of partitioning and processing big graphs in a distributed system,\nhowever, there is only few work targets subgraph listing on dynamic graphs in a\ndistributed environment. In this paper, we propose an efficient approach,\ncalled Distributed and Dynamic Subgraph Listing (DDSL), which can incrementally\nupdate the results instead of running from scratch. DDSL follows a general\ndistributed join framework. In this framework, we use a Neighbor-Preserved\nstorage for data graphs, which takes bounded extra space and supports dynamic\nupdating. After that, we propose a comprehensive cost model to estimate the I/O\ncost of listing subgraphs. Then based on this cost model, we develop an\nalgorithm to find the optimal join tree for a given pattern. To handle dynamic\ngraphs, we propose an efficient left-deep join algorithm to incrementally\nupdate the join results. Extensive experiments are conducted on real-world\ndatasets. The results show that DDSL outperforms existing methods in dealing\nwith both static dynamic graphs in terms of the responding time.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 04:58:58 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 06:28:52 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 13:17:46 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Jian", "Xun", ""], ["Wang", "Yue", ""], ["Lei", "Xiayu", ""], ["Shen", "Yanyan", ""], ["Chen", "Lei", ""]]}, {"id": "1810.06021", "submitter": "Alejandro Alcalde-Barros", "authors": "Alejandro Alcalde-Barros, Diego Garc\\'ia-Gil, Salvador Garc\\'ia,\n  Francisco Herrera", "title": "DPASF: A Flink Library for Streaming Data preprocessing", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data preprocessing techniques are devoted to correct or alleviate errors in\ndata. Discretization and feature selection are two of the most extended data\npreprocessing techniques. Although we can find many proposals for static Big\nData preprocessing, there is little research devoted to the continuous Big Data\nproblem. Apache Flink is a recent and novel Big Data framework, following the\nMapReduce paradigm, focused on distributed stream and batch data processing. In\nthis paper we propose a data stream library for Big Data preprocessing, named\nDPASF, under Apache Flink. We have implemented six of the most popular data\npreprocessing algorithms, three for discretization and the rest for feature\nselection. The algorithms have been tested using two Big Data datasets.\nExperimental results show that preprocessing can not only reduce the size of\nthe data, but to maintain or even improve the original accuracy in a short\ntime. DPASF contains useful algorithms when dealing with Big Data data streams.\nThe preprocessing algorithms included in the library are able to tackle Big\nDatasets efficiently and to correct imperfections in the data.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 11:59:18 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Alcalde-Barros", "Alejandro", ""], ["Garc\u00eda-Gil", "Diego", ""], ["Garc\u00eda", "Salvador", ""], ["Herrera", "Francisco", ""]]}, {"id": "1810.06742", "submitter": "Abolfazl Asudeh", "authors": "Abolfazl Asudeh, Zhongjun Jin, H. V. Jagadish", "title": "Assessing and Remedying Coverage for a Given Dataset", "comments": "in ICDE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis impacts virtually every aspect of our society today. Often,\nthis analysis is performed on an existing dataset, possibly collected through a\nprocess that the data scientists had limited control over. The existing data\nanalyzed may not include the complete universe, but it is expected to cover the\ndiversity of items in the universe. Lack of adequate coverage in the dataset\ncan result in undesirable outcomes such as biased decisions and algorithmic\nracism, as well as creating vulnerabilities such as opening up room for\nadversarial attacks.\n  In this paper, we assess the coverage of a given dataset over multiple\ncategorical attributes. We first provide efficient techniques for traversing\nthe combinatorial explosion of value combinations to identify any regions of\nattribute space not adequately covered by the data. Then, we determine the\nleast amount of additional data that must be obtained to resolve this lack of\nadequate coverage. We confirm the value of our proposal through both\ntheoretical analyses and comprehensive experiments on real data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 22:59:14 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 23:26:29 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Asudeh", "Abolfazl", ""], ["Jin", "Zhongjun", ""], ["Jagadish", "H. V.", ""]]}, {"id": "1810.06911", "submitter": "Mario Lezoche", "authors": "Mario Lezoche (CRAN), Herv\\'e Panetto (CRAN)", "title": "Cyber-Physical Systems, a new formal paradigm to model redundancy and\n  resiliency", "comments": null, "journal-ref": "Enterprise Information Systems, Taylor \\& Francis, 2018", "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-Physical Systems (CPS) are systems composed by a physical component\nthat is controlled or monitored by a cyber-component, a computer-based\nalgorithm. Advances in CPS technologies and science are enabling capability,\nadaptability, scalability, resiliency, safety, security, and usability that\nwill far exceed the simple embedded systems of today. CPS technologies are\ntransforming the way people interact with engineered systems. New smart CPS are\ndriving innovation in various sectors such as agriculture, energy,\ntransportation, healthcare, and manufacturing. They are leading the 4-th\nIndustrial Revolution (Industry 4.0) that is having benefits thanks to the high\nflexibility of production. The Industry 4.0 production paradigm is\ncharacterized by high intercommunicating properties of its production elements\nin all the manufacturing processes. This is the reason it is a core concept how\nthe systems should be structurally optimized to have the adequate level of\nredundancy to be satisfactorily resilient. This goal can benefit from formal\nmethods well known in various scientific domains such as artificial\nintelligence. So, the current research concerns the proposal of a CPS\nmeta-model and its instantiation. In this way it lists all kind of\nrelationships that may occur between the CPSs themselves and between their\n(cyber-and physical-) components. Using the CPS meta-model formalization, with\nan adaptation of the Formal Concept Analysis (FCA) formal approach, this paper\npresents a way to optimize the modelling of CPS systems emphasizing their\nredundancy and their resiliency.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 10:10:06 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Lezoche", "Mario", "", "CRAN"], ["Panetto", "Herv\u00e9", "", "CRAN"]]}, {"id": "1810.07116", "submitter": "Souad Bouasker", "authors": "Souad Bouasker", "title": "Motifs corr\\'el\\'es rares : Caract\\'erisation et nouvelles\n  repr\\'esentations concises", "comments": "in French. Master's thesis 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, rare pattern mining proves to be of added-value in different data\nmining applications since these patterns allow conveying knowledge on rare and\nunexpected events. However, the extraction of rare patterns suffers from two\nmain limits, namely the large number of mined patterns in real-life\napplications, as well as the low informativeness quality of several rare\npatterns. In this situation, we propose to use the correlation measure, bond,\nin the mining process in order to only retain those rare patterns having a\ncertain degree of correlation between their respective items. A\ncharacterization of the resulting set, of rare correlated patterns, is then\ncarried out based on the study of constraints of distinct types induced by the\nrarity and the correlation. In addition, based on the equivalence classes\nassociated to a closure operator dedicated to the bond measure, we propose\nconcise representations of rare correlated patterns. We then design a new\nalgorithm CRP_Miner dedicated to the extraction of the whole set of rare\ncorrelated patterns. We also introduce the CRPR_Miner algorithm allowing an\nefficient extraction of the proposed concise representations. In addition, we\ndesign two other algorithms which allow to us the query and the regeneration of\nthe whole set of rare correlated patterns. The carried out experimental studies\nshow the effectiveness of the algorithm CRPR_Miner and prove the compactness\nrate offered by the proposed concise representations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 16:17:45 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Bouasker", "Souad", ""]]}, {"id": "1810.07355", "submitter": "Masajiro Iwasaki", "authors": "Masajiro Iwasaki and Daisuke Miyazaki", "title": "Optimization of Indexing Based on k-Nearest Neighbor Graph for Proximity\n  Search in High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for high-dimensional vector data with high accuracy is an\ninevitable search technology for various types of data. Graph-based indexes are\nknown to reduce the query time for high-dimensional data. To further improve\nthe query time by using graphs, we focused on the indegrees and outdegrees of\ngraphs. While a sufficient number of incoming edges (indegrees) are\nindispensable for increasing search accuracy, an excessive number of outgoing\nedges (outdegrees) should be suppressed so as to not increase the query time.\nTherefore, we propose three degree-adjustment methods: static degree adjustment\nof not only outdegrees but also indegrees, dynamic degree adjustment with which\noutdegrees are determined by the search accuracy users require, and path\nadjustment to remove edges that have alternative search paths to reduce\noutdegrees. We also show how to obtain optimal degree-adjustment parameters and\nthat our methods outperformed previous methods for image and textual data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 02:22:34 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Iwasaki", "Masajiro", ""], ["Miyazaki", "Daisuke", ""]]}, {"id": "1810.07822", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli and Michael Benedikt", "title": "When Can We Answer Queries Using Result-Bounded Data Interfaces?", "comments": "75 pages; journal version of the PODS'18 paper arXiv:1706.07936. Many\n  errors fixed relative to the previous version, and some erroneous results\n  removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider answering queries on data available through access methods, that\nprovide lookup access to the tuples matching a given binding. Such interfaces\nare common on the Web; further, they often have bounds on how many results they\ncan return, e.g., because of pagination or rate limits. We thus study\nresult-bounded methods, which may return only a limited number of tuples. We\nstudy how to decide if a query is answerable using result-bounded methods,\ni.e., how to compute a plan that returns all answers to the query using the\nmethods, assuming that the underlying data satisfies some integrity\nconstraints. We first show how to reduce answerability to a query containment\nproblem with constraints. Second, we show \"schema simplification\" theorems\ndescribing when and how result-bounded services can be used. Finally, we use\nthese theorems to give decidability and complexity results about answerability\nfor common constraint classes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 22:32:15 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 17:49:56 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Amarilli", "Antoine", ""], ["Benedikt", "Michael", ""]]}, {"id": "1810.08047", "submitter": "Sepanta Zeighami", "authors": "Sepanta Zeighami and Raymong Chi-Wing Wong", "title": "Finding Average Regret Ratio Minimizing Set in Database", "comments": "Submitted to ICDE '19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting a certain number of data points (or records) from a database which\n\"best\" satisfy users' expectations is a very prevalent problem with many\napplications. One application is a hotel booking website showing a certain\nnumber of hotels on a single page. However, this problem is very challenging\nsince the selected points should \"collectively\" satisfy the expectation of all\nusers. Showing a certain number of data points to a single user could decrease\nthe satisfaction of a user because the user may not be able to see his/her\nfavorite point which could be found in the original database. In this paper, we\nwould like to find a set of k points such that on average, the satisfaction\n(ratio) of a user is maximized. This problem takes into account the probability\ndistribution of the users and considers the satisfaction (ratio) of all users,\nwhich is more reasonable in practice, compared with the existing studies that\nonly consider the worst-case satisfaction (ratio) of the users, which may not\nreflect the whole population and is not useful in some applications. Motivated\nby this, in this paper, we propose algorithms for this problem. Finally, we\nconducted experiments to show the effectiveness and the efficiency of the\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:42:04 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Zeighami", "Sepanta", ""], ["Wong", "Raymong Chi-Wing", ""]]}, {"id": "1810.08062", "submitter": "Andrey Rivkin", "authors": "Diego Calvanese, Marco Montali, Fabio Patrizi, Andrey Rivkin", "title": "Modeling and In-Database Management of Relational, Data-Aware Processes\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last two decades, it has been increasingly acknowledged that the\nengineering of information systems usually requires a huge effort in\nintegrating master data and business processes. This has led to a plethora of\nproposals, both from academia and the industry. However, such approaches\ntypically come with ad-hoc abstractions to represent and interact with the data\ncomponent. This has a twofold disadvantage. On the one hand, they cannot be\nused to effortlessly enrich an existing relational database with dynamics. On\nthe other hand, they generally do not allow for integrated modelling,\nverification, and enactment. We attack these two challenges by proposing a\ndeclarative approach, fully grounded in SQL, that supports the agile modelling\nof relational data-aware processes directly on top of relational databases. We\nshow how this approach can be automatically translated into a concrete\nprocedural SQL dialect, executable directly inside any relational database\nengine. The translation exploits an in-database representation of process\nstates that, in turn, is used to handle, at once, process enactment with or\nwithout logging of the executed instances, as well as process verification. The\napproach has been implemented in a working prototype.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:57:49 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 16:06:15 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 22:47:18 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Calvanese", "Diego", ""], ["Montali", "Marco", ""], ["Patrizi", "Fabio", ""], ["Rivkin", "Andrey", ""]]}, {"id": "1810.08755", "submitter": "Mingxi Wu", "authors": "Mingxi Wu", "title": "Property Graph Type System and Data Definition Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property graph manages data by vertices and edges. Each vertex and edge can\nhave a property map, storing ad hoc attribute and its value. Label can be\nattached to vertices and edges to group them. While this schema-less\nmethodology is very flexible for data evolvement and for managing explosive\ngraph element, it has two shortcomings-- 1) data dependency 2) less\ncompression. Both problems can be solved by a schema based approach. In this\npaper, a type system used to model property graph is defined. Based on the type\nsystem, the associated data definition language (DDL) is proposed and multiple\ngraph instances created under this type system is discussed.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 05:58:48 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 21:51:44 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2018 14:38:39 GMT"}, {"version": "v4", "created": "Fri, 19 Apr 2019 09:06:37 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Wu", "Mingxi", ""]]}, {"id": "1810.08833", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Qin Zhang", "title": "MinJoin: Efficient Edit Similarity Joins via Local Hash Minima", "comments": "Accepted to KDD 2019, full version, 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing similarity joins under edit distance on a\nset of strings. Edit similarity joins is a fundamental problem in databases,\ndata mining and bioinformatics. It finds important applications in data\ncleaning and integration, collaborative filtering, genome sequence assembly,\netc. This problem has attracted significant attention in the past two decades.\nHowever, all previous algorithms either cannot scale well to long strings and\nlarge similarity thresholds, or suffer from imperfect accuracy.\n  In this paper we propose a new algorithm for edit similarity joins using a\nnovel string partition based approach. We show mathematically that with high\nprobability our algorithm achieves a perfect accuracy, and runs in linear time\nplus a data-dependent verification step. Experiments on real world datasets\nshow that our algorithm significantly outperforms the state-of-the-art\nalgorithms for edit similarity joins, and achieves perfect accuracy on all the\ndatasets that we have tested.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 17:54:37 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 05:10:14 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Zhang", "Haoyu", ""], ["Zhang", "Qin", ""]]}, {"id": "1810.09007", "submitter": "Sanket Vaibhav Mehta", "authors": "Sanket Vaibhav Mehta, Shagun Sodhani, Dhaval Patel", "title": "Spatial Co-location Pattern Mining - A new perspective using Graph\n  Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial co-location pattern mining refers to the task of discovering the\ngroup of objects or events that co-occur at many places. Extracting these\npatterns from spatial data is very difficult due to the complexity of spatial\ndata types, spatial relationships, and spatial auto-correlation. These patterns\nhave applications in domains including public safety, geo-marketing, crime\nprediction and ecology. Prior work focused on using the spatial join. While\nthese approaches provide state-of-the-art results, they are very expensive to\ncompute due to the multiway spatial join and scaling them to real-world\ndatasets is an open problem. We address these limitations by formulating the\nco-location pattern discovery as a clique enumeration problem over a\nneighborhood graph (which is materialized using a distributed graph database).\nWe propose three new traversal based algorithms, namely $CliqueEnum_G$,\n$CliqueEnum_K$ and $CliqueExtend$. We provide the empirical evidence for the\neffectiveness of our proposed algorithms by evaluating them for a large\nreal-life dataset. The three algorithms allow for a trade-off between time and\nmemory requirements and support interactive data analysis without having to\nrecompute all the intermediate results. These attributes make our algorithms\napplicable to a wide range of use cases for different data sizes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 19:05:33 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Mehta", "Sanket Vaibhav", ""], ["Sodhani", "Shagun", ""], ["Patel", "Dhaval", ""]]}, {"id": "1810.09152", "submitter": "Yang Cao", "authors": "Yang Cao, Yonghui Xiao, Li Xiong, Liquan Bai", "title": "PriSTE: From Location Privacy to Spatiotemporal Event Privacy", "comments": "accepted in ICDE 2019 (short paper) and the extended version accepted\n  in TKDE: 10.1109/TKDE.2019.2963312\n  https://ieeexplore.ieee.org/document/8946543", "journal-ref": "ICDE 2019", "doi": "10.1109/ICDE.2019.00153", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location privacy-preserving mechanisms (LPPMs) have been extensively studied\nfor protecting a user's location at each time point or a sequence of locations\nwith different timestamps (i.e., a trajectory). We argue that existing LPPMs\nare not capable of protecting the sensitive information in user's\nspatiotemporal activities, such as \"visited hospital in the last week\" or\n\"regularly commuting between Address 1 and Address 2 every morning and\nafternoon\" (it is easy to infer that Addresses 1 and 2 may be home and office).\nWe define such privacy as \\textit{Spatiotemporal Event Privacy}, which can be\nformalized as Boolean expressions between location and time predicates. To\nunderstand how much spatiotemporal event privacy that existing LPPMs can\nprovide, we first formally define spatiotemporal event privacy by extending the\nnotion of differential privacy, and then provide a framework for calculating\nthe spatiotemporal event privacy loss of a given LPPM under attackers who have\nknowledge of user's mobility pattern. We also show a case study of utilizing\nour framework to convert the state-of-the-art mechanism for location privacy,\ni.e., Planner Laplace Mechanism for Geo-indistinguishability, into one\nprotecting spatiotemporal event privacy. Our experiments on real-life and\nsynthetic data verified that the proposed method is effective and efficient.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 09:41:51 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 02:25:36 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Cao", "Yang", ""], ["Xiao", "Yonghui", ""], ["Xiong", "Li", ""], ["Bai", "Liquan", ""]]}, {"id": "1810.09227", "submitter": "Brandon Malone", "authors": "Brandon Malone and Alberto Garc\\'ia-Dur\\'an and Mathias Niepert", "title": "Knowledge Graph Completion to Predict Polypharmacy Side Effects", "comments": "13th International Conference on Data Integration in the Life\n  Sciences (DILS2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polypharmacy side effect prediction problem considers cases in which two\ndrugs taken individually do not result in a particular side effect; however,\nwhen the two drugs are taken in combination, the side effect manifests. In this\nwork, we demonstrate that multi-relational knowledge graph completion achieves\nstate-of-the-art results on the polypharmacy side effect prediction problem.\nEmpirical results show that our approach is particularly effective when the\nprotein targets of the drugs are well-characterized. In contrast to prior work,\nour approach provides more interpretable predictions and hypotheses for wet lab\nvalidation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 12:59:51 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Malone", "Brandon", ""], ["Garc\u00eda-Dur\u00e1n", "Alberto", ""], ["Niepert", "Mathias", ""]]}, {"id": "1810.09304", "submitter": "Marie-Laure Mugnier", "authors": "Stathis Delivorias, Michel Leclere, Marie-Laure Mugnier and Federico\n  Ulliana", "title": "On the k-Boundedness for Existential Rules", "comments": "20 pages, revised version of the paper published at RuleML+RR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chase is a fundamental tool for existential rules. Several chase variants\nare known, which differ on how they handle redundancies possibly caused by the\nintroduction of nulls. Given a chase variant, the halting problem takes as\ninput a set of existential rules and asks if this set of rules ensures the\ntermination of the chase for any factbase. It is well-known that this problem\nis undecidable for all known chase variants. The related problem of boundedness\nasks if a given set of existential rules is bounded, i.e., whether there is a\npredefined upper bound on the number of (breadth-first) steps of the chase,\nindependently from any factbase. This problem is already undecidable in the\nspecific case of datalog rules. However, knowing that a set of rules is bounded\nfor some chase variant does not help much in practice if the bound is unknown.\nHence, in this paper, we investigate the decidability of the k-boundedness\nproblem, which asks whether a given set of rules is bounded by an integer k. We\nprove that k-boundedness is decidable for three chase variants, namely the\noblivious, semi-oblivious and restricted chase.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 14:12:03 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Delivorias", "Stathis", ""], ["Leclere", "Michel", ""], ["Mugnier", "Marie-Laure", ""], ["Ulliana", "Federico", ""]]}, {"id": "1810.09355", "submitter": "Wolf-Tilo Balke", "authors": "Stephan Mennicke, Jan-Christoph Kalo, Denis Nagel, Hermann Kroll,\n  Wolf-Tilo Balke", "title": "Fast Dual Simulation Processing of Graph Database Queries (Supplement)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph database query languages feature expressive, yet computationally\nexpensive pattern matching capabilities. Answering optional query clauses in\nSPARQL for instance renders the query evaluation problem immediately\nPspace-complete. Therefore, light-weight graph pattern matching relations, such\nas simulation, have recently been investigated as promising alternatives to\nmore expensive query mechanisms like, e.g., computing subgraph isomorphism.\nStill, graph pattern matching alone lacks expressive query capabilities: all\npatterns are combined by usual join constructs, where more sophisticated\ncapabilities would be inevitable for making solutions useful to emerging\napplications. In this paper we bridge this gap by introducing a new dual\nsimulation process operating on SPARQL queries. In addition to supporting the\nfull syntactic structure of SPARQL queries, it features polynomial-time pattern\nmatching to compute an overapproximation of query results from the database.\nMoreover, to achieve runtimes competing with state-of-the-art database systems,\nwe develop a novel algorithmic solution to dual simulation graph pattern\nmatching, based on a system of inequalities that allows for several\noptimization heuristics. Finally, we achieve soundness of our process for\nSPARQL queries including UNION, AND and OPTIONAL operators not restricted to\nwell-designed patterns. Our experiments on synthetic and real-world graph data\npromise a clear gain for graph database systems when incorporating the new dual\nsimulation techniques. In this supplement paper we present in detail all\nproofs, discussions of experimental results, and complexity analysis for the\noriginal paper \"Fast Dual Simulation Processing of Graph Database Queries\"\nincluded in the Proceedings of the 35th IEEE International Conference on Data\nEngineering (ICDE 2019), Macau, China.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 15:18:54 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Mennicke", "Stephan", ""], ["Kalo", "Jan-Christoph", ""], ["Nagel", "Denis", ""], ["Kroll", "Hermann", ""], ["Balke", "Wolf-Tilo", ""]]}, {"id": "1810.09378", "submitter": "Henan Guan", "authors": "Yao Wu, Henan Guan", "title": "biggy: An Implementation of Unified Framework for Big Data Management\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various tools, softwares and systems are proposed and implemented to tackle\nthe challenges in big data on different emphases, e.g., data analysis, data\ntransaction, data query, data storage, data visualization, data privacy. In\nthis paper, we propose datar, a new prospective and unified framework for Big\nData Management System (BDMS) from the point of system architecture by\nleveraging ideas from mainstream computer structure. We introduce five key\ncomponents of datar by reviewing the cur- rent status of BDMS. Datar features\nwith configuration chain of pluggable engines, automatic dataflow on job\npipelines, intelligent self-driving system management and interactive user\ninterfaces. Moreover, we present biggy as an implementation of datar with\nmanipulation details demonstrated by four running examples. Evaluations on\nefficiency and scalability are carried out to show the performance. Our work\nargues that the envisioned datar is a feasible solution to the unified\nframework of BDMS, which can manage big data pluggablly, automatically and\nintelligently with specific functionalities, where specific functionalities\nrefer to input, storage, computation, control and output of big data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 15:53:22 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Wu", "Yao", ""], ["Guan", "Henan", ""]]}, {"id": "1810.09399", "submitter": "Milen Stoilov Marev", "authors": "Milen S. Marev, Ernesto Compatangelo, Wamberto Vasconcelos", "title": "Towards a context-dependent numerical data quality evaluation framework", "comments": "Keywords: Data Quality; Numerical Data; Evaluation framework. 12\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on numeric data, with emphasis on distinct characteristics\nlike varying significance, unstructured format, mass volume and real-time\nprocessing. We propose a novel, context-dependent valuation framework\nspecifically devised to assess quality in numeric datasets. Our framework uses\neight relevant data quality dimensions, and provide a simple metric to evaluate\ndataset quality along each dimension. We argue that the proposed set of\ndimensions and corresponding metrics adequately captures the unique quality\nantipatterns that are typically associated with numerical data. The\nintroduction of our framework is part of a wider research effort that aims at\ndeveloping an articulated numerical data quality improvement approach for Oil\nand Gas exploration and production workflows that is based on artificial\nintelligence techniques.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 16:47:46 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Marev", "Milen S.", ""], ["Compatangelo", "Ernesto", ""], ["Vasconcelos", "Wamberto", ""]]}, {"id": "1810.09524", "submitter": "Issam Ghabry", "authors": "Issam Ghabry", "title": "Selection of BJI configuration: Approach based on minimal transversals", "comments": "Masters thesis (2017) supervised by Sadok Ben Yahia and Mohamed\n  Nidhal Jelassi, in French. arXiv admin note: text overlap with\n  arXiv:1902.00911 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision systems deal with a large volume of data stored in new databases\ncalled data warehouses. Data warehouses are typically modeled by a star schema\nthat conventionally presents a central fact table and a set of dimension\ntables. The corresponding queries for this type of model are therefore very\ncomplex. In order to reduce the cost of executing complex queries, which\ncontain very expensive joins, the solution envisaged would be to guarantee a\ngood physical design of the data warehouses. Binary join indexes are very\nsuitable to reduce the cost of executing these joins. In this work, we proposed\na binary join index selection approach based on the notion of minimal\ntransversal. The final configuration obtained is composed of several indexes,\nwhich make it possible to optimize the execution cost of the query set.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 20:04:24 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Ghabry", "Issam", ""]]}, {"id": "1810.09780", "submitter": "Pavlos Fafalios", "authors": "Thanos Yannakis, Pavlos Fafalios, Yannis Tzitzikas", "title": "Heuristics-based Query Reordering for Federated Queries in SPARQL 1.1\n  and SPARQL-LD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The federated query extension of SPARQL 1.1 allows executing queries\ndistributed over different SPARQL endpoints. SPARQL-LD is a recent extension of\nSPARQL 1.1 which enables to directly query any HTTP web source containing RDF\ndata, like web pages embedded with RDFa, JSON-LD or Microformats, without\nrequiring the declaration of named graphs. This makes possible to query a large\nnumber of data sources (including SPARQL endpoints, online resources, or even\nWeb APIs returning RDF data) through a single one concise query. However, not\noptimal formulation of SPARQL 1.1 and SPARQL-LD queries can lead to a large\nnumber of calls to remote resources which in turn can lead to extremely high\nquery execution times. In this paper, we address this problem and propose a set\nof query reordering methods which make use of heuristics to reorder a set of\nSERVICE graph patterns based on their restrictiveness, without requiring the\ngathering and use of statistics from the remote sources. Such a query\noptimization approach is widely applicable since it can be exploited on top of\nexisting SPARQL 1.1 and SPARQL-LD implementations. Evaluation results show that\nquery reordering can highly decrease the query-execution time, while a method\nthat considers the number and type of unbound variables and joins achieves the\noptimal query plan in 88% of the cases.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 11:23:01 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Yannakis", "Thanos", ""], ["Fafalios", "Pavlos", ""], ["Tzitzikas", "Yannis", ""]]}, {"id": "1810.11152", "submitter": "Haida Zhang", "authors": "Haida Zhang, Zengfeng Huang, Xuemin Lin, Zhe Lin, Wenjie Zhang, Ying\n  Zhang", "title": "Efficient and High-Quality Seeded Graph Matching: Employing High Order\n  Structural Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by many real applications, we study the problem of seeded graph\nmatching. Given two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$, and a\nsmall set $S$ of pre-matched node pairs $[u, v]$ where $u \\in V_1$ and $v \\in\nV_2$, the problem is to identify a matching between $V_1$ and $V_2$ growing\nfrom $S$, such that each pair in the matching corresponds to the same\nunderlying entity. Recent studies on efficient and effective seeded graph\nmatching have drawn a great deal of attention and many popular methods are\nlargely based on exploring the similarity between local structures to identify\nmatching pairs. While these recent techniques work well on random graphs, their\naccuracy is low over many real networks. Motivated by this, we propose to\nutilize high order neighboring information to improve the matching accuracy. As\na result, a new framework of seeded graph matching is proposed, which employs\nPersonalized PageRank (PPR) to quantify the matching score of each node pair.\nTo further boost the matching accuracy, we propose a novel postponing strategy,\nwhich postpones the selection of pairs that have competitors with similar\nmatching scores. We theoretically prove that the postpone strategy indeed\nsignificantly improves the matching accuracy. To improve the scalability of\nmatching large graphs, we also propose efficient approximation techniques based\non algorithms for computing PPR heavy hitters. Our comprehensive experimental\nstudies on large-scale real datasets demonstrate that, compared with state of\nthe art approaches, our framework not only increases the precision and recall\nboth by a significant margin but also achieves speed-up up to more than one\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 00:38:04 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Zhang", "Haida", ""], ["Huang", "Zengfeng", ""], ["Lin", "Xuemin", ""], ["Lin", "Zhe", ""], ["Zhang", "Wenjie", ""], ["Zhang", "Ying", ""]]}, {"id": "1810.11308", "submitter": "Martin Hirzel", "authors": "Kanat Tangwongsan, Martin Hirzel, Scott Schneider", "title": "Sub-O(log n) Out-of-Order Sliding-Window Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding-window aggregation summarizes the most recent information in a data\nstream. Users specify how that summary is computed, usually as an associative\nbinary operator because this is the most general known form for which it is\npossible to avoid naively scanning every window. For strictly in-order\narrivals, there are algorithms with $O(1)$ time per window change assuming\nassociative operators. Meanwhile, it is common in practice for streams to have\ndata arriving slightly out of order, for instance, due to clock drifts or\ncommunication delays. Unfortunately, for out-of-order streams, one has to\nresort to latency-prone buffering or pay $O(\\log n)$ time per insert or evict,\nwhere $n$ is the window size.\n  This paper presents the design, analysis, and implementation of FiBA, a novel\nsliding-window aggregation algorithm with an amortized upper bound of $O(\\log\nd)$ time per insert or evict, where $d$ is the distance of the inserted or\nevicted value to the closer end of the window. This means $O(1)$ time for\nin-order arrivals and nearly $O(1)$ time for slightly out-of-order arrivals,\nwith a smooth transition towards $O(\\log n)$ as $d$ approaches $n$. We also\nprove a matching lower bound on running time, showing optimality. Our algorithm\nis as general as the prior state-of-the-art: it requires associativity, but not\ninvertibility nor commutativity. At the heart of the algorithm is a careful\ncombination of finger-searching techniques, lazy rebalancing, and\nposition-aware partial aggregates. We further show how to answer range queries\nthat aggregate subwindows for window sharing. Finally, our experimental\nevaluation shows that FiBA performs well in practice and supports the\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 13:17:20 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Tangwongsan", "Kanat", ""], ["Hirzel", "Martin", ""], ["Schneider", "Scott", ""]]}, {"id": "1810.11832", "submitter": "Luis Remis", "authors": "Luis Remis, Vishakha Gupta-Cledat, Christina Strong, Ragaad Altarawneh", "title": "VDMS: Efficient Big-Visual-Data Access for Machine Learning Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Visual Data Management System (VDMS), which enables faster\naccess to big-visual-data and adds support to visual analytics. This is\nachieved by searching for relevant visual data via metadata stored as a graph,\nand enabling faster access to visual data through new machine-friendly storage\nformats. VDMS differs from existing large scale photo serving, video streaming,\nand textual big-data management systems due to its primary focus on supporting\nmachine learning and data analytics pipelines that use visual data (images,\nvideos, and feature vectors), treating these as first class entities. We\ndescribe how to use VDMS via its user friendly interface and how it enables\nrich and efficient vision analytics through a machine learning pipeline for\nprocessing medical images. We show the improved performance of 2x in complex\nqueries over a comparable set-up.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 16:41:22 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 18:38:44 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2018 15:42:14 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Remis", "Luis", ""], ["Gupta-Cledat", "Vishakha", ""], ["Strong", "Christina", ""], ["Altarawneh", "Ragaad", ""]]}, {"id": "1810.12059", "submitter": "Massimiliano Morrellli", "authors": "Massimiliano Morrelli", "title": "Studio e confronto delle strutture di Apache Spark", "comments": "in Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  English. This document is designed to study the data structures that can be\nused in the Apache Spark framework and to evaluate the best performing ones to\nimplement solutions, in particular we will evaluate advantages / disadvantages\nderiving from the use of Dataset for job creation. The observation of the\nresults provides further support in evaluating the use of Dataset as an\nalternative to RDD, in order to understand its strengths and weaknesses. The\nexamination of the results is possible thanks to specifically designed and\nimplemented in Java 1.8 language. The execution of the jobs, entrusted to a\nsuitable distributed environment, will end with the comparison between\nexecution times and results obtained.\n  Italiano. Il presente documento nasce allo scopo di studiare le strutture\ndati utilizzabili nel framework Apache Spark e valutare quelle pi\\`u\nperformanti per implementare soluzioni; valuteremo in articolare i vantaggi /\nsvantaggi derivanti dall'utilizzo dei Dataset nella progettazione dei job.\nL'osservazione dei risultati fornisce ulteriore supporto nel valutare\nl'utilizzo dei Dataset in alternativa a RDD, al fine di comprederne i punti di\nforza e di debolezza. L'esame dei risultati \\`e possibile in virt\\`u di due\ncasi appositamente pensati e implementati in linguaggio Java 1.8. L'esecuzione\ndei job, affidata a un adeguato ambiente distribuito, si concluder\\`a con il\nconfronto tra tempi di esecuzione e risultati ottenuti.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 11:26:17 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Morrelli", "Massimiliano", ""]]}, {"id": "1810.12100", "submitter": "Robert Kent", "authors": "Robert E. Kent", "title": "The FOLE Table", "comments": "48 pages, 21 figures, 9 tables, submitted to T.A.C. for review in\n  August 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper continues the discussion of the representation of ontologies in\nthe first-order logical environment FOLE. According to Gruber, an ontology\ndefines the primitives with which to model the knowledge resources for a\ncommunity of discourse. These primitives, consisting of classes, relationships\nand properties, are represented by the entity-relationship-attribute ERA data\nmodel of Chen. An ontology uses formal axioms to constrain the interpretation\nof these primitives. In short, an ontology specifies a logical theory. A series\nof three papers by the author provide a rigorous mathematical representation\nfor the ERA data model in particular, and ontologies in general, within FOLE.\nThe first two papers, which provide a foundation and superstructure for FOLE,\nrepresent the formalism and semantics of (many-sorted) first-order logic in a\nclassification form corresponding to ideas discussed in the Information Flow\nFramework (IFF). The third paper will define an interpretation of FOLE in terms\nof the transformational passage, first described in (Kent, 2013), from the\nclassification form of first-order logic to an equivalent interpretation form,\nthereby defining the formalism and semantics of first-order logical/relational\ndatabase systems. Two papers will provide a precise mathematical basis for FOLE\ninterpretation: the current paper develops the notion of a FOLE relational\ntable following the relational model of Codd, and a follow-up paper will\ndevelop the notion of a FOLE relational database. Both of these papers expand\non material found in the paper (Kent, 2011). Although the classification form\nfollows the entity-relationship-attribute data model of Chen, the\ninterpretation form follows the relational data model of Codd. In general, the\nFOLE representation uses a conceptual structures approach, that is completely\ncompatible with formal concept analysis and information flow.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 18:24:41 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Kent", "Robert E.", ""]]}, {"id": "1810.12123", "submitter": "Pengfei Xu", "authors": "Pengfei Xu, Jiaheng Lu", "title": "Efficient Taxonomic Similarity Joins with Adaptive Overlap Constraint", "comments": null, "journal-ref": null, "doi": "10.1145/3269206.3269236", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A similarity join aims to find all similar pairs between two collections of\nrecords. Established approaches usually deal with synthetic differences like\ntypos and abbreviations, but neglect the semantic relations between words. Such\nrelations, however, are helpful for obtaining high-quality joining results. In\nthis paper, we leverage the taxonomy knowledge (i.e., a set of IS-A\nhierarchical relations) to define a similarity measure which finds\nsemantic-similar records from two datasets. Based on this measure, we develop a\nsimilarity join algorithm with prefix filtering framework to prune away\nirrelevant pairs effectively. Our technical contribution here is an algorithm\nthat judiciously selects critical parameters in a prefix filter to maximise its\nfiltering power, supported by an estimation technique and Monte Carlo\nsimulation process. Empirical experiments show that our proposed methods\nexhibit high efficiency and scalability, outperforming the state-of-art by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 13:42:10 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Xu", "Pengfei", ""], ["Lu", "Jiaheng", ""]]}, {"id": "1810.12125", "submitter": "Boyi Hou", "authors": "Boyi Hou, Qun Chen, Yanyan Wang, Youcef Nafa, Zhanhuai Li", "title": "Gradual Machine Learning for Entity Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually considered as a classification problem, entity resolution (ER) can be\nvery challenging on real data due to the prevalence of dirty values. The\nstate-of-the-art solutions for ER were built on a variety of learning models\n(most notably deep neural networks), which require lots of accurately labeled\ntraining data. Unfortunately, high-quality labeled data usually require\nexpensive manual work, and are therefore not readily available in many real\nscenarios. In this paper, we propose a novel learning paradigm for ER, called\ngradual machine learning, which aims to enable effective machine labeling\nwithout the requirement for manual labeling effort. It begins with some easy\ninstances in a task, which can be automatically labeled by the machine with\nhigh accuracy, and then gradually labels more challenging instances by\niterative factor graph inference. In gradual machine learning, the hard\ninstances in a task are gradually labeled in small stages based on the\nestimated evidential certainty provided by the labeled easier instances. Our\nextensive experiments on real data have shown that the performance of the\nproposed approach is considerably better than its unsupervised alternatives,\nand highly competitive compared to the state-of-the-art supervised techniques.\nUsing ER as a test case, we demonstrate that gradual machine learning is a\npromising paradigm potentially applicable to other challenging classification\ntasks requiring extensive labeling effort.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 13:47:52 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 11:58:22 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 08:40:51 GMT"}, {"version": "v4", "created": "Fri, 14 Jun 2019 01:12:48 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Hou", "Boyi", ""], ["Chen", "Qun", ""], ["Wang", "Yanyan", ""], ["Nafa", "Youcef", ""], ["Li", "Zhanhuai", ""]]}]