[{"id": "1110.0207", "submitter": "Carlos Granell", "authors": "Alain Tamayo and Carlos Granell and Joaqu\\'in Huerta", "title": "Analysing complexity of XML Schemas in geospatial web services", "comments": "9 pages, 10 tables, 4 figures; COM.Geo '11 Proceedings of the 2nd\n  International Conference on Computing for Geospatial Research & Applications,\n  ACM, no. 17, 2011", "journal-ref": "COM.Geo '11 Proceedings of the 2nd International Conference on\n  Computing for Geospatial Research & Applications, ACM, no. 17, 2011", "doi": "10.1145/1999320.1999337", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML Schema is the language used to define the structure of messages exchanged\nbetween OGC-based web service clients and providers. The size of these schemas\nhas been growing with time, reaching a state that makes its understanding and\neffective application a hard task. A first step to cope with this situation is\nto provide different ways to measure the complexity of the schemas. In this\nregard, we present in this paper an analysis of the complexity of XML schemas\nin OGC web services. We use a group of metrics found in the literature and\nintroduce new metrics to measure size and/or complexity of these schemas. The\nuse of adequate metrics allows us to quantify the complexity, quality and other\nproperties of the schemas, which can be very useful in different scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2011 17:49:47 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Tamayo", "Alain", ""], ["Granell", "Carlos", ""], ["Huerta", "Joaqu\u00edn", ""]]}, {"id": "1110.0209", "submitter": "Carlos Granell", "authors": "Alain Tamayo and Carlos Granell and Joaqu\\'in Huerta", "title": "Dealing with large schema sets in mobile SOS-based applications", "comments": "9 pages, 2 tables, 7 figures", "journal-ref": "COM.Geo '11 Proceedings of the 2nd International Conference on\n  Computing for Geospatial Research & Applications, ACM, no 16, 2011", "doi": "10.1145/1999320.1999336", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the adoption of OGC Web Services for server, desktop and web\napplications has been successful, its penetration in mobile devices has been\nslow. One of the main reasons is the performance problems associated with XML\nprocessing as it consumes a lot of memory and processing time, which are scarce\nresources in a mobile device. In this paper we propose an algorithm to generate\nefficient code for XML data binding for mobile SOS-based applications. The\nalgorithm take advantage of the fact that individual implementations use only\nsome portions of the standards' schemas, which allows the simplification of\nlarge XML schema sets in an application-specific manner by using a subset of\nXML instance files conforming to these schemas.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2011 18:08:34 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Tamayo", "Alain", ""], ["Granell", "Carlos", ""], ["Huerta", "Joaqu\u00edn", ""]]}, {"id": "1110.0235", "submitter": "Pablo Cordero", "authors": "Pablo Cordero, Julius Lucks, Rhiju Das", "title": "The Stanford RNA Mapping Database for sharing and visualizing RNA\n  structure mapping experiments", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have established an RNA Mapping Database (RMDB) to enable a new generation\nof structural, thermodynamic, and kinetic studies from quantitative\nsingle-nucleotide-resolution RNA structure mapping (freely available at\nhttp://rmdb.stanford.edu). Chemical and enzymatic mapping is a rapid, robust,\nand widespread approach to RNA characterization. Since its recent coupling with\nhigh-throughput sequencing techniques, accelerated software pipelines, and\nlarge-scale mutagenesis, the volume of mapping data has greatly increased, and\nthere is a critical need for a database to enable sharing, visualization, and\nmeta-analyses of these data. Through its on-line front-end, the RMDB allows\nusers to explore single-nucleotide-resolution chemical accessibility data in\nheat-map, bar-graph, and colored secondary structure graphics; to leverage\nthese data to generate secondary structure hypotheses; and to download the data\nin standardized and computer-friendly files, including the RDAT and\ncommunity-consensus SNRNASM formats. At the time of writing, the database\nhouses 38 entries, describing 2659 RNA sequences and comprising 355,084 data\npoints, and is growing rapidly.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2011 20:56:47 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Cordero", "Pablo", ""], ["Lucks", "Julius", ""], ["Das", "Rhiju", ""]]}, {"id": "1110.1328", "submitter": "Venu Satuluri", "authors": "Venu Satuluri and Srinivasan Parthasarathy", "title": "Bayesian Locality Sensitive Hashing for Fast Similarity Search", "comments": "13 pages, 5 Tables, 21 figures. Added acknowledgments in v3. A\n  slightly shorter version of this paper without the appendix has been\n  published in the PVLDB journal, 5(5):430-441, 2012.\n  http://vldb.org/pvldb/vol5/p430_venusatuluri_vldb2012.pdf", "journal-ref": "PVLDB 5(5):430-441, 2012", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of objects and an associated similarity measure, the\nall-pairs similarity search problem asks us to find all pairs of objects with\nsimilarity greater than a certain user-specified threshold. Locality-sensitive\nhashing (LSH) based methods have become a very popular approach for this\nproblem. However, most such methods only use LSH for the first phase of\nsimilarity search - i.e. efficient indexing for candidate generation. In this\npaper, we present BayesLSH, a principled Bayesian algorithm for the subsequent\nphase of similarity search - performing candidate pruning and similarity\nestimation using LSH. A simpler variant, BayesLSH-Lite, which calculates\nsimilarities exactly, is also presented. BayesLSH is able to quickly prune away\na large majority of the false positive candidate pairs, leading to significant\nspeedups over baseline approaches. For BayesLSH, we also provide probabilistic\nguarantees on the quality of the output, both in terms of accuracy and recall.\nFinally, the quality of BayesLSH's output can be easily tuned and does not\nrequire any manual setting of the number of hashes to use for similarity\nestimation, unlike standard approaches. For two state-of-the-art candidate\ngeneration algorithms, AllPairs and LSH, BayesLSH enables significant speedups,\ntypically in the range 2x-20x for a wide variety of datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 17:13:48 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2011 17:46:46 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2012 19:34:39 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Satuluri", "Venu", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1110.1700", "submitter": "Shirin Mohammadi", "authors": "Shirin Mohammadi, Ali A. Safaei, Fatemeh Abdi and Mostafa S. Haghjoo", "title": "Adaptive Data Stream Management System Using Learning Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern applications, data are received as infinite, rapid,\nunpredictable and time- variant data elements that are known as data streams.\nSystems which are able to process data streams with such properties are called\nData Stream Management Systems (DSMS). Due to the unpredictable and time-\nvariant properties of data streams as well as system, adaptivity of the DSMS is\na major requirement for each DSMS. Accordingly, determining parameters which\nare effective on the most important performance metric of a DSMS (i.e.,\nresponse time) and analysing them will affect on designing an adaptive DSMS. In\nthis paper, effective parameters on response time of DSMS are studied and\nanalysed and a solution is proposed for DSMSs' adaptivity. The proposed\nadaptive DSMS architecture includes a learning unit that frequently evaluates\nsystem to adjust the optimal value for each of tuneable effective. Learning\nAutomata is used as the learning mechanism of the learning unit to adjust the\nvalue of tuneable effective parameters. So, when system faces some changes, the\nlearning unit increases performance by tuning each of tuneable effective\nparameters to its optimum value. Evaluation results illustrate that after a\nwhile, parameters reach their optimum value and then DSMS's adaptivity will be\nimproved considerably.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2011 06:05:02 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Mohammadi", "Shirin", ""], ["Safaei", "Ali A.", ""], ["Abdi", "Fatemeh", ""], ["Haghjoo", "Mostafa S.", ""]]}, {"id": "1110.1729", "submitter": "L\\'aszl\\'o Dobos", "authors": "L\\'aszl\\'o Dobos, Alexander Szalay, Jos\\'e Blakeley, Tam\\'as\n  Budav\\'ari, Istv\\'an Csabai, Dragan Tomic, Milos Milovanovic, Marko Tintor\n  and Andrija Jovanovic", "title": "Array Requirements for Scientific Applications and an Implementation for\n  Microsoft SQL Server", "comments": null, "journal-ref": "Proceedings of the EDBT/ICDT 2011 Workshop on Array Databases", "doi": "10.1145/1966895.1966897", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines certain scenarios from the fields of astrophysics and\nfluid dynamics simulations which require high performance data warehouses that\nsupport array data type. A common feature of all these use cases is that\nsubsetting and preprocessing the data on the server side (as far as possible\ninside the database server process) is necessary to avoid the client-server\noverhead and to minimize IO utilization. Analyzing and summarizing the\nrequirements of the various fields help software engineers to come up with a\ncomprehensive design of an array extension to relational database systems that\ncovers a wide range of scientific applications. We also present a working\nimplementation of an array data type for Microsoft SQL Server 2008 to support\nlarge-scale scientific applications. We introduce the design of the array type,\nresults from a performance evaluation, and discuss the lessons learned from\nthis implementation. The library can be downloaded from our website at\nhttp://voservices.net/sqlarray/\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2011 12:16:48 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Dobos", "L\u00e1szl\u00f3", ""], ["Szalay", "Alexander", ""], ["Blakeley", "Jos\u00e9", ""], ["Budav\u00e1ri", "Tam\u00e1s", ""], ["Csabai", "Istv\u00e1n", ""], ["Tomic", "Dragan", ""], ["Milovanovic", "Milos", ""], ["Tintor", "Marko", ""], ["Jovanovic", "Andrija", ""]]}, {"id": "1110.2196", "submitter": "Bart Kuijpers", "authors": "Marc Giusti, Joos Heintz and Bart Kuijpers", "title": "The evaluation of geometric queries: constraint databases and quantifier\n  elimination", "comments": "This paper is representing work in progress of the authors. It is not\n  aimed for publication in the present form", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the algorithmic task of geometric elimination (e.g., quantifier\nelimination in the elementary field theories of real and complex numbers) by\nmeans of certain constraint database queries, called geometric queries. As a\nparticular case of such a geometric elimination task, we consider sample point\nqueries. We show exponential lower complexity bounds for evaluating geometric\nqueries in the general and in the particular case of sample point queries.\nAlthough this paper is of theoretical nature, its aim is to explore the\npossibilities and (complexity-)limits of computer implemented query evaluation\nalgorithms for Constraint Databases, based on the principles of the most\nadvanced geometric elimination procedures and their implementations, like,\ne.g., the software package \"Kronecker\".\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 20:55:17 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2011 18:59:22 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Giusti", "Marc", ""], ["Heintz", "Joos", ""], ["Kuijpers", "Bart", ""]]}, {"id": "1110.2294", "submitter": "Hugo Buddelmeijer", "authors": "Hugo Buddelmeijer and Edwin A. Valentijn", "title": "Query Driven Visualization of Astronomical Catalogs", "comments": "Accepted for publication in topical issue of Experimental Astronomy\n  on Astro-WISE information system", "journal-ref": null, "doi": "10.1007/s10686-011-9263-0", "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive visualization of astronomical catalogs requires novel techniques\ndue to the huge volumes and complex structure of the data produced by existing\nand upcoming astronomical surveys. The creation as well as the disclosure of\nthe catalogs can be handled by data pulling mechanisms. These prevent\nunnecessary processing and facilitate data sharing by having users request the\ndesired end products.\n  In this work we present query driven visualization as a logical continuation\nof data pulling. Scientists can request catalogs in a declarative way and set\nprocess parameters directly from within the visualization. This results in\nprofound interoperation between software with a high level of abstraction.\n  New messages for the Simple Application Messaging Protocol are proposed to\nachieve this abstraction. Support for these messages are implemented in the\nAstro-WISE information system and in a set of demonstrational applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 07:49:34 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Buddelmeijer", "Hugo", ""], ["Valentijn", "Edwin A.", ""]]}, {"id": "1110.2626", "submitter": "Kuruba Usha Rani", "authors": "K. Usha Rani", "title": "Analysis of Heart Diseases Dataset using Neural Network Approach", "comments": "8 pages, 2 figures, 1 table; International Journal of Data Mining &\n  Knowledge Management Process (IJDKP) Vol.1, No.5, September 2011", "journal-ref": null, "doi": "10.5121/ijdkp.2011.1501", "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the important techniques of Data mining is Classification. Many real\nworld problems in various fields such as business, science, industry and\nmedicine can be solved by using classification approach. Neural Networks have\nemerged as an important tool for classification. The advantages of Neural\nNetworks helps for efficient classification of given data. In this study a\nHeart diseases dataset is analyzed using Neural Network approach. To increase\nthe efficiency of the classification process parallel approach is also adopted\nin the training phase.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 10:56:29 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Rani", "K. Usha", ""]]}, {"id": "1110.2704", "submitter": "Jerome Darmont", "authors": "Huu Hoa Nguyen (ERIC), Nouria Harbi (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "An Efficient Fuzzy Clustering-Based Approach for Intrusion Detection", "comments": "15th East-European Conference on Advances and Databases and\n  Information Systems (ADBIS 11), Vienna : Austria (2011)", "journal-ref": null, "doi": null, "report-no": "ERIC:11-029", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to increase accuracy in detecting sophisticated cyber attacks poses\na great challenge not only to the research community but also to corporations.\nSo far, many approaches have been proposed to cope with this threat. Among\nthem, data mining has brought on remarkable contributions to the intrusion\ndetection problem. However, the generalization ability of data mining-based\nmethods remains limited, and hence detecting sophisticated attacks remains a\ntough task. In this thread, we present a novel method based on both clustering\nand classification for developing an efficient intrusion detection system\n(IDS). The key idea is to take useful information exploited from fuzzy\nclustering into account for the process of building an IDS. To this aim, we\nfirst present cornerstones to construct additional cluster features for a\ntraining set. Then, we come up with an algorithm to generate an IDS based on\nsuch cluster features and the original input features. Finally, we\nexperimentally prove that our method outperforms several well-known methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 17:06:12 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Nguyen", "Huu Hoa", "", "ERIC"], ["Harbi", "Nouria", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1110.2890", "submitter": "Rui Zhou", "authors": "Rui Zhou, Chengfei Liu, Jianxin Li and Jeffrey Xu Yu", "title": "ELCA Evaluation for Keyword Search on Probabilistic XML Data", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As probabilistic data management is becoming one of the main research focuses\nand keyword search is turning into a more popular query means, it is natural to\nthink how to support keyword queries on probabilistic XML data. With regards to\nkeyword query on deterministic XML documents, ELCA (Exclusive Lowest Common\nAncestor) semantics allows more relevant fragments rooted at the ELCAs to\nappear as results and is more popular compared with other keyword query result\nsemantics (such as SLCAs).\n  In this paper, we investigate how to evaluate ELCA results for keyword\nqueries on probabilistic XML documents. After defining probabilistic ELCA\nsemantics in terms of possible world semantics, we propose an approach to\ncompute ELCA probabilities without generating possible worlds. Then we develop\nan efficient stack-based algorithm that can find all probabilistic ELCA results\nand their ELCA probabilities for a given keyword query on a probabilistic XML\ndocument. Finally, we experimentally evaluate the proposed ELCA algorithm and\ncompare it with its SLCA counterpart in aspects of result effectiveness, time\nand space efficiency, and scalability.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 10:45:33 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Zhou", "Rui", ""], ["Liu", "Chengfei", ""], ["Li", "Jianxin", ""], ["Yu", "Jeffrey Xu", ""]]}, {"id": "1110.3017", "submitter": "Olaf Hartig", "authors": "Juan Sequeda and Olaf Hartig", "title": "Towards a Query Language for the Web of Data (A Vision Paper)", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on querying the Web of Data is still in its infancy. In this paper,\nwe provide an initial set of general features that we envision should be\nconsidered in order to define a query language for the Web of Data.\nFurthermore, for each of these features, we pose questions that have not been\naddressed before in the context of querying the Web of Data. We believe that\naddressing these questions and studying these features may guide the next 10\nyears of research on the Web of Data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 18:01:39 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Sequeda", "Juan", ""], ["Hartig", "Olaf", ""]]}, {"id": "1110.3158", "submitter": "Jerome Darmont", "authors": "Rashed Salem (ERIC), J\\'er\\^ome Darmont (ERIC), Omar Boussa\\\"id (ERIC)", "title": "Efficient Incremental Breadth-Depth XML Event Mining", "comments": null, "journal-ref": "15th International Database Engineering and Applications Symposium\n  (IDEAS 2011), Lisbon : Portugal (2011)", "doi": null, "report-no": "ERIC:11-013", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications log a large amount of events continuously. Extracting\ninteresting knowledge from logged events is an emerging active research area in\ndata mining. In this context, we propose an approach for mining frequent events\nand association rules from logged events in XML format. This approach is\ncomposed of two-main phases: I) constructing a novel tree structure called\nFrequency XML-based Tree (FXT), which contains the frequency of events to be\nmined; II) querying the constructed FXT using XQuery to discover frequent\nitemsets and association rules. The FXT is constructed with a single-pass over\nlogged data. We implement the proposed algorithm and study various performance\nissues. The performance study shows that the algorithm is efficient, for both\nconstructing the FXT and discovering association rules.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2011 09:25:52 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Salem", "Rashed", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Boussa\u00efd", "Omar", "", "ERIC"]]}, {"id": "1110.3569", "submitter": "Rahmat Widia Sembiring", "authors": "Rahmat Widia Sembiring, Jasni Mohamad Zain, Abdullah Embong", "title": "Dimension Reduction of Health Data Clustering", "comments": "10 pages, 9 figures, published at International Journal on New\n  Computer Architectures and Their Applications (IJNCAA)", "journal-ref": "International Journal on New Computer Architectures and Their\n  Applications (IJNCAA), 2011, Vol.1, No.4, 1041-1050", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current data tends to be more complex than conventional data and need\ndimension reduction. Dimension reduction is important in cluster analysis and\ncreates a smaller data in volume and has the same analytical results as the\noriginal representation. A clustering process needs data reduction to obtain an\nefficient processing time while clustering and mitigate curse of\ndimensionality. This paper proposes a model for extracting multidimensional\ndata clustering of health database. We implemented four dimension reduction\ntechniques such as Singular Value Decomposition (SVD), Principal Component\nAnalysis (PCA), Self Organizing Map (SOM) and FastICA. The results show that\ndimension reductions significantly reduce dimension and shorten processing time\nand also increased performance of cluster in several health datasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 03:40:07 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Sembiring", "Rahmat Widia", ""], ["Zain", "Jasni Mohamad", ""], ["Embong", "Abdullah", ""]]}, {"id": "1110.3741", "submitter": "Kevin Xu", "authors": "Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder, and Alfred O. Hero III", "title": "Multi-criteria Anomaly Detection using Pareto Depth Analysis", "comments": "Removed an unnecessary line from Algorithm 1", "journal-ref": "Advances in Neural Information Processing Systems 25 (2012)\n  854-862", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying patterns in a data set that exhibit\nanomalous behavior, often referred to as anomaly detection. In most anomaly\ndetection algorithms, the dissimilarity between data samples is calculated by a\nsingle criterion, such as Euclidean distance. However, in many cases there may\nnot exist a single dissimilarity measure that captures all possible anomalous\npatterns. In such a case, multiple criteria can be defined, and one can test\nfor anomalies by scalarizing the multiple criteria using a linear combination\nof them. If the importance of the different criteria are not known in advance,\nthe algorithm may need to be executed multiple times with different choices of\nweights in the linear combination. In this paper, we introduce a novel\nnon-parametric multi-criteria anomaly detection method using Pareto depth\nanalysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies\nunder multiple criteria without having to run an algorithm multiple times with\ndifferent choices of weights. The proposed PDA approach scales linearly in the\nnumber of criteria and is provably better than linear combinations of the\ncriteria.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 17:48:22 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 22:12:52 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2013 17:18:42 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Hsiao", "Ko-Jen", ""], ["Xu", "Kevin S.", ""], ["Calder", "Jeff", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1110.3767", "submitter": "Herve Jegou", "authors": "Herv\\'e J\\'egou (INRIA - IRISA), Teddy Furon (INRIA - IRISA),\n  Jean-Jacques Fuchs (INRIA - IRISA)", "title": "Anti-sparse coding for approximate nearest neighbor search", "comments": "submitted to ICASSP'2012; RR-7771 (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a binarization scheme for vectors of high dimension based\non the recent concept of anti-sparse coding, and shows its excellent\nperformance for approximate nearest neighbor search. Unlike other binarization\nschemes, this framework allows, up to a scaling factor, the explicit\nreconstruction from the binary representation of the original vector. The paper\nalso shows that random projections which are used in Locality Sensitive Hashing\nalgorithms, are significantly outperformed by regular frames for both synthetic\nand real data if the number of bits exceeds the vector dimensionality, i.e.,\nwhen high precision is required.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 19:16:16 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2011 06:13:45 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["J\u00e9gou", "Herv\u00e9", "", "INRIA - IRISA"], ["Furon", "Teddy", "", "INRIA - IRISA"], ["Fuchs", "Jean-Jacques", "", "INRIA - IRISA"]]}, {"id": "1110.3879", "submitter": "Akihiro Inokuchi", "authors": "Akihiro Inokuchi, Hiroaki Ikuta, and Takashi Washio", "title": "GTRACE-RS: Efficient Graph Sequence Mining using Reverse Search", "comments": null, "journal-ref": null, "doi": "10.1587/transinf.E95.D.1947", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mining of frequent subgraphs from labeled graph data has been studied\nextensively. Furthermore, much attention has recently been paid to frequent\npattern mining from graph sequences. A method, called GTRACE, has been proposed\nto mine frequent patterns from graph sequences under the assumption that\nchanges in graphs are gradual. Although GTRACE mines the frequent patterns\nefficiently, it still needs substantial computation time to mine the patterns\nfrom graph sequences containing large graphs and long sequences. In this paper,\nwe propose a new version of GTRACE that enables efficient mining of frequent\npatterns based on the principle of a reverse search. The underlying concept of\nthe reverse search is a general scheme for designing efficient algorithms for\nhard enumeration problems. Our performance study shows that the proposed method\nis efficient and scalable for mining both long and large graph sequence\npatterns and is several orders of magnitude faster than the original GTRACE.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 05:48:03 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Inokuchi", "Akihiro", ""], ["Ikuta", "Hiroaki", ""], ["Washio", "Takashi", ""]]}, {"id": "1110.6647", "submitter": "Andrew Pavlo", "authors": "Andrew Pavlo, Evan P.C. Jones, Stanley Zdonik", "title": "On Predictive Modeling for Optimizing Transaction Execution in Parallel\n  OLTP Systems", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.\n  85-96 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new emerging class of parallel database management systems (DBMS) is\ndesigned to take advantage of the partitionable workloads of on-line\ntransaction processing (OLTP) applications. Transactions in these systems are\noptimized to execute to completion on a single node in a shared-nothing cluster\nwithout needing to coordinate with other nodes or use expensive concurrency\ncontrol measures. But some OLTP applications cannot be partitioned such that\nall of their transactions execute within a single-partition in this manner.\nThese distributed transactions access data not stored within their local\npartitions and subsequently require more heavy-weight concurrency control\nprotocols. Further difficulties arise when the transaction's execution\nproperties, such as the number of partitions it may need to access or whether\nit will abort, are not known beforehand. The DBMS could mitigate these\nperformance issues if it is provided with additional information about\ntransactions. Thus, in this paper we present a Markov model-based approach for\nautomatically selecting which optimizations a DBMS could use, namely (1) more\nefficient concurrency control schemes, (2) intelligent scheduling, (3) reduced\nundo logging, and (4) speculative execution. To evaluate our techniques, we\nimplemented our models and integrated them into a parallel, main-memory OLTP\nDBMS to show that we can improve the performance of applications with diverse\nworkloads.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2011 20:21:05 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Pavlo", "Andrew", ""], ["Jones", "Evan P. C.", ""], ["Zdonik", "Stanley", ""]]}, {"id": "1110.6648", "submitter": "Konstantinos Karanasos", "authors": "Fran\\c{c}ois Goasdou\\'e, Konstantinos Karanasos, Julien Leblay, Ioana\n  Manolescu", "title": "View Selection in Semantic Web Databases", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.\n  97-108 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of a Semantic Web database, containing both explicit\ndata encoded in RDF triples, and implicit data, implied by the RDF semantics.\nBased on a query workload, we address the problem of selecting a set of views\nto be materialized in the database, minimizing a combination of query\nprocessing, view storage, and view maintenance costs. Starting from an existing\nrelational view selection method, we devise new algorithms for recommending\nview sets, and show that they scale significantly beyond the existing\nrelational ones when adapted to the RDF context. To account for implicit\ntriples in query answers, we propose a novel RDF query reformulation algorithm\nand an innovative way of incorporating it into view selection in order to avoid\na combinatorial explosion in the complexity of the selection process. The\ninterest of our techniques is demonstrated through a set of experiments.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2011 20:21:16 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Goasdou\u00e9", "Fran\u00e7ois", ""], ["Karanasos", "Konstantinos", ""], ["Leblay", "Julien", ""], ["Manolescu", "Ioana", ""]]}, {"id": "1110.6649", "submitter": "Feifei Li", "authors": "Jeffrey Jestes, Ke Yi, Feifei Li", "title": "Building Wavelet Histograms on Large Data in MapReduce", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.\n  109-120 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce is becoming the de facto framework for storing and processing\nmassive data, due to its excellent scalability, reliability, and elasticity. In\nmany MapReduce applications, obtaining a compact accurate summary of data is\nessential. Among various data summarization tools, histograms have proven to be\nparticularly important and useful for summarizing data, and the wavelet\nhistogram is one of the most widely used histograms. In this paper, we\ninvestigate the problem of building wavelet histograms efficiently on large\ndatasets in MapReduce. We measure the efficiency of the algorithms by both\nend-to-end running time and communication cost. We demonstrate straightforward\nadaptations of existing exact and approximate methods for building wavelet\nhistograms to MapReduce clusters are highly inefficient. To that end, we design\nnew algorithms for computing exact and approximate wavelet histograms and\ndiscuss their implementation in MapReduce. We illustrate our techniques in\nHadoop, and compare to baseline solutions with extensive experiments performed\nin a heterogeneous Hadoop cluster of 16 nodes, using large real and synthetic\ndatasets, up to hundreds of gigabytes. The results suggest significant (often\norders of magnitude) performance improvement achieved by our new algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2011 20:21:30 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Jestes", "Jeffrey", ""], ["Yi", "Ke", ""], ["Li", "Feifei", ""]]}, {"id": "1110.6650", "submitter": "Di Yang", "authors": "Di Yang, Elke A. Rundensteiner, Matthew O. Ward", "title": "Summarization and Matching of Density-Based Clusters in Streaming\n  Environments", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.\n  121-132 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density-based cluster mining is known to serve a broad range of applications\nranging from stock trade analysis to moving object monitoring. Although methods\nfor efficient extraction of density-based clusters have been studied in the\nliterature, the problem of summarizing and matching of such clusters with\narbitrary shapes and complex cluster structures remains unsolved. Therefore,\nthe goal of our work is to extend the state-of-art of density-based cluster\nmining in streams from cluster extraction only to now also support analysis and\nmanagement of the extracted clusters. Our work solves three major technical\nchallenges. First, we propose a novel multi-resolution cluster summarization\nmethod, called Skeletal Grid Summarization (SGS), which captures the key\nfeatures of density-based clusters, covering both their external shape and\ninternal cluster structures. Second, in order to summarize the extracted\nclusters in real-time, we present an integrated computation strategy C-SGS,\nwhich piggybacks the generation of cluster summarizations within the online\nclustering process. Lastly, we design a mechanism to efficiently execute\ncluster matching queries, which identify similar clusters for given cluster of\nanalyst's interest from clusters extracted earlier in the stream history. Our\nexperimental study using real streaming data shows the clear superiority of our\nproposed methods in both efficiency and effectiveness for cluster summarization\nand cluster matching queries to other potential alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2011 20:21:40 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Yang", "Di", ""], ["Rundensteiner", "Elke A.", ""], ["Ward", "Matthew O.", ""]]}, {"id": "1110.6651", "submitter": "Thanh Nguyen", "authors": "Thanh Nguyen, Viviane Moreira, Huong Nguyen, Hoa Nguyen, Juliana\n  Freire", "title": "Multilingual Schema Matching for Wikipedia Infoboxes", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.\n  133-144 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has taken advantage of Wikipedia's multilingualism as a\nresource for cross-language information retrieval and machine translation, as\nwell as proposed techniques for enriching its cross-language structure. The\navailability of documents in multiple languages also opens up new opportunities\nfor querying structured Wikipedia content, and in particular, to enable answers\nthat straddle different languages. As a step towards supporting such queries,\nin this paper, we propose a method for identifying mappings between attributes\nfrom infoboxes that come from pages in different languages. Our approach finds\nmappings in a completely automated fashion. Because it does not require\ntraining data, it is scalable: not only can it be used to find mappings between\nmany language pairs, but it is also effective for languages that are\nunder-represented and lack sufficient training samples. Another important\nbenefit of our approach is that it does not depend on syntactic similarity\nbetween attribute names, and thus, it can be applied to language pairs that\nhave distinct morphologies. We have performed an extensive experimental\nevaluation using a corpus consisting of pages in Portuguese, Vietnamese, and\nEnglish. The results show that not only does our approach obtain high precision\nand recall, but it also outperforms state-of-the-art techniques. We also\npresent a case study which demonstrates that the multilingual mappings we\nderive lead to substantial improvements in answer quality and coverage for\nstructured queries over Wikipedia content.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2011 20:21:47 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Nguyen", "Thanh", ""], ["Moreira", "Viviane", ""], ["Nguyen", "Huong", ""], ["Nguyen", "Hoa", ""], ["Freire", "Juliana", ""]]}, {"id": "1110.6652", "submitter": "Guimei Liu", "authors": "Guimei Liu, Haojun Zhang, Limsoon Wong", "title": "Controlling False Positives in Association Rule Mining", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.\n  145-156 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association rule mining is an important problem in the data mining area. It\nenumerates and tests a large number of rules on a dataset and outputs rules\nthat satisfy user-specified constraints. Due to the large number of rules being\ntested, rules that do not represent real systematic effect in the data can\nsatisfy the given constraints purely by random chance. Hence association rule\nmining often suffers from a high risk of false positive errors. There is a lack\nof comprehensive study on controlling false positives in association rule\nmining. In this paper, we adopt three multiple testing correction\napproaches---the direct adjustment approach, the permutation-based approach and\nthe holdout approach---to control false positives in association rule mining,\nand conduct extensive experiments to study their performance. Our results show\nthat (1) Numerous spurious rules are generated if no correction is made. (2)\nThe three approaches can control false positives effectively. Among the three\napproaches, the permutation-based approach has the highest power of detecting\nreal association rules, but it is very computationally expensive. We employ\nseveral techniques to reduce its cost effectively.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2011 20:22:00 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Liu", "Guimei", ""], ["Zhang", "Haojun", ""], ["Wong", "Limsoon", ""]]}]