[{"id": "1706.00327", "submitter": "Thanh Lam Hoang", "authors": "Hoang Thanh Lam, Johann-Michael Thiebaut, Mathieu Sinn, Bei Chen, Tiep\n  Mai and Oznur Alkan", "title": "One button machine for automating feature engineering in relational\n  databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature engineering is one of the most important and time consuming tasks in\npredictive analytics projects. It involves understanding domain knowledge and\ndata exploration to discover relevant hand-crafted features from raw data. In\nthis paper, we introduce a system called One Button Machine, or OneBM for\nshort, which automates feature discovery in relational databases. OneBM\nautomatically performs a key activity of data scientists, namely, joining of\ndatabase tables and applying advanced data transformations to extract useful\nfeatures from data. We validated OneBM in Kaggle competitions in which OneBM\nachieved performance as good as top 16% to 24% data scientists in three Kaggle\ncompetitions. More importantly, OneBM outperformed the state-of-the-art system\nin a Kaggle competition in terms of prediction accuracy and ranking on Kaggle\nleaderboard. The results show that OneBM can be useful for both data scientists\nand non-experts. It helps data scientists reduce data exploration time allowing\nthem to try and error many ideas in short time. On the other hand, it enables\nnon-experts, who are not familiar with data science, to quickly extract value\nfrom their data with a little effort, time and cost.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 14:44:34 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Lam", "Hoang Thanh", ""], ["Thiebaut", "Johann-Michael", ""], ["Sinn", "Mathieu", ""], ["Chen", "Bei", ""], ["Mai", "Tiep", ""], ["Alkan", "Oznur", ""]]}, {"id": "1706.00757", "submitter": "Juan Colmenares", "authors": "Ying Lu and Juan A. Colmenares", "title": "Efficient Detection of Points of Interest from Georeferenced Visual\n  Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people take photos and videos with smartphones and more recently with\n360-degree cameras at popular places and events, and share them in social\nmedia. Such visual content is produced in large volumes in urban areas, and it\nis a source of information that online users could exploit to learn what has\ngot the interest of the general public on the streets of the cities where they\nlive or plan to visit. A key step to providing users with that information is\nto identify the most popular k spots in specified areas. In this paper, we\npropose a clustering and incremental sampling (C&IS) approach that trades off\naccuracy of top-k results for detection speed. It uses clustering to determine\nareas with high density of visual content, and incremental sampling, controlled\nby stopping criteria, to limit the amount of computational work. It leverages\nspatial metadata, which represent the scenes in the visual content, to rapidly\ndetect the hotspots, and uses a recently proposed Gaussian probability model to\ndescribe the capture intention distribution in the query area. We evaluate the\napproach with metadata, derived from a non-synthetic, user-generated dataset,\nfor regular mobile and 360-degree visual content. Our results show that the\nC&IS approach offers 2.8x-19x reductions in processing time over an optimized\nbaseline, while in most cases correctly identifying 4 out of 5 top locations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 17:03:06 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Lu", "Ying", ""], ["Colmenares", "Juan A.", ""]]}, {"id": "1706.01449", "submitter": "Firas Abuzaid", "authors": "Firas Abuzaid, Geet Sethi, Peter Bailis, Matei Zaharia", "title": "To Index or Not to Index: Optimizing Exact Maximum Inner Product Search", "comments": "12 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact Maximum Inner Product Search (MIPS) is an important task that is widely\npertinent to recommender systems and high-dimensional similarity search. The\nbrute-force approach to solving exact MIPS is computationally expensive, thus\nspurring recent development of novel indexes and pruning techniques for this\ntask. In this paper, we show that a hardware-efficient brute-force approach,\nblocked matrix multiply (BMM), can outperform the state-of-the-art MIPS solvers\nby over an order of magnitude, for some -- but not all -- inputs.\n  In this paper, we also present a novel MIPS solution, MAXIMUS, that takes\nadvantage of hardware efficiency and pruning of the search space. Like BMM,\nMAXIMUS is faster than other solvers by up to an order of magnitude, but again\nonly for some inputs. Since no single solution offers the best runtime\nperformance for all inputs, we introduce a new data-dependent optimizer,\nOPTIMUS, that selects online with minimal overhead the best MIPS solver for a\ngiven input. Together, OPTIMUS and MAXIMUS outperform state-of-the-art MIPS\nsolvers by 3.2$\\times$ on average, and up to 10.9$\\times$, on widely studied\nMIPS datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:56:43 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 22:08:15 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 00:52:25 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Abuzaid", "Firas", ""], ["Sethi", "Geet", ""], ["Bailis", "Peter", ""], ["Zaharia", "Matei", ""]]}, {"id": "1706.01762", "submitter": "Klaus-Dieter Schewe", "authors": "Egon B\\\"orger, Klaus-Dieter Schewe", "title": "Specifying Transaction Control to Serialize Concurrent Program\n  Executions", "comments": "14 pages, 1 figure", "journal-ref": "LNCS, vol. 8477, Springer 2014, pp. 142-157", "doi": "10.1007/978-3-662-43652-3_13", "report-no": null, "categories": "cs.DB cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a programming language independent transaction controller and an\noperator which when applied to concurrent programs with shared locations turns\ntheir behavior with respect to some abstract termination criterion into a\ntransactional behavior. We prove the correctness property that concurrent runs\nunder the transaction controller are serialisable. We specify the transaction\ncontroller TaCtl and the operator TA in terms of Abstract State Machines. This\nmakes TaCtl applicable to a wide range of programs and in particular provides\nthe possibility to use it as a plug-in when specifying concurrent system\ncomponents in terms of Abstract State Machines.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 13:43:08 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["B\u00f6rger", "Egon", ""], ["Schewe", "Klaus-Dieter", ""]]}, {"id": "1706.02109", "submitter": "Maikel Van Eck", "authors": "Maikel L. van Eck, Natalia Sidorova, Wil M.P. van der Aalst", "title": "Guided Interaction Exploration in Artifact-centric Process Models", "comments": "10 pages, 4 figures, to be published in proceedings of the 19th IEEE\n  Conference on Business Informatics, CBI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artifact-centric process models aim to describe complex processes as a\ncollection of interacting artifacts. Recent development in process mining allow\nfor the discovery of such models. However, the focus is often on the\nrepresentation of the individual artifacts rather than their interactions.\nBased on event data we can automatically discover composite state machines\nrepresenting artifact-centric processes. Moreover, we provide ways of\nvisualizing and quantifying interactions among different artifacts. For\nexample, we are able to highlight strongly correlated behaviours in different\nartifacts. The approach has been fully implemented as a ProM plug-in; the CSM\nMiner provides an interactive artifact-centric process discovery tool focussing\non interactions. The approach has been evaluated using real life data sets,\nincluding the personal loan and overdraft process of a Dutch financial\ninstitution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 09:55:48 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["van Eck", "Maikel L.", ""], ["Sidorova", "Natalia", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1706.02263", "submitter": "Rianne van den Berg", "authors": "Rianne van den Berg, Thomas N. Kipf, Max Welling", "title": "Graph Convolutional Matrix Completion", "comments": "9 pages, 3 figures, updated with additional experimental evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider matrix completion for recommender systems from the point of view\nof link prediction on graphs. Interaction data such as movie ratings can be\nrepresented by a bipartite user-item graph with labeled edges denoting observed\nratings. Building on recent progress in deep learning on graph-structured data,\nwe propose a graph auto-encoder framework based on differentiable message\npassing on the bipartite interaction graph. Our model shows competitive\nperformance on standard collaborative filtering benchmarks. In settings where\ncomplimentary feature information or structured data such as a social network\nis available, our framework outperforms recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:05:19 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 19:20:03 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Berg", "Rianne van den", ""], ["Kipf", "Thomas N.", ""], ["Welling", "Max", ""]]}, {"id": "1706.02473", "submitter": "Marco Guarnieri", "authors": "Marco Guarnieri and Srdjan Marinovic and David Basin", "title": "Securing Databases from Probabilistic Inference", "comments": "A short version of this paper has been accepted at the 30th IEEE\n  Computer Security Foundations Symposium (CSF 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases can leak confidential information when users combine query results\nwith probabilistic data dependencies and prior knowledge. Current research\noffers mechanisms that either handle a limited class of dependencies or lack\ntractable enforcement algorithms. We propose a foundation for Database\nInference Control based on ProbLog, a probabilistic logic programming language.\nWe leverage this foundation to develop Angerona, a provably secure enforcement\nmechanism that prevents information leakage in the presence of probabilistic\ndependencies. We then provide a tractable inference algorithm for a practically\nrelevant fragment of ProbLog. We empirically evaluate Angerona's performance\nshowing that it scales to relevant security-critical problems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 08:17:16 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Guarnieri", "Marco", ""], ["Marinovic", "Srdjan", ""], ["Basin", "David", ""]]}, {"id": "1706.02562", "submitter": "Benjamin Rubinstein", "authors": "Benjamin I. P. Rubinstein, Francesco Ald\\`a", "title": "Pain-Free Random Differential Privacy with Sensitivity Sampling", "comments": "12 pages, 9 figures, 1 table; full report of paper accepted into\n  ICML'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular approaches to differential privacy, such as the Laplace and\nexponential mechanisms, calibrate randomised smoothing through global\nsensitivity of the target non-private function. Bounding such sensitivity is\noften a prohibitively complex analytic calculation. As an alternative, we\npropose a straightforward sampler for estimating sensitivity of non-private\nmechanisms. Since our sensitivity estimates hold with high probability, any\nmechanism that would be $(\\epsilon,\\delta)$-differentially private under\nbounded global sensitivity automatically achieves\n$(\\epsilon,\\delta,\\gamma)$-random differential privacy (Hall et al., 2012),\nwithout any target-specific calculations required. We demonstrate on worked\nexample learners how our usable approach adopts a naturally-relaxed privacy\nguarantee, while achieving more accurate releases even for non-private\nfunctions that are black-box computer programs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 13:06:34 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Rubinstein", "Benjamin I. P.", ""], ["Ald\u00e0", "Francesco", ""]]}, {"id": "1706.02591", "submitter": "Serkan Ayvaz", "authors": "Serkan Ayvaz and Mehmet Aydar", "title": "Dynamic Discovery of Type Classes and Relations in Semantic Web Data", "comments": null, "journal-ref": "J Data Semant (2019) 8: 57", "doi": "10.1007/s13740-019-00102-6", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuing development of Semantic Web technologies and the increasing\nuser adoption in the recent years have accelerated the progress incorporating\nexplicit semantics with data on the Web. With the rapidly growing RDF (Resource\nDescription Framework) data on the Semantic Web, processing large semantic\ngraph data have become more challenging. Constructing a summary graph structure\nfrom the raw RDF can help obtain semantic type relations and reduce the\ncomputational complexity for graph processing purposes. In this paper, we\naddressed the problem of graph summarization in RDF graphs, and we proposed an\napproach for building summary graph structures automatically from RDF graph\ndata. Moreover, we introduced a measure to help discover optimum class\ndissimilarity thresholds and an effective method to discover the type classes\nautomatically. In future work, we plan to investigate further improvement\noptions on the scalability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 11:58:31 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Ayvaz", "Serkan", ""], ["Aydar", "Mehmet", ""]]}, {"id": "1706.02785", "submitter": "Ophir Lojkine", "authors": "Ophir Lojkine", "title": "Optimal parameters for bloom-filtered joins in Spark", "comments": "The article is in Russian, but an analysis of the data used in it is\n  available in english at the following address:\n  https://github.com/lovasoa/spark-bloomfiltered-join-analysis/blob/master/analysis.ipynb", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an algorithm that joins relational database tables\nefficiently in a distributed environment using Bloom filters of an optimal\nsize. We propose not to use fixed-size bloom filters as in previous research,\nbut to find an optimal size for the bloom filters, by creating a mathematical\nmodel of the join algorithm, and then finding the optimal parameters using\ntraditional mathematical optimization.\n  This algorithm with optimal parameters beats both previous approaches using\nbloom filters and the default SparkSQL engine not only on star-joins, but also\non traditional database schema. The experiments were conducted on a standard\nTPC-H database stored as parquet files on a distributed file system.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 22:29:41 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 12:49:15 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Lojkine", "Ophir", ""]]}, {"id": "1706.03317", "submitter": "Marius Rafailescu", "authors": "Marius Rafailescu", "title": "Fault Tolerant Consensus Agreement Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently a new fault tolerant and simple mechanism was designed for solving\ncommit consensus problem. It is based on replicated validation of messages sent\nbetween transaction participants and a special dispatcher validator manager\nnode. This paper presents a correctness, safety proofs and performance analysis\nof this algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 07:28:48 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Rafailescu", "Marius", ""]]}, {"id": "1706.03327", "submitter": "G Kavitha", "authors": "Ms. Ganesan Kavitha and Dr. Lawrance Raj", "title": "Educational Data Mining and Learning Analytics - Educational Assistance\n  for Teaching and Learning", "comments": "5 Pages, 5 Tables and 1 Figure, Internal Journal of Computer and\n  Organization Trends (IJCOT), March 2017", "journal-ref": null, "doi": "10.14445/22492593/IJCOT-V41P304", "report-no": null, "categories": "cs.SY cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Teaching and Learning process of an educational institution needs to be\nmonitored and effectively analysed for enhancement. Teaching and Learning is a\nvital element for an educational institution. It is also one of the criteria\nset by majority of the Accreditation Agencies around the world. Learning\nanalytics and Educational Data Mining are relatively new. Learning analytics\nrefers to the collection of large volume of data about students in an\neducational setting and to analyse the data to predict the students' future\nperformance and identify risk. Educational Data Mining (EDM) is develops\nmethods to analyse the data produced by the students in educational settings\nand these methods helps to understand the students and the setting where they\nlearn. Aim of this research is to collect large collection of data on students'\nperformance in their assessment to discover the students at risk of failing the\nfinal exam. This analysis will help to understand how the students are\nprogressing. The proposed research aimed to utilize the result of the analysis\nto identify the students at risk and provide recommendations for improvement.\nThe proposed research aimed to collect and analyse the result of the assessment\nat the course level to enhance the teaching and learning process. The research\naimed to discuss two feature selection techniques namely information gain and\ngain ratio and adopted to use gain ratio as the feature selection technique.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 09:03:39 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Kavitha", "Ms. Ganesan", ""], ["Raj", "Dr. Lawrance", ""]]}, {"id": "1706.03374", "submitter": "Kijung Shin", "authors": "Kijung Shin, Bryan Hooi, Jisu Kim, Christos Faloutsos", "title": "DenseAlert: Incremental Dense-Subtensor Detection in Tensor Streams", "comments": "to be published in 23rd ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining (KDD-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a stream of retweet events - how can we spot fraudulent lock-step\nbehavior in such multi-aspect data (i.e., tensors) evolving over time? Can we\ndetect it in real time, with an accuracy guarantee? Past studies have shown\nthat dense subtensors tend to indicate anomalous or even fraudulent behavior in\nmany tensor data, including social media, Wikipedia, and TCP dumps. Thus,\nseveral algorithms have been proposed for detecting dense subtensors rapidly\nand accurately. However, existing algorithms assume that tensors are static,\nwhile many real-world tensors, including those mentioned above, evolve over\ntime.\n  We propose DenseStream, an incremental algorithm that maintains and updates a\ndense subtensor in a tensor stream (i.e., a sequence of changes in a tensor),\nand DenseAlert, an incremental algorithm spotting the sudden appearances of\ndense subtensors. Our algorithms are: (1) Fast and 'any time': updates by our\nalgorithms are up to a million times faster than the fastest batch algorithms,\n(2) Provably accurate: our algorithms guarantee a lower bound on the density of\nthe subtensor they maintain, and (3) Effective: our DenseAlert successfully\nspots anomalies in real-world tensors, especially those overlooked by existing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 16:26:40 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Shin", "Kijung", ""], ["Hooi", "Bryan", ""], ["Kim", "Jisu", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1706.03968", "submitter": "Alexander Krause", "authors": "Alexander Krause, Annett Ungeth\\\"um, Thomas Kissinger, Dirk Habich,\n  Wolfgang Lehner", "title": "Asynchronous Graph Pattern Matching on Multiprocessor Systems", "comments": "14 Pages, Extended version for ADBIS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching on large graphs is the foundation for a variety of\napplication domains. Strict latency requirements and continuously increasing\ngraph sizes demand the usage of highly parallel in-memory graph processing\nengines that need to consider non-uniform memory access (NUMA) and concurrency\nissues to scale up on modern multiprocessor systems. To tackle these aspects,\ngraph partitioning becomes increasingly important. Hence, we present a\ntechnique to process graph pattern matching on NUMA systems in this paper. As a\nscalable pattern matching processing infrastructure, we leverage a\ndata-oriented architecture that preserves data locality and minimizes\nconcurrency-related bottlenecks on NUMA systems. We show in detail, how graph\npattern matching can be asynchronously processed on a multiprocessor system.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 09:32:37 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 13:58:32 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Krause", "Alexander", ""], ["Ungeth\u00fcm", "Annett", ""], ["Kissinger", "Thomas", ""], ["Habich", "Dirk", ""], ["Lehner", "Wolfgang", ""]]}, {"id": "1706.04043", "submitter": "Klaus-Dieter Schewe", "authors": "Egon B\\\"orger, Klaus-Dieter Schewe, Qing Wang", "title": "Serialisable Multi-Level Transaction Control: A Specification and\n  Verification", "comments": "25 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1706.01762", "journal-ref": "Sci. Comput. Program. 131: 42-58 (2016)", "doi": "10.1016/j.scico.2016.03.008", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a programming language independent controller TaCtl for multi-level\ntransactions and an operator $TA$, which when applied to concurrent programs\nwith multi-level shared locations containing hierarchically structured complex\nvalues, turns their behavior with respect to some abstract termination\ncriterion into a transactional behavior. We prove the correctness property that\nconcurrent runs under the transaction controller are serialisable, assuming an\nInverse Operation Postulate to guarantee recoverability. For its applicability\nto a wide range of programs we specify the transaction controller TaCtl and the\noperator $TA$ in terms of Abstract State Machines (ASMs). This allows us to\nmodel concurrent updates at different levels of nested locations in a precise\nyet simple manner, namely in terms of partial ASM updates. It also provides the\npossibility to use the controller TaCtl and the operator $TA$ as a plug-in when\nspecifying concurrent system components in terms of sequential ASMs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 08:34:35 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["B\u00f6rger", "Egon", ""], ["Schewe", "Klaus-Dieter", ""], ["Wang", "Qing", ""]]}, {"id": "1706.04266", "submitter": "Chuancong Gao", "authors": "Chuancong Gao, Jiannan Wang, Jian Pei, Rui Li, Yi Chang", "title": "Preference-driven Similarity Join", "comments": null, "journal-ref": null, "doi": "10.1145/3106426.3106484", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity join, which can find similar objects (e.g., products, names,\naddresses) across different sources, is powerful in dealing with variety in big\ndata, especially web data. Threshold-driven similarity join, which has been\nextensively studied in the past, assumes that a user is able to specify a\nsimilarity threshold, and then focuses on how to efficiently return the object\npairs whose similarities pass the threshold. We argue that the assumption about\na well set similarity threshold may not be valid for two reasons. The optimal\nthresholds for different similarity join tasks may vary a lot. Moreover, the\nend-to-end time spent on similarity join is likely to be dominated by a\nback-and-forth threshold-tuning process.\n  In response, we propose preference-driven similarity join. The key idea is to\nprovide several result-set preferences, rather than a range of thresholds, for\na user to choose from. Intuitively, a result-set preference can be considered\nas an objective function to capture a user's preference on a similarity join\nresult. Once a preference is chosen, we automatically compute the similarity\njoin result optimizing the preference objective. As the proof of concept, we\ndevise two useful preferences and propose a novel preference-driven similarity\njoin framework coupled with effective optimization techniques. Our approaches\nare evaluated on four real-world web datasets from a diverse range of\napplication scenarios. The experiments show that preference-driven similarity\njoin can achieve high-quality results without a tedious threshold-tuning\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 21:59:11 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 06:30:34 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 18:08:05 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Gao", "Chuancong", ""], ["Wang", "Jiannan", ""], ["Pei", "Jian", ""], ["Li", "Rui", ""], ["Chang", "Yi", ""]]}, {"id": "1706.05476", "submitter": "Zijian Li", "authors": "Zijian Li, Xun Jian, Xiang Lian, Lei Chen", "title": "An Efficient Probabilistic Approach for Graph Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph similarity search is a common and fundamental operation in graph\ndatabases. One of the most popular graph similarity measures is the Graph Edit\nDistance (GED) mainly because of its broad applicability and high\ninterpretability. Despite its prevalence, exact GED computation is proved to be\nNP-hard, which could result in unsatisfactory computational efficiency on large\ngraphs. However, exactly accurate search results are usually unnecessary for\nreal-world applications especially when the responsiveness is far more\nimportant than the accuracy. Thus, in this paper, we propose a novel\nprobabilistic approach to efficiently estimate GED, which is further leveraged\nfor the graph similarity search. Specifically, we first take branches as\nelementary structures in graphs, and introduce a novel graph similarity measure\nby comparing branches between graphs, i.e., Graph Branch Distance (GBD), which\ncan be efficiently calculated in polynomial time. Then, we formulate the\nrelationship between GED and GBD by considering branch variations as the result\nascribed to graph edit operations, and model this process by probabilistic\napproaches. By applying our model, the GED between any two graphs can be\nefficiently estimated by their GBD, and these estimations are finally utilized\nin the graph similarity search. Extensive experiments show that our approach\nhas better accuracy, efficiency and scalability than other comparable methods\nin the graph similarity search over real and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 05:25:10 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 19:42:42 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Li", "Zijian", ""], ["Jian", "Xun", ""], ["Lian", "Xiang", ""], ["Chen", "Lei", ""]]}, {"id": "1706.05714", "submitter": "Stratos Idreos", "authors": "Stratos Idreos and Lukas M. Maas and Mike S. Kester", "title": "Evolutionary Data Systems", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anyone in need of a data system today is confronted with numerous complex\noptions in terms of system architectures, such as traditional relational\ndatabases, NoSQL and NewSQL solutions as well as several sub-categories like\ncolumn-stores, row-stores etc. This overwhelming array of choices makes\nbootstrapping data-driven applications difficult and time consuming, requiring\nexpertise often not accessible due to cost issues (e.g., to scientific labs or\nsmall businesses). In this paper, we present the vision of evolutionary data\nsystems that free systems architects and application designers from the\ncomplex, cumbersome and expensive process of designing and tuning specialized\ndata system architectures that fit only a single, static application scenario.\nSetting up an evolutionary system is as simple as identifying the data. As new\ndata and queries come in, the system automatically evolves so that its\narchitecture matches the properties of the incoming workload at all times.\nInspired by the theory of evolution, at any given point in time, an\nevolutionary system may employ multiple competing solutions down at the low\nlevel of database architectures -- characterized as combinations of data\nlayouts, access methods and execution strategies. Over time, \"the fittest wins\"\nand becomes the dominant architecture until the environment (workload) changes.\nIn our initial prototype, we demonstrate solutions that can seamlessly evolve\n(back and forth) between a key-value store and a column-store architecture in\norder to adapt to changing workloads.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 20:07:56 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Idreos", "Stratos", ""], ["Maas", "Lukas M.", ""], ["Kester", "Mike S.", ""]]}, {"id": "1706.05913", "submitter": "Pontus Svenson", "authors": "Magnus J\\\"andel, Pontus Svenson, Ronnie Johansson", "title": "Fusing restricted information", "comments": "9 pages, author contacts: xpontus@gmail.com, ronniej@kth.se", "journal-ref": "Proc 17th Int Conf on Information Fusion (2014)", "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information fusion deals with the integration and merging of data and\ninformation from multiple (heterogeneous) sources. In many cases, the\ninformation that needs to be fused has security classification. The result of\nthe fusion process is then by necessity restricted with the strictest\ninformation security classification of the inputs. This has severe drawbacks\nand limits the possible dissemination of the fusion results. It leads to\ndecreased situational awareness: the organization knows information that would\nenable a better situation picture, but since parts of the information is\nrestricted, it is not possible to distribute the most correct situational\ninformation. In this paper, we take steps towards defining fusion and data\nmining processes that can be used even when all the underlying data that was\nused cannot be disseminated. The method we propose here could be used to\nproduce a classifier where all the sensitive information has been removed and\nwhere it can be shown that an antagonist cannot even in principle obtain\nknowledge about the classified information by using the classifier or situation\npicture.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 13:05:33 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["J\u00e4ndel", "Magnus", ""], ["Svenson", "Pontus", ""], ["Johansson", "Ronnie", ""]]}, {"id": "1706.05916", "submitter": "Audra McMillan", "authors": "Anna C. Gilbert and Audra McMillan", "title": "Local Differential Privacy for Physical Sensor Data and Sparse Recovery", "comments": "appeared at CISS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the utility of locally differentially private thermal\nsensor data. We design a locally differentially private recovery algorithm for\nthe 1-dimensional, discrete heat source location problem and analyse its\nperformance in terms of the Earth Mover Distance error. Our work indicates that\nit is possible to produce locally private sensor measurements that both keep\nthe exact locations of the heat sources private and permit recovery of the\n\"general geographic vicinity\" of the sources. We also discuss the relationship\nbetween the property of an inverse problem being ill-conditioned and the amount\nof noise needed to maintain privacy.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 03:15:22 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 15:43:20 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 02:06:44 GMT"}, {"version": "v4", "created": "Fri, 23 Mar 2018 22:13:16 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Gilbert", "Anna C.", ""], ["McMillan", "Audra", ""]]}, {"id": "1706.06664", "submitter": "Anshumali Shrivastava", "authors": "Chen Luo, Anshumali Shrivastava", "title": "Arrays of (locality-sensitive) Count Estimators (ACE): High-Speed\n  Anomaly Detection via Cache Lookups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is one of the frequent and important subroutines deployed\nin large-scale data processing systems. Even being a well-studied topic,\nexisting techniques for unsupervised anomaly detection require storing\nsignificant amounts of data, which is prohibitive from memory and latency\nperspective. In the big-data world existing methods fail to address the new set\nof memory and latency constraints. In this paper, we propose ACE (Arrays of\n(locality-sensitive) Count Estimators) algorithm that can be 60x faster than\nthe ELKI package~\\cite{DBLP:conf/ssd/AchtertBKSZ09}, which has the fastest\nimplementation of the unsupervised anomaly detection algorithms. ACE algorithm\nrequires less than $4MB$ memory, to dynamically compress the full data\ninformation into a set of count arrays. These tiny $4MB$ arrays of counts are\nsufficient for unsupervised anomaly detection. At the core of the ACE\nalgorithm, there is a novel statistical estimator which is derived from the\nsampling view of Locality Sensitive Hashing(LSH). This view is significantly\ndifferent and efficient than the widely popular view of LSH for near-neighbor\nsearch. We show the superiority of ACE algorithm over 11 popular baselines on 3\nbenchmark datasets, including the KDD-Cup99 data which is the largest available\nbenchmark comprising of more than half a million entries with ground truth\nanomaly labels.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 21:09:22 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Luo", "Chen", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1706.06697", "submitter": "Florian Gross", "authors": "Florian Gross", "title": "Index Search Algorithms for Databases and Modern CPUs", "comments": "Master Thesis with survey on hardware optimized index search\n  algorithms; 70 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, many different indexing techniques and search algorithms have\nbeen proposed, including CSS-trees, CSB+ trees, k-ary binary search, and fast\narchitecture sensitive tree search. There have also been papers on how best to\nset the many different parameters of these index structures, such as the node\nsize of CSB+ trees.\n  These indices have been proposed because CPU speeds have been increasing at a\ndramatically higher rate than memory speeds, giving rise to the Von Neumann\nCPU--Memory bottleneck. To hide the long latencies caused by memory access, it\nhas become very important to well-utilize the features of modern CPUs. In order\nto drive down the average number of CPU clock cycles required to execute CPU\ninstructions, and thus increase throughput, it has become important to achieve\na good utilization of CPU resources. Some of these are the data and instruction\ncaches, and the translation lookaside buffers. But it also has become important\nto avoid branch misprediction penalties, and utilize vectorization provided by\nCPUs in the form of SIMD instructions.\n  While the layout of index structures has been heavily optimized for the data\ncache of modern CPUs, the instruction cache has been neglected so far. In this\npaper, we present NitroGen, a framework for utilizing code generation for\nspeeding up index traversal in main memory database systems. By bringing\ntogether data and code, we make index structures use the dormant resource of\nthe instruction cache. We show how to combine index compilation with previous\napproaches, such as binary tree search, cache-sensitive tree search, and the\narchitecture-sensitive tree search presented by Kim et al.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 23:01:52 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Gross", "Florian", ""]]}, {"id": "1706.07294", "submitter": "Adeyinka K. Akanbi MR", "authors": "A. K. Akanbi, M. Masinde", "title": "A Framework for Accurate Drought Forecasting System Using\n  Semantics-Based Data Integration Middleware", "comments": "5 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1601.01920", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advancement in Wireless Sensor Networks (WSN) has made it\nbecome an invaluable component of a reliable environmental monitoring system;\nthey form the digital skin' through which to 'sense' and collect the context of\nthe surroundings and provides information on the process leading to complex\nevents such as drought. However, these environmental properties are measured by\nvarious heterogeneous sensors of different modalities in distributed locations\nmaking up the WSN, using different abstruse terms and vocabulary in most cases\nto denote the same observed property, causing data heterogeneity. Adding\nsemantics and understanding the relationships that exist between the observed\nproperties, and augmenting it with local indigenous knowledge is necessary for\nan accurate drought forecasting system. In this paper, we propose the framework\nfor the semantic representation of sensor data and integration with indigenous\nknowledge on drought using a middleware for an efficient drought forecasting\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 13:21:50 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Akanbi", "A. K.", ""], ["Masinde", "M.", ""]]}, {"id": "1706.07835", "submitter": "David Keator", "authors": "David B. Keator, Jinran Chen, B Nolan Nichols, Fariba Fana, Hal Stern,\n  Tallie Z. Baram, Steven L. Small", "title": "A Semantic Cross-Species Derived Data Management Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing dynamic information in large multi-site, multi-species, and\nmulti-discipline consortia is a challenging task for data management\napplications. Often in academic research studies the goals for informatics\nteams are to build applications that provide extract-transform-load (ETL)\nfunctionality to archive and catalog source data that has been collected by the\nresearch teams. In consortia that cross species and methodological or\nscientific domains, building interfaces that supply data in a usable fashion\nand make intuitive sense to scientists from dramatically different backgrounds\nincreases the complexity for developers. Further, reusing source data from\noutside one's scientific domain is fraught with ambiguities in understanding\nthe data types, analysis methodologies, and how to combine the data with those\nfrom other research teams. We report on the design, implementation, and\nperformance of a semantic data management application to support the NIMH\nfunded Conte Center at the University of California, Irvine. The Center is\ntesting a theory of the consequences of \"fragmented\" (unpredictable, high\nentropy) early-life experiences on adolescent cognitive and emotional outcomes\nin both humans and rodents. It employs cross-species neuroimaging, epigenomic,\nmolecular, and neuroanatomical approaches in humans and rodents to assess the\npotential consequences of fragmented unpredictable experience on brain\nstructure and circuitry. To address this multi-technology, multi-species\napproach, the system uses semantic web techniques based on the Neuroimaging\nData Model (NIDM) to facilitate data ETL functionality. We find this approach\nenables a low-cost, easy to maintain, and semantically meaningful information\nmanagement system, enabling the diverse research teams to access and use the\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 18:58:40 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Keator", "David B.", ""], ["Chen", "Jinran", ""], ["Nichols", "B Nolan", ""], ["Fana", "Fariba", ""], ["Stern", "Hal", ""], ["Baram", "Tallie Z.", ""], ["Small", "Steven L.", ""]]}, {"id": "1706.07936", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Michael Benedikt", "title": "When Can We Answer Queries Using Result-Bounded Data Interfaces?", "comments": "45 pages, 2 tables, 43 references. Complete version with proofs of\n  the PODS'18 paper. The main text of this paper is almost identical to the\n  PODS'18 except that we have fixed some small mistakes. Relative to the\n  earlier arXiv version, many errors were corrected, and some terminology has\n  changed", "journal-ref": null, "doi": "10.1145/3196959.3196965", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider answering queries where the underlying data is available only\nover limited interfaces which provide lookup access to the tuples matching a\ngiven binding, but possibly restricting the number of output tuples returned.\nInterfaces imposing such \"result bounds\" are common in accessing data via the\nweb. Given a query over a set of relations as well as some integrity\nconstraints that relate the queried relations to the data sources, we examine\nthe problem of deciding if the query is answerable over the interfaces; that\nis, whether there exists a plan that returns all answers to the query, assuming\nthe source data satisfies the integrity constraints.\n  The first component of our analysis of answerability is a reduction to a\nquery containment problem with constraints. The second component is a set of\n\"schema simplification\" theorems capturing limitations on how interfaces with\nresult bounds can be useful to obtain complete answers to queries. These\nresults also help to show decidability for the containment problem that\ncaptures answerability, for many classes of constraints. The final component in\nour analysis of answerability is a \"linearization\" method, showing that query\ncontainment with certain guarded dependencies -- including those that emerge\nfrom answerability problems -- can be reduced to query containment for a\nwell-behaved class of linear dependencies. Putting these components together,\nwe get a detailed picture of how to check answerability over result-bounded\nservices.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 10:41:20 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 15:44:43 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Benedikt", "Michael", ""]]}, {"id": "1706.08259", "submitter": "Remco Dijkman", "authors": "Remco Dijkman, Juntao Gao, Paul Grefen, Arthur ter Hofstede", "title": "Relational Algebra for In-Database Process Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The execution logs that are used for process mining in practice are often\nobtained by querying an operational database and storing the result in a flat\nfile. Consequently, the data processing power of the database system cannot be\nused anymore for this information, leading to constrained flexibility in the\ndefinition of mining patterns and limited execution performance in mining large\nlogs. Enabling process mining directly on a database - instead of via\nintermediate storage in a flat file - therefore provides additional flexibility\nand efficiency. To help facilitate this ideal of in-database process mining,\nthis paper formally defines a database operator that extracts the 'directly\nfollows' relation from an operational database. This operator can both be used\nto do in-database process mining and to flexibly evaluate process mining\nrelated queries, such as: \"which employee most frequently changes the 'amount'\nattribute of a case from one task to the next\". We define the operator using\nthe well-known relational algebra that forms the formal underpinning of\nrelational databases. We formally prove equivalence properties of the operator\nthat are useful for query optimization and present time-complexity properties\nof the operator. By doing so this paper formally defines the necessary\nrelational algebraic elements of a 'directly follows' operator, which are\nrequired for implementation of such an operator in a DBMS.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 07:31:36 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Dijkman", "Remco", ""], ["Gao", "Juntao", ""], ["Grefen", "Paul", ""], ["ter Hofstede", "Arthur", ""]]}, {"id": "1706.09479", "submitter": "Joseph Near", "authors": "Noah Johnson, Joseph P. Near, Dawn Song", "title": "Towards Practical Differential Privacy for SQL Queries", "comments": "Extended & updated from VLDB 2018 version", "journal-ref": null, "doi": "10.1145/3177732.3177733", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy promises to enable general data analytics while\nprotecting individual privacy, but existing differential privacy mechanisms do\nnot support the wide variety of features and databases used in real-world\nSQL-based analytics systems.\n  This paper presents the first practical approach for differential privacy of\nSQL queries. Using 8.1 million real-world queries, we conduct an empirical\nstudy to determine the requirements for practical differential privacy, and\ndiscuss limitations of previous approaches in light of these requirements. To\nmeet these requirements we propose elastic sensitivity, a novel method for\napproximating the local sensitivity of queries with general equijoins. We prove\nthat elastic sensitivity is an upper bound on local sensitivity and can\ntherefore be used to enforce differential privacy using any local\nsensitivity-based mechanism.\n  We build FLEX, a practical end-to-end system to enforce differential privacy\nfor SQL queries using elastic sensitivity. We demonstrate that FLEX is\ncompatible with any existing database, can enforce differential privacy for\nreal-world SQL queries, and incurs negligible (0.03%) performance overhead.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 20:44:28 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 20:40:10 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 20:07:43 GMT"}, {"version": "v4", "created": "Mon, 14 May 2018 19:27:46 GMT"}, {"version": "v5", "created": "Tue, 4 Sep 2018 15:59:30 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Johnson", "Noah", ""], ["Near", "Joseph P.", ""], ["Song", "Dawn", ""]]}]