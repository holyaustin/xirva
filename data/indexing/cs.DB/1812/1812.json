[{"id": "1812.00939", "submitter": "Yusuke Kawamoto", "authors": "Yusuke Kawamoto and Takao Murakami", "title": "Local Obfuscation Mechanisms for Hiding Probability Distributions", "comments": "Full version of Proc. ESORICS 2019 (with a longer appendix)", "journal-ref": null, "doi": "10.1007/978-3-030-29959-0_7", "report-no": null, "categories": "cs.CR cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a formal model for the information leakage of probability\ndistributions and define a notion called distribution privacy as the local\ndifferential privacy for probability distributions. Roughly, the distribution\nprivacy of a local obfuscation mechanism means that the attacker cannot\nsignificantly gain any information on the distribution of the mechanism's input\nby observing its output. Then we show that existing local mechanisms can hide\ninput distributions in terms of distribution privacy, while deteriorating the\nutility by adding too much noise. For example, we prove that the Laplace\nmechanism needs to add a large amount of noise proportionally to the infinite\nWasserstein distance between the two distributions we want to make\nindistinguishable. To improve the tradeoff between distribution privacy and\nutility, we introduce a local obfuscation mechanism, called a tupling\nmechanism, that adds random dummy data to the output. Then we apply this\nmechanism to the protection of user attributes in location based services. By\nexperiments, we demonstrate that the tupling mechanism outperforms popular\nlocal mechanisms in terms of attribute obfuscation and service quality.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 18:06:20 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 08:37:22 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 14:12:13 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 11:16:03 GMT"}, {"version": "v5", "created": "Tue, 6 Aug 2019 17:06:09 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Kawamoto", "Yusuke", ""], ["Murakami", "Takao", ""]]}, {"id": "1812.01663", "submitter": "Jinfei Liu", "authors": "Jinfei Liu, Juncheng Yang, Li Xiong, Jian Pei, Jun Luo, Yuzhang Guo,\n  Shuaicheng Ma, and Chenglin Fan", "title": "Skyline Diagram: Efficient Space Partitioning for Skyline Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skyline queries are important in many application domains. In this paper, we\npropose a novel structure Skyline Diagram, which given a set of points,\npartitions the plane into a set of regions, referred to as skyline polyominos.\nAll query points in the same skyline polyomino have the same skyline query\nresults. Similar to $k^{th}$-order Voronoi diagram commonly used to facilitate\n$k$ nearest neighbor ($k$NN) queries, skyline diagram can be used to facilitate\nskyline queries and many other applications. However, it may be computationally\nexpensive to build the skyline diagram. By exploiting some interesting\nproperties of skyline, we present several efficient algorithms for building the\ndiagram with respect to three kinds of skyline queries, quadrant, global, and\ndynamic skylines. In addition, we propose an approximate skyline diagram which\ncan significantly reduce the space cost. Experimental results on both real and\nsynthetic datasets show that our algorithms are efficient and scalable.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 20:15:39 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Liu", "Jinfei", ""], ["Yang", "Juncheng", ""], ["Xiong", "Li", ""], ["Pei", "Jian", ""], ["Luo", "Jun", ""], ["Guo", "Yuzhang", ""], ["Ma", "Shuaicheng", ""], ["Fan", "Chenglin", ""]]}, {"id": "1812.01741", "submitter": "Shantanu Sharma", "authors": "Sharad Mehrotra, Kerim Yasin Oktay, Shantanu Sharma", "title": "Exploiting Data Sensitivity on Partitioned Data", "comments": "This chapter will appear in the book titled: From Database to Cyber\n  Security: Essays Dedicated to Sushil Jajodia on the Occasion of His 70th\n  Birthday", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several researchers have proposed solutions for secure data outsourcing on\nthe public clouds based on encryption, secret-sharing, and trusted hardware.\nExisting approaches, however, exhibit many limitations including high\ncomputational complexity, imperfect security, and information leakage. This\nchapter describes an emerging trend in secure data processing that recognizes\nthat an entire dataset may not be sensitive, and hence, non-sensitivity of data\ncan be exploited to overcome some of the limitations of existing\nencryption-based approaches. In particular, data and computation can be\npartitioned into sensitive or non-sensitive datasets - sensitive data can\neither be encrypted prior to outsourcing or stored/processed locally on trusted\nservers. The non-sensitive dataset, on the other hand, can be outsourced and\nprocessed in the cleartext. While partitioned computing can bring new\nefficiencies since it does not incur (expensive) encrypted data processing\ncosts on non-sensitive data, it can lead to information leakage. We study\npartitioned computing in two contexts - first, in the context of the hybrid\ncloud where local resources are integrated with public cloud resources to form\nan effective and secure storage and computational platform for enterprise data.\nIn the hybrid cloud, sensitive data is stored on the private cloud to prevent\nleakage and a computation is partitioned between private and public clouds.\nCare must be taken that the public cloud cannot infer any information about\nsensitive data from inter-cloud data access during query processing. We then\nconsider partitioned computing in a public cloud only setting, where sensitive\ndata is encrypted before outsourcing. We formally define a partitioned security\ncriterion that any approach to partitioned computing on public clouds must\nensure in order to not introduce any new vulnerabilities to the existing secure\nsolution.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 23:00:04 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Mehrotra", "Sharad", ""], ["Oktay", "Kerim Yasin", ""], ["Sharma", "Shantanu", ""]]}, {"id": "1812.01790", "submitter": "Charith Perera", "authors": "Balkis Abidi, Sadok Ben Yahia, Charith Perera", "title": "Hybrid Microaggregation for Privacy-Preserving Data Mining", "comments": "16", "journal-ref": "Journal of Ambient Intelligence and Humanized Computing, 2018", "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-Anonymity by microaggregation is one of the most commonly used\nanonymization techniques. This success is owe to the achievement of a worth of\ninterest tradeoff between information loss and identity disclosure risk.\nHowever, this method may have some drawbacks. On the disclosure limitation\nside, there is a lack of protection against attribute disclosure. On the data\nutility side, dealing with a real datasets is a challenging task to achieve.\nIndeed, the latter are characterized by their large number of attributes and\nthe presence of noisy data, such that outliers or, even, data with missing\nvalues. Generating an anonymous individual data useful for data mining tasks,\nwhile decreasing the influence of noisy data is a compelling task to achieve.\nIn this paper, we introduce a new microaggregation method, called HM-PFSOM,\nbased on fuzzy possibilistic clustering. Our proposed method operates through\nan hybrid manner. This means that the anonymization process is applied per\nblock of similar data. Thus, we can help to decrease the information loss\nduring the anonymization process. The HMPFSOM approach proposes to study the\ndistribution of confidential attributes within each sub-dataset. Then,\naccording to the latter distribution, the privacy parameter k is determined, in\nsuch a way to preserve the diversity of confidential attributes within the\nanonymized microdata. This allows to decrease the disclosure risk of\nconfidential information.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 07:42:33 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Abidi", "Balkis", ""], ["Yahia", "Sadok Ben", ""], ["Perera", "Charith", ""]]}, {"id": "1812.01801", "submitter": "Ryota Yamanaka", "authors": "Shota Matsumoto, Ryota Yamanaka, Hirokazu Chiba", "title": "Mapping RDF Graphs to Property Graphs", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing amounts of scientific and social data are published in the\nResource Description Framework (RDF). Although the RDF data can be queried\nusing the SPARQL language, even the SPARQL-based operation has a limitation in\nimplementing traversal or analytical algorithms. Recently, a variety of graph\ndatabase implementations dedicated to analyses on the property graph model have\nemerged. However, the RDF model and the property graph model are not\ninteroperable. Here, we developed a framework based on the Graph to Graph\nMapping Language (G2GML) for mapping RDF graphs to property graphs to make the\nmost of accumulated RDF data. Using this framework, graph data described in the\nRDF model can be converted to the property graph model and can be loaded to\nseveral graph database engines for further analysis. Future works include\nimplementing and utilizing graph algorithms to make the most of the accumulated\ndata in various analytical engines.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 03:22:37 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Matsumoto", "Shota", ""], ["Yamanaka", "Ryota", ""], ["Chiba", "Hirokazu", ""]]}, {"id": "1812.01823", "submitter": "Guangyan Hu", "authors": "Guangyan Hu, Desheng Zhang, Sandro Rigo, Thu D. Nguyen", "title": "Approximation with Error Bounds in Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a sampling framework to support approximate computing with\nestimated error bounds in Spark. Our framework allows sampling to be performed\nat the beginning of a sequence of multiple transformations ending in an\naggregation operation. The framework constructs a data provenance tree as the\ncomputation proceeds, then combines the tree with multi-stage sampling and\npopulation estimation theories to compute error bounds for the aggregation.\nWhen information about output keys are available early, the framework can also\nuse adaptive stratified reservoir sampling to avoid (or reduce) key losses in\nthe final output and to achieve more consistent error bounds across popular and\nrare keys. Finally, the framework includes an algorithm to dynamically choose\nsampling rates to meet user specified constraints on the CDF of error bounds in\nthe outputs. We have implemented a prototype of our framework called\nApproxSpark, and used it to implement five approximate applications from\ndifferent domains. Evaluation results show that ApproxSpark can (a)\nsignificantly reduce execution time if users can tolerate small amounts of\nuncertainties and, in many cases, loss of rare keys, and (b) automatically find\nsampling rates to meet user specified constraints on error bounds. We also\nexplore and discuss extensively trade-offs between sampling rates, execution\ntime, accuracy and key loss.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 05:40:28 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 03:51:18 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 15:01:52 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Hu", "Guangyan", ""], ["Zhang", "Desheng", ""], ["Rigo", "Sandro", ""], ["Nguyen", "Thu D.", ""]]}, {"id": "1812.02386", "submitter": "Cheng Xu", "authors": "Cheng Xu and Ce Zhang and Jianliang Xu", "title": "vChain: Enabling Verifiable Boolean Range Queries over Blockchain\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains have recently been under the spotlight due to the boom of\ncryptocurrencies and decentralized applications. There is an increasing demand\nfor querying the data stored in a blockchain database. To ensure query\nintegrity, the user can maintain the entire blockchain database and query the\ndata locally. However, this approach is not economic, if not infeasible,\nbecause of the blockchain's huge data size and considerable maintenance costs.\nIn this paper, we take the first step toward investigating the problem of\nverifiable query processing over blockchain databases. We propose a novel\nframework, called vChain, that alleviates the storage and computing costs of\nthe user and employs verifiable queries to guarantee the results' integrity. To\nsupport verifiable Boolean range queries, we propose an accumulator-based\nauthenticated data structure that enables dynamic aggregation over arbitrary\nquery attributes. Two new indexes are further developed to aggregate\nintra-block and inter-block data records for efficient query verification. We\nalso propose an inverted prefix tree structure to accelerate the processing of\na large number of subscription queries simultaneously. Security analysis and\nempirical study validate the robustness and practicality of the proposed\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 07:33:37 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Xu", "Cheng", ""], ["Zhang", "Ce", ""], ["Xu", "Jianliang", ""]]}, {"id": "1812.03280", "submitter": "Chaoyue Niu", "authors": "Chaoyue Niu, Zhenzhe Zheng, Fan Wu, Xiaofeng Gao, and Guihai Chen", "title": "Achieving Data Truthfulness and Privacy Preservation in Data Markets", "comments": "The early version of this work appeared as a poster paper in IEEE\n  ICDE 2017, titled \"Trading Data in Good Faith: Integrating Truthfulness and\n  Privacy Preservation in Data Markets\". Later, the longer version was accepted\n  as a regular paper by the journal IEEE TKDE. The current manuscript in arXiv\n  is the full version. Please visit\n  https://github.com/NiuChaoyue/TKDE-2018-TPDM for source code", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering (TKDE), Volume\n  31, Issue 1, Pages 105--119, 2019", "doi": "10.1109/TKDE.2018.2822727", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a significant business paradigm, many online information platforms have\nemerged to satisfy society's needs for person-specific data, where a service\nprovider collects raw data from data contributors, and then offers value-added\ndata services to data consumers. However, in the data trading layer, the data\nconsumers face a pressing problem, i.e., how to verify whether the service\nprovider has truthfully collected and processed data? Furthermore, the data\ncontributors are usually unwilling to reveal their sensitive personal data and\nreal identities to the data consumers. In this paper, we propose TPDM, which\nefficiently integrates data Truthfulness and Privacy preservation in Data\nMarkets. TPDM is structured internally in an Encrypt-then-Sign fashion, using\npartially homomorphic encryption and identity-based signature. It\nsimultaneously facilitates batch verification, data processing, and outcome\nverification, while maintaining identity preservation and data confidentiality.\nWe also instantiate TPDM with a profile matching service and a distribution\nfitting service, and extensively evaluate their performances on Yahoo! Music\nratings dataset and 2009 RECS dataset, respectively. Our analysis and\nevaluation results reveal that TPDM achieves several desirable properties,\nwhile incurring low computation and communication overheads when supporting\nlarge-scale data markets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 08:05:46 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Niu", "Chaoyue", ""], ["Zheng", "Zhenzhe", ""], ["Wu", "Fan", ""], ["Gao", "Xiaofeng", ""], ["Chen", "Guihai", ""]]}, {"id": "1812.03651", "submitter": "Joseph Hellerstein", "authors": "Joseph M. Hellerstein, Jose Faleiro, Joseph E. Gonzalez, Johann\n  Schleier-Smith, Vikram Sreekanti, Alexey Tumanov and Chenggang Wu", "title": "Serverless Computing: One Step Forward, Two Steps Back", "comments": "8 pages, draft for CIDR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing offers the potential to program the cloud in an\nautoscaling, pay-as-you go manner. In this paper we address critical gaps in\nfirst-generation serverless computing, which place its autoscaling potential at\nodds with dominant trends in modern computing: notably data-centric and\ndistributed computing, but also open source and custom hardware. Put together,\nthese gaps make current serverless offerings a bad fit for cloud innovation and\nparticularly bad for data systems innovation. In addition to pinpointing some\nof the main shortfalls of current serverless architectures, we raise a set of\nchallenges we believe must be met to unlock the radical potential that the\ncloud---with its exabytes of storage and millions of cores---should offer to\ninnovative developers.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 07:10:57 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Hellerstein", "Joseph M.", ""], ["Faleiro", "Jose", ""], ["Gonzalez", "Joseph E.", ""], ["Schleier-Smith", "Johann", ""], ["Sreekanti", "Vikram", ""], ["Tumanov", "Alexey", ""], ["Wu", "Chenggang", ""]]}, {"id": "1812.03831", "submitter": "Nofar Carmeli", "authors": "Nofar Carmeli, Markus Kr\\\"oll", "title": "On the Enumeration Complexity of Unions of Conjunctive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the enumeration complexity of Unions of Conjunctive Queries(UCQs).\nWe aim to identify the UCQs that are tractable in the sense that the answer\ntuples can be enumerated with a linear preprocessing phase and a constant delay\nbetween every successive tuples. It has been established that, in the absence\nof self-joins and under conventional complexity assumptions, the CQs that admit\nsuch an evaluation are precisely the free-connex ones. A union of tractable CQs\nis always tractable. We generalize the notion of free-connexity from CQs to\nUCQs, thus showing that some unions containing intractable CQs are, in fact,\ntractable. Interestingly, some unions consisting of only intractable CQs are\ntractable too. We show how to use the techniques presented in this article also\nin settings where the database contains cardinality dependencies (including\nfunctional dependencies and key constraints) or when the UCQs contain\ndisequalities. The question of finding a full characterization of the\ntractability of UCQs remains open. Nevertheless, we prove that for several\nclasses of queries, free-connexity fully captures the tractable UCQs.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 14:39:35 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 18:20:24 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 12:29:44 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 15:47:39 GMT"}, {"version": "v5", "created": "Thu, 6 May 2021 13:08:20 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Carmeli", "Nofar", ""], ["Kr\u00f6ll", "Markus", ""]]}, {"id": "1812.03975", "submitter": "Zhiwei Fan", "authors": "Zhiwei Fan, Jianqiao Zhu, Zuyu Zhang, Aws Albarghouthi, Paraschos\n  Koutris, Jignesh Patel", "title": "Scaling-Up In-Memory Datalog Processing: Observations and Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive query processing has experienced a recent resurgence, as a result\nof its use in many modern application domains, including data integration,\ngraph analytics, security, program analysis, networking and decision making.\nDue to the large volumes of data being processed, several research efforts,\nacross multiple communities, have explored how to scale up recursive queries,\ntypically expressed in Datalog. Our experience with these tools indicated that\ntheir performance does not translate across domains (e.g., a tool design for\nlarge-scale graph analytics does not exhibit the same performance on\nprogram-analysis tasks, and vice versa). As a result, we designed and\nimplemented a general-purpose Datalog engine, called RecStep, on top of a\nparallel single-node relational system. In this paper, we outline the different\ntechniques we use in RecStep, and the contribution of each technique to overall\nperformance. We also present results from a detailed set of experiments\ncomparing RecStep with a number of other Datalog systems using both graph\nanalytics and program-analysis tasks, summarizing pros and cons of existing\ntechniques based on the analysis of our observations. We show that RecStep\ngenerally outperforms the state-of-the-art parallel Datalog engines on complex\nand large-scale Datalog program evaluation, by a 4-6X margin. An additional\ninsight from our work is that we show that it is possible to build a\nhigh-performance Datalog system on top of a relational engine, an idea that has\nbeen dismissed in past work in this area.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 18:52:23 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Fan", "Zhiwei", ""], ["Zhu", "Jianqiao", ""], ["Zhang", "Zuyu", ""], ["Albarghouthi", "Aws", ""], ["Koutris", "Paraschos", ""], ["Patel", "Jignesh", ""]]}, {"id": "1812.04298", "submitter": "Joachim Schopfel", "authors": "Otmane Azeroual, Gunter Saake, Mohammad Abuosba, Joachim Sch\\\"opfel\n  (GERIICO)", "title": "Text data mining and data quality management for research information\n  systems in the context of open data and open science", "comments": null, "journal-ref": "ICOA 2018 3e colloque international sur le libre acc{\\`e}s, Nov\n  2018, Rabat, Morocco. 2018, Actes du 3e colloque international sur le libre\n  acc{\\`e}s. Le libre acc{\\`e}s {\\`a} la science : fondements, enjeux et\n  dynamiques. https://icoa2018.sciencesconf.org/", "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the implementation and use of research information systems (RIS) in\nscientific institutions, text data mining and semantic technologies are a key\ntechnology for the meaningful use of large amounts of data. It is not the\ncollection of data that is difficult, but the further processing and\nintegration of the data in RIS. Data is usually not uniformly formatted and\nstructured, such as texts and tables that cannot be linked. These include\nvarious source systems with their different data formats such as project and\npublication databases, CERIF and RCD data model, etc. Internal and external\ndata sources continue to develop. On the one hand, they must be constantly\nsynchronized and the results of the data links checked. On the other hand, the\ntexts must be processed in natural language and certain information extracted.\nUsing text data mining, the quality of the metadata is analyzed and this\nidentifies the entities and general keywords. So that the user is supported in\nthe search for interesting research information. The information age makes it\neasier to store huge amounts of data and increase the number of documents on\nthe internet, in institutions' intranets, in newswires and blogs is\noverwhelming. Search engines should help to specifically open up these sources\nof information and make them usable for administrative and research purposes.\nAgainst this backdrop, the aim of this paper is to provide an overview of text\ndata mining techniques and the management of successful data quality for RIS in\nthe context of open data and open science in scientific institutions and\nlibraries, as well as to provide ideas for their application. In particular,\nsolutions for the RIS will be presented.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 09:39:55 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Azeroual", "Otmane", "", "GERIICO"], ["Saake", "Gunter", "", "GERIICO"], ["Abuosba", "Mohammad", "", "GERIICO"], ["Sch\u00f6pfel", "Joachim", "", "GERIICO"]]}, {"id": "1812.04329", "submitter": "Matthias Lanzinger", "authors": "Georg Gottlob, Matthias Lanzinger, Reinhard Pichler", "title": "Semantic Width of Conjunctive Queries and Constraint Satisfaction\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering Conjunctive Queries (CQs) and solving Constraint Satisfaction\nProblems (CSPs) are arguably among the most fundamental tasks in Computer\nScience. They are classical NP-complete problems. Consequently, the search for\ntractable fragments of these problems has received a lot of research interest\nover the decades. This research has traditionally progressed along three\northogonal threads. a) Reformulating queries into simpler, equivalent, queries\n(semantic optimization) b) Bounding answer sizes based on structural properties\nof the query c) Decomposing the query in such a way that global consistency\nfollows from local consistency. Much progress has been made by various works\nthat connect two of these threads. Bounded answer sizes and decompositions have\nbeen shown to be tightly connected through the important notions of fractional\nhypertree width and, more recently, submodular width. recent papers by\nBarcel\\'o et al. study decompositions up to generalized hypertree width under\nsemantic optimization. In this work, we connect all three of these threads by\nintroducing a general notion of semantic width and investigating semantic\nversions of fractional hypertree width, adaptive width, submodular width and\nthe fractional cover number.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 11:01:42 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 13:37:44 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Gottlob", "Georg", ""], ["Lanzinger", "Matthias", ""], ["Pichler", "Reinhard", ""]]}, {"id": "1812.04379", "submitter": "Floris Geerts", "authors": "Floris Geerts", "title": "On the expressive power of linear algebra on graphs", "comments": "51 pages, revised extended version of conference paper (International\n  Conference on Database Theory 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most graph query languages are rooted in logic. By contrast, in this paper we\nconsider graph query languages rooted in linear algebra. More specifically, we\nconsider MATLANG, a matrix query language recently introduced, in which some\nbasic linear algebra functionality is supported. We investigate the problem of\ncharacterising equivalence of graphs, represented by their adjacency matrices,\nfor various fragments of MATLANG. A complete picture is painted of the impact\nof the linear algebra operations in MATLANG on their ability to distinguish\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 13:13:38 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 14:54:35 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Geerts", "Floris", ""]]}, {"id": "1812.04386", "submitter": "Jasper Koehorst", "authors": "Jesse C.J. van Dam, Jasper J. Koehorst, Peter J. Schaap and Maria\n  Suarez-Diez", "title": "The Empusa code generator: bridging the gap between the intended and the\n  actual content of RDF resources", "comments": null, "journal-ref": null, "doi": "10.1038/s41597-019-0263-7", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The RDF data model facilitates integration of diverse data available in\nstructured and semi-structured formats. To obtain an RDF graph with a low\namount of errors and internal redundancy, the chosen ontology must be\nconsistently applied. However, with each addition of new diverse data the\nontology must evolve thereby increasing its complexity, which could lead to\naccumulation of unintended erroneous composites. Thus, there is a need for a\ngatekeeping system that compares the intended content described in the ontology\nwith the actual content of the resource.\n  Here we present Empusa, a tool that has been developed to facilitate the\ncreation of composite RDF resources from disparate sources. Empusa can be used\nto convert a schema into an associated application programming interface (API)\nthat can be used to perform data consistency checks and generates Markdown\ndocumentation to make persistent URLs resolvable. In this way, the use of\nEmpusa ensures consistency within and between the ontology (OWL), the Shape\nExpressions (ShEx) describing the graph structure, and the content of the\nresource.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 13:25:19 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["van Dam", "Jesse C. J.", ""], ["Koehorst", "Jasper J.", ""], ["Schaap", "Peter J.", ""], ["Suarez-Diez", "Maria", ""]]}, {"id": "1812.05095", "submitter": "Marek Sawerwain", "authors": "Marek Sawerwain and Marek Wr\\'oblewski", "title": "Recommendation systems with quantum k-NN and Grover's algorithms for\n  data processing", "comments": "17 pages, 5 figures", "journal-ref": "Int. J. Appl. Math. Comput. Sci., 2019, Vol. 29, No. 1, 139 - 150", "doi": "10.2478/amcs-2019-0011", "report-no": null, "categories": "quant-ph cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we discuss the implementation of a quantum recommendation\nsystem that uses a quantum variant of the k-nearest neighbours algorithm and\nthe Grover algorithm to search for a specific element in unstructured database.\nIn addition to the presentation of the recommendation system as an algorithm,\nthe article also shows a main steps in construction of a suitable quantum\ncircuit for realisation of a given recommendation system. The computational\ncomplexity of individual calculation steps during recommendation system was\nalso indicated. The verification correctness of a proposed recommendation\nsystem was also analysed, indicating an algebraic equation describing the\nprobability of success of the recommendation. The article also shows numerical\nexamples presenting the behaviour of the recommendation system for two selected\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 20:03:51 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Sawerwain", "Marek", ""], ["Wr\u00f3blewski", "Marek", ""]]}, {"id": "1812.05762", "submitter": "Doris Xin", "authors": "Doris Xin, Stephen Macke, Litian Ma, Jialin Liu, Shuchen Song, Aditya\n  Parameswaran", "title": "Helix: Holistic Optimization for Accelerating Iterative Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning workflow development is a process of trial-and-error:\ndevelopers iterate on workflows by testing out small modifications until the\ndesired accuracy is achieved. Unfortunately, existing machine learning systems\nfocus narrowly on model training---a small fraction of the overall development\ntime---and neglect to address iterative development. We propose Helix, a\nmachine learning system that optimizes the execution across\niterations---intelligently caching and reusing, or recomputing intermediates as\nappropriate. Helix captures a wide variety of application needs within its\nScala DSL, with succinct syntax defining unified processes for data\npreprocessing, model specification, and learning. We demonstrate that the reuse\nproblem can be cast as a Max-Flow problem, while the caching problem is\nNP-Hard. We develop effective lightweight heuristics for the latter. Empirical\nevaluation shows that Helix is not only able to handle a wide variety of use\ncases in one unified workflow but also much faster, providing run time\nreductions of up to 19x over state-of-the-art systems, such as DeepDive or\nKeystoneML, on four real-world applications in natural language processing,\ncomputer vision, social and natural sciences.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 02:32:45 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Xin", "Doris", ""], ["Macke", "Stephen", ""], ["Ma", "Litian", ""], ["Liu", "Jialin", ""], ["Song", "Shuchen", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1812.05804", "submitter": "Andrew Simmons", "authors": "Andrew J. Simmons, Scott Barnett, Simon Vajda and Rajesh Vasa", "title": "Data Provenance for Sport", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysts often discover irregularities in their underlying dataset,\nwhich need to be traced back to the original source and corrected. Standards\nfor representing data provenance (i.e. the origins of the data), such as the\nW3C PROV standard, can assist with this process, however require a mapping\nbetween abstract provenance concepts and the domain of use in order to apply\nthem effectively. We propose a custom notation for expressing provenance of\ninformation in the sport performance analysis domain, and map our notation to\nconcepts in the W3C PROV standard where possible. We evaluate the functionality\nof W3C PROV (without specialisations) and the VisTrails workflow manager\n(without extensions), and find that as is, neither are able to fully capture\nsport performance analysis workflows, notably due to limitations surrounding\ncapture of automated and manual activities respectively. Furthermore, their\nnotations suffer from ineffective use of visual design space, and present\npotential usability issues as their terminology is unlikely to match that of\nsport practitioners. Our findings suggest that one-size-fits-all provenance and\nworkflow systems are a poor fit in practice, and that their notation and\nfunctionality need to be optimised for the domain of use.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 07:41:25 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Simmons", "Andrew J.", ""], ["Barnett", "Scott", ""], ["Vajda", "Simon", ""], ["Vasa", "Rajesh", ""]]}, {"id": "1812.07024", "submitter": "Fatemeh Nargesian", "authors": "Fatemeh Nargesian, Ken Q. Pu, Bahar Ghadiri Bashardoost, Erkang Zhu,\n  Ren\\'ee J. Miller", "title": "Data Lake Organization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of creating a navigation structure that allows a user\nto most effectively navigate a data lake. We define an organization as a graph\nthat contains nodes representing sets of attributes within a data lake and\nedges indicating subset relationships among nodes. We present a new\nprobabilistic model of how users interact with an organization and define the\nlikelihood of a user finding a table using the organization. We propose the\ndata lake organization problem as the problem of finding an organization that\nmaximizes the expected probability of discovering tables by navigating an\norganization. We propose an approximate algorithm for the data lake\norganization problem. We show the effectiveness of the algorithm on both real\ndata lakes containing data from open data portals and on benchmarks that\nemulate the observed characteristics of real data lakes. Through a formal user\nstudy, we show that navigation can help users discover relevant tables that\ncannot be found by keyword search. In addition, in our study, 42% of users\npreferred the use of navigation and 58% preferred keyword search, suggesting\nthese are complementary and both useful modalities for data discovery in data\nlakes. Our experiments show that data lake organizations take into account the\ndata lake distribution and outperform an existing hand-curated taxonomy and a\ncommon baseline organization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 19:42:22 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 15:13:11 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 21:42:35 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Nargesian", "Fatemeh", ""], ["Pu", "Ken Q.", ""], ["Bashardoost", "Bahar Ghadiri", ""], ["Zhu", "Erkang", ""], ["Miller", "Ren\u00e9e J.", ""]]}, {"id": "1812.07208", "submitter": "Siddharth Dawar", "authors": "Siddharth Dawar, Debajyoti Bera, Vikram Goyal", "title": "High-utility itemset mining for subadditive monotone utility functions", "comments": "Pre-print of our paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-utility Itemset Mining (HUIM) finds itemsets from a transaction database\nwith utility no less than a user-defined threshold where the utility of an\nitemset is defined as the sum of the item-wise utilities. In this paper, we\ngeneralize this notion to utility functions that need not be a simple sum of\nindividual utilities. In particular, we study generalized utility functions\nthat are subadditive and monotone (SM). We also describe a novel function that\nallows us to include external information in the form of a relationship graph\nfor computing utility. Next, we focus on algorithms for HUIM problems with SM\nutility functions. We note that the existing HUIM algorithms use upper-bounds\nlike \"Transaction Weighted Utility\" and \"Exact-Utility, Remaining-Utility\" for\nefficient search-space exploration. We derive analogous and tighter\nupper-bounds for SM utility functions. We design a novel inverted-list data\nstructure called SMI-list and a new algorithm called SM-Miner to mine HUIs for\nSM functions. We explain how existing tree-based and projection-based HUIM\nalgorithms can be adapted using these bounds. We experimentally compare\nadaptations of some of the latest HUIM algorithms and point out some caveats\nthat should be kept in mind while handling utility functions that allow\nintegration of domain knowledge with a transaction database.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:26:15 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 06:05:54 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Dawar", "Siddharth", ""], ["Bera", "Debajyoti", ""], ["Goyal", "Vikram", ""]]}, {"id": "1812.07527", "submitter": "Chen Luo", "authors": "Chen Luo, Michael J. Carey", "title": "LSM-based Storage Techniques: A Survey", "comments": "This is a pre-print of an article published in VLDB Journal. The\n  final authenticated version is available online at:\n  https://doi.org/10.1007/s00778-019-00555-y", "journal-ref": "VLDB Journal, 2019", "doi": "10.1007/s00778-019-00555-y", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Log-Structured Merge-tree (LSM-tree) has been widely adopted\nfor use in the storage layer of modern NoSQL systems. Because of this, there\nhave been a large number of research efforts, from both the database community\nand the operating systems community, that try to improve various aspects of\nLSM-trees. In this paper, we provide a survey of recent research efforts on\nLSM-trees so that readers can learn the state-of-the-art in LSM-based storage\ntechniques. We provide a general taxonomy to classify the literature of\nLSM-trees, survey the efforts in detail, and discuss their strengths and\ntrade-offs. We further survey several representative LSM-based open-source\nNoSQL systems and discuss some potential future research directions resulting\nfrom the survey.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 17:48:00 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 00:15:12 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 16:16:49 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Luo", "Chen", ""], ["Carey", "Michael J.", ""]]}, {"id": "1812.07607", "submitter": "Sanjay Krishnan", "authors": "Sanjay Krishnan, Adam Dziedzic, Aaron J. Elmore", "title": "DeepLens: Towards a Visual Data Management System", "comments": "In Proceeds of CIDR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in deep learning have greatly widened the scope of automatic\ncomputer vision algorithms and enable users to ask questions directly about the\ncontent in images and video. This paper explores the necessary steps towards a\nfuture Visual Data Management System (VDMS), where the predictions of such deep\nlearning models are stored, managed, queried, and indexed. We propose a query\nand data model that disentangles the neural network models used, the query\nworkload, and the data source semantics from the query processing layer. Our\nsystem, DeepLens, is based on dataflow query processing systems and this\nresearch prototype presents initial experiments to elicit important open\nresearch questions in visual analytics systems. One of our main conclusions is\nthat any future \"declarative\" VDMS will have to revisit query optimization and\nautomated physical design from a unified perspective of performance and\naccuracy tradeoffs. Physical design and query optimization choices can not only\nchange performance by orders of magnitude, they can potentially affect the\naccuracy of results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 19:25:26 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Krishnan", "Sanjay", ""], ["Dziedzic", "Adam", ""], ["Elmore", "Aaron J.", ""]]}, {"id": "1812.07658", "submitter": "Zhongjun Jin", "authors": "Zhongjun Jin, Christopher Baik, Michael Cafarella, H. V. Jagadish,\n  Yuze Lou", "title": "Demonstration of a Multiresolution Schema Mapping System", "comments": "4 pages, 5 figures, CIDR 2019", "journal-ref": "9th Biennial Conference on Innovative Data Systems Research (CIDR\n  2019)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Enterprise databases usually contain large and complex schemas. Authoring\ncomplete schema mapping queries in this case requires deep knowledge about the\nsource and target schemas and is thereby very challenging to programmers.\nSample-driven schema mapping allows the user to describe the schema mapping\nusing data records. However, real data records are still harder to specify than\nother useful insights about the desired schema mapping the user might have. In\nthis project, we develop a schema mapping system, PRISM, that enables\nmultiresolution schema mapping. The end user is not limited to providing\nhigh-resolution constraints like exact data records but may also provide\nconstraints of various resolutions, like incomplete data records, value ranges,\nand data types. This new interaction paradigm gives the user more flexibility\nin describing the desired schema mapping. This demonstration showcases how to\nuse PRISM for schema mapping in a real database.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 21:58:19 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Jin", "Zhongjun", ""], ["Baik", "Christopher", ""], ["Cafarella", "Michael", ""], ["Jagadish", "H. V.", ""], ["Lou", "Yuze", ""]]}, {"id": "1812.07695", "submitter": "Yuliang Li", "authors": "Yuliang Li, Jianguo Wang, Benjamin Pullman, Nuno Bandeira, and Yannis\n  Papakonstantinou", "title": "Index-based, High-dimensional, Cosine Threshold Querying with Optimality\n  Guarantees", "comments": "full version of an ICDT 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a database of vectors, a cosine threshold query returns all vectors in\nthe database having cosine similarity to a query vector above a given threshold\n{\\theta}. These queries arise naturally in many applications, such as document\nretrieval, image search, and mass spectrometry. The present paper considers the\nefficient evaluation of such queries, providing novel optimality guarantees and\nexhibiting good performance on real datasets. We take as a starting point\nFagin's well-known Threshold Algorithm (TA), which can be used to answer cosine\nthreshold queries as follows: an inverted index is first built from the\ndatabase vectors during pre-processing; at query time, the algorithm traverses\nthe index partially to gather a set of candidate vectors to be later verified\nfor {\\theta}-similarity. However, directly applying TA in its raw form misses\nsignificant optimization opportunities. Indeed, we first show that one can take\nadvantage of the fact that the vectors can be assumed to be normalized, to\nobtain an improved, tight stopping condition for index traversal and to\nefficiently compute it incrementally. Then we show that one can take advantage\nof data skewness to obtain better traversal strategies. In particular, we show\na novel traversal strategy that exploits a common data skewness condition which\nholds in multiple domains including mass spectrometry, documents, and image\ndatabases. We show that under the skewness assumption, the new traversal\nstrategy has a strong, near-optimal performance guarantee. The techniques\ndeveloped in the paper are quite general since they can be applied to a large\nclass of similarity functions beyond cosine.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:30:39 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 19:47:13 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Li", "Yuliang", ""], ["Wang", "Jianguo", ""], ["Pullman", "Benjamin", ""], ["Bandeira", "Nuno", ""], ["Papakonstantinou", "Yannis", ""]]}, {"id": "1812.07854", "submitter": "Panos Vassiliadis", "authors": "Panos Vassiliadis and Patrick Marcel and Stefano Rizzi", "title": "Beyond Roll-Up's and Drill-Down's: An Intentional Analytics Model to\n  Reinvent OLAP (long-version)", "comments": "Long v. of a paper submitted to Information Systems. Includes the\n  formal specification of the Intentional Analytics Model at the Appendix", "journal-ref": "Information Systems, volume 85, November 2019. pp. 68-91, ISSN\n  0306-4379", "doi": "10.1016/j.is.2019.03.011", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper structures a novel vision for OLAP by fundamentally redefining\nseveral of the pillars on which OLAP has been based for the last 20 years. We\nredefine OLAP queries, in order to move to higher degrees of abstraction from\nroll-up's and drill-down's, and we propose a set of novel intentional OLAP\noperators, namely, describe, assess, explain, predict, and suggest, which\nexpress the user's need for results. We fundamentally redefine what a query\nanswer is, and escape from the constraint that the answer is a set of tuples;\non the contrary, we complement the set of tuples with models (typically, but\nnot exclusively, results of data mining algorithms over the involved data) that\nconcisely represent the internal structure or correlations of the data. Due to\nthe diverse nature of the involved models, we come up (for the first time ever,\nto the best of our knowledge) with a unifying framework for them, that places\nits pillars on the extension of each data cell of a cube with information about\nthe models that pertain to it -- practically converting the small parts that\nbuild up the models to data that annotate each cell. We exploit this\ndata-to-model mapping to provide highlights of the data, by isolating data and\nmodels that maximize the delivery of new information to the user. We introduce\na novel method for assessing the surprise that a new query result brings to the\nuser, with respect to the information contained in previous results the user\nhas seen via a new interestingness measure. The individual parts of our\nproposal are integrated in a new data model for OLAP, which we call the\nIntentional Analytics Model. We complement our contribution with a list of\nsignificant open problems for the community to address.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 10:06:26 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 19:04:36 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Vassiliadis", "Panos", ""], ["Marcel", "Patrick", ""], ["Rizzi", "Stefano", ""]]}, {"id": "1812.08032", "submitter": "Cagatay Turkay", "authors": "Cagatay Turkay, Nicola Pezzotti, Carsten Binnig, Hendrik Strobelt,\n  Barbara Hammer, Daniel A. Keim, Jean-Daniel Fekete, Themis Palpanas, Yunhai\n  Wang, Florin Rusu", "title": "Progressive Data Science: Potential and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science requires time-consuming iterative manual activities. In\nparticular, activities such as data selection, preprocessing, transformation,\nand mining, highly depend on iterative trial-and-error processes that could be\nsped-up significantly by providing quick feedback on the impact of changes. The\nidea of progressive data science is to compute the results of changes in a\nprogressive manner, returning a first approximation of results quickly and\nallow iterative refinements until converging to a final result. Enabling the\nuser to interact with the intermediate results allows an early detection of\nerroneous or suboptimal choices, the guided definition of modifications to the\npipeline and their quick assessment. In this paper, we discuss the\nprogressiveness challenges arising in different steps of the data science\npipeline. We describe how changes in each step of the pipeline impact the\nsubsequent steps and outline why progressive data science will help to make the\nprocess more effective. Computing progressive approximations of outcomes\nresulting from changes creates numerous research challenges, especially if the\nchanges are made in the early steps of the pipeline. We discuss these\nchallenges and outline first steps towards progressiveness, which, we argue,\nwill ultimately help to significantly speed-up the overall data science\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 15:45:03 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 17:02:46 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Turkay", "Cagatay", ""], ["Pezzotti", "Nicola", ""], ["Binnig", "Carsten", ""], ["Strobelt", "Hendrik", ""], ["Hammer", "Barbara", ""], ["Keim", "Daniel A.", ""], ["Fekete", "Jean-Daniel", ""], ["Palpanas", "Themis", ""], ["Wang", "Yunhai", ""], ["Rusu", "Florin", ""]]}, {"id": "1812.09141", "submitter": "Anastasios Gounaris", "authors": "Christos Bellas and Anastasios Gounaris", "title": "Speeding-up the Verification Phase of Set Similarity Joins in the GPGPU\n  paradigm", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of exact set similarity joins using a co-process\nCPU-GPU scheme. The state-of-the-art CPU solutions split the wok in two main\nphases. First, filtering and index building takes place to reduce the candidate\nsets to be compared as much as possible; then the pairs are compared to verify\nwhether they should become part of the result. We investigate in-depth\nsolutions for transferring the second, so-called verification phase, to the GPU\naddressing several challenges regarding the data serialization and layout, the\nthread management and the techniques to compare sets of tokens. Using real\ndatasets, we provide concrete experimental proofs that our solutions have\nreached their maximum potential, since they totally overlap verification with\nCPU tasks, and manage to yield significant speed-ups, up to 2.6X in our cases.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:24:56 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Bellas", "Christos", ""], ["Gounaris", "Anastasios", ""]]}, {"id": "1812.09162", "submitter": "Nicolas Le Scouarnec", "authors": "Fabien Andr\\'e, Anne-Marie Kermarrec and Nicolas Le Scouarnec", "title": "Quicker ADC : Unlocking the hidden potential of Product Quantization\n  with SIMD", "comments": "Open-source implementation at\n  http://github.com/nlescoua/faiss-quickeradc", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019 Early Access", "doi": "10.1109/TPAMI.2019.2952606", "report-no": null, "categories": "cs.CV cs.AI cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. A common approach is to rely\non Product Quantization, which allows the storage of large vector databases in\nmemory and efficient distance computations. Yet, implementations of nearest\nneighbor search with Product Quantization have their performance limited by the\nmany memory accesses they perform. Following this observation, Andr\\'e et al.\nproposed Quick ADC with up to $6\\times$ faster implementations of $m\\times{}4$\nproduct quantizers (PQ) leveraging specific SIMD instructions. Quicker ADC is a\ngeneralization of Quick ADC not limited to $m\\times{}4$ codes and supporting\nAVX-512, the latest revision of SIMD instruction set. In doing so, Quicker ADC\nfaces the challenge of using efficiently 5,6 and 7-bit shuffles that do not\nalign to computer bytes or words. To this end, we introduce (i) irregular\nproduct quantizers combining sub-quantizers of different granularity and (ii)\nsplit tables allowing lookup tables larger than registers. We evaluate Quicker\nADC with multiple indexes including Inverted Multi-Indexes and IVF HNSW and\nshow that it outperforms the reference optimized implementations (i.e., FAISS\nand polysemous codes) for numerous configurations. Finally, we release an\nopen-source fork of FAISS enhanced with Quicker ADC at\nhttp://github.com/nlescoua/faiss-quickeradc.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:51:27 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 21:39:48 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Andr\u00e9", "Fabien", ""], ["Kermarrec", "Anne-Marie", ""], ["Scouarnec", "Nicolas Le", ""]]}, {"id": "1812.09233", "submitter": "Shantanu Sharma", "authors": "Sharad Mehrotra, Shantanu Sharma, Jeffrey D. Ullman, and Anurag Mishra", "title": "Partitioned Data Security on Outsourced Sensitive and Non-sensitive Data", "comments": "Accepted in IEEE International Conference on Data Engineering (ICDE),\n  2019. arXiv admin note: text overlap with arXiv:1812.01741", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite extensive research on cryptography, secure and efficient query\nprocessing over outsourced data remains an open challenge. This paper continues\nalong the emerging trend in secure data processing that recognizes that the\nentire dataset may not be sensitive, and hence, non-sensitivity of data can be\nexploited to overcome limitations of existing encryption-based approaches. We\npropose a new secure approach, entitled query binning (QB) that allows\nnon-sensitive parts of the data to be outsourced in clear-text while\nguaranteeing that no information is leaked by the joint processing of\nnon-sensitive data (in clear-text) and sensitive data (in encrypted form). QB\nmaps a query to a set of queries over the sensitive and non-sensitive data in a\nway that no leakage will occur due to the joint processing over sensitive and\nnon-sensitive data. Interestingly, in addition to improve performance, we show\nthat QB actually strengthens the security of the underlying cryptographic\ntechnique by preventing size, frequency-count, and workload-skew attacks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 03:06:17 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Mehrotra", "Sharad", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""], ["Mishra", "Anurag", ""]]}, {"id": "1812.09519", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Pierre Bourhis, Stefan Mengel, Matthias Niewerth", "title": "Enumeration on Trees with Tractable Combined Complexity and Efficient\n  Updates", "comments": "16 pages of main material, 37 references, 11 pages of appendix. This\n  is the extended version with proofs of the PODS'19 paper. Except for minor\n  rephrasings and formatting differences, the contents are exactly the same as\n  the version published in the PODS'19 proceedings", "journal-ref": null, "doi": "10.1145/3294052.3319702", "report-no": null, "categories": "cs.DB cs.DS cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm to enumerate the results on trees of monadic\nsecond-order (MSO) queries represented by nondeterministic tree automata. After\nlinear time preprocessing (in the input tree), we can enumerate answers with\nlinear delay (in each answer). We allow updates on the tree to take place at\nany time, and we can then restart the enumeration after logarithmic time in the\ntree. Further, all our combined complexities are polynomial in the automaton.\n  Our result follows our previous circuit-based enumeration algorithms based on\ndeterministic tree automata, and is also inspired by our earlier result on\nwords and nondeterministic sequential extended variable-set automata in the\ncontext of document spanners. We extend these results and combine them with a\nrecent tree balancing scheme by Niewerth, so that our enumeration structure\nsupports updates to the underlying tree in logarithmic time (with leaf\ninsertions, leaf deletions, and node relabelings). Our result implies that, for\nMSO queries with free first-order variables, we can enumerate the results with\nlinear preprocessing and constant-delay and update the underlying tree in\nlogarithmic time, which improves on several known results for words and trees.\n  Building on lower bounds from data structure research, we also show\nunconditionally that up to a doubly logarithmic factor the update time of our\nalgorithm is optimal. Thus, unlike other settings, there can be no algorithm\nwith constant update time.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 12:16:35 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 16:14:02 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Mengel", "Stefan", ""], ["Niewerth", "Matthias", ""]]}, {"id": "1812.09526", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Ryan R. Curtin, Benjamin Moseley, Hung Q. Ngo,\n  XuanLong Nguyen, Dan Olteanu, Maximilian Schleich", "title": "Functional Aggregate Queries with Additive Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by fundamental applications in databases and relational machine\nlearning, we formulate and study the problem of answering functional aggregate\nqueries (FAQ) in which some of the input factors are defined by a collection of\nadditive inequalities between variables. We refer to these queries as FAQ-AI\nfor short.\n  To answer FAQ-AI in the Boolean semiring, we define relaxed tree\ndecompositions and relaxed submodular and fractional hypertree width\nparameters. We show that an extension of the InsideOut algorithm using\nChazelle's geometric data structure for solving the semigroup range search\nproblem can answer Boolean FAQ-AI in time given by these new width parameters.\nThis new algorithm achieves lower complexity than known solutions for FAQ-AI.\nIt also recovers some known results in database query answering.\n  Our second contribution is a relaxation of the set of polymatroids that gives\nrise to the counting version of the submodular width, denoted by #subw. This\nnew width is sandwiched between the submodular and the fractional hypertree\nwidths. Any FAQ and FAQ-AI over one semiring can be answered in time\nproportional to #subw and respectively to the relaxed version of #subw.\n  We present three applications of our FAQ-AI framework to relational machine\nlearning: k-means clustering, training linear support vector machines, and\ntraining models using non-polynomial loss. These optimization problems can be\nsolved over a database asymptotically faster than computing the join of the\ndatabase relations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 13:05:18 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 10:59:32 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 23:08:57 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2020 05:20:46 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Curtin", "Ryan R.", ""], ["Moseley", "Benjamin", ""], ["Ngo", "Hung Q.", ""], ["Nguyen", "XuanLong", ""], ["Olteanu", "Dan", ""], ["Schleich", "Maximilian", ""]]}, {"id": "1812.09551", "submitter": "Chao Zhang", "authors": "Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian\n  Sadler, Michelle Vanni, Jiawei Han", "title": "TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term\n  Embedding and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taxonomy construction is not only a fundamental task for semantic analysis of\ntext corpora, but also an important step for applications such as information\nfiltering, recommendation, and Web search. Existing pattern-based methods\nextract hypernym-hyponym term pairs and then organize these pairs into a\ntaxonomy. However, by considering each term as an independent concept node,\nthey overlook the topical proximity and the semantic correlations among terms.\nIn this paper, we propose a method for constructing topic taxonomies, wherein\nevery node represents a conceptual topic and is defined as a cluster of\nsemantically coherent concept terms. Our method, TaxoGen, uses term embeddings\nand hierarchical clustering to construct a topic taxonomy in a recursive\nfashion. To ensure the quality of the recursive process, it consists of: (1) an\nadaptive spherical clustering module for allocating terms to proper levels when\nsplitting a coarse topic into fine-grained ones; (2) a local embedding module\nfor learning term embeddings that maintain strong discriminative power at\ndifferent levels of the taxonomy. Our experiments on two real datasets\ndemonstrate the effectiveness of TaxoGen compared with baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 16:11:17 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zhang", "Chao", ""], ["Tao", "Fangbo", ""], ["Chen", "Xiusi", ""], ["Shen", "Jiaming", ""], ["Jiang", "Meng", ""], ["Sadler", "Brian", ""], ["Vanni", "Michelle", ""], ["Han", "Jiawei", ""]]}, {"id": "1812.09905", "submitter": "Xuli Liu", "authors": "Xuli Liu and Jihao Jin and Qi Wang and Tong Ruan and Yangming Zhou and\n  Daqi Gao and Yichao Yin", "title": "PatientEG Dataset: Bringing Event Graph Model with Temporal Relations to\n  Electronic Medical Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical activities, such as diagnoses, medicine treatments, and laboratory\ntests, as well as temporal relations between these activities are the basic\nconcepts in clinical research. However, existing relational data model on\nelectronic medical records (EMRs) lacks explicit and accurate semantic\ndefinitions of these concepts. It leads to the inconvenience of query\nconstruction and the inefficiency of query execution where multi-table join\nqueries are frequently required. In this paper, we propose a patient event\ngraph (PatientEG) model to capture the characteristics of EMRs. We respectively\ndefine five types of medical entities, five types of medical events and five\ntypes of temporal relations. Based on the proposed model, we also construct a\nPatientEG dataset with 191,294 events, 3,429 distinct entities, and 545,993\ntemporal relations using EMRs from Shanghai Shuguang hospital. To help to\nnormalize entity values which contain synonyms, hyponymies, and abbreviations,\nwe link them with the Chinese biomedical knowledge graph. With the help of\nPatientEG dataset, we are able to conveniently perform complex queries for\nclinical research such as auxiliary diagnosis and therapeutic effectiveness\nanalysis. In addition, we provide a SPARQL endpoint to access PatientEG dataset\nand the dataset is also publicly available online. Also, we list several\nillustrative SPARQL queries on our website.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 12:05:42 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Liu", "Xuli", ""], ["Jin", "Jihao", ""], ["Wang", "Qi", ""], ["Ruan", "Tong", ""], ["Zhou", "Yangming", ""], ["Gao", "Daqi", ""], ["Yin", "Yichao", ""]]}, {"id": "1812.09987", "submitter": "Batya Kenig", "authors": "Batya Kenig and Dan Suciu", "title": "Integrity Constraints Revisited: From Exact to Approximate Implication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrity constraints such as functional dependencies (FD), and multi-valued\ndependencies (MVD) are fundamental in database schema design. Likewise,\nprobabilistic conditional independences (CI) are crucial for reasoning about\nmultivariate probability distributions. The implication problem studies whether\na set of constraints (antecedents) implies another constraint (consequent), and\nhas been investigated in both the database and the AI literature, under the\nassumption that all constraints hold exactly. However, many applications today\nconsider constraints that hold only approximately. In this paper we define an\napproximate implication as a linear inequality between the degree of\nsatisfaction of the antecedents and consequent, and we study the relaxation\nproblem: when does an exact implication relax to an approximate implication? We\nuse information theory to define the degree of satisfaction, and prove several\nresults. First, we show that any implication from a set of data dependencies\n(MVDs+FDs) can be relaxed to a simple linear inequality with a factor at most\nquadratic in the number of variables; when the consequent is an FD, the factor\ncan be reduced to 1. Second, we prove that there exists an implication between\nCIs that does not admit any relaxation; however, we prove that every\nimplication between CIs relaxes \"in the limit\". Finally, we show that the\nimplication problem for differential constraints in market basket analysis also\nadmits a relaxation with a factor equal to 1. Our results recover, and\nsometimes extend, several previously known results about the implication\nproblem: implication of MVDs can be checked by considering only 2-tuple\nrelations, and the implication of differential constraints for frequent item\nsets can be checked by considering only databases containing a single\ntransaction.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 21:43:04 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 20:21:11 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 22:59:43 GMT"}, {"version": "v4", "created": "Sun, 22 Nov 2020 16:42:52 GMT"}, {"version": "v5", "created": "Wed, 25 Nov 2020 07:49:43 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Kenig", "Batya", ""], ["Suciu", "Dan", ""]]}, {"id": "1812.10321", "submitter": "Hiroki Kanezashi", "authors": "Hiroki Kanezashi, Toyotaro Suzumura, Dario Garcia-Gasulla, Min-hwan Oh\n  and Satoshi Matsuoka", "title": "Adaptive Pattern Matching with Reinforcement Learning for Dynamic Graphs", "comments": "10 pages and 11 figures", "journal-ref": null, "doi": "10.1109/HiPC.2018.00019", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pattern matching algorithms to handle million-scale dynamic graphs are\nwidely used in many applications such as social network analytics and\nsuspicious transaction detections from financial networks. On the other hand,\nthe computation complexity of many graph pattern matching algorithms is\nexpensive, and it is not affordable to extract patterns from million-scale\ngraphs. Moreover, most real-world networks are time-evolving, updating their\nstructures continuously, which makes it harder to update and output newly\nmatched patterns in real time. Many incremental graph pattern matching\nalgorithms which reduce the number of updates have been proposed to handle such\ndynamic graphs. However, it is still challenging to recompute vertices in the\nincremental graph pattern matching algorithms in a single process, and that\nprevents the real-time analysis. We propose an incremental graph pattern\nmatching algorithm to deal with time-evolving graph data and also propose an\nadaptive optimization system based on reinforcement learning to recompute\nvertices in the incremental process more efficiently. Then we discuss the\nqualitative efficiency of our system with several types of data graphs and\npattern graphs. We evaluate the performance using million-scale attributed and\ntime-evolving social graphs. Our incremental algorithm is up to 10.1 times\nfaster than an existing graph pattern matching and 1.95 times faster with the\nadaptive systems in a computation node than naive incremental processing.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 05:34:26 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kanezashi", "Hiroki", ""], ["Suzumura", "Toyotaro", ""], ["Garcia-Gasulla", "Dario", ""], ["Oh", "Min-hwan", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1812.10564", "submitter": "Yongjoo Park", "authors": "Yongjoo Park, Jingyi Qing, Xiaoyang Shen, Barzan Mozafari", "title": "BlinkML: Efficient Maximum Likelihood Estimation with Probabilistic\n  Guarantees", "comments": "22 pages, SIGMOD 2019", "journal-ref": null, "doi": "10.1145/3299869.3300077", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising volume of datasets has made training machine learning (ML) models\na major computational cost in the enterprise. Given the iterative nature of\nmodel and parameter tuning, many analysts use a small sample of their entire\ndata during their initial stage of analysis to make quick decisions (e.g., what\nfeatures or hyperparameters to use) and use the entire dataset only in later\nstages (i.e., when they have converged to a specific model). This sampling,\nhowever, is performed in an ad-hoc fashion. Most practitioners cannot precisely\ncapture the effect of sampling on the quality of their model, and eventually on\ntheir decision-making process during the tuning phase. Moreover, without\nsystematic support for sampling operators, many optimizations and reuse\nopportunities are lost.\n  In this paper, we introduce BlinkML, a system for fast, quality-guaranteed ML\ntraining. BlinkML allows users to make error-computation tradeoffs: instead of\ntraining a model on their full data (i.e., full model), BlinkML can quickly\ntrain an approximate model with quality guarantees using a sample. The quality\nguarantees ensure that, with high probability, the approximate model makes the\nsame predictions as the full model. BlinkML currently supports any ML model\nthat relies on maximum likelihood estimation (MLE), which includes Generalized\nLinear Models (e.g., linear regression, logistic regression, max entropy\nclassifier, Poisson regression) as well as PPCA (Probabilistic Principal\nComponent Analysis). Our experiments show that BlinkML can speed up the\ntraining of large-scale ML tasks by 6.26x-629x while guaranteeing the same\npredictions, with 95% probability, as the full model.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 22:35:21 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Park", "Yongjoo", ""], ["Qing", "Jingyi", ""], ["Shen", "Xiaoyang", ""], ["Mozafari", "Barzan", ""]]}, {"id": "1812.10568", "submitter": "Shucheng Zhong", "authors": "Yongjoo Park, Shucheng Zhong, Barzan Mozafari", "title": "QuickSel: Quick Selectivity Learning with Mixture Models", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3389727", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the selectivity of a query is a key step in almost any cost-based\nquery optimizer. Most of today's databases rely on histograms or samples that\nare periodically refreshed by re-scanning the data as the underlying data\nchanges. Since frequent scans are costly, these statistics are often stale and\nlead to poor selectivity estimates. As an alternative to scans, query-driven\nhistograms have been proposed, which refine the histograms based on the actual\nselectivities of the observed queries. Unfortunately, these approaches are\neither too costly to use in practice---i.e., require an exponential number of\nbuckets---or quickly lose their advantage as they observe more queries.\n  In this paper, we propose a selectivity learning framework, called QuickSel,\nwhich falls into the query-driven paradigm but does not use histograms.\nInstead, it builds an internal model of the underlying data, which can be\nrefined significantly faster (e.g., only 1.9 milliseconds for 300 queries).\nThis fast refinement allows QuickSel to continuously learn from each query and\nyield increasingly more accurate selectivity estimates over time. Unlike\nquery-driven histograms, QuickSel relies on a mixture model and a new\noptimization algorithm for training its model. Our extensive experiments on two\nreal-world datasets confirm that, given the same target accuracy, QuickSel is\n34.0x-179.4x faster than state-of-the-art query-driven histograms, including\nISOMER and STHoles. Further, given the same space budget, QuickSel is\n26.8%-91.8% more accurate than periodically-updated histograms and samples,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 23:05:32 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 20:30:39 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Park", "Yongjoo", ""], ["Zhong", "Shucheng", ""], ["Mozafari", "Barzan", ""]]}, {"id": "1812.10734", "submitter": "Yannis Tzitzikas", "authors": "Anna Kokolaki and Yannis Tzitzikas", "title": "Facetize: An Interactive Tool for Cleaning and Transforming Datasets for\n  Facilitating Exploratory Search", "comments": "10 pages, 4 figures, 1 table (systems x functionalities matrix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a plethora of datasets in various formats which are usually stored\nin files, hosted in catalogs, or accessed through SPARQL endpoints. In most\ncases, these datasets cannot be straightforwardly explored by end users, for\nsatisfying recall-oriented information needs. To fill this gap, in this paper\nwe present the design and implementation of Facetize, an editor that allows\nusers to transform (in an interactive manner) datasets, either static (i.e.\nstored in files), or dynamic (i.e. being the results of SPARQL queries), to\ndatasets that can be directly explored effectively by themselves or other\nusers. The latter (exploration) is achieved through the familiar interaction\nparadigm of Faceted Search (and Preference-enriched Faceted Search).\nSpecifically in this paper we describe the requirements, we introduce the\nrequired set of transformations, and then we detail the functionality and the\nimplementation of the editor Facetize that realizes these transformations. The\nsupported operations cover a wide range of tasks (selection, visibility,\ndeletions, edits, definition of hierarchies, intervals, derived attributes, and\nothers) and Facetize enables the user to carry them out in a user-friendly and\nguided manner, without presupposing any technical background (regarding data\nrepresentation or query languages). Finally we present the results of an\nevaluation with users. To the best of your knowledge, this is the first editor\nfor this kind of tasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 14:46:08 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Kokolaki", "Anna", ""], ["Tzitzikas", "Yannis", ""]]}, {"id": "1812.10926", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Philippe Fournier-Viger, Han-Chieh\n  Chao, and Philip S. Yu", "title": "HUOPM: High Utility Occupancy Pattern Mining", "comments": "Accepted by IEEE Transactions on Cybernetics, 14 pages", "journal-ref": "IEEE Transactions on Cybernetics, 2019", "doi": "10.1109/TCYB.2019.2896267", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining useful patterns from varied types of databases is an important\nresearch topic, which has many real-life applications. Most studies have\nconsidered the frequency as sole interestingness measure for identifying high\nquality patterns. However, each object is different in nature. The relative\nimportance of objects is not equal, in terms of criteria such as the utility,\nrisk, or interest. Besides, another limitation of frequent patterns is that\nthey generally have a low occupancy, i.e., they often represent small sets of\nitems in transactions containing many items, and thus may not be truly\nrepresentative of these transactions. To extract high quality patterns in real\nlife applications, this paper extends the occupancy measure to also assess the\nutility of patterns in transaction databases. We propose an efficient algorithm\nnamed High Utility Occupancy Pattern Mining (HUOPM). It considers user\npreferences in terms of frequency, utility, and occupancy. A novel\nFrequency-Utility tree (FU-tree) and two compact data structures, called the\nutility-occupancy list and FU-table, are designed to provide global and partial\ndownward closure properties for pruning the search space. The proposed method\ncan efficiently discover the complete set of high quality patterns without\ncandidate generation. Extensive experiments have been conducted on several\ndatasets to evaluate the effectiveness and efficiency of the proposed\nalgorithm. Results show that the derived patterns are intelligible, reasonable\nand acceptable, and that HUOPM with its pruning strategies outperforms the\nstate-of-the-art algorithm, in terms of runtime and search space, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 08:57:43 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Fournier-Viger", "Philippe", ""], ["Chao", "Han-Chieh", ""], ["Yu", "Philip S.", ""]]}, {"id": "1812.10942", "submitter": "Tejas Kulkarni", "authors": "Tejas Kulkarni and Graham Cormode and Divesh Srivastava", "title": "Answering Range Queries Under Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the fraction of a population having an input within a specified\ninterval i.e. a \\emph{range query}, is a fundamental data analysis primitive.\nRange queries can also be used to compute other interesting statistics such as\n\\emph{quantiles}, and to build prediction models. However, frequently the data\nis subject to privacy concerns when it is drawn from individuals, and relates\nfor example to their financial, health, religious or political status. In this\npaper, we introduce and analyze methods to support range queries under the\nlocal variant of differential privacy, an emerging standard for\nprivacy-preserving data analysis.\n  The local model requires that each user releases a noisy view of her private\ndata under a privacy guarantee. While many works address the problem of range\nqueries in the trusted aggregator setting, this problem has not been addressed\nspecifically under untrusted aggregation (local DP) model even though many\nprimitives have been developed recently for estimating a discrete distribution.\nWe describe and analyze two classes of approaches for range queries, based on\nhierarchical histograms and the Haar wavelet transform. We show that both have\nstrong theoretical accuracy guarantees on variance. In practice, both methods\nare fast and require minimal computation and communication resources. Our\nexperiments show that the wavelet approach is most accurate in high privacy\nsettings, while the hierarchical approach dominates for weaker privacy\nrequirements.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 10:25:50 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 12:26:42 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Kulkarni", "Tejas", ""], ["Cormode", "Graham", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1812.10977", "submitter": "Susana Ladra", "authors": "Sandra \\'Alvarez-Garc\\'ia, Borja Freire, Susana Ladra, \\'Oscar\n  Pedreira", "title": "Compact and Efficient Representation of General Graph Databases", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Knowledge and Information Systems, 2018", "doi": "10.1007/s10115-018-1275-x", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a compact data structure to store labeled\nattributed graphs based on the k2-tree, which is a very compact data structure\ndesigned to represent a simple directed graph. The idea we propose can be seen\nas an extension of the k2-tree to support property graphs. In addition to the\nstatic approach, we also propose a dynamic version of the storage\nrepresentation, which allows exible schemas and insertion or deletion of data.\nWe provide an implementation of a basic set of operations, which can be\ncombined to form complex queries over these graphs with attributes. We evaluate\nthe performance of our proposal with existing graph database systems and prove\nthat our compact attributed graph representation obtains also competitive time\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 12:46:15 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["\u00c1lvarez-Garc\u00eda", "Sandra", ""], ["Freire", "Borja", ""], ["Ladra", "Susana", ""], ["Pedreira", "\u00d3scar", ""]]}, {"id": "1812.11346", "submitter": "Fotis Savva", "authors": "Fotis Savva, Christos Anagnostopoulos, Peter Triantafillou", "title": "Explaining Aggregates for Exploratory Analytics", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/BigData.2018.8621953", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysts wishing to explore multivariate data spaces, typically pose queries\ninvolving selection operators, i.e., range or radius queries, which define data\nsubspaces of possible interest and then use aggregation functions, the results\nof which determine their exploratory analytics interests. However, such\naggregate query (AQ) results are simple scalars and as such, convey limited\ninformation about the queried subspaces for exploratory analysis. We address\nthis shortcoming aiding analysts to explore and understand data subspaces by\ncontributing a novel explanation mechanism coined XAXA: eXplaining Aggregates\nfor eXploratory Analytics. XAXA's novel AQ explanations are represented using\nfunctions obtained by a three-fold joint optimization problem. Explanations\nassume the form of a set of parametric piecewise-linear functions acquired\nthrough a statistical learning model. A key feature of the proposed solution is\nthat model training is performed by only monitoring AQs and their answers\non-line. In XAXA, explanations for future AQs can be computed without any\ndatabase (DB) access and can be used to further explore the queried data\nsubspaces, without issuing any more queries to the DB. We evaluate the\nexplanation accuracy and efficiency of XAXA through theoretically grounded\nmetrics over real-world and synthetic datasets and query workloads.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 11:43:32 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 17:04:57 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Savva", "Fotis", ""], ["Anagnostopoulos", "Christos", ""], ["Triantafillou", "Peter", ""]]}, {"id": "1812.11542", "submitter": "Xin Hu", "authors": "Xin Hu, Guoyin Wang and Jiangli Duan", "title": "Mining Maximal Dynamic Spatial Co-Location Patterns", "comments": "10 pages,7 figures", "journal-ref": null, "doi": "10.1109/TNNLS.2020.2979875", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spatial co-location pattern represents a subset of spatial features whose\ninstances are prevalently located together in a geographic space. Although many\nalgorithms of mining spatial co-location pattern have been proposed, there are\nstill some problems: 1) they miss some meaningful patterns (e.g.,\n{Ganoderma_lucidumnew, maple_treedead} and {water_hyacinthnew(increase),\nalgaedead(decrease)}), and get the wrong conclusion that the instances of two\nor more features increase/decrease (i.e., new/dead) in the same/approximate\nproportion, which has no effect on prevalent patterns. 2) Since the number of\nprevalent spatial co-location patterns is very large, the efficiency of\nexisting methods is very low to mine prevalent spatial co-location patterns.\nTherefore, first, we propose the concept of dynamic spatial co-location pattern\nthat can reflect the dynamic relationships among spatial features. Second, we\nmine small number of prevalent maximal dynamic spatial co-location patterns\nwhich can derive all prevalent dynamic spatial co-location patterns, which can\nimprove the efficiency of obtaining all prevalent dynamic spatial co-location\npatterns. Third, we propose an algorithm for mining prevalent maximal dynamic\nspatial co-location patterns and two pruning strategies. Finally, the\neffectiveness and efficiency of the method proposed as well as the pruning\nstrategies are verified by extensive experiments over real/synthetic datasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 14:20:45 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 09:11:51 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 03:44:24 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Hu", "Xin", ""], ["Wang", "Guoyin", ""], ["Duan", "Jiangli", ""]]}]