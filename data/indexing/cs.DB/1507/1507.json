[{"id": "1507.00257", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Babak Salimi", "title": "From Causes for Database Queries to Repairs and Model-Based Diagnosis\n  and Back", "comments": "To appear in Theory of Computing Systems. By invitation to special\n  issue with extended papers from ICDT 2015 (paper arXiv:1412.4311)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we establish and investigate connections between causes for\nquery answers in databases, database repairs wrt. denial constraints, and\nconsistency-based diagnosis. The first two are relatively new research areas in\ndatabases, and the third one is an established subject in knowledge\nrepresentation. We show how to obtain database repairs from causes, and the\nother way around. Causality problems are formulated as diagnosis problems, and\nthe diagnoses provide causes and their responsibilities. The vast body of\nresearch on database repairs can be applied to the newer problems of computing\nactual causes for query answers and their responsibilities. These connections,\nwhich are interesting per se, allow us, after a transition -inspired by\nconsistency-based diagnosis- to computational problems on hitting sets and\nvertex covers in hypergraphs, to obtain several new algorithmic and complexity\nresults for database causality.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 15:20:29 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 17:52:10 GMT"}, {"version": "v3", "created": "Sun, 23 Oct 2016 17:37:58 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Salimi", "Babak", ""]]}, {"id": "1507.00541", "submitter": "Ond\\v{r}ej Vaverka", "authors": "Ondrej Vaverka, Vilem Vychodil", "title": "Relational Division in Rank-Aware Databases", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2016.02.060", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a survey of existing approaches to relational division in\nrank-aware databases, discuss issues of the present approaches, and outline\ngeneralizations of several types of classic division-like operations. We work\nin a model which generalizes the Codd model of data by considering tuples in\nrelations annotated by ranks, indicating degrees to which tuples in relations\nmatch queries. The approach utilizes complete residuated lattices as the basic\nstructures of degrees. We argue that unlike the classic model, relational\ndivisions are fundamental operations which cannot in general be expressed by\nmeans of other operations. In addition, we compare the existing and proposed\noperations and identify those which are faithful counterparts of universally\nquantified queries formulated in relational calculi. We introduce Pseudo Tuple\nCalculus in the ranked model which is further used to show mutual definability\nof the various forms of divisions presented in the paper.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 12:09:29 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Vaverka", "Ondrej", ""], ["Vychodil", "Vilem", ""]]}, {"id": "1507.00646", "submitter": "Zhensong Qian", "authors": "Oliver Schulte and Zhensong Qian", "title": "SQL for SRL: Structure Learning Inside a Database System", "comments": "3 pages, 1 figure, Position Paper of the Fifth International Workshop\n  on Statistical Relational AI at UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The position we advocate in this paper is that relational algebra can provide\na unified language for both representing and computing with\nstatistical-relational objects, much as linear algebra does for traditional\nsingle-table machine learning. Relational algebra is implemented in the\nStructured Query Language (SQL), which is the basis of relational database\nmanagement systems. To support our position, we have developed the FACTORBASE\nsystem, which uses SQL as a high-level scripting language for\nstatistical-relational learning of a graphical model structure. The design\nphilosophy of FACTORBASE is to manage statistical models as first-class\ncitizens inside a database. Our implementation shows how our SQL constructs in\nFACTORBASE facilitate fast, modular, and reliable program development.\nEmpirical evidence from six benchmark databases indicates that leveraging\ndatabase system capabilities achieves scalable model structure learning.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 16:07:48 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Schulte", "Oliver", ""], ["Qian", "Zhensong", ""]]}, {"id": "1507.00674", "submitter": "Cibele Freire", "authors": "Cibele Freire, Wolfgang Gatterbauer, Neil Immerman, Alexandra Meliou", "title": "A Characterization of the Complexity of Resilience and Responsibility\n  for Self-join-free Conjunctive Queries", "comments": "36 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several research thrusts in the area of data management have focused on\nunderstanding how changes in the data affect the output of a view or standing\nquery. Example applications are explaining query results, propagating updates\nthrough views, and anonymizing datasets. These applications usually rely on\nunderstanding how interventions in a database impact the output of a query. An\nimportant aspect of this analysis is the problem of deleting a minimum number\nof tuples from the input tables to make a given Boolean query false. We refer\nto this problem as \"the resilience of a query\" and show its connections to the\nwell-studied problems of deletion propagation and causal responsibility. In\nthis paper, we study the complexity of resilience for self-join-free\nconjunctive queries, and also make several contributions to previous known\nresults for the problems of deletion propagation with source side-effects and\ncausal responsibility: (1) We define the notion of resilience and provide a\ncomplete dichotomy for the class of self-join-free conjunctive queries with\narbitrary functional dependencies; this dichotomy also extends and generalizes\nprevious tractability results on deletion propagation with source side-effects.\n(2) We formalize the connection between resilience and causal responsibility,\nand show that resilience has a larger class of tractable queries than\nresponsibility. (3) We identify a mistake in a previous dichotomy for the\nproblem of causal responsibility and offer a revised characterization based on\nnew, simpler, and more intuitive notions. (4) Finally, we extend the dichotomy\nfor causal responsibility in two ways: (a) we treat cases where the input\ntables contain functional dependencies, and (b) we compute responsibility for a\nset of tuples specified via wildcards.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 17:45:32 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Freire", "Cibele", ""], ["Gatterbauer", "Wolfgang", ""], ["Immerman", "Neil", ""], ["Meliou", "Alexandra", ""]]}, {"id": "1507.00819", "submitter": "Alexandra Meliou", "authors": "Matteo Brucato, Azza Abouzied, Alexandra Meliou", "title": "Improving package recommendations through query relaxation", "comments": null, "journal-ref": "Matteo Brucato, Azza Abouzied, and Alexandra Meliou. Improving\n  Package Recommendations Through Query Relaxation. In Proceedings of the 1st\n  International DATA4U Workshop, in conjunction with VLDB, 2014", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems aim to identify items that are likely to be of\ninterest to users. In many cases, users are interested in package\nrecommendations as collections of items. For example, a dietitian may wish to\nderive a dietary plan as a collection of recipes that is nutritionally\nbalanced, and a travel agent may want to produce a vacation package as a\ncoordinated collection of travel and hotel reservations. Recent work has\nexplored extending recommendation systems to support packages of items. These\nsystems need to solve complex combinatorial problems, enforcing various\nproperties and constraints defined on sets of items. Introducing constraints on\npackages makes recommendation queries harder to evaluate, but also harder to\nexpress: Queries that are under-specified produce too many answers, whereas\nqueries that are over-specified frequently miss interesting solutions.\n  In this paper, we study query relaxation techniques that target package\nrecommendation systems. Our work offers three key insights: First, even when\nthe original query result is not empty, relaxing constraints can produce\npreferable solutions. Second, a solution due to relaxation can only be\npreferred if it improves some property specified by the query. Third,\nrelaxation should not treat all constraints as equals: some constraints are\nmore important to the users than others. Our contributions are threefold: (a)\nwe define the problem of deriving package recommendations through query\nrelaxation, (b) we design and experimentally evaluate heuristics that relax\nquery constraints to derive interesting packages, and (c) we present a crowd\nstudy that evaluates the sensitivity of real users to different kinds of\nconstraints and demonstrates that query relaxation is a powerful tool in\ndiversifying package recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 05:33:04 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Brucato", "Matteo", ""], ["Abouzied", "Azza", ""], ["Meliou", "Alexandra", ""]]}, {"id": "1507.00942", "submitter": "Alexandra Meliou", "authors": "Matteo Brucato, Rahul Ramakrishna, Azza Abouzied, Alexandra Meliou", "title": "PackageBuilder: From Tuples to Packages", "comments": null, "journal-ref": "PVLDB, vol. 7, no. 13, 2014, pp. 1593-1596", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this demo, we present PackageBuilder, a system that extends database\nsystems to support package queries. A package is a collection of tuples that\nindividually satisfy base constraints and collectively satisfy global\nconstraints. The need for package support arises in a variety of scenarios: For\nexample, in the creation of meal plans, users are not only interested in the\nnutritional content of individual meals (base constraints), but also care to\nspecify daily consumption limits and control the balance of the entire plan\n(global constraints). We introduce PaQL, a declarative SQL-based package query\nlanguage, and the interface abstractions which allow users to interactively\nspecify package queries and easily navigate through their results. To\nefficiently evaluate queries, the system employs pruning and heuristics, as\nwell as state-of-the-art constraint optimization solvers. We demonstrate\nPackageBuilder by allowing attendees to interact with the system's interface,\nto define PaQL queries and to observe how query evaluation is performed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 15:22:57 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Brucato", "Matteo", ""], ["Ramakrishna", "Rahul", ""], ["Abouzied", "Azza", ""], ["Meliou", "Alexandra", ""]]}, {"id": "1507.01066", "submitter": "Dylan Hutchison", "authors": "Dylan Hutchison and Jeremy Kepner and Vijay Gadepally and Adam Fuchs", "title": "Graphulo Implementation of Server-Side Sparse Matrix Multiply in the\n  Accumulo Database", "comments": "To be presented at IEEE HPEC 2015: http://www.ieee-hpec.org/", "journal-ref": null, "doi": "10.1109/HPEC.2015.7322448", "report-no": null, "categories": "cs.DB cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Apache Accumulo database excels at distributed storage and indexing and\nis ideally suited for storing graph data. Many big data analytics compute on\ngraph data and persist their results back to the database. These graph\ncalculations are often best performed inside the database server. The GraphBLAS\nstandard provides a compact and efficient basis for a wide range of graph\napplications through a small number of sparse matrix operations. In this\narticle, we implement GraphBLAS sparse matrix multiplication server-side by\nleveraging Accumulo's native, high-performance iterators. We compare the\nmathematics and performance of inner and outer product implementations, and\nshow how an outer product implementation achieves optimal performance near\nAccumulo's peak write rate. We offer our work as a core component to the\nGraphulo library that will deliver matrix math primitives for graph analytics\nwithin Accumulo.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 05:20:22 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2015 05:47:18 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Hutchison", "Dylan", ""], ["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Fuchs", "Adam", ""]]}, {"id": "1507.01345", "submitter": "Zhi-Hong Deng", "authors": "Zhi-Hong Deng", "title": "DiffNodesets: An Efficient Structure for Fast Mining Frequent Itemsets", "comments": "22 pages, 13 figures", "journal-ref": "Applied Soft Computing. 41 (2016) 214-223", "doi": "10.1016/j.asoc.2016.01.010", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent itemsets is an essential problem in data mining and plays an\nimportant role in many data mining applications. In recent years, some itemset\nrepresentations based on node sets have been proposed, which have shown to be\nvery efficient for mining frequent itemsets. In this paper, we propose\nDiffNodeset, a novel and more efficient itemset representation, for mining\nfrequent itemsets. Based on the DiffNodeset structure, we present an efficient\nalgorithm, named dFIN, to mining frequent itemsets. To achieve high efficiency,\ndFIN finds frequent itemsets using a set-enumeration tree with a hybrid search\nstrategy and directly enumerates frequent itemsets without candidate generation\nunder some case. For evaluating the performance of dFIN, we have conduct\nextensive experiments to compare it against with existing leading algorithms on\na variety of real and synthetic datasets. The experimental results show that\ndFIN is significantly faster than these leading algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 08:04:25 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Deng", "Zhi-Hong", ""]]}, {"id": "1507.01443", "submitter": "Erik Ferragut", "authors": "Erik M. Ferragut, Jason Laska", "title": "Nonparametric Bayesian Modeling for Automated Database Schema Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of merging databases arises in many government and commercial\napplications. Schema matching, a common first step, identifies equivalent\nfields between databases. We introduce a schema matching framework that builds\nnonparametric Bayesian models for each field and compares them by computing the\nprobability that a single model could have generated both fields. Our\nexperiments show that our method is more accurate and faster than the existing\ninstance-based matching algorithms in part because of the use of nonparametric\nBayesian models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 13:26:02 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Ferragut", "Erik M.", ""], ["Laska", "Jason", ""]]}, {"id": "1507.01663", "submitter": "Hengfeng Wei", "authors": "Hengfeng Wei, Yu Huang, Jiannong Cao, Jian Lu", "title": "Almost Strong Consistency: \"Good Enough\" in Distributed Storage Systems", "comments": "17 pages, including 5 pages for appendix; 7 figures; 5 tables; to be\n  submitted to VLDB'2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A consistency/latency tradeoff arises as soon as a distributed storage system\nreplicates data. For low latency, modern storage systems often settle for weak\nconsistency conditions, which provide little, or even worse, no guarantee for\ndata consistency. In this paper we propose the notion of almost strong\nconsistency as a better balance option for the consistency/latency tradeoff. It\nprovides both deterministically bounded staleness of data versions for each\nread and probabilistic quantification on the rate of \"reading stale values\",\nwhile achieving low latency. In the context of distributed storage systems, we\ninvestigate almost strong consistency in terms of 2-atomicity. Our 2AM\n(2-Atomicity Maintenance) algorithm completes both reads and writes in one\ncommunication round-trip, and guarantees that each read obtains the value of\nwithin the latest 2 versions. To quantify the rate of \"reading stale values\",\nwe decompose the so-called \"old-new inversion\" phenomenon into concurrency\npatterns and read-write patterns, and propose a stochastic queueing model and a\n\"timed balls-into-bins model\" to analyze them, respectively. The theoretical\nanalysis not only demonstrates that \"old-new inversions\" rarely occur as\nexpected, but also reveals that the read-write pattern dominates in\nguaranteeing such rare data inconsistencies. These are further confirmed by the\nexperimental results, showing that 2-atomicity is \"good enough\" in distributed\nstorage systems by achieving low latency, bounded staleness, and rare data\ninconsistencies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 03:22:32 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 03:21:05 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Wei", "Hengfeng", ""], ["Huang", "Yu", ""], ["Cao", "Jiannong", ""], ["Lu", "Jian", ""]]}, {"id": "1507.01685", "submitter": "Archana Bhat", "authors": "Archana N., S. S. Pawar", "title": "Periodicity Detection of Outlier Sequences Using Constraint Based\n  Pattern Tree with MAD", "comments": "7 pages, 6 figures", "journal-ref": "International Journal of Advanced Studies in Computer Science and\n  Engineering (IJASCSE), Volume 4, Issue 6;June 2015; PageNumber: 34-40", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patterns that appear rarely or unusually in the data can be defined as\noutlier patterns. The basic idea behind detecting outlier patterns is\ncomparison of their relative frequencies with frequent patterns. Their\nfrequencies of appearance are less and thus have lesser support in the data.\nDetecting outlier patterns is an important data mining task which will reveal\nsome interesting facts. The search for periodicity of patterns gives the\nbehavior of these patterns across time as to when they repeat likely. This in\nturn helps in prediction of events. These patterns are found in Time\nseries-data, social networks etc. In this paper, an algorithm for periodic\noutlier pattern detection is proposed with the usage of a Constraint Based FP\n(Frequent Pattern)-tree as the underlying data structure for time series data.\nThe growth of the tree is limited by using level and monotonic constraints. The\nprotein sequence of bacteria named E.Coli is collected and periodic outlier\npatterns in the sequence are identified. Further the enhancement of results is\nobtained by finding the Median Absolute Deviation (MAD) in defining candidate\noutlier patterns. The comparative results between STNR-out (Suffix Tree Noise\nResilient for Outlier Detection) and proposed algorithm are illustrated. The\nresults show the effectiveness and applicability of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 06:48:40 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["N.", "Archana", ""], ["Pawar", "S. S.", ""]]}, {"id": "1507.01708", "submitter": "Carlo Sartiani", "authors": "Dario Colazzo, Carlo Sartiani", "title": "Typing Regular Path Query Languages for Data Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular path query languages for data graphs are essentially \\emph{untyped}.\nThe lack of type information greatly limits the optimization opportunities for\nquery engines and makes application development more complex. In this paper we\ndiscuss a simple, yet expressive, schema language for edge-labelled data\ngraphs. This schema language is, then, used to define a query type inference\napproach with good precision properties.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 08:43:59 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Colazzo", "Dario", ""], ["Sartiani", "Carlo", ""]]}, {"id": "1507.01759", "submitter": "Anjali Radkar", "authors": "Anjali N. Radkar, S.S. Pawar", "title": "Mining high on-shelf utility itemsets with negative values from dynamic\n  updated database", "comments": "7 pages, 4 figures", "journal-ref": "International Journal of advanced studies in Computer Science and\n  Engineering IJASCSE, Volume 4, Issue 6, pp.27-33,2015", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utility mining emerged to overcome the limitations of frequent itemset mining\nby considering the utility of an item. Utility of an item is based on user's\ninterest or preference. Recently, temporal data mining has become a core\ntechnical data processing technique to deal with changing data. On-shelf\nutility mining considers on-shelf time period of item and gets the accurate\nutility values of itemsets in temporal database. In traditional on-shelf\nutility mining, profits of all items in databases are considered as positive\nvalues. However, in real applications, some items may have negative profit.\nMost of the traditional algorithms are used to handle static database. In\npractical situations, temporal databases are continually appended or updated.\nHigh on-shelf utility itemsets needs to be updated. Re-running the temporal\nmining algorithm every time is ineffective since it neglects previously\ndiscovered itemsets. It repeats the work done previously. In this paper, an\neffective algorithm is proposed to find high on-shelf utility itemsets with\nnegative values from the dynamic updated temporal database.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 11:40:47 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Radkar", "Anjali N.", ""], ["Pawar", "S. S.", ""]]}, {"id": "1507.02321", "submitter": "Olivier Cur\\'e", "authors": "Olivier Cur\\'e and Hubert Naacke and Mohamed-Amine Baazizi and Bernd\n  Amann", "title": "On the Evaluation of RDF Distribution Algorithms Implemented over Apache\n  Spark", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying very large RDF data sets in an efficient manner requires a\nsophisticated distribution strategy. Several innovative solutions have recently\nbeen proposed for optimizing data distribution with predefined query workloads.\nThis paper presents an in-depth analysis and experimental comparison of five\nrepresentative and complementary distribution approaches. For achieving fair\nexperimental results, we are using Apache Spark as a common parallel computing\nframework by rewriting the concerned algorithms using the Spark API. Spark\nprovides guarantees in terms of fault tolerance, high availability and\nscalability which are essential in such systems. Our different implementations\naim to highlight the fundamental implementation-independent characteristics of\neach approach in terms of data preparation, load balancing, data replication\nand to some extent to query answering cost and performance. The presented\nmeasures are obtained by testing each system on one synthetic and one\nreal-world data set over query workloads with differing characteristics and\ndifferent partitioning constraints.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 21:51:11 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Cur\u00e9", "Olivier", ""], ["Naacke", "Hubert", ""], ["Baazizi", "Mohamed-Amine", ""], ["Amann", "Bernd", ""]]}, {"id": "1507.02357", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, William Arcand, David Bestor, Bill Bergeron, Chansup\n  Byun, Lauren Edwards, Vijay Gadepally, Matthew Hubbell, Peter Michaleas,\n  Julie Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Albert Reuther", "title": "Lustre, Hadoop, Accumulo", "comments": "6 pages; accepted to IEEE High Performance Extreme Computing\n  conference, Waltham, MA, 2015", "journal-ref": null, "doi": "10.1109/HPEC.2015.7322476", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data processing systems impose multiple views on data as it is processed by\nthe system. These views include spreadsheets, databases, matrices, and graphs.\nThere are a wide variety of technologies that can be used to store and process\ndata through these different steps. The Lustre parallel file system, the Hadoop\ndistributed file system, and the Accumulo database are all designed to address\nthe largest and the most challenging data storage problems. There have been\nmany ad-hoc comparisons of these technologies. This paper describes the\nfoundational principles of each technology, provides simple models for\nassessing their capabilities, and compares the various technologies on a\nhypothetical common cluster. These comparisons indicate that Lustre provides 2x\nmore storage capacity, is less likely to loose data during 3 simultaneous drive\nfailures, and provides higher bandwidth on general purpose workloads. Hadoop\ncan provide 4x greater read bandwidth on special purpose workloads. Accumulo\nprovides 10,000x lower latency on random lookups than either Lustre or Hadoop\nbut Accumulo's bulk bandwidth is 10x less. Significant recent work has been\ndone to enable mix-and-match solutions that allow Lustre, Hadoop, and Accumulo\nto be combined in different ways.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 03:00:06 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Kepner", "Jeremy", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Edwards", "Lauren", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matthew", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1507.03049", "submitter": "Feilong Liu", "authors": "Feilong Liu and Spyros Blanas (The Ohio State University)", "title": "Forecasting the cost of processing multi-join queries via hashing for\n  main-memory databases (Extended version)", "comments": "15 pages, 8 figures, extended version of the paper to appear in\n  SoCC'15", "journal-ref": null, "doi": "10.1145/2806777.2806944", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database management systems (DBMSs) carefully optimize complex multi-join\nqueries to avoid expensive disk I/O. As servers today feature tens or hundreds\nof gigabytes of RAM, a significant fraction of many analytic databases becomes\nmemory-resident. Even after careful tuning for an in-memory environment, a\nlinear disk I/O model such as the one implemented in PostgreSQL may make query\nresponse time predictions that are up to 2X slower than the optimal multi-join\nquery plan over memory-resident data. This paper introduces a memory I/O cost\nmodel to identify good evaluation strategies for complex query plans with\nmultiple hash-based equi-joins over memory-resident data. The proposed cost\nmodel is carefully validated for accuracy using three different systems,\nincluding an Amazon EC2 instance, to control for hardware-specific differences.\nPrior work in parallel query evaluation has advocated right-deep and bushy\ntrees for multi-join queries due to their greater parallelization and\npipelining potential. A surprising finding is that the conventional wisdom from\nshared-nothing disk-based systems does not directly apply to the modern\nshared-everything memory hierarchy. As corroborated by our model, the\nperformance gap between the optimal left-deep and right-deep query plan can\ngrow to about 10X as the number of joins in the query increases.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 00:17:59 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 19:57:52 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Liu", "Feilong", "", "The Ohio State University"], ["Blanas", "Spyros", "", "The Ohio State University"]]}, {"id": "1507.04180", "submitter": "S\\\"oren Auer", "authors": "Ali Ismayilov and Dimitris Kontokostas and S\\\"oren Auer and Jens\n  Lehmann and Sebastian Hellmann", "title": "Wikidata through the Eyes of DBpedia", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  DBpedia is one of the first and most prominent nodes of the Linked Open Data\ncloud. It provides structured data for more than 100 Wikipedia language\neditions as well as Wikimedia Commons, has a mature ontology and a stable and\nthorough Linked Data publishing lifecycle. Wikidata, on the other hand, has\nrecently emerged as a user curated source for structured information which is\nincluded in Wikipedia. In this paper, we present how Wikidata is incorporated\nin the DBpedia ecosystem. Enriching DBpedia with structured information from\nWikidata provides added value for a number of usage scenarios. We outline those\nscenarios and describe the structure and conversion process of the\nDBpediaWikidata dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 11:59:07 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Ismayilov", "Ali", ""], ["Kontokostas", "Dimitris", ""], ["Auer", "S\u00f6ren", ""], ["Lehmann", "Jens", ""], ["Hellmann", "Sebastian", ""]]}, {"id": "1507.04461", "submitter": "Shantanu Sharma", "authors": "Foto Afrati, Shlomi Dolev, Ephraim Korach, Shantanu Sharma, Jeffrey D.\n  Ullman", "title": "Assignment Problems of Different-Sized Inputs in MapReduce", "comments": "This paper is accepted in ACM Transactions on Knowledge Discovery\n  from Data (TKDD), August 2016. Preliminary versions of this paper have\n  appeared in the proceeding of DISC 2014 and BeyondMR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A MapReduce algorithm can be described by a mapping schema, which assigns\ninputs to a set of reducers, such that for each required output there exists a\nreducer that receives all the inputs that participate in the computation of\nthis output. Reducers have a capacity, which limits the sets of inputs that\nthey can be assigned. However, individual inputs may vary in terms of size. We\nconsider, for the first time, mapping schemas where input sizes are part of the\nconsiderations and restrictions. One of the significant parameters to optimize\nin any MapReduce job is communication cost between the map and reduce phases.\nThe communication cost can be optimized by minimizing the number of copies of\ninputs sent to the reducers. The communication cost is closely related to the\nnumber of reducers of constrained capacity that are used to accommodate\nappropriately the inputs, so that the requirement of how the inputs must meet\nin a reducer is satisfied. In this work, we consider a family of problems where\nit is required that each input meets with each other input in at least one\nreducer. We also consider a slightly different family of problems in which,\neach input of a list, X, is required to meet each input of another list, Y, in\nat least one reducer. We prove that finding an optimal mapping schema for these\nfamilies of problems is NP-hard, and present a bin-packing-based approximation\nalgorithm for finding a near optimal mapping schema.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 06:46:19 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 19:07:51 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Afrati", "Foto", ""], ["Dolev", "Shlomi", ""], ["Korach", "Ephraim", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""]]}, {"id": "1507.04614", "submitter": "Olaf Hartig", "authors": "Olaf Hartig and Jorge P\\'erez", "title": "LDQL: A Query Language for the Web of Linked Data (Extended Version)", "comments": "39 pages, Extended version of a paper published in ISWC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web of Linked Data is composed of tons of RDF documents interlinked to\neach other forming a huge repository of distributed semantic data. Effectively\nquerying this distributed data source is an important open problem in the\nSemantic Web area. In this paper, we propose LDQL, a declarative language to\nquery Linked Data on the Web. One of the novelties of LDQL is that it expresses\nseparately (i) patterns that describe the expected query result, and (ii) Web\nnavigation paths that select the data sources to be used for computing the\nresult. We present a formal syntax and semantics, prove equivalence rules, and\nstudy the expressiveness of the language. In particular, we show that LDQL is\nstrictly more expressive than the query formalisms that have been proposed\npreviously for Linked Data on the Web. The high expressiveness allows LDQL to\ndefine queries for which a complete execution is not computationally feasible\nover the Web. We formally study this issue and provide a syntactic sufficient\ncondition to avoid this problem; queries satisfying this condition are ensured\nto have a procedure to be effectively evaluated over the Web of Linked Data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 15:19:03 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2015 09:14:16 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Hartig", "Olaf", ""], ["P\u00e9rez", "Jorge", ""]]}, {"id": "1507.04955", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli", "title": "Structurally Tractable Uncertain Data", "comments": "11 pages, 1 figure, 1 table. To appear in SIGMOD/PODS PhD Symposium\n  2015", "journal-ref": null, "doi": "10.1145/2744680.2744690", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many data management applications must deal with data which is uncertain,\nincomplete, or noisy. However, on existing uncertain data representations, we\ncannot tractably perform the important query evaluation tasks of determining\nquery possibility, certainty, or probability: these problems are hard on\narbitrary uncertain input instances. We thus ask whether we could restrict the\nstructure of uncertain data so as to guarantee the tractability of exact query\nevaluation. We present our tractability results for tree and tree-like\nuncertain data, and a vision for probabilistic rule reasoning. We also study\nuncertainty about order, proposing a suitable representation, and study\nuncertain data conditioned by additional observations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 13:04:32 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Amarilli", "Antoine", ""]]}, {"id": "1507.05591", "submitter": "Yeounoh Chung", "authors": "Yeounoh Chung, Michael Lind Mortensen, Carsten Binnig, Tim Kraska", "title": "Estimating the Impact of Unknown Unknowns on Aggregate Query Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common practice for data scientists to acquire and integrate disparate\ndata sources to achieve higher quality results. But even with a perfectly\ncleaned and merged data set, two fundamental questions remain: (1) is the\nintegrated data set complete and (2) what is the impact of any unknown (i.e.,\nunobserved) data on query results?\n  In this work, we develop and analyze techniques to estimate the impact of the\nunknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key\nidea is that the overlap between different data sources enables us to estimate\nthe number and values of the missing data items. Our main techniques are\nparameter-free and do not assume prior knowledge about the distribution.\nThrough a series of experiments, we show that estimating the impact of unknown\nunknowns is invaluable to better assess the results of aggregate queries over\nintegrated data sources.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 18:59:25 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 19:40:34 GMT"}, {"version": "v3", "created": "Sat, 26 Dec 2015 17:52:08 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Chung", "Yeounoh", ""], ["Mortensen", "Michael Lind", ""], ["Binnig", "Carsten", ""], ["Kraska", "Tim", ""]]}, {"id": "1507.06103", "submitter": "Marco Manna", "authors": "Marco Manna and Francesco Ricca and Giorgio Terracina", "title": "Taming Primary Key Violations to Query Large Inconsistent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistent query answering over a database that violates primary key\nconstraints is a classical hard problem in database research that has been\ntraditionally dealt with logic programming. However, the applicability of\nexisting logic-based solutions is restricted to data sets of moderate size.\nThis paper presents a novel decomposition and pruning strategy that reduces, in\npolynomial time, the problem of computing the consistent answer to a\nconjunctive query over a database subject to primary key constraints to a\ncollection of smaller problems of the same sort that can be solved\nindependently. The new strategy is naturally modeled and implemented using\nAnswer Set Programming (ASP). An experiment run on benchmarks from the database\nworld prove the effectiveness and efficiency of our ASP-based approach also on\nlarge data sets. To appear in Theory and Practice of Logic Programming (TPLP),\nProceedings of ICLP 2015.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 08:56:03 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Manna", "Marco", ""], ["Ricca", "Francesco", ""], ["Terracina", "Giorgio", ""]]}, {"id": "1507.07629", "submitter": "Garrick Orchard", "authors": "Garrick Orchard and Ajinkya Jayawant and Gregory Cohen and Nitish\n  Thakor", "title": "Converting Static Image Datasets to Spiking Neuromorphic Datasets Using\n  Saccades", "comments": "10 pages, 6 figures in Frontiers in Neuromorphic Engineering, special\n  topic on Benchmarks and Challenges for Neuromorphic Engineering, 2015 (under\n  review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating datasets for Neuromorphic Vision is a challenging task. A lack of\navailable recordings from Neuromorphic Vision sensors means that data must\ntypically be recorded specifically for dataset creation rather than collecting\nand labelling existing data. The task is further complicated by a desire to\nsimultaneously provide traditional frame-based recordings to allow for direct\ncomparison with traditional Computer Vision algorithms. Here we propose a\nmethod for converting existing Computer Vision static image datasets into\nNeuromorphic Vision datasets using an actuated pan-tilt camera platform. Moving\nthe sensor rather than the scene or image is a more biologically realistic\napproach to sensing and eliminates timing artifacts introduced by monitor\nupdates when simulating motion on a computer monitor. We present conversion of\ntwo popular image datasets (MNIST and Caltech101) which have played important\nroles in the development of Computer Vision, and we provide performance metrics\non these datasets using spike-based recognition algorithms. This work\ncontributes datasets for future use in the field, as well as results from\nspike-based algorithms against which future works can compare. Furthermore, by\nconverting datasets already popular in Computer Vision, we enable more direct\ncomparison with frame-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 03:23:25 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Orchard", "Garrick", ""], ["Jayawant", "Ajinkya", ""], ["Cohen", "Gregory", ""], ["Thakor", "Nitish", ""]]}, {"id": "1507.07911", "submitter": "Pablo Barcelo", "authors": "Pablo Barcelo (University of Chile), Gaelle Fontaine (University of\n  Chile), Anthony Widjaja Lin (Yale-NUS College, Singapore)", "title": "Expressive Path Queries on Graph with Data", "comments": "39 pages", "journal-ref": "Logical Methods in Computer Science, Volume 11, Issue 4 (October\n  5, 2015) lmcs:1602", "doi": "10.2168/LMCS-11(4:1)2015", "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph data models have recently become popular owing to their applications,\ne.g., in social networks and the semantic web. Typical navigational query\nlanguages over graph databases - such as Conjunctive Regular Path Queries\n(CRPQs) - cannot express relevant properties of the interaction between the\nunderlying data and the topology. Two languages have been recently proposed to\novercome this problem: walk logic (WL) and regular expressions with memory\n(REM). In this paper, we begin by investigating fundamental properties of WL\nand REM, i.e., complexity of evaluation problems and expressive power. We first\nshow that the data complexity of WL is nonelementary, which rules out its\npracticality. On the other hand, while REM has low data complexity, we point\nout that many natural data/topology properties of graphs expressible in WL\ncannot be expressed in REM. To this end, we propose register logic, an\nextension of REM, which we show to be able to express many natural graph\nproperties expressible in WL, while at the same time preserving the\nelementariness of data complexity of REMs. It is also incomparable to WL in\nterms of expressive power.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 19:37:41 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 14:47:06 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2015 21:03:11 GMT"}, {"version": "v4", "created": "Mon, 5 Oct 2015 09:49:06 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Barcelo", "Pablo", "", "University of Chile"], ["Fontaine", "Gaelle", "", "University of\n  Chile"], ["Lin", "Anthony Widjaja", "", "Yale-NUS College, Singapore"]]}, {"id": "1507.08257", "submitter": "Khaled Alyoubi", "authors": "Khaled H. Alyoubi, Sven Helmer, Peter T. Wood", "title": "Ordering Selection Operators Using the Minmax Regret Rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimising queries in real-world situations under imperfect conditions is\nstill a problem that has not been fully solved. We consider finding the optimal\norder in which to execute a given set of selection operators under partial\nignorance of their selectivities. The selectivities are modelled as intervals\nrather than exact values and we apply a concept from decision theory, the\nminimisation of the maximum regret, as a measure of optimality. We show that\nthe associated decision problem is NP-hard, which renders a brute-force\napproach to solving it impractical. Nevertheless, by investigating properties\nof the problem and identifying special cases which can be solved in polynomial\ntime, we gain insight that we use to develop a novel heuristic for solving the\ngeneral problem. We also evaluate minmax regret query optimisation\nexperimentally, showing that it outperforms a currently employed strategy of\noptimisers that uses mean values for uncertain parameters.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 18:46:03 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Alyoubi", "Khaled H.", ""], ["Helmer", "Sven", ""], ["Wood", "Peter T.", ""]]}, {"id": "1507.08492", "submitter": "Georgia Kougka", "authors": "Georgia Kougka and Anastasios Gounaris", "title": "Cost optimization of data flows based on task re-ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing big data in a highly dynamic environment becomes more and more\ncritical because of the increasingly need for end-to-end processing of this\ndata. Modern data flows are quite complex and there are not efficient,\ncost-based, fully-automated, scalable optimization solutions that can\nfacilitate flow designers. The state-of-the-art proposals fail to provide near\noptimal solutions even for simple data flows. To tackle this problem, we\nintroduce a set of approximate algorithms for defining the execution order of\nthe constituent tasks, in order to minimize the total execution cost of a data\nflow. We also present the advantages of the parallel execution of data flows.\nWe validated our proposals in both a real tool and synthetic flows and the\nresults show that we can achieve significant speed-ups, moving much closer to\noptimal solutions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 13:25:40 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Kougka", "Georgia", ""], ["Gounaris", "Anastasios", ""]]}]