[{"id": "1604.00111", "submitter": "Hung Ngo", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Dan Suciu", "title": "Computing Join Queries with Functional Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Gottlob, Lee, Valiant, and Valiant (GLVV) presented an output size\nbound for join queries with functional dependencies (FD), based on a linear\nprogram on polymatroids. GLVV bound strictly generalizes the bound of Atserias,\nGrohe and Marx (AGM) for queries with no FD, in which case there are known\nalgorithms running within AGM bound and thus are worst-case optimal.\n  A main result of this paper is an algorithm for computing join queries with\nFDs, running within GLVV bound up to a poly-log factor. In particular, our\nalgorithm is worst-case optimal for any query where the GLVV bound is tight. As\nan unexpected by-product, our algorithm manages to solve a harder problem,\nwhere (some) input relations may have prescribed maximum degree bounds, of\nwhich both functional dependencies and cardinality bounds are special cases.\n  We extend Gottlob et al. framework by replacing all variable subsets with the\nlattice of closed sets (under the given FDs). This gives us new insights into\nthe structure of the worst-case bound and worst-case instances. While it is\nstill open whether GLVV bound is tight in general, we show that it is tight on\ndistributive lattices and some other simple lattices. Distributive lattices\ncapture a strict superset of queries with no FD and with simple FDs. We also\npresent two simpler algorithms which are also worst-case optimal on\ndistributive lattices within a single-$\\log$ factor, but they do not match GLVV\nbound on a general lattice. Our algorithms are designed based on a novel\nprinciple: we turn a proof of a polymatroid-based output size bound into an\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 02:28:40 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 00:21:42 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Suciu", "Dan", ""]]}, {"id": "1604.00967", "submitter": "Yuliang Li", "authors": "Alin Deutsch, Yuliang Li, Victor Vianu", "title": "Verification of Hierarchical Artifact Systems", "comments": "Full version of the accepted PODS paper", "journal-ref": null, "doi": "10.1145/2902251.2902275", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven workflows, of which IBM's Business Artifacts are a prime\nexponent, have been successfully deployed in practice, adopted in industrial\nstandards, and have spawned a rich body of research in academia, focused\nprimarily on static analysis. The present work represents a significant advance\non the problem of artifact verification, by considering a much richer and more\nrealistic model than in previous work, incorporating core elements of IBM's\nsuccessful Guard-Stage-Milestone model. In particular, the model features task\nhierarchy, concurrency, and richer artifact data. It also allows database key\nand foreign key dependencies, as well as arithmetic constraints. The results\nshow decidability of verification and establish its complexity, making use of\nnovel techniques including a hierarchy of Vector Addition Systems and a variant\nof quantifier elimination tailored to our context.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 18:16:14 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Deutsch", "Alin", ""], ["Li", "Yuliang", ""], ["Vianu", "Victor", ""]]}, {"id": "1604.01166", "submitter": "Pierre Schaus", "authors": "John O.R. Aoga and Tias Guns and Pierre Schaus", "title": "An Efficient Algorithm for Mining Frequent Sequence with Constraint\n  Programming", "comments": "frequent sequence mining, constraint programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main advantage of Constraint Programming (CP) approaches for sequential\npattern mining (SPM) is their modularity, which includes the ability to add new\nconstraints (regular expressions, length restrictions, etc). The current best\nCP approach for SPM uses a global constraint (module) that computes the\nprojected database and enforces the minimum frequency; it does this with a\nfiltering algorithm similar to the PrefixSpan method. However, the resulting\nsystem is not as scalable as some of the most advanced mining systems like\nZaki's cSPADE. We show how, using techniques from both data mining and CP, one\ncan use a generic constraint solver and yet outperform existing specialized\nsystems. This is mainly due to two improvements in the module that computes the\nprojected frequencies: first, computing the projected database can be sped up\nby pre-computing the positions at which an symbol can become unsupported by a\nsequence, thereby avoiding to scan the full sequence each time; and second by\ntaking inspiration from the trailing used in CP solvers to devise a\nbacktracking-aware data structure that allows fast incremental storing and\nrestoring of the projected database. Detailed experiments show how this\napproach outperforms existing CP as well as specialized systems for SPM, and\nthat the gain in efficiency translates directly into increased efficiency for\nother settings such as mining with regular expressions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 08:15:24 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Aoga", "John O. R.", ""], ["Guns", "Tias", ""], ["Schaus", "Pierre", ""]]}, {"id": "1604.01848", "submitter": "Paraschos Koutris", "authors": "Paul Beame, Paraschos Koutris, Dan Suciu", "title": "Worst-Case Optimal Algorithms for Parallel Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the communication complexity for the problem of\ncomputing a conjunctive query on a large database in a parallel setting with\n$p$ servers. In contrast to previous work, where upper and lower bounds on the\ncommunication were specified for particular structures of data (either data\nwithout skew, or data with specific types of skew), in this work we focus on\nworst-case analysis of the communication cost. The goal is to find worst-case\noptimal parallel algorithms, similar to the work of [18] for sequential\nalgorithms.\n  We first show that for a single round we can obtain an optimal worst-case\nalgorithm. The optimal load for a conjunctive query $q$ when all relations have\nsize equal to $M$ is $O(M/p^{1/\\psi^*})$, where $\\psi^*$ is a new query-related\nquantity called the edge quasi-packing number, which is different from both the\nedge packing number and edge cover number of the query hypergraph. For multiple\nrounds, we present algorithms that are optimal for several classes of queries.\nFinally, we show a surprising connection to the external memory model, which\nallows us to translate parallel algorithms to external memory algorithms. This\ntechnique allows us to recover (within a polylogarithmic factor) several recent\nresults on the I/O complexity for computing join queries, and also obtain\noptimal algorithms for other classes of queries.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 02:15:13 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Beame", "Paul", ""], ["Koutris", "Paraschos", ""], ["Suciu", "Dan", ""]]}, {"id": "1604.02377", "submitter": "Heikki Virkkunen", "authors": "Heikki Virkkunen", "title": "A New Communication Theory on Complex Information and a Groundbreaking\n  New Declarative Method to Update Object Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article I introduce a new communication theory for complex\ninformation represented as a direct graph of nodes. In addition, I introduce an\napplication for the theory, a new radical method, embed, that can be used to\nupdate object databases declaratively. The embed method revolutionizes updating\nof object databases. One embed method call can replace dozens of lines of\ncomplicated updating code in a traditional client program of an object\ndatabase, which is a huge improvement. As a declarative method the embed method\ntakes only one natural parameter, the root object of a modified object\nstructure in the run-time memory, which makes it extremely easy to use.The\ncommunication theory behind the embed method states that modified complex\ninformation represented as a directed graph of nodes can always be transferred\nback to its original system in an exact and meaningful way. The theory can also\nhave additional applications since today information often has a network or\ndirected graph like structure and it often evolves or it is maintained. The\nembed method applies the communication theory by modeling the object database\nand a modified object structure in the run-time memory as directed graphs of\nnodes without any artificial limitations. For example, different kinds of\ncircular substructures and their modifications are allowed. Persistence in the\nobject database is defined in a well known and accepted way, by reachability\nfrom persistent root objects. The embed method also removes structures of\ngarbage objects from the object database if any appear during the update\noperation, leaving the database in a consistent state. The embed method applies\nreference counting techniques. It understands local topology of the object\ndatabase, avoiding examining unrelated objects in the database. For these\nreasons, the embed method is efficient and it scales for databases of different\nsizes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 23:44:11 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Virkkunen", "Heikki", ""]]}, {"id": "1604.02761", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Pierre Bourhis, Pierre Senellart", "title": "Tractable Lineages on Treelike Instances: Limits and Extensions", "comments": "33 pages, 2 tables. To appear at PODS'16. Some omitted proofs are\n  available in the thesis of the first author", "journal-ref": null, "doi": "10.1145/2902251.2902301", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query evaluation on probabilistic databases is generally intractable\n(#P-hard). Existing dichotomy results have identified which queries are\ntractable (or safe), and connected them to tractable lineages. In our previous\nwork, using different tools, we showed that query evaluation is linear-time on\nprobabilistic databases for arbitrary monadic second-order queries, if we bound\nthe treewidth of the instance.\n  In this paper, we study limitations and extensions of this result. First, for\nprobabilistic query evaluation, we show that MSO tractability cannot extend\nbeyond bounded treewidth: there are even FO queries that are hard on any\nefficiently constructible unbounded-treewidth class of graphs. This dichotomy\nrelies on recent polynomial bounds on the extraction of planar graphs as\nminors, and implies lower bounds in non-probabilistic settings, for query\nevaluation and match counting in subinstance-closed families. Second, we show\nhow to explain our tractability result in terms of lineage: the lineage of MSO\nqueries on bounded-treewidth instances can be represented as bounded-treewidth\ncircuits, polynomial-size OBDDs, and linear-size d-DNNFs. By contrast, we can\nstrengthen the previous dichotomy to lineages, and show that there are even\nUCQs with disequalities that have superpolynomial OBDDs on all\nunbounded-treewidth graph classes; we give a characterization of such queries.\nLast, we show how bounded-treewidth tractability explains the tractability of\nthe inversion-free safe queries: we can rewrite their input instances to have\nbounded-treewidth.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 23:43:46 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Senellart", "Pierre", ""]]}, {"id": "1604.03001", "submitter": "Genqiang Wu", "authors": "Genqiang Wu and Yeping He and Jingzheng Wu and Xianyao Xia", "title": "Inherit Differential Privacy in Distributed Setting: Multiparty\n  Randomized Function Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to achieve differential privacy in the distributed setting, where the\ndataset is distributed among the distrustful parties, is an important problem.\nWe consider in what condition can a protocol inherit the differential privacy\nproperty of a function it computes. The heart of the problem is the secure\nmultiparty computation of randomized function. A notion \\emph{obliviousness} is\nintroduced, which captures the key security problems when computing a\nrandomized function from a deterministic one in the distributed setting. By\nthis observation, a sufficient and necessary condition about computing a\nrandomized function from a deterministic one is given. The above result can not\nonly be used to determine whether a protocol computing differentially private\nfunction is secure, but also be used to construct secure one. Then we prove\nthat the differential privacy property of a function can be inherited by the\nprotocol computing it if the protocol privately computes it. A composition\ntheorem of differentially private protocols is also presented. We also\nconstruct some protocols to generate random variate in the distributed setting,\nsuch as the uniform random variates and the inversion method. By using these\nfundamental protocols, we construct protocols of the Gaussian mechanism, the\nLaplace mechanism and the Exponential mechanism. Importantly, all these\nprotocols satisfy obliviousness and so can be proved to be secure in a\nsimulation based manner. We also provide a complexity bound of computing\nrandomized function in the distribute setting. Finally, to show that our\nresults are fundamental and powerful to multiparty differential privacy, we\nconstruct a differentially private empirical risk minimization protocol.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 15:38:42 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Wu", "Genqiang", ""], ["He", "Yeping", ""], ["Wu", "Jingzheng", ""], ["Xia", "Xianyao", ""]]}, {"id": "1604.03212", "submitter": "Vivek R", "authors": "R Vivek, Prasad Mirje, N Sushmitha", "title": "Recommendations for web service composition by mining usage logs", "comments": null, "journal-ref": null, "doi": "10.5121/ijdkp.2016.6207", "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web service composition has been one of the most researched topics of the\npast decade. Novel methods of web service composition are being proposed in the\nliterature include Semantics-based composition, WSDLbased composition. Although\nthese methods provide promising results for composition, search and discovery\nof web service based on QoS parameter of network and semantics or ontology\nassociated with WSDL, they do not address composition based on usage of web\nservice. Web Service usage logs capture time series data of web service\ninvocation by business objects, which innately captures patterns or workflows\nassociated with business operations. Web service composition based on such\npatterns and workflows can greatly streamline the business operations. In this\nresearch work, we try to explore and implement methods of mining web service\nusage logs. Main objectives include Identifying usage association of services.\nLinking one service invocation with other, Evaluation of the causal\nrelationship between associations of services\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 12:11:11 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Vivek", "R", ""], ["Mirje", "Prasad", ""], ["Sushmitha", "N", ""]]}, {"id": "1604.03214", "submitter": "Reham Ibrahim", "authors": "Reham I. Abdel Monem, Ali H. El-Bastawissy and Mohamed M. Elwakil", "title": "DIRA: A Framework Of Data Integration Using Data Quality", "comments": "International Journal of Data Mining & Knowledge Management Process\n  (IJDKP) Vol.6, No.2, March 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration is the process of collecting data from different data\nsources and providing user with unified view of answers that meet his\nrequirements. The quality of query answers can be improved by identifying the\nquality of data sources according to some quality measures and retrieving data\nfrom only significant ones. Query answers that returned from significant data\nsources can be ranked according to quality requirements that specified in user\nquery and proposed queries types to return only top-k query answers. In this\npaper, Data integration framework called data integration to return ranked\nalternatives (DIRA) will be introduced depending on data quality assessment\nmodule that will use data sources quality to choose the significant ones and\nranking algorithm to return top-k query answers according to different queries\ntypes.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 11:08:49 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Monem", "Reham I. Abdel", ""], ["El-Bastawissy", "Ali H.", ""], ["Elwakil", "Mohamed M.", ""]]}, {"id": "1604.03226", "submitter": "Yingjun Wu", "authors": "Yingjun Wu and Wentian Guo and Chee-Yong Chan and Kian-Lee Tan", "title": "Fast Failure Recovery for Main-Memory DBMSs on Multicores", "comments": "To appear in SIGMOD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Main-memory database management systems (DBMS) can achieve excellent\nperformance when processing massive volume of on-line transactions on modern\nmulti-core machines. But existing durability schemes, namely, tuple-level and\ntransaction-level logging-and-recovery mechanisms, either degrade the\nperformance of transaction processing or slow down the process of failure\nrecovery. In this paper, we show that, by exploiting application semantics, it\nis possible to achieve speedy failure recovery without introducing any costly\nlogging overhead to the execution of concurrent transactions. We propose\nPACMAN, a parallel database recovery mechanism that is specifically designed\nfor lightweight, coarse-grained transaction-level logging. PACMAN leverages a\ncombination of static and dynamic analyses to parallelize the log recovery: at\ncompile time, PACMAN decomposes stored procedures by carefully analyzing\ndependencies within and across programs; at recovery time, PACMAN exploits the\navailability of the runtime parameter values to attain an execution schedule\nwith a high degree of parallelism. As such, recovery performance is remarkably\nincreased. We evaluated PACMAN in a fully-fledged main-memory DBMS running on a\n40-core machine. Compared to several state-of-the-art database recovery\nmechanisms, PACMAN can significantly reduce recovery time without compromising\nthe efficiency of transaction processing.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 03:00:33 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 09:29:25 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Wu", "Yingjun", ""], ["Guo", "Wentian", ""], ["Chan", "Chee-Yong", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1604.03234", "submitter": "Jia Yu", "authors": "Jia Yu, Mohamed Sarwat", "title": "Hippo: A Fast, yet Scalable, Database Indexing Approach", "comments": "12 pages, 10 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though existing database indexes (e.g., B+-Tree) speed up the query\nexecution, they suffer from two main drawbacks: (1) A database index usually\nyields 5% to 15% additional storage overhead which results in non-ignorable\ndollar cost in big data scenarios especially when deployed on modern storage\ndevices like Solid State Disk (SSD) or Non-Volatile Memory (NVM). (2)\nMaintaining a database index incurs high latency because the DBMS has to find\nand update those index pages affected by the underlying table changes. This\npaper proposes Hippo a fast, yet scalable, database indexing approach. Hippo\nonly stores the pointers of disk pages along with light weight histogram-based\nsummaries. The proposed structure significantly shrinks index storage and\nmaintenance overhead without compromising much on query execution performance.\nExperiments, based on real Hippo implementation inside PostgreSQL 9.5, using\nthe TPC-H benchmark show that Hippo achieves up to two orders of magnitude less\nstorage space and up to three orders of magnitude less maintenance overhead\nthan traditional database indexes, i.e., B+-Tree. Furthermore, the experiments\nalso show that Hippo achieves comparable query execution performance to that of\nthe B+-Tree for various selectivity factors.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 03:41:17 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Yu", "Jia", ""], ["Sarwat", "Mohamed", ""]]}, {"id": "1604.03413", "submitter": "Othmane Rezine", "authors": "Parosh Aziz Abdulla and C. Aiswarya and Mohamed Faouzi Atig and Marco\n  Montali and Othmane Rezine", "title": "Recency-Bounded Verification of Dynamic Database-Driven Systems\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a formalism to model database-driven systems, called database\nmanipulating systems (DMS). The actions of a DMS modify the current instance of\na relational database by adding new elements into the database, deleting tuples\nfrom the relations and adding tuples to the relations. The elements which are\nmodified by an action are chosen by (full) first-order queries. DMS is a highly\nexpressive model and can be thought of as a succinct representation of an\ninfinite state relational transition system, in line with similar models\nproposed in the literature. We propose monadic second order logic (MSO-FO) to\nreason about sequences of database instances appearing along a run.\nUnsurprisingly, the linear-time model checking problem of DMS against MSO-FO is\nundecidable. Towards decidability, we propose under-approximate model checking\nof DMS, where the under-approximation parameter is the \"bound on recency\". In a\n$k$-recency-bounded run, only the most recent $k$ elements in the current\nactive domain may be modified by an action. More runs can be verified by\nincreasing the bound on recency. Our main result shows that recency-bounded\nmodel checking of DMS against MSO-FO is decidable, by a reduction to the\nsatisfiability problem of MSO over nested words.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 14:01:21 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Abdulla", "Parosh Aziz", ""], ["Aiswarya", "C.", ""], ["Atig", "Mohamed Faouzi", ""], ["Montali", "Marco", ""], ["Rezine", "Othmane", ""]]}, {"id": "1604.03583", "submitter": "Tarique Siddiqui", "authors": "Tarique Siddiqui, Albert Kim, John Lee, Karrie Karahalios, Aditya\n  Parameswaran", "title": "Effortless Data Exploration with zenvisage: An Expressive and\n  Interactive Visual Analytics System", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization is by far the most commonly used mechanism to explore\ndata, especially by novice data analysts and data scientists. And yet, current\nvisual analytics tools are rather limited in their ability to guide data\nscientists to interesting or desired visualizations: the process of visual data\nexploration remains cumbersome and time-consuming. We propose zenvisage, a\nplatform for effortlessly visualizing interesting patterns, trends, or insights\nfrom large datasets. We describe zenvisage's general purpose visual query\nlanguage, ZQL (\"zee-quel\") for specifying the desired visual trend, pattern, or\ninsight - ZQL draws from use-cases in a variety of domains, including biology,\nmechanical engineering, climate science, and commerce. We formalize the\nexpressiveness of ZQL via a visual exploration algebra, and demonstrate that\nZQL is at least as expressive as that algebra. While analysts are free to use\nZQL directly, we also expose ZQL via a visual specification interface that we\ndescribe in this paper. We then describe our architecture and optimizations,\npreliminary experiments in supporting and optimizing for ZQL queries in our\ninitial zenvisage prototype, and a user study to evaluate whether data\nscientists are able to effectively use zenvisage for real applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 21:00:46 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 02:09:53 GMT"}, {"version": "v3", "created": "Thu, 4 Jan 2018 06:09:34 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Siddiqui", "Tarique", ""], ["Kim", "Albert", ""], ["Lee", "John", ""], ["Karahalios", "Karrie", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1604.03607", "submitter": "Dylan Hutchison", "authors": "Dylan Hutchison, Bill Howe, Dan Suciu", "title": "Lara: A Key-Value Algebra underlying Arrays and Relations", "comments": "Working draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data processing systems roughly group into families such as relational,\narray, graph, and key-value. Many data processing tasks exceed the capabilities\nof any one family, require data stored across families, or run faster when\npartitioned onto multiple families. Discovering ways to execute computation\namong multiple available systems, let alone discovering an optimal execution\nplan, is challenging given semantic differences between disparate families of\nsystems. In this paper we introduce a new algebra, Lara, which underlies and\nunifies algebras representing the families above in order to facilitate\ntranslation between systems. We describe the operations and objects of\nLara---union, join, and ext on associative tables---and show her properties and\nequivalences to other algebras. Multi-system optimization has a bright future,\nin which we proffer Lara for the role of universal connector.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 22:22:16 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Hutchison", "Dylan", ""], ["Howe", "Bill", ""], ["Suciu", "Dan", ""]]}, {"id": "1604.04185", "submitter": "Xiaokui Xiao", "authors": "Boyu Tian and Xiaokui Xiao", "title": "SLING: A Near-Optimal Index Structure for SimRank", "comments": "A short version of this paper will appear in SIGMOD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SimRank is a similarity measure for graph nodes that has numerous\napplications in practice. Scalable SimRank computation has been the subject of\nextensive research for more than a decade, and yet, none of the existing\nsolutions can efficiently derive SimRank scores on large graphs with provable\naccuracy guarantees. In particular, the state-of-the-art solution requires up\nto a few seconds to compute a SimRank score in million-node graphs, and does\nnot offer any worst-case assurance in terms of the query error.\n  This paper presents SLING, an efficient index structure for SimRank\ncomputation. SLING guarantees that each SimRank score returned has at most\n$\\varepsilon$ additive error, and it answers any single-pair and single-source\nSimRank queries in $O(1/\\varepsilon)$ and $O(n/\\varepsilon)$ time,\nrespectively. These time complexities are near-optimal, and are significantly\nbetter than the asymptotic bounds of the most recent approach. Furthermore,\nSLING requires only $O(n/\\varepsilon)$ space (which is also near-optimal in an\nasymptotic sense) and $O(m/\\varepsilon + n\\log \\frac{n}{\\delta}/\\varepsilon^2)$\npre-computation time, where $\\delta$ is the failure probability of the\npreprocessing algorithm. We experimentally evaluate SLING with a variety of\nreal-world graphs with up to several millions of nodes. Our results demonstrate\nthat SLING is up to $10000$ times (resp. $110$ times) faster than competing\nmethods for single-pair (resp. single-source) SimRank queries, at the cost of\nhigher space overheads.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 15:17:01 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Tian", "Boyu", ""], ["Xiao", "Xiaokui", ""]]}, {"id": "1604.04894", "submitter": "Yahia Lebbah", "authors": "Mehdi Maamar, Nadjib Lazaar, Samir Loudni, Yahia Lebbah", "title": "A global constraint for closed itemset mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the set of closed frequent patterns is one of the fundamental\nproblems in Data Mining. Recent Constraint Programming (CP) approaches for\ndeclarative itemset mining have proven their usefulness and flexibility. But\nthe wide use of reified constraints in current CP approaches raises many\ndifficulties to cope with high dimensional datasets. This paper proposes CLOSED\nPATTERN global constraint which does not require any reified constraints nor\nany extra variables to encode efficiently the Closed Frequent Pattern Mining\n(CFPM) constraint. CLOSED-PATTERN captures the particular semantics of the CFPM\nproblem in order to ensure a polynomial pruning algorithm ensuring domain\nconsistency. The computational properties of our constraint are analyzed and\ntheir practical effectiveness is experimentally evaluated.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 16:32:27 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Maamar", "Mehdi", ""], ["Lazaar", "Nadjib", ""], ["Loudni", "Samir", ""], ["Lebbah", "Yahia", ""]]}, {"id": "1604.05006", "submitter": "Heng Zhang", "authors": "Heng Zhang, Yan Zhang, Jia-Huai You", "title": "Expressive Completeness of Existential Rule Languages for Ontology-based\n  Query Answering", "comments": "10 pages; the full version of a paper to appear in IJCAI 2016.\n  Changes (regarding to v1): a new reference has been added, and some typos\n  have been corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existential rules, also known as data dependencies in Databases, have been\nrecently rediscovered as a promising family of languages for Ontology-based\nQuery Answering. In this paper, we prove that disjunctive embedded dependencies\nexactly capture the class of recursively enumerable ontologies in\nOntology-based Conjunctive Query Answering (OCQA). Our expressive completeness\nresult does not rely on any built-in linear order on the database. To establish\nthe expressive completeness, we introduce a novel semantic definition for OCQA\nontologies. We also show that neither the class of disjunctive tuple-generating\ndependencies nor the class of embedded dependencies is expressively complete\nfor recursively enumerable OCQA ontologies.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 06:16:49 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 13:22:52 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Zhang", "Heng", ""], ["Zhang", "Yan", ""], ["You", "Jia-Huai", ""]]}, {"id": "1604.05272", "submitter": "Radhakrishna Vangipuram", "authors": "Vangipuram Radhakrishna, P.V.Kumar, V.Janaki", "title": "A Novel Approach for Mining Similarity Profiled Temporal Association\n  Patterns", "comments": "Technical Journal of the Faculty of Engineering, 14 pages", "journal-ref": "Rev. Tec. Ing. Univ. Zulia. Vol. 38, No 3, 80 - 93, 2015", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of frequent pattern mining from non-temporal databases is studied\nextensively by various researchers working in areas of data mining, temporal\ndatabases and information retrieval. However, Conventional frequent pattern\nalgorithms are not suitable to find similar temporal association patterns from\ntemporal databases. A Temporal database is a database which can store past,\npresent and future information. The objective of this research is to come up\nwith a novel approach so as to find similar temporal association patterns w.r.t\nuser specified threshold and a given reference support time sequence using\nconcept of Venn diagrams. For this, we maintain two types of supports called\npositive support and negative support values to find similar temporal\nassociation patterns of user interest. The main advantage of our method is\nthat, it performs only a single scan of temporal database to find temporal\nassociation patterns similar to specified reference support sequence. This\nsingle database scan approach hence eliminates the huge overhead incurred when\nthe database is scanned multiple times. The present approach also eliminates\nthe need to compute and maintain true support values of all the subsets of\ntemporal patterns of previous stages when computing temporal patterns of next\nstage.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 18:30:46 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Radhakrishna", "Vangipuram", ""], ["Kumar", "P. V.", ""], ["Janaki", "V.", ""]]}, {"id": "1604.05274", "submitter": "Radhakrishna Vangipuram", "authors": "M.S.B.Phridvi Raj, Vangipuram Radhakrishna, C.V.Guru Rao", "title": "A Novel Gaussian Based Similarity Measure for Clustering Customer\n  Transactions Using Transaction Sequence Vector", "comments": "Technical Journal of the faculty of Engineering, April 2015, 12\n  pages, Journal Indexed in Scopus and Web of Science", "journal-ref": "Rev. Tec. Ing. Univ. Zulia. Vol. 38, No 1, 85 - 96, April 2015", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering Transactions in sequence, temporal and time series databases is\nachieving an important attention from the database researchers and software\nindustry. Significant research is carried out towards defining and validating\nthe suitability of new similarity measures for sequence, temporal, time series\ndatabases which can accurately and efficiently find the similarity between user\ntransactions in the given database to predict the user behavior. The\ndistribution of items present in the transactions contributes to a great extent\nin finding the degree of similarity between them. This forms the key idea of\nthe proposed similarity measure. The main objective of the research is to first\ndesign the efficient similarity measure which essentially considers the\ndistribution of the items in the item set over the entire transaction data set\nand also considers the commonality of items present in the transactions, which\nis the major drawback in the Jaccard, Cosine, Euclidean similarity measures. We\nthen carry out the analysis for worst case, the average case and best case\nsituations. The Similarity measure designed is Gaussian based and preserves the\nproperties of Gaussian function. The proposed similarity measure may be used to\nboth cluster and classify the user transactions and predict the user behaviors.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 18:42:55 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Raj", "M. S. B. Phridvi", ""], ["Radhakrishna", "Vangipuram", ""], ["Rao", "C. V. Guru", ""]]}, {"id": "1604.06412", "submitter": "Paolo Missier", "authors": "Paolo Missier and Jacek Cala and Eldarina Wijaya", "title": "The data, they are a-changin'", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of deriving actionable knowledge from large datasets has been\ndecreasing thanks to a convergence of positive factors: low cost data\ngeneration, inexpensively scalable storage and processing infrastructure\n(cloud), software frameworks and tools for massively distributed data\nprocessing, and parallelisable data analytics algorithms. One observation that\nis often overlooked, however, is that each of these elements is not immutable,\nrather they all evolve over time. This suggests that the value of such\nderivative knowledge may decay over time, unless it is preserved by reacting to\nthose changes. Our broad research goal is to develop models, methods, and tools\nfor selectively reacting to changes by balancing costs and benefits, i.e.\nthrough complete or partial re-computation of some of the underlying processes.\nIn this paper we present an initial model for reasoning about change and\nre-computations, and show how analysis of detailed provenance of derived\nknowledge informs re-computation decisions. We illustrate the main ideas\nthrough a real-world case study in genomics, namely on the interpretation of\nhuman variants in support of genetic diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 18:40:20 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Missier", "Paolo", ""], ["Cala", "Jacek", ""], ["Wijaya", "Eldarina", ""]]}, {"id": "1604.06770", "submitter": "Mostafa Milani", "authors": "Mostafa Milani, Andrea Cali, Leopoldo Bertossi", "title": "A Hybrid Approach to Query Answering under Expressive Datalog+/-", "comments": "Extended version of RR'16 paper, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datalog+/- is a family of ontology languages that combine good computational\nproperties with high expressive power. Datalog+/- languages are provably able\nto capture the most relevant Semantic Web languages. In this paper we consider\nthe class of weakly-sticky (WS) Datalog+/- programs, which allow for certain\nuseful forms of joins in rule bodies as well as extending the well-known class\nof weakly-acyclic TGDs. So far, only non-deterministic algorithms were known\nfor answering queries on WS Datalog+/- programs. We present novel deterministic\nquery answering algorithms under WS Datalog+/-. In particular, we propose: (1)\na bottom-up grounding algorithm based on a query-driven chase, and (2) a hybrid\napproach based on transforming a WS program into a so-called sticky one, for\nwhich query rewriting techniques are known. We discuss how our algorithms can\nbe optimized and effectively applied for query answering in real-world\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 18:46:10 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 18:22:01 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Milani", "Mostafa", ""], ["Cali", "Andrea", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1604.07180", "submitter": "Steffen Staab", "authors": "Steffen Staab and Sophie Stalla-Bourdillon and Laura Carmichael", "title": "Observing and Recommending from a Social Web with Biases", "comments": "Technical Report, University of Southampton, March 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research question this report addresses is: how, and to what extent,\nthose directly involved with the design, development and employment of a\nspecific black box algorithm can be certain that it is not unlawfully\ndiscriminating (directly and/or indirectly) against particular persons with\nprotected characteristics (e.g. gender, race and ethnicity)?\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 09:39:57 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Staab", "Steffen", ""], ["Stalla-Bourdillon", "Sophie", ""], ["Carmichael", "Laura", ""]]}, {"id": "1604.07202", "submitter": "Mathura Bai Baikadolla", "authors": "B.Mathura Bai, N.Mangathayaru, B.Padmaja Rani", "title": "An Approach to Find Missing Values in Medical Datasets", "comments": "7 pages,ACM Digital Library, ICEMIS September 2015", "journal-ref": null, "doi": "10.1145/2832987.2833083", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining medical datasets is a challenging problem before data mining\nresearchers as these datasets have several hidden challenges compared to\nconventional datasets.Starting from the collection of samples through field\nexperiments and clinical trials to performing classification,there are numerous\nchallenges at every stage in the mining process. The preprocessing phase in the\nmining process itself is a challenging issue when, we work on medical datasets.\nOne of the prime challenges in mining medical datasets is handling missing\nvalues which is part of preprocessing phase. In this paper, we address the\nissue of handling missing values in medical dataset consisting of categorical\nattribute values. The main contribution of this research is to use the proposed\nimputation measure to estimate and fix the missing values. We discuss a case\nstudy to demonstrate the working of proposed measure.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 11:16:26 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Bai", "B. Mathura", ""], ["Mangathayaru", "N.", ""], ["Rani", "B. Padmaja", ""]]}, {"id": "1604.07371", "submitter": "Srikanth Kandula", "authors": "Robert Grandl, Srikanth Kandula, Sriram Rao, Aditya Akella and\n  Janardhan Kulkarni", "title": "Do the Hard Stuff First: Scheduling Dependent Computations in\n  Data-Analytics Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": "MSR-TR-2016-19", "categories": "cs.DC cs.DB cs.OS cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scheduler that improves cluster utilization and job completion\ntimes by packing tasks having multi-resource requirements and\ninter-dependencies. While the problem is algorithmically very hard, we achieve\nnear-optimality on the job DAGs that appear in production clusters at a large\nenterprise and in benchmarks such as TPC-DS. A key insight is that carefully\nhandling the long-running tasks and those with tough-to-pack resource needs\nwill produce good-enough schedules. However, which subset of tasks to treat\ncarefully is not clear (and intractable to discover). Hence, we offer a search\nprocedure that evaluates various possibilities and outputs a preferred schedule\norder over tasks. An online component enforces the schedule orders desired by\nthe various jobs running on the cluster. In addition, it packs tasks, overbooks\nthe fungible resources and guarantees bounded unfairness for a variety of\ndesirable fairness schemes. Relative to the state-of-the art schedulers, we\nspeed up 50% of the jobs by over 30% each.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 19:20:18 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Grandl", "Robert", ""], ["Kandula", "Srikanth", ""], ["Rao", "Sriram", ""], ["Akella", "Aditya", ""], ["Kulkarni", "Janardhan", ""]]}, {"id": "1604.07939", "submitter": "Andre Araujo", "authors": "Andre Araujo, Jason Chaves, Haricharan Lakshman, Roland Angst, Bernd\n  Girod", "title": "Large-Scale Query-by-Image Video Retrieval Using Bloom Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of using image queries to retrieve videos from a\ndatabase. Our focus is on large-scale applications, where it is infeasible to\nindex each database video frame independently. Our main contribution is a\nframework based on Bloom filters, which can be used to index long video\nsegments, enabling efficient image-to-video comparisons. Using this framework,\nwe investigate several retrieval architectures, by considering different types\nof aggregation and different functions to encode visual information -- these\nplay a crucial role in achieving high performance. Extensive experiments show\nthat the proposed technique improves mean average precision by 24% on a public\ndataset, while being 4X faster, compared to the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 05:46:52 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 17:58:16 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Araujo", "Andre", ""], ["Chaves", "Jason", ""], ["Lakshman", "Haricharan", ""], ["Angst", "Roland", ""], ["Girod", "Bernd", ""]]}, {"id": "1604.08377", "submitter": "Fariz Darari", "authors": "Fariz Darari, Simon Razniewski, Radityo Eko Prasojo, Werner Nutt", "title": "Enabling Fine-grained RDF Data Completeness Assessment", "comments": "This is a preprint version of a paper published in the Proceedings of\n  the 16th International Conference on Web Engineering (ICWE 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, more and more RDF data is becoming available on the Semantic Web.\nWhile the Semantic Web is generally incomplete by nature, on certain topics, it\nalready contains complete information and thus, queries may return all answers\nthat exist in reality. In this paper we develop a technique to check query\ncompleteness based on RDF data annotated with completeness information, taking\ninto account data-specific inferences that lead to an inference problem which\nis $\\Pi^P_2$-complete. We then identify a practically relevant fragment of\ncompleteness information, suitable for crowdsourced, entity-centric RDF data\nsources such as Wikidata, for which we develop an indexing technique that\nallows to scale completeness reasoning to Wikidata-scale data sources. We\nverify the applicability of our framework using Wikidata and develop COOL-WD, a\ncompleteness tool for Wikidata, used to annotate Wikidata with completeness\nstatements and reason about the completeness of query answers over Wikidata.\nThe tool is available at http://cool-wd.inf.unibz.it/.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 11:24:49 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Darari", "Fariz", ""], ["Razniewski", "Simon", ""], ["Prasojo", "Radityo Eko", ""], ["Nutt", "Werner", ""]]}, {"id": "1604.08407", "submitter": "Wenqiang Liu", "authors": "Wenqiang Liu, Jun Liu, Haimeng Duan, Xie He, Bifan Wei", "title": "Exploiting Source-Object Network to Resolve Object Conflicts in Linked\n  Data", "comments": "This paper had been accepted by ESWC2017 Research Tracks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable effort has been made to increase the scale of Linked Data.\nHowever, an inevitable problem when dealing with data integration from multiple\nsources is that multiple different sources often provide conflicting objects\nfor a certain predicate of the same real-world entity, so-called object\nconflicts problem. Currently, the object conflicts problem has not received\nsufficient attention in the Linked Data community. In this paper, we first\nformalize the object conflicts resolution problem as computing the joint\ndistribution of variables on a heterogeneous information network called the\nSource-Object Network, which successfully captures the all correlations from\nobjects and Linked Data sources. Then, we introduce a novel approach based on\nnetwork effects called ObResolution(Object Resolution), to identify a true\nobject from multiple conflicting objects. ObResolution adopts a pairwise Markov\nRandom Field (pMRF) to model all evidences under a unified framework. Extensive\nexperimental results on six real-world datasets show that our method exhibits\nhigher accuracy than existing approaches and it is robust and consistent in\nvarious domains. \\keywords{Linked Data, Object Conflicts, Linked Data Quality,\nTruth Discovery\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 13:22:54 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 21:32:48 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 22:38:10 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Liu", "Wenqiang", ""], ["Liu", "Jun", ""], ["Duan", "Haimeng", ""], ["He", "Xie", ""], ["Wei", "Bifan", ""]]}, {"id": "1604.08568", "submitter": "Alejandro Vaisman Dr.", "authors": "Alexander Campos and Jorge Mozzino and Alejandro Vaisman", "title": "Towards Temporal Graph Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the extensive literature on graph databases (GDBs), temporal GDBs\nhave not received too much attention so far. Temporal GBDs can capture, for\nexample, the evolution of social networks across time, a relevant topic in data\nanalysis nowadays. In this paper we propose a data model and query language\n(denoted TEG-QL) for temporal GDBs, based on the notion of attribute graphs.\nThis allows a straightforward translation to Neo4J, a well-known GBD. We\npresent extensive examples of the use of TEG-QL, and comment our\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 19:34:43 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 13:13:22 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Campos", "Alexander", ""], ["Mozzino", "Jorge", ""], ["Vaisman", "Alejandro", ""]]}, {"id": "1604.08816", "submitter": "Massimiliano Zanin", "authors": "M. Zanin, D. Papo, P. A. Sousa, E. Menasalvas, A. Nicchi, E. Kubik, S.\n  Boccaletti", "title": "Combining complex networks and data mining: why and how", "comments": "58 pages, 19 figures", "journal-ref": null, "doi": "10.1016/j.physrep.2016.04.005", "report-no": null, "categories": "physics.soc-ph cs.DB cs.IR cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing power of computer technology does not dispense with the need\nto extract meaningful in- formation out of data sets of ever growing size, and\nindeed typically exacerbates the complexity of this task. To tackle this\ngeneral problem, two methods have emerged, at chronologically different times,\nthat are now commonly used in the scientific community: data mining and complex\nnetwork theory. Not only do complex network analysis and data mining share the\nsame general goal, that of extracting information from complex systems to\nultimately create a new compact quantifiable representation, but they also\noften address similar problems too. In the face of that, a surprisingly low\nnumber of researchers turn out to resort to both methodologies. One may then be\ntempted to conclude that these two fields are either largely redundant or\ntotally antithetic. The starting point of this review is that this state of\naffairs should be put down to contingent rather than conceptual differences,\nand that these two fields can in fact advantageously be used in a synergistic\nmanner. An overview of both fields is first provided, some fundamental concepts\nof which are illustrated. A variety of contexts in which complex network theory\nand data mining have been used in a synergistic manner are then presented.\nContexts in which the appropriate integration of complex network metrics can\nlead to improved classification rates with respect to classical data mining\nalgorithms and, conversely, contexts in which data mining can be used to tackle\nimportant issues in complex network theory applications are illustrated.\nFinally, ways to achieve a tighter integration between complex networks and\ndata mining, and open lines of research are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 13:06:32 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 11:12:05 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Zanin", "M.", ""], ["Papo", "D.", ""], ["Sousa", "P. A.", ""], ["Menasalvas", "E.", ""], ["Nicchi", "A.", ""], ["Kubik", "E.", ""], ["Boccaletti", "S.", ""]]}, {"id": "1604.08903", "submitter": "Olivier Cur\\'e", "authors": "Hubert Naacke and Olivier Cur\\'e and Bernd Amann", "title": "SPARQL query processing with Apache Spark", "comments": "13 pages (ACM 2 columns format), 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of linked data sources and the size of the linked open data graph\nkeep growing every day. As a consequence, semantic RDF services are more and\nmore confronted to various \"big data\" problems. Query processing is one of them\nand needs to be efficiently addressed with executions over scalable, highly\navailable and fault tolerant frameworks. Data management systems requiring\nthese properties are rarely built from scratch but are rather designed on top\nof an existing cluster computing engine. In this work, we consider the\nprocessing of SPARQL queries with Apache Spark. We propose and compare five\ndifferent query processing approaches based on different join execution models\nand Spark components. A detailed experimentation, on real-world and synthetic\ndata sets, emphasizes that two approaches tailored for the RDF data model\noutperform the other ones on all major query shapes, i.e., star, snowflake,\nchain and hybrid.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 16:40:56 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 11:26:13 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Naacke", "Hubert", ""], ["Cur\u00e9", "Olivier", ""], ["Amann", "Bernd", ""]]}]