[{"id": "1105.0049", "submitter": "Magdalini Eirinaki", "authors": "Anup Patel, Niveeta Sharma, Magdalini Eirinaki", "title": "Negative Database for Data Security", "comments": "appeared in Proceedings of the International Conference on Computing\n  in Engineering, Science and Information (ICC 2009), April 2009, Fullerton,\n  California", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Security is a major issue in any web-based application. There have been\napproaches to handle intruders in any system, however, these approaches are not\nfully trustable; evidently data is not totally protected. Real world databases\nhave information that needs to be securely stored. The approach of generating\nnegative database could help solve such problem. A Negative Database can be\ndefined as a database that contains huge amount of data consisting of\ncounterfeit data along with the real data. Intruders may be able to get access\nto such databases, but, as they try to extract information, they will retrieve\ndata sets that would include both the actual and the negative data. In this\npaper we present our approach towards implementing the concept of negative\ndatabase to help prevent data theft from malicious users and provide efficient\ndata retrieval for all valid users.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2011 05:19:42 GMT"}], "update_date": "2011-05-03", "authors_parsed": [["Patel", "Anup", ""], ["Sharma", "Niveeta", ""], ["Eirinaki", "Magdalini", ""]]}, {"id": "1105.0350", "submitter": "Ramya  C", "authors": "C. Ramya, G. Kavitha and Dr. K. S. Shreedhara", "title": "Preprocessing: A Prerequisite for Discovering Patterns in Web Usage\n  Mining Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web log data is usually diverse and voluminous. This data must be assembled\ninto a consistent, integrated and comprehensive view, in order to be used for\npattern discovery. Without properly cleaning, transforming and structuring the\ndata prior to the analysis, one cannot expect to find meaningful patterns. As\nin most data mining applications, data preprocessing involves removing and\nfiltering redundant and irrelevant data, removing noise, transforming and\nresolving any inconsistencies. In this paper, a complete preprocessing\nmethodology having merging, data cleaning, user/session identification and data\nformatting and summarization activities to improve the quality of data by\nreducing the quantity of data has been proposed. To validate the efficiency of\nthe proposed preprocessing methodology, several experiments are conducted and\nthe results show that the proposed methodology reduces the size of Web access\nlog files down to 73-82% of the initial size and offers richer logs that are\nstructured for further stages of Web Usage Mining (WUM). So preprocessing of\nraw data in this WUM process is the central theme of this paper.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2011 15:07:34 GMT"}], "update_date": "2011-05-03", "authors_parsed": [["Ramya", "C.", ""], ["Kavitha", "G.", ""], ["Shreedhara", "Dr. K. S.", ""]]}, {"id": "1105.1364", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Lechen Li", "title": "Achieving Data Privacy through Secrecy Views and Null-Based Virtual\n  Updates", "comments": "Minor revisions of journal resubmission, 2012", "journal-ref": "IEEE Transaction on Knowledge and Data Engineering, 2013,\n  25(5):987-1000", "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There may be sensitive information in a relational database, and we might\nwant to keep it hidden from a user or group thereof. In this work, sensitive\ndata is characterized as the contents of a set of secrecy views. For a user\nwithout permission to access that sensitive data, the database instance he\nqueries is updated to make the contents of the views empty or contain only\ntuples with null values. In particular, if this user poses a query about any of\nthese views, no meaningful information is returned. Since the database is not\nexpected to be physically changed to produce this result, the updates are only\nvirtual. And also minimal in a precise way. These minimal updates are reflected\nin the secrecy view contents, and also in the fact that query answers, while\nbeing privacy preserving, are also maximally informative. Virtual updates are\nbased on the use of null values as used in the SQL standard. We provide the\nsemantics of secrecy views and the virtual updates. The different ways in which\nthe underlying database is virtually updated are specified as the models of a\nlogic program with stable model semantics. The program becomes the basis for\nthe computation of the \"secret answers\" to queries, i.e. those that do not\nreveal the sensitive information.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2011 19:34:23 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2011 02:53:11 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2012 18:33:40 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Li", "Lechen", ""]]}, {"id": "1105.1930", "submitter": "Fabian Suchanek", "authors": "Anisoara Nica, Fabian Suchanek (INRIA Saclay - Ile de France), Aparna\n  Varde", "title": "Emerging multidisciplinary research across database management systems", "comments": null, "journal-ref": "SIGMOD REcords (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The database community is exploring more and more multidisciplinary avenues:\nData semantics overlaps with ontology management; reasoning tasks venture into\nthe domain of artificial intelligence; and data stream management and\ninformation retrieval shake hands, e.g., when processing Web click-streams.\nThese new research avenues become evident, for example, in the topics that\ndoctoral students choose for their dissertations. This paper surveys the\nemerging multidisciplinary research by doctoral students in database systems\nand related areas. It is based on the PIKM 2010, which is the 3rd Ph.D.\nworkshop at the International Conference on Information and Knowledge\nManagement (CIKM). The topics addressed include ontology development, data\nstreams, natural language processing, medical databases, green energy, cloud\ncomputing, and exploratory search. In addition to core ideas from the workshop,\nwe list some open research questions in these multidisciplinary areas.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2011 12:36:17 GMT"}], "update_date": "2011-05-11", "authors_parsed": [["Nica", "Anisoara", "", "INRIA Saclay - Ile de France"], ["Suchanek", "Fabian", "", "INRIA Saclay - Ile de France"], ["Varde", "Aparna", ""]]}, {"id": "1105.1950", "submitter": "Reza Keyvan", "authors": "Mahnoosh Kholghi (Department of Electronic, Computer and IT, Islamic\n  Azad University, Qazvin Branch, Qazvin, Iran and member of Young Researchers\n  Club) and Mohammadreza Keyvanpour (Department of Computer Engineering Alzahra\n  University Tehran, Iran)", "title": "An analytical framework for data stream mining techniques based on\n  challenges and requirements", "comments": null, "journal-ref": "International Journal of Engineering Science and Technology\n  (IJEST)Vol. 3 No. 3 Mar 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of applications that generate massive streams of data need\nintelligent data processing and online analysis. Real-time surveillance\nsystems, telecommunication systems, sensor networks and other dynamic\nenvironments are such examples. The imminent need for turning such data into\nuseful information and knowledge augments the development of systems,\nalgorithms and frameworks that address streaming challenges. The storage,\nquerying and mining of such data sets are highly computationally challenging\ntasks. Mining data streams is concerned with extracting knowledge structures\nrepresented in models and patterns in non stopping streams of information.\nGenerally, two main challenges are designing fast mining methods for data\nstreams and need to promptly detect changing concepts and data distribution\nbecause of highly dynamic nature of data streams. The goal of this article is\nto analyze and classify the application of diverse data mining techniques in\ndifferent challenges of data stream mining. In this paper, we present the\ntheoretical foundations of data stream analysis and propose an analytical\nframework for data stream mining techniques.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2011 13:58:10 GMT"}], "update_date": "2011-05-11", "authors_parsed": [["Kholghi", "Mahnoosh", "", "Department of Electronic, Computer and IT, Islamic\n  Azad University, Qazvin Branch, Qazvin, Iran and member of Young Researchers\n  Club"], ["Keyvanpour", "Mohammadreza", "", "Department of Computer Engineering Alzahra\n  University Tehran, Iran"]]}, {"id": "1105.2255", "submitter": "Yael Amsterdamer", "authors": "Yael Amsterdamer, Daniel Deutch, Val Tannen", "title": "On the Limitations of Provenance for Queries With Difference", "comments": "TAPP 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The annotation of the results of database transformations was shown to be\nvery effective for various applications. Until recently, most works in this\ncontext focused on positive query languages. The provenance semirings is a\nparticular approach that was proven effective for these languages, and it was\nshown that when propagating provenance with semirings, the expected equivalence\naxioms of the corresponding query languages are satisfied. There have been\nseveral attempts to extend the framework to account for relational algebra\nqueries with difference. We show here that these suggestions fail to satisfy\nsome expected equivalence axioms (that in particular hold for queries on\n\"standard\" set and bag databases). Interestingly, we show that this is not a\npitfall of these particular attempts, but rather every such attempt is bound to\nfail in satisfying these axioms, for some semirings. Finally, we show\nparticular semirings for which an extension for supporting difference is\n(im)possible.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2011 17:22:32 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Amsterdamer", "Yael", ""], ["Deutch", "Daniel", ""], ["Tannen", "Val", ""]]}, {"id": "1105.2264", "submitter": "Artem Chebotko", "authors": "Craig Franke, Samuel Morin, Artem Chebotko, John Abraham, Pearl\n  Brazier", "title": "Distributed Semantic Web Data Management in HBase and MySQL Cluster", "comments": "In Proc. of the 4th IEEE International Conference on Cloud Computing\n  (CLOUD'11)", "journal-ref": null, "doi": "10.1109/CLOUD.2011.19", "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various computing and data resources on the Web are being enhanced with\nmachine-interpretable semantic descriptions to facilitate better search,\ndiscovery and integration. This interconnected metadata constitutes the\nSemantic Web, whose volume can potentially grow the scale of the Web. Efficient\nmanagement of Semantic Web data, expressed using the W3C's Resource Description\nFramework (RDF), is crucial for supporting new data-intensive,\nsemantics-enabled applications. In this work, we study and compare two\napproaches to distributed RDF data management based on emerging cloud computing\ntechnologies and traditional relational database clustering technologies. In\nparticular, we design distributed RDF data storage and querying schemes for\nHBase and MySQL Cluster and conduct an empirical comparison of these approaches\non a cluster of commodity machines using datasets and queries from the Third\nProvenance Challenge and Lehigh University Benchmark. Our study reveals\ninteresting patterns in query evaluation, shows that our algorithms are\npromising, and suggests that cloud computing has a great potential for scalable\nSemantic Web data management.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2011 17:46:15 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Franke", "Craig", ""], ["Morin", "Samuel", ""], ["Chebotko", "Artem", ""], ["Abraham", "John", ""], ["Brazier", "Pearl", ""]]}, {"id": "1105.2813", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Dan Suciu", "title": "Optimal Upper and Lower Bounds for Boolean Expressions by Dissociation", "comments": "7 pages, 2 figures; for details see the project page:\n  http://LaPushDB.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops upper and lower bounds for the probability of Boolean\nexpressions by treating multiple occurrences of variables as independent and\nassigning them new individual probabilities. Our technique generalizes and\nextends the underlying idea of a number of recent approaches which are\nvaryingly called node splitting, variable renaming, variable splitting, or\ndissociation for probabilistic databases. We prove that the probabilities we\nassign to new variables are the best possible in some sense.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2011 19:41:45 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Suciu", "Dan", ""]]}, {"id": "1105.4004", "submitter": "Miguel A. Martinez-Prieto", "authors": "Sandra \\'Alvarez-Garc\\'ia and Nieves R. Brisaboa and Javier D.\n  Fern\\'andez and Miguel A. Mart\\'inez-Prieto", "title": "Compressed k2-Triples for Full-In-Memory RDF Engines", "comments": "In Proc. of AMCIS'2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Current \"data deluge\" has flooded the Web of Data with very large RDF\ndatasets. They are hosted and queried through SPARQL endpoints which act as\nnodes of a semantic net built on the principles of the Linked Data project.\nAlthough this is a realistic philosophy for global data publishing, its query\nperformance is diminished when the RDF engines (behind the endpoints) manage\nthese huge datasets. Their indexes cannot be fully loaded in main memory, hence\nthese systems need to perform slow disk accesses to solve SPARQL queries. This\npaper addresses this problem by a compact indexed RDF structure (called\nk2-triples) applying compact k2-tree structures to the well-known\nvertical-partitioning technique. It obtains an ultra-compressed representation\nof large RDF graphs and allows SPARQL queries to be full-in-memory performed\nwithout decompression. We show that k2-triples clearly outperforms\nstate-of-the-art compressibility and traditional vertical-partitioning query\nresolution, remaining very competitive with multi-index solutions.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2011 02:11:20 GMT"}], "update_date": "2011-05-23", "authors_parsed": [["\u00c1lvarez-Garc\u00eda", "Sandra", ""], ["Brisaboa", "Nieves R.", ""], ["Fern\u00e1ndez", "Javier D.", ""], ["Mart\u00ednez-Prieto", "Miguel A.", ""]]}, {"id": "1105.4251", "submitter": "Hoa Nguyen", "authors": "Hoa Nguyen (University of Utah), Ariel Fuxman (Microsoft Research),\n  Stelios Paparizos (Microsoft Research), Juliana Freire (University of Utah),\n  Rakesh Agrawal (Microsoft Research)", "title": "Synthesizing Products for Online Catalogs", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 7, pp.\n  409-418 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A high-quality, comprehensive product catalog is essential to the success of\nProduct Search engines and shopping sites such as Yahoo! Shopping, Google\nProduct Search or Bing Shopping. But keeping catalogs up-to-date becomes a\nchallenging task, calling for the need of automated techniques. In this paper,\nwe introduce the problem of product synthesis, a key component of catalog\ncreation and maintenance. Given a set of offers advertised by merchants, the\ngoal is to identify new products and add them to the catalog together with\ntheir (structured) attributes. A fundamental challenge is the scale of the\nproblem: a Product Search engine receives data from thousands of merchants and\nmillions of products; the product taxonomy contains thousands of categories,\nwhere each category comes in a different schema; and merchants use\nrepresentations for products that are different from the ones used in the\ncatalog of the Product Search engine.\n  We propose a system that provides an end-to-end solution to the product\nsynthesis problem, and includes components for extraction, and addresses issues\ninvolved in data extraction from offers, schema reconciliation, and data\nfusion. We developed a novel and scalable technique for schema matching which\nleverages knowledge about previously-known instance-level associations between\noffers and products; and it is trained using automatically created training\nsets (no manually-labeled data is needed). We present an experimental\nevaluation of our system using data from Bing Shopping for more than 800K\noffers, a thousand merchants, and 400 categories. The evaluation confirms that\nour approach is able to automatically generate a large number of accurate\nproduct specifications, and that our schema reconciliation component\noutperforms state-of-the-art schema matching techniques in terms of precision\nand recall.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2011 12:06:39 GMT"}], "update_date": "2011-05-24", "authors_parsed": [["Nguyen", "Hoa", "", "University of Utah"], ["Fuxman", "Ariel", "", "Microsoft Research"], ["Paparizos", "Stelios", "", "Microsoft Research"], ["Freire", "Juliana", "", "University of Utah"], ["Agrawal", "Rakesh", "", "Microsoft Research"]]}, {"id": "1105.4252", "submitter": "Sandeep Tata", "authors": "Avrilia Floratou (University of Wisconsin-Madison), Jignesh Patel\n  (University of Wisconsin-Madison), Eugene Shekita (IBM Research), Sandeep\n  Tata (IBM Research)", "title": "Column-Oriented Storage Techniques for MapReduce", "comments": "VLDB2011", "journal-ref": null, "doi": null, "report-no": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 7, pp.\n  419-429 (2011)", "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of MapReduce often run into performance problems when they scale up\ntheir workloads. Many of the problems they encounter can be overcome by\napplying techniques learned from over three decades of research on parallel\nDBMSs. However, translating these techniques to a MapReduce implementation such\nas Hadoop presents unique challenges that can lead to new design choices. This\npaper describes how column-oriented storage techniques can be incorporated in\nHadoop in a way that preserves its popular programming APIs.\n  We show that simply using binary storage formats in Hadoop can provide a 3x\nperformance boost over the naive use of text files. We then introduce a\ncolumn-oriented storage format that is compatible with the replication and\nscheduling constraints of Hadoop and show that it can speed up MapReduce jobs\non real workloads by an order of magnitude. We also show that dealing with\ncomplex column types such as arrays, maps, and nested records, which are common\nin MapReduce jobs, can incur significant CPU overhead. Finally, we introduce a\nnovel skip list column format and lazy record construction strategy that avoids\ndeserializing unwanted records to provide an additional 1.5x performance boost.\nExperiments on a real intranet crawl are used to show that our column-oriented\nstorage techniques can improve the performance of the map phase in Hadoop by as\nmuch as two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2011 12:07:37 GMT"}], "update_date": "2011-05-24", "authors_parsed": [["Floratou", "Avrilia", "", "University of Wisconsin-Madison"], ["Patel", "Jignesh", "", "University of Wisconsin-Madison"], ["Shekita", "Eugene", "", "IBM Research"], ["Tata", "Sandeep", "", "IBM Research"]]}, {"id": "1105.4253", "submitter": "David Lomet", "authors": "David Lomet (Microsoft Research, USA), Kostas Tzoumas (Aalborg\n  University, Denmark), Michael Zwilling (Microsoft)", "title": "Implementing Performance Competitive Logical Recovery", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 7, pp.\n  430-439 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New hardware platforms, e.g. cloud, multi-core, etc., have led to a\nreconsideration of database system architecture. Our Deuteronomy project\nseparates transactional functionality from data management functionality,\nenabling a flexible response to exploiting new platforms. This separation\nrequires, however, that recovery is described logically. In this paper, we\nextend current recovery methods to work in this logical setting. While this is\nstraightforward in principle, performance is an issue. We show how ARIES style\nrecovery optimizations can work for logical recovery where page information is\nnot captured on the log. In side-by-side performance experiments using a common\nlog, we compare logical recovery with a state-of-the art ARIES style recovery\nimplementation and show that logical redo performance can be competitive.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2011 12:08:23 GMT"}], "update_date": "2011-05-24", "authors_parsed": [["Lomet", "David", "", "Microsoft Research, USA"], ["Tzoumas", "Kostas", "", "Aalborg\n  University, Denmark"], ["Zwilling", "Michael", "", "Microsoft"]]}, {"id": "1105.4254", "submitter": "Ashwin Machanavajjhala", "authors": "Ashwin Machanavajjhala (Yahoo! Research), Aleksandra Korolova\n  (Stanford University), Atish Das Sarma (Google)", "title": "Personalized Social Recommendations - Accurate or Private?", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 7, pp.\n  440-450 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent surge of social networks like Facebook, new forms of\nrecommendations have become possible - personalized recommendations of ads,\ncontent, and even new friend and product connections based on one's social\ninteractions. Since recommendations may use sensitive social information, it is\nspeculated that these recommendations are associated with privacy risks. The\nmain contribution of this work is in formalizing these expected trade-offs\nbetween the accuracy and privacy of personalized social recommendations.\n  In this paper, we study whether \"social recommendations\", or recommendations\nthat are solely based on a user's social network, can be made without\ndisclosing sensitive links in the social graph. More precisely, we quantify the\nloss in utility when existing recommendation algorithms are modified to satisfy\na strong notion of privacy, called differential privacy. We prove lower bounds\non the minimum loss in utility for any recommendation algorithm that is\ndifferentially private. We adapt two privacy preserving algorithms from the\ndifferential privacy literature to the problem of social recommendations, and\nanalyze their performance in comparison to the lower bounds, both analytically\nand experimentally. We show that good private social recommendations are\nfeasible only for a small subset of the users in the social network or for a\nlenient setting of privacy parameters.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2011 12:09:04 GMT"}], "update_date": "2011-05-24", "authors_parsed": [["Machanavajjhala", "Ashwin", "", "Yahoo! Research"], ["Korolova", "Aleksandra", "", "Stanford University"], ["Sarma", "Atish Das", "", "Google"]]}, {"id": "1105.4395", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Alexandra Meliou, Dan Suciu", "title": "Default-all is dangerous!", "comments": "4 pages, 6 figures, preprint of paper appearing in Proceedings of\n  TaPP '11 (3rd USENIX Workshop on the Theory and Practice of Provenance); for\n  details see the project page: http://db.cs.washington.edu/causality/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the default-all propagation scheme for database annotations is\ndangerous. Dangerous here means that it can propagate annotations to the query\noutput which are semantically irrelevant to the query the user asked. This is\nthe result of considering all relationally equivalent queries and returning the\nunion of their where-provenance in an attempt to define a propagation scheme\nthat is insensitive to query rewriting. We propose an alternative\nquery-rewrite-insensitive (QRI) where-provenance called minimum propagation. It\nis analogous to the minimum witness basis for why-provenance, straight-forward\nto evaluate, and returns all relevant and only relevant annotations.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2011 03:19:25 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Meliou", "Alexandra", ""], ["Suciu", "Dan", ""]]}, {"id": "1105.4452", "submitter": "Christian von der Weth", "authors": "Christian von der Weth, Anwitaman Datta", "title": "GutenTag: A Multi-Term Caching Optimized Tag Query Processor for\n  Key-Value Based NoSQL Storage Systems", "comments": "22 pages, 21 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NoSQL systems are more and more deployed as back-end infrastructure for\nlarge-scale distributed online platforms like Google, Amazon or Facebook. Their\napplicability results from the fact that most services of online platforms\naccess the stored data objects via their primary key. However, NoSQL systems do\nnot efficiently support services referring more than one data object, e.g. the\nterm-based search for data objects. To address this issue we propose our\narchitecture based on an inverted index on top of a NoSQL system. For queries\ncomprising more than one term, distributed indices yield a limited performance\nin large distributed systems. We propose two extensions to cope with this\nchallenge. Firstly, we store index entries not only for single term but also\nfor a selected set of term combinations depending on their popularity derived\nfrom a query history. Secondly, we additionally cache popular keys on gateway\nnodes, which are a common concept in real-world systems, acting as interface\nfor services when accessing data objects in the back end. Our results show that\nwe can significantly reduces the bandwidth consumption for processing queries,\nwith an acceptable, marginal increase in the load of the gateway nodes.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2011 09:53:02 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["von der Weth", "Christian", ""], ["Datta", "Anwitaman", ""]]}, {"id": "1105.4702", "submitter": "Joachim Selke", "authors": "Joachim Selke and Wolf-Tilo Balke", "title": "Exploiting Conceptual Knowledge for Querying Information Systems", "comments": "International Conference on Philosophy's Relevance in Information\n  Science (PRIS), Paderborn, Germany, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas today's information systems are well-equipped for efficient query\nhandling, their strict mathematical foundations hamper their use for everyday\ntasks. In daily life, people expect information to be offered in a personalized\nand focused way. But currently, personalization in digital systems still only\ntakes explicit knowledge into account and does not yet process conceptual\ninformation often naturally implied by users. We discuss how to bridge the gap\nbetween users and today's systems, building on results from cognitive\npsychology.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2011 08:01:15 GMT"}], "update_date": "2011-05-25", "authors_parsed": [["Selke", "Joachim", ""], ["Balke", "Wolf-Tilo", ""]]}, {"id": "1105.5575", "submitter": "Amani Tahat", "authors": "Amani Tahat and Wa'el Salah", "title": "Comprehensive online Atomic Database Management System (DBMS) with\n  Highly Qualified Computing Capabilities", "comments": "20 pages,4 figures, 5 tables, 2 appendixes; International Journal of\n  Database Management Systems (IJDMS), Vol.3, No.2, May 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.atom-ph astro-ph.CO astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intensive need of atomic data is expanding continuously in a wide variety\nof applications (e.g. fusion energy and astrophysics, laser-produced, plasma\nresearches, and plasma processing).This paper will introduce our ongoing\nresearch work to build a comprehensive, complete, up-to-date, user friendly and\nonline atomic Database Management System (DBMS) namely called AIMS by using\nSQLite (http://www.sqlite.org/about.html)(8). Programming language tools and\ntechniques will not be covered here. The system allows the generation of\nvarious atomic data based on professional online atomic calculators. The\nongoing work is a step forward to bring detailed atomic model accessible to a\nwide community of laboratory and astrophysical plasma diagnostics. AIMS is a\nprofessional worldwide tool for supporting several educational purposes and can\nbe considered as a complementary database of IAEA atomic databases. Moreover,\nit will be an exceptional strategy of incorporating the output data of several\natomic codes to external spectral models.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 14:45:38 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Tahat", "Amani", ""], ["Salah", "Wa'el", ""]]}, {"id": "1105.5951", "submitter": "Muhammad Shahzad muhammad shahzad", "authors": "Muhammad Tayyab Shahzad, Muhammad Rizwan", "title": "Performance of Short-Commit in Extreme Database Environment", "comments": "19 pages. International Journal of Database Management Systems, ISSN\n  : 0975-5705 (Online); International Journal of Database Management Systems\n  (IJDMS)2011, 0975-5985 (Print)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atomic commit protocols are used where data integrity is more important than\ndata availability. Two-Phase commit (2PC) is a standard commit protocol for\ncommercial database management systems. To reduce certain drawbacks in 2PC\nprotocol people have suggested different variance of this protocol.\nShort-Commit protocol is developed with an objective to achieve low cost\ntransaction commitment cost with non-blocking capability. In this paper we have\nbriefly explained short-commit protocol executing pattern. Experimental\nanalysis and results are presented to support the claim that short-commit can\nwork efficiently in extreme database environment.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 11:58:42 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Shahzad", "Muhammad Tayyab", ""], ["Rizwan", "Muhammad", ""]]}, {"id": "1105.6001", "submitter": "Daniel Lemire", "authors": "Antonio Badia and Daniel Lemire", "title": "A Call to Arms: Revisiting Database Design", "comments": "Removed spurious column break. Nothing else was changed", "journal-ref": "Antonio Badia and Daniel Lemire. A call to arms: revisiting\n  database design. SIGMOD Record 40 (3), pages 61-69, 2011", "doi": "10.1145/2070736.2070750", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good database design is crucial to obtain a sound, consistent database, and -\nin turn - good database design methodologies are the best way to achieve the\nright design. These methodologies are taught to most Computer Science\nundergraduates, as part of any Introduction to Database class. They can be\nconsidered part of the \"canon\", and indeed, the overall approach to database\ndesign has been unchanged for years. Moreover, none of the major database\nresearch assessments identify database design as a strategic research\ndirection.\n  Should we conclude that database design is a solved problem?\n  Our thesis is that database design remains a critical unsolved problem.\nHence, it should be the subject of more research. Our starting point is the\nobservation that traditional database design is not used in practice - and if\nit were used it would result in designs that are not well adapted to current\nenvironments. In short, database design has failed to keep up with the times.\nIn this paper, we put forth arguments to support our viewpoint, analyze the\nroot causes of this situation and suggest some avenues of research.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 14:07:10 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2011 18:05:08 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2011 17:48:38 GMT"}, {"version": "v4", "created": "Tue, 2 Oct 2012 16:56:04 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Badia", "Antonio", ""], ["Lemire", "Daniel", ""]]}, {"id": "1105.6118", "submitter": "Amani Tahat", "authors": "Amani Tahat, Maurice HT Ling", "title": "Mapping Relational Operations onto Hypergraph Model", "comments": "21 pages", "journal-ref": "The Python Papers 6(1): 4,2011", "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relational model is the most commonly used data model for storing large\ndatasets, perhaps due to the simplicity of the tabular format which had\nrevolutionized database management systems. However, many real world objects\nare recursive and associative in nature which makes storage in the relational\nmodel difficult. The hypergraph model is a generalization of a graph model,\nwhere each hypernode can be made up of other nodes or graphs and each hyperedge\ncan be made up of one or more edges. It may address the recursive and\nassociative limitations of relational model. However, the hypergraph model is\nnon-tabular; thus, loses the simplicity of the relational model. In this study,\nwe consider the means to convert a relational model into a hypergraph model in\ntwo layers. At the bottom layer, each relational tuple can be considered as a\nstar graph centered where the primary key node is surrounded by non-primary key\nattributes. At the top layer, each tuple is a hypernode, and a relation is a\nset of hypernodes. We presented a reference implementation of relational\noperators (project, rename, select, inner join, natural join, left join, right\njoin, outer join and Cartesian join) on a hypergraph model. Using a simple\nexample, we demonstrate that a relation and relational operators can be\nimplemented on this hypergraph model.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 21:34:51 GMT"}], "update_date": "2011-06-01", "authors_parsed": [["Tahat", "Amani", ""], ["Ling", "Maurice HT", ""]]}]