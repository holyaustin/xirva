[{"id": "2009.00035", "submitter": "Raul Castro Fernandez", "authors": "Raul Castro Fernandez, Kyle Chard, Ben Blaiszik, Sanjay Krishnan,\n  Aaron Elmore, Ziad Obermeyer, Josh Risley, Sendhil Mullainathan, Michael\n  Franklin, Ian Foster", "title": "The Data Station: Combining Data, Compute, and Market Forces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Data Stations, a new data architecture that we are\ndesigning to tackle some of the most challenging data problems that we face\ntoday: access to sensitive data; data discovery and integration; and governance\nand compliance. Data Stations depart from modern data lakes in that both data\nand derived data products, such as machine learning models, are sealed and\ncannot be directly seen, accessed, or downloaded by anyone. Data Stations do\nnot deliver data to users; instead, users bring questions to data. This\ninversion of the usual relationship between data and compute mitigates many of\nthe security risks that are otherwise associated with sharing and working with\nsensitive data.\n  Data Stations are designed following the principle that many data problems\nrequire human involvement, and that incentives are the key to obtaining such\ninvolvement. To that end, Data Stations implement market designs to create,\nmanage, and coordinate the use of incentives. We explain the motivation for\nthis new kind of platform and its design.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 18:04:20 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Fernandez", "Raul Castro", ""], ["Chard", "Kyle", ""], ["Blaiszik", "Ben", ""], ["Krishnan", "Sanjay", ""], ["Elmore", "Aaron", ""], ["Obermeyer", "Ziad", ""], ["Risley", "Josh", ""], ["Mullainathan", "Sendhil", ""], ["Franklin", "Michael", ""], ["Foster", "Ian", ""]]}, {"id": "2009.00061", "submitter": "Hakon Gudbjartsson", "authors": "Sigmar K. Stef\\'ansson and H\\'akon Gu{\\dh}bjartsson", "title": "SparkGOR: A unified framework for genomic data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivation: Our goal was to combine the capabilities of Spark and GOR into a\nsingle computing framework for use in analysis of large scale genome data.\n  Results: We have created a relational query engine that unites SparkSQL and\nGORpipe into a single declarative query framework. This has been achieved by\nallowing embedding of SQL expressions into the high-level relational statement\nsyntax in GOR and by supporting virtual relations and nested GORpipe\nexpressions within SQL. Furthermore, we have built drivers to enable Spark and\nGOR to use and leverage their preferred file formats, Parquet and GORZ\nrespectively, and introduced APIs to allow the use of GOR with Spark\ndataframes.\n  Availability: The SparkGOR version of the GORpipe software is open-source and\nfreely available at https://gorpipe-website.now.sh and\nhttps://github.com/gorpipe.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 18:59:58 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Stef\u00e1nsson", "Sigmar K.", ""], ["Gu\u00f0bjartsson", "H\u00e1kon", ""]]}, {"id": "2009.00099", "submitter": "Behrooz Omidvar-Tehrani", "authors": "Behrooz Omidvar-Tehrani, Sruthi Viswanathan, Jean-Michel Renders", "title": "Interactive and Explainable Point-of-Interest Recommendation using\n  Look-alike Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommending Points-of-Interest (POIs) is surfacing in many location-based\napplications. The literature contains personalized and socialized POI\nrecommendation approaches which employ historical check-ins and social links to\nmake recommendations. However these systems still lack customizability\n(incorporating session-based user interactions with the system) and\ncontextuality (incorporating the situational context of the user), particularly\nin cold start situations, where nearly no user information is available. In\nthis paper, we propose LikeMind, a POI recommendation system which tackles the\nchallenges of cold start, customizability, contextuality, and explainability by\nexploiting look-alike groups mined in public POI datasets. LikeMind\nreformulates the problem of POI recommendation, as recommending explainable\nlook-alike groups (and their POIs) which are in line with user's interests.\nLikeMind frames the task of POI recommendation as an exploratory process where\nusers interact with the system by expressing their favorite POIs, and their\ninteractions impact the way look-alike groups are selected out. Moreover,\nLikeMind employs \"mindsets\", which capture actual situation and intent of the\nuser, and enforce the semantics of POI interestingness. In an extensive set of\nexperiments, we show the quality of our approach in recommending relevant\nlook-alike groups and their POIs, in terms of efficiency and effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 21:05:21 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Omidvar-Tehrani", "Behrooz", ""], ["Viswanathan", "Sruthi", ""], ["Renders", "Jean-Michel", ""]]}, {"id": "2009.00166", "submitter": "Botao Peng", "authors": "Botao Peng, Panagiota Fatourou, Themis Palpanas", "title": "ParIS+: Data Series Indexing on Multi-Core Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data series similarity search is a core operation for several data series\nanalysis applications across many different domains. Nevertheless, even\nstate-of-the-art techniques cannot provide the time performance required for\nlarge data series collections. We propose ParIS and ParIS+, the first\ndisk-based data series indices carefully designed to inherently take advantage\nof multi-core architectures, in order to accelerate similarity search\nprocessing times. Our experiments demonstrate that ParIS+ completely removes\nthe CPU latency during index construction for disk-resident data, and for exact\nquery answering is up to 1 order of magnitude faster than the current state of\nthe art index scan method, and up to 3 orders of magnitude faster than the\noptimized serial scan method. ParIS+ (which is an evolution of the ADS+ index)\nowes its efficiency to the effective use of multi-core and multi-socket\narchitectures, in order to distribute and execute in parallel both index\nconstruction and query answering, and to the exploitation of the Single\nInstruction Multiple Data (SIMD) capabilities of modern CPUs, in order to\nfurther parallelize the execution of instructions inside each core.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 01:25:37 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Peng", "Botao", ""], ["Fatourou", "Panagiota", ""], ["Palpanas", "Themis", ""]]}, {"id": "2009.00361", "submitter": "Ana Sofia Gomes", "authors": "Jo\\~ao Oliveirinha, Ana Sofia Gomes, Pedro Cardoso, Pedro Bizarro", "title": "Railgun: streaming windows for mission critical systems", "comments": "Previously submitted to CIDR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some mission critical systems, such as fraud detection, require accurate,\nreal-time metrics over long time windows on applications that demand high\nthroughputs and low latencies. As these applications need to run \"forever\",\ncope with large and spiky data loads, they further require to be run in a\ndistributed setting. Unsurprisingly, we are unaware of any distributed\nstreaming system that provides all those properties. Instead, existing systems\ntake large simplifications, such as implementing sliding windows as a fixed set\nof partially overlapping windows, jeopardizing metric accuracy (violating\nfinancial regulator rules) or latency (breaching service agreements).\n  In this paper, we propose Railgun, a fault-tolerant, elastic, and distributed\nstreaming system supporting real-time sliding windows for scenarios requiring\nhigh loads and millisecond-level latencies. We benchmarked an initial prototype\nof Railgun using real data, showing significant lower latency than Flink, and\nlow memory usage, independent of window size.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 11:33:19 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 11:00:01 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 10:08:03 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Oliveirinha", "Jo\u00e3o", ""], ["Gomes", "Ana Sofia", ""], ["Cardoso", "Pedro", ""], ["Bizarro", "Pedro", ""]]}, {"id": "2009.00373", "submitter": "Nur Al Hasan Haldar", "authors": "Nur Al Hasan Haldar, Jianxin Li, Mohammed Eunus Ali, Taotao Cai, Timos\n  Sellis, Mark Reynolds", "title": "Top-k Socio-Spatial Co-engaged Location Selection for Social Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of location-based social networks, users can tag their daily\nactivities in different locations through check-ins. These check-in locations\nsignify user preferences for various socio-spatial activities and can be used\nto build their profiles to improve the quality of services in some applications\nsuch as recommendation systems, advertising, and group formation. To support\nsuch applications, in this paper, we formulate a new problem of identifying\ntop-k Socio-Spatial co-engaged Location Selection (SSLS) for users in a social\ngraph, that selects the best set of k locations from a large number of location\ncandidates relating to the user and her friends. The selected locations should\nbe (i) spatially and socially relevant to the user and her friends, and (ii)\ndiversified in both spatially and socially to maximize the coverage of friends\nin the spatial space. This problem has been proved as NP-hard. To address the\nchallenging problem, we first develop a branch-and-bound based Exact solution\nby designing some pruning strategies based on the derived bounds on diversity.\nTo make the solution scalable for large datasets, we also develop an\napproximate solution by deriving the relaxed bounds and advanced termination\nrules to filter out insignificant intermediate results. To further accelerate\nthe efficiency, we present one fast exact approach and a meta-heuristic\napproximate approach by avoiding the repeated computation of diversity at the\nrunning time. Finally, we have performed extensive experiments to evaluate the\nperformance of our proposed models and algorithms against the adapted existing\nmethods using four real-world large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 12:05:37 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 11:37:26 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Haldar", "Nur Al Hasan", ""], ["Li", "Jianxin", ""], ["Ali", "Mohammed Eunus", ""], ["Cai", "Taotao", ""], ["Sellis", "Timos", ""], ["Reynolds", "Mark", ""]]}, {"id": "2009.00524", "submitter": "Binhang Yuan", "authors": "Binhang Yuan and Dimitrije Jankov and Jia Zou and Yuxin Tang and\n  Daniel Bourgeois and Chris Jermaine", "title": "Tensor Relational Algebra for Machine Learning System Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) systems have to support various tensor operations.\nHowever, such ML systems were largely developed without asking: what are the\nfoundational abstractions necessary for building machine learning systems? We\nbelieve that proper computational and implementation abstractions will allow\nfor the construction of self-configuring, declarative ML systems, especially\nwhen the goal is to execute tensor operations in a distributed environment, or\npartitioned across multiple AI accelerators (ASICs). To this end, we first\nintroduce a tensor relational algebra (TRA), which is expressive to encode any\ntensor operation that can be written in the Einstein notation. We consider how\nTRA expressions can be re-written into an implementation algebra (IA) that\nenables effective implementation in a distributed environment, as well as how\nexpressions in the IA can be optimized. Our empirical study shows that the\noptimized implementation provided by IA can reach or even out-perform carefully\nengineered HPC or ML systems for large scale tensor manipulations and ML\nworkflows in distributed clusters.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 15:51:24 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 23:21:30 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Yuan", "Binhang", ""], ["Jankov", "Dimitrije", ""], ["Zou", "Jia", ""], ["Tang", "Yuxin", ""], ["Bourgeois", "Daniel", ""], ["Jermaine", "Chris", ""]]}, {"id": "2009.00786", "submitter": "Botao Peng", "authors": "Botao Peng, Panagiota Fatourou, Themis Palpanas", "title": "MESSI: In-Memory Data Series Indexing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data series similarity search is a core operation for several data series\nanalysis applications across many different domains. However, the\nstate-of-the-art techniques fail to deliver the time performance required for\ninteractive exploration, or analysis of large data series collections. In this\nwork, we propose MESSI, the first data series index designed for in-memory\noperation on modern hardware. Our index takes advantage of the modern hardware\nparallelization opportunities (i.e., SIMD instructions, multi-core and\nmulti-socket architectures), in order to accelerate both index construction and\nsimilarity search processing times. Moreover, it benefits from a careful design\nin the setup and coordination of the parallel workers and data structures, so\nthat it maximizes its performance for in-memory operations. Our experiments\nwith synthetic and real datasets demonstrate that overall MESSI is up to 4x\nfaster at index construction, and up to 11x faster at query answering than the\nstate-of-the-art parallel approach. MESSI is the first to answer exact\nsimilarity search queries on 100GB datasets in _50msec (30-75msec across\ndiverse datasets), which enables real-time, interactive data exploration on\nvery large data series collections.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 02:10:18 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Peng", "Botao", ""], ["Fatourou", "Panagiota", ""], ["Palpanas", "Themis", ""]]}, {"id": "2009.01121", "submitter": "Andreas Zuefle", "authors": "Andreas Zuefle", "title": "Uncertain Spatial Data Management:An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both the current trends in technology such as smartphones, general mobile\ndevices, stationary sensors, and satellites as well as a new user mentality of\nusing this technology to voluntarily share enriched location information\nproduces a flood of geo-spatial and geo-spatiotemporal data. This data flood\nprovides tremendous potential for discovering new and useful knowledge. But in\naddition to the fact that measurements are imprecise, spatial data is often\ninterpolated between discrete observations. To reduce communication and\nbandwidth utilization, data is often subjected to a reduction, thereby\neliminating some of the known/recorded values. These issues introduce the\nnotion of uncertainty in spatial data management, an aspect raising the\nimminent need for scalable and flexible solutions. The main scope of this\nchapter is to survey existing techniques for managing, querying, and mining\nuncertain spatial data. First, this chapter surveys common data representations\nfor uncertain data, explains the commonly used possible worlds semantics to\ninterpret an uncertain database, and surveys existing system to process\nuncertain data. Then, this chapter defines the notion of probabilistic result\nsemantics to distinguish the task of computing individual object probabilities\nversus computing entire result probabilities. This is important, as, for many\nqueries, the problem of computing object-level probabilities can be solved\nefficiently, whereas result-level probabilities are hard to compute. Finally,\nthis chapter introduces a novel paradigm to efficiently answer any kind of\nquery on uncertain data: the Paradigm of Equivalent Worlds, which groups the\nexponential set of possible database worlds into a polynomial number of sets of\nequivalent worlds that can be processed efficiently. Examples and use-cases of\nquerying uncertain spatial data are provided using the example of uncertain\nrange queries.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:57:57 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Zuefle", "Andreas", ""]]}, {"id": "2009.01444", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Sara Evensen and Chang Ge and Dongjin Choi and \\c{C}a\\u{g}atay\n  Demiralp", "title": "Data Programming by Demonstration: A Framework for Interactively\n  Learning Labeling Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DB cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data programming is a programmatic weak supervision approach to efficiently\ncurate large-scale labeled training data. Writing data programs (labeling\nfunctions) requires, however, both programming literacy and domain expertise.\nMany subject matter experts have neither programming proficiency nor time to\neffectively write data programs. Furthermore, regardless of one's expertise in\ncoding or machine learning, transferring domain expertise into labeling\nfunctions by enumerating rules and thresholds is not only time consuming but\nalso inherently difficult. Here we propose a new framework, data programming by\ndemonstration (DPBD), to generate labeling rules using interactive\ndemonstrations of users. DPBD aims to relieve the burden of writing labeling\nfunctions from users, enabling them to focus on higher-level semantics such as\nidentifying relevant signals for labeling tasks. We operationalize our\nframework with Ruler, an interactive system that synthesizes labeling rules for\ndocument classification by using span-level annotations of users on document\nexamples. We compare Ruler with conventional data programming through a user\nstudy conducted with 10 data scientists creating labeling functions for\nsentiment and spam classification tasks. We find that Ruler is easier to use\nand learn and offers higher overall satisfaction, while providing\ndiscriminative model performances comparable to ones achieved by conventional\ndata programming.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 04:25:08 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 01:44:22 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 22:44:04 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Evensen", "Sara", ""], ["Ge", "Chang", ""], ["Choi", "Dongjin", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "2009.01614", "submitter": "Botao Peng", "authors": "Botao Peng", "title": "Data Series Indexing Gone Parallel", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.00166,\n  arXiv: 2009.00786", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data series similarity search is a core operation for several data series\nanalysis applications across many different domains. However, the\nstate-of-the-art techniques fail to deliver the time performance required for\ninteractive exploration, or analysis of large data series collections. In this\nPh.D. work, we present the first data series indexing solutions, for both\non-disk and in-memory data, that are designed to inherently take advantage of\nmulti-core architectures, in order to accelerate similarity search processing\ntimes. Our experiments on a variety of synthetic and real data demonstrate that\nour approaches are up to orders of magnitude faster than the alternatives. More\nspecifically, our on-disk solution can answer exact similarity search queries\non 100GB datasets in a few seconds, and our in-memory solution in a few\nmilliseconds, which enables real-time, interactive data exploration on very\nlarge data series collections.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 02:26:19 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Peng", "Botao", ""]]}, {"id": "2009.01769", "submitter": "Davide Mario Longo", "authors": "Wolfgang Fischl, Georg Gottlob, Davide Mario Longo, Reinhard Pichler", "title": "HyperBench: A Benchmark and Tool for Hypergraphs and Empirical Findings", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.08181", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the intractability of answering Conjunctive Queries (CQs) and\nsolving Constraint Satisfaction Problems (CSPs), several notions of hypergraph\ndecompositions have been proposed -- giving rise to different notions of width,\nnoticeably, plain, generalized, and fractional hypertree width (hw, ghw, and\nfhw). Given the increasing interest in using such decomposition methods in\npractice, a publicly accessible repository of decomposition software, as well\nas a large set of benchmarks, and a web-accessible workbench for inserting,\nanalyzing, and retrieving hypergraphs are called for.\n  We address this need by providing (i) concrete implementations of hypergraph\ndecompositions (including new practical algorithms), (ii) a new, comprehensive\nbenchmark of hypergraphs stemming from disparate CQ and CSP collections, and\n(iii) HyperBench, our new web-inter\\-face for accessing the benchmark and the\nresults of our analyses. In addition, we describe a number of actual\nexperiments we carried out with this new infrastructure.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 13:08:55 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Fischl", "Wolfgang", ""], ["Gottlob", "Georg", ""], ["Longo", "Davide Mario", ""], ["Pichler", "Reinhard", ""]]}, {"id": "2009.02043", "submitter": "Fredrik Olsson", "authors": "Fredrik Olsson, Magnus Sahlgren", "title": "Data Readiness for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document concerns data readiness in the context of machine learning and\nNatural Language Processing. It describes how an organization may proceed to\nidentify, make available, validate, and prepare data to facilitate automated\nanalysis methods. The contents of the document is based on the practical\nchallenges and frequently asked questions we have encountered in our work as an\napplied research institute with helping organizations and companies, both in\nthe public and private sectors, to use data in their business processes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 07:53:43 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 12:03:58 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Olsson", "Fredrik", ""], ["Sahlgren", "Magnus", ""]]}, {"id": "2009.02258", "submitter": "Tiemo Bang", "authors": "Tiemo Bang (Technical University Darmstadt and SAP SE), Norman May\n  (SAP SE), Ilia Petrov (Reutlingen University), Carsten Binnig (Technical\n  University Darmstadt)", "title": "AnyDB: An Architecture-less DBMS for Any Workload", "comments": "Submitted to 11th Annual Conference on Innovative Data Systems\n  Research (CIDR 21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a radical new approach for scale-out distributed\nDBMSs. Instead of hard-baking an architectural model, such as a shared-nothing\narchitecture, into the distributed DBMS design, we aim for a new class of\nso-called architecture-less DBMSs. The main idea is that an architecture-less\nDBMS can mimic any architecture on a per-query basis on-the-fly without any\nadditional overhead for reconfiguration. Our initial results show that our\narchitecture-less DBMS AnyDB can provide significant speed-ups across varying\nworkloads compared to a traditional DBMS implementing a static architecture.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 15:38:27 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Bang", "Tiemo", "", "Technical University Darmstadt and SAP SE"], ["May", "Norman", "", "SAP SE"], ["Petrov", "Ilia", "", "Reutlingen University"], ["Binnig", "Carsten", "", "Technical\n  University Darmstadt"]]}, {"id": "2009.02678", "submitter": "Raja Appuswamy", "authors": "Raja Appuswamy and Vincent Joguin", "title": "Universal Layout Emulation for Long-Term Database Archival", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on alternate media technologies, like film, synthetic DNA, and\nglass, for long-term data archival has received a lot of attention recently due\nto the media obsolescence issues faced by contemporary storage media like tape,\nHard Disk Drives (HDD), and Solid State Disks (SSD). While researchers have\ndeveloped novel layout and encoding techniques for archiving databases on these\nnew media types, one key question remains unaddressed: How do we ensure that\nthe decoders developed today will be available and executable by a user who is\nrestoring an archived database several decades later in the future, on a\ncomputing platform that potentially does not even exist today?\n  In this paper, we make the case for Universal Layout Emulation (ULE), a new\napproach for future-proof, long-term database archival that advocates archiving\ndecoders together with the data to ensure successful recovery. In order to do\nso, ULE brings together concepts from Data Management and Digital Preservation\ncommunities by using emulation for archiving decoders. In order to show that\nULE can be implemented in practice, we present the design and evaluation of\nMicr'Olonys, an end-to-end long-term database archival system that can be used\nto archive databases using visual analog media like film, microform, and\narchival paper.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 09:06:13 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 10:09:25 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Appuswamy", "Raja", ""], ["Joguin", "Vincent", ""]]}, {"id": "2009.02995", "submitter": "Markus Iser", "authors": "Markus Iser, Luca Springer, Carsten Sinz", "title": "Collaborative Management of Benchmark Instances and their Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental evaluation is an integral part in the design process of\nalgorithms. Publicly available benchmark instances are widely used to evaluate\nmethods in SAT solving. For the interpretation of results and the design of\nalgorithm portfolios their attributes are crucial. Capturing the interrelation\nof benchmark instances and their attributes is considerably simplified through\nour specification of a benchmark instance identifier. Thus, our tool increases\nthe availability of both by providing means to manage and retrieve benchmark\ninstances by their attributes and vice versa. Like this, it facilitates the\ndesign and analysis of SAT experiments and the exchange of results.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:23:08 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Iser", "Markus", ""], ["Springer", "Luca", ""], ["Sinz", "Carsten", ""]]}, {"id": "2009.03052", "submitter": "Stefano Leucci", "authors": "Marco Bressan, Stefano Leucci, Alessandro Panconesi", "title": "Faster motif counting via succinct color coding and adaptive sampling", "comments": null, "journal-ref": "ACM Trans. Knowl. Discov. Data 15, 6, Article 96 (June 2021)", "doi": "10.1145/3447397", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing the distribution of induced connected\nsubgraphs, aka \\emph{graphlets} or \\emph{motifs}, in large graphs. The current\nstate-of-the-art algorithms estimate the motif counts via uniform sampling, by\nleveraging the color coding technique by Alon, Yuster and Zwick. In this work\nwe extend the applicability of this approach, by introducing a set of\nalgorithmic optimizations and techniques that reduce the running time and space\nusage of color coding and improve the accuracy of the counts. To this end, we\nfirst show how to optimize color coding to efficiently build a compact table of\na representative subsample of all graphlets in the input graph. For $8$-node\nmotifs, we can build such a table in one hour for a graph with $65$M nodes and\n$1.8$B edges, which is $2000$ times larger than the state of the art. We then\nintroduce a novel adaptive sampling scheme that breaks the \"additive error\nbarrier\" of uniform sampling, guaranteeing multiplicative approximations\ninstead of just additive ones. This allows us to count not only the most\nfrequent motifs, but also extremely rare ones. For instance, on one graph we\naccurately count nearly $10.000$ distinct $8$-node motifs whose relative\nfrequency is so small that uniform sampling would literally take centuries to\nfind them. Our results show that color coding is still the most promising\napproach to scalable motif counting.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:54:19 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 17:21:23 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bressan", "Marco", ""], ["Leucci", "Stefano", ""], ["Panconesi", "Alessandro", ""]]}, {"id": "2009.03158", "submitter": "Yuya Sasaki", "authors": "Yuya Sasaki, Yasuhiro Fujiwara, Makoto Onizuka", "title": "Efficient Network Reliability Computation in Uncertain Graphs", "comments": null, "journal-ref": "EDBT2019", "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network reliability is an important metric to evaluate the connectivity among\ngiven vertices in uncertain graphs. Since the network reliability problem is\nknown as #P-complete, existing studies have used approximation techniques. In\nthis paper, we propose a new sampling-based approach that efficiently and\naccurately approximates network reliability. Our approach improves efficiency\nby reducing the number of samples based on stratified sampling. We\ntheoretically guarantee that our approach improves the accuracy of\napproximation by using lower and upper bounds of network reliability, even\nthough it reduces the number of samples. To efficiently compute the bounds, we\ndevelop an extended BDD, called S2BDD. During constructing the S2BDD, our\napproach employs dynamic programming for efficiently sampling possible graphs.\nOur experiment with real datasets demonstrates that our approach is up to 51.2\ntimes faster than the existing sampling-based approach with higher accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 07:08:29 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Sasaki", "Yuya", ""], ["Fujiwara", "Yasuhiro", ""], ["Onizuka", "Makoto", ""]]}, {"id": "2009.03304", "submitter": "Marion Ludwig", "authors": "Fabian Kovacs, Max Thonagel, Marion Ludwig, Alexander Albrecht, Hannes\n  Priehn, Manuel Hegner, Dirk Enders, Lennart Hickstein, Maximilian von\n  Knobloch, Anne Rothhardt, Jochen Walker", "title": "Conquery: an open source application to analyze high content healthcare\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Big data in healthcare must be exploited to achieve a substantial\nincrease in efficiency and competitiveness. Especially the analysis of\npatient-related data possesses huge potential to considerably improve\ndecision-making processes in the healthcare sector. Most analytical approaches\nused today are highly time- and resource-consuming. The presented software\nsolution Conquery is an open source software tool providing advanced, but\nintuitive data analysis without the need for specialized statistical training.\nResults: We developed a highly scalable column-oriented distributed timeseries\ndatabase and analysis platform. Its main application is the analysis of\nper-person medical records by non-technical medical professionals. Complex\nanalyses can be realized in a web-frontend without necessitating deep knowledge\nof the underlying data and structure. Queries are evaluated by a bespoke\ndistributed query-engine for medical records. We present a custom compression\nscheme to facilitate low response times that uses online calculated as well as\nprecomputed metadata and statistics. Conclusions: Conquery enables users fast\nqueries and analysis of large datasets without requiring technical expertise.\nThis reduces the technical burden in the decision-making process, to facilitate\nbetter data utilization in the healthcare sector. As the only open-source\nsoftware in Germany that explicitly addresses the stringent use and analysis of\nhealth data, Conquery is of great value to the healthcare community.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 08:23:27 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 10:21:56 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Kovacs", "Fabian", ""], ["Thonagel", "Max", ""], ["Ludwig", "Marion", ""], ["Albrecht", "Alexander", ""], ["Priehn", "Hannes", ""], ["Hegner", "Manuel", ""], ["Enders", "Dirk", ""], ["Hickstein", "Lennart", ""], ["von Knobloch", "Maximilian", ""], ["Rothhardt", "Anne", ""], ["Walker", "Jochen", ""]]}, {"id": "2009.03358", "submitter": "Jin Cao", "authors": "Jin Cao and Yibo Zhao and Linjun Zhang and Jason Li", "title": "A Lightweight Algorithm to Uncover Deep Relationships in Data Tables", "comments": "9 pages, 4 figures, paper presented on AutoML 2019 (The Third\n  International Workshop on Automation in Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data we collect today are in tabular form, with rows as records and\ncolumns as attributes associated with each record. Understanding the structural\nrelationship in tabular data can greatly facilitate the data science process.\nTraditionally, much of this relational information is stored in table schema\nand maintained by its creators, usually domain experts. In this paper, we\ndevelop automated methods to uncover deep relationships in a single data table\nwithout expert or domain knowledge. Our method can decompose a data table into\nlayers of smaller tables, revealing its deep structure. The key to our approach\nis a computationally lightweight forward addition algorithm that we developed\nto recursively extract the functional dependencies between table columns that\nare scalable to tables with many columns. With our solution, data scientists\nwill be provided with automatically generated, data-driven insights when\nexploring new data sets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 18:25:15 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Cao", "Jin", ""], ["Zhao", "Yibo", ""], ["Zhang", "Linjun", ""], ["Li", "Jason", ""]]}, {"id": "2009.03520", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Sajjadur Rahman and Peter Griggs and \\c{C}a\\u{g}atay Demiralp", "title": "Leam: An Interactive System for In-situ Visual Text Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in scale and availability of digital text generated on the\nweb, enterprises such as online retailers and aggregators often use text\nanalytics to mine and analyze the data to improve their services and products\nalike. Text data analysis is an iterative, non-linear process with diverse\nworkflows spanning multiple stages, from data cleaning to visualization.\nExisting text analytics systems usually accommodate a subset of these stages\nand often fail to address challenges related to data heterogeneity, provenance,\nworkflow reusability and reproducibility, and compatibility with established\npractices. Based on a set of design considerations we derive from these\nchallenges, we propose Leam, a system that treats the text analysis process as\na single continuum by combining advantages of computational notebooks,\nspreadsheets, and visualization tools. Leam features an interactive user\ninterface for running text analysis workflows, a new data model for managing\nmultiple atomic and composite data types, and an expressive algebra that\ncaptures diverse sets of operations representing various stages of text\nanalysis and enables coordination among different components of the system,\nincluding data, code, and visualizations. We report our current progress in\nLeam development while demonstrating its usefulness with usage examples.\nFinally, we outline a number of enhancements to Leam and identify several\nresearch directions for developing an interactive visual text analysis system.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 05:18:29 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Rahman", "Sajjadur", ""], ["Griggs", "Peter", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "2009.03776", "submitter": "Yuya Sasaki", "authors": "Yuya Sasaki, Yoshiharu Ishikawa, Yasuhiro Fujiwara, Makoto Onizuka", "title": "Sequenced Route Query with Semantic Hierarchy", "comments": null, "journal-ref": "EDBT2018", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trip planning query searches for preferred routes starting from a given\npoint through multiple Point-of-Interests (PoI) that match user requirements.\nAlthough previous studies have investigated trip planning queries, they lack\nflexibility for finding routes because all of them output routes that strictly\nmatch user requirements. We study trip planning queries that output multiple\nroutes in a flexible manner. We propose a new type of query called skyline\nsequenced route (SkySR) query, which searches for all preferred sequenced\nroutes to users by extending the shortest route search with the semantic\nsimilarity of PoIs in the route. Flexibility is achieved by the {\\it semantic\nhierarchy} of the PoI category. We propose an efficient algorithm for the SkySR\nquery, bulk SkySR algorithm that simultaneously searches for sequenced routes\nand prunes unnecessary routes effectively. Experimental evaluations show that\nthe proposed approach significantly outperforms the existing approaches in\nterms of response time (up to four orders of magnitude). Moreover, we develop a\nprototype service that uses the SkySR query, and conduct a user test to\nevaluate its usefulness.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 14:08:14 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Sasaki", "Yuya", ""], ["Ishikawa", "Yoshiharu", ""], ["Fujiwara", "Yasuhiro", ""], ["Onizuka", "Makoto", ""]]}, {"id": "2009.04283", "submitter": "Ioana Manolescu", "authors": "Mhd Yamen Haddad (CEDAR), Angelos Anadiotis (CEDAR), Yamen Mhd, Ioana\n  Manolescu (CEDAR)", "title": "Graph-based keyword search in heterogeneous data sources", "comments": "Informal publication only", "journal-ref": "36{\\`e}me Conf{\\'e}rence sur la Gestion de Donn{\\'e}es --\n  Principes, Technologies et Applications (BDA 2020), Oct 2020, Online, France", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data journalism is the field of investigative journalism which focuses on\ndigital data by treating them as first-class citizens. Following the trends in\nhuman activity, which leaves strong digital traces, data journalism becomes\nincreasingly important. However, as the number and the diversity of data\nsources increase, heterogeneous data models with different structure, or even\nno structure at all, need to be considered in query answering. Inspired by our\ncollaboration with Le Monde, a leading French newspaper, we designed a novel\nquery algorithm for exploiting such heterogeneous corpora through keyword\nsearch. We model our underlying data as graphs and, given a set of search\nterms, our algorithm nds links between them within and across the heterogeneous\ndatasets included in the graph. We draw inspiration from prior work on keyword\nsearch in structured and unstructured data, which we extend with the data\nheterogeneity dimension, which makes the keyword search problem computationally\nharder. We implement our algorithm and we evaluate its performance using\nsynthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 13:05:32 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 14:23:46 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Haddad", "Mhd Yamen", "", "CEDAR"], ["Anadiotis", "Angelos", "", "CEDAR"], ["Mhd", "Yamen", "", "CEDAR"], ["Manolescu", "Ioana", "", "CEDAR"]]}, {"id": "2009.04462", "submitter": "Jian Pei", "authors": "Jian Pei", "title": "A Survey on Data Pricing: from Economics to Data Science", "comments": null, "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 2021", "doi": "10.1109/TKDE.2020.3045927", "report-no": null, "categories": "econ.TH cs.AI cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data are invaluable. How can we assess the value of data objectively,\nsystematically and quantitatively? Pricing data, or information goods in\ngeneral, has been studied and practiced in dispersed areas and principles, such\nas economics, marketing, electronic commerce, data management, data mining and\nmachine learning. In this article, we present a unified, interdisciplinary and\ncomprehensive overview of this important direction. We examine various\nmotivations behind data pricing, understand the economics of data pricing and\nreview the development and evolution of pricing models according to a series of\nfundamental principles. We discuss both digital products and data products. We\nalso consider a series of challenges and directions for future work.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 19:31:38 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 23:10:46 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Pei", "Jian", ""]]}, {"id": "2009.04540", "submitter": "Daniel Kang", "authors": "Daniel Kang, John Guibas, Peter Bailis, Tatsunori Hashimoto, Matei\n  Zaharia", "title": "Task-agnostic Indexes for Deep Learning-based Queries over Unstructured\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured data is now commonly queried by using target deep neural\nnetworks (DNNs) to produce structured information, e.g., object types and\npositions in video. As these target DNNs can be computationally expensive,\nrecent work uses proxy models to produce query-specific proxy scores. These\nproxy scores are then used in downstream query processing algorithms for\nimproved query execution speeds. Unfortunately, proxy models are often trained\nper-query, require large amounts of training data from the target DNN, and new\ntraining methods per query type.\n  In this work, we develop an index construction method (task-agnostic semantic\ntrainable index, TASTI) that produces reusable embeddings that can be used to\ngenerate proxy scores for a wide range of queries, removing the need for\nquery-specific proxies. We observe that many queries over the same dataset only\nrequire access to the schema induced by the target DNN. For example, an\naggregation query counting the number of cars and a selection query selecting\nframes of cars require only the object types per frame of video. To leverage\nthis opportunity, TASTI produces embeddings per record that have the key\nproperty that close embeddings have similar extracted attributes under the\ninduced schema. Given this property, we show that clustering by embeddings can\nbe used to answer downstream queries efficiently. We theoretically analyze\nTASTI and show that low training error guarantees downstream query accuracy for\na natural class of queries. We evaluate TASTI on four video and text datasets,\nand three query types. We show that TASTI can be 10x less expensive to\nconstruct than proxy models and can outperform them by up to 24x at query time.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 19:55:09 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Kang", "Daniel", ""], ["Guibas", "John", ""], ["Bailis", "Peter", ""], ["Hashimoto", "Tatsunori", ""], ["Zaharia", "Matei", ""]]}, {"id": "2009.04611", "submitter": "Xikui Wang", "authors": "Xikui Wang, Michael J. Carey, Vassilis J. Tsotras", "title": "Subscribing to Big Data at Scale", "comments": "36 pages, 47 figures, submitted to TOCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, data is being actively generated by a variety of devices, services,\nand applications. Such data is important not only for the information that it\ncontains, but also for its relationships to other data and to interested users.\nMost existing Big Data systems focus on passively answering queries from users,\nrather than actively collecting data, processing it, and serving it to users.\nTo satisfy both passive and active requests at scale, users need either to\nheavily customize an existing passive Big Data system or to glue multiple\nsystems together. Either choice would require significant effort from users and\nincur additional overhead. In this paper, we present the BAD (Big Active Data)\nsystem, which is designed to preserve the merits of passive Big Data systems\nand introduce new features for actively serving Big Data to users at scale. We\nshow the design and implementation of the BAD system, demonstrate how BAD\nfacilitates providing both passive and active data services, investigate the\nBAD system's performance at scale, and illustrate the complexities that would\nresult from instead providing BAD-like services with a \"glued\" system.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 00:04:53 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Wang", "Xikui", ""], ["Carey", "Michael J.", ""], ["Tsotras", "Vassilis J.", ""]]}, {"id": "2009.05032", "submitter": "Timo Homburg", "authors": "Timo Homburg, Steffen Staab, Daniel Janke", "title": "GeoSPARQL+: Syntax, Semantics and System for Integrated Querying of\n  Graph, Raster and Vector Data -- Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to semantically represent and query raster data in a\nSemantic Web graph. We extend the GeoSPARQL vocabulary and query language to\nsupport raster data as a new type of geospatial data. We define new filter\nfunctions and illustrate our approach using several use cases on real-world\ndata sets. Finally, we describe a prototypical implementation and validate the\nfeasibility of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:53:19 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Homburg", "Timo", ""], ["Staab", "Steffen", ""], ["Janke", "Daniel", ""]]}, {"id": "2009.05774", "submitter": "Christian Anti\\'c", "authors": "Christian Antic", "title": "Finite Horn Monoids via Propositional Horn Theory Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.DM math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing complex objects as the composition of elementary ones is a common\nstrategy in computer science and science in general. This paper contributes to\nthe foundations of knowledge representation and database theory by introducing\nand studying the sequential composition of propositional Horn theories.\nSpecifically, we show that the notion of composition gives rise to a family of\nmonoids and seminearrings, which we will call {\\em Horn monoids} and {\\em Horn\nseminearrings} in this paper. Particularly, we show that the combination of\ncomposition and union yields the structure of a finite idempotent seminearring.\nWe also show that the restricted class of proper propositional Krom-Horn\ntheories, which only contain rules with exactly one body atom, yields a finite\nidempotent semiring. On the semantic side, we show that the van Emden-Kowalski\nimmediate consequence operator of a theory can be represented via composition,\nwhich allows us to compute its least model semantics without any explicit\nreference to operators. This bridges the conceptual gap between the syntax and\nsemantics of a propositional Horn theory in a mathematically satisfactory way.\nMoreover, it gives rise to an algebraic meta-calculus for propositional Horn\ntheories. In a broader sense, this paper is a first step towards an algebra of\nrule-based logical theories and in the future we plan to adapt and generalize\nthe methods of this paper to wider classes of theories, most importantly to\nfirst-, and higher-order logic programs, and non-monotonic logic programs under\nthe stable model or answer set semantics and extensions thereof.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 11:57:30 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 14:47:23 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 17:18:49 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Antic", "Christian", ""]]}, {"id": "2009.05798", "submitter": "S Subhashree", "authors": "Subhashree S and P Sreenivasa Kumar", "title": "A Simple and Efficient Framework for Identifying Relation-gaps in\n  Ontologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though many ontologies have huge number of classes, one cannot find a good\nnumber of object properties connecting the classes in most of the cases. Adding\nobject properties makes an ontology richer and more applicable for tasks such\nas Question Answering. In this context, the question of which two classes\nshould be considered for discovering object properties becomes very important.\nWe address the above question in this paper. We propose a simple machine\nlearning framework which exhibits low time complexity and yet gives promising\nresults with respect to both precision as well as number of class-pairs\nretrieved.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 14:06:39 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["S", "Subhashree", ""], ["Kumar", "P Sreenivasa", ""]]}, {"id": "2009.06116", "submitter": "Jannis Born", "authors": "Jannis Born, Nina Wiedemann, Gabriel Br\\\"andle, Charlotte Buhre,\n  Bastian Rieck, Karsten Borgwardt", "title": "Accelerating COVID-19 Differential Diagnosis with Explainable Ultrasound\n  Image Analysis", "comments": "8 pages, 4 figures", "journal-ref": "Applied Sciences 2021 (special issue on: \"Fighting COVID-19:\n  Emerging Techniques and Aid Systems for Prevention, Forecasting and\n  Diagnosis\")", "doi": "10.3390/app11020672", "report-no": null, "categories": "cs.CV cs.DB cs.DL cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controlling the COVID-19 pandemic largely hinges upon the existence of fast,\nsafe, and highly-available diagnostic tools. Ultrasound, in contrast to CT or\nX-Ray, has many practical advantages and can serve as a globally-applicable\nfirst-line examination technique. We provide the largest publicly available\nlung ultrasound (US) dataset for COVID-19 consisting of 106 videos from three\nclasses (COVID-19, bacterial pneumonia, and healthy controls); curated and\napproved by medical experts. On this dataset, we perform an in-depth study of\nthe value of deep learning methods for differential diagnosis of COVID-19. We\npropose a frame-based convolutional neural network that correctly classifies\nCOVID-19 US videos with a sensitivity of 0.98+-0.04 and a specificity of\n0.91+-08 (frame-based sensitivity 0.93+-0.05, specificity 0.87+-0.07). We\nfurther employ class activation maps for the spatio-temporal localization of\npulmonary biomarkers, which we subsequently validate for human-in-the-loop\nscenarios in a blindfolded study with medical experts. Aiming for scalability\nand robustness, we perform ablation studies comparing mobile-friendly, frame-\nand video-based architectures and show reliability of the best model by\naleatoric and epistemic uncertainty estimates. We hope to pave the road for a\ncommunity effort toward an accessible, efficient and interpretable screening\nmethod and we have started to work on a clinical validation of the proposed\nmethod. Data and code are publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 23:52:03 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Born", "Jannis", ""], ["Wiedemann", "Nina", ""], ["Br\u00e4ndle", "Gabriel", ""], ["Buhre", "Charlotte", ""], ["Rieck", "Bastian", ""], ["Borgwardt", "Karsten", ""]]}, {"id": "2009.06194", "submitter": "Takahiro Komamizu", "authors": "Takahiro Komamizu", "title": "SPARQL with XQuery-based Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linked Open Data (LOD) has been proliferated over various domains, however,\nthere are still lots of open data in various format other than RDF, a standard\ndata description framework in LOD. These open data can also be connected to\nentities in LOD when they are associated with URIs. Document-centric XML data\nare such open data that are connected with entities in LOD as supplemental\ndocuments for these entities, and to convert these XML data into RDF requires\nvarious techniques such as information extraction, ontology design and ontology\nmapping with human prior knowledge. To utilize document-centric XML data linked\nfrom entities in LOD, in this paper, a SPARQL-based seamless access method on\nRDF and XML data is proposed. In particular, an extension to SPARQL,\nXQueryFILTER, which enables XQuery as a filter in SPARQL is proposed. For\nefficient query processing of the combination of SPARQL and XQuery, a database\ntheory-based query optimization is proposed. Real-world scenario-based\nexperiments in this paper showcase that effectiveness of XQueryFILTER and\nefficiency of the optimization.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 04:44:59 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Komamizu", "Takahiro", ""]]}, {"id": "2009.06209", "submitter": "Alessandro Berti Mr", "authors": "Alessandro Berti, Wil van der Aalst, David Zang, Magdalena Lang", "title": "An Open-Source Integration of Process Mining Features into the Camunda\n  Workflow Engine: Data Extraction and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process mining provides techniques to improve the performance and compliance\nof operational processes. Although sometimes the term \"workflow mining\" is\nused, the application in the context of Workflow Management (WFM) and Business\nProcess Management (BPM) systems is limited. The main reason is that WFM/BPM\nsystems control the process, leaving less room for flexibility and the\ncorresponding deviations. However, as this paper shows, it is easy to extract\nevent data from systems like Camunda, one of the leading open-source WFM/BPM\nsystems. Moreover, although the respective process engines control the process\nflow, process mining is still able to provide valuable insights, such as the\nanalysis of the performance of the paths and the mining of the decision rules.\nThis demo paper presents a process mining connector to Camunda that extracts\nevent logs and process models, allowing for the application of existing process\nmining tools. We also analyzed the added value of different process mining\ntechniques in the context of Camunda. We discuss a subset of process mining\ntechniques that nicely complements the process intelligence capabilities of\nCamunda. Through this demo paper, we hope to boost the use of process mining\namong Camunda users.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 05:49:32 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Berti", "Alessandro", ""], ["van der Aalst", "Wil", ""], ["Zang", "David", ""], ["Lang", "Magdalena", ""]]}, {"id": "2009.06505", "submitter": "M. Emre Gursoy", "authors": "Mehmet Emre Gursoy, Vivekanand Rajasekar, Ling Liu", "title": "Utility-Optimized Synthesis of Differentially Private Location Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially private location trace synthesis (DPLTS) has recently emerged\nas a solution to protect mobile users' privacy while enabling the analysis and\nsharing of their location traces. A key challenge in DPLTS is to best preserve\nthe utility in location trace datasets, which is non-trivial considering the\nhigh dimensionality, complexity and heterogeneity of datasets, as well as the\ndiverse types and notions of utility. In this paper, we present OptaTrace: a\nutility-optimized and targeted approach to DPLTS. Given a real trace dataset D,\nthe differential privacy parameter epsilon controlling the strength of privacy\nprotection, and the utility/error metric Err of interest; OptaTrace uses\nBayesian optimization to optimize DPLTS such that the output error (measured in\nterms of given metric Err) is minimized while epsilon-differential privacy is\nsatisfied. In addition, OptaTrace introduces a utility module that contains\nseveral built-in error metrics for utility benchmarking and for choosing Err,\nas well as a front-end web interface for accessible and interactive DPLTS\nservice. Experiments show that OptaTrace's optimized output can yield\nsubstantial utility improvement and error reduction compared to previous work.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:07:45 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Gursoy", "Mehmet Emre", ""], ["Rajasekar", "Vivekanand", ""], ["Liu", "Ling", ""]]}, {"id": "2009.06538", "submitter": "Jianyu Yang", "authors": "Jianyu Yang, Tianhao Wang, Ninghui Li, Xiang Cheng, Sen Su", "title": "Answering Multi-Dimensional Range Queries under Local Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of answering multi-dimensional range\nqueries under local differential privacy. There are three key technical\nchallenges: capturing the correlations among attributes, avoiding the curse of\ndimensionality, and dealing with the large domains of attributes. None of the\nexisting approaches satisfactorily deals with all three challenges. Overcoming\nthese three challenges, we first propose an approach called Two-Dimensional\nGrids (TDG). Its main idea is to carefully use binning to partition the\ntwo-dimensional (2-D) domains of all attribute pairs into 2-D grids that can\nanswer all 2-D range queries and then estimate the answer of a higher\ndimensional range query from the answers of the associated 2-D range queries.\nHowever, in order to reduce errors due to noises, coarse granularities are\nneeded for each attribute in 2-D grids, losing fine-grained distribution\ninformation for individual attributes. To correct this deficiency, we further\npropose Hybrid-Dimensional Grids (HDG), which also introduces 1-D grids to\ncapture finer-grained information on distribution of each individual attribute\nand combines information from 1-D and 2-D grids to answer range queries. To\nmake HDG consistently effective, we provide a guideline for properly choosing\ngranularities of grids based on an analysis of how different sources of errors\nare impacted by these choices. Extensive experiments conducted on real and\nsynthetic datasets show that HDG can give a significant improvement over the\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 16:08:53 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yang", "Jianyu", ""], ["Wang", "Tianhao", ""], ["Li", "Ninghui", ""], ["Cheng", "Xiang", ""], ["Su", "Sen", ""]]}, {"id": "2009.06625", "submitter": "Meng Wang", "authors": "Xinyue Zhang, Meng Wang, Muhammad Saleem, Axel-Cyrille Ngonga Ngomo,\n  Guilin Qi, and Haofen Wang", "title": "Revealing Secrets in SPARQL Session Level", "comments": "18 pages. Accepted by ISWC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on Semantic Web technologies, knowledge graphs help users to discover\ninformation of interest by using live SPARQL services. Answer-seekers often\nexamine intermediate results iteratively and modify SPARQL queries repeatedly\nin a search session. In this context, understanding user behaviors is critical\nfor effective intention prediction and query optimization. However, these\nbehaviors have not yet been researched systematically at the SPARQL session\nlevel. This paper reveals secrets of session-level user search behaviors by\nconducting a comprehensive investigation over massive real-world SPARQL query\nlogs. In particular, we thoroughly assess query changes made by users w.r.t.\nstructural and data-driven features of SPARQL queries. To illustrate the\npotentiality of our findings, we employ an application example of how to use\nour findings, which might be valuable to devise efficient SPARQL caching,\nauto-completion, query suggestion, approximation, and relaxation techniques in\nthe future.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 16:00:21 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 03:39:41 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhang", "Xinyue", ""], ["Wang", "Meng", ""], ["Saleem", "Muhammad", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""], ["Qi", "Guilin", ""], ["Wang", "Haofen", ""]]}, {"id": "2009.07203", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Bunyamin Sisman, Hao Wei, Xin Luna Dong, Shuiwang Ji", "title": "CorDEL: A Contrastive Deep Learning Approach for Entity Linkage", "comments": "Accepted by the 20th IEEE International Conference on Data Mining\n  (ICDM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linkage (EL) is a critical problem in data cleaning and integration.\nIn the past several decades, EL has typically been done by rule-based systems\nor traditional machine learning models with hand-curated features, both of\nwhich heavily depend on manual human inputs. With the ever-increasing growth of\nnew data, deep learning (DL) based approaches have been proposed to alleviate\nthe high cost of EL associated with the traditional models. Existing\nexploration of DL models for EL strictly follows the well-known twin-network\narchitecture. However, we argue that the twin-network architecture is\nsub-optimal to EL, leading to inherent drawbacks of existing models. In order\nto address the drawbacks, we propose a novel and generic contrastive DL\nframework for EL. The proposed framework is able to capture both syntactic and\nsemantic matching signals and pays attention to subtle but critical\ndifferences. Based on the framework, we develop a contrastive DL approach for\nEL, called CorDEL, with three powerful variants. We evaluate CorDEL with\nextensive experiments conducted on both public benchmark datasets and a\nreal-world dataset. CorDEL outperforms previous state-of-the-art models by 5.2%\non public benchmark datasets. Moreover, CorDEL yields a 2.4% improvement over\nthe current best DL model on the real-world dataset, while reducing the number\nof training parameters by 97.6%.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 16:33:05 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 21:30:29 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 00:30:33 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Wang", "Zhengyang", ""], ["Sisman", "Bunyamin", ""], ["Wei", "Hao", ""], ["Dong", "Xin Luna", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2009.07396", "submitter": "Victor Zhong", "authors": "Victor Zhong, Mike Lewis, Sida I. Wang, Luke Zettlemoyer", "title": "Grounded Adaptation for Zero-shot Executable Semantic Parsing", "comments": "EMNLP 2020 long paper. 14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Grounded Adaptation for Zero-shot Executable Semantic Parsing\n(GAZP) to adapt an existing semantic parser to new environments (e.g. new\ndatabase schemas). GAZP combines a forward semantic parser with a backward\nutterance generator to synthesize data (e.g. utterances and SQL queries) in the\nnew environment, then selects cycle-consistent examples to adapt the parser.\nUnlike data-augmentation, which typically synthesizes unverified examples in\nthe training environment, GAZP synthesizes examples in the new environment\nwhose input-output consistency are verified. On the Spider, Sparc, and CoSQL\nzero-shot semantic parsing tasks, GAZP improves logical form and execution\naccuracy of the baseline parser. Our analyses show that GAZP outperforms\ndata-augmentation in the training environment, performance increases with the\namount of GAZP-synthesized data, and cycle-consistency is central to successful\nadaptation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 00:16:59 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 00:37:15 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 20:44:05 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Zhong", "Victor", ""], ["Lewis", "Mike", ""], ["Wang", "Sida I.", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2009.07410", "submitter": "Sumit Purohit", "authors": "Sumit Purohit, Nhuy Van, George Chin", "title": "Semantic Property Graph for Scalable Knowledge Graph Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are a natural and fundamental representation of describing the\nactivities, relationships, and evolution of various complex systems. Many\ndomains such as communication, citation, procurement, biology, social media,\nand transportation can be modeled as a set of entities and their relationships.\nResource Description Framework (RDF) and Labeled Property Graph (LPG) are two\nof the most used data models to encode information in a graph. Both models are\nsimilar in terms of using basic graph elements such as nodes and edges but\ndiffer in terms of modeling approach, expressibility, serialization, and target\napplications. RDF is a flexible data exchange model for expressing information\nabout entities but it tends to a have high memory footprint and inefficient\nstorage, which does not make it a natural choice to perform scalable graph\nanalytics. In contrast, LPG has gained traction as a reliable model in\nperforming scalable graph analytic tasks such as sub-graph matching, network\nalignment, and real-time knowledge graph query. It provides efficient storage,\nfast traversal, and flexibility to model various real-world domains. At the\nsame time, the LPGs lack the support of a formal knowledge representation such\nas an ontology to provide automated knowledge inference. We propose Semantic\nProperty Graph (SPG) as a logical projection of reified RDF into LPG model. SPG\ncontinues to use RDF ontology to define type hierarchy of the projected graph\nand validate it against a given ontology. We present a framework to convert\nreified RDF graphs into SPG using two different computing environments. We also\npresent cloud-based graph migration capabilities using Amazon Web Services.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 01:18:29 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 21:26:05 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Purohit", "Sumit", ""], ["Van", "Nhuy", ""], ["Chin", "George", ""]]}, {"id": "2009.07770", "submitter": "Polly Fahey", "authors": "Isolde Adler and Polly Fahey", "title": "Faster Property Testers in a Variation of the Bounded Degree Model", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property testing algorithms are highly efficient algorithms, that come with\nprobabilistic accuracy guarantees. For a property P, the goal is to distinguish\ninputs that have P from those that are far from having P with high probability\ncorrectly, by querying only a small number of local parts of the input. In\nproperty testing on graphs, the distance is measured by the number of edge\nmodifications (additions or deletions), that are necessary to transform a graph\ninto one with property P. Much research has focussed on the query complexity of\nsuch algorithms, i. e. the number of queries the algorithm makes to the input,\nbut in view of applications, the running time of the algorithm is equally\nrelevant.\n  In (Adler, Harwath STACS 2018), a natural extension of the bounded degree\ngraph model of property testing to relational databases of bounded degree was\nintroduced, and it was shown that on databases of bounded degree and bounded\ntree-width, every property that is expressible in monadic second-order logic\nwith counting (CMSO) is testable with constant query complexity and sublinear\nrunning time. It remains open whether this can be improved to constant running\ntime.\n  In this paper we introduce a new model, which is based on the bounded degree\nmodel, but the distance measure allows both edge (tuple) modifications and\nvertex (element) modifications. Our main theorem shows that on databases of\nbounded degree and bounded tree-width, every property that is expressible in\nCMSO is testable with constant query complexity and constant running time in\nthe new model. We also show that every property that is testable in the\nclassical model is testable in our model with the same query complexity and\nrunning time, but the converse is not true.\n  We argue that our model is natural and our meta-theorem showing constant-time\nCMSO testability supports this.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 15:55:44 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Adler", "Isolde", ""], ["Fahey", "Polly", ""]]}, {"id": "2009.08044", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Nick Gonsalves, Christina Lee, Anand Raman, Brendan\n  Walsh, Siddhartha Prasad, Dalitso Banda, Lucy Zhang, Lei Zhang, William T.\n  Freeman", "title": "Large-Scale Intelligent Microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.DC cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deploying Machine Learning (ML) algorithms within databases is a challenge\ndue to the varied computational footprints of modern ML algorithms and the\nmyriad of database technologies each with its own restrictive syntax. We\nintroduce an Apache Spark-based micro-service orchestration framework that\nextends database operations to include web service primitives. Our system can\norchestrate web services across hundreds of machines and takes full advantage\nof cluster, thread, and asynchronous parallelism. Using this framework, we\nprovide large scale clients for intelligent services such as speech, vision,\nsearch, anomaly detection, and text analysis. This allows users to integrate\nready-to-use intelligence into any datastore with an Apache Spark connector. To\neliminate the majority of overhead from network communication, we also\nintroduce a low-latency containerized version of our architecture. Finally, we\ndemonstrate that the services we investigate are competitive on a variety of\nbenchmarks, and present two applications of this framework to create\nintelligent search engines, and real-time auto race analytics systems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 03:38:28 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 20:51:47 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Hamilton", "Mark", ""], ["Gonsalves", "Nick", ""], ["Lee", "Christina", ""], ["Raman", "Anand", ""], ["Walsh", "Brendan", ""], ["Prasad", "Siddhartha", ""], ["Banda", "Dalitso", ""], ["Zhang", "Lucy", ""], ["Zhang", "Lei", ""], ["Freeman", "William T.", ""]]}, {"id": "2009.08150", "submitter": "Guy Khazma", "authors": "Paula Ta-Shma, Guy Khazma, Gal Lushi, Oshrit Feder", "title": "Extensible Data Skipping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data skipping reduces I/O for SQL queries by skipping over irrelevant data\nobjects (files) based on their metadata. We extend this notion by allowing\ndevelopers to define their own data skipping metadata types and indexes using a\nflexible API. Our framework is the first to natively support data skipping for\narbitrary data types (e.g. geospatial, logs) and queries with User Defined\nFunctions (UDFs). We integrated our framework with Apache Spark and it is now\ndeployed across multiple products/services at IBM. We present our extensible\ndata skipping APIs, discuss index design, and implement various metadata\nindexes, requiring only around 30 lines of additional code per index. In\nparticular we implement data skipping for a third party library with geospatial\nUDFs and demonstrate speedups of two orders of magnitude. Our centralized\nmetadata approach provides a x3.6 speed up even when compared to queries which\nare rewritten to exploit Parquet min/max metadata. We demonstrate that\nextensible data skipping is applicable to broad class of applications, where\nuser defined indexes achieve significant speedups and cost savings with very\nlow development cost.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 08:34:51 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 14:16:02 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ta-Shma", "Paula", ""], ["Khazma", "Guy", ""], ["Lushi", "Gal", ""], ["Feder", "Oshrit", ""]]}, {"id": "2009.08791", "submitter": "Julie Bu Daher", "authors": "Julie Bu Daher, Armelle Brun and Anne Boyer", "title": "Multi-source Data Mining for e-Learning", "comments": null, "journal-ref": "7th International Symposium \"From Data to Models and Back\n  (DataMod)\" 2018 Jun 25", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is the task of discovering interesting, unexpected or valuable\nstructures in large datasets and transforming them into an understandable\nstructure for further use . Different approaches in the domain of data mining\nhave been proposed, among which pattern mining is the most important one.\nPattern mining mining involves extracting interesting frequent patterns from\ndata. Pattern mining has grown to be a topic of high interest where it is used\nfor different purposes, for example, recommendations. Some of the most common\nchallenges in this domain include reducing the complexity of the process and\navoiding the redundancy within the patterns. So far, pattern mining has mainly\nfocused on the mining of a single data source. However, with the increase in\nthe amount of data, in terms of volume, diversity of sources and nature of\ndata, mining multi-source and heterogeneous data has become an emerging\nchallenge in this domain. This challenge is the main focus of our work where we\npropose to mine multi-source data in order to extract interesting frequent\npatterns.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:39:45 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Daher", "Julie Bu", ""], ["Brun", "Armelle", ""], ["Boyer", "Anne", ""]]}, {"id": "2009.09471", "submitter": "Yue Zhao", "authors": "Zheng Li, Yue Zhao, Jialin Fu", "title": "SYNC: A Copula based Framework for Generating Synthetic Data from\n  Aggregated Sources", "comments": "Proceedings of the 2020 IEEE International Conference on Data Mining\n  Workshops (ICDMW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A synthetic dataset is a data object that is generated programmatically, and\nit may be valuable to creating a single dataset from multiple sources when\ndirect collection is difficult or costly. Although it is a fundamental step for\nmany data science tasks, an efficient and standard framework is absent. In this\npaper, we study a specific synthetic data generation task called downscaling, a\nprocedure to infer high-resolution, harder-to-collect information (e.g.,\nindividual level records) from many low-resolution, easy-to-collect sources,\nand propose a multi-stage framework called SYNC (Synthetic Data Generation via\nGaussian Copula). For given low-resolution datasets, the central idea of SYNC\nis to fit Gaussian copula models to each of the low-resolution datasets in\norder to correctly capture dependencies and marginal distributions, and then\nsample from the fitted models to obtain the desired high-resolution subsets.\nPredictive models are then used to merge sampled subsets into one, and finally,\nsampled datasets are scaled according to low-resolution marginal constraints.\nWe make four key contributions in this work: 1) propose a novel framework for\ngenerating individual level data from aggregated data sources by combining\nstate-of-the-art machine learning and statistical techniques, 2) perform\nsimulation studies to validate SYNC's performance as a synthetic data\ngeneration algorithm, 3) demonstrate its value as a feature engineering tool,\nas well as an alternative to data collection in situations where gathering is\ndifficult through two real-world datasets, 4) release an easy-to-use framework\nimplementation for reproducibility and scalability at the production level that\neasily incorporates new data.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 16:36:25 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Li", "Zheng", ""], ["Zhao", "Yue", ""], ["Fu", "Jialin", ""]]}, {"id": "2009.09488", "submitter": "Albert Atserias", "authors": "Albert Atserias and Phokion G. Kolaitis", "title": "Consistency, Acyclicity, and Positive Semirings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several different settings, one comes across situations in which the\nobjects of study are locally consistent but globally inconsistent. Earlier work\nabout probability distributions by Vorob'ev (1962) and about database relations\nby Beeri, Fagin, Maier, Yannakakis (1983) produced characterizations of when\nlocal consistency always implies global consistency. Towards a common\ngeneralization of these results, we consider K-relations, that is, relations\nover a set of attributes such that each tuple in the relation is associated\nwith an element from an arbitrary, but fixed, positive semiring K. We introduce\nthe notions of projection of a K-relation, consistency of two K-relations, and\nglobal consistency of a collection of K-relations; these notions are natural\nextensions of the corresponding notions about probability distributions and\ndatabase relations. We then show that a collection of sets of attributes has\nthe property that every pairwise consistent collection of K-relations over\nthose attributes is globally consistent if and only if the sets of attributes\nform an acyclic hypergraph. This generalizes the aforementioned results by\nVorob'ev and by Beeri et al., and demonstrates that K-relations over positive\nsemirings constitute a natural framework for the study of the interplay between\nlocal and global consistency. In the course of the proof, we introduce a notion\nof join of two K-relations and argue that it is the \"right\" generalization of\nthe join of two database relations. Furthermore, to show that non-acyclic\nhypergraphs yield pairwise consistent K-relations that are globally\ninconsistent, we generalize a construction by Tseitin (1968) in his study of\nhard-to-prove tautologies in propositional logic.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 18:12:31 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Atserias", "Albert", ""], ["Kolaitis", "Phokion G.", ""]]}, {"id": "2009.09671", "submitter": "Dimitrios Vasilas", "authors": "Dimitrios Vasilas (DELYS, SU), Marc Shapiro (DELYS, SU), Bradley King,\n  Sara Hamouda (DELYS, SU)", "title": "Towards application-specific query processing systems", "comments": null, "journal-ref": "36{\\`e}me Conf{\\'e}rence sur la Gestion de Donn{\\'e}es --\n  Principes, Technologies et Applications (BDA 2020), Oct 2020, Paris, France", "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database systems use query processing subsystems for enabling efficient\nquery-based data retrieval. An essential aspect of designing any\nquery-intensive application is tuning the query system to fit the application's\nrequirements and workload characteristics. However, the configuration\nparameters provided by traditional database systems do not cover the design\ndecisions and trade-offs that arise from the geo-distribution of users and\ndata. In this paper, we present a vision towards a new type of query system\narchitecture that addresses this challenge by enabling query systems to be\ndesigned and deployed in a per use case basis. We propose a distributed\nabstraction called Query Processing Unit that encapsulates primitive query\nprocessing tasks, and show how it can be used as a building block for\nassembling query systems. Using this approach, application architects can\nconstruct query systems specialized to their use cases, by controlling the\nquery system's architecture and the placement of its state. We demonstrate the\nexpressiveness of this approach by applying it to the design of a query system\nthat can flexibly place its state in the data center or at the edge, and show\nthat state placement decisions affect the trade-off between query response time\nand query result freshness.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 08:13:19 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Vasilas", "Dimitrios", "", "DELYS, SU"], ["Shapiro", "Marc", "", "DELYS, SU"], ["King", "Bradley", "", "DELYS, SU"], ["Hamouda", "Sara", "", "DELYS, SU"]]}, {"id": "2009.09801", "submitter": "Michael Thomazo", "authors": "Meghyn Bienvenu (UB, CNRS, Bordeaux INP, LaBRI), Quentin Mani\\`ere\n  (UB, CNRS, Bordeaux INP, LaBRI), Micha\\\"el Thomazo (VALDA )", "title": "Answering Counting Queries over DL-Lite Ontologies", "comments": null, "journal-ref": "Twenty-Ninth International Joint Conference on Artificial\n  Intelligence (IJCAI 2020), 2020, Yokohama, Japan", "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology-mediated query answering (OMQA) is a promising approach to data\naccess and integration that has been actively studied in the knowledge\nrepresentation and database communities for more than a decade. The vast\nmajority of work on OMQA focuses on conjunctive queries, whereas more\nexpressive queries that feature counting or other forms of aggregation remain\nlargely unex-plored. In this paper, we introduce a general form of counting\nquery, relate it to previous proposals, and study the complexity of answering\nsuch queries in the presence of DL-Lite ontologies. As it follows from existing\nwork that query answering is intractable and often of high complexity, we\nconsider some practically relevant restrictions, for which we establish\nimproved complexity bounds.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 11:10:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Bienvenu", "Meghyn", "", "UB, CNRS, Bordeaux INP, LaBRI"], ["Mani\u00e8re", "Quentin", "", "UB, CNRS, Bordeaux INP, LaBRI"], ["Thomazo", "Micha\u00ebl", "", "VALDA"]]}, {"id": "2009.09806", "submitter": "Paolo Pareti Dr.", "authors": "Paolo Pareti and George Konstantinidis and Fabio Mogavero and Timothy\n  J. Norman", "title": "SHACL Satisfiability and Containment (Extended Paper)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shapes Constraint Language (SHACL) is a recent W3C recommendation\nlanguage for validating RDF data. Specifically, SHACL documents are collections\nof constraints that enforce particular shapes on an RDF graph. Previous work on\nthe topic has provided theoretical and practical results for the validation\nproblem, but did not consider the standard decision problems of satisfiability\nand containment, which are crucial for verifying the feasibility of the\nconstraints and important for design and optimization purposes. In this paper,\nwe undertake a thorough study of different features of non-recursive SHACL by\nproviding a translation to a new first-order language, called SCL, that\nprecisely captures the semantics of SHACL w.r.t. satisfiability and\ncontainment. We study the interaction of SHACL features in this logic and\nprovide the detailed map of decidability and complexity results of the\naforementioned decision problems for different SHACL sublanguages. Notably, we\nprove that both problems are undecidable for the full language, but we present\ndecidable combinations of interesting features.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:52:03 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 10:55:19 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Pareti", "Paolo", ""], ["Konstantinidis", "George", ""], ["Mogavero", "Fabio", ""], ["Norman", "Timothy J.", ""]]}, {"id": "2009.09822", "submitter": "Kwei-Herng Lai", "authors": "Kwei-Herng Lai, Daochen Zha, Guanchu Wang, Junjie Xu, Yue Zhao, Devesh\n  Kumar, Yile Chen, Purav Zumkhawaka, Minyang Wan, Diego Martinez, Xia Hu", "title": "TODS: An Automated Time Series Outlier Detection System", "comments": "Accepted by AAAI'21 demo track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TODS, an automated Time Series Outlier Detection System for\nresearch and industrial applications. TODS is a highly modular system that\nsupports easy pipeline construction. The basic building block of TODS is\nprimitive, which is an implementation of a function with hyperparameters. TODS\ncurrently supports 70 primitives, including data processing, time series\nprocessing, feature analysis, detection algorithms, and a reinforcement module.\nUsers can freely construct a pipeline using these primitives and perform end-\nto-end outlier detection with the constructed pipeline. TODS provides a\nGraphical User Interface (GUI), where users can flexibly design a pipeline with\ndrag-and-drop. Moreover, a data-driven searcher is provided to automatically\ndiscover the most suitable pipelines given a dataset. TODS is released under\nApache 2.0 license at https://github.com/datamllab/tods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 15:36:43 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 16:21:31 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 00:00:47 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Lai", "Kwei-Herng", ""], ["Zha", "Daochen", ""], ["Wang", "Guanchu", ""], ["Xu", "Junjie", ""], ["Zhao", "Yue", ""], ["Kumar", "Devesh", ""], ["Chen", "Yile", ""], ["Zumkhawaka", "Purav", ""], ["Wan", "Minyang", ""], ["Martinez", "Diego", ""], ["Hu", "Xia", ""]]}, {"id": "2009.09883", "submitter": "Max Halford", "authors": "Max Halford and Philippe Saint-Pierre and Franck Morvan", "title": "Selectivity Estimation with Attribute Value Dependencies using Linked\n  Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational query optimisers rely on cost models to choose between different\nquery execution plans. Selectivity estimates are known to be a crucial input to\nthe cost model. In practice, standard selectivity estimation procedures are\nprone to large errors. This is mostly because they rely on the so-called\nattribute value independence and join uniformity assumptions. Therefore,\nmultidimensional methods have been proposed to capture dependencies between two\nor more attributes both within and across relations. However, these methods\nrequire a large computational cost which makes them unusable in practice. We\npropose a method based on Bayesian networks that is able to capture\ncross-relation attribute value dependencies with little overhead. Our proposal\nis based on the assumption that dependencies between attributes are preserved\nwhen joins are involved. Furthermore, we introduce a parameter for trading\nbetween estimation accuracy and computational cost. We validate our work by\ncomparing it with other relevant methods on a large workload derived from the\nJOB and TPC-DS benchmarks. Our results show that our method is an order of\nmagnitude more efficient than existing methods, whilst maintaining a high level\nof accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 14:05:05 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Halford", "Max", ""], ["Saint-Pierre", "Philippe", ""], ["Morvan", "Franck", ""]]}, {"id": "2009.09884", "submitter": "Max Halford", "authors": "Max Halford and Philippe Saint-Pierre and Franck Morvan", "title": "Selectivity correction with online machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer systems are full of heuristic rules which drive the decisions they\nmake. These rules of thumb are designed to work well on average, but ignore\nspecific information about the available context, and are thus sub-optimal. The\nemerging field of machine learning for systems attempts to learn decision rules\nwith machine learning algorithms. In the database community, many recent\nproposals have been made to improve selectivity estimation with batch machine\nlearning methods. Such methods are all batch methods which require retraining\nand cannot handle concept drift, such as workload changes and schema\nmodifications. We present online machine learning as an alternative approach.\nOnline models learn on the fly and do not require storing data, they are more\nlightweight than batch models, and finally may adapt to concept drift. As an\nexperiment, we teach models to improve the selectivity estimates made by\nPostgreSQL's cost model. Our experiments make the case that simple online\nmodels are able to compete with a recently proposed deep learning method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 14:05:29 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Halford", "Max", ""], ["Saint-Pierre", "Philippe", ""], ["Morvan", "Franck", ""]]}, {"id": "2009.09895", "submitter": "Michel Fliess", "authors": "Michel Fliess, C\\'edric Join", "title": "Data mining and time series segmentation via extrema: preliminary\n  investigations", "comments": "13th International Conference on Modeling, Optimization and\n  Simulation (MOSIM 2020), Agadir (Morocco), 12-14 November 2020, in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series segmentation is one of the many data mining tools. This paper, in\nFrench, takes local extrema as perceptually interesting points (PIPs). The\nblurring of those PIPs by the quick fluctuations around any time series is\ntreated via an additive decomposition theorem, due to Cartier and Perrin, and\nalgebraic estimation techniques, which are already useful in automatic control\nand signal processing. Our approach is validated by several computer\nillustrations. They underline the importance of the choice of a threshold for\nthe extrema detection.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 16:24:01 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 13:29:34 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Fliess", "Michel", ""], ["Join", "C\u00e9dric", ""]]}, {"id": "2009.09957", "submitter": "Renpeng Zou", "authors": "Renpeng Zou (1), Xixiang Lv (1), Jingsong Zhao (1) ((1) School of\n  Cyber Engineering, Xidian University, Xian, China)", "title": "SPChain: Blockchain-based Medical Data Sharing and Privacy-preserving\n  eHealth System", "comments": null, "journal-ref": "Information Processing & Management, 2021, 58(4): 102604", "doi": "10.1016/j.ipm.2021.102604", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of eHealth systems has brought great convenience to people's\nlife. Researchers have been combining new technologies to make eHealth systems\nwork better for patients. The Blockchain-based eHealth system becomes popular\nbecause of its unique distributed tamper-resistant and privacy-preserving\nfeatures. However, due to the security issues of the blockchain system, there\nare many security risks in eHealth systems utilizing the blockchain technology.\ni.e. 51% attacks can destroy blockchain-based systems. Besides, trivial\ntransactions and frequent calls of smart contracts in the blockchain system\nbring additional costs and security risks to blockchain-based eHealth systems.\nWorse still, electronic medical records (EMRs) are controlled by medical\ninstitutions rather than patients, which causes privacy leakage issues. In this\npaper, we propose a medical data Sharing and Privacy-preserving eHealth system\nbased on blockChain technology (SPChain). We combine RepuCoin with the\nSNARKs-based chameleon hash function to resist underlying blockchain attacks,\nand design a new chain structure to make microblocks contribute to the weight\nof blockchain. The system allows patients to share their EMRs among different\nmedical institutions in a privacy-preserving way. Besides, authorized medical\ninstitutions can label wrong EMRs with the patients' permissions in the case of\nmisdiagnosis. Security analysis and performance evaluation demonstrate that the\nproposed system can provide a strong security guarantee with a high efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 15:28:45 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 04:51:02 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zou", "Renpeng", ""], ["Lv", "Xixiang", ""], ["Zhao", "Jingsong", ""]]}, {"id": "2009.10135", "submitter": "Silviu Maniu", "authors": "Silviu Maniu, Stratis Ioannidis, Bogdan Cautis", "title": "Bandits Under The Influence (Extended Version)", "comments": "27 pages, 4 figures, 6 tables. Extended version of accepted ICDM 2020\n  conference article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems should adapt to user interests as the latter evolve. A\nprevalent cause for the evolution of user interests is the influence of their\nsocial circle. In general, when the interests are not known, online algorithms\nthat explore the recommendation space while also exploiting observed\npreferences are preferable. We present online recommendation algorithms rooted\nin the linear multi-armed bandit literature. Our bandit algorithms are tailored\nprecisely to recommendation scenarios where user interests evolve under social\ninfluence. In particular, we show that our adaptations of the classic LinREL\nand Thompson Sampling algorithms maintain the same asymptotic regret bounds as\nin the non-social case. We validate our approach experimentally using both\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 19:02:00 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Maniu", "Silviu", ""], ["Ioannidis", "Stratis", ""], ["Cautis", "Bogdan", ""]]}, {"id": "2009.10331", "submitter": "Waqas Ali Mr", "authors": "Waqas Ali, Muhammad Saleem, Bin Yao, Aidan Hogan, Axel-Cyrille Ngonga\n  Ngomo", "title": "Storage, Indexing, Query Processing, and Benchmarking in Centralized and\n  Distributed RDF Engines: A Survey", "comments": "reference list is been updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advancements of the Semantic Web and Linked Data have changed the\nworking of the traditional web. There is significant adoption of the Resource\nDescription Framework (RDF) format for saving of web-based data. This massive\nadoption has paved the way for the development of various centralized and\ndistributed RDF processing engines. These engines employ various mechanisms to\nimplement critical components of the query processing engines such as data\nstorage, indexing, language support, and query execution. All these components\ngovern how queries are executed and can have a substantial effect on the query\nruntime. For example, the storage of RDF data in various ways significantly\naffects the data storage space required and the query runtime performance. The\ntype of indexing approach used in RDF engines is critical for fast data lookup.\nThe type of the underlying querying language (e.g., SPARQL or SQL) used for\nquery execution is a crucial optimization component of the RDF storage\nsolutions. Finally, query execution involving different join orders\nsignificantly affects the query response time. This paper provides a\ncomprehensive review of centralized and distributed RDF engines in terms of\nstorage, indexing, language support, and query execution.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 05:55:02 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 15:57:01 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ali", "Waqas", ""], ["Saleem", "Muhammad", ""], ["Yao", "Bin", ""], ["Hogan", "Aidan", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "2009.10373", "submitter": "Michele Linardi", "authors": "Michele Linardi and Themis Palpanas", "title": "Scalable Data Series Subsequence Matching with ULISSE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data series similarity search is an important operation and at the core of\nseveral analysis tasks and applications related to data series collections.\nDespite the fact that data series indexes enable fast similarity search, all\nexisting indexes can only answer queries of a single length (fixed at index\nconstruction time), which is a severe limitation. In this work, we propose\nULISSE, the first data series index structure designed for answering similarity\nsearch queries of variable length (within some range). Our contribution is\ntwo-fold. First, we introduce a novel representation technique, which\neffectively and succinctly summarizes multiple sequences of different length.\nBased on the proposed index, we describe efficient algorithms for approximate\nand exact similarity search, combining disk based index visits and in-memory\nsequential scans. Our approach supports non Z-normalized and Z-normalized\nsequences, and can be used with no changes with both Euclidean Distance and\nDynamic Time Warping, for answering both k-NN and epsilon-range queries. We\nexperimentally evaluate our approach using several synthetic and real datasets.\nThe results show that ULISSE is several times, and up to orders of magnitude\nmore efficient in terms of both space and time cost, when compared to competing\napproaches. (Paper published in VLDBJ 2020)\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 08:04:20 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Linardi", "Michele", ""], ["Palpanas", "Themis", ""]]}, {"id": "2009.10376", "submitter": "Xiaofan Li", "authors": "Xiaofan Li, Rui Zhou, Lu Chen, Chengfei Liu, Qiang He, Yun Yang", "title": "Efficiently Finding a Maximal Clique Summary via Effective Sampling", "comments": "20 pages, 80 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximal clique enumeration (MCE) is a fundamental problem in graph theory and\nis used in many applications, such as social network analysis, bioinformatics,\nintelligent agent systems, cyber security, etc. Most existing MCE algorithms\nfocus on improving the efficiency rather than reducing the output size. The\noutput unfortunately could consist of a large number of maximal cliques. In\nthis paper, we study how to report a summary of less overlapping maximal\ncliques. The problem was studied before, however, after examining the pioneer\napproach, we consider it still not satisfactory. To advance the research along\nthis line, our paper attempts to make four contributions: (a) we propose a more\neffective sampling strategy, which produces a much smaller summary but still\nensures that the summary can somehow witness all the maximal cliques and the\nexpectation of each maximal clique witnessed by the summary is above a\npredefined threshold; (b) we prove that the sampling strategy is optimal under\ncertain optimality conditions; (c) we apply clique-size bounding and design new\nenumeration order to approach the optimality conditions; and (d) to verify\nexperimentally, we test eight real benchmark datasets that have a variety of\ngraph characteristics. The results show that our new sampling strategy\nconsistently outperforms the state-of-the-art approach by producing smaller\nsummaries and running faster on all the datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 08:07:16 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 08:59:54 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Li", "Xiaofan", ""], ["Zhou", "Rui", ""], ["Chen", "Lu", ""], ["Liu", "Chengfei", ""], ["He", "Qiang", ""], ["Yang", "Yun", ""]]}, {"id": "2009.10669", "submitter": "Joris Nix", "authors": "Jens Dittrich, Joris Nix, Christian Sch\\\"on", "title": "There is No Such Thing as an \"Index\"! or: The next 500 Indexing Papers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Index structures are a building block of query processing and computer\nscience in general. Since the dawn of computer technology there have been index\nstructures. And since then, a myriad of index structures are being invented and\npublished each and every year. In this paper we argue that the very idea of\n\"inventing an index\" is a misleading concept in the first place. It is the\nanalogue of \"inventing a physical query plan\". This paper is a paradigm shift\nin which we propose to drop the idea to handcraft index structures (as done for\nbinary search trees over B-trees to any form of learned index) altogether. We\npresent a new automatic index breeding framework coined Genetic Generic\nGeneration of Index Structures (G3oI). It is based on the observation that\nalmost all index structures are assembled along three principal dimensions: (1)\nstructural building blocks, e.g., a B-tree is assembled from two different\nstructural node types (inner and leaf nodes), (2) a couple of invariants, e.g.,\nfor a B-tree all paths have the same length, and (3) decisions on the internal\nlayout of nodes (row or column layout, etc.). We propose a generic indexing\nframework that can mimic many existing index structures along those dimensions.\nBased on that framework we propose a generic genetic index generation algorithm\nthat, given a workload and an optimization goal, can automatically assemble and\nmutate, in other words 'breed' new index structure 'species'. In our\nexperiments we follow multiple goals. We reexamine some good old wisdom from\ndatabase technology. Given a specific workload, will G3oI even breed an index\nthat is equivalent to what our textbooks and papers currently recommend for\nsuch a workload? Or can we do even more? Our initial results strongly indicate\nthat generated indexes are the next step in designing \"index structures\".\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 16:44:27 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Dittrich", "Jens", ""], ["Nix", "Joris", ""], ["Sch\u00f6n", "Christian", ""]]}, {"id": "2009.10955", "submitter": "Tianhui Shi", "authors": "Tianhui Shi, Mingshu Zhai, Yi Xu, Jidong Zhai", "title": "GraphPi: High Performance Graph Pattern Matching through Effective\n  Redundancy Elimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pattern matching, which aims to discover structural patterns in graphs,\nis considered one of the most fundamental graph mining problems in many real\napplications. Despite previous efforts, existing systems face two main\nchallenges. First, inherent symmetry existing in patterns can introduce a large\namount of redundant computation. Second, different matching orders for a\npattern have significant performance differences and are quite hard to predict.\nWhen these factors are mixed, this problem becomes extremely complicated. High\nefficient pattern matching remains an open problem currently. To address these\nchallenges, we propose GraphPi, a high performance distributed pattern matching\nsystem. GraphPi utilizes a new algorithm based on 2-cycles in group theory to\ngenerate multiple sets of asymmetric restrictions, where each set can eliminate\nredundant computation completely. We further design an accurate performance\nmodel to determine the optimal matching order and asymmetric restriction set\nfor efficient pattern matching. We evaluate GraphPi on Tianhe-2A supercomputer.\nResults show that GraphPi outperforms the state-ofthe-art system, by up to 105X\nfor 6 real-world graph datasets on a single node. We also scale GraphPi to\n1,024 computing nodes (24,576 cores).\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 06:58:03 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Shi", "Tianhui", ""], ["Zhai", "Mingshu", ""], ["Xu", "Yi", ""], ["Zhai", "Jidong", ""]]}, {"id": "2009.10989", "submitter": "Chin-Chia Michael Yeh", "authors": "Chin-Chia Michael Yeh, Dhruv Gelda, Zhongfang Zhuang, Yan Zheng, Liang\n  Gou, Wei Zhang", "title": "Towards a Flexible Embedding Learning Framework", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is a fundamental building block for analyzing\nentities in a database. While the existing embedding learning methods are\neffective in various data mining problems, their applicability is often limited\nbecause these methods have pre-determined assumptions on the type of semantics\ncaptured by the learned embeddings, and the assumptions may not well align with\nspecific downstream tasks. In this work, we propose an embedding learning\nframework that 1) uses an input format that is agnostic to input data type, 2)\nis flexible in terms of the relationships that can be embedded into the learned\nrepresentations, and 3) provides an intuitive pathway to incorporate domain\nknowledge into the embedding learning process. Our proposed framework utilizes\na set of entity-relation-matrices as the input, which quantifies the affinities\namong different entities in the database. Moreover, a sampling mechanism is\ncarefully designed to establish a direct connection between the input and the\ninformation captured by the output embeddings. To complete the representation\nlearning toolbox, we also outline a simple yet effective post-processing\ntechnique to properly visualize the learned embeddings. Our empirical results\ndemonstrate that the proposed framework, in conjunction with a set of relevant\nentity-relation-matrices, outperforms the existing state-of-the-art approaches\nin various data mining tasks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 08:00:56 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Yeh", "Chin-Chia Michael", ""], ["Gelda", "Dhruv", ""], ["Zhuang", "Zhongfang", ""], ["Zheng", "Yan", ""], ["Gou", "Liang", ""], ["Zhang", "Wei", ""]]}, {"id": "2009.11013", "submitter": "Jiabo He", "authors": "Jiabo He, Sarah Erfani, Sudanthi Wijewickrema, Stephen O'Leary,\n  Kotagiri Ramamohanarao", "title": "Segmented Pairwise Distance for Time Series with Large Discontinuities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series with large discontinuities are common in many scenarios. However,\nexisting distance-based algorithms (e.g., DTW and its derivative algorithms)\nmay perform poorly in measuring distances between these time series pairs. In\nthis paper, we propose the segmented pairwise distance (SPD) algorithm to\nmeasure distances between time series with large discontinuities. SPD is\northogonal to distance-based algorithms and can be embedded in them. We\nvalidate advantages of SPD-embedded algorithms over corresponding\ndistance-based ones on both open datasets and a proprietary dataset of surgical\ntime series (of surgeons performing a temporal bone surgery in a virtual\nreality surgery simulator). Experimental results demonstrate that SPD-embedded\nalgorithms outperform corresponding distance-based ones in distance measurement\nbetween time series with large discontinuities, measured by the Silhouette\nindex (SI).\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 09:17:57 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["He", "Jiabo", ""], ["Erfani", "Sarah", ""], ["Wijewickrema", "Sudanthi", ""], ["O'Leary", "Stephen", ""], ["Ramamohanarao", "Kotagiri", ""]]}, {"id": "2009.11102", "submitter": "Jan Philipp Portisch", "authors": "Sven Hertling, Jan Portisch, Heiko Paulheim", "title": "Supervised Ontology and Instance Matching with MELT", "comments": "accepted at the the Fifteenth International Workshop on Ontology\n  Matching collocated with the 19th International Semantic Web Conference\n  ISWC-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present MELT-ML, a machine learning extension to the\nMatching and EvaLuation Toolkit (MELT) which facilitates the application of\nsupervised learning for ontology and instance matching. Our contributions are\ntwofold: We present an open source machine learning extension to the matching\ntoolkit as well as two supervised learning use cases demonstrating the\ncapabilities of the new extension.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 14:42:33 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Hertling", "Sven", ""], ["Portisch", "Jan", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2009.11463", "submitter": "Xiao Hu", "authors": "Xiao Hu, Paraschos Koutris, Spyros Blanas", "title": "Algorithms for a Topology-aware Massively Parallel Computation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the prior work in massively parallel data processing assumes\nhomogeneity, i.e., every computing unit has the same computational capability,\nand can communicate with every other unit with the same latency and bandwidth.\nHowever, this strong assumption of a uniform topology rarely holds in practical\nsettings, where computing units are connected through complex networks. To\naddress this issue, Blanas et al. recently proposed a topology-aware massively\nparallel computation model that integrates the network structure and\nheterogeneity in the modeling cost. The network is modeled as a directed graph,\nwhere each edge is associated with a cost function that depends on the data\ntransferred between the two endpoints. The computation proceeds in synchronous\nrounds, and the cost of each round is measured as the maximum cost over all the\nedges in the network.\n  In this work, we take the first step into investigating three fundamental\ndata processing tasks in this topology-aware parallel model: set intersection,\ncartesian product, and sorting. We focus on network topologies that are tree\ntopologies, and present both lower bounds, as well as (asymptotically) matching\nupper bounds. The optimality of our algorithms is with respect to the initial\ndata distribution among the network nodes, instead of assuming worst-case\ndistribution as in previous results. Apart from the theoretical optimality of\nour results, our protocols are simple, use a constant number of rounds, and we\nbelieve can be implemented in practical settings as well.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 03:22:46 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Hu", "Xiao", ""], ["Koutris", "Paraschos", ""], ["Blanas", "Spyros", ""]]}, {"id": "2009.11543", "submitter": "Cheol Ryu", "authors": "Yongsik Kwon, Cheol Ryu, Sang Kyun Cha, Arthur H. Lee, Kunsoo Park,\n  Bongki Moon", "title": "Compressed Key Sort and Fast Index Reconstruction", "comments": "26 pages and 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an index key compression scheme based on the notion\nof distinction bits by proving that the distinction bits of index keys are\nsufficient information to determine the sorted order of the index keys\ncorrectly. While the actual compression ratio may vary depending on the\ncharacteristics of datasets (an average of 2.76 to one compression ratio was\nobserved in our experiments), the index key compression scheme leads to\nsignificant performance improvements during the reconstruction of large-scale\nindexes. Our index key compression can be effectively used in database\nreplication and index recovery of modern main-memory database systems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 08:21:10 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Kwon", "Yongsik", ""], ["Ryu", "Cheol", ""], ["Cha", "Sang Kyun", ""], ["Lee", "Arthur H.", ""], ["Park", "Kunsoo", ""], ["Moon", "Bongki", ""]]}, {"id": "2009.11558", "submitter": "Hideyuki Kawashima", "authors": "Takayuki Tanabe, Takashi Hoshino, Hideyuki Kawashima, Osamu Tatebe", "title": "An Analysis of Concurrency Control Protocols for In-Memory Databases\n  with CCBench (Extended Version)", "comments": "A short version is accepted at VLDB 2020 (PVLDB Volume 13, Issue 13).\n  Code is at https://github.com/thawk105/ccbench", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents yet another concurrency control analysis platform,\nCCBench. CCBench supports seven protocols (Silo, TicToc, MOCC, Cicada, SI, SI\nwith latch-free SSN, 2PL) and seven versatile optimization methods and enables\nthe configuration of seven workload parameters. We analyzed the protocols and\noptimization methods using various workload parameters and a thread count of\n224. Previous studies focused on thread scalability and did not explore the\nspace analyzed here. We classified the optimization methods on the basis of\nthree performance factors: CPU cache, delay on conflict, and version lifetime.\nAnalyses using CCBench and 224 threads, produced six insights. (I1) The\nperformance of optimistic concurrency control protocol for a read only workload\nrapidly degrades as cardinality increases even without L3 cache misses. (I2)\nSilo can outperform TicToc for some write-intensive workloads by using\ninvisible reads optimization. (I3) The effectiveness of two approaches to\ncoping with conflict (wait and no-wait) depends on the situation. (I4) OCC\nreads the same record two or more times if a concurrent transaction\ninterruption occurs, which can improve performance. (I5) Mixing different\nimplementations is inappropriate for deep analysis. (I6) Even a\nstate-of-the-art garbage collection method cannot improve the performance of\nmulti-version protocols if there is a single long transaction mixed into the\nworkload. On the basis of I4, we defined the read phase extension optimization\nin which an artificial delay is added to the read phase. On the basis of I6, we\ndefined the aggressive garbage collection optimization in which even visible\nversions are collected. The code for CCBench and all the data in this paper are\navailable online at GitHub.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 09:11:26 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Tanabe", "Takayuki", ""], ["Hoshino", "Takashi", ""], ["Kawashima", "Hideyuki", ""], ["Tatebe", "Osamu", ""]]}, {"id": "2009.11564", "submitter": "Simon Razniewski", "authors": "Gerhard Weikum, Luna Dong, Simon Razniewski, Fabian Suchanek", "title": "Machine Knowledge: Creation and Curation of Comprehensive Knowledge\n  Bases", "comments": "Submitted to Foundations and Trends in Databases", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equipping machines with comprehensive knowledge of the world's entities and\ntheir relationships has been a long-standing goal of AI. Over the last decade,\nlarge-scale knowledge bases, also known as knowledge graphs, have been\nautomatically constructed from web contents and text sources, and have become a\nkey asset for search engines. This machine knowledge can be harnessed to\nsemantically interpret textual phrases in news, social media and web tables,\nand contributes to question answering, natural language processing and data\nanalytics. This article surveys fundamental concepts and practical methods for\ncreating and curating large knowledge bases. It covers models and methods for\ndiscovering and canonicalizing entities and their semantic types and organizing\nthem into clean taxonomies. On top of this, the article discusses the automatic\nextraction of entity-centric properties. To support the long-term life-cycle\nand the quality assurance of machine knowledge, the article presents methods\nfor constructing open schemas and for knowledge curation. Case studies on\nacademic projects and industrial knowledge graphs complement the survey of\nconcepts and methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 09:28:13 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 23:18:35 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Weikum", "Gerhard", ""], ["Dong", "Luna", ""], ["Razniewski", "Simon", ""], ["Suchanek", "Fabian", ""]]}, {"id": "2009.11648", "submitter": "Michele Linardi", "authors": "Michele Linardi", "title": "Effective and Efficient Variable-Length Data Series Analytics", "comments": "The author was supervised by Themis Palpanas. arXiv admin note:\n  substantial text overlap with arXiv:2009.10373", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last twenty years, data series similarity search has emerged as a\nfundamental operation at the core of several analysis tasks and applications\nrelated to data series collections. Many solutions to different mining problems\nwork by means of similarity search. In this regard, all the proposed solutions\nrequire the prior knowledge of the series length on which similarity search is\nperformed. In several cases, the choice of the length is critical and sensibly\ninfluences the quality of the expected outcome. Unfortunately, the obvious\nbrute-force solution, which provides an outcome for all lengths within a given\nrange is computationally untenable. In this Ph.D. work, we present the first\nsolutions that inherently support scalable and variable-length similarity\nsearch in data series, applied to sequence/subsequences matching, motif and\ndiscord discovery problems.The experimental results show that our approaches\nare up to orders of magnitude faster than the alternatives. They also\ndemonstrate that we can remove the unrealistic constraint of performing\nanalytics using a predefined length, leading to more intuitive and actionable\nresults, which would have otherwise been missed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 09:59:14 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Linardi", "Michele", ""]]}, {"id": "2009.12157", "submitter": "Bolong Zheng", "authors": "Bolong Zheng, Qi Hu, Lingfeng Ming, Jilin Hu, Lu Chen, Kai Zheng,\n  Christian S. Jensen", "title": "SOUP: Spatial-Temporal Demand Forecasting and Competitive Supply", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a setting with an evolving set of requests for transportation\nfrom an origin to a destination before a deadline and a set of agents capable\nof servicing the requests. In this setting, an assignment authority is to\nassign agents to requests such that the average idle time of the agents is\nminimized. An example is the scheduling of taxis (agents) to meet incoming\nrequests for trips while ensuring that the taxis are empty as little as\npossible. In this paper, we study the problem of spatial-temporal demand\nforecasting and competitive supply (SOUP). We address the problem in two steps.\nFirst, we build a granular model that provides spatial-temporal predictions of\nrequests. Specifically, we propose a Spatial-Temporal Graph Convolutional\nSequential Learning (ST-GCSL) algorithm that predicts the service requests\nacross locations and time slots. Second, we provide means of routing agents to\nrequest origins while avoiding competition among the agents. In particular, we\ndevelop a demand-aware route planning (DROP) algorithm that considers both the\nspatial-temporal predictions and the supplydemand state. We report on extensive\nexperiments with realworld and synthetic data that offer insight into the\nperformance of the solution and show that it is capable of outperforming the\nstate-of-the-art proposals.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 08:51:40 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 13:23:57 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zheng", "Bolong", ""], ["Hu", "Qi", ""], ["Ming", "Lingfeng", ""], ["Hu", "Jilin", ""], ["Chen", "Lu", ""], ["Zheng", "Kai", ""], ["Jensen", "Christian S.", ""]]}, {"id": "2009.12414", "submitter": "Haruna Isah", "authors": "Chantal Montgomery, Haruna Isah, Farhana Zulkernine", "title": "Towards a Natural Language Query Processing System", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling the information retrieval gap between non-technical database\nend-users and those with the knowledge of formal query languages has been an\ninteresting area of data management and analytics research. The use of natural\nlanguage interfaces to query information from databases offers the opportunity\nto bridge the communication challenges between end-users and systems that use\nformal query languages. Previous research efforts mainly focused on developing\nstructured query interfaces to relational databases. However, the evolution of\nunstructured big data such as text, images, and video has exposed the\nlimitations of traditional structured query interfaces. While the existing web\nsearch tools prove the popularity and usability of natural language query, they\nreturn complete documents and web pages instead of focused query responses and\nare not applicable to database systems. This paper reports our study on the\ndesign and development of a natural language query interface to a backend\nrelational database. The novelty in the study lies in defining a graph database\nas a middle layer to store necessary metadata needed to transform a natural\nlanguage query into structured query language that can be executed on backend\ndatabases. We implemented and evaluated our approach using a restaurant\ndataset. The translation results for some sample queries yielded a 90% accuracy\nrate.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 19:52:20 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Montgomery", "Chantal", ""], ["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""]]}, {"id": "2009.12415", "submitter": "Haruna Isah", "authors": "Ruoran Liu, Haruna Isah, Farhana Zulkernine", "title": "A Big Data Lake for Multilevel Streaming Analytics", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large organizations are seeking to create new architectures and scalable\nplatforms to effectively handle data management challenges due to the explosive\nnature of data rarely seen in the past. These data management challenges are\nlargely posed by the availability of streaming data at high velocity from\nvarious sources in multiple formats. The changes in data paradigm have led to\nthe emergence of new data analytics and management architecture. This paper\nfocuses on storing high volume, velocity and variety data in the raw formats in\na data storage architecture called a data lake. First, we present our study on\nthe limitations of traditional data warehouses in handling recent changes in\ndata paradigms. We discuss and compare different open source and commercial\nplatforms that can be used to develop a data lake. We then describe our\nend-to-end data lake design and implementation approach using the Hadoop\nDistributed File System (HDFS) on the Hadoop Data Platform (HDP). Finally, we\npresent a real-world data lake development use case for data stream ingestion,\nstaging, and multilevel streaming analytics which combines structured and\nunstructured data. This study can serve as a guide for individuals or\norganizations planning to implement a data lake solution for their use cases.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 19:57:21 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Liu", "Ruoran", ""], ["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""]]}, {"id": "2009.12922", "submitter": "Olga Poppe", "authors": "Olga Poppe, Tayo Amuneke, Dalitso Banda, Aritra De, Ari Green, Manon\n  Knoertzer, Ehi Nosakhare, Karthik Rajendran, Deepak Shankargouda, Meina Wang,\n  Alan Au, Carlo Curino, Qun Guo, Alekh Jindal, Ajay Kalhan, Morgan Oslake,\n  Sonia Parchani, Vijay Ramani, Raj Sellappan, Saikat Sen, Sheetal Shrotri,\n  Soundararajan Srinivasan, Ping Xia, Shize Xu, Alicia Yang, Yiwen Zhu", "title": "Seagull: An Infrastructure for Load Prediction and Optimized Resource\n  Allocation", "comments": "Technical report for the paper in VLDB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microsoft Azure is dedicated to guarantee high quality of service to its\ncustomers, in particular, during periods of high customer activity, while\ncontrolling cost. We employ a Data Science (DS) driven solution to predict user\nload and leverage these predictions to optimize resource allocation. To this\nend, we built the Seagull infrastructure that processes per-server telemetry,\nvalidates the data, trains and deploys ML models. The models are used to\npredict customer load per server (24h into the future), and optimize service\noperations. Seagull continually re-evaluates accuracy of predictions, fallback\nto previously known good models and triggers alerts as appropriate. We deployed\nthis infrastructure in production for PostgreSQL and MySQL servers across all\nAzure regions, and applied it to the problem of scheduling server backups\nduring low-load time. This minimizes interference with user-induced load and\nimproves customer experience.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 18:41:32 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 19:22:57 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Poppe", "Olga", ""], ["Amuneke", "Tayo", ""], ["Banda", "Dalitso", ""], ["De", "Aritra", ""], ["Green", "Ari", ""], ["Knoertzer", "Manon", ""], ["Nosakhare", "Ehi", ""], ["Rajendran", "Karthik", ""], ["Shankargouda", "Deepak", ""], ["Wang", "Meina", ""], ["Au", "Alan", ""], ["Curino", "Carlo", ""], ["Guo", "Qun", ""], ["Jindal", "Alekh", ""], ["Kalhan", "Ajay", ""], ["Oslake", "Morgan", ""], ["Parchani", "Sonia", ""], ["Ramani", "Vijay", ""], ["Sellappan", "Raj", ""], ["Sen", "Saikat", ""], ["Shrotri", "Sheetal", ""], ["Srinivasan", "Soundararajan", ""], ["Xia", "Ping", ""], ["Xu", "Shize", ""], ["Yang", "Alicia", ""], ["Zhu", "Yiwen", ""]]}, {"id": "2009.13395", "submitter": "Satoshi Takahashi", "authors": "Satoshi Takahashi, Keiko Yamaguchi, and Asuka Watanabe", "title": "CAT STREET: Chronicle Archive of Tokyo Street-fashion", "comments": "19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of daily-life fashion trends can provide us a profound\nunderstanding of our societies and cultures. However, no appropriate digital\narchive exists that includes images illustrating what people wore in their\ndaily lives over an extended period. In this study, we propose a new fashion\nimage archive, Chronicle Archive of Tokyo Street-fashion (CAT STREET), to shed\nlight on daily-life fashion trends. CAT STREET includes images showing what\npeople wore in their daily lives during 1970--2017, and these images contain\ntimestamps and street location annotations. This novel database combined with\nmachine learning enables us to observe daily-life fashion trends over a long\nterm and analyze them quantitatively. To evaluate the potential of our proposed\napproach with the novel database, we corroborated the rules-of-thumb of two\nfashion trend phenomena that have been observed and discussed qualitatively in\nprevious studies. Through these empirical analyses, we verified that our\napproach to quantify fashion trends can help in exploring unsolved research\nquestions. We also demonstrate CAT STREET's potential to find new standpoints\nto promote the understanding of societies and cultures through fashion embedded\nin consumers' daily lives.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:16:45 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 13:54:35 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Takahashi", "Satoshi", ""], ["Yamaguchi", "Keiko", ""], ["Watanabe", "Asuka", ""]]}, {"id": "2009.13631", "submitter": "Zuozhi Wang", "authors": "Zuozhi Wang, Kai Zeng, Botong Huang, Wei Chen, Xiaozong Cui, Bo Wang,\n  Ji Liu, Liya Fan, Dachuan Qu, Zhenyu Hou, Tao Guan, Chen Li, Jingren Zhou", "title": "Tempura: A General Cost Based Optimizer Framework for Incremental Data\n  Processing (Extended Version)", "comments": "19 pages, 8 figures. The short version of this paper is accepeted at\n  VLDB 2021 (PVLDB Volume 14, Issue 1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental processing is widely-adopted in many applications, ranging from\nincremental view maintenance, stream computing, to recently emerging\nprogressive data warehouse and intermittent query processing. Despite many\nalgorithms developed on this topic, none of them can produce an incremental\nplan that always achieves the best performance, since the optimal plan is data\ndependent. In this paper, we develop a novel cost-based optimizer framework,\ncalled Tempura, for optimizing incremental data processing. We propose an\nincremental query planning model called TIP based on the concept of\ntime-varying relations, which can formally model incremental processing in its\nmost general form. We give a full specification of Tempura, which can not only\nunify various existing techniques to generate an optimal incremental plan, but\nalso allow the developer to add their rewrite rules. We study how to explore\nthe plan space and search for an optimal incremental plan. We conduct a\nthorough experimental evaluation of Tempura in various incremental processing\nscenarios to show its effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:17:52 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wang", "Zuozhi", ""], ["Zeng", "Kai", ""], ["Huang", "Botong", ""], ["Chen", "Wei", ""], ["Cui", "Xiaozong", ""], ["Wang", "Bo", ""], ["Liu", "Ji", ""], ["Fan", "Liya", ""], ["Qu", "Dachuan", ""], ["Hou", "Zhenyu", ""], ["Guan", "Tao", ""], ["Li", "Chen", ""], ["Zhou", "Jingren", ""]]}, {"id": "2009.13768", "submitter": "Kanat Tangwongsan", "authors": "Kanat Tangwongsan, Martin Hirzel, Scott Schneider", "title": "In-Order Sliding-Window Aggregation in Worst-Case Constant Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding-window aggregation is a widely-used approach for extracting insights\nfrom the most recent portion of a data stream. The aggregations of interest can\nusually be expressed as binary operators that are associative but not\nnecessarily commutative nor invertible. Non-invertible operators, however, are\ndifficult to support efficiently. In a 2017 conference paper, we introduced\nDABA, the first algorithm for sliding-window aggregation with worst-case\nconstant time. Before DABA, if a window had size $n$, the best published\nalgorithms would require $O(\\log n)$ aggregation steps per window\noperation---and while for strictly in-order streams, this bound could be\nimproved to $O(1)$ aggregation steps on average, it was not known how to\nachieve an $O(1)$ bound for the worst-case, which is critical for\nlatency-sensitive applications.\n  This article is an extended version of our 2017 paper. Besides describing\nDABA in more detail, this article introduces a new variant, DABA Lite, which\nachieves the same time bounds in less memory. Whereas DABA requires space for\nstoring $2n$ partial aggregates, DABA Lite only requires space for $n+2$\npartial aggregates. Our experiments on synthetic and real data support the\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 04:11:13 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Tangwongsan", "Kanat", ""], ["Hirzel", "Martin", ""], ["Schneider", "Scott", ""]]}, {"id": "2009.13819", "submitter": "Ester Livshits", "authors": "Ester Livshits and Benny Kimelfeld", "title": "The Shapley Value of Inconsistency Measures for Functional Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the inconsistency of a database is motivated by various goals\nincluding reliability estimation for new datasets and progress indication in\ndata cleaning. Another goal is to attribute to individual tuples a level of\nresponsibility to the overall inconsistency, and thereby prioritize tuples in\nthe explanation or inspection of dirt. Therefore, inconsistency quantification\nand attribution have been a subject of much research in Knowledge\nRepresentation and, more recently, in Databases. As in many other fields, a\nconventional responsibility sharing mechanism is the Shapley value from\ncooperative game theory. In this paper, we carry out a systematic investigation\nof the complexity of the Shapley value in common inconsistency measures for\nfunctional-dependency (FD) violations. For several measures we establish a full\nclassification of the FD sets into tractable and intractable classes with\nrespect to Shapley-value computation. We also study the complexity of\napproximation in intractable cases.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 07:10:36 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Livshits", "Ester", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "2009.13821", "submitter": "Ester Livshits", "authors": "Nofar Carmeli, Martin Grohe, Benny Kimelfeld, Ester Livshits, and\n  Muhammad Tibi", "title": "Database Repairing with Soft Functional Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common interpretation of soft constraints penalizes the database for every\nviolation of every constraint, where the penalty is the cost (weight) of the\nconstraint. A computational challenge is that of finding an optimal subset: a\ncollection of database tuples that minimizes the total penalty when each tuple\nhas a cost of being excluded. When the constraints are strict (i.e., have an\ninfinite cost), this subset is a \"cardinality repair\" of an inconsistent\ndatabase; in soft interpretations, this subset corresponds to a \"most probable\nworld\" of a probabilistic database, a \"most likely intention\" of a\nprobabilistic unclean database, and so on. Within the class of functional\ndependencies, the complexity of finding a cardinality repair is thoroughly\nunderstood. Yet, very little is known about the complexity of this problem in\nthe more general soft semantics. This paper makes a significant progress in\nthis direction. In addition to general insights about the hardness and\napproximability of the problem, we present algorithms for two special cases: a\nsingle functional dependency, and a bipartite matching. The latter is the\nproblem of finding an optimal \"almost matching\" of a bipartite graph where a\npenalty is paid for every lost edge and every violation of monogamy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 07:17:16 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Carmeli", "Nofar", ""], ["Grohe", "Martin", ""], ["Kimelfeld", "Benny", ""], ["Livshits", "Ester", ""], ["Tibi", "Muhammad", ""]]}, {"id": "2009.14040", "submitter": "Peter Fettke", "authors": "Peter Fettke and Wolfgang Reisig", "title": "Modelling service-oriented systems and cloud services with Heraklit", "comments": "13 pages, 3 figures, presented at the 16th International Workshop on\n  Engineering Service-Oriented Applications and Cloud Services, Heraklion,\n  Greece, September 28-30, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern and next generation digital infrastructures are technically based on\nservice oriented structures, cloud services, and other architectures that\ncompose large systems from smaller subsystems. The composition of subsystems is\nparticularly challenging, as the subsystems themselves may be represented in\ndifferent languages, modelling methods, etc. It is quite challenging to\nprecisely conceive, understand, and represent this kind of technology, in\nparticular for a given level of abstraction. To capture refinement and\nabstraction principles, various forms of \"technology stacks\" and other\nsemi-formal or natural language based on presentations have been suggested.\nGenerally, useful concepts to compose such systems in a systematic way are even\nmore rare. Heraklit provides means, principles, and unifying techniques to\nmodel and to analyze digital infrastructures. Heraklit integrates composition\nand hierarchies of subsystems, concrete and abstract data structures, as well\nas descriptions of behaviour. A distinguished set of means supports the modeler\nto express their ideas. The modeller is free to choose the level of\nabstraction, as well as the kind of composition. Heraklit integrates new\nconcepts with tried and tested ones. Such a framework provides the foundation\nfor a comprehensive Systems Mining as the next step after Process Mining.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 14:23:07 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Fettke", "Peter", ""], ["Reisig", "Wolfgang", ""]]}, {"id": "2009.14094", "submitter": "Daniel Schuster", "authors": "Daniel Schuster and Sebastiaan van Zelst and Wil M. P. van der Aalst", "title": "Alignment Approximation for Process Trees", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-72693-5_19", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing observed behavior (event data generated during process executions)\nwith modeled behavior (process models), is an essential step in process mining\nanalyses. Alignments are the de-facto standard technique for calculating\nconformance checking statistics. However, the calculation of alignments is\ncomputationally complex since a shortest path problem must be solved on a state\nspace which grows non-linearly with the size of the model and the observed\nbehavior, leading to the well-known state space explosion problem. In this\npaper, we present a novel framework to approximate alignments on process trees\nby exploiting their hierarchical structure. Process trees are an important\nprocess model formalism used by state-of-the-art process mining techniques such\nas the inductive mining approaches. Our approach exploits structural properties\nof a given process tree and splits the alignment computation problem into\nsmaller sub-problems. Finally, sub-results are composed to obtain an alignment.\nOur experiments show that our approach provides a good balance between accuracy\nand computation time.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 15:24:42 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:40:22 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Schuster", "Daniel", ""], ["van Zelst", "Sebastiaan", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2009.14525", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Edward Curry", "title": "Visual Semantic Multimedia Event Model for Complex Event Detection in\n  Video Streams", "comments": "15 pages, 14 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia data is highly expressive and has traditionally been very\ndifficult for a machine to interpret. Middleware systems such as complex event\nprocessing (CEP) mine patterns from data streams and send notifications to\nusers in a timely fashion. Presently, CEP systems have inherent limitations to\nprocess multimedia streams due to its data complexity and the lack of an\nunderlying structured data model. In this work, we present a visual event\nspecification method to enable complex multimedia event processing by creating\na semantic knowledge representation derived from low-level media streams. The\nmethod enables the detection of high-level semantic concepts from the media\nstreams using an ensemble of pattern detection capabilities. The semantic model\nis aligned with a multimedia CEP engine deep learning models to give\nflexibility to end-users to build rules using spatiotemporal event calculus.\nThis enhances CEP capability to detect patterns from media streams and bridge\nthe semantic gap between highly expressive knowledge-centric user queries to\nthe low-level features of the multi-media data. We have built a small traffic\nevent ontology prototype to validate the approach and performance. The paper\ncontribution is threefold: i) we present a knowledge graph representation for\nmultimedia streams, ii) a hierarchical event network to detect visual patterns\nfrom media streams and iii) define complex pattern rules for complex multimedia\nevent reasoning using event calculus\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 09:22:23 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Yadav", "Piyush", ""], ["Curry", "Edward", ""]]}, {"id": "2009.14821", "submitter": "Sareen Shah", "authors": "Sareen Shah", "title": "A Systematic Method for On-The-Fly Denormalization of Relational\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized relational databases are a common method for storing data, but\npulling out usable denormalized data for consumption generally requires either\ndirect access to the source data or creation of an appropriate view or table by\na database administrator. End-users are thus limited in their ability to\nexplore and use data that is stored in this manner. Presented here is a method\nfor performing automated denormalization of relational databases at run-time,\nwithout requiring access to source data or ongoing intervention by a database\nadministrator. Furthermore, this method does not require a restructure of the\ndatabase itself and so it can be flexibly applied as a layer on top of already\nexisting databases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 17:51:37 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 00:58:37 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Shah", "Sareen", ""]]}]