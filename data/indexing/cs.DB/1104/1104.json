[{"id": "1104.0459", "submitter": "Minghua Chen", "authors": "Yaping Li, Minghua Chen, Qiwei Li, Wei Zhang", "title": "Enabling Multi-level Trust in Privacy Preserving Data Mining", "comments": "20 pages, 5 figures. Accepted for publication in IEEE Transactions on\n  Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy Preserving Data Mining (PPDM) addresses the problem of developing\naccurate models about aggregated data without access to precise information in\nindividual data record. A widely studied \\emph{perturbation-based PPDM}\napproach introduces random perturbation to individual values to preserve\nprivacy before data is published. Previous solutions of this approach are\nlimited in their tacit assumption of single-level trust on data miners.\n  In this work, we relax this assumption and expand the scope of\nperturbation-based PPDM to Multi-Level Trust (MLT-PPDM). In our setting, the\nmore trusted a data miner is, the less perturbed copy of the data it can\naccess. Under this setting, a malicious data miner may have access to\ndifferently perturbed copies of the same data through various means, and may\ncombine these diverse copies to jointly infer additional information about the\noriginal data that the data owner does not intend to release. Preventing such\n\\emph{diversity attacks} is the key challenge of providing MLT-PPDM services.\nWe address this challenge by properly correlating perturbation across copies at\ndifferent trust levels. We prove that our solution is robust against diversity\nattacks with respect to our privacy goal. That is, for data miners who have\naccess to an arbitrary collection of the perturbed copies, our solution prevent\nthem from jointly reconstructing the original data more accurately than the\nbest effort using any individual copy in the collection. Our solution allows a\ndata owner to generate perturbed copies of its data for arbitrary trust levels\non-demand. This feature offers data owners maximum flexibility.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2011 03:48:13 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2011 15:49:06 GMT"}], "update_date": "2011-04-06", "authors_parsed": [["Li", "Yaping", ""], ["Chen", "Minghua", ""], ["Li", "Qiwei", ""], ["Zhang", "Wei", ""]]}, {"id": "1104.0553", "submitter": "Pierre Senellart", "authors": "Michael Benedikt, Georg Gottlob, Pierre Senellart", "title": "Determining Relevance of Accesses at Runtime (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the situation where a query is to be answered using Web sources that\nrestrict the accesses that can be made on backend relational data by requiring\nsome attributes to be given as input of the service. The accesses provide\nlookups on the collection of attributes values that match the binding. They can\ndiffer in whether or not they require arguments to be generated from prior\naccesses. Prior work has focused on the question of whether a query can be\nanswered using a set of data sources, and in developing static access plans\n(e.g., Datalog programs) that implement query answering. We are interested in\ndynamic aspects of the query answering problem: given partial information about\nthe data, which accesses could provide relevant data for answering a given\nquery? We consider immediate and long-term notions of \"relevant accesses\", and\nascertain the complexity of query relevance, for both conjunctive queries and\narbitrary positive queries. In the process, we relate dynamic relevance of an\naccess to query containment under access limitations and characterize the\ncomplexity of this problem; we produce several complexity results about\ncontainment that are of interest by themselves.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2011 12:56:47 GMT"}, {"version": "v2", "created": "Thu, 5 May 2011 13:21:02 GMT"}, {"version": "v3", "created": "Mon, 30 May 2011 08:44:06 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Benedikt", "Michael", ""], ["Gottlob", "Georg", ""], ["Senellart", "Pierre", ""]]}, {"id": "1104.0867", "submitter": "Dan Olteanu", "authors": "Dan Olteanu and Jakub Zavodny", "title": "Factorised Representations of Query Results", "comments": "44 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query tractability has been traditionally defined as a function of input\ndatabase and query sizes, or of both input and output sizes, where the query\nresult is represented as a bag of tuples. In this report, we introduce a\nframework that allows to investigate tractability beyond this setting. The key\ninsight is that, although the cardinality of a query result can be exponential,\nits structure can be very regular and thus factorisable into a nested\nrepresentation whose size is only polynomial in the size of both the input\ndatabase and query.\n  For a given query result, there may be several equivalent representations,\nand we quantify the regularity of the result by its readability, which is the\nminimum over all its representations of the maximum number of occurrences of\nany tuple in that representation. We give a characterisation of\nselect-project-join queries based on the bounds on readability of their results\nfor any input database. We complement it with an algorithm that can find\nasymptotically optimal upper bounds and corresponding factorised\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 15:09:09 GMT"}], "update_date": "2011-04-06", "authors_parsed": [["Olteanu", "Dan", ""], ["Zavodny", "Jakub", ""]]}, {"id": "1104.1311", "submitter": "Gowri Shankar Ramaswamy", "authors": "Gowri Shankar Ramaswamy and F Sagayaraj Francis", "title": "Latent table discovery by semantic relationship extraction between\n  unrelated sets of entity sets of structured data sources", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 2, March 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying is one of the basic functionality expected from a database system.\nQuery efficiency is adversely affected by increase in the number of\nparticipating tables. Also, querying based on syntax largely limits the gamut\nof queries a database system can process. Syntactic queries rely on the\ndatabase table structure, which is a cause of concern for large organisations\ndue to incompatibility between heterogeneous systems that store data\ndistributed across geographic locations. Solution to these problems is answered\nto some extent by moving towards semantic technology by making data and the\ndatabase meaningful. In doing so, relationship between sets of entity sets will\nnot be limited only to syntactic constraints but would also permit semantic\nconnections nonetheless such relationships may be tacit, intangible and\ninvisible. The goal of this work is to extract such hidden relationships\nbetween unrelated sets of entity sets and store them in a tangible form. A few\nsample cases are provided to vindicate that the proposed work improves querying\nsignificantly.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2011 12:11:53 GMT"}], "update_date": "2011-04-08", "authors_parsed": [["Ramaswamy", "Gowri Shankar", ""], ["Francis", "F Sagayaraj", ""]]}, {"id": "1104.1605", "submitter": "Bogdan Cautis", "authors": "Silviu Maniu, Bogdan Cautis", "title": "Efficient Top-K Retrieval in Online Social Tagging Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this paper top-k query answering in social tagging systems,\nalso known as folksonomies. This problem requires a significant departure from\nexisting, socially agnostic techniques. In a network-aware context, one can\n(and should) exploit the social links, which can indicate how users relate to\nthe seeker and how much weight their tagging actions should have in the result\nbuild-up. We propose an algorithm that has the potential to scale to current\napplications. While the problem has already been considered in previous\nliterature, this was done either under strong simplifying assumptions or under\nchoices that cannot scale to even moderate-size real world applications. We\nfirst consider a key aspect of the problem, which is accessing the closest or\nmost relevant users for a given seeker. We describe how this can be done on the\nfly (without any pre-computations) for several possible choices - arguably the\nmost natural ones - of proximity computation in a user network. Based on this,\nour top-k algorithm is sound and complete, while addressing the scalability\nissues of the existing ones. Importantly, our technique is instance optimal in\nthe case when the search relies exclusively on the social weight of tagging\nactions. To further reduce response times, we then consider directions for\nefficiency by approximation. Extensive experiments on real world data show that\nour techniques can drastically improve the response time, without sacrificing\nprecision.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2011 16:33:16 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2011 13:32:57 GMT"}, {"version": "v3", "created": "Mon, 3 Oct 2011 16:07:51 GMT"}, {"version": "v4", "created": "Thu, 29 Mar 2012 10:10:08 GMT"}, {"version": "v5", "created": "Wed, 15 Aug 2012 20:36:09 GMT"}, {"version": "v6", "created": "Fri, 5 Oct 2012 15:59:20 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Maniu", "Silviu", ""], ["Cautis", "Bogdan", ""]]}, {"id": "1104.2079", "submitter": "Kim Nguyen", "authors": "V\\'eronique Benzaken and Giuseppe Castagna and Dario Colazzo and Kim\n  Nguyen", "title": "Optimizing XML querying using type-based document projection", "comments": "65 pages A4 format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML data projection (or pruning) is a natural optimization for main memory\nquery engines: given a query Q over a document D, the subtrees of D that are\nnot necessary to evaluate Q are pruned, thus producing a smaller document D';\nthe query Q is then executed on D', hence avoiding to allocate and process\nnodes that will never be reached by Q. In this article, we propose a new\napproach, based on types, that greatly improves current solutions. Besides\nproviding comparable or greater precision and far lesser pruning overhead, our\nsolution ---unlike current approaches--- takes into account backward axes,\npredicates, and can be applied to multiple queries rather than just to single\nones. A side contribution is a new type system for XPath able to handle\nbackward axes. The soundness of our approach is formally proved. Furthermore,\nwe prove that the approach is also complete (i.e., yields the best possible\ntype-driven pruning) for a relevant class of queries and Schemas. We further\nvalidate our approach using the XMark and XPathMark benchmarks and show that\npruning not only improves the main memory query engine's performances (as\nexpected) but also those of state of the art native XML databases.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 22:32:31 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Benzaken", "V\u00e9ronique", ""], ["Castagna", "Giuseppe", ""], ["Colazzo", "Dario", ""], ["Nguyen", "Kim", ""]]}, {"id": "1104.2284", "submitter": "Ramya  C", "authors": "C. Ramya, K S Shreedhara and G Kavitha", "title": "Preprocessing: A Prerequisite for Discovering Patterns in WUM Process", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web log data is usually diverse and voluminous. This data must be assembled\ninto a consistent, integrated and comprehensive view, in order to be used for\npattern discovery. Without properly cleaning, transforming and structuring the\ndata prior to the analysis, one cannot expect to find meaningful patterns. As\nin most data mining applications, data preprocessing involves removing and\nfiltering redundant and irrelevant data, removing noise, transforming and\nresolving any inconsistencies. In this paper, a complete preprocessing\nmethodology having merging, data cleaning, user/session identification and data\nformatting and summarization activities to improve the quality of data by\nreducing the quantity of data has been proposed. To validate the efficiency of\nthe proposed preprocessing methodology, several experiments are conducted and\nthe results show that the proposed methodology reduces the size of Web access\nlog files down to 73-82% of the initial size and offers richer logs that are\nstructured for further stages of Web Usage Mining (WUM). So preprocessing of\nraw data in this WUM process is the central theme of this paper.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 17:42:57 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Ramya", "C.", ""], ["Shreedhara", "K S", ""], ["Kavitha", "G", ""]]}, {"id": "1104.2599", "submitter": "Loris D'Antoni", "authors": "Rajeev Alur and Loris D'Antoni", "title": "Streaming Tree Transducers", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theory of tree transducers provides a foundation for understanding\nexpressiveness and complexity of analysis problems for specification languages\nfor transforming hierarchically structured data such as XML documents. We\nintroduce streaming tree transducers as an analyzable, executable, and\nexpressive model for transforming unranked ordered trees in a single pass.\nGiven a linear encoding of the input tree, the transducer makes a single\nleft-to-right pass through the input, and computes the output in linear time\nusing a finite-state control, a visibly pushdown stack, and a finite number of\nvariables that store output chunks that can be combined using the operations of\nstring-concatenation and tree-insertion. We prove that the expressiveness of\nthe model coincides with transductions definable using monadic second-order\nlogic (MSO). Existing models of tree transducers either cannot implement all\nMSO-definable transformations, or require regular look ahead that prohibits\nsingle-pass implementation. We show a variety of analysis problems such as\ntype-checking and checking functional equivalence are solvable for our model.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 20:00:07 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2011 22:59:50 GMT"}, {"version": "v3", "created": "Thu, 8 Dec 2011 18:16:58 GMT"}, {"version": "v4", "created": "Thu, 19 Jan 2012 04:13:12 GMT"}, {"version": "v5", "created": "Tue, 21 Feb 2012 21:24:09 GMT"}], "update_date": "2012-02-23", "authors_parsed": [["Alur", "Rajeev", ""], ["D'Antoni", "Loris", ""]]}, {"id": "1104.2721", "submitter": "Andreas Baldi", "authors": "Alaa H. AL-Hamami, Soukaena H. Hashem", "title": "Optimal Cell Towers Distribution by using Spatial Mining and Geographic\n  Information System", "comments": "5 pages", "journal-ref": "World of Computer Science and Information Technology Journal\n  (WCSIT) , ISSN: 2221-0741, Vol. 1, No. 2, 44-48, 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The appearance of wireless communication is dramatically changing our life.\nMobile telecommunications emerged as a technological marvel allowing for access\nto personal and other services, devices, computation and communication, in any\nplace and at any time through effortless plug and play. Setting up wireless\nmobile networks often requires: Frequency Assignment, Communication Protocol\nselection, Routing schemes selection, and cells towers distributions. This\nresearch aims to optimize the cells towers distribution by using spatial mining\nwith Geographic Information System (GIS) as a tool. The distribution\noptimization could be done by applying the Digital Elevation Model (DEM) on the\nimage of the area which must be covered with two levels of hierarchy. The\nresearch will apply the spatial association rules technique on the second level\nto select the best square in the cell for placing the antenna. From that the\nproposal will try to minimize the number of installed towers, makes tower's\nlocation feasible, and provides full area coverage.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 11:01:56 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["AL-Hamami", "Alaa H.", ""], ["Hashem", "Soukaena H.", ""]]}, {"id": "1104.2756", "submitter": "Tanzima Hashem", "authors": "Tanzima Hashem, Lars Kulik, Rui Zhang", "title": "Privacy Preserving Moving KNN Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach that protects trajectory privacy of users who\naccess location-based services through a moving k nearest neighbor (MkNN)\nquery. An MkNN query continuously returns the k nearest data objects for a\nmoving user (query point). Simply updating a user's imprecise location such as\na region instead of the exact position to a location-based service provider\n(LSP) cannot ensure privacy of the user for an MkNN query: continuous\ndisclosure of regions enables the LSP to follow a user's trajectory. We\nidentify the problem of trajectory privacy that arises from the overlap of\nconsecutive regions while requesting an MkNN query and provide the first\nsolution to this problem. Our approach allows a user to specify the confidence\nlevel that represents a bound of how much more the user may need to travel than\nthe actual kth nearest data object. By hiding a user's required confidence\nlevel and the required number of nearest data objects from an LSP, we develop a\ntechnique to prevent the LSP from tracking the user's trajectory for MkNN\nqueries. We propose an efficient algorithm for the LSP to find k nearest data\nobjects for a region with a user's specified confidence level, which is an\nessential component to evaluate an MkNN query in a privacy preserving manner;\nthis algorithm is at least two times faster than the state-of-the-art\nalgorithm. Extensive experimental studies validate the effectiveness of our\ntrajectory privacy protection technique and the efficiency of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 13:28:08 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Hashem", "Tanzima", ""], ["Kulik", "Lars", ""], ["Zhang", "Rui", ""]]}, {"id": "1104.3212", "submitter": "Hongrae Lee", "authors": "Hongrae Lee (University of British Columbia), Raymond T. Ng\n  (University of British Columbia), Kyuseok Shim (Seoul National University)", "title": "Similarity Join Size Estimation using Locality Sensitive Hashing", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 6, pp.\n  338-349 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity joins are important operations with a broad range of applications.\nIn this paper, we study the problem of vector similarity join size estimation\n(VSJ). It is a generalization of the previously studied set similarity join\nsize estimation (SSJ) problem and can handle more interesting cases such as\nTF-IDF vectors. One of the key challenges in similarity join size estimation is\nthat the join size can change dramatically depending on the input similarity\nthreshold.\n  We propose a sampling based algorithm that uses the\nLocality-Sensitive-Hashing (LSH) scheme. The proposed algorithm LSH-SS uses an\nLSH index to enable effective sampling even at high thresholds. We compare the\nproposed technique with random sampling and the state-of-the-art technique for\nSSJ (adapted to VSJ) and demonstrate LSH-SS offers more accurate estimates at\nboth high and low similarity thresholds and small variance using real-world\ndata sets.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2011 08:44:29 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Lee", "Hongrae", "", "University of British Columbia"], ["Ng", "Raymond T.", "", "University of British Columbia"], ["Shim", "Kyuseok", "", "Seoul National University"]]}, {"id": "1104.3214", "submitter": "Debabrata Dash", "authors": "Debabrata Dash (ArcSight), Neoklis Polyzotis (UC Santa Cruz),\n  Anastasia Ailamaki (EPFL)", "title": "CoPhy: A Scalable, Portable, and Interactive Index Advisor for Large\n  Workloads", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 6, pp.\n  362-372 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Index tuning, i.e., selecting the indexes appropriate for a workload, is a\ncrucial problem in database system tuning. In this paper, we solve index tuning\nfor large problem instances that are common in practice, e.g., thousands of\nqueries in the workload, thousands of candidate indexes and several hard and\nsoft constraints. Our work is the first to reveal that the index tuning problem\nhas a well structured space of solutions, and this space can be explored\nefficiently with well known techniques from linear optimization. Experimental\nresults demonstrate that our approach outperforms state-of-the-art commercial\nand research techniques by a significant margin (up to an order of magnitude).\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2011 08:51:27 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Dash", "Debabrata", "", "ArcSight"], ["Polyzotis", "Neoklis", "", "UC Santa Cruz"], ["Ailamaki", "Anastasia", "", "EPFL"]]}, {"id": "1104.3216", "submitter": "Feng Niu", "authors": "Feng Niu (University of Wisconsin-Madison), Christopher R\\'e\n  (University of Wisconsin-Madison), AnHai Doan (University of\n  Wisconsin-Madison), Jude Shavlik (University of Wisconsin-Madison)", "title": "Tuffy: Scaling up Statistical Inference in Markov Logic Networks using\n  an RDBMS", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 6, pp.\n  373-384 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Logic Networks (MLNs) have emerged as a powerful framework that\ncombines statistical and logical reasoning; they have been applied to many data\nintensive problems including information extraction, entity resolution, and\ntext mining. Current implementations of MLNs do not scale to large real-world\ndata sets, which is preventing their wide-spread adoption. We present Tuffy\nthat achieves scalability via three novel contributions: (1) a bottom-up\napproach to grounding that allows us to leverage the full power of the\nrelational optimizer, (2) a novel hybrid architecture that allows us to perform\nAI-style local search efficiently using an RDBMS, and (3) a theoretical insight\nthat shows when one can (exponentially) improve the efficiency of stochastic\nlocal search. We leverage (3) to build novel partitioning, loading, and\nparallel algorithms. We show that our approach outperforms state-of-the-art\nimplementations in both quality and speed on several publicly available\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2011 08:52:25 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Niu", "Feng", "", "University of Wisconsin-Madison"], ["R\u00e9", "Christopher", "", "University of Wisconsin-Madison"], ["Doan", "AnHai", "", "University of\n  Wisconsin-Madison"], ["Shavlik", "Jude", "", "University of Wisconsin-Madison"]]}, {"id": "1104.3217", "submitter": "Michael Cafarella", "authors": "Eaman Jahani (University of Michigan), Michael J. Cafarella\n  (University of Michigan), Christopher R\\'e (University of Wisconsin-Madison)", "title": "Automatic Optimization for MapReduce Programs", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 6, pp.\n  385-396 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MapReduce distributed programming framework has become popular, despite\nevidence that current implementations are inefficient, requiring far more\nhardware than a traditional relational databases to complete similar tasks.\nMapReduce jobs are amenable to many traditional database query optimizations\n(B+Trees for selections, column-store- style techniques for projections, etc),\nbut existing systems do not apply them, substantially because free-form user\ncode obscures the true data operation being performed. For example, a selection\nin SQL is easily detected, but a selection in a MapReduce program is embedded\nin Java code along with lots of other program logic. We could ask the\nprogrammer to provide explicit hints about the program's data semantics, but\none of MapReduce's attractions is precisely that it does not ask the user for\nsuch information. This paper covers Manimal, which automatically analyzes\nMapReduce programs and applies appropriate data- aware optimizations, thereby\nrequiring no additional help at all from the programmer. We show that Manimal\nsuccessfully detects optimization opportunities across a range of data\noperations, and that it yields speedups of up to 1,121% on previously-written\nMapReduce programs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2011 08:53:59 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Jahani", "Eaman", "", "University of Michigan"], ["Cafarella", "Michael J.", "", "University of Michigan"], ["R\u00e9", "Christopher", "", "University of Wisconsin-Madison"]]}, {"id": "1104.4163", "submitter": "Saurabh  Pal", "authors": "Umesh Kumar Pandey and Saurabh Pal", "title": "Data Mining : A prediction of performer or underperformer using\n  classification", "comments": "5 pages, 1 figure", "journal-ref": "(IJCSIT) International Journal of Computer Science and Information\n  Technology, Vol. 2(2), 2011, 686-690", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now a day's students have a large set of data having precious information\nhidden. Data mining technique can help to find this hidden information. In this\npaper, data mining techniques name Byes classification method is used on these\ndata to help an institution. Institutions can find those students who are\nconsistently perform well. This study will help to institution reduce the drop\nput ratio to a significant level and improve the performance level of the\ninstitution.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 03:50:34 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Pandey", "Umesh Kumar", ""], ["Pal", "Saurabh", ""]]}, {"id": "1104.4164", "submitter": "Saurabh  Pal", "authors": "Umesh Kumar Pandey and Saurabh Pal", "title": "A Data Mining view on Class Room Teaching Language", "comments": "6 pages, 3 figures", "journal-ref": "(IJCSI) International Journal of Computer Science Issue, Vol. 8,\n  Issue 2, March -2011, 277-282, ISSN:1694-0814", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From ancient period in India, educational institution embarked to use class\nroom teaching. Where a teacher explains the material and students understand\nand learn the lesson. There is no absolute scale for measuring knowledge but\nexamination score is one scale which shows the performance indicator of\nstudents. So it is important that appropriate material is taught but it is\nvital that while teaching which language is chosen, class notes must be\nprepared and attendance. This study analyses the impact of language on the\npresence of students in class room. The main idea is to find out the support,\nconfidence and interestingness level for appropriate language and attendance in\nthe classroom. For this purpose association rule is used.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 03:54:49 GMT"}], "update_date": "2011-04-22", "authors_parsed": [["Pandey", "Umesh Kumar", ""], ["Pal", "Saurabh", ""]]}, {"id": "1104.4384", "submitter": "Nitin Gupta", "authors": "Nitin Gupta", "title": "EMBANKS: Towards Disk Based Algorithms For Keyword-Search In Structured\n  Databases", "comments": "45 pages, 15 figures, Bachelors Thesis submission to IIT Bombay", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a lot of interest in the field of keyword\nquerying relational databases. A variety of systems such as DBXplorer [ACD02],\nDiscover [HP02] and ObjectRank [BHP04] have been proposed. Another such system\nis BANKS, which enables data and schema browsing together with keyword-based\nsearch for relational databases. It models tuples as nodes in a graph,\nconnected by links induced by foreign key and other relationships. The size of\nthe database graph that BANKS uses is proportional to the sum of the number of\nnodes and edges in the graph. Systems such as SPIN, which search on Personal\nInformation Networks and use BANKS as the backend, maintain a lot of\ninformation about the users' data. Since these systems run on the user\nworkstation which have other demands of memory, such a heavy use of memory is\nunreasonable and if possible, should be avoided. In order to alleviate this\nproblem, we introduce EMBANKS (acronym for External Memory BANKS), a framework\nfor an optimized disk-based BANKS system. The complexity of this framework\nposes many questions, some of which we try to answer in this thesis. We\ndemonstrate that the cluster representation proposed in EMBANKS enables\nin-memory processing of very large database graphs. We also present detailed\nexperiments that show that EMBANKS can significantly reduce database load time\nand query execution times when compared to the original BANKS algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2011 03:49:09 GMT"}], "update_date": "2011-04-25", "authors_parsed": [["Gupta", "Nitin", ""]]}, {"id": "1104.4899", "submitter": "Zoran Majkic", "authors": "Zoran Majkic", "title": "Data Base Mappings and Theory of Sketches", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we will present the two basic operations for database schemas\nused in database mapping systems (separation and Data Federation), and we will\nexplain why the functorial semantics for database mappings needed a new base\ncategory instead of usual Set category. Successively, it is presented a\ndefinition of the graph G for a schema database mapping system, and the\ndefinition of its sketch category Sch(G). Based on this framework we presented\nfunctorial semantics for database mapping systems with the new base category\nDB.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 12:25:56 GMT"}], "update_date": "2011-04-27", "authors_parsed": [["Majkic", "Zoran", ""]]}, {"id": "1104.5139", "submitter": "Hela Limam", "authors": "Hela Limam and Jalel Akaichi", "title": "Web services synchronization health care application", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": "10.5121/ijwest.2011.2204", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance of Web Services technologies and the emergence of Web\nServices into the information space, tremendous opportunities for empowering\nusers and organizations appear in various application domains including\nelectronic commerce, travel, intelligence information gathering and analysis,\nhealth care, digital government, etc. In fact, Web services appear to be s\nsolution for integrating distributed, autonomous and heterogeneous information\nsources. However, as Web services evolve in a dynamic environment which is the\nInternet many changes can occur and affect them. A Web service is affected when\none or more of its associated information sources is affected by schema\nchanges. Changes can alter the information sources contents but also their\nschemas which may render Web services partially or totally undefined. In this\npaper, we propose a solution for integrating information sources into Web\nservices. Then we tackle the Web service synchronization problem by\nsubstituting the affected information sources. Our work is illustrated with a\nhealthcare case study.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2011 13:43:12 GMT"}], "update_date": "2011-04-28", "authors_parsed": [["Limam", "Hela", ""], ["Akaichi", "Jalel", ""]]}, {"id": "1104.5510", "submitter": "Fahad Saeed", "authors": "Fahad Saeed, Trairak Pisitkun, Mark A. Knepper and Jason D. Hoffert", "title": "Mining Temporal Patterns from iTRAQ Mass Spectrometry(LC-MS/MS) Data", "comments": "12 pages, 10 figures, The Proceedings of the ISCA 3rd International\n  Conference on Bioinformatics and Computational Biology (BiCoB), pp 152-159\n  New Orleans, Louisiana, USA, March 23-25, 2011 (ISBN: 978-1-880843-81-9)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.DB cs.DS q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale proteomic analysis is emerging as a powerful technique in biology\nand relies heavily on data acquired by state-of-the-art mass spectrometers. As\nwith any other field in Systems Biology, computational tools are required to\ndeal with this ocean of data. iTRAQ (isobaric Tags for Relative and Absolute\nquantification) is a technique that allows simultaneous quantification of\nproteins from multiple samples. Although iTRAQ data gives useful insights to\nthe biologist, it is more complex to perform analysis and draw biological\nconclusions because of its multi-plexed design. One such problem is to find\nproteins that behave in a similar way (i.e. change in abundance) among various\ntime points since the temporal variations in the proteomics data reveal\nimportant biological information. Distance based methods such as Euclidian\ndistance or Pearson coefficient, and clustering techniques such as k-mean etc,\nare not able to take into account the temporal information of the series. In\nthis paper, we present an linear-time algorithm for clustering similar patterns\namong various iTRAQ time course data irrespective of their absolute values. The\nalgorithm, referred to as Temporal Pattern Mining(TPM), maps the data from a\nCartesian plane to a discrete binary plane. After the mapping a dynamic\nprogramming technique allows mining of similar data elements that are\ntemporally closer to each other. The proposed algorithm accurately clusters\niTRAQ data that are temporally closer to each other with more than 99%\naccuracy. Experimental results for different problem sizes are analyzed in\nterms of quality of clusters, execution time and scalability for large data\nsets. An example from our proteomics data is provided at the end to demonstrate\nthe performance of the algorithm and its ability to cluster temporal series\nirrespective of their distance from each other.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 20:41:43 GMT"}], "update_date": "2011-05-02", "authors_parsed": [["Saeed", "Fahad", ""], ["Pisitkun", "Trairak", ""], ["Knepper", "Mark A.", ""], ["Hoffert", "Jason D.", ""]]}, {"id": "1104.5517", "submitter": "Patrick Nicholson", "authors": "Amr Elmasry, Meng He, J. Ian Munro, and Patrick K. Nicholson", "title": "Dynamic Range Majority Data Structures", "comments": "16 pages, Preliminary version appeared in ISAAC 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $P$ of coloured points on the real line, we study the problem of\nanswering range $\\alpha$-majority (or \"heavy hitter\") queries on $P$. More\nspecifically, for a query range $Q$, we want to return each colour that is\nassigned to more than an $\\alpha$-fraction of the points contained in $Q$. We\npresent a new data structure for answering range $\\alpha$-majority queries on a\ndynamic set of points, where $\\alpha \\in (0,1)$. Our data structure uses O(n)\nspace, supports queries in $O((\\lg n) / \\alpha)$ time, and updates in $O((\\lg\nn) / \\alpha)$ amortized time. If the coordinates of the points are integers,\nthen the query time can be improved to $O(\\lg n / (\\alpha \\lg \\lg n) +\n(\\lg(1/\\alpha))/\\alpha))$. For constant values of $\\alpha$, this improved query\ntime matches an existing lower bound, for any data structure with\npolylogarithmic update time. We also generalize our data structure to handle\nsets of points in d-dimensions, for $d \\ge 2$, as well as dynamic arrays, in\nwhich each entry is a colour.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 21:35:40 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 16:40:45 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Elmasry", "Amr", ""], ["He", "Meng", ""], ["Munro", "J. Ian", ""], ["Nicholson", "Patrick K.", ""]]}]