[{"id": "1407.0116", "submitter": "Maurizio Naldi", "authors": "Maurizio Naldi and Giuseppe D'Acquisto", "title": "Differential privacy for counting queries: can Bayes estimation help\n  uncover the true value?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is achieved by the introduction of Laplacian noise in\nthe response to a query, establishing a precise trade-off between the level of\ndifferential privacy and the accuracy of the database response (via the amount\nof noise introduced). Multiple queries may improve the accuracy but erode the\nprivacy budget. We examine the case where we submit just a single counting\nquery. We show that even in that case a Bayesian approach may be used to\nimprove the accuracy for the same amount of noise injected, if we know the size\nof the database and the probability of a positive response to the query.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 07:03:22 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Naldi", "Maurizio", ""], ["D'Acquisto", "Giuseppe", ""]]}, {"id": "1407.0120", "submitter": "Daniel Schall", "authors": "Daniel Schall and Theo H\\\"arder", "title": "Dynamic Physiological Partitioning on a Shared-nothing Database Cluster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional DBMS servers are usually over-provisioned for most of their daily\nworkloads and, because they do not show good-enough energy proportionality,\nwaste a lot of energy while underutilized. A cluster of small (wimpy) servers,\nwhere its size can be dynamically adjusted to the current workload, offers\nbetter energy characteristics for these workloads. Yet, data migration,\nnecessary to balance utilization among the nodes, is a non-trivial and\ntime-consuming task that may consume the energy saved. For this reason, a\nsophisticated and easy to adjust partitioning scheme fostering dynamic\nreorganization is needed. In this paper, we adapt a technique originally\ncreated for SMP systems, called physiological partitioning, to distribute data\namong nodes, that allows to easily repartition data without interrupting\ntransactions. We dynamically partition DB tables based on the nodes'\nutilization and given energy constraints and compare our approach with physical\npartitioning and logical partitioning methods. To quantify possible energy\nsaving and its conceivable drawback on query runtimes, we evaluate our\nimplementation on an experimental cluster and compare the results w.r.t.\nperformance and energy consumption. Depending on the workload, we can\nsubstantially save energy without sacrificing too much performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 07:18:43 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 07:18:59 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Schall", "Daniel", ""], ["H\u00e4rder", "Theo", ""]]}, {"id": "1407.0386", "submitter": "Daniel Schall", "authors": "Daniel Schall and Theo H\\\"arder", "title": "Energy and Performance-Can a Wimpy-Node Cluster Challenge a Brawny\n  Server?", "comments": "arXiv admin note: substantial text overlap with arXiv:1407.0120", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional DBMS servers are usually over-provisioned for most of their daily\nworkloads and, because they do not show good energy proportionality, waste a\nlot of energy while underutilized. A cluster of small (wimpy) servers, where\nthe number of nodes can dynamically adjust to the current workload, might offer\nbetter energy characteristics for these workloads. Yet, clusters suffer from\n\"friction losses\" and may not be able to quickly adapt to the workload, whereas\na single, brawny server delivers performance instantaneously. In this paper, we\ncompare a small cluster of lightweight nodes to a single server in terms of\nperformance and energy efficiency. We run several benchmarks, consisting of\nOLTP and OLAP queries at variable utilization to test the system's ability to\nadjust to the workloads. To quantify possible energy saving and its conceivable\ndrawback on query runtime, we evaluate our implementation on a cluster as well\nas on a single, brawny server and compare the results w.r.t. performance and\nenergy consumption. Our findings confirm that - based on the workload - energy\ncan be saved without sacrificing too much performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 07:29:37 GMT"}, {"version": "v2", "created": "Mon, 7 Jul 2014 07:32:53 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Schall", "Daniel", ""], ["H\u00e4rder", "Theo", ""]]}, {"id": "1407.0454", "submitter": "Yingyi Bu", "authors": "Sattam Alsubaiee, Yasser Altowim, Hotham Altwaijry, Alexander Behm,\n  Vinayak Borkar, Yingyi Bu, Michael Carey, Inci Cetindil, Madhusudan\n  Cheelangi, Khurram Faraaz, Eugenia Gabrielova, Raman Grover, Zachary\n  Heilbron, Young-Seok Kim, Chen Li, Guangqiang Li, Ji Mahn Ok, Nicola Onose,\n  Pouria Pirzadeh, Vassilis Tsotras, Rares Vernica, Jian Wen, Till Westmann", "title": "AsterixDB: A Scalable, Open Source BDMS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AsterixDB is a new, full-function BDMS (Big Data Management System) with a\nfeature set that distinguishes it from other platforms in today's open source\nBig Data ecosystem. Its features make it well-suited to applications like web\ndata warehousing, social data storage and analysis, and other use cases related\nto Big Data. AsterixDB has a flexible NoSQL style data model; a query language\nthat supports a wide range of queries; a scalable runtime; partitioned,\nLSM-based data storage and indexing (including B+-tree, R-tree, and text\nindexes); support for external as well as natively stored data; a rich set of\nbuilt-in types; support for fuzzy, spatial, and temporal types and queries; a\nbuilt-in notion of data feeds for ingestion of data; and transaction support\nakin to that of a NoSQL store.\n  Development of AsterixDB began in 2009 and led to a mid-2013 initial open\nsource release. This paper is the first complete description of the resulting\nopen source AsterixDB system. Covered herein are the system's data model, its\nquery language, and its software architecture. Also included are a summary of\nthe current status of the project and a first glimpse into how AsterixDB\nperforms when compared to alternative technologies, including a parallel\nrelational DBMS, a popular NoSQL store, and a popular Hadoop-based SQL data\nanalytics platform, for things that both technologies can do. Also included is\na brief description of some initial trials that the system has undergone and\nthe lessons learned (and plans laid) based on those early \"customer\"\nengagements.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 04:29:54 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Alsubaiee", "Sattam", ""], ["Altowim", "Yasser", ""], ["Altwaijry", "Hotham", ""], ["Behm", "Alexander", ""], ["Borkar", "Vinayak", ""], ["Bu", "Yingyi", ""], ["Carey", "Michael", ""], ["Cetindil", "Inci", ""], ["Cheelangi", "Madhusudan", ""], ["Faraaz", "Khurram", ""], ["Gabrielova", "Eugenia", ""], ["Grover", "Raman", ""], ["Heilbron", "Zachary", ""], ["Kim", "Young-Seok", ""], ["Li", "Chen", ""], ["Li", "Guangqiang", ""], ["Ok", "Ji Mahn", ""], ["Onose", "Nicola", ""], ["Pirzadeh", "Pouria", ""], ["Tsotras", "Vassilis", ""], ["Vernica", "Rares", ""], ["Wen", "Jian", ""], ["Westmann", "Till", ""]]}, {"id": "1407.0455", "submitter": "Yingyi Bu", "authors": "Yingyi Bu, Vinayak Borkar, Jianfeng Jia, Michael J. Carey, Tyson\n  Condie", "title": "Pregelix: Big(ger) Graph Analytics on A Dataflow Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing need for distributed graph processing systems that are\ncapable of gracefully scaling to very large graph datasets. Unfortunately, this\nchallenge has not been easily met due to the intense memory pressure imposed by\nprocess-centric, message passing designs that many graph processing systems\nfollow. Pregelix is a new open source distributed graph processing system that\nis based on an iterative dataflow design that is better tuned to handle both\nin-memory and out-of-core workloads. As such, Pregelix offers improved\nperformance characteristics and scaling properties over current open source\nsystems (e.g., we have seen up to 15x speedup compared to Apache Giraph and up\nto 35x speedup compared to distributed GraphLab), and makes more effective use\nof available machine resources to support Big(ger) Graph Analytics.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 05:04:28 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Bu", "Yingyi", ""], ["Borkar", "Vinayak", ""], ["Jia", "Jianfeng", ""], ["Carey", "Michael J.", ""], ["Condie", "Tyson", ""]]}, {"id": "1407.0481", "submitter": "Konstantinos Kotis", "authors": "Konstantinos Kotis, Iraklis Athanasakis, George Vouros", "title": "Semantic Integration & Single-Site Opening of Multiple Governmental Data\n  Sources", "comments": "21 pages, 7 figures, live demo at\n  http://www.samos.gr/apps/s3-ai/eGovTicketApp.xhtml", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In many cases, government data is still \"locked\" in several \"data silos\",\neven within the boundaries of a single (inter-)national public organization\nwith disparate and distributed organizational units and departments spread\nacross multiple sites. Opening data and enabling its unified querying from a\nsingle site in an efficient and effective way is a semantic application\nintegration and open government data challenge. This paper describes how NARA\nis using Semantic Web technology to implement an application integration\napproach within the boundaries of its organization via opening and querying\nmultiple governmental data sources from a single site. The generic approach\nproposed, namely S3-AI, provides support to answering unified,\nontology-mediated, federated queries to data produced and exploited by\ndisparate applications, while these are being located in different\norganizational sites. S3-AI preserves ownership, autonomy and independency of\napplications and data. The paper extensively demonstrates S3-AI, using the D2RQ\nand Fuseki technologies, for addressing the needs of a governmental \"IT\nhelpdesk support\" case.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 08:40:41 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Kotis", "Konstantinos", ""], ["Athanasakis", "Iraklis", ""], ["Vouros", "George", ""]]}, {"id": "1407.1121", "submitter": "Qiang Ma", "authors": "Qiang Ma, S. Muthukrishnan, Mark Sandler", "title": "Frugal Streaming for Estimating Quantiles:One (or two) memory suffices", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications require processing streams of data for estimating\nstatistical quantities such as quantiles with small amount of memory. In many\nsuch applications, in fact, one needs to compute such statistical quantities\nfor each of a large number of groups, which additionally restricts the amount\nof memory available for the stream for any particular group. We address this\nchallenge and introduce frugal streaming, that is algorithms that work with\ntiny -- typically, sub-streaming -- amount of memory per group.\n  We design a frugal algorithm that uses only one unit of memory per group to\ncompute a quantile for each group. For stochastic streams where data items are\ndrawn from a distribution independently, we analyze and show that the algorithm\nfinds an approximation to the quantile rapidly and remains stably close to it.\nWe also propose an extension of this algorithm that uses two units of memory\nper group. We show with extensive experiments with real world data from HTTP\ntrace and Twitter that our frugal algorithms are comparable to existing\nstreaming algorithms for estimating any quantile, but these existing algorithms\nuse far more space per group and are unrealistic in frugal applications;\nfurther, the two memory frugal algorithm converges significantly faster than\nthe one memory algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 05:12:04 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Ma", "Qiang", ""], ["Muthukrishnan", "S.", ""], ["Sandler", "Mark", ""]]}, {"id": "1407.1807", "submitter": "Mohammed Al-Maolegi", "authors": "Raed Shatnawi, Qutaibah Althebyan, Baraq Ghalib, Mohammed Al-Maolegi", "title": "Building A Smart Academic Advising System Using Association Rule Mining", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In an academic environment, student advising is considered a paramount\nactivity for both advisors and student to improve the academic performance of\nstudents. In universities of large numbers of students, advising is a\ntime-consuming activity that may take a considerable effort of advisors and\nuniversity administration in guiding students to complete their registration\nsuccessfully and efficiently. Current systems are traditional and depend\ngreatly on the effort of the advisor to find the best selection of courses to\nimprove students performance. There is a need for a smart system that can\nadvise a large number of students every semester. In this paper, we propose a\nsmart system that uses association rule mining to help both students and\nadvisors in selecting and prioritizing courses. The system helps students to\nimprove their performance by suggesting courses that meet their current needs\nand at the same time improve their academic performance. The system uses\nassociation rule mining to find associations between courses that have been\nregistered by students in many previous semesters. The system successfully\ngenerates a list of association rules that guide a particular student to select\ncourses registered by similar students.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 19:28:35 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Shatnawi", "Raed", ""], ["Althebyan", "Qutaibah", ""], ["Ghalib", "Baraq", ""], ["Al-Maolegi", "Mohammed", ""]]}, {"id": "1407.2279", "submitter": "Gosta Grahne", "authors": "Gosta Grahne and Adrian Onet", "title": "The data-exchange chase under the microscope", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.6682", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we take closer look at recent developments for the chase\nprocedure, and provide additional results. Our analysis allows us create a\ntaxonomy of the chase variations and the properties they satisfy. Two of the\nmost central problems regarding the chase is termination, and discovery of\nrestricted classes of sets of dependencies that guarantee termination of the\nchase. The search for the restricted classes has been motivated by a fairly\nrecent result that shows that it is undecidable to determine whether the chase\nwith a given dependency set will terminate on a given instance. There is a\nsmall dissonance here, since the quest has been for classes of sets of\ndependencies guaranteeing termination of the chase on all instances, even\nthough the latter problem was not known to be undecidable. We resolve the\ndissonance in this paper by showing that determining whether the chase with a\ngiven set of dependencies terminates on all instances is coRE-complete. For the\nhardness proof we use a reduction from word rewriting systems, thereby also\nshowing the close connection between the chase and word rewriting. The same\nreduction also gives us the aforementioned instance-dependent RE-completeness\nresult as a byproduct. For one of the restricted classes guaranteeing\ntermination on all instances, the stratified sets dependencies, we provide new\ncomplexity results for the problem of testing whether a given set of\ndependencies belongs to it. These results rectify some previous claims that\nhave occurred in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 21:31:32 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Grahne", "Gosta", ""], ["Onet", "Adrian", ""]]}, {"id": "1407.2704", "submitter": "Yongrui Qin", "authors": "Yongrui Qin, Quan Z. Sheng, Nickolas J.G. Falkner, Schahram Dustdar,\n  Hua Wang and Athanasios V. Vasilakos", "title": "When Things Matter: A Data-Centric View of the Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advances in radio-frequency identification (RFID), low-cost\nwireless sensor devices, and Web technologies, the Internet of Things (IoT)\napproach has gained momentum in connecting everyday objects to the Internet and\nfacilitating machine-to-human and machine-to-machine communication with the\nphysical world. While IoT offers the capability to connect and integrate both\ndigital and physical entities, enabling a whole new class of applications and\nservices, several significant challenges need to be addressed before these\napplications and services can be fully realized. A fundamental challenge\ncenters around managing IoT data, typically produced in dynamic and volatile\nenvironments, which is not only extremely large in scale and volume, but also\nnoisy, and continuous. This article surveys the main techniques and\nstate-of-the-art research efforts in IoT from data-centric perspectives,\nincluding data stream processing, data storage models, complex event\nprocessing, and searching in IoT. Open research issues for IoT data management\nare also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 06:19:58 GMT"}, {"version": "v2", "created": "Mon, 21 Jul 2014 01:44:13 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Qin", "Yongrui", ""], ["Sheng", "Quan Z.", ""], ["Falkner", "Nickolas J. G.", ""], ["Dustdar", "Schahram", ""], ["Wang", "Hua", ""], ["Vasilakos", "Athanasios V.", ""]]}, {"id": "1407.2845", "submitter": "Emilio Ferrara", "authors": "Santa Agreste, Pasquale De Meo, Emilio Ferrara, Domenico Ursino", "title": "XML Matchers: approaches and challenges", "comments": "34 pages, 8 tables, 7 figures", "journal-ref": "Knowledge-based systems 66: 190-209, 2014", "doi": "10.1016/j.knosys.2014.04.044", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schema Matching, i.e. the process of discovering semantic correspondences\nbetween concepts adopted in different data source schemas, has been a key topic\nin Database and Artificial Intelligence research areas for many years. In the\npast, it was largely investigated especially for classical database models\n(e.g., E/R schemas, relational databases, etc.). However, in the latest years,\nthe widespread adoption of XML in the most disparate application fields pushed\na growing number of researchers to design XML-specific Schema Matching\napproaches, called XML Matchers, aiming at finding semantic matchings between\nconcepts defined in DTDs and XSDs. XML Matchers do not just take well-known\ntechniques originally designed for other data models and apply them on\nDTDs/XSDs, but they exploit specific XML features (e.g., the hierarchical\nstructure of a DTD/XSD) to improve the performance of the Schema Matching\nprocess. The design of XML Matchers is currently a well-established research\narea. The main goal of this paper is to provide a detailed description and\nclassification of XML Matchers. We first describe to what extent the\nspecificities of DTDs/XSDs impact on the Schema Matching task. Then we\nintroduce a template, called XML Matcher Template, that describes the main\ncomponents of an XML Matcher, their role and behavior. We illustrate how each\nof these components has been implemented in some popular XML Matchers. We\nconsider our XML Matcher Template as the baseline for objectively comparing\napproaches that, at first glance, might appear as unrelated. The introduction\nof this template can be useful in the design of future XML Matchers. Finally,\nwe analyze commercial tools implementing XML Matchers and introduce two\nchallenging issues strictly related to this topic, namely XML source clustering\nand uncertainty management in XML Matchers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 16:14:11 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Agreste", "Santa", ""], ["De Meo", "Pasquale", ""], ["Ferrara", "Emilio", ""], ["Ursino", "Domenico", ""]]}, {"id": "1407.2899", "submitter": "Gabriela Montoya", "authors": "Gabriela Montoya (LINA), Hala Skaf-Molli (LINA), Pascal Molli (LINA),\n  Maria-Esther Vidal", "title": "Fedra: Query Processing for SPARQL Federations with Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data replication and deployment of local SPARQL endpoints improve scalability\nand availability of public SPARQL endpoints, making the consumption of Linked\nData a reality. This solution requires synchronization and specific query\nprocessing strategies to take advantage of replication. However, existing\nreplication aware techniques in federations of SPARQL endpoints do not consider\ndata dynamicity. We propose Fedra, an approach for querying federations of\nendpoints that benefits from replication. Participants in Fedra federations can\ncopy fragments of data from several datasets, and describe them using\nprovenance and views. These descriptions enable Fedra to reduce the number of\nselected endpoints while satisfying user divergence requirements. Experiments\non real-world datasets suggest savings of up to three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 18:39:47 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Montoya", "Gabriela", "", "LINA"], ["Skaf-Molli", "Hala", "", "LINA"], ["Molli", "Pascal", "", "LINA"], ["Vidal", "Maria-Esther", ""]]}, {"id": "1407.3191", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts, Samuel L. Ventura, Mauricio Sadinle, Stephen E.\n  Fienberg", "title": "A Comparison of Blocking Methods for Record Linkage", "comments": "22 pages, 2 tables, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage seeks to merge databases and to remove duplicates when unique\nidentifiers are not available. Most approaches use blocking techniques to\nreduce the computational complexity associated with record linkage. We review\ntraditional blocking techniques, which typically partition the records\naccording to a set of field attributes, and consider two variants of a method\nknown as locality sensitive hashing, sometimes referred to as \"private\nblocking.\" We compare these approaches in terms of their recall, reduction\nratio, and computational complexity. We evaluate these methods using different\nsynthetic datafiles and conclude with a discussion of privacy-related issues.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 15:06:03 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Ventura", "Samuel L.", ""], ["Sadinle", "Mauricio", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1407.3512", "submitter": "Radhakrishnan Delhibabu", "authors": "Radhakrishnan Delhibabu and Andreas Behrend", "title": "A New Rational Algorithm for View Updating in Relational Databases", "comments": "arXiv admin note: substantial text overlap with arXiv:1301.5154", "journal-ref": null, "doi": "10.1007/s10489-014-0579-0", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The dynamics of belief and knowledge is one of the major components of any\nautonomous system that should be able to incorporate new pieces of information.\nIn order to apply the rationality result of belief dynamics theory to various\npractical problems, it should be generalized in two respects: first it should\nallow a certain part of belief to be declared as immutable; and second, the\nbelief state need not be deductively closed. Such a generalization of belief\ndynamics, referred to as base dynamics, is presented in this paper, along with\nthe concept of a generalized revision algorithm for knowledge bases (Horn or\nHorn logic with stratified negation). We show that knowledge base dynamics has\nan interesting connection with kernel change via hitting set and abduction. In\nthis paper, we show how techniques from disjunctive logic programming can be\nused for efficient (deductive) database updates. The key idea is to transform\nthe given database together with the update request into a disjunctive\n(datalog) logic program and apply disjunctive techniques (such as minimal model\nreasoning) to solve the original update problem. The approach extends and\nintegrates standard techniques for efficient query answering and integrity\nchecking. The generation of a hitting set is carried out through a hyper\ntableaux calculus and magic set that is focused on the goal of minimality.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jul 2014 23:08:09 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Delhibabu", "Radhakrishnan", ""], ["Behrend", "Andreas", ""]]}, {"id": "1407.3685", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall, Jon Hills and Jason Lines", "title": "Finding Motif Sets in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": "CMPC14-03", "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series motifs are representative subsequences that occur frequently in a\ntime series; a motif set is the set of subsequences deemed to be instances of a\ngiven motif. We focus on finding motif sets. Our motivation is to detect motif\nsets in household electricity-usage profiles, representing repeated patterns of\nhousehold usage.\n  We propose three algorithms for finding motif sets. Two are greedy algorithms\nbased on pairwise comparison, and the third uses a heuristic measure of set\nquality to find the motif set directly. We compare these algorithms on\nsimulated datasets and on electricity-usage data. We show that Scan MK, the\nsimplest way of using the best-matching pair to find motif sets, is less\naccurate on our synthetic data than Set Finder and Cluster MK, although the\nlatter is very sensitive to parameter settings. We qualitatively analyse the\noutputs for the electricity-usage data and demonstrate that both Scan MK and\nSet Finder can discover useful motif sets in such data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 15:01:57 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Bagnall", "Anthony", ""], ["Hills", "Jon", ""], ["Lines", "Jason", ""]]}, {"id": "1407.3745", "submitter": "Sutanay Choudhury", "authors": "Sutanay Choudhury, Lawrence Holder, George Chin, Patrick Mackey,\n  Khushbu Agarwal, John Feo", "title": "Query Optimization for Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": "PNNL-SA-103238, Pacific Northwest National Laboratory, Richland, WA", "categories": "cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Given a query graph that represents a pattern of interest, the emerging\npattern detection problem can be viewed as a continuous query problem on a\ndynamic graph. We present an incremental algorithm for continuous query\nprocessing on dynamic graphs. The algorithm is based on the concept of query\ndecomposition; we decompose a query graph into smaller subgraphs and assemble\nthe result of sub-queries to find complete matches with the specified query.\nThe novelty of our work lies in using the subgraph distributional statistics\ncollected from the dynamic graph to generate the decomposition. We introduce a\n\"Lazy Search\" algorithm where the search strategy is decided on a\nvertex-to-vertex basis depending on the likelihood of a match in the vertex\nneighborhood. We also propose a metric named \"Relative Selectivity\" that is\nused to select between different query decomposition strategies. Our\nexperiments performed on real online news, network traffic stream and a\nsynthetic social network benchmark demonstrate 10-100x speedups over competing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 17:53:30 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Choudhury", "Sutanay", ""], ["Holder", "Lawrence", ""], ["Chin", "George", ""], ["Mackey", "Patrick", ""], ["Agarwal", "Khushbu", ""], ["Feo", "John", ""]]}, {"id": "1407.3850", "submitter": "Stephan G\\\"unnemann", "authors": "Stephan G\\\"unnemann, Hardy Kremer, Matthias Hannen, Thomas Seidl", "title": "KDD-SC: Subspace Clustering Extensions for Knowledge Discovery\n  Frameworks", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing high dimensional data is a challenging task. For these data it is\nknown that traditional clustering algorithms fail to detect meaningful\npatterns. As a solution, subspace clustering techniques have been introduced.\nThey analyze arbitrary subspace projections of the data to detect clustering\nstructures.\n  In this paper, we present our subspace clustering extension for KDD\nframeworks, termed KDD-SC. In contrast to existing subspace clustering\ntoolkits, our solution neither is a standalone product nor is it tightly\ncoupled to a specific KDD framework. Our extension is realized by a common\ncodebase and easy-to-use plugins for three of the most popular KDD frameworks,\nnamely KNIME, RapidMiner, and WEKA. KDD-SC extends these frameworks such that\nthey offer a wide range of different subspace clustering functionalities. It\nprovides a multitude of algorithms, data generators, evaluation measures, and\nvisualization techniques specifically designed for subspace clustering. These\nfunctionalities integrate seamlessly with the frameworks' existing features\nsuch that they can be flexibly combined. KDD-SC is publicly available on our\nwebsite.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 00:15:11 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["G\u00fcnnemann", "Stephan", ""], ["Kremer", "Hardy", ""], ["Hannen", "Matthias", ""], ["Seidl", "Thomas", ""]]}, {"id": "1407.3859", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Christian Anderson, William Arcand, David Bestor, Bill\n  Bergeron, Chansup Byun, Matthew Hubbell, Peter Michaleas, Julie Mullen, David\n  O'Gwynn, Andrew Prout, Albert Reuther, Antonio Rosa, Charles Yee (MIT)", "title": "D4M 2.0 Schema: A General Purpose High Performance Schema for the\n  Accumulo Database", "comments": "6 pages; IEEE HPEC 2013", "journal-ref": null, "doi": "10.1109/HPEC.2013.6670318", "report-no": null, "categories": "cs.DB astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-traditional, relaxed consistency, triple store databases are the backbone\nof many web companies (e.g., Google Big Table, Amazon Dynamo, and Facebook\nCassandra). The Apache Accumulo database is a high performance open source\nrelaxed consistency database that is widely used for government applications.\nObtaining the full benefits of Accumulo requires using novel schemas. The\nDynamic Distributed Dimensional Data Model (D4M)[http://d4m.mit.edu] provides a\nuniform mathematical framework based on associative arrays that encompasses\nboth traditional (i.e., SQL) and non-traditional databases. For non-traditional\ndatabases D4M naturally leads to a general purpose schema that can be used to\nfully index and rapidly query every unique string in a dataset. The D4M 2.0\nSchema has been applied with little or no customization to cyber,\nbioinformatics, scientific citation, free text, and social media data. The D4M\n2.0 Schema is simple, requires minimal parsing, and achieves the highest\npublished Accumulo ingest rates. The benefits of the D4M 2.0 Schema are\nindependent of the D4M interface. Any interface to Accumulo can achieve these\nbenefits by using the D4M 2.0 Schema\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 01:54:45 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kepner", "Jeremy", "", "MIT"], ["Anderson", "Christian", "", "MIT"], ["Arcand", "William", "", "MIT"], ["Bestor", "David", "", "MIT"], ["Bergeron", "Bill", "", "MIT"], ["Byun", "Chansup", "", "MIT"], ["Hubbell", "Matthew", "", "MIT"], ["Michaleas", "Peter", "", "MIT"], ["Mullen", "Julie", "", "MIT"], ["O'Gwynn", "David", "", "MIT"], ["Prout", "Andrew", "", "MIT"], ["Reuther", "Albert", "", "MIT"], ["Rosa", "Antonio", "", "MIT"], ["Yee", "Charles", "", "MIT"]]}, {"id": "1407.4765", "submitter": "Leif Walsh", "authors": "Zardosht Kasheff, Leif Walsh", "title": "Ark: A Real-World Consensus Implementation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ark is an implementation of a consensus algorithm similar to Paxos and Raft,\ndesigned as an improvement over the existing consensus algorithm used by\nMongoDB and TokuMX.\n  Ark was designed from first principles, improving on the election algorithm\nused by TokuMX, to fix deficiencies in MongoDB's consensus algorithms that can\ncause data loss. It ultimately has many similarities with Raft, but diverges in\na few ways, mainly to support other features like chained replication and\nunacknowledged writes.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 18:32:31 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["Kasheff", "Zardosht", ""], ["Walsh", "Leif", ""]]}, {"id": "1407.4831", "submitter": "Wolfgang Stock", "authors": "Laura Schumann and Wolfgang G. Stock", "title": "The Information Service Evaluation (ISE) Model", "comments": "20 pp", "journal-ref": "Webology, 11(1), June, 2014, Art. 115", "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Information services are an inherent part of our everyday life. Especially\nsince ubiquitous cities are being developed all over the world their number is\nincreasing even faster. They aim at facilitating the production of information\nand the access to the needed information and are supposed to make life easier.\nUntil today many different evaluation models (among others, TAM, TAM 2, TAM 3,\nUTAUT and MATH) have been developed to measure the quality and acceptance of\nthese services. Still, they only consider subareas of the whole concept that\nrepresents an information service. As a holistic and comprehensive approach,\nthe ISE Model studies five dimensions that influence adoption, use, impact and\ndiffusion of the information service: information service quality, information\nuser, information acceptance, information environment and time. All these\naspects have a great impact on the final grading and of the success (or\nfailure) of the service. Our model combines approaches, which study subjective\nimpressions of users (e.g., the perceived service quality), and\nuser-independent, more objective approaches (e.g., the degree of gamification\nof a system). Furthermore, we adopt results of network economics, especially\nthe Success breeds success-principle.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:29:21 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Schumann", "Laura", ""], ["Stock", "Wolfgang G.", ""]]}, {"id": "1407.5553", "submitter": "Jerome Le Ny", "authors": "Jerome Le Ny", "title": "Privacy-Preserving Filtering for Event Streams", "comments": "This version subsumes both the previous version and arXiv:1304.2313", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale information systems such as intelligent transportation\nsystems, smart grids or smart buildings collect data about the activities of\ntheir users to optimize their operations. To encourage participation and\nadoption of these systems, it is becoming increasingly important that the\ndesign process take privacy issues into consideration. In a typical scenario,\nsignals originate from many sensors capturing events involving the users, and\nseveral statistics of interest need to be continuously published in real-time.\nThis paper considers the problem of providing differential privacy guarantees\nfor such multi-input multi-output systems processing event streams. We show how\nto construct and optimize various extensions of the zero-forcing equalization\nmechanism, which we previously proposed for single-input single-output systems.\nSome of these extensions can take a model of the input signals into account. We\nillustrate our privacy-preserving filter design methodology through the problem\nof privately monitoring and forecasting occupancy in a building equipped with\nmultiple motion detection sensors.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 16:31:21 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 15:17:41 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Ny", "Jerome Le", ""]]}, {"id": "1407.5661", "submitter": "Scott Sawyer", "authors": "Scott M. Sawyer and B. David O'Gwynn", "title": "Evaluating Accumulo Performance for a Scalable Cyber Data Processing\n  Pipeline", "comments": "To appear at 2014 IEEE High Performance Extreme Computing Conference\n  (HPEC '14)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming, big data applications face challenges in creating scalable data\nflow pipelines, in which multiple data streams must be collected, stored,\nqueried, and analyzed. These data sources are characterized by their volume (in\nterms of dataset size), velocity (in terms of data rates), and variety (in\nterms of fields and types). For many applications, distributed NoSQL databases\nare effective alternatives to traditional relational database management\nsystems. This paper considers a cyber situational awareness system that uses\nthe Apache Accumulo database to provide scalable data warehousing, real-time\ndata ingest, and responsive querying for human users and analytic algorithms.\nWe evaluate Accumulo's ingestion scalability as a function of number of client\nprocesses and servers. We also describe a flexible data model with effective\ntechniques for query planning and query batching to deliver responsive results.\nQuery performance is evaluated in terms of latency of the client receiving\ninitial result sets. Accumulo performance is measured on a database of up to 8\nnodes using real cyber data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 20:34:32 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Sawyer", "Scott M.", ""], ["O'Gwynn", "B. David", ""]]}, {"id": "1407.6350", "submitter": "Ewa J. Infeld", "authors": "Ewa J. Infeld", "title": "Symmetric Disclosure: a Fresh Look at k-Anonymity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We analyze how the sparsity of a typical aggregate social relation impacts\nthe network overhead of online communication systems designed to provide\nk-anonymity. Once users are grouped in anonymity sets there will likely be few\nrelated pairs of users between any two particular sets, and so the sets need to\nbe large in order to provide cover traffic between them. We can reduce the\nassociated overhead by having both parties in a communication specify both the\norigin and the target sets of the communication. We propose to call this\ncommunication primitive \"symmetric disclosure.\" If in order to retrieve\nmessages a user specifies a group from which he expects to receive them, the\nnegative impact of the sparsity is offset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 13:15:14 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Infeld", "Ewa J.", ""]]}, {"id": "1407.6439", "submitter": "Ce Zhang", "authors": "Christopher R\\'e, Amir Abbas Sadeghian, Zifei Shan, Jaeho Shin, Feiran\n  Wang, Sen Wu, Ce Zhang", "title": "Feature Engineering for Knowledge Base Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge base construction (KBC) is the process of populating a knowledge\nbase, i.e., a relational database together with inference rules, with\ninformation extracted from documents and structured sources. KBC blurs the\ndistinction between two traditional database problems, information extraction\nand information integration. For the last several years, our group has been\nbuilding knowledge bases with scientific collaborators. Using our approach, we\nhave built knowledge bases that have comparable and sometimes better quality\nthan those constructed by human volunteers. In contrast to these knowledge\nbases, which took experts a decade or more human years to construct, many of\nour projects are constructed by a single graduate student.\n  Our approach to KBC is based on joint probabilistic inference and learning,\nbut we do not see inference as either a panacea or a magic bullet: inference is\na tool that allows us to be systematic in how we construct, debug, and improve\nthe quality of such systems. In addition, inference allows us to construct\nthese systems in a more loosely coupled way than traditional approaches. To\nsupport this idea, we have built the DeepDive system, which has the design goal\nof letting the user \"think about features---not algorithms.\" We think of\nDeepDive as declarative in that one specifies what they want but not how to get\nit. We describe our approach with a focus on feature engineering, which we\nargue is an understudied problem relative to its importance to end-to-end\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 03:34:41 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 17:00:00 GMT"}, {"version": "v3", "created": "Thu, 18 Sep 2014 14:38:06 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["R\u00e9", "Christopher", ""], ["Sadeghian", "Amir Abbas", ""], ["Shan", "Zifei", ""], ["Shin", "Jaeho", ""], ["Wang", "Feiran", ""], ["Wu", "Sen", ""], ["Zhang", "Ce", ""]]}, {"id": "1407.6812", "submitter": "Robert Hoehndorf", "authors": "Robert Hoehndorf and Luke Slater and Paul N. Schofield and Georgios V.\n  Gkoutos", "title": "Aber-OWL: a framework for ontology-based data access in biology", "comments": null, "journal-ref": null, "doi": "10.1186/s12859-015-0456-9", "report-no": null, "categories": "cs.DB cs.IR q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many ontologies have been developed in biology and these ontologies\nincreasingly contain large volumes of formalized knowledge commonly expressed\nin the Web Ontology Language (OWL). Computational access to the knowledge\ncontained within these ontologies relies on the use of automated reasoning. We\nhave developed the Aber-OWL infrastructure that provides reasoning services for\nbio-ontologies. Aber-OWL consists of an ontology repository, a set of web\nservices and web interfaces that enable ontology-based semantic access to\nbiological data and literature. Aber-OWL is freely available at\nhttp://aber-owl.net.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 08:33:12 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Hoehndorf", "Robert", ""], ["Slater", "Luke", ""], ["Schofield", "Paul N.", ""], ["Gkoutos", "Georgios V.", ""]]}, {"id": "1407.7223", "submitter": "Zhanpeng Huang", "authors": "Zhanpeng Huang, Pan Hui, Christoph Peylo", "title": "When Augmented Reality Meets Big Data", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With computing and sensing woven into the fabric of everyday life, we live in\nan era where we are awash in a flood of data from which we can gain rich\ninsights. Augmented reality (AR) is able to collect and help analyze the\ngrowing torrent of data about user engagement metrics within our personal\nmobile and wearable devices. This enables us to blend information from our\nsenses and the digitalized world in a myriad of ways that was not possible\nbefore. AR and big data have a logical maturity that inevitably converge them.\nThe tread of harnessing AR and big data to breed new interesting applications\nis starting to have a tangible presence. In this paper, we explore the\npotential to capture value from the marriage between AR and big data\ntechnologies, following with several challenges that must be addressed to fully\nrealize this potential.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jul 2014 13:21:10 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Huang", "Zhanpeng", ""], ["Hui", "Pan", ""], ["Peylo", "Christoph", ""]]}, {"id": "1407.8515", "submitter": "Suryakant Patil Dr", "authors": "Suryakant B. Patil, Vijay S. Suryawanshi, Dipali V. Suryawanshi,\n  Preeti Patil", "title": "Integrated ERP System for Improving the Functional efficiency of the\n  organization by Customized Architecture", "comments": "9 Pages; IJCEA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ERP is a kind of package which consist front end and backend as DBMS like\na collection of DBMSs. You can create DBMS to manage one aspect of your\nbusiness. For example, a publishing house has a database of books that keeps\ninformation about books such as Author Name, Title, Translator Name, etc. But\nthis database app only helps enter books data and search them. It doesn't help\nthem, for example, sell books. They get or develop another DBMS database that\nhas all the Books data plus prices, discount formulas, names of common clients,\netc. Now they connect the Books database to Sales database and maybe also the\ninventory database. Now its DBMS slowly turning into an ERP. They may add\npayroll database and connect it to this ERP. They may develop sales staff and\ncommissions database and connect it to this ERP and so on. In the traditional\nDatabase management system the different databases are used for the various\nCampuses of the JSPM Group of Education like Wagholi Campus, Tathwade Campus,\nNarhe Campus, Hadpsar Campuses, Bhavdhan Campus as well as Corporate office at\nKatraj of same organization so it is not possible to keep different databases\nfor the same so in this paper proposed the use of Integrated Database for the\nEntire organization using ERP system. The Proposed ERP system applied on the\nexisting Architecture of the JSPM Group; the marginal difference observed in\nthe Databases need to be accessed to generate the same number of Reports when\nuse the Traditional DBMS which end up with improvement in the Functional\nefficiency of Organizational Architecture.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 05:33:53 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Patil", "Suryakant B.", ""], ["Suryawanshi", "Vijay S.", ""], ["Suryawanshi", "Dipali V.", ""], ["Patil", "Preeti", ""]]}]