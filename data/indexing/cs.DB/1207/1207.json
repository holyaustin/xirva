[{"id": "1207.0132", "submitter": "Sunita Sarawagi", "authors": "Rakesh Pimplikar, Sunita Sarawagi", "title": "Answering Table Queries on the Web using Column Keywords", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  908-919 (2012)", "doi": "10.14778/2336664.2336665", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design of a structured search engine which returns a\nmulti-column table in response to a query consisting of keywords describing\neach of its columns. We answer such queries by exploiting the millions of\ntables on the Web because these are much richer sources of structured knowledge\nthan free-format text. However, a corpus of tables harvested from arbitrary\nHTML web pages presents huge challenges of diversity and redundancy not seen in\ncentrally edited knowledge bases. We concentrate on one concrete task in this\npaper. Given a set of Web tables T1, . . ., Tn, and a query Q with q sets of\nkeywords Q1, . . ., Qq, decide for each Ti if it is relevant to Q and if so,\nidentify the mapping between the columns of Ti and query columns. We represent\nthis task as a graphical model that jointly maps all tables by incorporating\ndiverse sources of clues spanning matches in different parts of the table,\ncorpus-wide co-occurrence statistics, and content overlap across table columns.\nWe define a novel query segmentation model for matching keywords to table\ncolumns, and a robust mechanism of exploiting content overlap across table\ncolumns. We design efficient inference algorithms based on bipartite matching\nand constrained graph cuts to solve the joint labeling task. Experiments on a\nworkload of 59 queries over a 25 million web table corpus shows significant\nboost in accuracy over baseline IR methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:10:21 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Pimplikar", "Rakesh", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "1207.0134", "submitter": "Lukas Blunschi", "authors": "Lukas Blunschi, Claudio Jossen, Donald Kossman, Magdalini Mori, Kurt\n  Stockinger", "title": "SODA: Generating SQL for Business Users", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  932-943 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of data warehouses is to enable business analysts to make better\ndecisions. Over the years the technology has matured and data warehouses have\nbecome extremely successful. As a consequence, more and more data has been\nadded to the data warehouses and their schemas have become increasingly\ncomplex. These systems still work great in order to generate pre-canned\nreports. However, with their current complexity, they tend to be a poor match\nfor non tech-savvy business analysts who need answers to ad-hoc queries that\nwere not anticipated. This paper describes the design, implementation, and\nexperience of the SODA system (Search over DAta Warehouse). SODA bridges the\ngap between the business needs of analysts and the technical complexity of\ncurrent data warehouses. SODA enables a Google-like search experience for data\nwarehouses by taking keyword queries of business users and automatically\ngenerating executable SQL. The key idea is to use a graph pattern matching\nalgorithm that uses the metadata model of the data warehouse. Our results with\nreal data from a global player in the financial services industry show that\nSODA produces queries with high precision and recall, and makes it much easier\nfor business users to interactively explore highly-complex data warehouses.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:15:28 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Blunschi", "Lukas", ""], ["Jossen", "Claudio", ""], ["Kossman", "Donald", ""], ["Mori", "Magdalini", ""], ["Stockinger", "Kurt", ""]]}, {"id": "1207.0135", "submitter": "Manolis Terrovitis", "authors": "Manolis Terrovitis, John Liagouris, Nikos Mamoulis, Spiros\n  Skiadopoulos", "title": "Privacy Preservation by Disassociation", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  944-955 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on protection against identity disclosure in the\npublication of sparse multidimensional data. Existing multidimensional\nanonymization techniquesa) protect the privacy of users either by altering the\nset of quasi-identifiers of the original data (e.g., by generalization or\nsuppression) or by adding noise (e.g., using differential privacy) and/or (b)\nassume a clear distinction between sensitive and non-sensitive information and\nsever the possible linkage. In many real world applications the above\ntechniques are not applicable. For instance, consider web search query logs.\nSuppressing or generalizing anonymization methods would remove the most\nvaluable information in the dataset: the original query terms. Additionally,\nweb search query logs contain millions of query terms which cannot be\ncategorized as sensitive or non-sensitive since a term may be sensitive for a\nuser and non-sensitive for another. Motivated by this observation, we propose\nan anonymization technique termed disassociation that preserves the original\nterms but hides the fact that two or more different terms appear in the same\nrecord. We protect the users' privacy by disassociating record terms that\nparticipate in identifying combinations. This way the adversary cannot\nassociate with high probability a record with a rare combination of terms. To\nthe best of our knowledge, our proposal is the first to employ such a technique\nto provide protection against identity disclosure. We propose an anonymization\nalgorithm based on our approach and evaluate its performance on real and\nsynthetic datasets, comparing it against other state-of-the-art methods based\non generalization and differential privacy.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:16:16 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Terrovitis", "Manolis", ""], ["Liagouris", "John", ""], ["Mamoulis", "Nikos", ""], ["Skiadopoulos", "Spiros", ""]]}, {"id": "1207.0136", "submitter": "Bhargav Kanagal", "authors": "Bhargav Kanagal, Amr Ahmed, Sandeep Pandey, Vanja Josifovski, Jeff\n  Yuan, Lluis Garcia-Pueyo", "title": "Supercharging Recommender Systems using Taxonomies for Learning User\n  Purchase Behavior", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  956-967 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems based on latent factor models have been effectively used\nfor understanding user interests and predicting future actions. Such models\nwork by projecting the users and items into a smaller dimensional space,\nthereby clustering similar users and items together and subsequently compute\nsimilarity between unknown user-item pairs. When user-item interactions are\nsparse (sparsity problem) or when new items continuously appear (cold start\nproblem), these models perform poorly. In this paper, we exploit the\ncombination of taxonomies and latent factor models to mitigate these issues and\nimprove recommendation accuracy. We observe that taxonomies provide structure\nsimilar to that of a latent factor model: namely, it imposes human-labeled\ncategories (clusters) over items. This leads to our proposed taxonomy-aware\nlatent factor model (TF) which combines taxonomies and latent factors using\nadditive models. We develop efficient algorithms to train the TF models, which\nscales to large number of users/items and develop scalable\ninference/recommendation algorithms by exploiting the structure of the\ntaxonomy. In addition, we extend the TF model to account for the temporal\ndynamics of user interests using high-order Markov chains. To deal with\nlarge-scale data, we develop a parallel multi-core implementation of our TF\nmodel. We empirically evaluate the TF model for the task of predicting user\npurchases using a real-world shopping dataset spanning more than a million\nusers and products. Our experiments demonstrate the benefits of using our TF\nmodels over existing approaches, in terms of both prediction accuracy and\nrunning time.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:17:05 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Kanagal", "Bhargav", ""], ["Ahmed", "Amr", ""], ["Pandey", "Sandeep", ""], ["Josifovski", "Vanja", ""], ["Yuan", "Jeff", ""], ["Garcia-Pueyo", "Lluis", ""]]}, {"id": "1207.0137", "submitter": "Christoph Koch", "authors": "Yanif Ahmad, Oliver Kennedy, Christoph Koch, Milos Nikolic", "title": "DBToaster: Higher-order Delta Processing for Dynamic, Frequently Fresh\n  Views", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  968-979 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications ranging from algorithmic trading to scientific data analysis\nrequire realtime analytics based on views over databases that change at very\nhigh rates. Such views have to be kept fresh at low maintenance cost and\nlatencies. At the same time, these views have to support classical SQL, rather\nthan window semantics, to enable applications that combine current with aged or\nhistorical data. In this paper, we present viewlet transforms, a recursive\nfinite differencing technique applied to queries. The viewlet transform\nmaterializes a query and a set of its higher-order deltas as views. These views\nsupport each other's incremental maintenance, leading to a reduced overall view\nmaintenance cost. The viewlet transform of a query admits efficient evaluation,\nthe elimination of certain expensive query operations, and aggressive\nparallelization. We develop viewlet transforms into a workable query execution\ntechnique, present a heuristic and cost-based optimization framework, and\nreport on experiments with a prototype dynamic data management system that\ncombines viewlet transforms with an optimizing compilation technique. The\nsystem supports tens of thousands of complete view refreshes a second for a\nwide range of queries.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:17:47 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Ahmad", "Yanif", ""], ["Kennedy", "Oliver", ""], ["Koch", "Christoph", ""], ["Nikolic", "Milos", ""]]}, {"id": "1207.0138", "submitter": "Manoj Agarwal", "authors": "Manoj K Agarwal, Krithi Ramamritham, Manish Bhide", "title": "Real Time Discovery of Dense Clusters in Highly Dynamic Graphs:\n  Identifying Real World Events in Highly Dynamic Environments", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  980-991 (2012)", "doi": null, "report-no": null, "categories": "cs.DB cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their real time nature, microblog streams are a rich source of dynamic\ninformation, for example, about emerging events. Existing techniques for\ndiscovering such events from a microblog stream in real time (such as Twitter\ntrending topics), have several lacunae when used for discovering emerging\nevents; extant graph based event detection techniques are not practical in\nmicroblog settings due to their complexity; and conventional techniques, which\nhave been developed for blogs, web-pages, etc., involving the use of keyword\nsearch, are only useful for finding information about known events. Hence, in\nthis paper, we present techniques to discover events that are unraveling in\nmicroblog message streams in real time so that such events can be reported as\nsoon as they occur. We model the problem as discovering dense clusters in\nhighly dynamic graphs. Despite many recent advances in graph analysis, ours is\nthe first technique to identify dense clusters in massive and highly dynamic\ngraphs in real time. Given the characteristics of microblog streams, in order\nto find clusters without missing any events, we propose and exploit a novel\ngraph property which we call short-cycle property. Our algorithms find these\nclusters efficiently in spite of rapid changes to the microblog streams.\nFurther we present a novel ranking function to identify the important events.\nBesides proving the correctness of our algorithms we show their practical\nutility by evaluating them using real world microblog data. These demonstrate\nour technique's ability to discover, with high precision and recall, emerging\nevents in high intensity data streams in real time. Many recent web\napplications create data which can be represented as massive dynamic graphs.\nOur technique can be easily extended to discover, in real time, interesting\npatterns in such graphs.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:18:29 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Agarwal", "Manoj K", ""], ["Ramamritham", "Krithi", ""], ["Bhide", "Manish", ""]]}, {"id": "1207.0139", "submitter": "Odysseas Papapetrou", "authors": "Odysseas Papapetrou, Minos Garofalakis, Antonios Deligiannakis", "title": "Sketch-based Querying of Distributed Sliding-Window Data Streams", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  992-1003 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While traditional data-management systems focus on evaluating single, ad-hoc\nqueries over static data sets in a centralized setting, several emerging\napplications require (possibly, continuous) answers to queries on dynamic data\nthat is widely distributed and constantly updated. Furthermore, such query\nanswers often need to discount data that is \"stale\", and operate solely on a\nsliding window of recent data arrivals (e.g., data updates occurring over the\nlast 24 hours). Such distributed data streaming applications mandate novel\nalgorithmic solutions that are both time- and space-efficient (to manage\nhigh-speed data streams), and also communication-efficient (to deal with\nphysical data distribution). In this paper, we consider the problem of complex\nquery answering over distributed, high-dimensional data streams in the\nsliding-window model. We introduce a novel sketching technique (termed\nECM-sketch) that allows effective summarization of streaming data over both\ntime-based and count-based sliding windows with probabilistic accuracy\nguarantees. Our sketch structure enables point as well as inner-product\nqueries, and can be employed to address a broad range of problems, such as\nmaintaining frequency statistics, finding heavy hitters, and computing\nquantiles in the sliding-window model. Focusing on distributed environments, we\ndemonstrate how ECM-sketches of individual, local streams can be composed to\ngenerate a (low-error) ECM-sketch summary of the order-preserving aggregation\nof all streams; furthermore, we show how ECM-sketches can be exploited for\ncontinuous monitoring of sliding-window queries over distributed streams. Our\nextensive experimental study with two real-life data sets validates our\ntheoretical claims and verifies the effectiveness of our techniques. To the\nbest of our knowledge, ours is the first work to address efficient,\nguaranteed-error complex query answ...[truncated].\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:19:09 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Papapetrou", "Odysseas", ""], ["Garofalakis", "Minos", ""], ["Deligiannakis", "Antonios", ""]]}, {"id": "1207.0140", "submitter": "Hoang Tam Vo", "authors": "Hoang Tam Vo, Sheng Wang, Divyakant Agrawal, Gang Chen, Beng Chin Ooi", "title": "LogBase: A Scalable Log-structured Database System in the Cloud", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  1004-1015 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous applications such as financial transactions (e.g., stock trading)\nare write-heavy in nature. The shift from reads to writes in web applications\nhas also been accelerating in recent years. Write-ahead-logging is a common\napproach for providing recovery capability while improving performance in most\nstorage systems. However, the separation of log and application data incurs\nwrite overheads observed in write-heavy environments and hence adversely\naffects the write throughput and recovery time in the system. In this paper, we\nintroduce LogBase - a scalable log-structured database system that adopts\nlog-only storage for removing the write bottleneck and supporting fast system\nrecovery. LogBase is designed to be dynamically deployed on commodity clusters\nto take advantage of elastic scaling property of cloud environments. LogBase\nprovides in-memory multiversion indexes for supporting efficient access to data\nmaintained in the log. LogBase also supports transactions that bundle read and\nwrite operations spanning across multiple records. We implemented the proposed\nsystem and compared it with HBase and a disk-based log-structured\nrecord-oriented system modeled after RAMCloud. The experimental results show\nthat LogBase is able to provide sustained write throughput, efficient data\naccess out of the cache, and effective system recovery.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:19:50 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Vo", "Hoang Tam", ""], ["Wang", "Sheng", ""], ["Agrawal", "Divyakant", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1207.0141", "submitter": "Beng Chin Ooi", "authors": "Wei Lu, Yanyan Shen, Su Chen, Beng Chin Ooi", "title": "Efficient Processing of k Nearest Neighbor Joins using MapReduce", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  1016-1027 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k nearest neighbor join (kNN join), designed to find k nearest neighbors from\na dataset S for every object in another dataset R, is a primitive operation\nwidely adopted by many data mining applications. As a combination of the k\nnearest neighbor query and the join operation, kNN join is an expensive\noperation. Given the increasing volume of data, it is difficult to perform a\nkNN join on a centralized machine efficiently. In this paper, we investigate\nhow to perform kNN join using MapReduce which is a well-accepted framework for\ndata-intensive applications over clusters of computers. In brief, the mappers\ncluster objects into groups; the reducers perform the kNN join on each group of\nobjects separately. We design an effective mapping mechanism that exploits\npruning rules for distance filtering, and hence reduces both the shuffling and\ncomputational costs. To reduce the shuffling cost, we propose two approximate\nalgorithms to minimize the number of replicas. Extensive experiments on our\nin-house cluster demonstrate that our proposed methods are efficient, robust\nand scalable.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:20:31 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Lu", "Wei", ""], ["Shen", "Yanyan", ""], ["Chen", "Su", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1207.0142", "submitter": "Nikolay Laptev", "authors": "Nikolay Laptev, Kai Zeng, Carlo Zaniolo", "title": "Early Accurate Results for Advanced Analytics on MapReduce", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  1028-1039 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate results based on samples often provide the only way in which\nadvanced analytical applications on very massive data sets can satisfy their\ntime and resource constraints. Unfortunately, methods and tools for the\ncomputation of accurate early results are currently not supported in\nMapReduce-oriented systems although these are intended for `big data'.\nTherefore, we proposed and implemented a non-parametric extension of Hadoop\nwhich allows the incremental computation of early results for arbitrary\nwork-flows, along with reliable on-line estimates of the degree of accuracy\nachieved so far in the computation. These estimates are based on a technique\ncalled bootstrapping that has been widely employed in statistics and can be\napplied to arbitrary functions and data distributions. In this paper, we\ndescribe our Early Accurate Result Library (EARL) for Hadoop that was designed\nto minimize the changes required to the MapReduce framework. Various tests of\nEARL of Hadoop are presented to characterize the frequent situations where EARL\ncan provide major speed-ups over the current version of Hadoop.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:21:10 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Laptev", "Nikolay", ""], ["Zeng", "Kai", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1207.0143", "submitter": "Xuan Liu", "authors": "Xuan Liu, Meiyu Lu, Beng Chin Ooi, Yanyan Shen, Sai Wu, Meihui Zhang", "title": "CDAS: A Crowdsourcing Data Analytics System", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  1040-1051 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some complex problems, such as image tagging and natural language processing,\nare very challenging for computers, where even state-of-the-art technology is\nyet able to provide satisfactory accuracy. Therefore, rather than relying\nsolely on developing new and better algorithms to handle such tasks, we look to\nthe crowdsourcing solution -- employing human participation -- to make good the\nshortfall in current technology. Crowdsourcing is a good supplement to many\ncomputer tasks. A complex job may be divided into computer-oriented tasks and\nhuman-oriented tasks, which are then assigned to machines and humans\nrespectively. To leverage the power of crowdsourcing, we design and implement a\nCrowdsourcing Data Analytics System, CDAS. CDAS is a framework designed to\nsupport the deployment of various crowdsourcing applications. The core part of\nCDAS is a quality-sensitive answering model, which guides the crowdsourcing\nengine to process and monitor the human tasks. In this paper, we introduce the\nprinciples of our quality-sensitive model. To satisfy user required accuracy,\nthe model guides the crowdsourcing query engine for the design and processing\nof the corresponding crowdsourcing jobs. It provides an estimated accuracy for\neach generated result based on the human workers' historical performances. When\nverifying the quality of the result, the model employs an online strategy to\nreduce waiting time. To show the effectiveness of the model, we implement and\ndeploy two analytics jobs on CDAS, a twitter sentiment analytics job and an\nimage tagging job. We use real Twitter and Flickr data as our queries\nrespectively. We compare our approaches with state-of-the-art classification\nand image annotation techniques. The results show that the human-assisted\nmethods can indeed achieve a much higher accuracy. By embedding the\nquality-sensitive model into crowdsourcing query engine, we\neffectiv...[truncated].\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:21:52 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Liu", "Xuan", ""], ["Lu", "Meiyu", ""], ["Ooi", "Beng Chin", ""], ["Shen", "Yanyan", ""], ["Wu", "Sai", ""], ["Zhang", "Meihui", ""]]}, {"id": "1207.0144", "submitter": "Arnab Bhattacharya", "authors": "Mayank Sachan, Arnab Bhattacharya", "title": "Mining Statistically Significant Substrings using the Chi-Square\n  Statistic", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  1052-1063 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identification of statistically significant patterns in a\nsequence of data has been applied to many domains such as intrusion detection\nsystems, financial models, web-click records, automated monitoring systems,\ncomputational biology, cryptology, and text analysis. An observed pattern of\nevents is deemed to be statistically significant if it is unlikely to have\noccurred due to randomness or chance alone. We use the chi-square statistic as\na quantitative measure of statistical significance. Given a string of\ncharacters generated from a memoryless Bernoulli model, the problem is to\nidentify the substring for which the empirical distribution of single letters\ndeviates the most from the distribution expected from the generative Bernoulli\nmodel. This deviation is captured using the chi-square measure. The most\nsignificant substring (MSS) of a string is thus defined as the substring having\nthe highest chi-square value. Till date, to the best of our knowledge, there\ndoes not exist any algorithm to find the MSS in better than O(n^2) time, where\nn denotes the length of the string. In this paper, we propose an algorithm to\nfind the most significant substring, whose running time is O(n^{3/2}) with high\nprobability. We also study some variants of this problem such as finding the\ntop-t set, finding all substrings having chi-square greater than a fixed\nthreshold and finding the MSS among substrings greater than a given length. We\nexperimentally demonstrate the asymptotic behavior of the MSS on varying the\nstring size and alphabet size. We also describe some applications of our\nalgorithm on cryptology and real world data from finance and sports. Finally,\nwe compare our technique with the existing heuristics for finding the MSS.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:22:30 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Sachan", "Mayank", ""], ["Bhattacharya", "Arnab", ""]]}, {"id": "1207.0145", "submitter": "Martina-Cezara Albutiu", "authors": "Martina-Cezara Albutiu, Alfons Kemper, Thomas Neumann", "title": "Massively Parallel Sort-Merge Joins in Main Memory Multi-Core Database\n  Systems", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  1064-1075 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two emerging hardware trends will dominate the database system technology in\nthe near future: increasing main memory capacities of several TB per server and\nmassively parallel multi-core processing. Many algorithmic and control\ntechniques in current database technology were devised for disk-based systems\nwhere I/O dominated the performance. In this work we take a new look at the\nwell-known sort-merge join which, so far, has not been in the focus of research\nin scalable massively parallel multi-core data processing as it was deemed\ninferior to hash joins. We devise a suite of new massively parallel sort-merge\n(MPSM) join algorithms that are based on partial partition-based sorting.\nContrary to classical sort-merge joins, our MPSM algorithms do not rely on a\nhard to parallelize final merge step to create one complete sort order. Rather\nthey work on the independently created runs in parallel. This way our MPSM\nalgorithms are NUMA-affine as all the sorting is carried out on local memory\npartitions. An extensive experimental evaluation on a modern 32-core machine\nwith one TB of main memory proves the competitive performance of MPSM on large\nmain memory databases with billions of objects. It scales (almost) linearly in\nthe number of employed cores and clearly outperforms competing hash join\nproposals - in particular it outperforms the \"cutting-edge\" Vectorwise parallel\nquery engine by a factor of four.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:23:09 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Albutiu", "Martina-Cezara", ""], ["Kemper", "Alfons", ""], ["Neumann", "Thomas", ""]]}, {"id": "1207.0147", "submitter": "Tian Luo", "authors": "Tian Luo, Rubao Lee, Michael Mesnier, Feng Chen, Xiaodong Zhang", "title": "hStorage-DB: Heterogeneity-aware Data Management to Exploit the Full\n  Capability of Hybrid Storage Systems", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp.\n  1076-1087 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As storage systems become increasingly heterogeneous and complex, it adds\nburdens on DBAs, causing suboptimal performance even after a lot of human\nefforts have been made. In addition, existing monitoring-based storage\nmanagement by access pattern detections has difficulties to handle workloads\nthat are highly dynamic and concurrent. To achieve high performance by best\nutilizing heterogeneous storage devices, we have designed and implemented a\nheterogeneity-aware software framework for DBMS storage management called\nhStorage-DB, where semantic information that is critical for storage I/O is\nidentified and passed to the storage manager. According to the collected\nsemantic information, requests are classified into different types. Each type\nis assigned a proper QoS policy supported by the underlying storage system, so\nthat every request will be served with a suitable storage device. With\nhStorage-DB, we can well utilize semantic information that cannot be detected\nthrough data access monitoring but is particularly important for a hybrid\nstorage system. To show the effectiveness of hStorage-DB, we have implemented a\nsystem prototype that consists of an I/O request classification enabled DBMS,\nand a hybrid storage system that is organized into a two-level caching\nhierarchy. Our performance evaluation shows that hStorage-DB can automatically\nmake proper decisions for data allocation in different storage devices and make\nsubstantial performance improvements in a cost-efficient way.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 20:41:39 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Luo", "Tian", ""], ["Lee", "Rubao", ""], ["Mesnier", "Michael", ""], ["Chen", "Feng", ""], ["Zhang", "Xiaodong", ""]]}, {"id": "1207.0361", "submitter": "Arnab Bhattacharya", "authors": "Sourav Dutta and Arnab Bhattacharya", "title": "INSTRUCT: Space-Efficient Structure for Indexing and Complete Query\n  Management of String Databases", "comments": "International Conference on Management of Data (COMAD), 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous expanse of search engines, dictionary and thesaurus storage,\nand other text mining applications, combined with the popularity of readily\navailable scanning devices and optical character recognition tools, has\nnecessitated efficient storage, retrieval and management of massive text\ndatabases for various modern applications. For such applications, we propose a\nnovel data structure, INSTRUCT, for efficient storage and management of\nsequence databases. Our structure uses bit vectors for reusing the storage\nspace for common triplets, and hence, has a very low memory requirement.\nINSTRUCT efficiently handles prefix and suffix search queries in addition to\nthe exact string search operation by iteratively checking the presence of\ntriplets. We also propose an extension of the structure to handle substring\nsearch efficiently, albeit with an increase in the space requirements. This\nextension is important in the context of trie-based solutions which are unable\nto handle such queries efficiently. We perform several experiments portraying\nthat INSTRUCT outperforms the existing structures by nearly a factor of two in\nterms of space requirements, while the query times are better. The ability to\nhandle insertion and deletion of strings in addition to supporting all kinds of\nqueries including exact search, prefix/suffix search and substring search makes\nINSTRUCT a complete data structure.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2012 12:38:47 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2012 04:54:37 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Dutta", "Sourav", ""], ["Bhattacharya", "Arnab", ""]]}, {"id": "1207.0872", "submitter": "EPTCS", "authors": "Catuscia Palamidessi (INRIA and LIX, Ecole Polytechnique, France),\n  Marco Stronati (Universit\\`a di Pisa, Italy)", "title": "Differential Privacy for Relational Algebra: Improving the Sensitivity\n  Bounds via Constraint Systems", "comments": "In Proceedings QAPL 2012, arXiv:1207.0559", "journal-ref": "EPTCS 85, 2012, pp. 92-105", "doi": "10.4204/EPTCS.85.7", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a modern approach in privacy-preserving data analysis\nto control the amount of information that can be inferred about an individual\nby querying a database. The most common techniques are based on the\nintroduction of probabilistic noise, often defined as a Laplacian parametric on\nthe sensitivity of the query. In order to maximize the utility of the query, it\nis crucial to estimate the sensitivity as precisely as possible.\n  In this paper we consider relational algebra, the classical language for\nqueries in relational databases, and we propose a method for computing a bound\non the sensitivity of queries in an intuitive and compositional way. We use\nconstraint-based techniques to accumulate the information on the possible\nvalues for attributes provided by the various components of the query, thus\nmaking it possible to compute tight bounds on the sensitivity.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 01:24:57 GMT"}], "update_date": "2012-07-05", "authors_parsed": [["Palamidessi", "Catuscia", "", "INRIA and LIX, Ecole Polytechnique, France"], ["Stronati", "Marco", "", "Universit\u00e0 di Pisa, Italy"]]}, {"id": "1207.0913", "submitter": "Rong-Hua Li", "authors": "Rong-Hua Li, Jeffrey Xu Yu, Zechao Shang", "title": "Estimating Node Influenceability in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Influence analysis is a fundamental problem in social network analysis and\nmining. The important applications of the influence analysis in social network\ninclude influence maximization for viral marketing, finding the most\ninfluential nodes, online advertising, etc. For many of these applications, it\nis crucial to evaluate the influenceability of a node. In this paper, we study\nthe problem of evaluating influenceability of nodes in social network based on\nthe widely used influence spread model, namely, the independent cascade model.\nSince this problem is #P-complete, most existing work is based on Naive\nMonte-Carlo (\\nmc) sampling. However, the \\nmc estimator typically results in a\nlarge variance, which significantly reduces its effectiveness. To overcome this\nproblem, we propose two families of new estimators based on the idea of\nstratified sampling. We first present two basic stratified sampling (\\bss)\nestimators, namely \\bssi estimator and \\bssii estimator, which partition the\nentire population into $2^r$ and $r+1$ strata by choosing $r$ edges\nrespectively. Second, to further reduce the variance, we find that both \\bssi\nand \\bssii estimators can be recursively performed on each stratum, thus we\npropose two recursive stratified sampling (\\rss) estimators, namely \\rssi\nestimator and \\rssii estimator. Theoretically, all of our estimators are shown\nto be unbiased and their variances are significantly smaller than the variance\nof the \\nmc estimator. Finally, our extensive experimental results on both\nsynthetic and real datasets demonstrate the efficiency and accuracy of our new\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 06:49:22 GMT"}], "update_date": "2012-07-05", "authors_parsed": [["Li", "Rong-Hua", ""], ["Yu", "Jeffrey Xu", ""], ["Shang", "Zechao", ""]]}, {"id": "1207.1423", "submitter": "Eric P. Xing", "authors": "Eric P. Xing, Rong Yan, Alexander G. Hauptmann", "title": "Mining Associated Text and Images with Dual-Wing Harmoniums", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-633-641", "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-wing harmonium model for mining multimedia data that\nextends and improves on earlier models based on two-layer random fields, which\ncapture bidirectional dependencies between hidden topic aspects and observed\ninputs. This model can be viewed as an undirected counterpart of the two-layer\ndirected models such as LDA for similar tasks, but bears significant difference\nin inference/learning cost tradeoffs, latent topic representations, and topic\nmixing mechanisms. In particular, our model facilitates efficient inference and\nrobust topic mixing, and potentially provides high flexibilities in modeling\nthe latent topic spaces. A contrastive divergence and a variational algorithm\nare derived for learning. We specialized our model to a dual-wing harmonium for\ncaptioned images, incorporating a multivariate Poisson for word-counts and a\nmultivariate Gaussian for color histogram. We present empirical results on the\napplications of this model to classification, retrieval and image annotation on\nnews video collections, and we report an extensive comparison with various\nextant models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:28:40 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Xing", "Eric P.", ""], ["Yan", "Rong", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1207.1535", "submitter": "Telcia Niom", "authors": "Prof Rudresh Shirwaikar, Nikhil Rajadhyax", "title": "Data Mining on Educational Domain", "comments": "6 pages; http://www.ijascse.in/publications2012. arXiv admin note:\n  text overlap with arXiv:1201.3417 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Educational data mining (EDM) is defined as the area of scientific inquiry\ncentered around the development of methods for making discoveries within the\nunique kinds of data that come from educational settings, and using those\nmethods to better understand students and the settings which they learn in.\nData mining enables organizations to use their current reporting capabilities\nto uncover and understand hidden patterns in vast databases. As a result of\nthis insight, institutions are able to allocate resources and staff more\neffectively. In this paper, we present a real-world experiment conducted in\nShree Rayeshwar Institute of Engineering and Information Technology (SRIEIT) in\nGoa, India. Here we found the relevant subjects in an undergraduate syllabus\nand the strength of their relationship. We have also focused on classification\nof students into different categories such as good, average, poor depending on\ntheir marks scored by them by obtaining a decision tree which will predict the\nperformance of the students and accordingly help the weaker section of students\nto improve in their academics. We have also found clusters of students for\nhelping in analyzing student's performance and also improvising the subject\nteaching in that particular subject.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2012 07:06:06 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Shirwaikar", "Prof Rudresh", ""], ["Rajadhyax", "Nikhil", ""]]}, {"id": "1207.2189", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Owen Kaser, Eduardo Gutarra", "title": "Reordering Rows for Better Compression: Beyond the Lexicographic Order", "comments": "to appear in ACM TODS", "journal-ref": "ACM Trans. Database Syst. 37, 3, Article 20 (September 2012)", "doi": "10.1145/2338626.2338627", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting database tables before compressing them improves the compression\nrate. Can we do better than the lexicographical order? For minimizing the\nnumber of runs in a run-length encoding compression scheme, the best approaches\nto row-ordering are derived from traveling salesman heuristics, although there\nis a significant trade-off between running time and compression. A new\nheuristic, Multiple Lists, which is a variant on Nearest Neighbor that trades\noff compression for a major running-time speedup, is a good option for very\nlarge tables. However, for some compression schemes, it is more important to\ngenerate long runs rather than few runs. For this case, another novel\nheuristic, Vortex, is promising. We find that we can improve run-length\nencoding up to a factor of 3 whereas we can improve prefix coding by up to 80%:\nthese gains are on top of the gains due to lexicographically sorting the table.\nWe prove that the new row reordering is optimal (within 10%) at minimizing the\nruns of identical values within columns, in a few cases.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2012 21:47:13 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2012 16:21:15 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2014 18:46:09 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Lemire", "Daniel", ""], ["Kaser", "Owen", ""], ["Gutarra", "Eduardo", ""]]}, {"id": "1207.2619", "submitter": "Mutaz Al-Debei", "authors": "Mutaz M. Al-Debei, Mohammad Mourhaf Al Asswad, Sergio de Cesare and\n  Mark Lycett", "title": "Conceptual Modelling and The Quality of Ontologies: Endurantism Vs.\n  Perdurantism", "comments": null, "journal-ref": "International Journal of Database Management Systems, 4(3), 2012", "doi": "10.5121/ijdms.2012.4301", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies are key enablers for sharing precise and machine-understandable\nsemantics among different applications and parties. Yet, for ontologies to meet\nthese expectations, their quality must be of a good standard. The quality of an\nontology is strongly based on the design method employed. This paper addresses\nthe design problems related to the modelling of ontologies, with specific\nconcentration on the issues related to the quality of the conceptualisations\nproduced. The paper aims to demonstrate the impact of the modelling paradigm\nadopted on the quality of ontological models and, consequently, the potential\nimpact that such a decision can have in relation to the development of software\napplications. To this aim, an ontology that is conceptualised based on the\nObject-Role Modelling (ORM) approach (a representative of endurantism) is\nre-engineered into a one modelled on the basis of the Object Paradigm (OP) (a\nrepresentative of perdurantism). Next, the two ontologies are analytically\ncompared using the specified criteria. The conducted comparison highlights that\nusing the OP for ontology conceptualisation can provide more expressive,\nreusable, objective and temporal ontologies than those conceptualised on the\nbasis of the ORM approach.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 12:44:16 GMT"}], "update_date": "2012-07-13", "authors_parsed": [["Al-Debei", "Mutaz M.", ""], ["Asswad", "Mohammad Mourhaf Al", ""], ["de Cesare", "Sergio", ""], ["Lycett", "Mark", ""]]}, {"id": "1207.2837", "submitter": "Abdurashid Mamadolimov Ph.D.", "authors": "Abdurashid Mamadolimov", "title": "Search Algorithms for Conceptual Graph Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a database composed of a set of conceptual graphs. Using\nconceptual graphs and graph homomorphism it is possible to build a basic\nquery-answering mechanism based on semantic search. Graph homomorphism defines\na partial order over conceptual graphs. Since graph homomorphism checking is an\nNP-Complete problem, the main requirement for database organizing and managing\nalgorithms is to reduce the number of homomorphism checks. Searching is a basic\noperation for database manipulating problems. We consider the problem of\nsearching for an element in a partially ordered set. The goal is to minimize\nthe number of queries required to find a target element in the worst case.\nFirst we analyse conceptual graph database operations. Then we propose a new\nalgorithm for a subclass of lattices. Finally, we suggest a parallel search\nalgorithm for a general poset. Keywords. Conceptual Graph, Graph Homomorphism,\nPartial Order, Lattice, Search, Database.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 03:32:16 GMT"}], "update_date": "2012-07-13", "authors_parsed": [["Mamadolimov", "Abdurashid", ""]]}, {"id": "1207.2900", "submitter": "Rajesh Pasupuleti", "authors": "P. Rajesh, G. Narasimha, N. Saisumanth", "title": "Privacy Preserving MFI Based Similarity Measure For Hierarchical\n  Document Clustering", "comments": "6 pages,1 table,1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing nature of World Wide Web has imposed great challenges for\nresearchers in improving the search efficiency over the internet. Now days web\ndocument clustering has become an important research topic to provide most\nrelevant documents in huge volumes of results returned in response to a simple\nquery. In this paper, first we proposed a novel approach, to precisely define\nclusters based on maximal frequent item set (MFI) by Apriori algorithm.\nAfterwards utilizing the same maximal frequent item set (MFI) based similarity\nmeasure for Hierarchical document clustering. By considering maximal frequent\nitem sets, the dimensionality of document set is decreased. Secondly, providing\nprivacy preserving of open web documents is to avoiding duplicate documents.\nThere by we can protect the privacy of individual copy rights of documents.\nThis can be achieved using equivalence relation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 10:01:41 GMT"}], "update_date": "2012-07-13", "authors_parsed": [["Rajesh", "P.", ""], ["Narasimha", "G.", ""], ["Saisumanth", "N.", ""]]}, {"id": "1207.3532", "submitter": "Xifeng Yan Xifeng Yan", "authors": "Yang Li, Pegah Kamousi, Fangqiu Han, Shengqi Yang, Xifeng Yan, Subhash\n  Suri", "title": "Memory Efficient De Bruijn Graph Construction", "comments": "13 pages, 19 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively parallel DNA sequencing technologies are revolutionizing genomics\nresearch. Billions of short reads generated at low costs can be assembled for\nreconstructing the whole genomes. Unfortunately, the large memory footprint of\nthe existing de novo assembly algorithms makes it challenging to get the\nassembly done for higher eukaryotes like mammals. In this work, we investigate\nthe memory issue of constructing de Bruijn graph, a core task in leading\nassembly algorithms, which often consumes several hundreds of gigabytes memory\nfor large genomes. We propose a disk-based partition method, called Minimum\nSubstring Partitioning (MSP), to complete the task using less than 10 gigabytes\nmemory, without runtime slowdown. MSP breaks the short reads into multiple\nsmall disjoint partitions so that each partition can be loaded into memory,\nprocessed individually and later merged with others to form a de Bruijn graph.\nBy leveraging the overlaps among the k-mers (substring of length k), MSP\nachieves astonishing compression ratio: The total size of partitions is reduced\nfrom $\\Theta(kn)$ to $\\Theta(n)$, where $n$ is the size of the short read\ndatabase, and $k$ is the length of a $k$-mer. Experimental results show that\nour method can build de Bruijn graphs using a commodity computer for any\nlarge-volume sequence dataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2012 19:45:19 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Li", "Yang", ""], ["Kamousi", "Pegah", ""], ["Han", "Fangqiu", ""], ["Yang", "Shengqi", ""], ["Yan", "Xifeng", ""], ["Suri", "Subhash", ""]]}, {"id": "1207.4371", "submitter": "Klaus Berberich", "authors": "Klaus Berberich and Srikanta Bedathur", "title": "Computing n-Gram Statistics in MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics about n-grams (i.e., sequences of contiguous words or other tokens\nin text documents or other string data) are an important building block in\ninformation retrieval and natural language processing. In this work, we study\nhow n-gram statistics, optionally restricted by a maximum n-gram length and\nminimum collection frequency, can be computed efficiently harnessing MapReduce\nfor distributed data processing. We describe different algorithms, ranging from\nan extension of word counting, via methods based on the Apriori principle, to a\nnovel method Suffix-\\sigma that relies on sorting and aggregating suffixes. We\nexamine possible extensions of our method to support the notions of\nmaximality/closedness and to perform aggregations beyond occurrence counting.\nAssuming Hadoop as a concrete MapReduce implementation, we provide insights on\nan efficient implementation of the methods. Extensive experiments on The New\nYork Times Annotated Corpus and ClueWeb09 expose the relative benefits and\ntrade-offs of the methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 13:21:10 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Berberich", "Klaus", ""], ["Bedathur", "Srikanta", ""]]}, {"id": "1207.4525", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien, Konstantina Palla, Alex Davies, Gjergji Kasneci,\n  Thore Graepel, Zoubin Ghahramani", "title": "SiGMa: Simple Greedy Matching for Aligning Large Knowledge Bases", "comments": "10 pages + 2 pages appendix; 5 figures -- initial preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has enabled the creation of a growing number of large-scale\nknowledge bases in a variety of domains containing complementary information.\nTools for automatically aligning these knowledge bases would make it possible\nto unify many sources of structured knowledge and answer complex queries.\nHowever, the efficient alignment of large-scale knowledge bases still poses a\nconsiderable challenge. Here, we present Simple Greedy Matching (SiGMa), a\nsimple algorithm for aligning knowledge bases with millions of entities and\nfacts. SiGMa is an iterative propagation algorithm which leverages both the\nstructural information from the relationship graph as well as flexible\nsimilarity measures between entity properties in a greedy local search, thus\nmaking it scalable. Despite its greedy nature, our experiments indicate that\nSiGMa can efficiently match some of the world's largest knowledge bases with\nhigh precision. We provide additional experiments on benchmark datasets which\ndemonstrate that SiGMa can outperform state-of-the-art approaches both in\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 00:15:05 GMT"}], "update_date": "2012-07-20", "authors_parsed": [["Lacoste-Julien", "Simon", ""], ["Palla", "Konstantina", ""], ["Davies", "Alex", ""], ["Kasneci", "Gjergji", ""], ["Graepel", "Thore", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1207.4567", "submitter": "Rong-Hua Li", "authors": "Rong-Hua Li, Jeffrey Xu Yu", "title": "Efficient Core Maintenance in Large Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-core decomposition in a graph is a fundamental problem for social\nnetwork analysis. The problem of $k$-core decomposition is to calculate the\ncore number for every node in a graph. Previous studies mainly focus on\n$k$-core decomposition in a static graph. There exists a linear time algorithm\nfor $k$-core decomposition in a static graph. However, in many real-world\napplications such as online social networks and the Internet, the graph\ntypically evolves over time. Under such applications, a key issue is to\nmaintain the core number of nodes given the graph changes over time. A simple\nimplementation is to perform the linear time algorithm to recompute the core\nnumber for every node after the graph is updated. Such simple implementation is\nexpensive when the graph is very large. In this paper, we propose a new\nefficient algorithm to maintain the core number for every node in a dynamic\ngraph. Our main result is that only certain nodes need to update their core\nnumber given the graph is changed by inserting/deleting an edge. We devise an\nefficient algorithm to identify and recompute the core number of such nodes.\nThe complexity of our algorithm is independent of the graph size. In addition,\nto further accelerate the algorithm, we develop two pruning strategies by\nexploiting the lower and upper bounds of the core number. Finally, we conduct\nextensive experiments over both real-world and synthetic datasets, and the\nresults demonstrate the efficiency of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 06:57:10 GMT"}], "update_date": "2012-07-20", "authors_parsed": [["Li", "Rong-Hua", ""], ["Yu", "Jeffrey Xu", ""]]}, {"id": "1207.4570", "submitter": "Farzad Parseh", "authors": "Farzad Parseh, Davood Karimzadgan Moghaddam, Mir Mohsen Pedram,\n  Rohollah Esmaeli Manesh, Mohammad (behdad) Jamshidi", "title": "Presentation an Approach for Optimization of Semantic Web Language Based\n  on the Document Structure", "comments": "7 pages, 8 figures, 2 Tables", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 3, No 1, May 2012 ISSN (Online): 1694-0814", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern tree are based on integrated rules which are equal to a combination\nof some points connected to each other in a hierarchical structure, called\nEnquiry Hierarchical (EH). The main operation in pattern enquiry seeking is to\nlocate the steps that match the given EH in the dataset. A point of algorithms\nhas offered for EH matching; but the majority of this algorithms seeks all of\nthe enquiry steps to access all EHs in the dataset. A few algorithms such as\nseek only steps that satisfy end points of EH. All of above algorithms are\ntrying to locate a way just for investigating direct testing of steps and to\nlocate the answer of enquiry, directly via these points. In this paper, we\ndescribe a novel algorithm to locate the answer of enquiry without access to\nreal point of the dataset blindly. In this algorithm, first, the enquiry will\nbe executed on enquiry schema and this leads to a schema. Using this plan, it\nwill be clear how to seek end steps and how to achieve enquiry dataset, before\nseeking of the dataset steps. Therefore, none of dataset steps will be seek\nblindly.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 07:14:15 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2012 05:05:20 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Parseh", "Farzad", "", "behdad"], ["Moghaddam", "Davood Karimzadgan", "", "behdad"], ["Pedram", "Mir Mohsen", "", "behdad"], ["Manesh", "Rohollah Esmaeli", "", "behdad"], ["Mohammad", "", "", "behdad"], ["Jamshidi", "", ""]]}, {"id": "1207.4821", "submitter": "Angus Macdonald", "authors": "Angus Macdonald", "title": "The Architecture of an Autonomic, Resource-Aware, Workstation-Based\n  Distributed Database System", "comments": "Ph.D. Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Distributed software systems that are designed to run over workstation\nmachines within organisations are termed workstation-based. Workstation-based\nsystems are characterised by dynamically changing sets of machines that are\nused primarily for other, user-centric tasks. They must be able to adapt to and\nutilize spare capacity when and where it is available, and ensure that the\nnon-availability of an individual machine does not affect the availability of\nthe system. This thesis focuses on the requirements and design of a\nworkstation-based database system, which is motivated by an analysis of\nexisting database architectures that are typically run over static, specially\nprovisioned sets of machines. A typical clustered database system -- one that\nis run over a number of specially provisioned machines -- executes queries\ninteractively, returning a synchronous response to applications, with its data\nmade durable and resilient to the failure of machines. There are no existing\nworkstation-based databases. Furthermore, other workstation-based systems do\nnot attempt to achieve the requirements of interactivity and durability,\nbecause they are typically used to execute asynchronous batch processing jobs\nthat tolerate data loss -- results can be re-computed. These systems use\nexternal servers to store the final results of computations rather than\nworkstation machines. This thesis describes the design and implementation of a\nworkstation-based database system and investigates its viability by evaluating\nits performance against existing clustered database systems and testing its\navailability during machine failures.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 22:01:39 GMT"}], "update_date": "2012-07-23", "authors_parsed": [["Macdonald", "Angus", ""]]}, {"id": "1207.4958", "submitter": "Arnab Bhattacharya", "authors": "Ashish Gupta, Akshay Mittal, Arnab Bhattacharya", "title": "Minimally Infrequent Itemset Mining using Pattern-Growth Paradigm and\n  Residual Trees", "comments": "Best paper award in International Conference on Management of Data\n  (COMAD), 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Itemset mining has been an active area of research due to its successful\napplication in various data mining scenarios including finding association\nrules. Though most of the past work has been on finding frequent itemsets,\ninfrequent itemset mining has demonstrated its utility in web mining,\nbioinformatics and other fields. In this paper, we propose a new algorithm\nbased on the pattern-growth paradigm to find minimally infrequent itemsets. A\nminimally infrequent itemset has no subset which is also infrequent. We also\nintroduce the novel concept of residual trees. We further utilize the residual\ntrees to mine multiple level minimum support itemsets where different\nthresholds are used for finding frequent itemsets for different lengths of the\nitemset. Finally, we analyze the behavior of our algorithm with respect to\ndifferent parameters and show through experiments that it outperforms the\ncompeting ones.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 11:11:54 GMT"}], "update_date": "2012-07-23", "authors_parsed": [["Gupta", "Ashish", ""], ["Mittal", "Akshay", ""], ["Bhattacharya", "Arnab", ""]]}, {"id": "1207.5226", "submitter": "George Beskales", "authors": "George Beskales and Ihab F. Ilyas and Lukasz Golab and Artur Galiullin", "title": "On the Relative Trust between Inconsistent Data and Inaccurate\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional dependencies (FDs) specify the intended data semantics while\nviolations of FDs indicate deviation from these semantics. In this paper, we\nstudy a data cleaning problem in which the FDs may not be completely correct,\ne.g., due to data evolution or incomplete knowledge of the data semantics. We\nargue that the notion of relative trust is a crucial aspect of this problem: if\nthe FDs are outdated, we should modify them to fit the data, but if we suspect\nthat there are problems with the data, we should modify the data to fit the\nFDs. In practice, it is usually unclear how much to trust the data versus the\nFDs. To address this problem, we propose an algorithm for generating\nnon-redundant solutions (i.e., simultaneous modifications of the data and the\nFDs) corresponding to various levels of relative trust. This can help users\ndetermine the best way to modify their data and/or FDs to achieve consistency.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2012 13:10:33 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2012 07:20:49 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Beskales", "George", ""], ["Ilyas", "Ihab F.", ""], ["Golab", "Lukasz", ""], ["Galiullin", "Artur", ""]]}, {"id": "1207.5425", "submitter": "Ana Cerdeira-Pena", "authors": "Nieves R. Brisaboa, Ana Cerdeira-Pena, Gonzalo Navarro, Oscar Pedreira", "title": "Ranked Document Retrieval in (Almost) No Space", "comments": "This is an extended version of the paper that will appear in Proc. of\n  SPIRE'2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranked document retrieval is a fundamental task in search engines. Such\nqueries are solved with inverted indexes that require additional 45%-80% of the\ncompressed text space, and take tens to hundreds of microseconds per query. In\nthis paper we show how ranked document retrieval queries can be solved within\ntens of milliseconds using essentially no extra space over an in-memory\ncompressed representation of the document collection. More precisely, we\nenhance wavelet trees on bytecodes (WTBCs), a data structure that rearranges\nthe bytes of the compressed collection, so that they support ranked conjunctive\nand disjunctive queries, using just 6%-18% of the compressed text space.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 15:34:39 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Cerdeira-Pena", "Ana", ""], ["Navarro", "Gonzalo", ""], ["Pedreira", "Oscar", ""]]}, {"id": "1207.5466", "submitter": "Yongge Wang", "authors": "Yongge Wang and Xintao Wu", "title": "Approximate Inverse Frequent Itemset Mining: Privacy, Complexity, and\n  Approximation", "comments": null, "journal-ref": "Proc. 5th IEEE ICDM, pages 482-289, 2005", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to generate synthetic basket data sets for better benchmark testing,\nit is important to integrate characteristics from real-life databases into the\nsynthetic basket data sets. The characteristics that could be used for this\npurpose include the frequent itemsets and association rules. The problem of\ngenerating synthetic basket data sets from frequent itemsets is generally\nreferred to as inverse frequent itemset mining. In this paper, we show that the\nproblem of approximate inverse frequent itemset mining is {\\bf NP}-complete.\nThen we propose and analyze an approximate algorithm for approximate inverse\nfrequent itemset mining, and discuss privacy issues related to the synthetic\nbasket data set. In particular, we propose an approximate algorithm to\ndetermine the privacy leakage in a synthetic basket data set.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 17:57:47 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Wang", "Yongge", ""], ["Wu", "Xintao", ""]]}, {"id": "1207.5777", "submitter": "Udayan Khurana", "authors": "Udayan Khurana and Amol Deshpande", "title": "Efficient Snapshot Retrieval over Historical Graph Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of managing historical data for large evolving\ninformation networks like social networks or citation networks, with the goal\nto enable temporal and evolutionary queries and analysis. We present the design\nand architecture of a distributed graph database system that stores the entire\nhistory of a network and provides support for efficient retrieval of multiple\ngraphs from arbitrary time points in the past, in addition to maintaining the\ncurrent state for ongoing updates. Our system exposes a general programmatic\nAPI to process and analyze the retrieved snapshots. We introduce DeltaGraph, a\nnovel, extensible, highly tunable, and distributed hierarchical index structure\nthat enables compactly recording the historical information, and that supports\nefficient retrieval of historical graph snapshots for single-site or parallel\nprocessing. Along with the original graph data, DeltaGraph can also maintain\nand index auxiliary information; this functionality can be used to extend the\nstructure to efficiently execute queries like subgraph pattern matching over\nhistorical data. We develop analytical models for both the storage space needed\nand the snapshot retrieval times to aid in choosing the right parameters for a\nspecific scenario. In addition, we present strategies for materializing\nportions of the historical graph state in memory to further speed up the\nretrieval process. Secondly, we present an in-memory graph data structure\ncalled GraphPool that can maintain hundreds of historical graph instances in\nmain memory in a non-redundant manner. We present a comprehensive experimental\nevaluation that illustrates the effectiveness of our proposed techniques at\nmanaging historical graph information.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2012 19:15:28 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Khurana", "Udayan", ""], ["Deshpande", "Amol", ""]]}, {"id": "1207.5990", "submitter": "Mehdi Ahmed-Nacer", "authors": "Mehdi Ahmed-Nacer (INRIA Lorraine - LORIA), St\\'ephane Martin (INRIA\n  Lorraine - LORIA), Pascal Urso (INRIA Lorraine - LORIA)", "title": "File system on CRDT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we show how to manage a distributed hierarchical structure\nrepresenting a file system. This structure is optimistically replicated, each\nuser work on his local replica, and updates are sent to other replica. The\ndifferent replicas eventually observe same view of file systems. At this stage,\nconflicts between updates are very common. We claim that conflict resolution\nshould rely as little as possible on users. In this report we propose a simple\nand modular solution to resolve these problems and maintain data consistency.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 13:50:08 GMT"}], "update_date": "2012-07-26", "authors_parsed": [["Ahmed-Nacer", "Mehdi", "", "INRIA Lorraine - LORIA"], ["Martin", "St\u00e9phane", "", "INRIA\n  Lorraine - LORIA"], ["Urso", "Pascal", "", "INRIA Lorraine - LORIA"]]}, {"id": "1207.6096", "submitter": "Grigory Yaroslavtsev", "authors": "Graham Cormode, Cecilia M. Procopiuc, Divesh Srivastava, Grigory\n  Yaroslavtsev", "title": "Accurate and Efficient Private Release of Datacubes and Contingency\n  Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in releasing aggregate information about sensitive data is\nto do so accurately while providing a privacy guarantee on the output. Recent\nwork focuses on the class of linear queries, which include basic counting\nqueries, data cubes, and contingency tables. The goal is to maximize the\nutility of their output, while giving a rigorous privacy guarantee. Most\nresults follow a common template: pick a \"strategy\" set of linear queries to\napply to the data, then use the noisy answers to these queries to reconstruct\nthe queries of interest. This entails either picking a strategy set that is\nhoped to be good for the queries, or performing a costly search over the space\nof all possible strategies.\n  In this paper, we propose a new approach that balances accuracy and\nefficiency: we show how to improve the accuracy of a given query set by\nanswering some strategy queries more accurately than others. This leads to an\nefficient optimal noise allocation for many popular strategies, including\nwavelets, hierarchies, Fourier coefficients and more. For the important case of\nmarginal queries we show that this strictly improves on previous methods, both\nanalytically and empirically. Our results also extend to ensuring that the\nreturned query answers are consistent with an (unknown) data set at minimal\nextra cost in terms of time and noise.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 19:31:18 GMT"}], "update_date": "2012-07-26", "authors_parsed": [["Cormode", "Graham", ""], ["Procopiuc", "Cecilia M.", ""], ["Srivastava", "Divesh", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1207.6253", "submitter": "Rui Henriques", "authors": "Rui Henriques and In\\^es Lynce and Vasco Manquinho", "title": "On When and How to use SAT to Mine Frequent Itemsets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new stream of research was born in the last decade with the goal of mining\nitemsets of interest using Constraint Programming (CP). This has promoted a\nnatural way to combine complex constraints in a highly flexible manner.\nAlthough CP state-of-the-art solutions formulate the task using Boolean\nvariables, the few attempts to adopt propositional Satisfiability (SAT)\nprovided an unsatisfactory performance. This work deepens the study on when and\nhow to use SAT for the frequent itemset mining (FIM) problem by defining\ndifferent encodings with multiple task-driven enumeration options and search\nstrategies. Although for the majority of the scenarios SAT-based solutions\nappear to be non-competitive with CP peers, results show a variety of\ninteresting cases where SAT encodings are the best option.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 12:33:46 GMT"}], "update_date": "2012-07-27", "authors_parsed": [["Henriques", "Rui", ""], ["Lynce", "In\u00eas", ""], ["Manquinho", "Vasco", ""]]}, {"id": "1207.6329", "submitter": "Sean Chester", "authors": "Sean Chester and Alex Thomo and S. Venkatesh and Sue Whitesides", "title": "Computing optimal k-regret minimizing sets with top-k depth contours", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regret minimizing sets are a very recent approach to representing a dataset D\nwith a small subset S of representative tuples. The set S is chosen such that\nexecuting any top-1 query on S rather than D is minimally perceptible to any\nuser. To discover an optimal regret minimizing set of a predetermined\ncardinality is conjectured to be a hard problem. In this paper, we generalize\nthe problem to that of finding an optimal k$regret minimizing set, wherein the\ndifference is computed over top-k queries, rather than top-1 queries.\n  We adapt known geometric ideas of top-k depth contours and the reverse top-k\nproblem. We show that the depth contours themselves offer a means of comparing\nthe optimality of regret minimizing sets using L2 distance. We design an\nO(cn^2) plane sweep algorithm for two dimensions to compute an optimal regret\nminimizing set of cardinality c. For higher dimensions, we introduce a greedy\nalgorithm that progresses towards increasingly optimal solutions by exploiting\nthe transitivity of L2 distance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 16:59:17 GMT"}], "update_date": "2012-07-27", "authors_parsed": [["Chester", "Sean", ""], ["Thomo", "Alex", ""], ["Venkatesh", "S.", ""], ["Whitesides", "Sue", ""]]}, {"id": "1207.6448", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Dhaval Chandarana, Rutvi Dave, Sharyu Page,\n  Shikha Gupta", "title": "Query Optimization Over Web Services Using A Mixed Approach", "comments": "10 pages, 1 figure", "journal-ref": "Advances in Computing & Inf. Technology, AISC 178, pp.381-389,\n  2012", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Web Service Management System (WSMS) can be well-thought-out as a\nconsistent and a secure way of managing the web services. Web Service has\nbecome a quintessential part of the web world, managing and sharing the\nresources of the business it is associated with. In this paper, we focus on the\nquery optimization aspect of handling the \"natural language\" query, queried to\nthe WSMS. The map-select-composite operations are piloted to select specific\nweb services. The main aftermath of our research is ensued in an algorithm\nwhich uses cost-based as well as heuristic based approach for query\noptimization. Query plan is formed after cost-based evaluation and using Greedy\nalgorithm. The heuristic based approach further optimizes the evaluation plan.\nThis scheme not only guarantees an optimal solution, which has a minimum\ndiversion from the ideal solution, but also saves time which is otherwise\nutilized in generating various query plans using many mathematical models and\nthen evaluating each one.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2012 04:55:07 GMT"}], "update_date": "2012-07-30", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Chandarana", "Dhaval", ""], ["Dave", "Rutvi", ""], ["Page", "Sharyu", ""], ["Gupta", "Shikha", ""]]}, {"id": "1207.6560", "submitter": "Phuong Le", "authors": "Nguyen Duc Thuan", "title": "Covering Rough Sets From a Topological Point of View", "comments": "4 pages", "journal-ref": "International Journal of Computer Theory and Engineering, Vol. 1,\n  No. 5, December, 2009, 606-609", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covering-based rough set theory is an extension to classical rough set. The\nmain purpose of this paper is to study covering rough sets from a topological\npoint of view. The relationship among upper approximations based on topological\nspaces are explored.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2012 14:49:18 GMT"}], "update_date": "2012-07-30", "authors_parsed": [["Thuan", "Nguyen Duc", ""]]}]