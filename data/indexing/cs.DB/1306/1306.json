[{"id": "1306.1073", "submitter": "Bernhard Haslhofer", "authors": "Bernhard Haslhofer and Simeon Warner and Carl Lagoze and Martin Klein\n  and Robert Sanderson and Herbert van de Sompel and Michael L. Nelson", "title": "Web Synchronization Simulations using the ResourceSync Framework", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintenance of multiple, distributed up-to-date copies of collections of\nchanging Web resources is important in many application contexts and is often\nachieved using ad hoc or proprietary synchronization solutions. ResourceSync is\na resource synchronization framework that integrates with the Web architecture\nand leverages XML sitemaps. We define a model for the ResourceSync framework as\na basis for understanding its properties. We then describe experiments in which\nsimulations of a variety of synchronization scenarios illustrate the effects of\nmodel configuration on consistency, latency, and data transfer efficiency.\nThese results provide insight into which congurations are appropriate for\nvarious application scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 12:06:03 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Haslhofer", "Bernhard", ""], ["Warner", "Simeon", ""], ["Lagoze", "Carl", ""], ["Klein", "Martin", ""], ["Sanderson", "Robert", ""], ["van de Sompel", "Herbert", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1306.1153", "submitter": "Xiaokui Xiao", "authors": "Andy Diwen Zhu, Xiaokui Xiao, Sibo Wang, Wenqing Lin", "title": "Efficient Single-Source Shortest Path and Distance Queries on Large\n  Graphs", "comments": "To appear in KDD 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates two types of graph queries: {\\em single source\ndistance (SSD)} queries and {\\em single source shortest path (SSSP)} queries.\nGiven a node $v$ in a graph $G$, an SSD query from $v$ asks for the distance\nfrom $v$ to any other node in $G$, while an SSSP query retrieves the shortest\npath from $v$ to any other node. These two types of queries are fundamental\nbuilding blocks of numerous graph algorithms, and they find important\napplications in graph analysis, especially in the computation of graph\nmeasures. Most of the existing solutions for SSD and SSSP queries, however,\nrequire that the input graph fits in the main memory, which renders them\ninapplicable for the massive disk-resident graphs commonly used in web and\nsocial applications. The only exceptions are a few techniques that are designed\nto be I/O efficient, but they all focus on undirected and/or unweighted graphs,\nand they only offer sub-optimal query efficiency.\n  To address the deficiency of existing work, this paper presents {\\em\nHighways-on-Disk (HoD)}, a disk-based index that supports both SSD and SSSP\nqueries on directed and weighted graphs. The key idea of HoD is to augment the\ninput graph with a set of auxiliary edges, and exploit them during query\nprocessing to reduce I/O and computation costs. We experimentally evaluate HoD\non both directed and undirected real-world graphs with up to billions of nodes\nand edges, and we demonstrate that HoD significantly outperforms alternative\nsolutions in terms of query efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 15:46:33 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Zhu", "Andy Diwen", ""], ["Xiao", "Xiaokui", ""], ["Wang", "Sibo", ""], ["Lin", "Wenqing", ""]]}, {"id": "1306.1334", "submitter": "Hitesh Chhinkaniwala", "authors": "Hitesh Chhinkaniwala and Sanjay Garg", "title": "Tuple Value Based Multiplicative Data Perturbation Approach To Preserve\n  Privacy In Data Stream Mining", "comments": "International Journal of Data Mining & Knowledge Management Process\n  (IJDKP) 9 pages", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.3, May 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge volume of data from domain specific applications such as medical,\nfinancial, library, telephone, shopping records and individual are regularly\ngenerated. Sharing of these data is proved to be beneficial for data mining\napplication. On one hand such data is an important asset to business decision\nmaking by analyzing it. On the other hand data privacy concerns may prevent\ndata owners from sharing information for data analysis. In order to share data\nwhile preserving privacy, data owner must come up with a solution which\nachieves the dual goal of privacy preservation as well as an accuracy of data\nmining task - clustering and classification. An efficient and effective\napproach has been proposed that aims to protect privacy of sensitive\ninformation and obtaining data clustering with minimum information loss.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 08:15:22 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Chhinkaniwala", "Hitesh", ""], ["Garg", "Sanjay", ""]]}, {"id": "1306.1689", "submitter": "Simon Razniewski", "authors": "Simon Razniewski, Marco Montali, Werner Nutt", "title": "Verification of Query Completeness over Processes [Extended Version]", "comments": "Extended version of a paper that was submitted to BPM 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data completeness is an essential aspect of data quality, and has in turn a\nhuge impact on the effective management of companies. For example, statistics\nare computed and audits are conducted in companies by implicitly placing the\nstrong assumption that the analysed data are complete. In this work, we are\ninterested in studying the problem of completeness of data produced by business\nprocesses, to the aim of automatically assessing whether a given database query\ncan be answered with complete information in a certain state of the process. We\nformalize so-called quality-aware processes that create data in the real world\nand store it in the company's information system possibly at a later point.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 11:17:41 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Razniewski", "Simon", ""], ["Montali", "Marco", ""], ["Nutt", "Werner", ""]]}, {"id": "1306.1723", "submitter": "Nur Rakhmawati", "authors": "Nur Aini Rakhmawati and J\\\"urgen Umbrich and Marcel Karnstedt and Ali\n  Hasnain and Michael Hausenblas", "title": "Querying over Federated SPARQL Endpoints ---A State of the Art Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The increasing amount of Linked Data and its inherent distributed nature have\nattracted significant attention throughout the research community and amongst\npractitioners to search data, in the past years. Inspired by research results\nfrom traditional distributed databases, different approaches for managing\nfederation over SPARQL Endpoints have been introduced. SPARQL is the\nstandardised query language for RDF, the default data model used in Linked Data\ndeployments and SPARQL Endpoints are a popular access mechanism provided by\nmany Linked Open Data (LOD) repositories. In this paper, we initially give an\noverview of the federation framework infrastructure and then proceed with a\ncomparison of existing SPARQL federation frameworks. Finally, we highlight\nshortcomings in existing frameworks, which we hope helps spawning new research\ndirections.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 13:42:19 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Rakhmawati", "Nur Aini", ""], ["Umbrich", "J\u00fcrgen", ""], ["Karnstedt", "Marcel", ""], ["Hasnain", "Ali", ""], ["Hausenblas", "Michael", ""]]}, {"id": "1306.1730", "submitter": "Laxmaiah Mettu", "authors": "M.Laxmaiah, A.Govardhan", "title": "A Conceptual Metadata Framework for Spatial Data Warehouse", "comments": null, "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.3, May 2013", "doi": "10.5121/IJDKP.2013.3306", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metadata represents the information about data to be stored in Data\nWarehouses.It is a mandatory element of Data Warehouse to build an efficient\nData Warehouse.Metadata helps in data integration,lineage,data quality and\npopulating transformed data into data warehouse.Spatial data warehouses are\nbased on spatial data mostly collected from Geographical Information\nSystems(GIS)and the transactional systems that are specific to an application\nor enterprise.Metadata design and deployment is the most critical phase in\nbuilding of data warehouse where it is mandatory to bring the spatial\ninformation and data modeling together.In this paper,we present a holistic\nmetadata framework that drives metadata creation for spatial data warehouse.\nTheoretically, the proposed metadata framework improves the efficiency of\naccessing of data in response to frequent queries on SDWs.In other words, the\nproposed framework decreases the response time of the query and accurate\ninformation is fetched from Data Warehouse including the spatial information.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 13:56:43 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Laxmaiah", "M.", ""], ["Govardhan", "A.", ""]]}, {"id": "1306.2459", "submitter": "Sutanay Choudhury", "authors": "Sutanay Choudhury, Lawrence Holder, George Chin, John Feo", "title": "Fast Search for Dynamic Multi-Relational Graphs", "comments": "SIGMOD Workshop on Dynamic Networks Management and Mining (DyNetMM),\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Acting on time-critical events by processing ever growing social media or\nnews streams is a major technical challenge. Many of these data sources can be\nmodeled as multi-relational graphs. Continuous queries or techniques to search\nfor rare events that typically arise in monitoring applications have been\nstudied extensively for relational databases. This work is dedicated to answer\nthe question that emerges naturally: how can we efficiently execute a\ncontinuous query on a dynamic graph? This paper presents an exact subgraph\nsearch algorithm that exploits the temporal characteristics of representative\nqueries for online news or social media monitoring. The algorithm is based on a\nnovel data structure called the Subgraph Join Tree (SJ-Tree) that leverages the\nstructural and semantic characteristics of the underlying multi-relational\ngraph. The paper concludes with extensive experimentation on several real-world\ndatasets that demonstrates the validity of this approach.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 09:21:42 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Choudhury", "Sutanay", ""], ["Holder", "Lawrence", ""], ["Chin", "George", ""], ["Feo", "John", ""]]}, {"id": "1306.2460", "submitter": "Sutanay Choudhury", "authors": "Sutanay Choudhury, Lawrence Holder, George Chin, Abhik Ray, Sherman\n  Beus, John Feo", "title": "StreamWorks - A system for Dynamic Graph Search", "comments": "SIGMOD 2013: International Conference on Management of Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Acting on time-critical events by processing ever growing social media, news\nor cyber data streams is a major technical challenge. Many of these data\nsources can be modeled as multi-relational graphs. Mining and searching for\nsubgraph patterns in a continuous setting requires an efficient approach to\nincremental graph search. The goal of our work is to enable real-time search\ncapabilities for graph databases. This demonstration will present a dynamic\ngraph query system that leverages the structural and semantic characteristics\nof the underlying multi-relational graph.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 09:24:06 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Choudhury", "Sutanay", ""], ["Holder", "Lawrence", ""], ["Chin", "George", ""], ["Ray", "Abhik", ""], ["Beus", "Sherman", ""], ["Feo", "John", ""]]}, {"id": "1306.2691", "submitter": "EPTCS", "authors": "Ivan Gazeau (INRIA), Dale Miller (INRIA), Catuscia Palamidessi (INRIA)", "title": "Preserving differential privacy under finite-precision semantics", "comments": "In Proceedings QAPL 2013, arXiv:1306.2413", "journal-ref": "EPTCS 117, 2013, pp. 1-18", "doi": "10.4204/EPTCS.117.1", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximation introduced by finite-precision representation of continuous\ndata can induce arbitrarily large information leaks even when the computation\nusing exact semantics is secure. Such leakage can thus undermine design efforts\naimed at protecting sensitive information. We focus here on differential\nprivacy, an approach to privacy that emerged from the area of statistical\ndatabases and is now widely applied also in other domains. In this approach,\nprivacy is protected by the addition of noise to a true (private) value. To\ndate, this approach to privacy has been proved correct only in the ideal case\nin which computations are made using an idealized, infinite-precision\nsemantics. In this paper, we analyze the situation at the implementation level,\nwhere the semantics is necessarily finite-precision, i.e. the representation of\nreal numbers and the operations on them, are rounded according to some level of\nprecision. We show that in general there are violations of the differential\nprivacy property, and we study the conditions under which we can still\nguarantee a limited (but, arguably, totally acceptable) variant of the\nproperty, under only a minor degradation of the privacy level. Finally, we\nillustrate our results on two cases of noise-generating distributions: the\nstandard Laplacian mechanism commonly used in differential privacy, and a\nbivariate version of the Laplacian recently introduced in the setting of\nprivacy-aware geolocation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 01:55:18 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Gazeau", "Ivan", "", "INRIA"], ["Miller", "Dale", "", "INRIA"], ["Palamidessi", "Catuscia", "", "INRIA"]]}, {"id": "1306.3558", "submitter": "Fabrizio Angiulli", "authors": "Fabrizio Angiulli and Fabio Fassetti and Luigi Palopoli and Giuseppe\n  Manco", "title": "Outlying Property Detection with Numerical Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outlying property detection problem is the problem of discovering the\nproperties distinguishing a given object, known in advance to be an outlier in\na database, from the other database objects. In this paper, we analyze the\nproblem within a context where numerical attributes are taken into account,\nwhich represents a relevant case left open in the literature. We introduce a\nmeasure to quantify the degree the outlierness of an object, which is\nassociated with the relative likelihood of the value, compared to the to the\nrelative likelihood of other objects in the database. As a major contribution,\nwe present an efficient algorithm to compute the outlierness relative to\nsignificant subsets of the data. The latter subsets are characterized in a\n\"rule-based\" fashion, and hence the basis for the underlying explanation of the\noutlierness.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2013 08:52:46 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Angiulli", "Fabrizio", ""], ["Fassetti", "Fabio", ""], ["Palopoli", "Luigi", ""], ["Manco", "Giuseppe", ""]]}, {"id": "1306.4069", "submitter": "Azhar Mahmood", "authors": "Azhar Mahmood, Shi Ke, Shaheen Khatoon", "title": "An Efficient Distributed Data Extraction Method for Mining Sensor\n  Networks Data", "comments": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 1, No 2, January 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of Sensor Networks (SNs) are deployed in real world applications\nwhich generate large amount of raw sensory data. Data mining technique to\nextract useful knowledge from these applications is an emerging research area\ndue to its crucial importance but still its a challenge to discover knowledge\nefficiently from the sensor network data. In this paper we proposed a\nDistributed Data Extraction (DDE) method to extract data from sensor networks\nby applying rules based clustering and association rule mining techniques. A\nsignificant amount of sensor readings sent from the sensors to the data\nprocessing point(s) may be lost or corrupted. DDE is also estimating these\nmissing values from available sensor reading instead of requesting the sensor\nnode to resend lost reading. DDE also apply data reduction which is able to\nreduce the data size while transmitting to sink. Results show our proposed\napproach exhibits the maximum data accuracy and efficient data extraction in\nterm of the entire networks energy consumption.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 05:40:45 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Mahmood", "Azhar", ""], ["Ke", "Shi", ""], ["Khatoon", "Shaheen", ""]]}, {"id": "1306.5424", "submitter": "Hubie Chen", "authors": "Hubie Chen, Moritz M\\\"uller", "title": "The Fine Classification of Conjunctive Queries and Parameterized\n  Logarithmic Space Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a fundamental investigation of the complexity of conjunctive query\nevaluation from the perspective of parameterized complexity. We classify sets\nof boolean conjunctive queries according to the complexity of this problem.\nPrevious work showed that a set of conjunctive queries is fixed-parameter\ntractable precisely when the set is equivalent to a set of queries having\nbounded treewidth. We present a fine classification of query sets up to\nparameterized logarithmic space reduction. We show that, in the bounded\ntreewidth regime, there are three complexity degrees and that the properties\nthat determine the degree of a query set are bounded pathwidth and bounded tree\ndepth. We also engage in a study of the two higher degrees via logarithmic\nspace machine characterizations and complete problems. Our work yields a\nsignificantly richer perspective on the complexity of conjunctive queries and,\nat the same time, suggests new avenues of research in parameterized complexity.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 15:16:39 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 13:51:08 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Chen", "Hubie", ""], ["M\u00fcller", "Moritz", ""]]}, {"id": "1306.5586", "submitter": "Robert Primmer", "authors": "Robert Primmer, Scott Nyman, Wayzen Lin", "title": "Creating a Relational Distributed Object Store", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In and of itself, data storage has apparent business utility. But when we can\nconvert data to information, the utility of stored data increases dramatically.\nIt is the layering of relation atop the data mass that is the engine for such\nconversion. Frank relation amongst discrete objects sporadically ingested is\nrare, making the process of synthesizing such relation all the more\nchallenging, but the challenge must be met if we are ever to see an equivalent\nbusiness value for unstructured data as we already have with structured data.\nThis paper describes a novel construct, referred to as a relational distributed\nobject store (RDOS), that seeks to solve the twin problems of how to\npersistently and reliably store petabytes of unstructured data while\nsimultaneously creating and persisting relations amongst billions of objects.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 12:00:10 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Primmer", "Robert", ""], ["Nyman", "Scott", ""], ["Lin", "Wayzen", ""]]}, {"id": "1306.5690", "submitter": "Dhammika Pieris", "authors": "Dhammika Pieris", "title": "Modifying the Entity relationship modelling notation: towards high\n  quality relational databases from better notated ER models", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The entity relationship modelling using the original ER notation has been\napplauded providing a natural view of data in conceptual modelling of\ninformation systems. However, the current ER to relational model transformation\nalgorithm is known to be insufficient in providing a complete and accurate\nrepresentation of the ER model undertaken for transformation. In an effort to\nderive better transformations from ER models, we have understood that\nmodifications should be introduced to both of the existing transformation\nalgorithm as well as to the ER notation. Introducing some new concepts, we have\nadapted the original ER notation and developed a new transformation algorithm\nbased on the existing one. This paper presents the modified ER notation with an\nER diagram drawn based on the new notation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 06:47:48 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Pieris", "Dhammika", ""]]}, {"id": "1306.5898", "submitter": "Harald Lampesberger", "authors": "Harald Lampesberger", "title": "A Grammatical Inference Approach to Language-Based Anomaly Detection in\n  XML", "comments": "Paper accepted at First Int. Workshop on Emerging Cyberthreats and\n  Countermeasures ECTCM 2013", "journal-ref": null, "doi": "10.1109/ARES.2013.90", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  False-positives are a problem in anomaly-based intrusion detection systems.\nTo counter this issue, we discuss anomaly detection for the eXtensible Markup\nLanguage (XML) in a language-theoretic view. We argue that many XML-based\nattacks target the syntactic level, i.e. the tree structure or element content,\nand syntax validation of XML documents reduces the attack surface. XML offers\nso-called schemas for validation, but in real world, schemas are often\nunavailable, ignored or too general. In this work-in-progress paper we describe\na grammatical inference approach to learn an automaton from example XML\ndocuments for detecting documents with anomalous syntax.\n  We discuss properties and expressiveness of XML to understand limits of\nlearnability. Our contributions are an XML Schema compatible lexical datatype\nsystem to abstract content in XML and an algorithm to learn visibly pushdown\nautomata (VPA) directly from a set of examples. The proposed algorithm does not\nrequire the tree representation of XML, so it can process large documents or\nstreams. The resulting deterministic VPA then allows stream validation of\ndocuments to recognize deviations in the underlying tree structure or\ndatatypes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 09:40:48 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Lampesberger", "Harald", ""]]}, {"id": "1306.5972", "submitter": "Paraschos Koutris", "authors": "Paul Beame, Paraschos Koutris and Dan Suciu", "title": "Communication Steps for Parallel Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing a relational query $q$ on a large input\ndatabase of size $n$, using a large number $p$ of servers. The computation is\nperformed in rounds, and each server can receive only $O(n/p^{1-\\varepsilon})$\nbits of data, where $\\varepsilon \\in [0,1]$ is a parameter that controls\nreplication. We examine how many global communication steps are needed to\ncompute $q$. We establish both lower and upper bounds, in two settings. For a\nsingle round of communication, we give lower bounds in the strongest possible\nmodel, where arbitrary bits may be exchanged; we show that any algorithm\nrequires $\\varepsilon \\geq 1-1/\\tau^*$, where $\\tau^*$ is the fractional vertex\ncover of the hypergraph of $q$. We also give an algorithm that matches the\nlower bound for a specific class of databases. For multiple rounds of\ncommunication, we present lower bounds in a model where routing decisions for a\ntuple are tuple-based. We show that for the class of tree-like queries there\nexists a tradeoff between the number of rounds and the space exponent\n$\\varepsilon$. The lower bounds for multiple rounds are the first of their\nkind. Our results also imply that transitive closure cannot be computed in O(1)\nrounds of communication.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 14:04:49 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Beame", "Paul", ""], ["Koutris", "Paraschos", ""], ["Suciu", "Dan", ""]]}, {"id": "1306.5982", "submitter": "Menaka Gandhi  J", "authors": "Menaka Gandhi.J and K.S.Gayathri", "title": "Activity Modeling in Smart Home using High Utility Pattern Mining over\n  Data Streams", "comments": "This research paper consists of 7 pages, 7 figures and 4 algorithms", "journal-ref": "\"Interactive mining of high utility patterns over data streams\",\n  Elsevier, Vol. 39, No. 15, 2012", "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Smart home technology is a better choice for the people to care about\nsecurity, comfort and power saving as well. It is required to develop\ntechnologies that recognize the Activities of Daily Living (ADLs) of the\nresidents at home and detect the abnormal behavior in the individual's\npatterns. Data mining techniques such as Frequent pattern mining (FPM), High\nUtility Pattern (HUP) Mining were used to find those activity patterns from the\ncollected sensor data. But applying the above technique for Activity\nRecognition from the temporal sensor data stream is highly complex and\nchallenging task. So, a new approach is proposed for activity recognition from\nsensor data stream which is achieved by constructing Frequent Pattern Stream\ntree (FPS - tree). FPS is a sliding window based approach to discover the\nrecent activity patterns over time from data streams. The proposed work aims at\nidentifying the frequent pattern of the user from the sensor data streams which\nare later modeled for activity recognition. The proposed FPM algorithm uses a\ndata structure called Linked Sensor Data Stream (LSDS) for storing the sensor\ndata stream information which increases the efficiency of frequent pattern\nmining algorithm through both space and time. The experimental results show the\nefficiency of the proposed algorithm and this FPM is further extended for\napplying for power efficiency using HUP to detect the high usage of power\nconsumption of residents at smart home.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 14:39:17 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["J", "Menaka Gandhi.", ""], ["Gayathri", "K. S.", ""]]}, {"id": "1306.6670", "submitter": "Olivier Cur\\'e", "authors": "Olivier Cur\\'e and David Faye and Guillaume Blin", "title": "Towards a better insight of RDF triples Ontology-guided Storage system\n  abilities", "comments": "15 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vision of the Semantic Web is becoming a reality with billions of RDF\ntriples being distributed over multiple queryable end-points (e.g. Linked\nData). Although there has been a body of work on RDF triples persistent\nstorage, it seems that, considering reasoning dependent queries, the problem of\nproviding an efficient, in terms of performance, scalability and data\nredundancy, partitioning of the data is still open. In regards to recent data\npartitioning studies, it seems reasonable to think that data partitioning\nshould be guided considering several directions (e.g. ontology, data,\napplication queries). This paper proposes several contributions: describe an\noverview of what a road map for data partitioning for RDF data efficient and\npersistent storage should contain, present some preliminary results and\nanalysis on the particular case of ontology-guided (property hierarchy)\npartitioning and finally introduce a set of semantic query rewriting rules to\nsupport querying RDF data needing OWL inferences\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 22:06:27 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Cur\u00e9", "Olivier", ""], ["Faye", "David", ""], ["Blin", "Guillaume", ""]]}, {"id": "1306.6734", "submitter": "Dhammika Pieris", "authors": "Dhammika Pieris", "title": "A novel ER model to relational model transformation algorithm for\n  semantically clear high quality database design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conceptual modelling using the entity relationship (ER) model has been widely\nused for database design for a long period of time. However, studies indicate\nthat creating a satisfactory relational model representation from an ER model\nis uncertain due to the insufficiencies both in the transformation methods used\nand in the relational model itself. In an effort to solve the issue the\noriginal ER notation has been modified, and accordingly, a new transformation\nalgorithm has been developed. This paper presents the proposed transformation\nalgorithm. Using a real world example it shows how the algorithm can be applied\nin practice. The paper also discusses how to validate the resulted database and\nreclaim the information that it represents.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 07:12:23 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Pieris", "Dhammika", ""]]}, {"id": "1306.6805", "submitter": "Sara Hajian", "authors": "Sara Hajian", "title": "Simultaneous Discrimination Prevention and Privacy Protection in Data\n  Publishing and Mining", "comments": "PhD Thesis defended on June 10, 2013, at the Department of Computer\n  Engineering and Mathematics of Universitat Rovira i Virgili. Advisors: Josep\n  Domingo-Ferrer and Dino Pedreschi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is an increasingly important technology for extracting useful\nknowledge hidden in large collections of data. There are, however, negative\nsocial perceptions about data mining, among which potential privacy violation\nand potential discrimination. Automated data collection and data mining\ntechniques such as classification have paved the way to making automated\ndecisions, like loan granting/denial, insurance premium computation. If the\ntraining datasets are biased in what regards discriminatory attributes like\ngender, race, religion, discriminatory decisions may ensue. In the first part\nof this thesis, we tackle discrimination prevention in data mining and propose\nnew techniques applicable for direct or indirect discrimination prevention\nindividually or both at the same time. We discuss how to clean training\ndatasets and outsourced datasets in such a way that direct and/or indirect\ndiscriminatory decision rules are converted to legitimate (non-discriminatory)\nclassification rules. In the second part of this thesis, we argue that privacy\nand discrimination risks should be tackled together. We explore the\nrelationship between privacy preserving data mining and discrimination\nprevention in data mining to design holistic approaches capable of addressing\nboth threats simultaneously during the knowledge discovery process. As part of\nthis effort, we have investigated for the first time the problem of\ndiscrimination and privacy aware frequent pattern discovery, i.e. the\nsanitization of the collection of patterns mined from a transaction database in\nsuch a way that neither privacy-violating nor discriminatory inferences can be\ninferred on the released patterns. Moreover, we investigate the problem of\ndiscrimination and privacy aware data publishing, i.e. transforming the data,\ninstead of patterns, in order to simultaneously fulfill privacy preservation\nand discrimination prevention.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 12:00:56 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Hajian", "Sara", ""]]}]