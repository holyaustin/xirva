[{"id": "0907.1413", "submitter": "Anand Sarwate", "authors": "Kamalika Chaudhuri, Anand D. Sarwate", "title": "Privacy constraints in regularized convex optimization", "comments": "This paper has been withdrawn by the authors due to some errors.\n  Corrections have been included in arXiv:0912.0071v4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is withdrawn due to some errors, which are corrected in\narXiv:0912.0071v4 [cs.LG].\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2009 06:51:54 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2010 20:36:43 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2011 17:05:53 GMT"}], "update_date": "2011-06-22", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Sarwate", "Anand D.", ""]]}, {"id": "0907.1632", "submitter": "Naveen Ashish", "authors": "Naveen Ashish, Sharad Mehrotra, Pouria Pirzadeh", "title": "Incorporating Integrity Constraints in Uncertain Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We develop an approach to incorporate additional knowledge, in the form of\ngeneral purpose integrity constraints (ICs), to reduce uncertainty in\nprobabilistic databases. While incorporating ICs improves data quality (and\nhence quality of answers to a query), it significantly complicates query\nprocessing. To overcome the additional complexity, we develop an approach to\nmap an uncertain relation U with ICs to another uncertain relation U', that\napproximates the set of consistent worlds represented by U. Queries over U can\ninstead be evaluated over U' achieving higher quality (due to reduced\nuncertainty in U') without additional complexity in query processing due to\nICs. We demonstrate the effectiveness and scalability of our approach to large\ndata-sets with complex constraints. We also present experimental results\ndemonstrating the utility of incorporating integrity constraints in uncertain\nrelations, in the context of an information extraction application.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2009 18:45:29 GMT"}], "update_date": "2009-07-10", "authors_parsed": [["Ashish", "Naveen", ""], ["Mehrotra", "Sharad", ""], ["Pirzadeh", "Pouria", ""]]}, {"id": "0907.1978", "submitter": "Matteo Magnani", "authors": "Matteo Magnani, Danilo Montesi", "title": "BPDMN: A Conservative Extension of BPMN with Enhanced Data\n  Representation Capabilities", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of business processes involves the usage of modeling languages,\ntools and methodologies. In this paper we highlight and address a relevant\nlimitation of the Business Process Modeling Notation (BPMN): its weak data\nrepresentation capabilities. In particular, we extend it with data-specific\nconstructs derived from existing data modeling notations and adapted to blend\ngracefully into BPMN diagrams. The extension has been developed taking existing\nmodeling languages and requirement analyses into account: we characterize our\nnotation using the Workfl ow Data Patterns and provide mappings to the main\nXML-based business process languages.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2009 17:34:55 GMT"}], "update_date": "2009-07-14", "authors_parsed": [["Magnani", "Matteo", ""], ["Montesi", "Danilo", ""]]}, {"id": "0907.2089", "submitter": "Sebastian Maneth", "authors": "A. Arroyuelo, F. Claude, S. Maneth, V. M\\\"akinen, G. Navarro, K.\n  Nguyen, J. Siren, N. V\\\"alim\\\"aki", "title": "Fast In-Memory XPath Search over Compressed Text and Tree Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of an XML document typically consists of text data. The\nXPath query language allows text search via the equal, contains, and\nstarts-with predicates. Such predicates can efficiently be implemented using a\ncompressed self-index of the document's text nodes. Most queries, however,\ncontain some parts of querying the text of the document, plus some parts of\nquerying the tree structure. It is therefore a challenge to choose an\nappropriate evaluation order for a given query, which optimally leverages the\nexecution speeds of the text and tree indexes. Here the SXSI system is\nintroduced; it stores the tree structure of an XML document using a bit array\nof opening and closing brackets, and stores the text nodes of the document\nusing a global compressed self-index. On top of these indexes sits an XPath\nquery engine that is based on tree automata. The engine uses fast counting\nqueries of the text index in order to dynamically determine whether to evaluate\ntop-down or bottom-up with respect to the tree structure. The resulting system\nhas several advantages over existing systems: (1) on pure tree queries (without\ntext search) such as the XPathMark queries, the SXSI system performs on par or\nbetter than the fastest known systems MonetDB and Qizx, (2) on queries that use\ntext search, SXSI outperforms the existing systems by 1--3 orders of magnitude\n(depending on the size of the result set), and (3) with respect to memory\nconsumption, SXSI outperforms all other systems for counting-only queries.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 03:19:23 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2011 11:09:37 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Arroyuelo", "A.", ""], ["Claude", "F.", ""], ["Maneth", "S.", ""], ["M\u00e4kinen", "V.", ""], ["Navarro", "G.", ""], ["Nguyen", "K.", ""], ["Siren", "J.", ""], ["V\u00e4lim\u00e4ki", "N.", ""]]}, {"id": "0907.2471", "submitter": "Oktie Hassanzadeh", "authors": "Oktie Hassanzadeh", "title": "Benchmarking Declarative Approximate Selection Predicates", "comments": "75 pages, 7 figures, February 2007, Masters Thesis at University of\n  Toronto", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Declarative data quality has been an active research topic. The fundamental\nprinciple behind a declarative approach to data quality is the use of\ndeclarative statements to realize data quality primitives on top of any\nrelational data source. A primary advantage of such an approach is the ease of\nuse and integration with existing applications. Several similarity predicates\nhave been proposed in the past for common quality primitives (approximate\nselections, joins, etc.) and have been fully expressed using declarative SQL\nstatements. In this thesis, new similarity predicates are proposed along with\ntheir declarative realization, based on notions of probabilistic information\nretrieval. Then, full declarative specifications of previously proposed\nsimilarity predicates in the literature are presented, grouped into classes\naccording to their primary characteristics. Finally, a thorough performance and\naccuracy study comparing a large number of similarity predicates for data\ncleaning operations is performed.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2009 00:40:06 GMT"}], "update_date": "2009-07-16", "authors_parsed": [["Hassanzadeh", "Oktie", ""]]}, {"id": "0907.2868", "submitter": "Thomas Bernecker", "authors": "Thomas Bernecker, Hans-Peter Kriegel, Nikos Mamoulis, Matthias Renz\n  and Andreas Zuefle", "title": "Scalable Probabilistic Similarity Ranking in Uncertain Databases\n  (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a scalable approach for probabilistic top-k similarity\nranking on uncertain vector data. Each uncertain object is represented by a set\nof vector instances that are assumed to be mutually-exclusive. The objective is\nto rank the uncertain data according to their distance to a reference object.\nWe propose a framework that incrementally computes for each object instance and\nranking position, the probability of the object falling at that ranking\nposition. The resulting rank probability distribution can serve as input for\nseveral state-of-the-art probabilistic ranking models. Existing approaches\ncompute this probability distribution by applying a dynamic programming\napproach of quadratic complexity. In this paper we theoretically as well as\nexperimentally show that our framework reduces this to a linear-time complexity\nwhile having the same memory requirements, facilitated by incremental accessing\nof the uncertain vector instances in increasing order of their distance to the\nreference object. Furthermore, we show how the output of our method can be used\nto apply probabilistic top-k ranking for the objects, according to different\nstate-of-the-art definitions. We conduct an experimental evaluation on\nsynthetic and real data, which demonstrates the efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2009 15:25:50 GMT"}], "update_date": "2009-07-17", "authors_parsed": [["Bernecker", "Thomas", ""], ["Kriegel", "Hans-Peter", ""], ["Mamoulis", "Nikos", ""], ["Renz", "Matthias", ""], ["Zuefle", "Andreas", ""]]}, {"id": "0907.2951", "submitter": "Luca Foschini", "authors": "Chiranjeeb Buragohain, Luca Foschini and Subhash Suri", "title": "Untangling the Braid: Finding Outliers in a Set of Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the performance of large shared computing systems such as the\ncloud computing infrastructure raises many challenging algorithmic problems.\nOne common problem is to track users with the largest deviation from the norm\n(outliers), for some measure of performance. Taking a stream-computing\nperspective, we can think of each user's performance profile as a stream of\nnumbers (such as response times), and the aggregate performance profile of the\nshared infrastructure as a \"braid\" of these intermixed streams. The monitoring\nsystem's goal then is to untangle this braid sufficiently to track the top k\noutliers. This paper investigates the space complexity of one-pass algorithms\nfor approximating outliers of this kind, proves lower bounds using multi-party\ncommunication complexity, and proposes small-memory heuristic algorithms. On\none hand, stream outliers are easily tracked for simple measures, such as max\nor min, but our theoretical results rule out even good approximations for most\nof the natural measures such as average, median, or the quantiles. On the other\nhand, we show through simulation that our proposed heuristics perform quite\nwell for a variety of synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2009 22:57:53 GMT"}], "update_date": "2009-07-20", "authors_parsed": [["Buragohain", "Chiranjeeb", ""], ["Foschini", "Luca", ""], ["Suri", "Subhash", ""]]}, {"id": "0907.3183", "submitter": "Shivnath Babu", "authors": "Nedyalko Borisov, Shivnath Babu, Sandeep Uttamchandani, Ramani\n  Routray, Aameek Singh", "title": "Why Did My Query Slow Down?", "comments": "A conference version of this work was published as: Why Did My Query\n  Slow Down, By Nedyalko Borisov, Sandeep Uttamchandani, Ramani Routray, and\n  Aameek Singh, In the Proc. of the 4th Biennial Conference on Innovative Data\n  Systems Research, Asilomar, CA, USA, Jan 4-7, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many enterprise environments have databases running on network-attached\nserver-storage infrastructure (referred to as Storage Area Networks or SANs).\nBoth the database and the SAN are complex systems that need their own separate\nadministrative teams. This paper puts forth the vision of an innovative\nmanagement framework to simplify administrative tasks that require an in-depth\nunderstanding of both the database and the SAN. As a concrete instance, we\nconsider the task of diagnosing the slowdown in performance of a database query\nthat is executed multiple times (e.g., in a periodic report-generation\nsetting). This task is very challenging because the space of possible causes\nincludes problems specific to the database, problems specific to the SAN, and\nproblems that arise due to interactions between the two systems. In addition,\nthe monitoring data available from these systems can be noisy.\n  We describe the design of DIADS which is an integrated diagnosis tool for\ndatabase and SAN administrators. DIADS generates and uses a powerful\nabstraction called Annotated Plan Graphs (APGs) that ties together the\nexecution path of queries in the database and the SAN. Using an innovative\nworkflow that combines domain-specific knowledge with machine-learning\ntechniques, DIADS was applied successfully to diagnose query slowdowns caused\nby complex combinations of events across a PostgreSQL database and a production\nSAN.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2009 06:37:37 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2011 05:23:52 GMT"}], "update_date": "2011-10-25", "authors_parsed": [["Borisov", "Nedyalko", ""], ["Babu", "Shivnath", ""], ["Uttamchandani", "Sandeep", ""], ["Routray", "Ramani", ""], ["Singh", "Aameek", ""]]}, {"id": "0907.5538", "submitter": "Francesco Carraro", "authors": "Francesco Carraro, Sergio Fonte, Diego Turrini, Maria Cristina De\n  Sanctis, Livia Giacomini", "title": "A preliminary XML-based search system for planetary data", "comments": "10 pages, 5 figures, submitted to Computers and Geosciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR astro-ph.EP cs.DB physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planetary sciences can benefit from several different sources of information,\ni.e. ground-based or near Earth-based observations, space missions and\nlaboratory experiments. The data collected from these sources, however, are\nspread over a number of smaller, separate communities and stored through\ndifferent facilities: this makes it difficult to integrate them. The IDIS\ninitiative, born in the context of the Europlanet project, performed a pilot\nstudy of the viability and the issues to be overcome in order to create an\nintegrated search system for planetary data. As part of the results of such\npilot study, the IDIS Small Bodies and Dust node developed a search system\nbased on a preliminary XML data model. Here we introduce the goals of the IDIS\ninitiative and describe the structure and the working of this search system.\nThe source code of the search system is released under GPL license to allow\npeople interested in participating to the IDIS initiative both as developers\nand as data providers to familiarise with the search environment and to allow\nthe creation of volunteer nodes to be integrated into the existing network.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2009 14:17:34 GMT"}], "update_date": "2009-08-03", "authors_parsed": [["Carraro", "Francesco", ""], ["Fonte", "Sergio", ""], ["Turrini", "Diego", ""], ["De Sanctis", "Maria Cristina", ""], ["Giacomini", "Livia", ""]]}]