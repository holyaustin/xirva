[{"id": "1610.00493", "submitter": "Luciano Barbosa", "authors": "Luciano Barbosa, Breno W. Carvalho, Bianca Zadrozny", "title": "Pooling Hybrid Representations for Web Structured Data Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically identifying data types of web structured data is a key step in\nthe process of web data integration. Web structured data is usually associated\nwith entities or objects in a particular domain. In this paper, we aim to map\nattributes of an entity in a given domain to pre-specified classes of\nattributes in the same domain based on their values. To perform this task, we\npropose a hybrid deep learning network that relies on the format of the\nattributes' values. It does so without any pre-processing or using pre-defined\nhand-crafted features. The hybrid network combines sequence-based neural\nnetworks, namely convolutional neural networks (CNN) and recurrent neural\nnetworks (RNN), to learn the sequence structure of attributes' values. The CNN\ncaptures short-distance dependencies in these sequences through a sliding\nwindow approach, and the RNN captures long-distance dependencies by storing\ninformation of previous characters. These networks create different vector\nrepresentations of the input sequence which are combined using a pooling layer.\nThis layer applies a specific operation on these vectors in order to capture\ntheir most useful patterns for the task. Finally, on top of the pooling layer,\na softmax function predicts the label of a given attribute value. We evaluate\nour strategy in four different web domains. The results show that the pooling\nnetwork outperforms previous approaches, which use some kind of input\npre-processing, in all domains.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 11:12:12 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Barbosa", "Luciano", ""], ["Carvalho", "Breno W.", ""], ["Zadrozny", "Bianca", ""]]}, {"id": "1610.00574", "submitter": "Sepehr Eghbali", "authors": "Sepehr Eghbali and Ladan Tahvildari", "title": "Fast Cosine Similarity Search in Binary Space with Angular Multi-index\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large dataset of binary codes and a binary query point, we address\nhow to efficiently find $K$ codes in the dataset that yield the largest cosine\nsimilarities to the query. The straightforward answer to this problem is to\ncompare the query with all items in the dataset, but this is practical only for\nsmall datasets. One potential solution to enhance the search time and achieve\nsublinear cost is to use a hash table populated with binary codes of the\ndataset and then look up the nearby buckets to the query to retrieve the\nnearest neighbors. However, if codes are compared in terms of cosine similarity\nrather than the Hamming distance, then the main issue is that the order of\nbuckets to probe is not evident. To examine this issue, we first elaborate on\nthe connection between the Hamming distance and the cosine similarity. Doing\nthis allows us to systematically find the probing sequence in the hash table.\nHowever, solving the nearest neighbor search with a single table is only\npractical for short binary codes. To address this issue, we propose the angular\nmulti-index hashing search algorithm which relies on building multiple hash\ntables on binary code substrings. The proposed search algorithm solves the\nexact angular $K$ nearest neighbor problem in a time that is often orders of\nmagnitude faster than the linear scan baseline and even approximation methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 23:16:37 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 12:55:36 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Eghbali", "Sepehr", ""], ["Tahvildari", "Ladan", ""]]}, {"id": "1610.02455", "submitter": "Ying Zhang Dr.", "authors": "Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Wenjie Zhang, Xuemin Lin", "title": "Approximate Nearest Neighbor Search on High Dimensional Data ---\n  Experiments, Analyses, and Improvement (v1.0)", "comments": "All source codes of the algorithms evaluated are available at\n  https://github.com/DBWangGroupUNSW/nns_benchmark. arXiv admin note: text\n  overlap with arXiv:1506.03163 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Nearest neighbor search (ANNS) is fundamental and essential\noperation in applications from many domains, such as databases, machine\nlearning, multimedia, and computer vision. Although many algorithms have been\ncontinuously proposed in the literature in the above domains each year, there\nis no comprehensive evaluation and analysis of their performances.\n  In this paper, we conduct a comprehensive experimental evaluation of many\nstate-of-the-art methods for approximate nearest neighbor search. Our study (1)\nis cross-disciplinary (i.e., including 16 algorithms in different domains, and\nfrom practitioners) and (2) has evaluated a diverse range of settings,\nincluding 20 datasets, several evaluation metrics, and different query\nworkloads. The experimental results are carefully reported and analyzed to\nunderstand the performance results. Furthermore, we propose a new method that\nachieves both high query efficiency and high recall empirically on majority of\nthe datasets under a wide range of settings.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 00:40:14 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Li", "Wen", ""], ["Zhang", "Ying", ""], ["Sun", "Yifang", ""], ["Wang", "Wei", ""], ["Zhang", "Wenjie", ""], ["Lin", "Xuemin", ""]]}, {"id": "1610.02483", "submitter": "Wan-Lei Zhao", "authors": "Wan-Lei Zhao, Cheng-Hao Deng, Chong-Wah Ngo", "title": "Boost K-Means", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its simplicity and versatility, k-means remains popular since it was\nproposed three decades ago. The performance of k-means has been enhanced from\ndifferent perspectives over the years. Unfortunately, a good trade-off between\nquality and efficiency is hardly reached. In this paper, a novel k-means\nvariant is presented. Different from most of k-means variants, the clustering\nprocedure is driven by an explicit objective function, which is feasible for\nthe whole l2-space. The classic egg-chicken loop in k-means has been simplified\nto a pure stochastic optimization procedure. The procedure of k-means becomes\nsimpler and converges to a considerably better local optima. The effectiveness\nof this new variant has been studied extensively in different contexts, such as\ndocument clustering, nearest neighbor search and image clustering. Superior\nperformance is observed across different scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 04:36:42 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 07:32:37 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhao", "Wan-Lei", ""], ["Deng", "Cheng-Hao", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "1610.02876", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Wil M. P. van der Aalst, Reinder Haakma", "title": "Heuristic Approaches for Generating Local Process Models through Log\n  Projections", "comments": "paper accepted and to appear in the proceedings of the IEEE Symposium\n  on Computational Intelligence and Data Mining (CIDM), special session on\n  Process Mining, part of the Symposium Series on Computational Intelligence\n  (SSCI)", "journal-ref": "Proceedings of the IEEE Symposium Series on Computational\n  Intelligence (SSCI), (2016) 1-8", "doi": "10.1109/SSCI.2016.7849948", "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Process Model (LPM) discovery is focused on the mining of a set of\nprocess models where each model describes the behavior represented in the event\nlog only partially, i.e. subsets of possible events are taken into account to\ncreate so-called local process models. Often such smaller models provide\nvaluable insights into the behavior of the process, especially when no adequate\nand comprehensible single overall process model exists that is able to describe\nthe traces of the process from start to end. The practical application of LPM\ndiscovery is however hindered by computational issues in the case of logs with\nmany activities (problems may already occur when there are more than 17 unique\nactivities). In this paper, we explore three heuristics to discover subsets of\nactivities that lead to useful log projections with the goal of speeding up LPM\ndiscovery considerably while still finding high-quality LPMs. We found that a\nMarkov clustering approach to create projection sets results in the largest\nimprovement of execution time, with discovered LPMs still being better than\nwith the use of randomly generated activity sets of the same size. Another\nheuristic, based on log entropy, yields a more moderate speedup, but enables\nthe discovery of higher quality LPMs. The third heuristic, based on the\nrelative information gain, shows unstable performance: for some data sets the\nspeedup and LPM quality are higher than with the log entropy based method,\nwhile for other data sets there is no speedup at all.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 12:12:46 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["van der Aalst", "Wil M. P.", ""], ["Haakma", "Reinder", ""]]}, {"id": "1610.03579", "submitter": "Farhana Murtaza Choudhury", "authors": "Farhana M. Choudhury, Zhifeng Bao, J. Shane Culpepper, Timos Sellis", "title": "Monitoring the Top-m Aggregation in a Sliding Window of Spatial Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study the problem of top-m rank aggregation of\nspatial objects in streaming queries, where, given a set of objects O, a stream\nof spatial queries (kNN or range), the goal is to report m objects with the\nhighest aggregate rank. The rank of an object w.r.t. an individual query is\ncomputed based on its distance from the query location, and the aggregate rank\nis computed from all of the individual rank orderings. Solutions to this\nfundamental problem can be used to monitor the popularity of spatial objects,\nwhich in turn can provide new analytical tools for spatial data. Our work draws\ninspiration from three different domains: rank aggregation, continuous queries\nand spatial databases. To the best of our knowledge, there is no prior work\nthat considers all three problem domains in a single context. Our problem is\ndifferent from the classical rank aggregation problem in the way that the rank\nof spatial objects are dependent on streaming queries whose locations are not\nknown a priori, and is different from the problem of continuous spatial queries\nbecause new query locations can arrive in any region, but do not move. In order\nto solve this problem, we show how to upper and lower bound the rank of an\nobject for any unseen query. Then we propose an approximation solution to\ncontinuously monitor the top-m objects efficiently, for which we design an\nInverted Rank File (IRF) index to guarantee the error bound of the solution. In\nparticular, we propose the notion of safe ranking to determine whether the\ncurrent result is still valid or not when new queries arrive, and propose the\nnotion of validation objects to limit the number of objects to update in the\ntop-m results. We also propose an exact solution for applications where an\napproximate solution is not sufficient. Last, we conduct extensive experiments\nto verify the efficiency and effectiveness of our solutions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 02:12:18 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 01:30:02 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Choudhury", "Farhana M.", ""], ["Bao", "Zhifeng", ""], ["Culpepper", "J. Shane", ""], ["Sellis", "Timos", ""]]}, {"id": "1610.04315", "submitter": "Renzo Angles", "authors": "Renzo Angles and Claudio Gutierrez", "title": "The multiset semantics of SPARQL patterns", "comments": "This is an extended and updated version of the paper accepted at the\n  International Semantic Web Conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper determines the algebraic and logic structure of the multiset\nsemantics of the core patterns of SPARQL. We prove that the fragment formed by\nAND, UNION, OPTIONAL, FILTER, MINUS and SELECT corresponds precisely to both,\nthe intuitive multiset relational algebra (projection, selection, natural join,\narithmetic union and except), and the multiset non-recursive Datalog with safe\nnegation.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 03:19:54 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Angles", "Renzo", ""], ["Gutierrez", "Claudio", ""]]}, {"id": "1610.04577", "submitter": "Kratika Tyagi", "authors": "Kratika Tyagi, Prof. Sanjeev Thakur", "title": "A Survey on Various Data Mining Techniques for ECG Meta Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Mining is the process of examining the information from different point\nof view and compressing it for the relevant data. This data can also be\nutilized to build the incomes. Data Mining is also known as Data or Knowledge\nDiscovery. The basic purpose of data mining is to search patterns which have\nminimal user inputs and efforts. Data Mining plays a very crucial role in the\nvarious fields. There are various data mining procedures which can be connected\nin different fields of innovation. By using data mining techniques, it is\nobserved that less time is taken for the prediction of any disease with more\naccuracy. In this paper we would review various data mining techniques which\nare categorized under classification, regression and clustering and apply these\nalgorithms over an ECG dataset. The purpose of this work is to determine the\nmost suitable data mining technique and use it to improve the accuracy of\nanalyzing ECG data for better decision making.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 11:41:23 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Tyagi", "Kratika", ""], ["Thakur", "Prof. Sanjeev", ""]]}, {"id": "1610.04752", "submitter": "Paolo Missier", "authors": "Paolo Missier and Jacek Cala and Maisha Rathi", "title": "Preserving the value of large scale data analytics over time through\n  selective re-computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pervasive problem in Data Science is that the knowledge generated by\npossibly expensive analytics processes is subject to decay over time, as the\ndata used to compute it drifts, the algorithms used in the processes are\nimproved, and the external knowledge embodied by reference datasets used in the\ncomputation evolves. Deciding when such knowledge outcomes should be refreshed,\nfollowing a sequence of data change events, requires problem-specific functions\nto quantify their value and its decay over time, as well as models for\nestimating the cost of their re-computation. What makes this problem\nchallenging is the ambition to develop a decision support system for informing\ndata analytics re-computation decisions over time, that is both generic and\ncustomisable. With the help of a case study from genomics, in this vision paper\nwe offer an initial formalisation of this problem, highlight research\nchallenges, and outline a possible approach based on the collection and\nanalysis of metadata from a history of past computations.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 16:08:22 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Missier", "Paolo", ""], ["Cala", "Jacek", ""], ["Rathi", "Maisha", ""]]}, {"id": "1610.04789", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves and H. V. Jagadish", "title": "Bsmooth: Learning from user feedback to disambiguate query terms in\n  interactive data retrieval", "comments": "30 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is great interest in supporting imprecise queries (e.g., keyword search\nor natural language queries) over databases today. To support such queries, the\ndatabase system is typically required to disambiguate parts of the\nuser-specified query against the database, using whatever resources are\nintrinsically available to it (the database schema, data values distributions,\nnatural language models etc). Often, systems will also have a user-interaction\nlog available, which can serve as an extrinsic resource to supplement their\nmodel based on their own intrinsic resources. This leads to a problem of how\nbest to combine the system's prior ranking with insight derived from the\nuser-interaction log. Statistical inference techniques such as maximum\nlikelihood or Bayesian updates from a subjective prior turn out not to apply in\na straightforward way due to possible noise from user search behavior and to\nencoding biases endemic to the system's models. In this paper, we address such\nlearning problem in interactive data retrieval, with specific focus on type\nclassification for user-specified query terms. We develop a novel Bayesian\nsmoothing algorithm, Bsmooth, which is simple, fast, flexible and accurate. We\nanalytically establish some desirable properties and show, through experiments\nagainst an independent benchmark, that the addition of such a learning layer\nperforms much better than standard methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 22:24:50 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 00:58:47 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 15:37:29 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""], ["Jagadish", "H. V.", ""]]}, {"id": "1610.04963", "submitter": "Hui Miao", "authors": "Hui Miao, Amit Chavan, Amol Deshpande", "title": "ProvDB: A System for Lifecycle Management of Collaborative Analysis\n  Workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data-driven methods are becoming pervasive in a wide variety of\ndisciplines, there is an urgent need to develop scalable and sustainable tools\nto simplify the process of data science, to make it easier to keep track of the\nanalyses being performed and datasets being generated, and to enable\nintrospection of the workflows. In this paper, we describe our vision of a\nunified provenance and metadata management system to support lifecycle\nmanagement of complex collaborative data science workflows. We argue that a\nlarge amount of information about the analysis processes and data artifacts\ncan, and should be, captured in a semi-passive manner; and we show that\nquerying and analyzing this information can not only simplify bookkeeping and\ndebugging tasks for data analysts but can also enable a rich new set of\ncapabilities like identifying flaws in the data science process itself. It can\nalso significantly reduce the time spent in fixing post-deployment problems\nthrough automated analysis and monitoring. We have implemented an initial\nprototype of our system, called ProvDB, on top of git (a version control\nsystem) and Neo4j (a graph database), and we describe its key features and\ncapabilities.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 03:22:58 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Miao", "Hui", ""], ["Chavan", "Amit", ""], ["Deshpande", "Amol", ""]]}, {"id": "1610.05121", "submitter": "Fang Junhua", "authors": "Junhua Fang, Rong Zhang, Tom Z.J.Fu, Zhenjie Zhang, Aoying Zhou,\n  Junhua Zhu", "title": "Parallel Stream Processing Against Workload Skewness and Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key-based workload partitioning is a common strategy used in parallel stream\nprocessing engines, enabling effective key-value tuple distribution over worker\nthreads in a logical operator. While randomized hashing on the keys is capable\nof balancing the workload for key-based partitioning when the keys generally\nfollow a static distribution, it is likely to generate poor balancing\nperformance when workload variance occurs on the incoming data stream. This\npaper presents a new key-based workload partitioning framework, with practical\nalgorithms to support dynamic workload assignment for stateful operators. The\nframework combines hash-based and explicit key-based routing strategies for\nworkload distribution, which specifies the destination worker threads for a\nhandful of keys and assigns the other keys with the hashing function. When\nshort-term distribution fluctuations occur to the incoming data stream, the\nsystem adaptively updates the routing table containing the chosen keys, in\norder to rebalance the workload with minimal migration overhead within the\nstateful operator. We formulate the rebalance operation as an optimization\nproblem, with multiple objectives on minimizing state migration costs,\ncontrolling the size of the routing table and breaking workload imbalance among\nworker threads. Despite of the NP-hardness nature behind the optimization\nformulation, we carefully investigate and justify the heuristics behind key\n(re)routing and state migration, to facilitate fast response to workload\nvariance with ignorable cost to the normal processing in the distributed\nsystem. Empirical studies on synthetic data and real-world stream applications\nvalidate the usefulness of our proposals and prove the huge advantage of our\napproaches over state-of-the-art solutions in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 14:03:41 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 09:04:03 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Fang", "Junhua", ""], ["Zhang", "Rong", ""], ["Fu", "Tom Z. J.", ""], ["Zhang", "Zhenjie", ""], ["Zhou", "Aoying", ""], ["Zhu", "Junhua", ""]]}, {"id": "1610.05796", "submitter": "Koray Mancuhan", "authors": "Koray Mancuhan and Chris Clifton", "title": "Decision Tree Classification on Outsourced Data", "comments": "Presented in the Data Ethics Workshop at the 20th ACM SIGKDD\n  Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a client-server decision tree learning method for\noutsourced private data. The privacy model is anatomization/fragmentation: the\nserver sees data values, but the link between sensitive and identifying\ninformation is encrypted with a key known only to clients. Clients have limited\nprocessing and storage capability. Both sensitive and identifying information\nthus are stored on the server. The approach presented also retains most\nprocessing at the server, and client-side processing is amortized over\npredictions made by the clients. Experiments on various datasets show that the\nmethod produces decision trees approaching the accuracy of a non-private\ndecision tree, while substantially reducing the client's computing resource\nrequirements.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 20:49:21 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Mancuhan", "Koray", ""], ["Clifton", "Chris", ""]]}, {"id": "1610.05815", "submitter": "Koray Mancuhan", "authors": "Koray Mancuhan and Chris Clifton", "title": "Statistical Learning Theory Approach for Data Classification with\n  l-diversity", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corporations are retaining ever-larger corpuses of personal data; the\nfrequency or breaches and corresponding privacy impact have been rising\naccordingly. One way to mitigate this risk is through use of anonymized data,\nlimiting the exposure of individual data to only where it is absolutely needed.\nThis would seem particularly appropriate for data mining, where the goal is\ngeneralizable knowledge rather than data on specific individuals. In practice,\ncorporate data miners often insist on original data, for fear that they might\n\"miss something\" with anonymized or differentially private approaches. This\npaper provides a theoretical justification for the use of anonymized data.\nSpecifically, we show that a support vector classifier trained on anatomized\ndata satisfying l-diversity should be expected to do as well as on the original\ndata. Anatomy preserves all data values, but introduces uncertainty in the\nmapping between identifying and sensitive values, thus satisfying l-diversity.\nThe theoretical effectiveness of the proposed approach is validated using\nseveral publicly available datasets, showing that we outperform the state of\nthe art for support vector classification using training data protected by\nk-anonymity, and are comparable to learning on the original data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 22:14:27 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Mancuhan", "Koray", ""], ["Clifton", "Chris", ""]]}, {"id": "1610.06044", "submitter": "Carl Kesselman", "authors": "Karl Czajkowski, Carl Kesselman, Robert Schuler, Hongsuda\n  Tangmunarunkit", "title": "ERMrest: an entity-relationship data storage service for web-based,\n  data-oriented collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific discovery is increasingly dependent on a scientist's ability to\nacquire, curate, integrate, analyze, and share large and diverse collections of\ndata. While the details vary from domain to domain, these data often consist of\ndiverse digital assets (e.g. image files, sequence data, or simulation outputs)\nthat are organized with complex relationships and context which may evolve over\nthe course of an investigation. In addition, discovery is often collaborative,\nsuch that sharing of the data and its organizational context is highly\ndesirable. Common systems for managing file or asset metadata hide their\ninherent relational structures, while traditional relational database systems\ndo not extend to the distributed collaborative environment often seen in\nscientific investigations. To address these issues, we introduce ERMrest, a\ncollaborative data management service which allows general entity-relationship\nmodeling of metadata manipulated by RESTful access methods. We present the\ndesign criteria, architecture, and service implementation, as well as describe\nan ecosystem of tools and services that we have created to integrate metadata\ninto an end-to-end scientific data life cycle. ERMrest has been deployed to\nhundreds of users across multiple scientific research communities and projects.\nWe present two representative use cases: an international consortium and an\nearly-phase, multidisciplinary research project.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 14:52:15 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Czajkowski", "Karl", ""], ["Kesselman", "Carl", ""], ["Schuler", "Robert", ""], ["Tangmunarunkit", "Hongsuda", ""]]}, {"id": "1610.06048", "submitter": "Koray Mancuhan", "authors": "Koray Mancuhan and Chris Clifton", "title": "K-Nearest Neighbor Classification Using Anatomized Data", "comments": "Technical Report. arXiv admin note: text overlap with arXiv:\n  1610.05815", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes k nearest neighbor classification with training data\nanonymized using anatomy. Anatomy preserves all data values, but introduces\nuncertainty in the mapping between identifying and sensitive values. We first\nstudy the theoretical effect of the anatomized training data on the k nearest\nneighbor error rate bounds, nearest neighbor convergence rate, and Bayesian\nerror. We then validate the derived bounds empirically. We show that 1)\nLearning from anatomized data approaches the limits of learning through the\nunprotected data (although requiring larger training data), and 2) nearest\nneighbor using anatomized data outperforms nearest neighbor on\ngeneralization-based anonymization.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:00:59 GMT"}], "update_date": "2016-10-30", "authors_parsed": [["Mancuhan", "Koray", ""], ["Clifton", "Chris", ""]]}, {"id": "1610.06084", "submitter": "Suresh Damodaran", "authors": "Suresh K. Damodaran and Pedro A. Colon-Hernandez", "title": "Portable Ontological Expressions in NoSQL Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant barrier to the portability of queries across di- verse physical\nimplementations of large data stores, espe- cially NoSQL data stores, is that\nthe queries reference the physical storage attributes, such as the table and\ncolumn names. In this paper, we describe a technique for embed- ding\nontological expressions called Address Expressions, or A-Expressions, in NoSQL\nqueries to improve their portability across diverse physical implementations.\nWe discuss an implementation of such queries over a MongoDB data store of the\nEnron email corpus with examples, and conduct a preliminary performance\nassessment.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 16:07:30 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Damodaran", "Suresh K.", ""], ["Colon-Hernandez", "Pedro A.", ""]]}, {"id": "1610.06249", "submitter": "Kien Do", "authors": "Kien Do and Truyen Tran and Svetha Venkatesh", "title": "Multilevel Anomaly Detection for Mixed Data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies are those deviating from the norm. Unsupervised anomaly detection\noften translates to identifying low density regions. Major problems arise when\ndata is high-dimensional and mixed of discrete and continuous attributes. We\npropose MIXMAD, which stands for MIXed data Multilevel Anomaly Detection, an\nensemble method that estimates the sparse regions across multiple levels of\nabstraction of mixed data. The hypothesis is for domains where multiple data\nabstractions exist, a data point may be anomalous with respect to the raw\nrepresentation or more abstract representations. To this end, our method\nsequentially constructs an ensemble of Deep Belief Nets (DBNs) with varying\ndepths. Each DBN is an energy-based detector at a predefined abstraction level.\nAt the bottom level of each DBN, there is a Mixed-variate Restricted Boltzmann\nMachine that models the density of mixed data. Predictions across the ensemble\nare finally combined via rank aggregation. The proposed MIXMAD is evaluated on\nhigh-dimensional realworld datasets of different characteristics. The results\ndemonstrate that for anomaly detection, (a) multilevel abstraction of\nhigh-dimensional and mixed data is a sensible strategy, and (b) empirically,\nMIXMAD is superior to popular unsupervised detection methods for both\nhomogeneous and mixed data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 00:04:55 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Do", "Kien", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1610.06264", "submitter": "Pablo Barcelo", "authors": "Renzo Angles, Marcelo Arenas, Pablo Barcelo, Aidan Hogan, Juan\n  Reutter, and Domagoj Vrgoc", "title": "Foundations of Modern Query Languages for Graph Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey foundational features underlying modern graph query languages. We\nfirst discuss two popular graph data models: edge-labelled graphs, where nodes\nare connected by directed, labelled edges; and property graphs, where nodes and\nedges can further have attributes. Next we discuss the two most fundamental\ngraph querying functionalities: graph patterns and navigational expressions. We\nstart with graph patterns, in which a graph-structured query is matched against\nthe data. Thereafter we discuss navigational expressions, in which patterns can\nbe matched recursively against the graph to navigate paths of arbitrary length;\nwe give an overview of what kinds of expressions have been proposed, and how\nthey can be combined with graph patterns. We also discuss several semantics\nunder which queries using the previous features can be evaluated, what effects\nthe selection of features and semantics has on complexity, and offer examples\nof such features in three modern languages that are used to query graphs:\nSPARQL, Cypher and Gremlin. We conclude by discussing the importance of\nformalisation for graph query languages; a summary of what is known about\nSPARQL, Cypher and Gremlin in terms of expressivity and complexity; and an\noutline of possible future directions for the area.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 01:59:48 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 21:24:42 GMT"}, {"version": "v3", "created": "Thu, 15 Jun 2017 19:31:04 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Angles", "Renzo", ""], ["Arenas", "Marcelo", ""], ["Barcelo", "Pablo", ""], ["Hogan", "Aidan", ""], ["Reutter", "Juan", ""], ["Vrgoc", "Domagoj", ""]]}, {"id": "1610.06298", "submitter": "Mohammed Eunus Ali Dr", "authors": "Md Tamzeed Islam, Bashima Islam, and Mohammed Eunus Ali", "title": "A System for Identifying and Visualizing Influential Communities", "comments": "in VLDB Companion (SoDAM 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the concept of influential communities in a\nco-author network. We term a community as the most influential if the community\nhas the highest influence among all other communities in the entire network.\nInfluence of a community depends on the impact of the contents (e.g., citations\nof papers) generated by the members of that community. We propose an algorithm\nto identify the top K influential communities of an online social network. As a\nworking prototype, we develop a visualization system that allows a user to find\nthe top K influential communities from a co-author network. A user can search\ntop K influential communities of particular research fields and our system\nprovides him/her with a visualization of these communities. A user can explore\nthe details of a community, such as authors, citations, and collaborations with\nother communities.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 06:23:15 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Islam", "Md Tamzeed", ""], ["Islam", "Bashima", ""], ["Ali", "Mohammed Eunus", ""]]}, {"id": "1610.06382", "submitter": "Andreas M. Wahl", "authors": "Andreas M. Wahl, Gregor Endler, Peter K. Schwab, Sebastian Herbst,\n  Richard Lenz", "title": "Anfrage-getriebener Wissenstransfer zur Unterstuetzung von\n  Datenanalysten", "comments": "in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In larger organizations, multiple teams of data scientists have to integrate\ndata from heterogeneous data sources as preparation for data analysis tasks.\nWriting effective analytical queries requires data scientists to have in-depth\nknowledge of the existence, semantics, and usage context of data sources. Once\ngathered, such knowledge is informally shared within a specific team of data\nscientists, but usually is neither formalized nor shared with other teams.\nPotential synergies remain unused. We therefore introduce a novel approach\nwhich extends data management systems with additional knowledge-sharing\ncapabilities to facilitate user collaboration without altering established data\nanalysis workflows. Relevant collective knowledge from the query log is\nextracted to support data source discovery and incremental data integration.\nExtracted knowledge is formalized and provided at query time.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 12:41:31 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Wahl", "Andreas M.", ""], ["Endler", "Gregor", ""], ["Schwab", "Peter K.", ""], ["Herbst", "Sebastian", ""], ["Lenz", "Richard", ""]]}, {"id": "1610.06500", "submitter": "Bernd Amann", "authors": "Nelly Vouzoukidou, Bernd Amann, Vassilis Christophides", "title": "Continuous Top-k Queries over Real-Time Web Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web has become a large-scale real-time information system forcing us to\nrevise both how to effectively assess relevance of information for a user and\nhow to efficiently implement information retrieval and dissemination\nfunctionality. To increase information relevance, Real-time Web applications\nsuch as Twitter and Facebook, extend content and social-graph relevance scores\nwith \"real-time\" user generated events (e.g. re-tweets, replies, likes). To\naccommodate high arrival rates of information items and user events we explore\na publish/subscribe paradigm in which we index queries and update on the fly\ntheir results each time a new item and relevant events arrive. In this setting,\nwe need to process continuous top-k text queries combining both static and\ndynamic scores. To the best of our knowledge, this is the first work addressing\nhow non-predictable, dynamic scores can be handled in a continuous top-k query\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 17:02:20 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Vouzoukidou", "Nelly", ""], ["Amann", "Bernd", ""], ["Christophides", "Vassilis", ""]]}, {"id": "1610.06978", "submitter": "Fernando Chirigati", "authors": "Fernando Chirigati, Harish Doraiswamy, Theodoros Damoulas, Juliana\n  Freire", "title": "Data Polygamy: The Many-Many Relationships among Urban Spatio-Temporal\n  Data Sets", "comments": null, "journal-ref": "Proceedings of the 2016 International Conference on Management of\n  Data (SIGMOD '16), pp. 1011-1025", "doi": "10.1145/2882903.2915245", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing ability to collect data from urban environments, coupled with\na push towards openness by governments, has resulted in the availability of\nnumerous spatio-temporal data sets covering diverse aspects of a city.\nDiscovering relationships between these data sets can produce new insights by\nenabling domain experts to not only test but also generate hypotheses. However,\ndiscovering these relationships is difficult. First, a relationship between two\ndata sets may occur only at certain locations and/or time periods. Second, the\nsheer number and size of the data sets, coupled with the diverse spatial and\ntemporal scales at which the data is available, presents computational\nchallenges on all fronts, from indexing and querying to analyzing them.\nFinally, it is non-trivial to differentiate between meaningful and spurious\nrelationships. To address these challenges, we propose Data Polygamy, a\nscalable topology-based framework that allows users to query for statistically\nsignificant relationships between spatio-temporal data sets. We have performed\nan experimental evaluation using over 300 spatial-temporal urban data sets\nwhich shows that our approach is scalable and effective at identifying\ninteresting relationships.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 23:59:30 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Chirigati", "Fernando", ""], ["Doraiswamy", "Harish", ""], ["Damoulas", "Theodoros", ""], ["Freire", "Juliana", ""]]}, {"id": "1610.07543", "submitter": "Yonghui Xiao", "authors": "Yang Cao, Masatoshi Yoshikawa, Yonghui Xiao, Li Xiong", "title": "Quantifying Differential Privacy under Temporal Correlations", "comments": "appears at ICDE 2017", "journal-ref": "ICDE 2017", "doi": "10.1109/ICDE.2017.132", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential Privacy (DP) has received increased attention as a rigorous\nprivacy framework. Existing studies employ traditional DP mechanisms (e.g., the\nLaplace mechanism) as primitives, which assume that the data are independent,\nor that adversaries do not have knowledge of the data correlations. However,\ncontinuously generated data in the real world tend to be temporally correlated,\nand such correlations can be acquired by adversaries. In this paper, we\ninvestigate the potential privacy loss of a traditional DP mechanism under\ntemporal correlations in the context of continuous data release. First, we\nmodel the temporal correlations using Markov model and analyze the privacy\nleakage of a DP mechanism when adversaries have knowledge of such temporal\ncorrelations. Our analysis reveals that the privacy leakage of a DP mechanism\nmay accumulate and increase over time. We call it temporal privacy leakage.\nSecond, to measure such privacy leakage, we design an efficient algorithm for\ncalculating it in polynomial time. Although the temporal privacy leakage may\nincrease over time, we also show that its supremum may exist in some cases.\nThird, to bound the privacy loss, we propose mechanisms that convert any\nexisting DP mechanism into one against temporal privacy leakage. Experiments\nwith synthetic data confirm that our approach is efficient and effective.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 18:54:18 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 15:38:41 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Cao", "Yang", ""], ["Yoshikawa", "Masatoshi", ""], ["Xiao", "Yonghui", ""], ["Xiong", "Li", ""]]}, {"id": "1610.07649", "submitter": "Qiong Hu", "authors": "Qiong Hu and Tomasz Imielinski", "title": "ALPINE: Anytime Mining with Definite Guarantees", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ALPINE is to our knowledge the first anytime algorithm to mine frequent\nitemsets and closed frequent itemsets. It guarantees that all itemsets with\nsupport exceeding the current checkpoint's support have been found before it\nproceeds further. Thus, it is very attractive for extremely long mining tasks\nwith very high dimensional data (for example in genetics) because it can offer\nintermediate meaningful and complete results. This ANYTIME feature is the most\nimportant contribution of ALPINE, which is also fast but not necessarily the\nfastest algorithm around. Another critical advantage of ALPINE is that it does\nnot require the apriori decided minimum support value.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 20:52:57 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Hu", "Qiong", ""], ["Imielinski", "Tomasz", ""]]}, {"id": "1610.07707", "submitter": "Xiaowang Zhang", "authors": "Xiaowang Zhang and Jiahui Zhang and Muhammad Qasim Yasin and Wenrui Wu\n  and Zhiyong Feng", "title": "Path discovery by Querying the federation of Relational Database and RDF\n  Graph", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of queries for detecting path is an important as those can extract\nimplicit binary relations over the nodes of input graphs. Most of the path\nquerying languages used by the RDF community, like property paths in W3C SPARQL\n1.1 and nested regular expressions in nSPARQL are based on the regular\nexpressions. Federated queries allow for combining graph patterns and\nrelational database that enables the evaluations over several heterogeneous\ndata resources within a single query. Federated queries in W3C SPARQL 1.1\ncurrently evaluated over different SPARQL endpoints. In this paper, we present\na federated path querying language as an extension of regular path querying\nlanguage for supporting RDF graph integration with relational database. The\nfederated path querying language is absolutely more expressive than nested\nregular expressions and negation-free property paths. Its additional\nexpressivity can be used for capturing the conjunction and federation of nested\nregular path queries. Despite the increase in expressivity, we also show that\nfederated path queries are still enjoy a low computational complexity and can\nbe evaluated efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 02:12:45 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Zhang", "Xiaowang", ""], ["Zhang", "Jiahui", ""], ["Yasin", "Muhammad Qasim", ""], ["Wu", "Wenrui", ""], ["Feng", "Zhiyong", ""]]}, {"id": "1610.07732", "submitter": "Anja Gruenheid", "authors": "Anja Gruenheid, Donald Kossmann, Divesh Srivastava", "title": "Online Event Integration with StoryPivot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data integration systems need to process large amounts of data from a\nvariety of data sources and with real-time integration constraints. They are\nnot only employed in enterprises for managing internal data but are also used\nfor a variety of web services that use techniques such as entity resolution or\ndata cleaning in live systems. In this work, we discuss a new generation of\ndata integration systems that operate on (un-)structured data in an online\nsetting, i.e., systems which process continuously modified datasets upon which\nthe integration task is based. We use as an example of such a system an online\nevent integration system called StoryPivot. It observes events extracted from\nnews articles in data sources such as the 'Guardian' or the 'Washington Post'\nwhich are integrated to show users the evolution of real-world stories over\ntime. The design decisions for StoryPivot are influenced by the trade-off\nbetween maintaining high quality integration results while at the same time\nbuilding a system that processes and integrates events in near real-time. We\nevaluate our design decisions with experiments on two real-world datasets and\ngeneralize our findings to other data integration tasks that have a similar\nsystem setup.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 05:10:18 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Gruenheid", "Anja", ""], ["Kossmann", "Donald", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1610.07754", "submitter": "Zhefeng Wang", "authors": "Zhefeng Wang, Yu Yang, Jian Pei and Enhong Chen", "title": "Activity Maximization by Effective Information Diffusion in Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a social network, even about the same information the excitements between\ndifferent pairs of users are different. If you want to spread a piece of new\ninformation and maximize the expected total amount of excitements, which seed\nusers should you choose? This problem indeed is substantially different from\nthe renowned influence maximization problem and cannot be tackled using the\nexisting approaches. In this paper, motivated by the demand in a few\ninteresting applications, we model the novel problem of activity maximization.\nWe tackle the problem systematically. We first analyze the complexity and the\napproximability of the problem. We develop an upper bound and a lower bound\nthat are submodular so that the Sandwich framework can be applied. We then\ndevise a polling-based randomized algorithm that guarantees a data dependent\napproximation factor. Our experiments on three real data sets clearly verify\nthe effectiveness and scalability of our method, as well as the advantage of\nour method against the other heuristic methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 07:24:37 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Wang", "Zhefeng", ""], ["Yang", "Yu", ""], ["Pei", "Jian", ""], ["Chen", "Enhong", ""]]}, {"id": "1610.07930", "submitter": "Upal Mahbub", "authors": "Upal Mahbub, Sayantan Sarkar, Vishal M. Patel, Rama Chellappa", "title": "Active User Authentication for Smartphones: A Challenge Data Set and\n  Benchmark Results", "comments": "8 pages, 12 figures, 6 tables. Best poster award at BTAS 2016", "journal-ref": null, "doi": "10.1109/BTAS.2016.7791155", "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, automated user verification techniques for smartphones are\ninvestigated. A unique non-commercial dataset, the University of Maryland\nActive Authentication Dataset 02 (UMDAA-02) for multi-modal user authentication\nresearch is introduced. This paper focuses on three sensors - front camera,\ntouch sensor and location service while providing a general description for\nother modalities. Benchmark results for face detection, face verification,\ntouch-based user identification and location-based next-place prediction are\npresented, which indicate that more robust methods fine-tuned to the mobile\nplatform are needed to achieve satisfactory verification accuracy. The dataset\nwill be made available to the research community for promoting additional\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 15:56:07 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Mahbub", "Upal", ""], ["Sarkar", "Sayantan", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1610.08015", "submitter": "Nicola Wadeson Dr", "authors": "Nicola Wadeson, Mark Basham", "title": "Savu: A Python-based, MPI Framework for Simultaneous Processing of\n  Multiple, N-dimensional, Large Tomography Datasets", "comments": "10 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diamond Light Source (DLS), the UK synchrotron facility, attracts scientists\nfrom across the world to perform ground-breaking x-ray experiments. With over\n3000 scientific users per year, vast amounts of data are collected across the\nexperimental beamlines, with the highest volume of data collected during\ntomographic imaging experiments. A growing interest in tomography as an imaging\ntechnique, has led to an expansion in the range of experiments performed, in\naddition to a growth in the size of the data per experiment.\n  Savu is a portable, flexible, scientific processing pipeline capable of\nprocessing multiple, n-dimensional datasets in serial on a PC, or in parallel\nacross a cluster. Developed at DLS, and successfully deployed across the\nbeamlines, it uses a modular plugin format to enable experiment-specific\nprocessing and utilises parallel HDF5 to remove RAM restrictions. The Savu\ndesign, described throughout this paper, focuses on easy integration of\nexisting and new functionality, flexibility and ease of use for users and\ndevelopers alike.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 13:22:09 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Wadeson", "Nicola", ""], ["Basham", "Mark", ""]]}, {"id": "1610.08411", "submitter": "Peng Cheng", "authors": "Peng Cheng, Xiang Lian, Xun Jian, Lei Chen", "title": "FROG: A Fast and Reliable Crowdsourcing Framework (Technical Report)", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, the crowdsourcing has gained much attention from both academia\nand industry, which outsources a number of tasks to human workers. Existing\nworks considered improving the task accuracy through voting or learning\nmethods, they usually did not fully take into account reducing the latency of\nthe task completion. When a task requester posts a group of tasks (e.g.,\nsentiment analysis), and one can only obtain answers of all tasks after the\nlast task is accomplished. As a consequence, the time delay of even one task in\nthis group could delay the next step of the task requester's work from minutes\nto days, which is quite undesirable for the task requester.\n  Inspired by the importance of the task accuracy and latency, in this paper,\nwe will propose a novel crowdsourcing framework, namely Fast and Reliable\ncrOwdsourcinG framework (FROG), which intelligently assigns tasks to workers,\nsuch that the latencies of tasks are reduced and the expected accuracies of\ntasks are met. Specifically, our FROG framework consists of two important\ncomponents, task scheduler and notification modules. For the task scheduler\nmodule, we formalize a FROG task scheduling (FROG-TS) problem, in which the\nserver actively assigns workers with tasks with high reliability and low\nlatency. We prove that the FROG-TS problem is NP-hard. Thus, we design two\nheuristic approaches, request-based and batch-based scheduling. For the\nnotification module, we define an efficient worker notifying (EWN) problem,\nwhich only sends task invitations to those workers with high probabilities of\naccepting the tasks. To tackle the EWN problem, we propose a smooth kernel\ndensity estimation approach to estimate the probability that a worker accepts\nthe task invitation. Through extensive experiments, we demonstrate the\neffectiveness and efficiency of our proposed FROG platform on both real and\nsynthetic data sets.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 16:44:11 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 13:32:28 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Cheng", "Peng", ""], ["Lian", "Xiang", ""], ["Jian", "Xun", ""], ["Chen", "Lei", ""]]}, {"id": "1610.08559", "submitter": "Ke Yang", "authors": "Ke Yang and Julia Stoyanovich", "title": "Measuring Fairness in Ranked Outputs", "comments": "5 pages, 7 figures, FATML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking and scoring are ubiquitous. We consider the setting in which an\ninstitution, called a ranker, evaluates a set of individuals based on\ndemographic, behavioral or other characteristics. The final output is a ranking\nthat represents the relative quality of the individuals. While automatic and\ntherefore seemingly objective, rankers can, and often do, discriminate against\nindividuals and systematically disadvantage members of protected groups. This\nwarrants a careful study of the fairness of a ranking scheme.\n  In this paper we propose fairness measures for ranked outputs. We develop a\ndata generation procedure that allows us to systematically control the degree\nof unfairness in the output, and study the behavior of our measures on these\ndatasets. We then apply our proposed measures to several real datasets, and\ndemonstrate cases of unfairness. Finally, we show preliminary results of\nincorporating our ranked fairness measures into an optimization framework, and\nshow potential for improving fairness of ranked outputs while maintaining\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 22:02:39 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Yang", "Ke", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "1610.09166", "submitter": "Amir Shaikhha", "authors": "Amir Shaikhha, Mohammad Dashti, Christoph Koch", "title": "Push vs. Pull-Based Loop Fusion in Query Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database query engines use pull-based or push-based approaches to avoid the\nmaterialization of data across query operators. In this paper, we study these\ntwo types of query engines in depth and present the limitations and advantages\nof each engine. Similarly, the programming languages community has developed\nloop fusion techniques to remove intermediate collections in the context of\ncollection programming. We draw parallels between the DB and PL communities by\ndemonstrating the connection between pipelined query engines and loop fusion\ntechniques. Based on this connection, we propose a new type of pull-based\nengine, inspired by a loop fusion technique, which combines the benefits of\nboth approaches. Then we experimentally evaluate the various engines, in the\ncontext of query compilation, for the first time in a fair environment,\neliminating the biasing impact of ancillary optimizations that have\ntraditionally only been used with one of the approaches. We show that for\nrealistic analytical workloads, there is no considerable advantage for either\nform of pipelined query engine, as opposed to what recent research suggests.\nAlso, by using microbenchmarks we show that our proposed engine dominates the\nexisting engines by combining the benefits of both.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 11:00:02 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Shaikhha", "Amir", ""], ["Dashti", "Mohammad", ""], ["Koch", "Christoph", ""]]}, {"id": "1610.09263", "submitter": "Vladimir Dzyuba", "authors": "Vladimir Dzyuba, Matthijs van Leeuwen, Luc De Raedt", "title": "Flexible constrained sampling with guarantees for pattern mining", "comments": "Accepted for publication in Data Mining & Knowledge Discovery journal\n  (ECML/PKDD 2017 journal track)", "journal-ref": null, "doi": "10.1007/s10618-017-0501-6", "report-no": null, "categories": "cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern sampling has been proposed as a potential solution to the infamous\npattern explosion. Instead of enumerating all patterns that satisfy the\nconstraints, individual patterns are sampled proportional to a given quality\nmeasure. Several sampling algorithms have been proposed, but each of them has\nits limitations when it comes to 1) flexibility in terms of quality measures\nand constraints that can be used, and/or 2) guarantees with respect to sampling\naccuracy. We therefore present Flexics, the first flexible pattern sampler that\nsupports a broad class of quality measures and constraints, while providing\nstrong guarantees regarding sampling accuracy. To achieve this, we leverage the\nperspective on pattern mining as a constraint satisfaction problem and build\nupon the latest advances in sampling solutions in SAT as well as existing\npattern mining algorithms. Furthermore, the proposed algorithm is applicable to\na variety of pattern languages, which allows us to introduce and tackle the\nnovel task of sampling sets of patterns. We introduce and empirically evaluate\ntwo variants of Flexics: 1) a generic variant that addresses the well-known\nitemset sampling task and the novel pattern set sampling task as well as a wide\nrange of expressive constraints within these tasks, and 2) a specialized\nvariant that exploits existing frequent itemset techniques to achieve\nsubstantial speed-ups. Experiments show that Flexics is both accurate and\nefficient, making it a useful tool for pattern-based data exploration.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 15:21:53 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 16:18:39 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Dzyuba", "Vladimir", ""], ["van Leeuwen", "Matthijs", ""], ["De Raedt", "Luc", ""]]}, {"id": "1610.09500", "submitter": "Yiming Lin", "authors": "Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao", "title": "Efficient Entity Resolution on Heterogeneous Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is the problem of identifying and merging records that\nrefer to the same real-world entity. In many scenarios, raw records are stored\nunder heterogeneous environment. Specifically, the schemas of records may\ndiffer from each other. To leverage such records better, most existing work\nassume that schema matching and data exchange have been done to convert records\nunder different schemas to those under a predefined schema. However, we observe\nthat schema matching would lose information in some cases, which could be\nuseful or even crucial to ER.\n  To leverage sufficient information from heterogeneous sources, in this paper,\nwe address several challenges of ER on heterogeneous records and show that none\nof existing similarity metrics or their transformations could be applied to\nfind similar records under heterogeneous settings. Motivated by this, we design\nthe similarity function and propose a novel framework to iteratively find\nrecords which refer to the same entity. Regarding efficiency, we build an index\nto generate candidates and accelerate similarity computation. Evaluations on\nreal-world datasets show the effectiveness and efficiency of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 12:51:52 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lin", "Yiming", ""], ["Wang", "Hongzhi", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}, {"id": "1610.09506", "submitter": "Yiming Lin", "authors": "Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao", "title": "Data Source Selection for Information Integration in Big Data Era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Big data era, information integration often requires abundant data\nextracted from massive data sources. Due to a large number of data sources,\ndata source selection plays a crucial role in information integration, since it\nis costly and even impossible to access all data sources. Data Source selection\nshould consider both efficiency and effectiveness issues. For efficiency, the\napproach should achieve high performance and be scalability to fit large data\nsource amount. From effectiveness aspect, data quality and overlapping of\nsources are to be considered, since data quality varies much from data sources,\nwith significant differences in the accuracy and coverage of the data provided,\nand the overlapping of sources can even lower the quality of data integrated\nfrom selected data sources.\n  In this paper, we study source selection problem in \\textit{Big Data Era} and\npropose methods which can scale to datasets with up to millions of data sources\nand guarantee the quality of results. Motivated by this, we propose a new\nobject function taking the expected number of true values a source can provide\nas a criteria to evaluate the contribution of a data source. Based on our\nproposed index we present a scalable algorithm and two pruning strategies to\nimprove the efficiency without sacrificing precision. Experimental results on\nboth real world and synthetic data sets show that our methods can select\nsources providing a large proportion of true values efficiently and can scale\nto massive data sources.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 13:17:50 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lin", "Yiming", ""], ["Wang", "Hongzhi", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}]