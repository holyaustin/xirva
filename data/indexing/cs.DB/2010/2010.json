[{"id": "2010.00152", "submitter": "Thanh Do", "authors": "Thanh Do, Goetz Graefe", "title": "Sort-based grouping and aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database query processing requires algorithms for duplicate removal,\ngrouping, and aggregation. Three algorithms exist: in-stream aggregation is\nmost efficient by far but requires sorted input; sort-based aggregation relies\non external merge sort; and hash aggregation relies on an in-memory hash table\nplus hash partitioning to temporary storage. Cost-based query optimization\nchooses which algorithm to use based on several factors including input and\noutput sizes, the sort order of the input, and the need for sorted output. For\nexample, hash-based aggregation is ideal for small output (e.g., TPC-H Query\n1), whereas sorting the entire input and aggregating after sorting are\npreferable when both aggregation input and output are large and the output\nneeds to be sorted for a subsequent operation such as a merge join.\n  Unfortunately, the size information required for a sound choice is often\ninaccurate or unavailable during query optimization, leading to sub-optimal\nalgorithm choices. To address this challenge, this paper introduces a new\nalgorithm for sort-based duplicate removal, grouping, and aggregation. The new\nalgorithm always performs at least as well as both traditional hash-based and\ntraditional sort-based algorithms. It can serve as a system's only aggregation\nalgorithm for unsorted inputs, thus preventing erroneous algorithm choices.\nFurthermore, the new algorithm produces sorted output that can speed up\nsubsequent operations. Google's F1 Query uses the new algorithm in production\nworkloads that aggregate petabytes of data every day.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 00:11:37 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Do", "Thanh", ""], ["Graefe", "Goetz", ""]]}, {"id": "2010.00307", "submitter": "Chi Wang", "authors": "Tianyu Liu and Chi Wang", "title": "Understanding the hardness of approximate query processing with joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the hardness of Approximate Query Processing (AQP) of various types\nof queries involving joins over multiple tables of possibly different sizes. In\nthe case where the query result is a single value (e.g., COUNT, SUM, and\nCOUNT(DISTINCT)), we prove worst-case information-theoretic lower bounds for\nAQP problems that are given parameters $\\epsilon$ and $\\delta$, and return\nestimated results within a factor of 1+$\\epsilon$ of the true results with\nerror probability at most $\\delta$. In particular, the lower bounds for\ncardinality estimation over joins under various settings are contained in our\nresults. Informally, our results show that for various database queries with\njoins, unless restricted to the set of queries whose results are always\nguaranteed to be above a very large threshold, the amount of information an AQP\nalgorithm needs for returning an accurate approximation is at least linear in\nthe number of rows in the largest table. Similar lower bounds even hold for\nsome special cases where additional information such as top-K heavy hitters and\nall frequency vectors are available. In the case of GROUP-BY where the query\nresult is not a single number, we study the lower bound for the amount of\ninformation used by any approximation algorithm that does not report any\nnon-existing group and does not miss groups of large total size. Our work\nextends the work of Alon, Gibbons, Matias, and Szegedy [AGMS99].We compare our\nlower bounds with the amount of information required by Bernoulli sampling to\ngive an accurate approximation. For COUNT queries with joins over multiple\ntables of the same size, the upper bound matches the lower bound, unless the\nproblem setting is restricted to the set of queries whose results are always\nguaranteed to be above a very large threshold.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 11:37:23 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Liu", "Tianyu", ""], ["Wang", "Chi", ""]]}, {"id": "2010.00330", "submitter": "Renan Souza", "authors": "Renan Souza, Leonardo G. Azevedo, V\\'itor Louren\\c{c}o, Elton Soares,\n  Raphael Thiago, Rafael Brand\\~ao, Daniel Civitarese, Emilio Vital Brazil,\n  Marcio Moreno, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, Marco A.\n  S. Netto", "title": "Workflow Provenance in the Lifecycle of Scientific Machine Learning", "comments": "21 pages, 10 figures, Under review in a scientific journal since June\n  30th, 2020. arXiv admin note: text overlap with arXiv:1910.04223", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has already fundamentally changed several businesses.\nMore recently, it has also been profoundly impacting the computational science\nand engineering domains, like geoscience, climate science, and health science.\nIn these domains, users need to perform comprehensive data analyses combining\nscientific data and ML models to provide for critical requirements, such as\nreproducibility, model explainability, and experiment data understanding.\nHowever, scientific ML is multidisciplinary, heterogeneous, and affected by the\nphysical constraints of the domain, making such analyses even more challenging.\nIn this work, we leverage workflow provenance techniques to build a holistic\nview to support the lifecycle of scientific ML. We contribute with (i)\ncharacterization of the lifecycle and taxonomy for data analyses; (ii) design\nprinciples to build this view, with a W3C PROV compliant data representation\nand a reference system architecture; and (iii) lessons learned after an\nevaluation in an Oil & Gas case using an HPC cluster with 393 nodes and 946\nGPUs. The experiments show that the principles enable queries that integrate\ndomain semantics with ML models while keeping low overhead (<1%), high\nscalability, and an order of magnitude of query acceleration under certain\nworkloads against without our representation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 13:09:48 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Souza", "Renan", ""], ["Azevedo", "Leonardo G.", ""], ["Louren\u00e7o", "V\u00edtor", ""], ["Soares", "Elton", ""], ["Thiago", "Raphael", ""], ["Brand\u00e3o", "Rafael", ""], ["Civitarese", "Daniel", ""], ["Brazil", "Emilio Vital", ""], ["Moreno", "Marcio", ""], ["Valduriez", "Patrick", ""], ["Mattoso", "Marta", ""], ["Cerqueira", "Renato", ""], ["Netto", "Marco A. S.", ""]]}, {"id": "2010.00728", "submitter": "Christina Pavlopoulou", "authors": "Christina Pavlopoulou, Michael J. Carey, Vassilis J. Tsotras", "title": "Revisiting Runtime Dynamic Optimization for Join Queries in Big Data\n  Management Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query Optimization remains an open problem for Big Data Management Systems.\nTraditional optimizers are cost-based and use statistical estimates of\nintermediate result cardinalities to assign costs and pick the best plan.\nHowever, such estimates tend to become less accurate because of filtering\nconditions caused either from undetected correlations between multiple\npredicates local to a single dataset, predicates with query parameters, or\npredicates involving user-defined functions (UDFs). Consequently, traditional\nquery optimizers tend to ignore or miscalculate those settings, thus leading to\nsuboptimal execution plans. Given the volume of today's data, a suboptimal plan\ncan quickly become very inefficient.\n  In this work, we revisit the old idea of runtime dynamic optimization and\nadapt it to a shared-nothing distributed database system, AsterixDB. The\noptimization runs in stages (re-optimization points), starting by first\nexecuting all predicates local to a single dataset. The intermediate result\ncreated from each stage is used to re-optimize the remaining query. This\nre-optimization approach avoids inaccurate intermediate result cardinality\nestimations, thus leading to much better execution plans. While it introduces\nthe overhead for materializing these intermediate results, our experiments show\nthat this overhead is relatively small and it is an acceptable price to pay\ngiven the optimization benefits. In fact, our experimental evaluation shows\nthat runtime dynamic optimization leads to much better execution plans as\ncompared to the current default AsterixDB plans as well as to plans produced by\nstatic cost-based optimization (i.e. based on the initial dataset statistics)\nand other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 23:42:18 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 19:30:23 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Pavlopoulou", "Christina", ""], ["Carey", "Michael J.", ""], ["Tsotras", "Vassilis J.", ""]]}, {"id": "2010.00843", "submitter": "Frederic Prost", "authors": "Dominique Duval (LJK), Rachid Echahed (LIG), Frederic Prost (LIG)", "title": "All You Need Is CONSTRUCT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In SPARQL, the query forms SELECT and CONSTRUCT have been the subject of\nseveral studies, both theoretical and practical. However, the composition of\nsuch queries and their interweaving when forming involved nested queries has\nnot yet received much interest in the literature. We mainly tackle the problem\nof composing such queries. For this purpose, we introduce a language close to\nSPARQL where queries can be nested at will, involving either CONSTRUCT or\nSELECT query forms and provide a formal semantics for it. This semantics is\nbased on a uniform interpretation of queries. This uniformity is due to an\nextension of the notion of RDF graphs to include isolated items such as\nvariables. As a key feature of this work, we show how classical SELECT queries\ncan be easily encoded as a particular case of CONSTRUCT queries.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:10:32 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Duval", "Dominique", "", "LJK"], ["Echahed", "Rachid", "", "LIG"], ["Prost", "Frederic", "", "LIG"]]}, {"id": "2010.01190", "submitter": "Olaf Hartig", "authors": "Sijin Cheng and Olaf Hartig", "title": "FedQPL: A Language for Logical Query Plans over Heterogeneous\n  Federations of RDF Data Sources (Extended Version)", "comments": "This manuscript is an extended version of a paper in the 22nd\n  International Conference on Information Integration and Web-based\n  Applications & Services (iiWAS2020). The difference to the conference version\n  is that this extended version includes an appendix with the full proofs of\n  the results in the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federations of RDF data sources provide great potential when queried for\nanswers and insights that cannot be obtained from one data source alone. A\nchallenge for planning the execution of queries over such a federation is that\nthe federation may be heterogeneous in terms of the types of data access\ninterfaces provided by the federation members. This challenge has not received\nmuch attention in the literature. This paper provides a solid formal foundation\nfor future approaches that aim to address this challenge. Our main conceptual\ncontribution is a formal language for representing query execution plans;\nadditionally, we identify a fragment of this language that can be used to\ncapture the result of selecting relevant data sources for different parts of a\ngiven query. As technical contributions, we show that this fragment is more\nexpressive than what is supported by existing source selection approaches,\nwhich effectively highlights an inherent limitation of these approaches.\nMoreover, we show that the source selection problem is NP-hard and in\n$\\Sigma_2^\\mathrm{P}$, and we provide a comprehensive set of rewriting rules\nthat can be used as a basis for query optimization.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 20:36:53 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Cheng", "Sijin", ""], ["Hartig", "Olaf", ""]]}, {"id": "2010.01516", "submitter": "Fengmei Jin", "authors": "Fengmei Jin, Wen Hua, Thomas Zhou, Jiajie Xu, Matteo Francia, Maria E\n  Orlowska, Xiaofang Zhou", "title": "Trajectory-Based Spatiotemporal Entity Linking", "comments": "15 pages, 3 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory-based spatiotemporal entity linking is to match the same moving\nobject in different datasets based on their movement traces. It is a\nfundamental step to support spatiotemporal data integration and analysis. In\nthis paper, we study the problem of spatiotemporal entity linking using\neffective and concise signatures extracted from their trajectories. This\nlinking problem is formalized as a k-nearest neighbor (k-NN) query on the\nsignatures. Four representation strategies (sequential, temporal, spatial, and\nspatiotemporal) and two quantitative criteria (commonality and unicity) are\ninvestigated for signature construction. A simple yet effective dimension\nreduction strategy is developed together with a novel indexing structure called\nthe WR-tree to speed up the search. A number of optimization methods are\nproposed to improve the accuracy and robustness of the linking. Our extensive\nexperiments on real-world datasets verify the superiority of our approach over\nthe state-of-the-art solutions in terms of both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 08:45:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Jin", "Fengmei", ""], ["Hua", "Wen", ""], ["Zhou", "Thomas", ""], ["Xu", "Jiajie", ""], ["Francia", "Matteo", ""], ["Orlowska", "Maria E", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "2010.01829", "submitter": "Phuc Nguyen Tri", "authors": "Phuc Nguyen and Natthawut Kertkeidkachorn and Ryutaro Ichise and\n  Hideaki Takeda", "title": "TabEAno: Table to Knowledge Graph Entity Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Open Data era, a large number of table resources have been made\navailable on the Web and data portals. However, it is difficult to directly\nutilize such data due to the ambiguity of entities, name variations,\nheterogeneous schema, missing, or incomplete metadata. To address these issues,\nwe propose a novel approach, namely TabEAno, to semantically annotate table\nrows toward knowledge graph entities. Specifically, we introduce a \"two-cells\"\nlookup strategy bases on the assumption that there is an existing logical\nrelation occurring in the knowledge graph between the two closed cells in the\nsame row of the table. Despite the simplicity of the approach, TabEAno\noutperforms the state of the art approaches in the two standard datasets e.g,\nT2D, Limaye with, and in the large-scale Wikipedia tables dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 07:39:02 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Nguyen", "Phuc", ""], ["Kertkeidkachorn", "Natthawut", ""], ["Ichise", "Ryutaro", ""], ["Takeda", "Hideaki", ""]]}, {"id": "2010.01951", "submitter": "Daniel Ayala", "authors": "Daniel Ayala, Inma Hern\\'andez, David Ruiz, Erhard Rahm", "title": "LEAPME: Learning-based Property Matching with Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration tasks such as the creation and extension of knowledge graphs\ninvolve the fusion of heterogeneous entities from many sources. Matching and\nfusion of such entities require to also match and combine their properties\n(attributes). However, previous schema matching approaches mostly focus on two\nsources only and often rely on simple similarity measurements. They thus face\nproblems in challenging use cases such as the integration of heterogeneous\nproduct entities from many sources.\n  We therefore present a new machine learning-based property matching approach\ncalled LEAPME (LEArning-based Property Matching with Embeddings) that utilizes\nnumerous features of both property names and instance values. The approach\nheavily makes use of word embeddings to better utilize the domain-specific\nsemantics of both property names and instance values. The use of supervised\nmachine learning helps exploit the predictive power of word embeddings.\n  Our comparative evaluation against five baselines for several multi-source\ndatasets with real-world data shows the high effectiveness of LEAPME. We also\nshow that our approach is even effective when training data from another domain\n(transfer learning) is used.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 12:42:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ayala", "Daniel", ""], ["Hern\u00e1ndez", "Inma", ""], ["Ruiz", "David", ""], ["Rahm", "Erhard", ""]]}, {"id": "2010.02628", "submitter": "Tatiana Makhalova", "authors": "Tatiana Makhalova, Aleksey Buzmakov, Sergei O. Kuznetsov and Amedeo\n  Napoli", "title": "Discovery data topology with the closure structure. Theoretical and\n  practical aspects", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are revisiting pattern mining and especially itemset\nmining, which allows one to analyze binary datasets in searching for\ninteresting and meaningful association rules and respective itemsets in an\nunsupervised way. While a summarization of a dataset based on a set of patterns\ndoes not provide a general and satisfying view over a dataset, we introduce a\nconcise representation -- the closure structure -- based on closed itemsets and\ntheir minimum generators, for capturing the intrinsic content of a dataset. The\nclosure structure allows one to understand the topology of the dataset in the\nwhole and the inherent complexity of the data. We propose a formalization of\nthe closure structure in terms of Formal Concept Analysis, which is well\nadapted to study this data topology. We present and demonstrate theoretical\nresults, and as well, practical results using the GDPM algorithm. GDPM is\nrather unique in its functionality as it returns a characterization of the\ntopology of a dataset in terms of complexity levels, highlighting the diversity\nand the distribution of the itemsets. Finally, a series of experiments shows\nhow GDPM can be practically used and what can be expected from the output.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:21:56 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 08:32:55 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 08:30:16 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Makhalova", "Tatiana", ""], ["Buzmakov", "Aleksey", ""], ["Kuznetsov", "Sergei O.", ""], ["Napoli", "Amedeo", ""]]}, {"id": "2010.02982", "submitter": "Alexandre Vigny", "authors": "Alexandre Vigny", "title": "Dynamic Query Evaluation Over Structures with Low Degree", "comments": "21 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the evaluation of first-order queries over classes of databases\nthat have bounded degree and low degree. More precisely, given a query and a\ndatabase, we want to efficiently test whether there is a solution, count how\nmany solutions there are, or be able to enumerate the set of all solutions.\n  Bounded and low degree are rather natural notions and both yield efficient\nalgorithms. For example, Berkholz, Keppeler, and Schweikardt showed in 2017\nthat over databases of bounded degree, not only any first order query can\nefficiently be tested, counted and enumerated, but the data structure used can\nbe updated when the database itself is updated.\n  This paper extends existing results in two directions. First, we show that\nover classes of databases with low degree, there is a data structure that\nenables us to test, count and enumerate the solutions of first order queries.\nThis data structure can also be efficiently recomputed when the database is\nupdated. Secondly, for classes of databases with bounded degree we show that,\nwithout increasing the preprocessing time, we can compute a data structure that\ndoes not depend on the query but only on its quantifier rank. We can therefore\nperform a single preprocessing that can later be used for many queries.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:16:58 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Vigny", "Alexandre", ""]]}, {"id": "2010.02987", "submitter": "Olga Poppe", "authors": "Olga Poppe, Chuan Lei, Elke A. Rundensteiner, David Maier", "title": "Event Trend Aggregation Under Rich Event Matching Semantics", "comments": "Technical report for the paper in SIGMOD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming applications from health care analytics to algorithmic trading\ndeploy Kleene queries to detect and aggregate event trends. Rich event matching\nsemantics determine how to compose events into trends. The expressive power of\nstate-of-the-art systems remains limited in that they do not support the rich\nvariety of these semantics. Worse yet, they suffer from long delays and high\nmemory costs because they opt to maintain aggregates at a fine granularity. To\novercome these limitations, our Coarse-Grained Event Trend Aggregation (Cogra)\napproach supports this rich diversity of event matching semantics within one\nsystem. Better yet, Cogra incrementally maintains aggregates at the coarsest\ngranularity possible for each of these semantics. In this way, Cogra minimizes\nthe number of aggregates -- reducing both time and space complexity. Our\nexperiments demonstrate that Cogra achieves up to four orders of magnitude\nspeed-up and up to eight orders of magnitude memory reduction compared to\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:26:09 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Poppe", "Olga", ""], ["Lei", "Chuan", ""], ["Rundensteiner", "Elke A.", ""], ["Maier", "David", ""]]}, {"id": "2010.02988", "submitter": "Olga Poppe", "authors": "Olga Poppe, Chuan Lei, Elke A. Rundensteiner, David Maier", "title": "GRETA: Graph-based Real-time Event Trend Aggregation", "comments": "Technical report for the paper in VLDB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming applications from algorithmic trading to traffic management deploy\nKleene patterns to detect and aggregate arbitrarily-long event sequences,\ncalled event trends. State-of-the-art systems process such queries in two\nsteps. Namely, they first construct all trends and then aggregate them. Due to\nthe exponential costs of trend construction, this two-step approach suffers\nfrom both a long delays and high memory costs. To overcome these limitations,\nwe propose the Graph-based Real-time Event Trend Aggregation (Greta) approach\nthat dynamically computes event trend aggregation without first constructing\nthese trends. We define the Greta graph to compactly encode all trends. Our\nGreta runtime incrementally maintains the graph, while dynamically propagating\naggregates along its edges. Based on the graph, the final aggregate is\nincrementally updated and instantaneously returned at the end of each query\nwindow. Our Greta runtime represents a win-win solution, reducing both the time\ncomplexity from exponential to quadratic and the space complexity from\nexponential to linear in the number of events. Our experiments demonstrate that\nGreta achieves up to four orders of magnitude speed-up and up to 50--fold\nmemory reduction compared to the state-of-the-art two-step approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:26:42 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Poppe", "Olga", ""], ["Lei", "Chuan", ""], ["Rundensteiner", "Elke A.", ""], ["Maier", "David", ""]]}, {"id": "2010.02989", "submitter": "Olga Poppe", "authors": "Olga Poppe, Allison Rozet, Chuan Lei, Elke A. Rundensteiner, David\n  Maier", "title": "Sharon: Shared Online Event Sequence Aggregation", "comments": "Technical report for the paper in ICDE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming systems evaluate massive workloads of event sequence aggregation\nqueries. State-of-the-art approaches suffer from long delays caused by not\nsharing intermediate results of similar queries and by constructing event\nsequences prior to their aggregation. To overcome these limitations, our Shared\nOnline Event Sequence Aggregation (Sharon) approach shares intermediate\naggregates among multiple queries while avoiding the expensive construction of\nevent sequences. Our Sharon optimizer faces two challenges. One, a sharing\ndecision is not always beneficial. Two, a sharing decision may exclude other\nsharing opportunities. To guide our Sharon optimizer, we compactly encode\nsharing candidates, their benefits, and conflicts among candidates into the\nSharon graph. Based on the graph, we map our problem of finding an optimal\nsharing plan to the Maximum Weight Independent Set (MWIS) problem. We then use\nthe guaranteed weight of a greedy algorithm for the MWIS problem to prune the\nsearch of our sharing plan finder without sacrificing its optimality. The\nSharon optimizer is shown to produce sharing plans that achieve up to an\n18-fold speed-up compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:27:26 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Poppe", "Olga", ""], ["Rozet", "Allison", ""], ["Lei", "Chuan", ""], ["Rundensteiner", "Elke A.", ""], ["Maier", "David", ""]]}, {"id": "2010.03090", "submitter": "Daniel Lemire", "authors": "John Keiser, Daniel Lemire", "title": "Validating UTF-8 In Less Than One Instruction Per Byte", "comments": null, "journal-ref": "Software: Practice and Experience 51 (5), 2021", "doi": "10.1002/spe.2920", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The majority of text is stored in UTF-8, which must be validated on\ningestion. We present the lookup algorithm, which outperforms UTF-8 validation\nroutines used in many libraries and languages by more than 10 times using\ncommonly available SIMD instructions. To ensure reproducibility, our work is\nfreely available as open source software.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 23:40:03 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 14:32:37 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 13:51:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Keiser", "John", ""], ["Lemire", "Daniel", ""]]}, {"id": "2010.03527", "submitter": "Julien Romero Dr.", "authors": "Julien Romero, Nicoleta Preda, Fabian Suchanek", "title": "Query Rewriting On Path Views Without Integrity Constraints", "comments": "This is the full version of the Datamod'2020 article, which\n  integrates all reviewer feedback, with the same text as the publisher version\n  except minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A view with a binding pattern is a parameterised query on a database. Such\nviews are used, e.g., to model Web services. To answer a query on such views,\none has to orchestrate the views together in execution plans. The goal is\nusually to find equivalent rewritings, which deliver precisely the same results\nas the query on all databases. However, such rewritings are usually possible\nonly in the presence of integrity constraints - and not all databases have such\nconstraints. In this paper, we describe a class of plans that give practical\nguarantees about their result even if there are no integrity constraints. We\nprovide a characterisation of such plans and a complete and correct algorithm\nto enumerate them. Finally, we show that our method can find plans on\nreal-world Web Services.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:12:43 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Romero", "Julien", ""], ["Preda", "Nicoleta", ""], ["Suchanek", "Fabian", ""]]}, {"id": "2010.03600", "submitter": "Hung Nguyen", "authors": "Hung T. Nguyen, Pierre J. Liang, Leman Akoglu", "title": "Anomaly Detection in Large Labeled Multi-Graph Databases", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within a large database G containing graphs with labeled nodes and directed,\nmulti-edges; how can we detect the anomalous graphs? Most existing work are\ndesigned for plain (unlabeled) and/or simple (unweighted) graphs. We introduce\nCODETECT, the first approach that addresses the anomaly detection task for\ngraph databases with such complex nature. To this end, it identifies a small\nrepresentative set S of structural patterns (i.e., node-labeled network motifs)\nthat losslessly compress database G as concisely as possible. Graphs that do\nnot compress well are flagged as anomalous. CODETECT exhibits two novel\nbuilding blocks: (i) a motif-based lossless graph encoding scheme, and (ii)\nfast memory-efficient search algorithms for S. We show the effectiveness of\nCODETECT on transaction graph databases from three different corporations,\nwhere existing baselines adjusted for the task fall behind significantly,\nacross different types of anomalies and performance metrics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 18:41:33 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Nguyen", "Hung T.", ""], ["Liang", "Pierre J.", ""], ["Akoglu", "Leman", ""]]}, {"id": "2010.03653", "submitter": "Nguyen Ho Ms.", "authors": "Van Long Ho, Nguyen Ho, Torben Bach Pedersen", "title": "Efficient Temporal Pattern Mining in Big Time Series Using Mutual\n  Information -- Full Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large time series are increasingly available from an ever wider range of\nIoT-enabled sensors deployed in different environments. Significant insights\ncan be obtained through mining temporal patterns from these time series. Unlike\ntraditional pattern mining, temporal pattern mining (TPM) adds additional\ntemporal aspect, e.g., the time intervals of the events, into extracted\npatterns, making them more expressive. However, adding this extra temporal\ninformation to the patterns also adds an additional exponential factor to the\nexponential growth of the search space, increases significantly the mining\ncomplexity. This raises an imperative need to have a more efficient and\nscalable method for temporal pattern mining. Existing TPM methods either cannot\nscale to large datasets, or work only on pre-processed temporal events rather\nthan on time series. This paper presents our comprehensive Frequent Temporal\nPattern Mining from Time Series (FTPMfTS) with the following contributions: (1)\nThe end-to-end FTPMfTS process that directly takes time series as input and\nproduces frequent temporal patterns as output. (2) The efficient Hierarchical\nTemporal Pattern Graph Mining (HTPGM) algorithm that uses efficient data\nstructures to enable fast computations of support and confidence, and employs\neffective pruning techniques to achieve significantly faster mining. (3) An\napproximate version of HTPGM which relies on mutual information to prune\nunpromising time series, and thus significantly reduce the search space. (4) An\nextensive experimental evaluation on synthetic and real-world datasets shows\nthat HTPGM outperforms the baselines in runtime and memory consumption, and can\nscale to big datasets. The approximate HTPGM achieves up to 2 orders of\nmagnitude speedup compared to the baselines while having high accuracy compared\nto the exact HTPGM.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 21:03:35 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 17:06:49 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 17:44:38 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 09:50:12 GMT"}, {"version": "v5", "created": "Thu, 22 Oct 2020 11:24:19 GMT"}, {"version": "v6", "created": "Sun, 2 May 2021 20:04:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ho", "Van Long", ""], ["Ho", "Nguyen", ""], ["Pedersen", "Torben Bach", ""]]}, {"id": "2010.03910", "submitter": "Huan Li", "authors": "Tiantian Liu, Huan Li, Hua Lu, Muhammad Aamir Cheema, and Lidan Shou", "title": "An Experimental Analysis of Indoor Spatial Queries: Modeling, Indexing,\n  and Processing", "comments": "An Experiment and Analysis Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Indoor location-based services (LBS), such as POI search and routing, are\noften built on top of typical indoor spatial queries. To support such queries\nand indoor LBS, multiple techniques including model/indexes and search\nalgorithms have been proposed. In this work, we conduct an extensive\nexperimental study on existing proposals for indoor spatial queries. We survey\nfive model/indexes, compare their algorithmic characteristics, and analyze\ntheir space and time complexities. We also design an in-depth benchmark with\nreal and synthetic datasets, evaluation tasks and performance metrics. Enabled\nby the benchmark, we obtain and report the performance results of all\nmodel/indexes under investigation. By analyzing the results, we summarize the\npros and cons of all techniques and suggest the best choice for typical\nscenarios.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 11:48:35 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Liu", "Tiantian", ""], ["Li", "Huan", ""], ["Lu", "Hua", ""], ["Cheema", "Muhammad Aamir", ""], ["Shou", "Lidan", ""]]}, {"id": "2010.04705", "submitter": "Ralph Foorthuis", "authors": "Ralph Foorthuis", "title": "Algorithmic Frameworks for the Detection of High Density Anomalies", "comments": "10 pages, 9 figures, 6 tables. Accepted for presentation at IEEE SSCI\n  CIDM 2020 (Symposium on Computational Intelligence in Data Mining)", "journal-ref": null, "doi": "10.1109/SSCI47803.2020.9308417", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explores the concept of high-density anomalies. As opposed to the\ntraditional concept of anomalies as isolated occurrences, high-density\nanomalies are deviant cases positioned in the most normal regions of the data\nspace. Such anomalies are relevant for various practical use cases, such as\nmisbehavior detection and data quality analysis. Effective methods for\nidentifying them are particularly important when analyzing very large or noisy\nsets, for which traditional anomaly detection algorithms will return many false\npositives. In order to be able to identify high-density anomalies, this study\nintroduces several non-parametric algorithmic frameworks for unsupervised\ndetection. These frameworks are able to leverage existing underlying anomaly\ndetection algorithms and offer different solutions for the balancing problem\ninherent in this detection task. The frameworks are evaluated with both\nsynthetic and real-world datasets, and are compared with existing baseline\nalgorithms for detecting traditional anomalies. The Iterative Partial Push\n(IPP) framework proves to yield the best detection results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:48:02 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 07:49:39 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Foorthuis", "Ralph", ""]]}, {"id": "2010.04895", "submitter": "Yingxia Shao", "authors": "Xingyu Yao, Yingxia Shao, Bin Cui, Lei Chen", "title": "UniNet: Scalable Network Representation Learning with\n  Metropolis-Hastings Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network representation learning (NRL) technique has been successfully adopted\nin various data mining and machine learning applications. Random walk based NRL\nis one popular paradigm, which uses a set of random walks to capture the\nnetwork structural information, and then employs word2vec models to learn the\nlow-dimensional representations. However, until now there is lack of a\nframework, which unifies existing random walk based NRL models and supports to\nefficiently learn from large networks. The main obstacle comes from the diverse\nrandom walk models and the inefficient sampling method for the random walk\ngeneration. In this paper, we first introduce a new and efficient edge sampler\nbased on Metropolis-Hastings sampling technique, and theoretically show the\nconvergence property of the edge sampler to arbitrary discrete probability\ndistributions. Then we propose a random walk model abstraction, in which users\ncan easily define different transition probability by specifying dynamic edge\nweights and random walk states. The abstraction is efficiently supported by our\nedge sampler, since our sampler can draw samples from unnormalized probability\ndistribution in constant time complexity. Finally, with the new edge sampler\nand random walk model abstraction, we carefully implement a scalable NRL\nframework called UniNet. We conduct comprehensive experiments with five random\nwalk based NRL models over eleven real-world datasets, and the results clearly\ndemonstrate the efficiency of UniNet over billion-edge networks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 04:06:20 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 09:46:37 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yao", "Xingyu", ""], ["Shao", "Yingxia", ""], ["Cui", "Bin", ""], ["Chen", "Lei", ""]]}, {"id": "2010.05073", "submitter": "Vincent Jacob", "authors": "Vincent Jacob, Fei Song, Arnaud Stiegler, Bijan Rad, Yanlei Diao,\n  Nesime Tatbul", "title": "Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to high-quality data repositories and benchmarks have been\ninstrumental in advancing the state of the art in many experimental research\ndomains. While advanced analytics tasks over time series data have been gaining\nlots of attention, lack of such community resources severely limits scientific\nprogress. In this paper, we present Exathlon, the first comprehensive public\nbenchmark for explainable anomaly detection over high-dimensional time series\ndata. Exathlon has been systematically constructed based on real data traces\nfrom repeated executions of large-scale stream processing jobs on an Apache\nSpark cluster. Some of these executions were intentionally disturbed by\nintroducing instances of six different types of anomalous events (e.g.,\nmisbehaving inputs, resource contention, process failures). For each of the\nanomaly instances, ground truth labels for the root cause interval as well as\nthose for the extended effect interval are provided, supporting the development\nand evaluation of a wide range of anomaly detection (AD) and explanation\ndiscovery (ED) tasks. We demonstrate the practical utility of Exathlon's\ndataset, evaluation methodology, and end-to-end data science pipeline design\nthrough an experimental study with three state-of-the-art AD and ED techniques.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 19:31:22 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 19:08:20 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Jacob", "Vincent", ""], ["Song", "Fei", ""], ["Stiegler", "Arnaud", ""], ["Rad", "Bijan", ""], ["Diao", "Yanlei", ""], ["Tatbul", "Nesime", ""]]}, {"id": "2010.05529", "submitter": "Phanwadee Sinthong", "authors": "Phanwadee Sinthong, Michael J. Carey", "title": "PolyFrame: A Retargetable Query-based Approach to Scaling DataFrames\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, the field of data science has been growing rapidly as\nvarious businesses have adopted statistical and machine learning techniques to\nempower their decision making and applications. Scaling data analysis, possibly\nincluding the application of custom machine learning models, to large volumes\nof data requires the utilization of distributed frameworks. This can lead to\nserious technical challenges for data analysts and reduce their productivity.\nAFrame, a Python data analytics library, is implemented as a layer on top of\nApache AsterixDB, addressing these issues by incorporating the data scientists'\ndevelopment environment and transparently scaling out the evaluation of\nanalytical operations through a Big Data management system. While AFrame is\nable to leverage data management facilities (e.g., indexes and query\noptimization) and allows users to interact with a very large volume of data,\nthe initial version only generated SQL++ queries and only operated against\nApache AsterixDB. In this work, we describe a new design that retargets\nAFrame's incremental query formation to other query-based database systems as\nwell, making it more flexible for deployment against other data management\nsystems with composable query languages.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:37:24 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 07:08:38 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Sinthong", "Phanwadee", ""], ["Carey", "Michael J.", ""]]}, {"id": "2010.05807", "submitter": "Keita Takenouchi", "authors": "Keita Takenouchi, Takashi Ishio, Joji Okada, Yuji Sakata", "title": "PATSQL: Efficient Synthesis of SQL Queries from Example Tables with\n  Quick Inference of Projected Columns", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SQL is one of the most popular tools for data analysis and used by an\nincreasing number of users without having expertise in databases. In order to\nhelp such non-experts to write correct SQL queries, several studies have\nproposed programming-by-example approaches. In these approaches, the user can\nobtain a desired query just by giving input and output (I/O) tables as an\nexample. While existing methods support a variety of SQL features such as\naggregation and nested query, they suffer a significant increase in\ncomputational cost as the scale of I/O tables increases. In this paper, we\npropose an efficient algorithm that synthesizes SQL queries from I/O tables.\nSpecifically, it has strengths in both the execution time and the scale of\nsupported tables. We adopt a sketch-based synthesis algorithm and focus on the\nquick inference of the columns used in the projection operator. In particular,\nwe restrict the structures of sketches based on transformation rules in\nrelational algebra and propagate a novel form of constraint using the output\ntable in a top-down manner. We implemented this algorithm in our tool PATSQL\nand evaluated it on 118 queries from prior benchmarks and Kaggle's tutorials.\nAs a result, PATSQL solved 72% of the benchmarks and found 92% of the solutions\nwithin a second. Our tool is available at https://naist-se.github.io/patsql/ .\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 16:02:27 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 07:01:56 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Takenouchi", "Keita", ""], ["Ishio", "Takashi", ""], ["Okada", "Joji", ""], ["Sakata", "Yuji", ""]]}, {"id": "2010.06037", "submitter": "Mart\\'in Mu\\~noz", "authors": "Mart\\'in Mu\\~noz, Cristian Riveros", "title": "Constant-delay enumeration algorithms for document spanners over nested\n  documents", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the most relevant document schemas used online, such as XML and JSON,\nhave a nested format. In recent years, the task of extracting data from large\nnested documents has become especially relevant. We model queries of this kind\nas Visibly Pushdown Transducers (VPT), a structure that extends visibly\npushdown automata with outputs. Since processing a string through a VPT can\ngenerate a huge number of outputs, we are interested in the task of enumerating\nthem one after another as efficiently as possible. This paper describes an\nalgorithm that enumerates these elements with output-linear delay after\npreprocessing the string in a single pass. We show applications of this result\non recursive document spanners over nested documents and show how our algorithm\ncan be adapted to enumerate the outputs in this context.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:26:31 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Mu\u00f1oz", "Mart\u00edn", ""], ["Riveros", "Cristian", ""]]}, {"id": "2010.06336", "submitter": "Olivier Cur\\'e", "authors": "Xiangnan Ren and Neha Sengupta and Xuguang Ren and Junhu Wang and\n  Olivier Cur\\'e", "title": "Finding Minimum Connected Subgraphs with Ontology Exploration on Large\n  RDF Data", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the following problem: given a knowledge graph (KG)\nand a set of input vertices (representing concepts or entities) and edge\nlabels, we aim to find the smallest connected subgraphs containing all of the\ninputs. This problem plays a key role in KG-based search engines and natural\nlanguage question answering systems, and it is a natural extension of the\nSteiner tree problem, which is known to be NP-hard. We present RECON, a system\nfor finding approximate answers. RECON aims at achieving high accuracy with\ninstantaneous response (i.e., sub-second/millisecond delay) over KGs with\nhundreds of millions edges without resorting to expensive computational\nresources. Furthermore, when no answer exists due to disconnection between\nconcepts and entities, RECON refines the input to a semantically similar one\nbased on the ontology, and attempt to find answers with respect to the refined\ninput. We conduct a comprehensive experimental evaluation of RECON. In\nparticular we compare it with five existing approaches for finding approximate\nSteiner trees. Our experiments on four large real and synthetic KGs show that\nRECON significantly outperforms its competitors and incurs a much smaller\nmemory footprint.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 12:48:41 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 07:38:07 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Ren", "Xiangnan", ""], ["Sengupta", "Neha", ""], ["Ren", "Xuguang", ""], ["Wang", "Junhu", ""], ["Cur\u00e9", "Olivier", ""]]}, {"id": "2010.06641", "submitter": "Samriddhi Singla", "authors": "Samriddhi Singla and Ahmed Eldawy", "title": "Raptor Zonal Statistics: Fully Distributed Zonal Statistics of Big\n  Raster + Vector Data [Pre-Print]", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in remote sensing technology have resulted in petabytes\nof data in raster format. This data is often processed in combination with high\nresolution vector data that represents, for example, city boundaries. One of\nthe common operations that combine big raster and vector data is the zonal\nstatistics which computes some statistics for each polygon in the vector\ndataset. This paper models the zonal statistics problem as a join problem and\nproposes a novel distributed system that can scale to petabytes of raster and\nvector data. The proposed method does not require any preprocessing or indexing\nwhich makes it perfect for ad-hoc queries that scientists usually want to run.\nWe devise a theoretical cost model that proves the efficiency of our algorithm\nover the baseline method. Furthermore, we run an extensive experimental\nevaluation on large scale satellite data with up-to a trillion pixels, and big\nvector data with up-to hundreds of millions of edges, and we show that our\nmethod can perfectly scale to big data with up-to two orders of magnitude\nperformance gain over Rasdaman and Google Earth Engine.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 19:15:09 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Singla", "Samriddhi", ""], ["Eldawy", "Ahmed", ""]]}, {"id": "2010.06654", "submitter": "Sheng Wang", "authors": "Sheng Wang, Yuan Sun, Zhifeng Bao", "title": "On the Efficiency of K-Means Clustering: Evaluation, Optimization, and\n  Algorithm Selection", "comments": "accepted to VLDB 2021; this is a technical report with five-page\n  appendix", "journal-ref": "PVLDB, 14(2): 163 - 175, 2021", "doi": "10.14778/3425879.3425887", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a thorough evaluation of the existing methods that\naccelerate Lloyd's algorithm for fast k-means clustering. To do so, we analyze\nthe pruning mechanisms of existing methods, and summarize their common pipeline\ninto a unified evaluation framework UniK. UniK embraces a class of well-known\nmethods and enables a fine-grained performance breakdown. Within UniK, we\nthoroughly evaluate the pros and cons of existing methods using multiple\nperformance metrics on a number of datasets. Furthermore, we derive an\noptimized algorithm over UniK, which effectively hybridizes multiple existing\nmethods for more aggressive pruning. To take this further, we investigate\nwhether the most efficient method for a given clustering task can be\nautomatically selected by machine learning, to benefit practitioners and\nresearchers.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 19:45:30 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 02:15:52 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wang", "Sheng", ""], ["Sun", "Yuan", ""], ["Bao", "Zhifeng", ""]]}, {"id": "2010.06760", "submitter": "Yu Xia", "authors": "Yu Xia, Xiangyao Yu, Andrew Pavlo, Srinivas Devadas", "title": "Taurus: Lightweight Parallel Logging for In-Memory Database Management\n  Systems (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing single-stream logging schemes are unsuitable for in-memory database\nmanagement systems (DBMSs) as the single log is often a performance bottleneck.\nTo overcome this problem, we present Taurus, an efficient parallel logging\nscheme that uses multiple log streams, and is compatible with both data and\ncommand logging. Taurus tracks and encodes transaction dependencies using a\nvector of log sequence numbers (LSNs). These vectors ensure that the\ndependencies are fully captured in logging and correctly enforced in recovery.\nOur experimental evaluation with an in-memory DBMS shows that Taurus's parallel\nlogging achieves up to 9.9x and 2.9x speedups over single-streamed data logging\nand command logging, respectively. It also enables the DBMS to recover up to\n22.9x and 75.6x faster than these baselines for data and command logging,\nrespectively. We also compare Taurus with two state-of-the-art parallel logging\nschemes and show that the DBMS achieves up to 2.8x better performance on NVMe\ndrives and 9.2x on HDDs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 01:20:54 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Xia", "Yu", ""], ["Yu", "Xiangyao", ""], ["Pavlo", "Andrew", ""], ["Devadas", "Srinivas", ""]]}, {"id": "2010.06973", "submitter": "James Thorne", "authors": "James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri,\n  Sebastian Riedel, Alon Halevy", "title": "Neural Databases", "comments": "Submitted to PVLDB vol 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural networks have shown impressive performance gains on\nlong-standing AI problems, and in particular, answering queries from natural\nlanguage text. These advances raise the question of whether they can be\nextended to a point where we can relax the fundamental assumption of database\nmanagement, namely, that our data is represented as fields of a pre-defined\nschema.\n  This paper presents a first step in answering that question. We describe\nNeuralDB, a database system with no pre-defined schema, in which updates and\nqueries are given in natural language. We develop query processing techniques\nthat build on the primitives offered by the state of the art Natural Language\nProcessing methods.\n  We begin by demonstrating that at the core, recent NLP transformers, powered\nby pre-trained language models, can answer select-project-join queries if they\nare given the exact set of relevant facts. However, they cannot scale to\nnon-trivial databases and cannot perform aggregation queries. Based on these\nfindings, we describe a NeuralDB architecture that runs multiple Neural SPJ\noperators in parallel, each with a set of database sentences that can produce\none of the answers to the query. The result of these operators is fed to an\naggregation operator if needed. We describe an algorithm that learns how to\ncreate the appropriate sets of facts to be fed into each of the Neural SPJ\noperators. Importantly, this algorithm can be trained by the Neural SPJ\noperator itself. We experimentally validate the accuracy of NeuralDB and its\ncomponents, showing that we can answer queries over thousands of sentences with\nvery high accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 11:31:53 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Thorne", "James", ""], ["Yazdani", "Majid", ""], ["Saeidi", "Marzieh", ""], ["Silvestri", "Fabrizio", ""], ["Riedel", "Sebastian", ""], ["Halevy", "Alon", ""]]}, {"id": "2010.07011", "submitter": "Jens Dittrich", "authors": "Jens Dittrich, Marcel Maltry", "title": "Database (Lecture) Streams on the Cloud: An Experience Report on\n  Teaching an Undergrad Database Lecture during a Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This is an experience report on teaching the undergrad lecture Big Data\nEngineering at Saarland University in summer term 2020 online. We describe our\nteaching philosophy, the tools used, what worked and what did not work. As we\nreceived extremely positive feedback from the students, in the future, we will\ncontinue to use the same teaching model for other lectures.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 11:08:06 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Dittrich", "Jens", ""], ["Maltry", "Marcel", ""]]}, {"id": "2010.07023", "submitter": "David Leslie", "authors": "David Leslie", "title": "Understanding bias in facial recognition technologies", "comments": "49 pages", "journal-ref": null, "doi": "10.5281/zenodo.4050457", "report-no": null, "categories": "cs.CY cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past couple of years, the growing debate around automated facial\nrecognition has reached a boiling point. As developers have continued to\nswiftly expand the scope of these kinds of technologies into an almost\nunbounded range of applications, an increasingly strident chorus of critical\nvoices has sounded concerns about the injurious effects of the proliferation of\nsuch systems. Opponents argue that the irresponsible design and use of facial\ndetection and recognition technologies (FDRTs) threatens to violate civil\nliberties, infringe on basic human rights and further entrench structural\nracism and systemic marginalisation. They also caution that the gradual creep\nof face surveillance infrastructures into every domain of lived experience may\neventually eradicate the modern democratic forms of life that have long\nprovided cherished means to individual flourishing, social solidarity and human\nself-creation. Defenders, by contrast, emphasise the gains in public safety,\nsecurity and efficiency that digitally streamlined capacities for facial\nidentification, identity verification and trait characterisation may bring. In\nthis explainer, I focus on one central aspect of this debate: the role that\ndynamics of bias and discrimination play in the development and deployment of\nFDRTs. I examine how historical patterns of discrimination have made inroads\ninto the design and implementation of FDRTs from their very earliest moments.\nAnd, I explain the ways in which the use of biased FDRTs can lead\ndistributional and recognitional injustices. The explainer concludes with an\nexploration of broader ethical questions around the potential proliferation of\npervasive face-based surveillance infrastructures and makes some\nrecommendations for cultivating more responsible approaches to the development\nand governance of these technologies.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:45:46 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Leslie", "David", ""]]}, {"id": "2010.07213", "submitter": "Manish Kesarwani", "authors": "Shazia Afzal, Rajmohan C, Manish Kesarwani, Sameep Mehta, Hima Patel", "title": "Data Readiness Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data exploration and quality analysis is an important yet tedious process in\nthe AI pipeline. Current practices of data cleaning and data readiness\nassessment for machine learning tasks are mostly conducted in an arbitrary\nmanner which limits their reuse and results in loss of productivity. We\nintroduce the concept of a Data Readiness Report as an accompanying\ndocumentation to a dataset that allows data consumers to get detailed insights\ninto the quality of input data. Data characteristics and challenges on various\nquality dimensions are identified and documented keeping in mind the principles\nof transparency and explainability. The Data Readiness Report also serves as a\nrecord of all data assessment operations including applied transformations.\nThis provides a detailed lineage for the purpose of data governance and\nmanagement. In effect, the report captures and documents the actions taken by\nvarious personas in a data readiness and assessment workflow. Overtime this\nbecomes a repository of best practices and can potentially drive a\nrecommendation system for building automated data readiness workflows on the\nlines of AutoML [8]. We anticipate that together with the Datasheets [9],\nDataset Nutrition Label [11], FactSheets [1] and Model Cards [15], the Data\nReadiness Report makes significant progress towards Data and AI lifecycle\ndocumentation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:26:29 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 13:30:05 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Afzal", "Shazia", ""], ["C", "Rajmohan", ""], ["Kesarwani", "Manish", ""], ["Mehta", "Sameep", ""], ["Patel", "Hima", ""]]}, {"id": "2010.07386", "submitter": "Christos Koutras", "authors": "Christos Koutras, George Siachamis, Andra Ionescu, Kyriakos Psarakis,\n  Jerry Brons, Marios Fragkoulis, Christoph Lofi, Angela Bonifati, Asterios\n  Katsifodimos", "title": "Valentine: Evaluating Matching Techniques for Dataset Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data scientists today search large data lakes to discover and integrate\ndatasets. In order to bring together disparate data sources, dataset discovery\nmethods rely on some form of schema matching: the process of establishing\ncorrespondences between datasets. Traditionally, schema matching has been used\nto find matching pairs of columns between a source and a target schema.\nHowever, the use of schema matching in dataset discovery methods differs from\nits original use. Nowadays schema matching serves as a building block for\nindicating and ranking inter-dataset relationships. Surprisingly, although a\ndiscovery method's success relies highly on the quality of the underlying\nmatching algorithms, the latest discovery methods employ existing schema\nmatching algorithms in an ad-hoc fashion due to the lack of openly-available\ndatasets with ground truth, reference method implementations, and evaluation\nmetrics. In this paper, we aim to rectify the problem of evaluating the\neffectiveness and efficiency of schema matching methods for the specific needs\nof dataset discovery. To this end, we propose Valentine, an extensible\nopen-source experiment suite to execute and organize large-scale automated\nmatching experiments on tabular data. Valentine includes implementations of\nseminal schema matching methods that we either implemented from scratch (due to\nabsence of open source code) or imported from open repositories. The\ncontributions of Valentine are: i) the definition of four schema matching\nscenarios as encountered in dataset discovery methods, ii) a principled dataset\nfabrication process tailored to the scope of dataset discovery methods and iii)\nthe most comprehensive evaluation of schema matching techniques to date,\noffering insight on the strengths and weaknesses of existing techniques, that\ncan serve as a guide for employing schema matching in future dataset discovery\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 20:12:23 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 19:19:28 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Koutras", "Christos", ""], ["Siachamis", "George", ""], ["Ionescu", "Andra", ""], ["Psarakis", "Kyriakos", ""], ["Brons", "Jerry", ""], ["Fragkoulis", "Marios", ""], ["Lofi", "Christoph", ""], ["Bonifati", "Angela", ""], ["Katsifodimos", "Asterios", ""]]}, {"id": "2010.07586", "submitter": "Zijie Wang", "authors": "Zijie Wang, Lixi Zhou, Amitabh Das, Valay Dave, Zhanpeng Jin, Jia Zou", "title": "Survive the Schema Changes: Integration of Unmanaged Data Using Deep\n  Learning", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is the king in the age of AI. However data integration is often a\nlaborious task that is hard to automate. Schema change is one significant\nobstacle to the automation of the end-to-end data integration process. Although\nthere exist mechanisms such as query discovery and schema modification language\nto handle the problem, these approaches can only work with the assumption that\nthe schema is maintained by a database. However, we observe diversified schema\nchanges in heterogeneous data and open data, most of which has no schema\ndefined. In this work, we propose to use deep learning to automatically deal\nwith schema changes through a super cell representation and automatic injection\nof perturbations to the training data to make the model robust to schema\nchanges. Our experimental results demonstrate that our proposed approach is\neffective for two real-world data integration scenarios: coronavirus data\nintegration, and machine log integration.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 08:10:37 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wang", "Zijie", ""], ["Zhou", "Lixi", ""], ["Das", "Amitabh", ""], ["Dave", "Valay", ""], ["Jin", "Zhanpeng", ""], ["Zou", "Jia", ""]]}, {"id": "2010.08238", "submitter": "Takao Murakami", "authors": "Takao Murakami and Kenta Takahashi", "title": "Toward Evaluating Re-identification Risks in the Local Privacy Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LDP (Local Differential Privacy) has recently attracted much attention as a\nmetric of data privacy that prevents the inference of personal data from\nobfuscated data in the local model. However, there are scenarios in which the\nadversary wants to perform re-identification attacks to link the obfuscated\ndata to users in this model. LDP can cause excessive obfuscation and destroy\nthe utility in these scenarios because it is not designed to directly prevent\nre-identification. In this paper, we propose a measure of reidentification\nrisks, which we call PIE (Personal Information Entropy). The PIE is designed so\nthat it directly prevents re-identification attacks in the local model. It\nlower-bounds the lowest possible re-identification error probability (i.e.,\nBayes error probability) of the adversary. We analyze the relation between LDP\nand the PIE, and analyze the PIE and utility in distribution estimation for two\nobfuscation mechanisms providing LDP. Through experiments, we show that when we\nconsider re-identification as a privacy risk, LDP can cause excessive\nobfuscation and destroy the utility. Then we show that the PIE can be used to\nguarantee low re-identification risks for the local obfuscation mechanisms\nwhile keeping high utility.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:38:39 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 23:58:03 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 11:46:42 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Murakami", "Takao", ""], ["Takahashi", "Kenta", ""]]}, {"id": "2010.08302", "submitter": "Xixi Lu", "authors": "Xixi Lu, Avigdor Gal, Hajo A. Reijers", "title": "Discovering Hierarchical Processes Using Flexible Activity Trees for\n  Event Abstraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processes, such as patient pathways, can be very complex, comprising of\nhundreds of activities and dozens of interleaved subprocesses. While existing\nprocess discovery algorithms have proven to construct models of high quality on\nclean logs of structured processes, it still remains a challenge when the\nalgorithms are being applied to logs of complex processes. The creation of a\nmulti-level, hierarchical representation of a process can help to manage this\ncomplexity. However, current approaches that pursue this idea suffer from a\nvariety of weaknesses. In particular, they do not deal well with interleaving\nsubprocesses. In this paper, we propose FlexHMiner, a three-step approach to\ndiscover processes with multi-level interleaved subprocesses. We implemented\nFlexHMiner in the open source Process Mining toolkit ProM. We used seven\nreal-life logs to compare the qualities of hierarchical models discovered using\ndomain knowledge, random clustering, and flat approaches. Our results indicate\nthat the hierarchical process models that the FlexHMiner generates compare\nfavorably to approaches that do not exploit hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 10:50:41 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Lu", "Xixi", ""], ["Gal", "Avigdor", ""], ["Reijers", "Hajo A.", ""]]}, {"id": "2010.08382", "submitter": "Luc Segoufin", "authors": "Arnaud Durand, Nicole Schweikardt, Luc Segoufin", "title": "Enumerating Answers to First-Order Queries over Databases of Low Degree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of relational databases has low degree if for all $\\delta>0$, all but\nfinitely many databases in the class have degree at most $n^{\\delta}$, where\n$n$ is the size of the database. Typical examples are databases of bounded\ndegree or of degree bounded by $\\log n$.\n  It is known that over a class of databases having low degree, first-order\nboolean queries can be checked in pseudo-linear time, i.e. for all $\\epsilon>0$\nin time bounded by $n^{1+\\epsilon}$. We generalize this result by considering\nquery evaluation.\n  We show that counting the number of answers to a query can be done in\npseudo-linear time and that after a pseudo-linear time preprocessing we can\ntest in constant time whether a given tuple is a solution to a query or\nenumerate the answers to a query ith constant delay.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 13:33:34 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Durand", "Arnaud", ""], ["Schweikardt", "Nicole", ""], ["Segoufin", "Luc", ""]]}, {"id": "2010.08622", "submitter": "Yunheng Han", "authors": "Yunheng Han and Hanan Samet", "title": "LiMITS: An Effective Approach for Trajectory Simplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectories represent the mobility of moving objects and thus is of great\nvalue in data mining applications. However, trajectory data is enormous in\nvolume, so it is expensive to store and process the raw data directly.\nTrajectories are also redundant so data compression techniques can be applied.\nIn this paper, we propose effective algorithms to simplify trajectories. We\nfirst extend existing algorithms by replacing the commonly used $L_2$ metric\nwith the $L_\\infty$ metric so that they can be generalized to high dimensional\nspace (e.g., 3-space in practice). Next, we propose a novel approach, namely\nL-infinity Multidimensional Interpolation Trajectory Simplification (LiMITS).\nLiMITS belongs to weak simplification and takes advantage of the $L_\\infty$\nmetric. It generates simplified trajectories by multidimensional interpolation.\nIt also allows a new format called compact representation to further improve\nthe compression ratio. Finally, We demonstrate the performance of LiMITS\nthrough experiments on real-world datasets, which show that it is more\neffective than other existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 20:46:51 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Han", "Yunheng", ""], ["Samet", "Hanan", ""]]}, {"id": "2010.08688", "submitter": "Takao Murakami", "authors": "Jacob Imola and Takao Murakami and Kamalika Chaudhuri", "title": "Locally Differentially Private Analysis of Graph Statistics", "comments": "This is a full version of the paper accepted at USENIX Security 2021;\n  The first and second authors made equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially private analysis of graphs is widely used for releasing\nstatistics from sensitive graphs while still preserving user privacy. Most\nexisting algorithms however are in a centralized privacy model, where a trusted\ndata curator holds the entire graph. As this model raises a number of privacy\nand security issues -- such as, the trustworthiness of the curator and the\npossibility of data breaches, it is desirable to consider algorithms in a more\ndecentralized local model where no server holds the entire graph.\n  In this work, we consider a local model, and present algorithms for counting\nsubgraphs -- a fundamental task for analyzing the connection patterns in a\ngraph -- with LDP (Local Differential Privacy). For triangle counts, we present\nalgorithms that use one and two rounds of interaction, and show that an\nadditional round can significantly improve the utility. For $k$-star counts, we\npresent an algorithm that achieves an order optimal estimation error in the\nnon-interactive local model. We provide new lower-bounds on the estimation\nerror for general graph statistics including triangle counts and $k$-star\ncounts. Finally, we perform extensive experiments on two real datasets, and\nshow that it is indeed possible to accurately estimate subgraph counts in the\nlocal differential privacy model.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 01:18:31 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 06:31:35 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Imola", "Jacob", ""], ["Murakami", "Takao", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "2010.08694", "submitter": "Xiao Hu", "authors": "Xiao Hu, Shouzhuo Sun, Shweta Patwa, Debmalya Panigrahi, Sudeepa Roy", "title": "Aggregated Deletion Propagation for Counting Conjunctive Query Answers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational complexity of minimizing the source\nside-effect in order to remove a given number of tuples from the output of a\nconjunctive query. This is a variant of the well-studied {\\em deletion\npropagation} problem, the difference being that we are interested in removing\nthe smallest subset of input tuples to remove a given number of output tuples}\nwhile deletion propagation focuses on removing a specific output tuple. We call\nthis the {\\em Aggregated Deletion Propagation} problem. We completely\ncharacterize the poly-time solvability of this problem for arbitrary\nconjunctive queries without self-joins. This includes a poly-time algorithm to\ndecide solvability, as well as an exact structural characterization of NP-hard\ninstances. We also provide a practical algorithm for this problem (a heuristic\nfor NP-hard instances) and evaluate its experimental performance on real and\nsynthetic datasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 01:50:27 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hu", "Xiao", ""], ["Sun", "Shouzhuo", ""], ["Patwa", "Shweta", ""], ["Panigrahi", "Debmalya", ""], ["Roy", "Sudeepa", ""]]}, {"id": "2010.08807", "submitter": "Yin Lin", "authors": "Yoko Nagafuchi, Yin Lin, Kaushal Mamgain, Abolfazl Asudeh, H. V.\n  Jagadish, You (Will) Wu, Cong Yu", "title": "MithraDetective: A System for Cherry-picked Trendlines Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a data set, misleading conclusions can be drawn from it by\ncherry-picking selected samples. One important class of conclusions is a trend\nderived from a data set of values over time. Our goal is to evaluate whether\nthe 'trends' described by the extracted samples are representative of the true\nsituation represented in the data. We demonstrate MithraDetective, a system to\ncompute a support score to indicate how cherry-picked a statement is; that is,\nwhether the reported trend is well-supported by the data. The system can also\nbe used to discover more supported alternatives. MithraDetective provides an\ninteractive visual interface for both tasks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 15:07:45 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Nagafuchi", "Yoko", "", "Will"], ["Lin", "Yin", "", "Will"], ["Mamgain", "Kaushal", "", "Will"], ["Asudeh", "Abolfazl", "", "Will"], ["Jagadish", "H. V.", "", "Will"], ["You", "", "", "Will"], ["Wu", "", ""], ["Yu", "Cong", ""]]}, {"id": "2010.08938", "submitter": "Xiaoshuang Chen", "authors": "Xiaoshuang Chen, Longbin Lai, Lu Qin, Xuemin Lin, and Boge Liu", "title": "A Framework to Quantify Approximate Simulation on Graph Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation and its variants (e.g., bisimulation and degree-preserving\nsimulation) are useful in a wide spectrum of applications. However, all\nsimulation variants are coarse \"yes-or-no\" indicators that simply confirm or\nrefute whether one node simulates another, which limits the scope and power of\ntheir utility. Therefore, it is meaningful to develop a fractional\n$\\chi$-simulation measure to quantify the degree to which one node simulates\nanother by the simulation variant $\\chi$. To this end, we first present several\nproperties necessary for a fractional $\\chi$-simulation measure. Then, we\npresent $FSim_\\chi$, a general fractional $\\chi$-simulation computation\nframework that can be configured to quantify the extent of all\n$\\chi$-simulations. Comprehensive experiments and real-world case studies show\nthe measure to be effective and the computation framework to be efficient.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 08:12:30 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Chen", "Xiaoshuang", ""], ["Lai", "Longbin", ""], ["Qin", "Lu", ""], ["Lin", "Xuemin", ""], ["Liu", "Boge", ""]]}, {"id": "2010.08995", "submitter": "Jinta Weng", "authors": "Jinta Weng, Ying Gao, Jing Qiu, Guozhu Ding, Huanqin Zheng", "title": "Construction and Application of Teaching System Based on Crowdsourcing\n  Knowledge Graph", "comments": "Number of references:15 Classification code:903.3 Information\n  Retrieval and Use Conference code: 235759", "journal-ref": "4th China Conference on Knowledge Graph and Semantic Computing,\n  CCKS 2019", "doi": "10.1007/978-981-15-1956-7_3", "report-no": null, "categories": "cs.DB cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the combination of crowdsourcing knowledge graph and teaching system,\nresearch methods to generate knowledge graph and its applications. Using two\ncrowdsourcing approaches, crowdsourcing task distribution and reverse captcha\ngeneration, to construct knowledge graph in the field of teaching system.\nGenerating a complete hierarchical knowledge graph of the teaching domain by\nnodes of school, student, teacher, course, knowledge point and exercise type.\nThe knowledge graph constructed in a crowdsourcing manner requires many users\nto participate collaboratively with fully consideration of teachers' guidance\nand users' mobilization issues. Based on the three subgraphs of knowledge\ngraph, prominent teacher, student learning situation and suitable learning\nroute could be visualized. Personalized exercises recommendation model is used\nto formulate the personalized exercise by algorithm based on the knowledge\ngraph. Collaborative creation model is developed to realize the crowdsourcing\nconstruction mechanism. Though unfamiliarity with the learning mode of\nknowledge graph and learners' less attention to the knowledge structure, system\nbased on Crowdsourcing Knowledge Graph can still get high acceptance around\nstudents and teachers\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 14:26:10 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Weng", "Jinta", ""], ["Gao", "Ying", ""], ["Qiu", "Jing", ""], ["Ding", "Guozhu", ""], ["Zheng", "Huanqin", ""]]}, {"id": "2010.09208", "submitter": "Malinga Perera", "authors": "R. Malinga Perera, Bastian Oetomo, Benjamin I. P. Rubinstein, Renata\n  Borovica-Gajic", "title": "DBA bandits: Self-driving index tuning under ad-hoc, analytical\n  workloads with safety guarantees", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating physical database design has remained a long-term interest in\ndatabase research due to substantial performance gains afforded by optimised\nstructures. Despite significant progress, a majority of today's commercial\nsolutions are highly manual, requiring offline invocation by database\nadministrators (DBAs) who are expected to identify and supply representative\ntraining workloads. Unfortunately, the latest advancements like query stores\nprovide only limited support for dynamic environments. This status quo is\nuntenable: identifying representative static workloads is no longer realistic;\nand physical design tools remain susceptible to the query optimiser's cost\nmisestimates (stemming from unrealistic assumptions such as attribute value\nindependence and uniformity of data distribution). We propose a self-driving\napproach to online index selection that eschews the DBA and query optimiser,\nand instead learns the benefits of viable structures through strategic\nexploration and direct performance observation. We view the problem as one of\nsequential decision making under uncertainty, specifically within the bandit\nlearning setting. Multi-armed bandits balance exploration and exploitation to\nprovably guarantee average performance that converges to a fixed policy that is\noptimal with perfect hindsight. Our comprehensive empirical results demonstrate\nup to 75% speed-up on shifting and ad-hoc workloads and 28% speed-up on static\nworkloads compared against a state-of-the-art commercial tuning tool.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 04:24:10 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 03:38:12 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Perera", "R. Malinga", ""], ["Oetomo", "Bastian", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Borovica-Gajic", "Renata", ""]]}, {"id": "2010.09393", "submitter": "Yusuke Kawamoto", "authors": "Natasha Fernandes, Yusuke Kawamoto, Takao Murakami", "title": "Locality Sensitive Hashing with Extended Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.IR cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extended differential privacy, a generalization of standard differential\nprivacy (DP) using a general metric, has been widely studied to provide\nrigorous privacy guarantees while keeping high utility. However, existing works\non extended DP are limited to few metrics such as the Euclidean metric.\nConsequently, they have only a small number of applications such as\nlocation-based services and document processing. In this paper, we propose a\ncouple of mechanisms providing extended DP with a different metric: angular\ndistance (or cosine distance). Our mechanisms are based on locality sensitive\nhashing (LSH), which can be applied to the angular distance and work well for\npersonal data in a high-dimensional space. We theoretically analyze the privacy\nproperties of our mechanisms, and prove extended DP for input data by taking\ninto account that LSH preserves the original metric only approximately. We\napply our mechanisms to friend matching based on high-dimensional personal data\nwith angular distance in the local model, and evaluate our mechanisms using two\nreal datasets. We show that LDP requires a very large privacy budget and that\nRAPPOR does not work in this application. Then we show that our mechanisms\nenable friend matching with high utility and rigorous privacy guarantees based\non extended DP.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 11:30:51 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:35:28 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 03:26:13 GMT"}, {"version": "v4", "created": "Wed, 12 May 2021 12:58:24 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Fernandes", "Natasha", ""], ["Kawamoto", "Yusuke", ""], ["Murakami", "Takao", ""]]}, {"id": "2010.09394", "submitter": "Junwoo Park", "authors": "Junwoo Park, Youngwoo Cho, Haneol Lee, Jaegul Choo, Edward Choi", "title": "Knowledge Graph-based Question Answering with Electronic Health Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) on Electronic Health Records (EHR), namely EHR QA,\ncan work as a crucial milestone towards developing an intelligent agent in\nhealthcare. EHR data are typically stored in a relational database, which can\nalso be converted to a Directed Acyclic Graph (DAG), allowing two approaches\nfor EHR QA: Table-based QA and Knowledge Graph-based QA. We hypothesize that\nthe graph-based approach is more suitable for EHR QA as graphs can represent\nrelations between entities and values more naturally compared to tables, which\nessentially require JOIN operations. To validate our hypothesis, we first\nconstruct EHR QA datasets based on MIMIC-III, where the same question-answer\npairs are represented in SQL (table-based) and SPARQL (graph-based),\nrespectively. We then test a state-of-the-art EHR QA model on both datasets\nwhere the model demonstrated superior QA performance on the SPARQL version.\nFinally, we open-source both MIMICSQL* and MIMIC-SPARQL* to encourage further\nEHR QA research in both direction\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 11:31:20 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Park", "Junwoo", ""], ["Cho", "Youngwoo", ""], ["Lee", "Haneol", ""], ["Choo", "Jaegul", ""], ["Choi", "Edward", ""]]}, {"id": "2010.09927", "submitter": "Arvind Srikantan", "authors": "Karthik Radhakrishnan, Arvind Srikantan, Xi Victoria Lin", "title": "ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries", "comments": "IntEx-SemPar Workshop at EMNLP 2020, 12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating natural language utterances to executable queries is a helpful\ntechnique in making the vast amount of data stored in relational databases\naccessible to a wider range of non-tech-savvy end users. Prior work in this\narea has largely focused on textual input that is linguistically correct and\nsemantically unambiguous. However, real-world user queries are often succinct,\ncolloquial, and noisy, resembling the input of a search engine. In this work,\nwe introduce data augmentation techniques and a sampling-based content-aware\nBERT model (ColloQL) to achieve robust text-to-SQL modeling over natural\nlanguage search (NLS) questions. Due to the lack of evaluation data, we curate\na new dataset of NLS questions and demonstrate the efficacy of our approach.\nColloQL's superior performance extends to well-formed text, achieving 84.9%\n(logical) and 90.7% (execution) accuracy on the WikiSQL dataset, making it, to\nthe best of our knowledge, the highest performing model that does not use\nexecution guided decoding.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:53:17 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Radhakrishnan", "Karthik", ""], ["Srikantan", "Arvind", ""], ["Lin", "Xi Victoria", ""]]}, {"id": "2010.10152", "submitter": "Xinjian Luo", "authors": "Xinjian Luo, Yuncheng Wu, Xiaokui Xiao, Beng Chin Ooi", "title": "Feature Inference Attack on Model Predictions in Vertical Federated\n  Learning", "comments": "Accepted at the IEEE 37th International Conference on Data\n  Engineering (ICDE 2021); 15 pages", "journal-ref": null, "doi": "10.1109/ICDE51399.2021.00023", "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is an emerging paradigm for facilitating multiple\norganizations' data collaboration without revealing their private data to each\nother. Recently, vertical FL, where the participating organizations hold the\nsame set of samples but with disjoint features and only one organization owns\nthe labels, has received increased attention. This paper presents several\nfeature inference attack methods to investigate the potential privacy leakages\nin the model prediction stage of vertical FL. The attack methods consider the\nmost stringent setting that the adversary controls only the trained vertical FL\nmodel and the model predictions, relying on no background information. We first\npropose two specific attacks on the logistic regression (LR) and decision tree\n(DT) models, according to individual prediction output. We further design a\ngeneral attack method based on multiple prediction outputs accumulated by the\nadversary to handle complex models, such as neural networks (NN) and random\nforest (RF) models. Experimental evaluations demonstrate the effectiveness of\nthe proposed attacks and highlight the need for designing private mechanisms to\nprotect the prediction outputs in vertical FL.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:38:49 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 12:52:08 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 11:44:35 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Luo", "Xinjian", ""], ["Wu", "Yuncheng", ""], ["Xiao", "Xiaokui", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2010.10198", "submitter": "Fatme Hachem", "authors": "Maria Luisa Damiani, Fatima Hachem, Christian Quadri, Matteo Rossini,\n  Sabrina Gaito", "title": "On Location Relevance and Diversity in Human Mobility Data", "comments": "The final version of this work will appear in ACM Transactions on\n  Spatial Algorithms and Systems 7, 2, Article 7 (October 25, 2020), 38 pages", "journal-ref": null, "doi": "10.1145/3423404", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theme of human mobility is transversal to multiple fields of study and\napplications, from ad-hoc networks to smart cities, from transportation\nplanning to recommendation systems on social networks. Despite the considerable\nefforts made by a few scientific communities and the relevant results obtained\nso far, there are still many issues only partially solved, that ask for general\nand quantitative methodologies to be addressed. A prominent aspect of\nscientific and practical relevance is how to characterize the mobility behavior\nof individuals. In this article, we look at the problem from a location-centric\nperspective: we investigate methods to extract, classify and quantify the\nsymbolic locations specified in telco trajectories, and use such measures to\nfeature user mobility. A major contribution is a novel trajectory summarization\ntechnique for the extraction of the locations of interest, i.e. attractive,\nfrom symbolic trajectories. The method is built on a density-based trajectory\nsegmentation technique tailored to telco data, which is proven to be robust\nagainst noise. To inspect the nature of those locations, we combine the two\ndimensions of location attractiveness and frequency into a novel location\ntaxonomy, which allows for a more accurate classification of the visited\nplaces. Another major contribution is the selection of suitable entropy-based\nmetrics for the characterization of single trajectories, based on the diversity\nof the locations of interest. All these components are integrated in a\nframework utilized for the analysis of 100,000+ telco trajectories. The\nexperiments show how the framework manages to dramatically reduce data\ncomplexity, provide high-quality information on the mobility behavior of people\nand finally succeed in grasping the nature of the locations visited by\nindividuals.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 11:23:04 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Damiani", "Maria Luisa", ""], ["Hachem", "Fatima", ""], ["Quadri", "Christian", ""], ["Rossini", "Matteo", ""], ["Gaito", "Sabrina", ""]]}, {"id": "2010.10246", "submitter": "Zhaojing Luo", "authors": "Zhaojing Luo, Sai Ho Yeung, Meihui Zhang, Kaiping Zheng, Lei Zhu, Gang\n  Chen, Feiyi Fan, Qian Lin, Kee Yuan Ngiam, Beng Chin Ooi", "title": "MLCask: Efficient Management of Component Evolution in Collaborative\n  Data Analytics Pipelines", "comments": "13 pages; added new baselines, i.e., MLflow and ModelDB, in Section\n  VII-C; added experience on the system deployment in Section VIII; added Table\n  I to clarify the correctness of the prioritized pipeline search in Section\n  VII-E", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-increasing adoption of machine learning for data analytics,\nmaintaining a machine learning pipeline is becoming more complex as both the\ndatasets and trained models evolve with time. In a collaborative environment,\nthe changes and updates due to pipeline evolution often cause cumbersome\ncoordination and maintenance work, raising the costs and making it hard to use.\nExisting solutions, unfortunately, do not address the version evolution\nproblem, especially in a collaborative environment where non-linear version\ncontrol semantics are necessary to isolate operations made by different user\nroles. The lack of version control semantics also incurs unnecessary storage\nconsumption and lowers efficiency due to data duplication and repeated data\npre-processing, which are avoidable. In this paper, we identify two main\nchallenges that arise during the deployment of machine learning pipelines, and\naddress them with the design of versioning for an end-to-end analytics system\nMLCask. The system supports multiple user roles with the ability to perform\nGit-like branching and merging operations in the context of the machine\nlearning pipelines. We define and accelerate the metric-driven merge operation\nby pruning the pipeline search tree using reusable history records and pipeline\ncompatibility information. Further, we design and implement the prioritized\npipeline search, which gives preference to the pipelines that probably yield\nbetter performance. The effectiveness of MLCask is evaluated through an\nextensive study over several real-world deployment cases. The performance\nevaluation shows that the proposed merge operation is up to 7.8x faster and\nsaves up to 11.9x storage space than the baseline method that does not utilize\nhistory records.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 13:34:48 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 14:00:35 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 08:01:19 GMT"}, {"version": "v4", "created": "Tue, 16 Mar 2021 12:54:40 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Luo", "Zhaojing", ""], ["Yeung", "Sai Ho", ""], ["Zhang", "Meihui", ""], ["Zheng", "Kaiping", ""], ["Zhu", "Lei", ""], ["Chen", "Gang", ""], ["Fan", "Feiyi", ""], ["Lin", "Qian", ""], ["Ngiam", "Kee Yuan", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2010.10343", "submitter": "David Kohan Marzag\\~ao", "authors": "David Kohan Marzag\\~ao, Trung Dong Huynh, Ayah Helal, Luc Moreau", "title": "Provenance Graph Kernel", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance is a record that describes how entities, activities, and agents\nhave influenced a piece of data. Such provenance information is commonly\nrepresented in graphs with relevant labels on both their nodes and edges. With\nthe growing adoption of provenance in a wide range of application domains,\nincreasingly, users are confronted with an abundance of graph data, which may\nprove challenging to analyse. Graph kernels, on the other hand, have been\nconsistently and successfully used to efficiently classify graphs. In this\npaper, we introduce a novel graph kernel called \\emph{provenance kernel}, which\nis inspired by and tailored for provenance data. It decomposes a provenance\ngraph into tree-patterns rooted at a given node and considers the labels of\nedges and nodes up to a certain distance from the root. We employ provenance\nkernels to classify provenance graphs from three application domains. Our\nevaluation shows that they perform well in terms of classification accuracy and\nyield competitive results when compared against standard graph kernel methods\nand the provenance network analytics method while taking significantly less\ntime.Moreover, we illustrate how the provenance types used in provenance\nkernels help improve the explainability of predictive models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:07:30 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Marzag\u00e3o", "David Kohan", ""], ["Huynh", "Trung Dong", ""], ["Helal", "Ayah", ""], ["Moreau", "Luc", ""]]}, {"id": "2010.10664", "submitter": "David Darais", "authors": "Phillip Nguyen, Alex Silence, David Darais, Joseph P. Near", "title": "DuetSGX: Differential Privacy with Secure Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy offers a formal privacy guarantee for individuals, but\nmany deployments of differentially private systems require a trusted third\nparty (the data curator). We propose DuetSGX, a system that uses secure\nhardware (Intel's SGX) to eliminate the need for a trusted data curator. Data\nowners submit encrypted data that can be decrypted only within a secure enclave\nrunning the DuetSGX system, ensuring that sensitive data is never available to\nthe data curator. Analysts submit queries written in the Duet language, which\nis specifically designed for verifying that programs satisfy differential\nprivacy; DuetSGX uses the Duet typechecker to verify that each query satisfies\ndifferential privacy before running it. DuetSGX therefore provides the benefits\nof local differential privacy and central differential privacy simultaneously:\nnoise is only added to final results, and there is no trusted third party. We\nhave implemented a proof-of-concept implementation of DuetSGX and we release it\nas open-source.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 23:08:03 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Nguyen", "Phillip", ""], ["Silence", "Alex", ""], ["Darais", "David", ""], ["Near", "Joseph P.", ""]]}, {"id": "2010.10884", "submitter": "Iztok Fister", "authors": "Iztok Fister, Iztok Fister Jr", "title": "uARMSolver: A framework for Association Rule Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel software framework for Association Rule Mining\nnamed uARMSolver. The framework is written fully in C++ and runs on all\nplatforms. It allows users to preprocess their data in a transaction database,\nto make discretization of data, to search for association rules and to guide a\npresentation/visualization of the best rules found using external tools. As\nopposed to the existing software packages or frameworks, this also supports\nnumerical and real-valued types of attributes besides the categorical ones.\nMining the association rules is defined as an optimization and solved using the\nnature-inspired algorithms that can be incorporated easily. Because the\nalgorithms normally discover a huge amount of association rules, the framework\nenables a modular inclusion of so-called visual guiders for extracting the\nknowledge hidden in data, and visualize these using external tools.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 10:36:31 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Fister", "Iztok", ""], ["Fister", "Iztok", "Jr"]]}, {"id": "2010.10900", "submitter": "Tommaso Soru", "authors": "Anand Panchbhai and Tommaso Soru and Edgard Marx", "title": "Exploring Sequence-to-Sequence Models for SPARQL Pattern Composition", "comments": "Proceedings of the First Indo-American Knowledge Graph and Semantic\n  Web Conference (KGSWC-India 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A booming amount of information is continuously added to the Internet as\nstructured and unstructured data, feeding knowledge bases such as DBpedia and\nWikidata with billions of statements describing millions of entities. The aim\nof Question Answering systems is to allow lay users to access such data using\nnatural language without needing to write formal queries. However, users often\nsubmit questions that are complex and require a certain level of abstraction\nand reasoning to decompose them into basic graph patterns. In this short paper,\nwe explore the use of architectures based on Neural Machine Translation called\nNeural SPARQL Machines to learn pattern compositions. We show that\nsequence-to-sequence models are a viable and promising option to transform long\nutterances into complex SPARQL queries.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:12:01 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Panchbhai", "Anand", ""], ["Soru", "Tommaso", ""], ["Marx", "Edgard", ""]]}, {"id": "2010.11075", "submitter": "Nils Barlaug", "authors": "Nils Barlaug, Jon Atle Gulla", "title": "Neural Networks for Entity Matching: A Survey", "comments": "Published in ACM Transactions on Knowledge Discovery from Data (TKDD)", "journal-ref": "ACM Transactions on Knowledge Discovery from Data, Volume 15,\n  Issue 3, April 2021", "doi": "10.1145/3442200", "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity matching is the problem of identifying which records refer to the same\nreal-world entity. It has been actively researched for decades, and a variety\nof different approaches have been developed. Even today, it remains a\nchallenging problem, and there is still generous room for improvement. In\nrecent years we have seen new methods based upon deep learning techniques for\nnatural language processing emerge.\n  In this survey, we present how neural networks have been used for entity\nmatching. Specifically, we identify which steps of the entity matching process\nexisting work have targeted using neural networks, and provide an overview of\nthe different techniques used at each step. We also discuss contributions from\ndeep learning in entity matching compared to traditional methods, and propose a\ntaxonomy of deep neural networks for entity matching.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:36:03 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 21:51:58 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Barlaug", "Nils", ""], ["Gulla", "Jon Atle", ""]]}, {"id": "2010.11465", "submitter": "Hongyu Ren", "authors": "Hongyu Ren, Jure Leskovec", "title": "Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental problems in Artificial Intelligence is to perform\ncomplex multi-hop logical reasoning over the facts captured by a knowledge\ngraph (KG). This problem is challenging, because KGs can be massive and\nincomplete. Recent approaches embed KG entities in a low dimensional space and\nthen use these embeddings to find the answer entities. However, it has been an\noutstanding challenge of how to handle arbitrary first-order logic (FOL)\nqueries as present methods are limited to only a subset of FOL operators. In\nparticular, the negation operator is not supported. An additional limitation of\npresent methods is also that they cannot naturally model uncertainty. Here, we\npresent BetaE, a probabilistic embedding framework for answering arbitrary FOL\nqueries over KGs. BetaE is the first method that can handle a complete set of\nfirst-order logical operations: conjunction ($\\wedge$), disjunction ($\\vee$),\nand negation ($\\neg$). A key insight of BetaE is to use probabilistic\ndistributions with bounded support, specifically the Beta distribution, and\nembed queries/entities as distributions, which as a consequence allows us to\nalso faithfully model uncertainty. Logical operations are performed in the\nembedding space by neural operators over the probabilistic embeddings. We\ndemonstrate the performance of BetaE on answering arbitrary FOL queries on\nthree large, incomplete KGs. While being more general, BetaE also increases\nrelative performance by up to 25.4% over the current state-of-the-art KG\nreasoning methods that can only handle conjunctive queries without negation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 06:11:39 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ren", "Hongyu", ""], ["Leskovec", "Jure", ""]]}, {"id": "2010.11497", "submitter": "Olivier Ruas", "authors": "George Giakkoupis (WIDE), Anne-Marie Kermarrec (EPFL), Olivier Ruas\n  (SPIRALS), Fran\\c{c}ois Ta\\\"iani (WIDE, IRISA)", "title": "Cluster-and-Conquer: When Randomness Meets Graph Locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-Nearest-Neighbors (KNN) graphs are central to many emblematic data mining\nand machine-learning applications. Some of the most efficient KNN graph\nalgorithms are incremental and local: they start from a random graph, which\nthey incrementally improve by traversing neighbors-of-neighbors links.\nParadoxically, this random start is also one of the key weaknesses of these\nalgorithms: nodes are initially connected to dissimilar neighbors, that lie far\naway according to the similarity metric. As a result, incremental algorithms\nmust first laboriously explore spurious potential neighbors before they can\nidentify similar nodes, and start converging. In this paper, we remove this\ndrawback with Cluster-and-Conquer (C 2 for short). Cluster-and-Conquer boosts\nthe starting configuration of greedy algorithms thanks to a novel lightweight\nclustering mechanism, dubbed FastRandomHash. FastRandomHash leverages\nrandom-ness and recursion to pre-cluster similar nodes at a very low cost. Our\nextensive evaluation on real datasets shows that Cluster-and-Conquer\nsignificantly outperforms existing approaches, including LSH, yielding\nspeed-ups of up to x4.42 while incurring only a negligible loss in terms of KNN\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 07:31:12 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Giakkoupis", "George", "", "WIDE"], ["Kermarrec", "Anne-Marie", "", "EPFL"], ["Ruas", "Olivier", "", "SPIRALS"], ["Ta\u00efani", "Fran\u00e7ois", "", "WIDE, IRISA"]]}, {"id": "2010.11538", "submitter": "Lei Zheng", "authors": "Lei Zheng, Ziming Shen, Hongzhi Wang", "title": "Efficient RDF Graph Storage based on Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph is an important cornerstone of artificial intelligence. The\nconstruction and release of large-scale knowledge graphs in various fields pose\nnew challenges to knowledge graph data management. Due to the maturity and\nstability, relational database is also suitable for RDF data storage. However,\nthe complex structure of RDF graph brings challenges to storage structure\ndesign for RDF graph in the relational database. To address the difficult\nproblem, this paper adopts reinforcement learning (RL) to optimize the storage\npartition method of RDF graph based on the relational database. We transform\nthe graph storage into a Markov decision process, and develop the reinforcement\nlearning algorithm for graph storage design. For effective RL-based storage\ndesign, we propose the data feature extraction method of RDF tables and the\nquery rewriting priority policy during model training. The extensive\nexperimental results demonstrate that our approach outperforms existing RDF\nstorage design methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:57:24 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zheng", "Lei", ""], ["Shen", "Ziming", ""], ["Wang", "Hongzhi", ""]]}, {"id": "2010.11721", "submitter": "Arvind Agarwal", "authors": "Vivek Iyer, Arvind Agarwal, Harshit Kumar", "title": "Multifaceted Context Representation using Dual Attention for Ontology\n  Alignment", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology Alignment is an important research problem that finds application in\nvarious fields such as data integration, data transfer, data preparation etc.\nState-of-the-art (SOTA) architectures in Ontology Alignment typically use naive\ndomain-dependent approaches with handcrafted rules and manually assigned\nvalues, making them unscalable and inefficient. Deep Learning approaches for\nontology alignment use domain-specific architectures that are not only\nin-extensible to other datasets and domains, but also typically perform worse\nthan rule-based approaches due to various limitations including over-fitting of\nmodels, sparsity of datasets etc. In this work, we propose VeeAlign, a Deep\nLearning based model that uses a dual-attention mechanism to compute the\ncontextualized representation of a concept in order to learn alignments. By\ndoing so, not only does our approach exploit both syntactic and semantic\nstructure of ontologies, it is also, by design, flexible and scalable to\ndifferent domains with minimal effort. We validate our approach on various\ndatasets from different domains and in multilingual settings, and show its\nsuperior performance over SOTA methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:28:38 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 11:31:42 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Iyer", "Vivek", ""], ["Agarwal", "Arvind", ""], ["Kumar", "Harshit", ""]]}, {"id": "2010.11827", "submitter": "Kunal Sawarkar", "authors": "Kunal Sawarkar, Meenkakshi Kodati", "title": "Automated Metadata Harmonization Using Entity Resolution & Contextual\n  Embedding", "comments": "Paper Accepted at Computing Conference, 2021 (Research Conference\n  formerly called Science and Information (SAI) Conference). This is a\n  replacement with change edit on conference status updated to \"Accepted\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ML Data Curation process typically consist of heterogeneous & federated\nsource systems with varied schema structures; requiring curation process to\nstandardize metadata from different schemas to an inter-operable schema. This\nmanual process of Metadata Harmonization & cataloging slows efficiency of\nML-Ops lifecycle. We demonstrate automation of this step with the help of\nentity resolution methods & also by using Cogntive Database's Db2Vec embedding\napproach to capture hidden inter-column & intra-column relationships which\ndetect similarity of metadata and then predict metadata columns from source\nschemas to any standardized schemas. Apart from matching schemas, we\ndemonstrate that it can also infer the correct ontological structure of the\ntarget data model.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 02:14:15 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 16:23:05 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Sawarkar", "Kunal", ""], ["Kodati", "Meenkakshi", ""]]}, {"id": "2010.11848", "submitter": "Carsten Lutz", "authors": "Cristina Feier, Carsten Lutz, Frank Wolter", "title": "From Conjunctive Queries to Instance Queries in Ontology-Mediated\n  Querying", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider ontology-mediated queries (OMQs) based on expressive description\nlogics of the ALC family and (unions) of conjunctive queries, studying the\nrewritability into OMQs based on instance queries (IQs). Our results include\nexact characterizations of when such a rewriting is possible and tight\ncomplexity bounds for deciding rewritability. We also give a tight complexity\nbound for the related problem of deciding whether a given MMSNP sentence is\nequivalent to a CSP.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:40:59 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Feier", "Cristina", ""], ["Lutz", "Carsten", ""], ["Wolter", "Frank", ""]]}, {"id": "2010.12243", "submitter": "G\\'abor Sz\\'arnyas", "authors": "M\\'arton Elekes, J\\'anos Benjamin Antal, G\\'abor Sz\\'arnyas", "title": "An analysis of the SIGMOD 2014 Programming Contest: Complex queries on\n  the LDBC social network graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report contains an analysis of the queries defined in the SIGMOD 2014\nProgramming Contest. We first describe the data set, then present the queries,\nproviding graphical illustrations for them and pointing out their caveats. Our\nintention is to document our lessons learnt and simplify the work of those who\nwill attempt to create a solution to this contest. We also demonstrate the\ninfluence of this contest by listing followup works which used these queries as\ninspiration to design better algorithms or to define interesting graph queries.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 09:04:23 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 20:26:23 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Elekes", "M\u00e1rton", ""], ["Antal", "J\u00e1nos Benjamin", ""], ["Sz\u00e1rnyas", "G\u00e1bor", ""]]}, {"id": "2010.12537", "submitter": "Haoyu Dong", "authors": "Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, Dongmei\n  Zhang", "title": "TUTA: Tree-based Transformers for Generally Structured Table\n  Pre-training", "comments": "KDD'21", "journal-ref": null, "doi": "10.1145/3447548.3467434", "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tables are widely used with various structures to organize and present data.\nRecent attempts on table understanding mainly focus on relational tables, yet\noverlook to other common table structures. In this paper, we propose TUTA, a\nunified pre-training architecture for understanding generally structured\ntables. Noticing that understanding a table requires spatial, hierarchical, and\nsemantic information, we enhance transformers with three novel structure-aware\nmechanisms. First, we devise a unified tree-based structure, called a\nbi-dimensional coordinate tree, to describe both the spatial and hierarchical\ninformation of generally structured tables. Upon this, we propose tree-based\nattention and position embedding to better capture the spatial and hierarchical\ninformation. Moreover, we devise three progressive pre-training objectives to\nenable representations at the token, cell, and table levels. We pre-train TUTA\non a wide range of unlabeled web and spreadsheet tables and fine-tune it on two\ncritical tasks in the field of table structure understanding: cell type\nclassification and table type classification. Experiments show that TUTA is\nhighly effective, achieving state-of-the-art on five widely-studied datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:22:31 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 06:56:14 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 13:20:11 GMT"}, {"version": "v4", "created": "Tue, 20 Jul 2021 01:18:05 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Zhiruo", ""], ["Dong", "Haoyu", ""], ["Jia", "Ran", ""], ["Li", "Jia", ""], ["Fu", "Zhiyi", ""], ["Han", "Shi", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2010.12548", "submitter": "Eleni Tzirita Zacharatou", "authors": "Eleni Tzirita Zacharatou, Andreas Kipf, Ibrahim Sabek, Varun Pandey,\n  Harish Doraiswamy, Volker Markl", "title": "The Case for Distance-Bounded Spatial Approximations", "comments": "11th Annual Conference on Innovative Data Systems Research (CIDR'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial approximations have been traditionally used in spatial databases to\naccelerate the processing of complex geometric operations. However,\napproximations are typically only used in a first filtering step to determine a\nset of candidate spatial objects that may fulfill the query condition. To\nprovide accurate results, the exact geometries of the candidate objects are\ntested against the query condition, which is typically an expensive operation.\nNevertheless, many emerging applications (e.g., visualization tools) require\ninteractive responses, while only needing approximate results. Besides,\nreal-world geospatial data is inherently imprecise, which makes exact data\nprocessing unnecessary. Given the uncertainty associated with spatial data and\nthe relaxed precision requirements of many applications, this vision paper\nadvocates for approximate spatial data processing techniques that omit exact\ngeometric tests and provide final answers solely on the basis of (fine-grained)\napproximations. Thanks to recent hardware advances, this vision can be realized\ntoday. Furthermore, our approximate techniques employ a distance-based error\nbound, i.e., a bound on the maximum spatial distance between false (or missing)\nand exact results which is crucial for meaningful analyses. This bound allows\nto control the precision of the approximation and trade accuracy for\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:16:46 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 16:53:41 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zacharatou", "Eleni Tzirita", ""], ["Kipf", "Andreas", ""], ["Sabek", "Ibrahim", ""], ["Pandey", "Varun", ""], ["Doraiswamy", "Harish", ""], ["Markl", "Volker", ""]]}, {"id": "2010.12597", "submitter": "Andreas Andreakis", "authors": "Andreas Andreakis, Ioannis Papapanagiotou", "title": "DBLog: A Watermark Based Change-Data-Capture Framework", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is a commonly observed pattern for applications to utilize multiple\nheterogeneous databases where each is used to serve a specific need such as\nstoring the canonical form of data or providing advanced search capabilities.\nFor applications it is hence desired to keep multiple databases in sync. We\nhave observed a series of distinct patterns that have tried to solve this\nproblem such as dual-writes and distributed transactions. However, these\napproaches have limitations with regard to feasibility, robustness, and\nmaintenance. An alternative approach that has recently emerged is to utilize\nChange-Data-Capture (CDC) in order to capture changed rows from a database's\ntransaction log and eventually deliver them downstream with low latency. In\norder to solve the data synchronization problem one also needs to replicate the\nfull state of a database and transaction logs typically do not contain the full\nhistory of changes. At the same time, there are use cases that require high\navailability of the transaction log events so that databases stay as closely\nin-sync as possible.\n  To address the above challenges, we developed a novel CDC framework for\ndatabases, namely DBLog. DBLog utilizes a watermark based approach that allows\nus to interleave transaction log events with rows that we directly select from\ntables to capture the full state. Our solution allows log events to continue\nprogress without stalling while processing selects. Selects can be triggered at\nany time on all tables, a specific table, or for specific primary keys of a\ntable. DBLog executes selects in chunks and tracks progress, allowing them to\npause and resume. The watermark approach does not use locks and has minimum\nimpact on the source. DBLog is currently used in production by tens of\nmicroservices at Netflix.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 18:06:26 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Andreakis", "Andreas", ""], ["Papapanagiotou", "Ioannis", ""]]}, {"id": "2010.12734", "submitter": "Wenshao Zhong", "authors": "Wenshao Zhong, Chen Chen, Xingbo Wu, Song Jiang", "title": "REMIX: Efficient Range Query for LSM-trees", "comments": "19th USENIX Conference on File and Storage Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSM-tree based key-value (KV) stores organize data in a multi-level structure\nfor high-speed writes. Range queries on traditional LSM-trees must seek and\nsort-merge data from multiple table files on the fly, which is expensive and\noften leads to mediocre read performance. To improve range query efficiency on\nLSM-trees, we introduce a space-efficient KV index data structure, named REMIX,\nthat records a globally sorted view of KV data spanning multiple table files. A\nrange query on multiple REMIX-indexed data files can quickly locate the target\nkey using a binary search, and retrieve subsequent keys in sorted order without\nkey comparisons. We build RemixDB, an LSM-tree based KV-store that adopts a\nwrite-efficient compaction strategy and employs REMIXes for fast point and\nrange queries. Experimental results show that REMIXes can substantially improve\nrange query performance in a write-optimized LSM-tree based KV-store.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 01:21:21 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 21:34:47 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zhong", "Wenshao", ""], ["Chen", "Chen", ""], ["Wu", "Xingbo", ""], ["Jiang", "Song", ""]]}, {"id": "2010.13149", "submitter": "Nir Regev", "authors": "Nir Regev, Lior Rokach, Asaf Shabtai", "title": "Approximating Aggregated SQL Queries With LSTM Networks", "comments": "12 pages, 5 figures, ICDE2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite continuous investments in data technologies, the latency of querying\ndata still poses a significant challenge. Modern analytic solutions require\nnear real-time responsiveness both to make them interactive and to support\nautomated processing. Current technologies (Hadoop, Spark, Dataflow) scan the\ndataset to execute queries. They focus on providing a scalable data storage to\nmaximize task execution speed. We argue that these solutions fail to offer an\nadequate level of interactivity since they depend on continual access to data.\nIn this paper we present a method for query approximation, also known as\napproximate query processing (AQP), that reduce the need to scan data during\ninference (query calculation), thus enabling a rapid query processing tool. We\nuse LSTM network to learn the relationship between queries and their results,\nand to provide a rapid inference layer for predicting query results. Our method\n(referred as ``Hunch``) produces a lightweight LSTM network which provides a\nhigh query throughput. We evaluated our method using twelve datasets and\ncompared to state-of-the-art AQP engines (VerdictDB, BlinkDB) from query\nlatency, model weight and accuracy perspectives. The results show that our\nmethod predicted queries' results with a normalized root mean squared error\n(NRMSE) ranging from approximately 1\\% to 4\\% which in the majority of our data\nsets was better then the compared benchmarks. Moreover, our method was able to\npredict up to 120,000 queries in a second (streamed together), and with a\nsingle query latency of no more than 2ms.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 16:17:58 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 18:46:52 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 11:32:09 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Regev", "Nir", ""], ["Rokach", "Lior", ""], ["Shabtai", "Asaf", ""]]}, {"id": "2010.13381", "submitter": "Fumiyuki Kato", "authors": "Fumiyuki Kato, Yang Cao, Masatoshi Yoshikawa", "title": "Secure and Efficient Trajectory-Based Contact Tracing using Trusted\n  Hardware", "comments": "Accepted by 7th International Workshop on Privacy and Security of Big\n  Data (PSBD 2020) in conjunction with 2020 IEEE International Conference on\n  Big Data (IEEE BigData 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has prompted technological measures to control the\nspread of the disease. Private contact tracing (PCT) is one of the promising\ntechniques for the purpose. However, the recently proposed Bluetooth-based PCT\nhas several limitations in terms of functionality and flexibility. The existing\nsystems are only able to detect direct contact (i.e., human-human contact), but\ncannot detect indirect contact (i.e., human-object, such as the disease\ntransmission through surface). Moreover, the rule of risky contact cannot be\nflexibly changed with the environmental situation and the nature of the virus.\nIn this paper, we propose a secure and efficient trajectory-based PCT system\nusing trusted hardware. We formalize trajectory-based PCT as a generalization\nof the well-studied Private Set Intersection (PSI), which is mostly based on\ncryptographic primitives and thus insufficient. We solve the problem by\nleveraging trusted hardware such as Intel SGX and designing a novel algorithm\nto achieve a secure, efficient and flexible PCT system. Our experiments on\nreal-world data show that the proposed system can achieve high performance and\nscalability. Specifically, our system (one single machine with Intel SGX) can\nprocess thousands of queries on 100 million records of trajectory data in a few\nseconds.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:22:56 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 07:42:41 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Kato", "Fumiyuki", ""], ["Cao", "Yang", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "2010.13442", "submitter": "Markus Schmid", "authors": "Markus L. Schmid and Nicole Schweikardt", "title": "A Purely Regular Approach to Non-Regular Core Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regular spanners (characterised by vset-automata) are closed under the\nalgebraic operations of union, join and projection, and have desirable\nalgorithmic properties. The core spanners (introduced by Fagin, Kimelfeld,\nReiss, and Vansummeren (PODS 2013, JACM 2015) as a formalisation of the core\nfunctionality of the query language AQL used in IBM's SystemT) additionally\nneed string equality selections and it has been shown by Freydenberger and\nHolldack (ICDT 2016, Theory of Computing Systems 2018) that this leads to high\ncomplexity and even undecidability of the typical problems in static analysis\nand query evaluation. We propose an alternative approach to core spanners: by\nincorporating the string-equality selections directly into the regular language\nthat represents the underlying regular spanner (instead of treating it as an\nalgebraic operation on the table extracted by the regular spanner), we obtain a\nfragment of core spanners that, while having slightly weaker expressive power\nthan the full class of core spanners, arguably still covers the intuitive\napplications of string equality selections for information extraction and has\nmuch better upper complexity bounds of the typical problems in static analysis\nand query evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:27:39 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Schmid", "Markus L.", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "2010.13561", "submitter": "Ben Hutchinson", "authors": "Ben Hutchinson, Andrew Smart, Alex Hanna, Emily Denton, Christina\n  Greer, Oddur Kjartansson, Parker Barnes, Margaret Mitchell", "title": "Towards Accountability for Machine Learning Datasets: Practices from\n  Software Engineering and Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rising concern for the societal implications of artificial intelligence\nsystems has inspired demands for greater transparency and accountability.\nHowever the datasets which empower machine learning are often used, shared and\nre-used with little visibility into the processes of deliberation which led to\ntheir creation. Which stakeholder groups had their perspectives included when\nthe dataset was conceived? Which domain experts were consulted regarding how to\nmodel subgroups and other phenomena? How were questions of representational\nbiases measured and addressed? Who labeled the data? In this paper, we\nintroduce a rigorous framework for dataset development transparency which\nsupports decision-making and accountability. The framework uses the cyclical,\ninfrastructural and engineering nature of dataset development to draw on best\npractices from the software development lifecycle. Each stage of the data\ndevelopment lifecycle yields a set of documents that facilitate improved\ncommunication and decision-making, as well as drawing attention the value and\nnecessity of careful data work. The proposed framework is intended to\ncontribute to closing the accountability gap in artificial intelligence\nsystems, by making visible the often overlooked work that goes into dataset\ncreation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 01:57:42 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 00:12:54 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hutchinson", "Ben", ""], ["Smart", "Andrew", ""], ["Hanna", "Alex", ""], ["Denton", "Emily", ""], ["Greer", "Christina", ""], ["Kjartansson", "Oddur", ""], ["Barnes", "Parker", ""], ["Mitchell", "Margaret", ""]]}, {"id": "2010.13619", "submitter": "Jonas Dann", "authors": "Jonas Dann and Daniel Ritter and Holger Fr\\\"oning", "title": "Exploring Memory Access Patterns for Graph Processing Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in business and technology (e.g., machine learning, social\nnetwork analysis) benefit from storing and processing growing amounts of\ngraph-structured data in databases and data science platforms. FPGAs as\naccelerators for graph processing with a customizable memory hierarchy promise\nsolving performance problems caused by inherent irregular memory access\npatterns on traditional hardware (e.g., CPU). However, developing such hardware\naccelerators is yet time-consuming and difficult and benchmarking is\nnon-standardized, hindering comprehension of the impact of memory access\npattern changes and systematic engineering of graph processing accelerators.\n  In this work, we propose a simulation environment for the analysis of graph\nprocessing accelerators based on simulating their memory access patterns.\nFurther, we evaluate our approach on two state-of-the-art FPGA graph processing\naccelerators and show reproducibility, comparablity, as well as the shortened\ndevelopment process by an example. Not implementing the cycle-accurate internal\ndata flow on accelerator hardware like FPGAs significantly reduces the\nimplementation time, increases the benchmark parameter transparency, and allows\ncomparison of graph processing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:34:39 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 11:43:23 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 20:17:31 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Dann", "Jonas", ""], ["Ritter", "Daniel", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2010.13637", "submitter": "Peng Gao", "authors": "Peng Gao, Fei Shao, Xiaoyuan Liu, Xusheng Xiao, Zheng Qin, Fengyuan\n  Xu, Prateek Mittal, Sanjeev R. Kulkarni, Dawn Song", "title": "Enabling Efficient Cyber Threat Hunting With Cyber Threat Intelligence", "comments": "Accepted paper at ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-based cyber threat hunting has emerged as an important solution to\ncounter sophisticated attacks. However, existing approaches require non-trivial\nefforts of manual query construction and have overlooked the rich external\nthreat knowledge provided by open-source Cyber Threat Intelligence (OSCTI). To\nbridge the gap, we propose ThreatRaptor, a system that facilitates threat\nhunting in computer systems using OSCTI. Built upon system auditing frameworks,\nThreatRaptor provides (1) an unsupervised, light-weight, and accurate NLP\npipeline that extracts structured threat behaviors from unstructured OSCTI\ntext, (2) a concise and expressive domain-specific query language, TBQL, to\nhunt for malicious system activities, (3) a query synthesis mechanism that\nautomatically synthesizes a TBQL query for hunting, and (4) an efficient query\nexecution engine to search the big audit logging data. Evaluations on a broad\nset of attack cases demonstrate the accuracy and efficiency of ThreatRaptor in\npractical threat hunting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:54:01 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 06:20:46 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Gao", "Peng", ""], ["Shao", "Fei", ""], ["Liu", "Xiaoyuan", ""], ["Xiao", "Xusheng", ""], ["Qin", "Zheng", ""], ["Xu", "Fengyuan", ""], ["Mittal", "Prateek", ""], ["Kulkarni", "Sanjeev R.", ""], ["Song", "Dawn", ""]]}, {"id": "2010.13717", "submitter": "Thomas Mu\\~noz", "authors": "Floris Geerts, Thomas Mu\\~noz, Cristian Riveros and Domagoj Vrgo\\v{c}", "title": "Expressive power of linear algebra query languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear algebra algorithms often require some sort of iteration or recursion\nas is illustrated by standard algorithms for Gaussian elimination, matrix\ninversion, and transitive closure. A key characteristic shared by these\nalgorithms is that they allow looping for a number of steps that is bounded by\nthe matrix dimension. In this paper we extend the matrix query language MATLANG\nwith this type of recursion, and show that this suffices to express classical\nlinear algebra algorithms. We study the expressive power of this language and\nshow that it naturally corresponds to arithmetic circuit families, which are\noften said to capture linear algebra. Furthermore, we analyze several\nsub-fragments of our language, and show that their expressive power is closely\ntied to logical formalisms on semiring-annotated relations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:59:09 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Geerts", "Floris", ""], ["Mu\u00f1oz", "Thomas", ""], ["Riveros", "Cristian", ""], ["Vrgo\u010d", "Domagoj", ""]]}, {"id": "2010.13721", "submitter": "Shuang Wang", "authors": "Shuang Wang, Hakan Ferhatosmanoglu", "title": "PPQ-Trajectory: Spatio-temporal Quantization for Querying in Large\n  Trajectory Repositories", "comments": "To appear at VLDB 2021", "journal-ref": null, "doi": "10.14778/3425879.3425891", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PPQ-trajectory, a spatio-temporal quantization based solution for\nquerying large dynamic trajectory data. PPQ-trajectory includes a\npartition-wise predictive quantizer (PPQ) that generates an error-bounded\ncodebook with autocorrelation and spatial proximity-based partitions. The\ncodebook is indexed to run approximate and exact spatio-temporal queries over\ncompressed trajectories. PPQ-trajectory includes a coordinate quadtree coding\nfor the codebook with support for exact queries. An incremental temporal\npartition-based index is utilised to avoid full reconstruction of trajectories\nduring queries. An extensive set of experimental results for spatio-temporal\nqueries on real trajectory datasets is presented. PPQ-trajectory shows\nsignificant improvements over the alternatives with respect to several\nperformance measures, including the accuracy of results when the summary is\nused directly to provide approximate query results, the spatial deviation with\nwhich spatio-temporal path queries can be answered when the summary is used as\nan index, and the time taken to construct the summary. Superior results on the\nquality of the summary and the compression ratio are also demonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:02:52 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Shuang", ""], ["Ferhatosmanoglu", "Hakan", ""]]}, {"id": "2010.14189", "submitter": "Roy Friedman", "authors": "Dolev Adas and Roy Friedman", "title": "Jiffy: A Fast, Memory Efficient, Wait-Free Multi-Producers\n  Single-Consumer Queue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications such as sharded data processing systems, sharded in-memory\nkey-value stores, data flow programming and load sharing applications, multiple\nconcurrent data producers are feeding requests into the same data consumer.\nThis can be naturally realized through concurrent queues, where each consumer\npulls its tasks from its dedicated queue. For scalability, wait-free queues are\noften preferred over lock based structures.\n  The vast majority of wait-free queue implementations, and even lock-free\nones, support the multi-producer multi-consumer model. Yet, this comes at a\npremium, since implementing wait-free multi-producer multi-consumer queues\nrequires utilizing complex helper data structures. The latter increases the\nmemory consumption of such queues and limits their performance and scalability.\nAdditionally, many such designs employ (hardware) cache unfriendly memory\naccess patterns.\n  In this work we study the implementation of wait-free multi-producer\nsingle-consumer queues. Specifically, we propose Jiffy, an efficient memory\nfrugal novel wait-free multi-producer single-consumer queue and formally prove\nits correctness. We then compare the performance and memory requirements of\nJiffy with other state of the art lock-free and wait-free queues. We show that\nindeed Jiffy can maintain good performance with up to 128 threads, delivers up\nto 50% better throughput than the next best construction we compared against,\nand consumes ~90% less memory.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 10:51:05 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 10:42:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Adas", "Dolev", ""], ["Friedman", "Roy", ""]]}, {"id": "2010.14227", "submitter": "Quanming Yao", "authors": "Yongqi Zhang and Quanming Yao and Lei Chen", "title": "Efficient, Simple and Automated Negative Sampling for Knowledge Graph\n  Embedding", "comments": "VLDB Journal accepted. arXiv admin note: text overlap with\n  arXiv:1812.06410", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative sampling, which samples negative triplets from non-observed ones in\nknowledge graph (KG), is an essential step in KG embedding. Recently,\ngenerative adversarial network (GAN), has been introduced in negative sampling.\nBy sampling negative triplets with large gradients, these methods avoid the\nproblem of vanishing gradient and thus obtain better performance. However, they\nmake the original model more complex and harder to train. In this paper,\nmotivated by the observation that negative triplets with large gradients are\nimportant but rare, we propose to directly keep track of them with the cache.\nIn this way, our method acts as a \"distilled\" version of previous GAN-based\nmethods, which does not waste training time on additional parameters to fit the\nfull distribution of negative triplets. However, how to sample from and update\nthe cache are two critical questions. We propose to solve these issues by\nautomated machine learning techniques. The automated version also covers\nGAN-based methods as special cases. Theoretical explanation of NSCaching is\nalso provided, justifying the superior over fixed sampling scheme. Besides, we\nfurther extend NSCaching with skip-gram model for graph embedding. Finally,\nextensive experiments show that our method can gain significant improvements on\nvarious KG embedding models and the skip-gram model, and outperforms the\nstate-of-the-art negative sampling methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 14:16:35 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 01:52:43 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Zhang", "Yongqi", ""], ["Yao", "Quanming", ""], ["Chen", "Lei", ""]]}, {"id": "2010.14931", "submitter": "Hebatalla Eldakiky", "authors": "Hebatalla Eldakiky, David Hung-Chang Du, Eman Ramadan (Department of\n  Computer Science and Engineering, University of Minnesota - Twin Cities, USA)", "title": "TurboKV: Scaling Up The Performance of Distributed Key-Value Stores With\n  In-Switch Coordination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power and flexibility of software-defined networks lead to a programmable\nnetwork infrastructure in which in-network computation can help accelerating\nthe performance of applications. This can be achieved by offloading some\ncomputational tasks to the network. However, what kind of computational tasks\nshould be delegated to the network to accelerate applications performance? In\nthis paper, we propose a way to exploit the usage of programmable switches to\nscale up the performance of distributed key-value stores. Moreover, as a\nproof-of-concept, we propose TurboKV, an efficient distributed key-value store\narchitecture that utilizes programmable switches as: 1) partition management\nnodes to store the key-value store partitions and replicas information; and 2)\nmonitoring stations to measure the load of storage nodes, this monitoring\ninformation is used to balance the load among storage nodes. We also propose a\nkey-based routing protocol to route the search queries of clients based on the\nrequested keys to targeted storage nodes. Our experimental results of an\ninitial prototype show that our proposed architecture improves the throughput\nand reduces the latency of distributed key-value stores when compared to the\nexisting architectures.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:17:30 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Eldakiky", "Hebatalla", "", "Department of\n  Computer Science and Engineering, University of Minnesota - Twin Cities, USA"], ["Du", "David Hung-Chang", "", "Department of\n  Computer Science and Engineering, University of Minnesota - Twin Cities, USA"], ["Ramadan", "Eman", "", "Department of\n  Computer Science and Engineering, University of Minnesota - Twin Cities, USA"]]}, {"id": "2010.14959", "submitter": "David Howey", "authors": "Adam Lewis-Douglas, Luke Pitt, David A. Howey", "title": "Galvanalyser: A Battery Test Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DB cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance and lifetime testing of batteries requires considerable effort\nand expensive specialist equipment. A wide range of potentiostats and battery\ntesters are available on the market, but there is no standardisation of data\nexchange and data storage between them. To address this, we present\nGalvanalyser, a battery test database developed to manage the growing\nchallenges of collating, managing and accessing data produced by multiple\ndifferent battery testers. Collation is managed by a client-side application,\nthe `Harvester', which pushes new data up to a PostgreSQL database on a server.\nData access is possible in two ways: firstly, a web application allows data to\nbe searched and viewed in a browser, with the option to plot data; secondly, a\nPython application programming interface (API) can connect directly to the\ndatabase and pull requested data sets into Python. We hope to make Galvanalyser\nopenly available soon. If you wish to test the system, please contact us for\nearly access.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 13:22:44 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Lewis-Douglas", "Adam", ""], ["Pitt", "Luke", ""], ["Howey", "David A.", ""]]}, {"id": "2010.15404", "submitter": "Xike Xie", "authors": "Ting Wang and Xike Xie and Xin Cao and Torben Bach Pedersen and Yang\n  Wang and Mingjun Xiao", "title": "On Efficient and Scalable Time-Continuous Spatial Crowdsourcing -- Full\n  Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of advanced mobile terminals opened up a new crowdsourcing\navenue, spatial crowdsourcing, to utilize the crowd potential to perform\nreal-world tasks. In this work, we study a new type of spatial crowdsourcing,\ncalled time-continuous spatial crowdsourcing (TCSC in short). It supports broad\napplications for long-term continuous spatial data acquisition, ranging from\nenvironmental monitoring to traffic surveillance in citizen science and\ncrowdsourcing projects. However, due to limited budgets and limited\navailability of workers in practice, the data collected is often incomplete,\nincurring data deficiency problem. To tackle that, in this work, we first\npropose an entropy-based quality metric, which captures the joint effects of\nincompletion in data acquisition and the imprecision in data interpolation.\nBased on that, we investigate quality-aware task assignment methods for both\nsingle- and multi-task scenarios. We show the NP-hardness of the single-task\ncase, and design polynomial-time algorithms with guaranteed approximation\nratios. We study novel indexing and pruning techniques for further enhancing\nthe performance in practice. Then, we extend the solution to multi-task\nscenarios and devise a parallel framework for speeding up the process of\noptimization. We conduct extensive experiments on both real and synthetic\ndatasets to show the effectiveness of our proposals.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 08:02:36 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Wang", "Ting", ""], ["Xie", "Xike", ""], ["Cao", "Xin", ""], ["Pedersen", "Torben Bach", ""], ["Wang", "Yang", ""], ["Xiao", "Mingjun", ""]]}, {"id": "2010.15879", "submitter": "Maciej Besta", "authors": "Maciej Besta, Dimitri Stanojevic, Tijana Zivic, Jagpreet Singh,\n  Maurice Hoerold, Torsten Hoefler", "title": "Log(Graph): A Near-Optimal High-Performance Graph Representation", "comments": null, "journal-ref": "Proceedings of the 27th International Conference on Parallel\n  Architectures and Compilation (PACT'18), 2018", "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's graphs used in domains such as machine learning or social network\nanalysis may contain hundreds of billions of edges. Yet, they are not\nnecessarily stored efficiently, and standard graph representations such as\nadjacency lists waste a significant number of bits while graph compression\nschemes such as WebGraph often require time-consuming decompression. To address\nthis, we propose Log(Graph): a graph representation that combines high\ncompression ratios with very low-overhead decompression to enable cheaper and\nfaster graph processing. The key idea is to encode a graph so that the parts of\nthe representation approach or match the respective storage lower bounds. We\ncall our approach \"graph logarithmization\" because these bounds are usually\nlogarithmic. Our high-performance Log(Graph) implementation based on modern\nbitwise operations and state-of-the-art succinct data structures achieves high\ncompression ratios as well as performance. For example, compared to the tuned\nGraph Algorithm Processing Benchmark Suite (GAPBS), it reduces graph sizes by\n20-35% while matching GAPBS' performance or even delivering speedups due to\nreducing amounts of transferred data. It approaches the compression ratio of\nthe established WebGraph compression library while enabling speedups of up to\nmore than 2x. Log(Graph) can improve the design of various graph processing\nengines or libraries on single NUMA nodes as well as distributed-memory\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:41:08 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Besta", "Maciej", ""], ["Stanojevic", "Dimitri", ""], ["Zivic", "Tijana", ""], ["Singh", "Jagpreet", ""], ["Hoerold", "Maurice", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.15981", "submitter": "Tianzheng Wang", "authors": "Yongjun He, Jiacheng Lu, Tianzheng Wang", "title": "CoroBase: Coroutine-Oriented Main-Memory Database Engine", "comments": "To appear in VLDB 2021", "journal-ref": null, "doi": "10.14778/3430915.3430932", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data stalls are a major overhead in main-memory database engines due to the\nuse of pointer-rich data structures. Lightweight coroutines ease the\nimplementation of software prefetching to hide data stalls by overlapping\ncomputation and asynchronous data prefetching. Prior solutions, however, mainly\nfocused on (1) individual components and operations and (2) intra-transaction\nbatching that requires interface changes, breaking backward compatibility. It\nwas not clear how they apply to a full database engine and how much end-to-end\nbenefit they bring under various workloads.\n  This paper presents \\corobase, a main-memory database engine that tackles\nthese challenges with a new coroutine-to-transaction paradigm.\nCoroutine-to-transaction models transactions as coroutines and thus enables\ninter-transaction batching, avoiding application changes but retaining the\nbenefits of prefetching. We show that on a 48-core server, CoroBase can perform\nclose to 2x better for read-intensive workloads and remain competitive for\nworkloads that inherently do not benefit from software prefetching.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 22:54:52 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["He", "Yongjun", ""], ["Lu", "Jiacheng", ""], ["Wang", "Tianzheng", ""]]}, {"id": "2010.16037", "submitter": "Mohamed Trabelsi", "authors": "Mohamed Trabelsi, Jin Cao, Jeff Heflin", "title": "Semantic Labeling Using a Deep Contextualized Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating schema labels automatically for column values of data tables has\nmany data science applications such as schema matching, and data discovery and\nlinking. For example, automatically extracted tables with missing headers can\nbe filled by the predicted schema labels which significantly minimizes human\neffort. Furthermore, the predicted labels can reduce the impact of inconsistent\nnames across multiple data tables. Understanding the connection between column\nvalues and contextual information is an important yet neglected aspect as\npreviously proposed methods treat each column independently. In this paper, we\npropose a context-aware semantic labeling method using both the column values\nand context. Our new method is based on a new setting for semantic labeling,\nwhere we sequentially predict labels for an input table with missing headers.\nWe incorporate both the values and context of each data column using the\npre-trained contextualized language model, BERT, that has achieved significant\nimprovements in multiple natural language processing tasks. To our knowledge,\nwe are the first to successfully apply BERT to solve the semantic labeling\ntask. We evaluate our approach using two real-world datasets from different\ndomains, and we demonstrate substantial improvements in terms of evaluation\nmetrics over state-of-the-art feature-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 03:04:22 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Trabelsi", "Mohamed", ""], ["Cao", "Jin", ""], ["Heflin", "Jeff", ""]]}, {"id": "2010.16340", "submitter": "Yuval Moskovitch", "authors": "Yuval Moskovitch and H. V. Jagadish", "title": "Patterns Count-Based Labels for Datasets", "comments": "ICDE2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counts of attribute-value combinations are central to the profiling of a\ndataset, particularly in determining fitness for use and in eliminating bias\nand unfairness. While counts of individual attribute values may be stored in\nsome dataset profiles, there are too many combinations of attributes for it to\nbe practical to store counts for each combination. In this paper, we develop\nthe notion of storing a \"label\" of limited size that can be used to obtain good\nestimates for these counts. A label, in this paper, contains information\nregarding the count of selected patterns--attributes values combinations--in\nthe data. We define an estimation function, that uses this label to estimate\nthe count of every pattern. We present the problem of finding the optimal label\ngiven a bound on its size and propose a heuristic algorithm for generating\noptimal labels. We experimentally show the accuracy of count estimates derived\nfrom the resulting labels and the efficiency of our algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 16:00:34 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 15:10:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Moskovitch", "Yuval", ""], ["Jagadish", "H. V.", ""]]}]