[{"id": "1112.0045", "submitter": "Aleksandar Stojmirovi\\'c", "authors": "Aleksandar Stojmirovi\\'c, Alexander Bliskovsky and Yi-Kuo Yu", "title": "CytoITMprobe: a network information flow plugin for Cytoscape", "comments": "16 pages, 6 figures. Version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DB q-bio.MN", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  To provide the Cytoscape users the possibility of integrating ITM Probe into\ntheir workflows, we developed CytoITMprobe, a new Cytoscape plugin.\nCytoITMprobe maintains all the desirable features of ITM Probe and adds\nadditional flexibility not achievable through its web service version. It\nprovides access to ITM Probe either through a web server or locally. The input,\nconsisting of a Cytoscape network, together with the desired origins and/or\ndestinations of information and a dissipation coefficient, is specified through\na query form. The results are shown as a subnetwork of significant nodes and\nseveral summary tables. Users can control the composition and appearance of the\nsubnetwork and interchange their ITM Probe results with other software tools\nthrough tab-delimited files.\n  The main strength of CytoITMprobe is its flexibility. It allows the user to\nspecify as input any Cytoscape network, rather than being restricted to the\npre-compiled protein-protein interaction networks available through the ITM\nProbe web service. Users may supply their own edge weights and\ndirectionalities. Consequently, as opposed to ITM Probe web service,\nCytoITMprobe can be applied to many other domains of network-based research\nbeyond protein-networks. It also enables seamless integration of ITM Probe\nresults with other Cytoscape plugins having complementary functionality for\ndata analysis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 22:10:50 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2012 22:22:23 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Stojmirovi\u0107", "Aleksandar", ""], ["Bliskovsky", "Alexander", ""], ["Yu", "Yi-Kuo", ""]]}, {"id": "1112.0343", "submitter": "Giorgio Orsi PhD", "authors": "Georg Gottlob and Giorgio Orsi and Andreas Pieris", "title": "Ontological Queries: Rewriting and Optimization (Extended Version)", "comments": "Extended version of \"Ontological Queries: Rewriting and Optimization\"\n  presented at ICDE 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Ontological queries are evaluated against an ontology rather than directly on\na database. The evaluation and optimization of such queries is an intriguing\nnew problem for database research.\n  In this paper we discuss two important aspects of this problem: query\nrewriting and query optimization. Query rewriting consists of the compilation\nof an ontological query into an equivalent query against the underlying\nrelational database. The focus here is on soundness and completeness. We review\nprevious results and present a new rewriting algorithm for rather general types\nof ontological constraints.\n  In particular, we show how a conjunctive query against an ontology can be\ncompiled into a union of conjunctive queries against the underlying database.\nOntological query optimization, in this context, attempts to improve this\nprocess so to produce possibly small and cost-effective UCQ rewritings for an\ninput query. We review existing optimization methods, and propose an effective\nnew method that works for linear Datalog+/-, a class of Datalog-based rules\nthat encompasses well-known description logics of the DL-Lite family.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2011 22:19:24 GMT"}], "update_date": "2011-12-05", "authors_parsed": [["Gottlob", "Georg", ""], ["Orsi", "Giorgio", ""], ["Pieris", "Andreas", ""]]}, {"id": "1112.0857", "submitter": "Jelle Hellings", "authors": "Jelle Hellings, George H. L. Fletcher, and Herman Haverkort", "title": "I/O efficient bisimulation partitioning on very large directed acyclic\n  graphs", "comments": null, "journal-ref": null, "doi": "10.1145/2213836.2213899", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the first efficient external-memory algorithm to\ncompute the bisimilarity equivalence classes of a directed acyclic graph (DAG).\nDAGs are commonly used to model data in a wide variety of practical\napplications, ranging from XML documents and data provenance models, to web\ntaxonomies and scientific workflows. In the study of efficient reasoning over\nmassive graphs, the notion of node bisimilarity plays a central role. For\nexample, grouping together bisimilar nodes in an XML data set is the first step\nin many sophisticated approaches to building indexing data structures for\nefficient XPath query evaluation. To date, however, only internal-memory\nbisimulation algorithms have been investigated. As the size of real-world DAG\ndata sets often exceeds available main memory, storage in external memory\nbecomes necessary. Hence, there is a practical need for an efficient approach\nto computing bisimulation in external memory.\n  Our general algorithm has a worst-case IO-complexity of O(Sort(|N| + |E|)),\nwhere |N| and |E| are the numbers of nodes and edges, resp., in the data graph\nand Sort(n) is the number of accesses to external memory needed to sort an\ninput of size n. We also study specializations of this algorithm to common\nvariations of bisimulation for tree-structured XML data sets. We empirically\nverify efficient performance of the algorithms on graphs and XML documents\nhaving billions of nodes and edges, and find that the algorithms can process\nsuch graphs efficiently even when very limited internal memory is available.\nThe proposed algorithms are simple enough for practical implementation and use,\nand open the door for further study of external-memory bisimulation algorithms.\nTo this end, the full open-source C++ implementation has been made freely\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 08:30:55 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Hellings", "Jelle", ""], ["Fletcher", "George H. L.", ""], ["Haverkort", "Herman", ""]]}, {"id": "1112.1117", "submitter": "Smriti Bhagat", "authors": "Mohammad Khabbaz, Smriti Bhagat, Laks V. S. Lakshmanan", "title": "Finding Heavy Paths in Graphs: A Rank Join Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs have been commonly used to model many applications. A natural problem\nwhich abstracts applications such as itinerary planning, playlist\nrecommendation, and flow analysis in information networks is that of finding\nthe heaviest path(s) in a graph. More precisely, we can model these\napplications as a graph with non-negative edge weights, along with a monotone\nfunction such as sum, which aggregates edge weights into a path weight,\ncapturing some notion of quality. We are then interested in finding the top-k\nheaviest simple paths, i.e., the $k$ simple (cycle-free) paths with the\ngreatest weight, whose length equals a given parameter $\\ell$. We call this the\n\\emph{Heavy Path Problem} (HPP). It is easy to show that the problem is\nNP-Hard.\n  In this work, we develop a practical approach to solve the Heavy Path problem\nby leveraging a strong connection with the well-known Rank Join paradigm. We\nfirst present an algorithm by adapting the Rank Join algorithm. We identify its\nlimitations and develop a new exact algorithm called HeavyPath and a scalable\nheuristic algorithm. We conduct a comprehensive set of experiments on three\nreal data sets and show that HeavyPath outperforms the baseline algorithms\nsignificantly, with respect to both $\\ell$ and $k$. Further, our heuristic\nalgorithm scales to longer lengths, finding paths that are empirically within\n50% of the optimum solution or better under various settings, and takes only a\nfraction of the running time compared to the exact algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 23:12:31 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2012 00:33:45 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Khabbaz", "Mohammad", ""], ["Bhagat", "Smriti", ""], ["Lakshmanan", "Laks V. S.", ""]]}, {"id": "1112.1668", "submitter": "Casey Bennett", "authors": "Casey Bennett, Thomas Doub", "title": "Data Mining and Electronic Health Records: Selecting Optimal Clinical\n  Treatments in Practice", "comments": "Keywords: Data Mining; Decision Support Systems, Clinical; Electronic\n  Health Records; Evidence-Based Medicine; Data Warehouse", "journal-ref": "Proceedings of the 6th International Conference on Data Mining.\n  (2010) pp. 313-318", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHR's) are only a first step in capturing and\nutilizing health-related data - the problem is turning that data into useful\ninformation. Models produced via data mining and predictive analysis profile\ninherited risks and environmental/behavioral factors associated with patient\ndisorders, which can be utilized to generate predictions about treatment\noutcomes. This can form the backbone of clinical decision support systems\ndriven by live data based on the actual population. The advantage of such an\napproach based on the actual population is that it is \"adaptive\". Here, we\nevaluate the predictive capacity of a clinical EHR of a large mental healthcare\nprovider (~75,000 distinct clients a year) to provide decision support\ninformation in a real-world clinical setting. Initial research has achieved a\n70% success rate in predicting treatment outcomes using these methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 19:44:19 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Bennett", "Casey", ""], ["Doub", "Thomas", ""]]}, {"id": "1112.1734", "submitter": "Marcos Domingues", "authors": "Marcos Aur\\'elio Domingues, Solange Oliveira Rezende", "title": "Using Taxonomies to Facilitate the Analysis of the Association Rules", "comments": "ECML/PKDD'05 The Second International Workshop on Knowledge Discovery\n  and Ontologies (KDO'05)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Data Mining process enables the end users to analyze, understand and use\nthe extracted knowledge in an intelligent system or to support in the\ndecision-making processes. However, many algorithms used in the process\nencounter large quantities of patterns, complicating the analysis of the\npatterns. This fact occurs with association rules, a Data Mining technique that\ntries to identify intrinsic patterns in large data sets. A method that can help\nthe analysis of the association rules is the use of taxonomies in the step of\npost-processing knowledge. In this paper, the GART algorithm is proposed, which\nuses taxonomies to generalize association rules, and the RulEE-GAR\ncomputational module, that enables the analysis of the generalized rules.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 23:33:15 GMT"}], "update_date": "2011-12-09", "authors_parsed": [["Domingues", "Marcos Aur\u00e9lio", ""], ["Rezende", "Solange Oliveira", ""]]}, {"id": "1112.2020", "submitter": "Rui Chen", "authors": "Rui Chen, Benjamin C. M. Fung, Bipin C. Desai", "title": "Differentially Private Trajectory Data Publication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing prevalence of location-aware devices, trajectory data has\nbeen generated and collected in various application domains. Trajectory data\ncarries rich information that is useful for many data analysis tasks. Yet,\nimproper publishing and use of trajectory data could jeopardize individual\nprivacy. However, it has been shown that existing privacy-preserving trajectory\ndata publishing methods derived from partition-based privacy models, for\nexample k-anonymity, are unable to provide sufficient privacy protection.\n  In this paper, motivated by the data publishing scenario at the Societe de\ntransport de Montreal (STM), the public transit agency in Montreal area, we\nstudy the problem of publishing trajectory data under the rigorous differential\nprivacy model. We propose an efficient data-dependent yet differentially\nprivate sanitization algorithm, which is applicable to different types of\ntrajectory data. The efficiency of our approach comes from adaptively narrowing\ndown the output domain by building a noisy prefix tree based on the underlying\ndata. Moreover, as a post-processing step, we make use of the inherent\nconstraints of a prefix tree to conduct constrained inferences, which lead to\nbetter utility. This is the first paper to introduce a practical solution for\npublishing large volume of trajectory data under differential privacy. We\nexamine the utility of sanitized data in terms of count queries and frequent\nsequential pattern mining. Extensive experiments on real-life trajectory data\nfrom the STM demonstrate that our approach maintains high utility and is\nscalable to large trajectory datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 05:19:57 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["Chen", "Rui", ""], ["Fung", "Benjamin C. M.", ""], ["Desai", "Bipin C.", ""]]}, {"id": "1112.2026", "submitter": "Vijaykumar Selvam", "authors": "Vijaykumar S and Saravanakumar S G", "title": "Future Robotics Database Management System along with Cloud TPS", "comments": "12 pages,5 figures,First model; International Journal on Cloud\n  Computing: Services and Architecture(IJCCSA),Vol.1, No.3, November 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper deals with memory management issues of robotics. In our proposal\nwe break one of the major issues in creating humanoid. . Database issue is the\ncomplicated thing in robotics schema design here in our proposal we suggest new\nconcept called NOSQL database for the effective data retrieval, so that the\nhumanoid robots will get the massive thinking ability in searching each items\nusing chained instructions. For query transactions in robotics we need an\neffective consistency transactions so by using latest technology called\nCloudTPS which guarantees full ACID properties so that the robot can make their\nqueries using multi-item transactions through this we obtain data consistency\nin data retrievals. In addition we included map reduce concepts it can splits\nthe job to the respective workers so that it can process the data in a parallel\nway.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 06:59:00 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["S", "Vijaykumar", ""], ["G", "Saravanakumar S", ""]]}, {"id": "1112.2040", "submitter": "V. Vijayakumar", "authors": "Vijayakumar V and Nedunchezhian R", "title": "Recent Trends and Research Issues in Video Association Mining", "comments": "13 pages; 1 Figure; 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-growing digital libraries and video databases, it is\nincreasingly important to understand and mine the knowledge from video database\nautomatically. Discovering association rules between items in a large video\ndatabase plays a considerable role in the video data mining research areas.\nBased on the research and development in the past years, application of\nassociation rule mining is growing in different domains such as surveillance,\nmeetings, broadcast news, sports, archives, movies, medical data, as well as\npersonal and online media collections. The purpose of this paper is to provide\ngeneral framework of mining the association rules from video database. This\narticle is also represents the research issues in video association mining\nfollowed by the recent trends.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 08:16:03 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["V", "Vijayakumar", ""], ["R", "Nedunchezhian", ""]]}, {"id": "1112.2137", "submitter": "Syed  Ibrahim", "authors": "S.P.Syed Ibrahim and K.R.Chandran", "title": "Compact Weighted Class Association Rule Mining using Information Gain", "comments": "13 pages; International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.1, No.6, November 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted association rule mining reflects semantic significance of item by\nconsidering its weight. Classification constructs the classifier and predicts\nthe new data instance. This paper proposes compact weighted class association\nrule mining method, which applies weighted association rule mining in the\nclassification and constructs an efficient weighted associative classifier.\nThis proposed associative classification algorithm chooses one non class\ninformative attribute from dataset and all the weighted class association rules\nare generated based on that attribute. The weight of the item is considered as\none of the parameter in generating the weighted class association rules. This\nproposed algorithm calculates the weight using the HITS model. Experimental\nresults show that the proposed system generates less number of high quality\nrules which improves the classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 16:22:00 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["Ibrahim", "S. P. Syed", ""], ["Chandran", "K. R.", ""]]}, {"id": "1112.2155", "submitter": "Ali Karami", "authors": "Ali Karami and Ahmad Baraani-Dastjerdi", "title": "A Concurrency Control Method Based on Commitment Ordering in Mobile\n  Databases", "comments": "15 pages, 13 figures, Journal: International Journal of Database\n  Management Systems (IJDMS)", "journal-ref": "International Journal of Database Management Systems (IJDMS),\n  Vol.3, No.4, November 2011, 39-53", "doi": "10.5121/ijdms.2011.3404", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disconnection of mobile clients from server, in an unclear time and for an\nunknown duration, due to mobility of mobile clients, is the most important\nchallenges for concurrency control in mobile database with client-server model.\nApplying pessimistic common classic methods of concurrency control (like 2pl)\nin mobile database leads to long duration blocking and increasing waiting time\nof transactions. Because of high rate of aborting transactions, optimistic\nmethods aren`t appropriate in mobile database. In this article, OPCOT\nconcurrency control algorithm is introduced based on optimistic concurrency\ncontrol method. Reducing communications between mobile client and server,\ndecreasing blocking rate and deadlock of transactions, and increasing\nconcurrency degree are the most important motivation of using optimistic method\nas the basis method of OPCOT algorithm. To reduce abortion rate of\ntransactions, in execution time of transactions` operators a timestamp is\nassigned to them. In other to checking commitment ordering property of\nscheduler, the assigned timestamp is used in server on time of commitment. In\nthis article, serializability of OPCOT algorithm scheduler has been proved by\nusing serializability graph. Results of evaluating simulation show that OPCOT\nalgorithm decreases abortion rate and waiting time of transactions in compare\nto 2pl and optimistic algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 17:27:51 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Karami", "Ali", ""], ["Baraani-Dastjerdi", "Ahmad", ""]]}, {"id": "1112.2336", "submitter": "Nasrin Mazaheri", "authors": "Nasrin Mazaheri Soudani and Ahmad Baraani-Dastgerdi", "title": "The Spatial Nearest Neighbor Skyline Queries", "comments": "15 pages, 14 figures, Journal:International Journal of Database\n  Management Systems (IJDMS)", "journal-ref": "International Journal of Database Management Systems (IJDMS),\n  Vol.3, No.4, November 2011, 65-79", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User preference queries are very important in spatial databases. With the\nhelp of these queries, one can found best location among points saved in\ndatabase. In many situation users evaluate quality of a location with its\ndistance from its nearest neighbor among a special set of points. There has\nbeen less attention about evaluating a location with its distance to nearest\nneighbors in spatial user preference queries. This problem has application in\nmany domains such as service recommendation systems and investment planning.\nRelated works in this field are based on top-k queries. The problem with top-k\nqueries is that user must set weights for attributes and a function for\naggregating them. This is hard for him in most cases. In this paper a new type\nof user preference queries called spatial nearest neighbor skyline queries will\nbe introduced in which user has some sets of points as query parameters. For\neach point in database attributes are its distances to the nearest neighbors\nfrom each set of query points. By separating this query as a subset of dynamic\nskyline queries N2S2 algorithm is provided for computing it. This algorithm has\ngood performance compared with the general branch and bound algorithm for\nskyline queries.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2011 08:43:54 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Soudani", "Nasrin Mazaheri", ""], ["Baraani-Dastgerdi", "Ahmad", ""]]}, {"id": "1112.2401", "submitter": "Rekik Jihen", "authors": "Jihen Drira Rekik, Leila Baccouche and Henda Ben Ghezala", "title": "A Real-Time Database QoS-aware Service Selection Protocol for MANET", "comments": "16 pages; International Journal of Database Management Systems\n  (IJDMS), November 2011, 101-116", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real-time database service selection depends typically to the system\nstability in order to handle the time-constrained transactions within their\ndeadline. However, applying the real-time database system in the mobile ad hoc\nnetworks requires considering the mobile nodes limited capacities. In this\npaper, we propose cross-layer service selection which combines performance\nmetrics measured in the real-time database system to those used by the routing\nprotocol in order to make the best selection decision. It ensures both\ntimeliness and energy efficiency by avoiding low-power and busy service\nprovider node. A multicast packet is used in order to reduce the transmission\ncost and network load when sending the same packet to multiple service\nproviders. In this paper, we evaluate the performance of our proposed protocol.\nSimulation results, using the Network Simulator NS2, improve that the protocol\ndecreases the deadline miss ratio of packets, increases the service\navailability and reduces the service response time.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2011 21:00:16 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Rekik", "Jihen Drira", ""], ["Baccouche", "Leila", ""], ["Ghezala", "Henda Ben", ""]]}, {"id": "1112.2605", "submitter": "Houari Mahfoud", "authors": "Houari Mahfoud (INRIA Lorraine - LORIA / LIFC), Abdessamad Imine\n  (INRIA Lorraine - LORIA / LIFC)", "title": "Secure Querying of Recursive XML Views: A Standard XPath-based Technique", "comments": "(2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the art approaches for securing XML documents allow users to\naccess data only through authorized views defined by annotating an XML grammar\n(e.g. DTD) with a collection of XPath expressions. To prevent improper\ndisclosure of confidential information, user queries posed on these views need\nto be rewritten into equivalent queries on the underlying documents. This\nrewriting enables us to avoid the overhead of view materialization and\nmaintenance. A major concern here is that query rewriting for recursive XML\nviews is still an open problem. To overcome this problem, some works have been\nproposed to translate XPath queries into non-standard ones, called Regular\nXPath queries. However, query rewriting under Regular XPath can be of\nexponential size as it relies on automaton model. Most importantly, Regular\nXPath remains a theoretical achievement. Indeed, it is not commonly used in\npractice as translation and evaluation tools are not available. In this paper,\nwe show that query rewriting is always possible for recursive XML views using\nonly the expressive power of the standard XPath. We investigate the extension\nof the downward class of XPath, composed only by child and descendant axes,\nwith some axes and operators and we propose a general approach to rewrite\nqueries under recursive XML views. Unlike Regular XPath-based works, we provide\na rewriting algorithm which processes the query only over the annotated DTD\ngrammar and which can run in linear time in the size of the query. An\nexperimental evaluation demonstrates that our algorithm is efficient and scales\nwell.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 16:21:27 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Mahfoud", "Houari", "", "INRIA Lorraine - LORIA / LIFC"], ["Imine", "Abdessamad", "", "INRIA Lorraine - LORIA / LIFC"]]}, {"id": "1112.2610", "submitter": "Asterios Katsifodimos", "authors": "Konstantinos Karanasos (INRIA Saclay - Ile de France, LRI), Asterios\n  Katsifodimos (INRIA Saclay - Ile de France, LRI), Ioana Manolescu (INRIA\n  Saclay - Ile de France, LRI), Spyros Zoupanos (INRIA Saclay - Ile de France,\n  LRI, MPII)", "title": "The ViP2P Platform: XML Views in P2P", "comments": "RR-7812 (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing volumes of XML data sources on the Web or produced by\nenterprises, organizations etc. raise many performance challenges for data\nmanagement applications. In this work, we are concerned with the distributed,\npeer-to-peer management of large corpora of XML documents, based on distributed\nhash table (or DHT, in short) overlay networks. We present ViP2P (standing for\nViews in Peer-to-Peer), a distributed platform for sharing XML documents based\non a structured P2P network infrastructure (DHT). At the core of ViP2P stand\ndistributed materialized XML views, defined by arbitrary XML queries, filled in\nwith data published anywhere in the network, and exploited to efficiently\nanswer queries issued by any network peer. ViP2P allows user queries to be\nevaluated over XML documents published by peers in two modes. First, a\nlong-running subscription mode, when a query can be registered in the system\nand receive answers incrementally when and if published data matches the query.\nSecond, queries can also be asked in an ad-hoc, snapshot mode, where results\nare required immediately and must be computed based on the results of other\nlong-running, subscription queries. ViP2P innovates over other similar\nDHT-based XML sharing platforms by using a very expressive structured XML query\nlanguage. This expressivity leads to a very flexible distribution of XML\ncontent in the ViP2P network, and to efficient snapshot query execution. ViP2P\nhas been tested in real deployments of hundreds of computers. We present the\nplatform architecture, its internal algorithms, and demonstrate its efficiency\nand scalability through a set of experiments. Our experimental results outgrow\nby orders of magnitude similar competitor systems in terms of data volumes,\nnetwork size and data dissemination throughput.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 16:29:25 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Karanasos", "Konstantinos", "", "INRIA Saclay - Ile de France, LRI"], ["Katsifodimos", "Asterios", "", "INRIA Saclay - Ile de France, LRI"], ["Manolescu", "Ioana", "", "INRIA\n  Saclay - Ile de France, LRI"], ["Zoupanos", "Spyros", "", "INRIA Saclay - Ile de France,\n  LRI, MPII"]]}, {"id": "1112.2661", "submitter": "Jong Yoon", "authors": "Jong P. Yoon", "title": "Location- and Time-Dependent VPD for Privacy-Preserving Wireless\n  Accesses to Cloud Services", "comments": "16 pages, 8 figures, International Journal on Cloud Computing:\n  Services and Architecture (IJCCSA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of smartphones in recent years has changed the wireless landscape.\nSmartphones have become a platform for online user interface to cloud\ndatabases. Cloud databases may provide a large set of user-private and\nsensitive data (i.e., objects), while smartphone users (i.e., subjects) provide\nlocation-sensitive information. Secure and private services in wireless\naccessing to cloud databases have been discussed actively for the past recent\nyears. However, the previous techniques are unsatisfactory for dynamism of\nmoving subjects' wireless accesses. In this paper, we propose a novel technique\nto dynamically generate virtual private databases (VPD) for each access by\ntaking subjects' location and time information into account. The contribution\nof this paper includes a privacy-preserving access control mechanism for\ndynamism of wireless access.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 19:22:24 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Yoon", "Jong P.", ""]]}, {"id": "1112.2663", "submitter": "Sankar Rajagopal Dr", "authors": "Dr. Sankar Rajagopal", "title": "Customer Data Clustering using Data Mining Technique", "comments": "11 pages, 2 figures and 1 table", "journal-ref": "International Journal of Database Management Systems ( IJDMS )\n  Vol.3, No.4, November 2011", "doi": "10.5121/ijdms.2011.3401", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and patterns extraction from customer data is very important\nfor business support and decision making. Timely identification of newly\nemerging trends is very important in business process. Large companies are\nhaving huge volume of data but starving for knowledge. To overcome the\norganization current issue, the new breed of technique is required that has\nintelligence and capability to solve the knowledge scarcity and the technique\nis called Data mining. The objectives of this paper are to identify the\nhigh-profit, high-value and low-risk customers by one of the data mining\ntechnique - customer clustering. In the first phase, cleansing the data and\ndeveloped the patterns via demographic clustering algorithm using IBM I-Miner.\nIn the second phase, profiling the data, develop the clusters and identify the\nhigh-value low-risk customers. This cluster typically represents the 10-20\npercent of customers which yields 80% of the revenue.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 20:52:14 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Rajagopal", "Dr. Sankar", ""]]}, {"id": "1112.3018", "submitter": "Marco Tereso Tomas", "authors": "Marco Tereso, Jorge Bernardino", "title": "Open Source CRM Systems for SMEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer Relationship Management (CRM) systems are very common in large\ncompanies. However, CRM systems are not very common in Small and Medium\nEnterprises (SMEs). Most SMEs do not implement CRM systems due to several\nreasons, such as lack of knowledge about CRM or lack of financial resources to\nimplement CRM systems. SMEs have to start implementing Information Systems (IS)\ntechnology into their business operations in order to improve business values\nand gain more competitive advantage over rivals. CRM system has the potential\nto help improve the business value and competitive capabilities of SMEs. Given\nthe high fixed costs of normal activity of companies, we intend to promote free\nand viable solutions for small and medium businesses. In this paper, we explain\nthe reasons why SMEs do not implement CRM system and the benefits of using open\nsource CRM system in SMEs. We also describe the functionalities of top open\nsource CRM systems, examining the applicability of these tools in fitting the\nneeds of SMEs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 20:19:12 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["Tereso", "Marco", ""], ["Bernardino", "Jorge", ""]]}, {"id": "1112.3062", "submitter": "Guy Kloss", "authors": "Miriam Ney and Guy K. Kloss and Andreas Schreiber", "title": "Using Provenance to support Good Laboratory Practice in Grid\n  Environments", "comments": "Book Chapter for \"Data Provenance and Data Management for eScience,\"\n  of Studies in Computational Intelligence series, Springer. 25 pages, 8\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Conducting experiments and documenting results is daily business of\nscientists. Good and traceable documentation enables other scientists to\nconfirm procedures and results for increased credibility. Documentation and\nscientific conduct are regulated and termed as \"good laboratory practice.\"\nLaboratory notebooks are used to record each step in conducting an experiment\nand processing data. Originally, these notebooks were paper based. Due to\ncomputerised research systems, acquired data became more elaborate, thus\nincreasing the need for electronic notebooks with data storage, computational\nfeatures and reliable electronic documentation. As a new approach to this, a\nscientific data management system (DataFinder) is enhanced with features for\ntraceable documentation. Provenance recording is used to meet requirements of\ntraceability, and this information can later be queried for further analysis.\nDataFinder has further important features for scientific documentation: It\nemploys a heterogeneous and distributed data storage concept. This enables\naccess to different types of data storage systems (e. g. Grid data\ninfrastructure, file servers). In this chapter we describe a number of building\nblocks that are available or close to finished development. These components\nare intended for assembling an electronic laboratory notebook for use in Grid\nenvironments, while retaining maximal flexibility on usage scenarios as well as\nmaximal compatibility overlap towards each other. Through the usage of such a\nsystem, provenance can successfully be used to trace the scientific workflow of\npreparation, execution, evaluation, interpretation and archiving of research\ndata. The reliability of research results increases and the research process\nremains transparent to remote research partners.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 22:30:36 GMT"}], "update_date": "2011-12-15", "authors_parsed": [["Ney", "Miriam", ""], ["Kloss", "Guy K.", ""], ["Schreiber", "Andreas", ""]]}, {"id": "1112.3134", "submitter": "Azade Roohany -", "authors": "Mohammad-Reza Feizi-Derakhshi and Azade Roohany", "title": "Proposing Cluster_Similarity Method in Order to Find as Much Better\n  Similarities in Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different ways of entering data into databases result in duplicate records\nthat cause increasing of databases' size. This is a fact that we cannot ignore\nit easily. There are several methods that are used for this purpose. In this\npaper, we have tried to increase the accuracy of operations by using cluster\nsimilarity instead of direct similarity of fields. So that clustering is done\non fields of database and according to accomplished clustering on fields,\nsimilarity degree of records is obtained. In this method by using present\ninformation in database, more logical similarity is obtained for deficient\ninformation that in general, the method of cluster similarity could improve\noperations 24% compared with previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 07:12:31 GMT"}], "update_date": "2011-12-15", "authors_parsed": [["Feizi-Derakhshi", "Mohammad-Reza", ""], ["Roohany", "Azade", ""]]}, {"id": "1112.4031", "submitter": "Tejaswini Hilage Abhijit", "authors": "Tejaswini Hilage, R.V.Kulkarni", "title": "Application of Data Mining Techniques to a Selected Business\n  Organisation with Special Reference to Buying Behaviour", "comments": "16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is a new concept & an exploration and analysis of large data\nsets, in order to discover meaningful patterns and rules. Many organizations\nare now using the data mining techniques to find out meaningful patterns from\nthe database. The present paper studies how data mining techniques can be apply\nto the large database. These data mining techniques give certain behavioral\npattern from the database. The results which come after analysis of the\ndatabase are useful for organization. This paper examines the result after\napplying association rule mining technique, rule induction technique and\nApriori algorithm. These techniques are applied to the database of shopping\nmall. Market basket analysis is performing by the above mentioned techniques\nand some important results are found such as buying behavior.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2011 06:27:52 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Hilage", "Tejaswini", ""], ["Kulkarni", "R. V.", ""]]}, {"id": "1112.4261", "submitter": "Chandrasekhar Sekhar t", "authors": "T.Chandrasekhar, K.Thangavel and E.Elayaraja", "title": "Performance Analysis of Enhanced Clustering Algorithm for Gene\n  Expression Data", "comments": "ISSN (Online): 1694-0814 http://www.IJCSI.org", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 6, No 3, November 2011", "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Microarrays are made it possible to simultaneously monitor the expression\nprofiles of thousands of genes under various experimental conditions. It is\nused to identify the co-expressed genes in specific cells or tissues that are\nactively used to make proteins. This method is used to analysis the gene\nexpression, an important task in bioinformatics research. Cluster analysis of\ngene expression data has proved to be a useful tool for identifying\nco-expressed genes, biologically relevant groupings of genes and samples. In\nthis paper we applied K-Means with Automatic Generations of Merge Factor for\nISODATA- AGMFI. Though AGMFI has been applied for clustering of Gene Expression\nData, this proposed Enhanced Automatic Generations of Merge Factor for ISODATA-\nEAGMFI Algorithms overcome the drawbacks of AGMFI in terms of specifying the\noptimal number of clusters and initialization of good cluster centroids.\nExperimental results on Gene Expression Data show that the proposed EAGMFI\nalgorithms could identify compact clusters with perform well in terms of the\nSilhouette Coefficients cluster measure.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 08:16:13 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Chandrasekhar", "T.", ""], ["Thangavel", "K.", ""], ["Elayaraja", "E.", ""]]}, {"id": "1112.5908", "submitter": "Leopoldo Bertossi", "authors": "Jaffer Gardezi and Leopoldo Bertossi", "title": "Query Answering under Matching Dependencies for Data Cleaning:\n  Complexity and Algorithms", "comments": "Conference submission, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching dependencies (MDs) have been recently introduced as declarative\nrules for entity resolution (ER), i.e. for identifying and resolving duplicates\nin relational instance $D$. A set of MDs can be used as the basis for a\npossibly non-deterministic mechanism that computes a duplicate-free instance\nfrom $D$. The possible results of this process are the clean, \"minimally\nresolved instances\" (MRIs). There might be several MRIs for $D$, and the\n\"resolved answers\" to a query are those that are shared by all the MRIs. We\ninvestigate the problem of computing resolved answers. We look at various sets\nof MDs, developing syntactic criteria for determining (in)tractability of the\nresolved answer problem, including a dichotomy result. For some tractable\nclasses of MDs and conjunctive queries, we present a query rewriting\nmethodology that can be used to retrieve the resolved answers. We also\ninvestigate connections with \"consistent query answering\", deriving further\ntractability results for MD-based ER.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2011 02:40:28 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Gardezi", "Jaffer", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1112.5957", "submitter": "Eya Ben Ahmed", "authors": "Eya Ben Ahmed, Ahlem Nabli, Fa\\\"iez Gargouri", "title": "Usage Des Mesures Pour La G\\'en\\'eration Des R\\`egles d'Associations\n  Cycliques", "comments": "18 pages, 3 figures; 7 \\`eme journ\\'ees Francophones sur les\n  Entrep\\^ots de donn\\'ees et l'Analyse en ligne (EDA'2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online analytical processing (OLAP) does not provide any explanation of\ncorrelations discovered between data. Thus, the coupling of OLAP and data\nmining, especially association rules, is considered as an efficient solution to\nthis problem. In this context, we mainly focus on a particular class of\nassociation rules which is the cyclic association rules. These rules aimed to\ndiscover patterns that display regular variation over user-defined intervals.\nGenerally,the generated patterns do not take an advantage from the\nspecificities of the multidimensional context namely, the consideration of the\nmeasures and their aggregations. In this paper, we introduce a novel method for\nextracting cyclic association rules from measures, and we redefine the\nevaluation metrics of association rules quality inspired of the temporal\nsummarizability of measures concept through the integration of appropriate\naggregation functions. To prove the usefulness of our approach, we conduct an\nempirical study on a real data warehouse.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2011 13:15:47 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2012 11:03:45 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Ahmed", "Eya Ben", ""], ["Nabli", "Ahlem", ""], ["Gargouri", "Fa\u00efez", ""]]}]