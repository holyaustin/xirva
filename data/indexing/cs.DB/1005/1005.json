[{"id": "1005.0198", "submitter": "Olivier Teste", "authors": "Houssem Jerbi (IRIT), Genevi\\`eve Pujolle (IRIT), Franck Ravat (IRIT),\n  Olivier Teste (IRIT)", "title": "Personnalisation de Syst\\`emes OLAP Annot\\'es", "comments": null, "journal-ref": "XXVIII\\`eme Congr\\`es Informatique des Organisations et Syst\\`emes\n  d'Information et de D\\'ecision - INFORSID'10, Marseille : France (2010)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with personalization of annotated OLAP systems. Data\nconstellation is extended to support annotations and user preferences.\nAnnotations reflect the decision-maker experience whereas user preferences\nenable users to focus on the most interesting data. User preferences allow\nannotated contextual recommendations helping the decision-maker during his/her\nmultidimensional navigations.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 06:36:10 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Jerbi", "Houssem", "", "IRIT"], ["Pujolle", "Genevi\u00e8ve", "", "IRIT"], ["Ravat", "Franck", "", "IRIT"], ["Teste", "Olivier", "", "IRIT"]]}, {"id": "1005.0201", "submitter": "Olivier Teste", "authors": "Franck Ravat (IRIT), Olivier Teste (IRIT), Gilles Zurfluh (IRIT)", "title": "Personnalisation de bases de donn\\'ees multidimensionnelles", "comments": null, "journal-ref": "Congr\\`es Informatique des Organisations et Syst\\`emes\n  d'Information et de D\\'ecision - INFORSID'07, Perros-Guirec : France (2007)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with decision support systems resting on multidimensional\nmodelling of data. Moreover, we intend to offer a set of concepts and\nmechanisms for personalized multidimensional database specifications. This\npersonalization consists in associating weights to different components of a\nmultidimensional schema. Personalization specifications are specified through\nthe use of a language based on the principle of Event Condition Action. This\npersonalisation determines multidimensional data display as well as their\nanalyses (with the use of drilling or rotating operations).\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 06:40:36 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Ravat", "Franck", "", "IRIT"], ["Teste", "Olivier", "", "IRIT"], ["Zurfluh", "Gilles", "", "IRIT"]]}, {"id": "1005.0212", "submitter": "Olivier Teste", "authors": "Fr\\'ed\\'eric Bret (IRIT), Olivier Teste (IRIT)", "title": "Construction graphique d'entrep\\^ots et de magasins de donn\\'ees", "comments": null, "journal-ref": "Congr\\`es INFormatique des ORganisations et Syst\\`emes\n  d'Information et de D\\'ecision - INFORSID'99, La Garde : France (1999)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, decisional systems have became a significant research topic in\ndatabases. Data warehouses and data marts are the main elements of such\nsystems. This paper presents our decisional support system. We present\ngraphical interfaces which help the administrator to build data warehouses and\ndata marts. We present a data warehouse building interface based on an\nobject-oriented conceptual model. This model allows the warehouse data\nhistorisation at three levels: attribute, class and environment. Also, we\npresent a data mart building interface which allows warehouse data to be\nreorganised through a multidimensional object-oriented model.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 07:43:22 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Bret", "Fr\u00e9d\u00e9ric", "", "IRIT"], ["Teste", "Olivier", "", "IRIT"]]}, {"id": "1005.0213", "submitter": "Olivier Teste", "authors": "Franck Ravat (IRIT), Olivier Teste (IRIT), Gilles Zurfluh (IRIT)", "title": "Alg\\`ebre OLAP et langage graphique", "comments": null, "journal-ref": "Congr\\`es Informatique des Organisations et Syst\\`emes\n  d'Information et de D\\'ecision - INFORSID'06, Hammamet : Tunisie (2006)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with OLAP systems based on multidimensional model. The\nconceptual model we provide, represents data through a constellation\n(multi-facts) composed of several multi-hierarchy dimensions. In this model,\ndata are displayed through multidimensional tables. We define a query algebra\nhandling these tables. This user oriented algebra is composed of a closure core\nof OLAP operators as soon as advanced operators dedicated to complex analysis.\nFinally, we specify a graphical OLAP language based on this algebra. This\nlanguage facilitates analyses of decision makers.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 07:43:25 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Ravat", "Franck", "", "IRIT"], ["Teste", "Olivier", "", "IRIT"], ["Zurfluh", "Gilles", "", "IRIT"]]}, {"id": "1005.0214", "submitter": "Olivier Teste", "authors": "Franck Ravat (IRIT), Olivier Teste (IRIT), Zurfluh Gilles (IRIT)", "title": "Mod\\'elisation et extraction de donn\\'ees pour un entrep\\^ot objet", "comments": null, "journal-ref": "Bases de donn\\'ees avanc\\'ees (BDA 2000), Blois : France (2000)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an object-oriented model for designing complex and\ntime-variant data warehouse data. The main contribution is the warehouse class\nconcept, which extends the class concept by temporal and archive filters as\nwell as a mapping function. Filters allow the keeping of relevant data changes\nwhereas the mapping function defines the warehouse class schema from a global\ndata source schema. The approach take into account static properties as well as\ndynamic properties. The behaviour extraction is based on the use-matrix\nconcept.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 07:43:45 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Ravat", "Franck", "", "IRIT"], ["Teste", "Olivier", "", "IRIT"], ["Gilles", "Zurfluh", "", "IRIT"]]}, {"id": "1005.0217", "submitter": "Olivier Teste", "authors": "Gilles Hubert (IRIT), Olivier Teste (IRIT)", "title": "Analyse multigraduelle OLAP", "comments": null, "journal-ref": "Revue des nouvelles technologies - RNTI, E-15 (2009) 253-258", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decisional systems are based on multidimensional databases improving OLAP\nanalyses. The paper describes a new OLAP operator named \"BLEND\" to perform\nmultigradual analyses. The operation transforms multidimensional structures\nduring querying in order to analyse measures according to various granularity\nlevels, which are reorganised into a single parameter. We study valid\ncombinations of the operation in the context of strict hierarchies. First\nexperimentations implement the operation in an R-OLAP framework showing the\nslight cost of this operation.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 07:47:20 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Hubert", "Gilles", "", "IRIT"], ["Teste", "Olivier", "", "IRIT"]]}, {"id": "1005.0218", "submitter": "Olivier Teste", "authors": "Faiza Ghozzi (IRIT), Franck Ravat (IRIT), Olivier Teste (IRIT), Gilles\n  Zurfluh (IRIT)", "title": "Contraintes pour mod\\`ele et langage multidimensionnels", "comments": null, "journal-ref": "Bases de donn\\'ees avanc\\'ees 2003, Lyon : France (2003)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines a constraint-based model dedicated to multidimensional\ndatabases. The model we define represents data through a constellation of facts\n(subjects of analyse) associated to dimensions (axis of analyse), which are\npossibly shared. Each dimension is organised according to several hierarchies\n(views of analyse) integrating several levels of data granularity. In order to\ninsure data consistency, we introduce 5 semantic constraints (exclusion,\ninclusion, partition, simultaneity, totality) which can be intra-dimension or\ninter-dimensions; the intra-dimension constraints allow the expression of\nconstraints between hierarchies within a same dimension whereas the\ninter-dimensions constraints focus on hierarchies of distinct dimensions. We\nalso study repercussions of these constraints on multidimensional manipulations\nand we provide extensions of the multidimensional operators.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 07:47:56 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Ghozzi", "Faiza", "", "IRIT"], ["Ravat", "Franck", "", "IRIT"], ["Teste", "Olivier", "", "IRIT"], ["Zurfluh", "Gilles", "", "IRIT"]]}, {"id": "1005.0219", "submitter": "Olivier Teste", "authors": "Franck Ravat (IRIT), Olivier Teste (IRIT)", "title": "Mod\\'elisation et manipulation de donn\\'ees historis\\'ees et archiv\\'ees\n  dans un entrep\\^ot orient\\'e objet", "comments": null, "journal-ref": "Bases de donn\\'ees avanc\\'ees 2001, Agadir : Maroc (2001)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with temporal and archive object-oriented data warehouse\nmodelling and querying. In a first step, we define a data model describing\nwarehouses as central repositories of complex and temporal data extracted from\none information source. The model is based on the concepts of warehouse object\nand environment. A warehouse object is composed of one current state, several\npast states (modelling value changes) and several archive states (summarising\nsome value changes). An environment defines temporal parts in a warehouse\nschema according to a relevant granularity (attribute, class or graph). In a\nsecond step, we provide a query algebra dedicated to data warehouses. This\nalgebra, which is based on common object algebras, integrates temporal\noperators and operators for querying object states. An other important\ncontribution concerns dedicated operators allowing users to transform warehouse\nobjects in temporal series as well as operators facilitating analytical\ntreatments.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 07:48:53 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Ravat", "Franck", "", "IRIT"], ["Teste", "Olivier", "", "IRIT"]]}, {"id": "1005.0220", "submitter": "Olivier Teste", "authors": "Olivier Teste (IRIT)", "title": "Elaboration d'entrep\\^ots de donn\\'ees complexes", "comments": null, "journal-ref": "congr\\`es INFormatique des ORganisations et Syst\\`emes\n  d'Information et de D\\'ecision - INFORSID'00, Lyon : France (2000)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the data warehouse modelling used in decision support\nsystems. We provide an object-oriented data warehouse model allowing data\nwarehouse description as a central repository of relevant, complex and temporal\ndata. Our model integrates three concepts such as warehouse object, environment\nand warehouse class. Each warehouse object is composed of one current state,\nseveral past states (modelling its detailed evolutions) and several archive\nstates (modelling its evolutions within a summarised form). The environment\nconcept defines temporal parts in the data warehouse schema with significant\ngranularities (attribute, class, graph). Finally, we provide five functions\naiming at defining the data warehouse structures and two functions allowing the\nwarehouse class inheritance hierarchy organisation.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 07:49:14 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Teste", "Olivier", "", "IRIT"]]}, {"id": "1005.0224", "submitter": "Olivier Teste", "authors": "Olivier Teste (IRIT)", "title": "Towards Conceptual Multidimensional Design in Decision Support Systems", "comments": null, "journal-ref": "5th East-European Conference on Advances in Databases and\n  Information Systems - ADBIS'01, Vilnius : Lithuania (2001)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional databases support efficiently on-line analytical processing\n(OLAP). In this paper, we depict a model dedicated to multidimensional\ndatabases. The approach we present designs decisional information through a\nconstellation of facts and dimensions. Each dimension is possibly shared\nbetween several facts and it is organised according to multiple hierarchies. In\naddition, we define a comprehensive query algebra regrouping the more popular\nmultidimensional operations in current commercial systems and research\napproaches. We introduce new operators dedicated to a constellation. Finally,\nwe describe a prototype that allows managers to query constellations of facts,\ndimensions and multiple hierarchies.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 07:55:33 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Teste", "Olivier", "", "IRIT"]]}, {"id": "1005.0662", "submitter": "Daniel Golovin", "authors": "Daniel Golovin", "title": "The B-Skip-List: A Simpler Uniquely Represented Alternative to B-Trees", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work, the author introduced the B-treap, a uniquely represented\nB-tree analogue, and proved strong performance guarantees for it. However, the\nB-treap maintains complex invariants and is very complex to implement. In this\npaper we introduce the B-skip-list, which has most of the guarantees of the\nB-treap, but is vastly simpler and easier to implement. Like the B-treap, the\nB-skip-list may be used to construct strongly history-independent index\nstructures and filesystems; such constructions reveal no information about the\nhistorical sequence of operations that led to the current logical state. For\nexample, a uniquely represented filesystem would support the deletion of a file\nin a way that, in a strong information-theoretic sense, provably removes all\nevidence that the file ever existed. Like the B-tree, the B-skip-list has depth\nO(log_B (n)) where B is the block transfer size of the external memory, uses\nlinear space with high probability, and supports efficient one-dimensional\nrange queries.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2010 01:44:41 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Golovin", "Daniel", ""]]}, {"id": "1005.0813", "submitter": "Robert Weigel", "authors": "R.S. Weigel, D. M. Lindholm, A. Wilson, and J. Faden", "title": "TSDS: high-performance merge, subset, and filter software for time\n  series-like data", "comments": "Submitted to Earth Science Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time Series Data Server (TSDS) is a software package for implementing a\nserver that provides fast super-setting, sub-setting, filtering, and uniform\ngridding of time series-like data. TSDS was developed to respond quickly to\nrequests for long time spans of data. Data may be served from a fast database,\ntypically created by aggregating granules (e.g., data files) from a remote data\nsource and storing them in a local cache that is optimized for serving time\nseries. The system was designed specifically for time series data, and is\noptimized for requests where the longest dimension of the requested data\nstructure is time. Scalar, vector, and spectrogram time series types are\nsupported. The user can interact with the server by requesting a time series, a\ndate range, and an optional filter to apply to the data. Available filters\ninclude strides, block average/minimum/maximum, exclude, and inequality.\nConstraint expressions are supported, which allow such operations as a request\nfor data from one time series when a different time series satisfied a\nspecified relationship. TSDS builds upon DAP (Data Access Protocol), NcML\n(netCDF Mark-up language) and related software libraries. In this work, we\ndescribe the current design of this server, as well as planned features and\npotential implementation strategies.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 23:30:24 GMT"}], "update_date": "2010-05-06", "authors_parsed": [["Weigel", "R. S.", ""], ["Lindholm", "D. M.", ""], ["Wilson", "A.", ""], ["Faden", "J.", ""]]}, {"id": "1005.0972", "submitter": "Rdv Ijcsis", "authors": "S. F. Rodd, U. P. Kulkarni", "title": "Adaptive Tuning Algorithm for Performance tuning of Database Management\n  System", "comments": "IEEE Publication format, International Journal of Computer Science\n  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Performance tuning of Database Management Systems(DBMS) is both complex and\nchallenging as it involves identifying and altering several key performance\ntuning parameters. The quality of tuning and the extent of performance\nenhancement achieved greatly depends on the skill and experience of the\nDatabase Administrator (DBA). As neural networks have the ability to adapt to\ndynamically changing inputs and also their ability to learn makes them ideal\ncandidates for employing them for tuning purpose. In this paper, a novel tuning\nalgorithm based on neural network estimated tuning parameters is presented. The\nkey performance indicators are proactively monitored and fed as input to the\nNeural Network and the trained network estimates the suitable size of the\nbuffer cache, shared pool and redo log buffer size. The tuner alters these\ntuning parameters using the estimated values using a rate change computing\nalgorithm. The preliminary results show that the proposed method is effective\nin improving the query response time for a variety of workload types. .\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 10:47:03 GMT"}], "update_date": "2010-05-07", "authors_parsed": [["Rodd", "S. F.", ""], ["Kulkarni", "U. P.", ""]]}, {"id": "1005.1567", "submitter": "Francesco Scarcello", "authors": "Gianluigi Greco and Francesco Scarcello", "title": "On The Power of Tree Projections: Structural Tractability of Enumerating\n  CSP Solutions", "comments": null, "journal-ref": "Constraints 18(1): 38-74 (2013)", "doi": "10.1007/s10601-012-9129-8", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of deciding whether CSP instances admit solutions has been deeply\nstudied in the literature, and several structural tractability results have\nbeen derived so far. However, constraint satisfaction comes in practice as a\ncomputation problem where the focus is either on finding one solution, or on\nenumerating all solutions, possibly projected to some given set of output\nvariables. The paper investigates the structural tractability of the problem of\nenumerating (possibly projected) solutions, where tractability means here\ncomputable with polynomial delay (WPD), since in general exponentially many\nsolutions may be computed. A general framework based on the notion of tree\nprojection of hypergraphs is considered, which generalizes all known\ndecomposition methods. Tractability results have been obtained both for classes\nof structures where output variables are part of their specification, and for\nclasses of structures where computability WPD must be ensured for any possible\nset of output variables. These results are shown to be tight, by exhibiting\ndichotomies for classes of structures having bounded arity and where the tree\ndecomposition method is considered.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2010 14:46:29 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2010 14:05:01 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Greco", "Gianluigi", ""], ["Scarcello", "Francesco", ""]]}, {"id": "1005.1934", "submitter": "Michael Wick", "authors": "Michael Wick, Andrew McCallum, Gerome Miklau", "title": "Scalable Probabilistic Databases with Factor Graphs and MCMC", "comments": "Submitted to VLDB 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic databases play a crucial role in the management and\nunderstanding of uncertain data. However, incorporating probabilities into the\nsemantics of incomplete databases has posed many challenges, forcing systems to\nsacrifice modeling power, scalability, or restrict the class of relational\nalgebra formula under which they are closed. We propose an alternative approach\nwhere the underlying relational database always represents a single world, and\nan external factor graph encodes a distribution over possible worlds; Markov\nchain Monte Carlo (MCMC) inference is then used to recover this uncertainty to\na desired level of fidelity. Our approach allows the efficient evaluation of\narbitrary queries over probabilistic databases with arbitrary dependencies\nexpressed by graphical models with structure that changes during inference.\nMCMC sampling provides efficiency by hypothesizing {\\em modifications} to\npossible worlds rather than generating entire worlds from scratch. Queries are\nthen run over the portions of the world that change, avoiding the onerous cost\nof running full queries over each sampled world. A significant innovation of\nthis work is the connection between MCMC sampling and materialized view\nmaintenance techniques: we find empirically that using view maintenance\ntechniques is several orders of magnitude faster than naively querying each\nsampled world. We also demonstrate our system's ability to answer relational\nqueries with aggregation, and demonstrate additional scalability through the\nuse of parallelization.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2010 20:06:57 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Wick", "Michael", ""], ["McCallum", "Andrew", ""], ["Miklau", "Gerome", ""]]}, {"id": "1005.3773", "submitter": "Guozhang Wang", "authors": "Guozhang Wang, Marcos Vaz Salles, Benjamin Sowell, Xun Wang, Tuan Cao,\n  Alan Demers, Johannes Gehrke, Walker White", "title": "Behavioral Simulations in MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific domains, researchers are turning to large-scale behavioral\nsimulations to better understand important real-world phenomena. While there\nhas been a great deal of work on simulation tools from the high-performance\ncomputing community, behavioral simulations remain challenging to program and\nautomatically scale in parallel environments. In this paper we present BRACE\n(Big Red Agent-based Computation Engine), which extends the MapReduce framework\nto process these simulations efficiently across a cluster. We can leverage\nspatial locality to treat behavioral simulations as iterated spatial joins and\ngreatly reduce the communication between nodes. In our experiments we achieve\nnearly linear scale-up on several realistic simulations.\n  Though processing behavioral simulations in parallel as iterated spatial\njoins can be very efficient, it can be much simpler for the domain scientists\nto program the behavior of a single agent. Furthermore, many simulations\ninclude a considerable amount of complex computation and message passing\nbetween agents, which makes it important to optimize the performance of a\nsingle node and the communication across nodes. To address both of these\nchallenges, BRACE includes a high-level language called BRASIL (the Big Red\nAgent SImulation Language). BRASIL has object oriented features for programming\nsimulations, but can be compiled to a data-flow representation for automatic\nparallelization and optimization. We show that by using various optimization\ntechniques, we can achieve both scalability and single-node performance similar\nto that of a hand-coded simulation.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2010 17:58:33 GMT"}], "update_date": "2010-05-21", "authors_parsed": [["Wang", "Guozhang", ""], ["Salles", "Marcos Vaz", ""], ["Sowell", "Benjamin", ""], ["Wang", "Xun", ""], ["Cao", "Tuan", ""], ["Demers", "Alan", ""], ["Gehrke", "Johannes", ""], ["White", "Walker", ""]]}, {"id": "1005.4344", "submitter": "Stilian Stoev", "authors": "Stilian A. Stoev and Murad S. Taqqu", "title": "Max-stable sketches: estimation of Lp-norms, dominance norms and point\n  queries for non-negative signals", "comments": null, "journal-ref": null, "doi": null, "report-no": "Department of Statistics, the University of Michigan, Technical\n  Report 433", "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable random sketches can be computed efficiently on fast streaming\npositive data sets by using only sequential access to the data. They can be\nused to answer point and Lp-norm queries for the signal. There is an intriguing\nconnection between the so-called p-stable (or sum-stable) and the max-stable\nsketches. Rigorous performance guarantees through error-probability estimates\nare derived and the algorithmic implementation is discussed.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2010 14:56:12 GMT"}], "update_date": "2010-05-25", "authors_parsed": [["Stoev", "Stilian A.", ""], ["Taqqu", "Murad S.", ""]]}, {"id": "1005.4695", "submitter": "Tanu Malik", "authors": "Tanu Malik, Raghvendra Prasad, Sanket Patil, Amitabh Chaudhary, Venkat\n  Venkatasubramanian", "title": "Providing Scalable Data Services in Ubiquitous Networks", "comments": "12pages, 4 figures, 2 algorithms, Workshop for Ubiquitous Data\n  Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topology is a fundamental part of a network that governs connectivity between\nnodes, the amount of data flow and the efficiency of data flow between nodes.\nIn traditional networks, due to physical limitations, topology remains static\nfor the course of the network operation. Ubiquitous data networks (UDNs),\nalternatively, are more adaptive and can be configured for changes in their\ntopology. This flexibility in controlling their topology makes them very\nappealing and an attractive medium for supporting \"anywhere, any place\"\ncommunication. However, it raises the problem of designing a dynamic topology.\nThe dynamic topology design problem is of particular interest to application\nservice providers who need to provide cost-effective data services on a\nubiquitous network. In this paper we describe algorithms that decide when and\nhow the topology should be reconfigured in response to a change in the data\ncommunication requirements of the network. In particular, we describe and\ncompare a greedy algorithm, which is often used for topology reconfiguration,\nwith a non-greedy algorithm based on metrical task systems. Experiments show\nthe algorithm based on metrical task system has comparable performance to the\ngreedy algorithm at a much lower reconfiguration cost.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2010 20:31:57 GMT"}], "update_date": "2010-05-27", "authors_parsed": [["Malik", "Tanu", ""], ["Prasad", "Raghvendra", ""], ["Patil", "Sanket", ""], ["Chaudhary", "Amitabh", ""], ["Venkatasubramanian", "Venkat", ""]]}, {"id": "1005.4714", "submitter": "Sushovan De", "authors": "Sushovan De, Subbarao Kambhampati", "title": "Defining and Mining Functional Dependencies in Probabilistic Databases", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional dependencies -- traditional, approximate and conditional are of\ncritical importance in relational databases, as they inform us about the\nrelationships between attributes. They are useful in schema normalization, data\nrectification and source selection. Most of these were however developed in the\ncontext of deterministic data. Although uncertain databases have started\nreceiving attention, these dependencies have not been defined for them, nor are\nfast algorithms available to evaluate their confidences. This paper defines the\nlogical extensions of various forms of functional dependencies for\nprobabilistic databases and explores the connections between them. We propose a\npruning-based exact algorithm to evaluate the confidence of functional\ndependencies, a Monte-Carlo based algorithm to evaluate the confidence of\napproximate functional dependencies and algorithms for their conditional\ncounterparts in probabilistic databases. Experiments are performed on both\nsynthetic and real data evaluating the performance of these algorithms in\nassessing the confidence of dependencies and mining them from data. We believe\nthat having these dependencies and algorithms available for probabilistic\ndatabases will drive adoption of probabilistic data storage in the industry.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2010 00:06:09 GMT"}, {"version": "v2", "created": "Mon, 13 Dec 2010 18:28:36 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["De", "Sushovan", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1005.4752", "submitter": "Djoerd Hiemstra", "authors": "Djoerd Hiemstra and Vojkan Mihajlovic", "title": "A database approach to information retrieval: The remarkable\n  relationship between language models and region models", "comments": "Published as CTIT Technical Report 05-35", "journal-ref": null, "doi": null, "report-no": "TR-CTIT-10-15", "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this report, we unify two quite distinct approaches to information\nretrieval: region models and language models. Region models were developed for\nstructured document retrieval. They provide a well-defined behaviour as well as\na simple query language that allows application developers to rapidly develop\napplications. Language models are particularly useful to reason about the\nranking of search results, and for developing new ranking approaches. The\nunified model allows application developers to define complex language modeling\napproaches as logical queries on a textual database. We show a remarkable\none-to-one relationship between region queries and the language models they\nrepresent for a wide variety of applications: simple ad-hoc search,\ncross-language retrieval, video retrieval, and web search.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2010 08:04:33 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Hiemstra", "Djoerd", ""], ["Mihajlovic", "Vojkan", ""]]}, {"id": "1005.5432", "submitter": "Secretary Aircc Journal", "authors": "Spits Warnars H.L.H", "title": "Attribute oriented induction with star schema", "comments": "23 Pages, IJDMS", "journal-ref": "International Journal of Database Management Systems 2.2 (2010)\n  20-42", "doi": "10.5121/ijdms.2010.2202", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper will propose a novel star schema attribute induction as a new\nattribute induction paradigm and as improving from current attribute oriented\ninduction. A novel star schema attribute induction will be examined with\ncurrent attribute oriented induction based on characteristic rule and using non\nrule based concept hierarchy by implementing both of approaches. In novel star\nschema attribute induction some improvements have been implemented like\nelimination threshold number as maximum tuples control for generalization\nresult, there is no ANY as the most general concept, replacement the role\nconcept hierarchy with concept tree, simplification for the generalization\nstrategy steps and elimination attribute oriented induction algorithm. Novel\nstar schema attribute induction is more powerful than the current attribute\noriented induction since can produce small number final generalization tuples\nand there is no ANY in the results.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2010 07:27:24 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["H", "Spits Warnars H. L.", ""]]}, {"id": "1005.5433", "submitter": "Secretary Aircc Journal", "authors": "Nouha Arfaoui and Jalel Akaichi", "title": "A Data Warehouse Assistant Design System Based on Clover Model", "comments": "15 Pages, IJDMS", "journal-ref": "International Journal of Database Management Systems 2.2 (2010)\n  57-71", "doi": "10.5121/ijdms.2010.2204", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Nowadays, Data Warehouse (DW) plays a crucial role in the process of decision\nmaking. However, their design remains a very delicate and difficult task either\nfor expert or users. The goal of this paper is to propose a new approach based\non the clover model, destined to assist users to design a DW. The proposed\napproach is based on two main steps. The first one aims to guide users in their\nchoice of DW schema model. The second one aims to finalize the chosen model by\noffering to the designer views related to former successful DW design\nexperiences.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2010 07:35:23 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Arfaoui", "Nouha", ""], ["Akaichi", "Jalel", ""]]}, {"id": "1005.5434", "submitter": "Secretary Aircc Journal", "authors": "B.N. Keshavamurthy, Mitesh Sharma and Durga Toshniwal", "title": "Efficient Support Coupled Frequent Pattern Mining Over Progressive\n  Databases", "comments": "10 Pages, IJDMS", "journal-ref": "International Journal of Database Management Systems 2.2 (2010)\n  73-82", "doi": "10.5121/ijdms.2010.2205", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  There have been many recent studies on sequential pattern mining. The\nsequential pattern mining on progressive databases is relatively very new, in\nwhich we progressively discover the sequential patterns in period of interest.\nPeriod of interest is a sliding window continuously advancing as the time goes\nby. As the focus of sliding window changes, the new items are added to the\ndataset of interest and obsolete items are removed from it and become up to\ndate. In general, the existing proposals do not fully explore the real world\nscenario, such as items associated with support in data stream applications\nsuch as market basket analysis. Thus mining important knowledge from supported\nfrequent items becomes a non trivial research issue. Our proposed novel\napproach efficiently mines frequent sequential pattern coupled with support\nusing progressive mining tree.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2010 07:38:51 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Keshavamurthy", "B. N.", ""], ["Sharma", "Mitesh", ""], ["Toshniwal", "Durga", ""]]}, {"id": "1005.5438", "submitter": "Secretary Aircc Journal", "authors": "Raddad Al King, Abdelkader Hameurlain, Franck Morvan", "title": "Query Routing and Processing in Peer-To-Peer Data Sharing Systems", "comments": "24 Pages, IJDMS", "journal-ref": "International Journal of Database Management Systems 2.2 (2010)\n  116-139", "doi": "10.5121/ijdms.2010.2208", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Sharing musical files via the Internet was the essential motivation of early\nP2P systems. Despite of the great success of the P2P file sharing systems,\nthese systems support only \"simple\" queries. The focus in such systems is how\nto carry out an efficient query routing in order to find the nodes storing a\ndesired file. Recently, several research works have been made to extend P2P\nsystems to be able to share data having a fine granularity (i.e. atomic\nattribute) and to process queries written with a highly expressive language\n(i.e. SQL). These works have led to the emergence of P2P data sharing systems\nthat represent a new generation of P2P systems and, on the other hand, a next\nstage in a long period of the database research area. ? The characteristics of\nP2P systems (e.g. large-scale, node autonomy and instability) make impractical\nto have a global catalog that represents often an essential component in\ntraditional database systems. Usually, such a catalog stores information about\ndata, schemas and data sources. Query routing and processing are two problems\naffected by the absence of a global catalog. Locating relevant data sources and\ngenerating a close to optimal execution plan become more difficult. In this\npaper, we concentrate our study on proposed solutions for the both problems.\nFurthermore, selected case studies of main P2P data sharing systems are\nanalyzed and compared.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2010 08:16:38 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["King", "Raddad Al", ""], ["Hameurlain", "Abdelkader", ""], ["Morvan", "Franck", ""]]}, {"id": "1005.5514", "submitter": "Paraskevas Lekeas", "authors": "Yannis Delveroudis, Paraskevas V. Lekeas", "title": "Managing Semantic Loss during Query Reformulation in Peer Data\n  Management Systems", "comments": "SWOD '07 Proceedings of the 2007 IEEE International Workshop on\n  Databases for Next Generation Researchers", "journal-ref": null, "doi": "10.1109/SWOD.2007.353199", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with the notion of semantic loss in Peer Data\nManagement Systems (PDMS) queries. We define such a notion and we give a\nmechanism that discovers semantic loss in a PDMS network. Next, we propose an\nalgorithm that addresses the problem of restoring such a loss. Further\nevaluation of our proposed algorithm is an ongoing work\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2010 11:13:08 GMT"}], "update_date": "2011-07-18", "authors_parsed": [["Delveroudis", "Yannis", ""], ["Lekeas", "Paraskevas V.", ""]]}, {"id": "1005.5543", "submitter": "Sudeepa Roy", "authors": "Susan B. Davidson, Sanjeev Khanna, Tova Milo, Debmalya Panigrahi,\n  Sudeepa Roy", "title": "Provenance Views for Module Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": "UPenn Tech Report MS-CIS-10-22", "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific workflow systems increasingly store provenance information about\nthe module executions used to produce a data item, as well as the parameter\nsettings and intermediate data items passed between module executions. However,\nauthors/owners of workflows may wish to keep some of this information\nconfidential. In particular, a module may be proprietary, and users should not\nbe able to infer its behavior by seeing mappings between all data inputs and\noutputs. The problem we address in this paper is the following: Given a\nworkflow, abstractly modeled by a relation R, a privacy requirement \\Gamma and\ncosts associated with data. The owner of the workflow decides which data\n(attributes) to hide, and provides the user with a view R' which is the\nprojection of R over attributes which have not been hidden. The goal is to\nminimize the cost of hidden data while guaranteeing that individual modules are\n\\Gamma -private. We call this the \"secureview\" problem. We formally define the\nproblem, study its complexity, and offer algorithmic solutions.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2010 15:41:19 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2011 05:57:16 GMT"}], "update_date": "2011-04-05", "authors_parsed": [["Davidson", "Susan B.", ""], ["Khanna", "Sanjeev", ""], ["Milo", "Tova", ""], ["Panigrahi", "Debmalya", ""], ["Roy", "Sudeepa", ""]]}, {"id": "1005.5732", "submitter": "Paraskevas Lekeas", "authors": "Foto Afrati, Victor Kyritsis, Paraskevas V. Lekeas and Dora Souliou", "title": "A New Framework for Join Product Skew", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-27392-6", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different types of data skew can result in load imbalance in the context of\nparallel joins under the shared nothing architecture. We study one important\ntype of skew, join product skew (JPS). A static approach based on frequency\nclasses is proposed which takes for granted the data distribution of join\nattribute values. It comes from the observation that the join selectivity can\nbe expressed as a sum of products of frequencies of the join attribute values.\nAs a consequence, an appropriate assignment of join sub-tasks, that takes into\nconsideration the magnitude of the frequency products can alleviate the join\nproduct skew. Motivated by the aforementioned remark, we propose an algorithm,\ncalled Handling Join Product Skew (HJPS), to handle join product skew.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2010 19:40:01 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2010 06:57:31 GMT"}], "update_date": "2012-02-02", "authors_parsed": [["Afrati", "Foto", ""], ["Kyritsis", "Victor", ""], ["Lekeas", "Paraskevas V.", ""], ["Souliou", "Dora", ""]]}]