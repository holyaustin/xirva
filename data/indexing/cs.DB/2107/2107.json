[{"id": "2107.00001", "submitter": "Jan Philipp Portisch", "authors": "Jan Portisch, Michael Hladik, Heiko Paulheim", "title": "Background Knowledge in Schema Matching: Strategy vs. Data", "comments": "accepted at the International Semantic Web Conference '21 (ISWC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of external background knowledge can be beneficial for the task of\nmatching schemas or ontologies automatically. In this paper, we exploit six\ngeneral-purpose knowledge graphs as sources of background knowledge for the\nmatching task. The background sources are evaluated by applying three different\nexploitation strategies. We find that explicit strategies still outperform\nlatent ones and that the choice of the strategy has a greater impact on the\nfinal alignment than the actual background dataset on which the strategy is\napplied. While we could not identify a universally superior resource, BabelNet\nachieved consistently good results. Our best matcher configuration with\nBabelNet performs very competitively when compared to other matching systems\neven though no dataset-specific optimizations were made.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 19:16:33 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Portisch", "Jan", ""], ["Hladik", "Michael", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2107.00183", "submitter": "Quan-Lin Li", "authors": "Fan-Qi Ma, Quan-Lin Li, Yi-Han Liu, Yan-Xia Chang", "title": "Stochastic Performance Modeling for Practical Byzantine Fault Tolerance\n  Consensus in Blockchain", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.PF math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The practical Byzantine fault tolerant (PBFT) consensus mechanism is one of\nthe most basic consensus algorithms (or protocols) in blockchain technologies,\nthus its performance evaluation is an interesting and challenging topic due to\na higher complexity of its consensus work in the peer-to-peer network. This\npaper describes a simple stochastic performance model of the PBFT consensus\nmechanism, which is refined as not only a queueing system with complicated\nservice times but also a level-independent quasi-birth-and-death (QBD) process.\nFrom the level-independent QBD process, we apply the matrix-geometric solution\nto obtain a necessary and sufficient condition under which the PBFT consensus\nsystem is stable, and to be able to numerically compute the stationary\nprobability vector of the QBD process. Thus we provide four useful performance\nmeasures of the PBFT consensus mechanism, and can numerically calculate the\nfour performance measures. Finally, we use some numerical examples to verify\nthe validity of our theoretical results, and show how the four performance\nmeasures are influenced by some key parameters of the PBFT consensus. By means\nof the theory of multi-dimensional Markov processes, we are optimistic that the\nmethodology and results given in this paper are applicable in a wide range\nresearch of PBFT consensus mechanism and even other types of consensus\nmechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:24:45 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ma", "Fan-Qi", ""], ["Li", "Quan-Lin", ""], ["Liu", "Yi-Han", ""], ["Chang", "Yan-Xia", ""]]}, {"id": "2107.00729", "submitter": "Ruoyu Wang", "authors": "Ruoyu Wang, Daniel Sun, Guoqiang Li, Raymond Wong, Shiping Chen", "title": "Essence of Factual Knowledge", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases are collections of domain-specific and commonsense facts.\nRecently, the sizes of KBs are rocketing due to automatic extraction for\nknowledge and facts. For example, the number of facts in WikiData is up to 974\nmillion! According to our observation, current KBs, especially domain KBs, show\nstrong relevance in relations according to some topics. These patterns can be\nused to conclude and infer for part of facts in the KBs. Therefore, the\noriginal KBs can be minimzed by extracting patterns and essential facts.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 20:09:56 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 08:50:02 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Ruoyu", ""], ["Sun", "Daniel", ""], ["Li", "Guoqiang", ""], ["Wong", "Raymond", ""], ["Chen", "Shiping", ""]]}, {"id": "2107.00873", "submitter": "Heiko Paulheim", "authors": "Malte Brockmeier, Yawen Liu, Sunita Pateer, Sven Hertling and Heiko\n  Paulheim", "title": "On-Demand and Lightweight Knowledge Graph Generation -- a Demonstration\n  with DBpedia", "comments": "Accepted at Semantics 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern large-scale knowledge graphs, such as DBpedia, are datasets which\nrequire large computational resources to serve and process. Moreover, they\noften have longer release cycles, which leads to outdated information in those\ngraphs. In this paper, we present DBpedia on Demand -- a system which serves\nDBpedia resources on demand without the need to materialize and store the\nentire graph, and which even provides limited querying functionality.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 07:15:27 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Brockmeier", "Malte", ""], ["Liu", "Yawen", ""], ["Pateer", "Sunita", ""], ["Hertling", "Sven", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2107.00938", "submitter": "Mathias-Felipe De-Lima-Santos", "authors": "Mathias-Felipe de-Lima-Santos and Arwa Kooli", "title": "Instagrammable Data: Using Visuals to Showcase More Than Numbers on AJ\n  Labs Instagram Page", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.GR cs.MM cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News outlets are developing formats dedicated to social platforms that\ncapture audience attention, such as Instagram stories, Facebook Instant\narticles, and YouTube videos. In some cases, these formats are created in\ncollaboration with the tech companies themselves. At the same time, the use of\ndata-driven storytelling is becoming increasingly integrated into the\never-complex business models of news outlets, generating more impact and\nvisibility. Previous studies have focused on studying these two effects\nseparately. To address this gap in the literature, this paper identifies and\nanalyzes the use of data journalism on the Instagram content of AJ Labs, the\nteam dedicated to producing data-driven and interactive stories for the Al\nJazeera news network. Drawing upon a mixed-method approach, this study examines\nthe use and characteristics of data stories on social media platforms. Results\nsuggest that there is reliance on producing visual content that covers topics\nsuch as politics and violence. In general, AJ Labs relies on the use of\ninfographics and produces its own unique data. To conclude, this paper suggests\npotential ways to improve the use of Instagram to tell data stories.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:51:49 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["de-Lima-Santos", "Mathias-Felipe", ""], ["Kooli", "Arwa", ""]]}, {"id": "2107.01241", "submitter": "Julia Stoyanovich", "authors": "Marcelo Arenas, Pedro Bahamondes and Julia Stoyanovich", "title": "Temporal Regular Path Queries: Syntax, Semantics, and Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the last decade, substantial progress has been made towards standardizing\nthe syntax of graph query languages, and towards understanding their semantics\nand complexity of evaluation. In this paper, we consider temporal property\ngraphs (TPGs) and propose temporal regular path queries (TRPQ) that incorporate\ntime into TPGs navigation. Starting with design principles, we propose a\nnatural syntactic extension of the MATCH clause of popular graph query\nlanguages. We then formally present the semantics of TRPQs, and study the\ncomplexity of their evaluation. We show that TRPQs can be evaluated in\npolynomial time if TPGs are time-stamped with time points. We also identify\nfragments of the TRPQ language that admit efficient evaluation over a more\nsuccinct interval-annotated representation. Our work on the syntax, and the\npositive complexity results, pave the way to implementations of TRPQs that are\nboth usable and practical.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 19:08:29 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Arenas", "Marcelo", ""], ["Bahamondes", "Pedro", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "2107.01354", "submitter": "Dong-Wan Choi", "authors": "Hakbin Kim and Dong-Wan Choi", "title": "Pool of Experts: Realtime Querying Specialized Knowledge in Massive\n  Neural Networks", "comments": "In SIGMOD/PODS 2021", "journal-ref": "SIGMOD Conference 2021: 2244-2252", "doi": "10.1145/3448016.3457326", "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the great success of deep learning technologies, training and\ndelivery of a practically serviceable model is still a highly time-consuming\nprocess. Furthermore, a resulting model is usually too generic and heavyweight,\nand hence essentially goes through another expensive model compression phase to\nfit in a resource-limited device like embedded systems. Inspired by the fact\nthat a machine learning task specifically requested by mobile users is often\nmuch simpler than it is supported by a massive generic model, this paper\nproposes a framework, called Pool of Experts (PoE), that instantly builds a\nlightweight and task-specific model without any training process. For a\nrealtime model querying service, PoE first extracts a pool of primitive\ncomponents, called experts, from a well-trained and sufficiently generic\nnetwork by exploiting a novel conditional knowledge distillation method, and\nthen performs our train-free knowledge consolidation to quickly combine\nnecessary experts into a lightweight network for a target task. Thanks to this\ntrain-free property, in our thorough empirical study, PoE can build a fairly\naccurate yet compact model in a realtime manner, whereas it takes a few minutes\nper query for the other training methods to achieve a similar level of the\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 06:31:54 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kim", "Hakbin", ""], ["Choi", "Dong-Wan", ""]]}, {"id": "2107.01464", "submitter": "Ibrahim Sabek", "authors": "Ibrahim Sabek, Kapil Vaidya, Dominik Horn, Andreas Kipf, Tim Kraska", "title": "When Are Learned Models Better Than Hash Functions?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to study when learned models are better hash functions,\nparticular for hash-maps. We use lightweight piece-wise linear models to\nreplace the hash functions as they have small inference times and are\nsufficiently general to capture complex distributions. We analyze the learned\nmodels in terms of: the model inference time and the number of collisions.\nSurprisingly, we found that learned models are not much slower to compute than\nhash functions if optimized correctly. However, it turns out that learned\nmodels can only reduce the number of collisions (i.e., the number of times\ndifferent keys have the same hash value) if the model is able to over-fit to\nthe data; otherwise, it can not be better than an ordinary hash function.\nHence, how much better a learned model is in avoiding collisions highly depends\non the data and the ability of the model to over-fit. To evaluate the\neffectiveness of learned models, we used them as hash functions in the bucket\nchaining and Cuckoo hash tables. For bucket chaining hash table, we found that\nlearned models can achieve 30% smaller sizes and 10% lower probe latency. For\nCuckoo hash tables, in some datasets, learned models can increase the ratio of\nkeys stored in their primary locations by around 10%. In summary, we found that\nlearned models can indeed outperform hash functions but only for certain data\ndistributions and with a limited margin.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 16:50:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Sabek", "Ibrahim", ""], ["Vaidya", "Kapil", ""], ["Horn", "Dominik", ""], ["Kipf", "Andreas", ""], ["Kraska", "Tim", ""]]}, {"id": "2107.01615", "submitter": "Ralph Foorthuis", "authors": "Ralph Foorthuis", "title": "A Typology of Data Anomalies", "comments": "13 pages, 5 figures. Presented at the 17th International Conference\n  on Information Processing and Management of Uncertainty in Knowledge-Based\n  Systems (IPMU 2018). Note: for a fully developed and more detailed typology\n  of anomalies, see the follow-up publication 'On the Nature and Types of\n  Anomalies: A Review of Deviations in Data'. arXiv admin note: text overlap\n  with arXiv:2007.15634", "journal-ref": null, "doi": "10.1007/978-3-319-91476-3_3", "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies are cases that are in some way unusual and do not appear to fit the\ngeneral patterns present in the dataset. Several conceptualizations exist to\ndistinguish between different types of anomalies. However, these are either too\nspecific to be generally applicable or so abstract that they neither provide\nconcrete insight into the nature of anomaly types nor facilitate the functional\nevaluation of anomaly detection algorithms. With the recent criticism on 'black\nbox' algorithms and analytics it has become clear that this is an undesirable\nsituation. This paper therefore introduces a general typology of anomalies that\noffers a clear and tangible definition of the different types of anomalies in\ndatasets. The typology also facilitates the evaluation of the functional\ncapabilities of anomaly detection algorithms and as a framework assists in\nanalyzing the conceptual levels of data, patterns and anomalies. Finally, it\nserves as an analytical tool for studying anomaly types from other typologies.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 13:12:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Foorthuis", "Ralph", ""]]}, {"id": "2107.01640", "submitter": "Gamage Dumindu Samaraweera", "authors": "G. Dumindu Samaraweera and J. Morris Chang", "title": "SEC-NoSQL: Towards Implementing High Performance Security-as-a-Service\n  for NoSQL Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the last few years, the explosion of Big Data has prompted cloud\ninfrastructures to provide cloud-based database services as cost effective,\nefficient and scalable solutions to store and process large volume of data.\nHence, NoSQL databases became more and more popular because of their inherent\nfeatures of better performance and high scalability compared to other\nrelational databases. However, with this deployment architecture where the\ninformation is stored in a public cloud, protection against the sensitive data\nis still being a major concern. Since the data owner does not have the full\ncontrol over his sensitive data in a cloud-based database solution, many\norganizations are reluctant to move forward with Database-as-a-Service (DBaaS)\nsolutions. Some of the recent work addressed this issue by introducing\nadditional layers to provide encryption mechanisms to encrypt data, however,\nthese approaches are more application specific and they need to be properly\nevaluated to ensure whether they can achieve high performance with the\nscalability when it comes to large volume of data in a cloud-based production\nenvironment. This paper proposes a practical system design and implementation\nto provide Security-as-a-Service for NoSQL databases (SEC-NoSQL) while\nsupporting the execution of query over encrypted data with guaranteed level of\nsystem performance. Several different models of implementations are proposed,\nand their performance is evaluated using YCSB benchmark considering large\nnumber of clients processing simultaneously. Experimental results show that our\ndesign fits well on encrypted data while maintaining the high performance and\nscalability. Moreover, to deploy our solution as a cloud-based service, a\npractical guide establishing Service Level Agreement (SLA) is also included.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 14:11:40 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Samaraweera", "G. Dumindu", ""], ["Chang", "J. Morris", ""]]}, {"id": "2107.01910", "submitter": "Maria-Esther Vidal", "authors": "Irlan Grangel-Gonzalez and Maria-Esther Vidal", "title": "Analyzing a Knowledge Graph of Industry4.0 Standards", "comments": "Based on the paper Irlan Grangel-Gonzalez, Maria-Esther Vidal:\n  Analyzing a Knowledge Graph of Industry 4.0 Standards. WWW (Companion Volume)\n  2021: 16-25", "journal-ref": null, "doi": "10.1145/3442442.3453541", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we tackle the problem of standard interoperability across\ndifferent standardization frameworks, and devise a knowledge-driven approach\nthat allows for the description of standards and standardization frameworks\ninto an Industry 4.0 knowledge graph (I40KG). The STO ontology represents\nproperties of standards and standardization frameworks, as well as\nrelationships among them. The I40KG integrates more than 200 standards and four\nstandardization frameworks. To populate the I40KG, the landscape of standards\nhas been analyzed from a semantic perspective and the resulting I40KG\nrepresents knowledge expressed in more than 200 industrial related documents\nincluding technical reports, research articles, and white papers. Additionally,\nthe I40KG has been linked to existing knowledge graphs and an automated\nreasoning has been implemented to reveal implicit relations between standards\nas well as mappings across standardization frameworks. We analyze both the\nnumber of discovered relations between standards and the accuracy of these\nrelations. Observed results indicate that both reasoning and linking processes\nenable for increasing the connectivity in the knowledge graph by up to 80%,\nwhilst up to 96% of the relations can be validated. These outcomes suggest that\nintegrating standards and standardization frameworks into the I40KG enables the\nresolution of semantic interoperability conflicts, empowering the communication\nin smart factories.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 10:03:06 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Grangel-Gonzalez", "Irlan", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "2107.01963", "submitter": "Zihao Zhao", "authors": "Zihao Zhao, Zhihong Shen, Mingjie Tang", "title": "PandaDB: Understanding Unstructured Data in Graph Database", "comments": "12pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, graph model is widely used in many applications, such as\nknowledge graph, financial anti-fraud. Unstructured data(such as images,\nvideos, and audios) is under explosive growing. So, queries of unstructured\ndata content on graph are widespread in a rich vein of real-world applications.\nMany graph database systems have started to support unstructured data to meet\nsuch demands. However, queries over structured and unstructured data on graph\nare often treated as separate tasks in most systems. These tasks are executed\non different module of the tools chain. Collaborative queries (i.e., involving\nboth data types) are not yet fully supported.This paper proposes a graph\ndatabase supporting collaborative queries on property graph, named PandaDB. Its\nto fulfill the emerging demands about querying unstructured data on property\ngraph model. PandaDB introduces CypherPlus, a query language which enables the\nusers to express collaborative queries using cypher semantics by introducing\nsub-property and a series of logical operators. PandaDB is built based on\nNeo4j, manage the unstructured data in the format of BLOB. The computable\npattern is proposed to introduce the content of unstructured data into\ncomputation. Moreover, to support the large-scale query, this paper proposes\nthe semantic index, cache and index the extracted computable pattern. The\ncollaborative query on graph is optimized by the min-cost optimization method.\nExperimental results on both public and in-house datasets show the performance\nachieved by PandaDB and its effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 12:13:38 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhao", "Zihao", ""], ["Shen", "Zhihong", ""], ["Tang", "Mingjie", ""]]}, {"id": "2107.01965", "submitter": "Maria-Esther Vidal", "authors": "Valentina Janev, Maria-Esther Vidal, Kemele Endris, Dea Pujic", "title": "Managing Knowledge in Energy Data Spaces", "comments": "Based on the article Valentina Janev, Maria-Esther Vidal, Kemele M.\n  Endris, Dea Pujic: Managing Knowledge in Energy Data Spaces. WWW (Companion\n  Volume) 2021: 7-15", "journal-ref": null, "doi": "10.1145/3442442.3453541", "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the energy domain grows at unprecedented rates and is usually\ngenerated by heterogeneous energy systems. Despite the great potential that big\ndata-driven technologies can bring to the energy sector, general adoption is\nstill lagging. Several challenges related to controlled data exchange and data\nintegration are still not wholly achieved. As a result, fragmented applications\nare developed against energy data silos, and data exchange is limited to few\napplications. In this paper, we analyze the challenges and requirements related\nto energy-related data applications. We also evaluate the use of Energy Data\nEcosystems (EDEs) as data-driven infrastructures to overcome the current\nlimitations of fragmented energy applications. EDEs are inspired by the\nInternational Data Space (IDS) initiative launched in Germany at the end of\n2014 with an overall objective to take both the development and use of the IDS\nreference architecture model to a European/global level. The reference\narchitecture model consists of four architectures related to business,\nsecurity, data and service, and software aspects. This paper illustrates the\napplicability of EDEs and IDS reference architecture in real-world scenarios\nfrom the energy sector. The analyzed scenario is positioned in the context of\nthe EU-funded H2020 project PLATOON.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 12:15:58 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Janev", "Valentina", ""], ["Vidal", "Maria-Esther", ""], ["Endris", "Kemele", ""], ["Pujic", "Dea", ""]]}, {"id": "2107.02482", "submitter": "Petros Kalendralis", "authors": "Matthijs Sloep, Petros Kalendralis, Ananya Choudhury, Lerau Seyben,\n  Jasper Snel, Nibin Moni George, Martijn Veening, Johannes A. Langendijk,\n  Andre Dekker, Johan van Soest, Rianne Fijten", "title": "A Knowledge graph representation of baseline characteristics for the\n  Dutch proton therapy research registry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cancer registries collect multisource data and provide valuable information\nthat can lead to unique research opportunities. In the Netherlands, a registry\nand model-based approach (MBA) are used for the selection of patients that are\neligible for proton therapy. We collected baseline characteristics including\ndemographic, clinical, tumour and treatment information. These data were\ntransformed into a machine readable format using the FAIR (Findable,\nAccessible, Interoperable, Reusable) data principles and resulted in a\nknowledge graph with baseline characteristics of proton therapy patients. With\nthis approach, we enable the possibility of linking external data sources and\noptimal flexibility to easily adapt the data structure of the existing\nknowledge graph to the needs of the clinic.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 08:51:19 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Sloep", "Matthijs", ""], ["Kalendralis", "Petros", ""], ["Choudhury", "Ananya", ""], ["Seyben", "Lerau", ""], ["Snel", "Jasper", ""], ["George", "Nibin Moni", ""], ["Veening", "Martijn", ""], ["Langendijk", "Johannes A.", ""], ["Dekker", "Andre", ""], ["van Soest", "Johan", ""], ["Fijten", "Rianne", ""]]}, {"id": "2107.02573", "submitter": "Francisco Lazaro", "authors": "Francisco L\\'azaro, Bal\\'azs Matuz", "title": "Irregular Invertible Bloom Look-Up Tables", "comments": "Accepted for presentation at ISTC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider invertible Bloom lookup tables (IBLTs) which are probabilistic\ndata structures that allow to store keyvalue pairs. An IBLT supports insertion\nand deletion of key-value pairs, as well as the recovery of all key-value pairs\nthat have been inserted, as long as the number of key-value pairs stored in the\nIBLT does not exceed a certain number. The recovery operation on an IBLT can be\nrepresented as a peeling process on a bipartite graph. We present a density\nevolution analysis of IBLTs which allows to predict the maximum number of\nkey-value pairs that can be inserted in the table so that recovery is still\nsuccessful with high probability. This analysis holds for arbitrary irregular\ndegree distributions and generalizes results in the literature. We complement\nour analysis by numerical simulations of our own IBLT design which allows to\nrecover a larger number of key-value pairs as state-of-the-art IBLTs of same\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:21:50 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["L\u00e1zaro", "Francisco", ""], ["Matuz", "Bal\u00e1zs", ""]]}, {"id": "2107.02885", "submitter": "Yan Zhao", "authors": "Yan Zhao, Imen Megdiche and Franck Ravat", "title": "Data Lake Ingestion Management", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data Lake (DL) is a Big Data analysis solution which ingests raw data in\ntheir native format and allows users to process these data upon usage. Data\ningestion is not a simple copy and paste of data, it is a complicated and\nimportant phase to ensure that ingested data are findable, accessible,\ninteroperable and reusable at all times. Our solution is threefold. Firstly, we\npropose a metadata model that includes information about external data sources,\ndata ingestion processes, ingested data, dataset veracity and dataset security.\nSecondly, we present the algorithms that ensure the ingestion phase (data\nstorage and metadata instanciation). Thirdly, we introduce a developed metadata\nmanagement system whereby users can easily consult different elements stored in\nDL.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:03:53 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zhao", "Yan", ""], ["Megdiche", "Imen", ""], ["Ravat", "Franck", ""]]}, {"id": "2107.03572", "submitter": "Feixiang Wang", "authors": "Feixiang Wang, Yixiang Fang, Yan Song, Shuang Li, Xinyun Chen", "title": "A Note on General Statistics of Publicly Accessible Knowledge Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge bases are prevalent in various domains and have been widely used in\na large number of real applications such as applications in online\nencyclopedia, social media, biomedical fields, bibliographical networks. Due to\ntheir great importance, knowledge bases have received much attention from both\nthe academia and industry community in recent years. In this paper, we provide\na summary of the general statistics of several open-source and publicly\naccessible knowledge bases, ranging from the number of objects, relations to\nthe object types and the relation types. With such statistics, this concise\nnote can not only help researchers form a better and quick understanding of\nexisting open accessible knowledge bases, but can also guide the general\naudience to use the resource effectively when they conduct research with\nknowledge bases.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 03:05:32 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wang", "Feixiang", ""], ["Fang", "Yixiang", ""], ["Song", "Yan", ""], ["Li", "Shuang", ""], ["Chen", "Xinyun", ""]]}, {"id": "2107.03660", "submitter": "Yushan Zhang", "authors": "Yushan Zhang, Peisen Yao, Rongxin Wu, Charles Zhang", "title": "Duplicate-sensitivity Guided Transformation Synthesis for DBMS\n  Correctness Bug Detection", "comments": "11 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database Management System (DBMS) plays a core role in modern software from\nmobile apps to online banking. It is critical that DBMS should provide correct\ndata to all applications. When the DBMS returns incorrect data, a correctness\nbug is triggered. Current production-level DBMSs still suffer from insufficient\ntesting due to the limited hand-written test cases. Recently several works\nproposed to automatically generate many test cases with query transformation, a\nprocess of generating an equivalent query pair and testing a DBMS by checking\nwhether the system returns the same result set for both queries. However, all\nof them still heavily rely on manual work to provide a transformation which\nlargely confines their exploration of the valid input query space.\n  This paper introduces duplicate-sensitivity guided transformation synthesis\nwhich automatically finds new transformations by first synthesizing many\ncandidates then filtering the nonequivalent ones. Our automated synthesis is\nachieved by mutating a query while keeping its duplicate sensitivity, which is\na necessary condition for query equivalence. After candidate synthesis, we keep\nthe mutant query which is equivalent to the given one by using a query\nequivalent checker. Furthermore, we have implemented our idea in a tool Eqsql\nand used it to test the production-level DBMSs. In two months, we detected in\ntotal 30 newly confirmed and unique bugs in MySQL, TiDB and CynosDB.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:47:35 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Zhang", "Yushan", ""], ["Yao", "Peisen", ""], ["Wu", "Rongxin", ""], ["Zhang", "Charles", ""]]}, {"id": "2107.03853", "submitter": "Valentina Janev", "authors": "Valentina Janev", "title": "Semantic Intelligence in Big Data Applications", "comments": "Chapter 4 in the Book: Sarika Jain, San Murugesan (Eds.) The Smarter\n  Web - Technologies and Applications Shaping the Future. Springer Nature\n  Switzerland AG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, data is growing at a tremendous rate and, according to the\nInternational Data Corporation, it is expected to reach 175 zettabytes by 2025.\nThe International Data Corporation also forecasts that more than 150B devices\nwill be connected across the globe by 2025, most of which will be creating data\nin real-time, while 90 zettabytes of data will be created by the Internet of\nThings devices. This vast amount of data creates several new opportunities for\nmodern enterprises, especially for analysing the enterprise value chains in a\nbroader sense. To leverage the potential of real data and build smart\napplications on top of sensory data, IoT-based systems integrate domain\nknowledge and context-relevant information. Semantic Intelligence is the\nprocess of bridging the semantic gap between human and computer comprehension\nby teaching a machine to think in terms of object-oriented concepts in the same\nway as a human does. Semantic intelligence technologies are the most important\ncomponent in developing artificially intelligent knowledge-based systems since\nthey assist machines in contextually and intelligently integrating and\nprocessing resources. This Chapter aims at demystifying semantic intelligence\nin distributed, enterprise and web-based information systems. It also discusses\nprominent tools that leverage semantics, handle large data at scale and address\nchallenges (e.g. heterogeneity, interoperability, machine learning\nexplainability) in different industrial applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 14:09:47 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Janev", "Valentina", ""]]}, {"id": "2107.03937", "submitter": "Wil van der Aalst", "authors": "Wil M.P. van der Aalst and Luis Santos", "title": "May I Take Your Order? On the Interplay Between Time and Order in\n  Process Mining", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Process mining starts from event data. The ordering of events is vital for\nthe discovery of process models. However, the timestamps of events may be\nunreliable or imprecise. To further complicate matters, also causally unrelated\nevents may be ordered in time. The fact that one event is followed by another\ndoes not imply that the former causes the latter. This paper explores the\nrelationship between time and order. Moreover, it describes an approach to\npreprocess event data having timestamp-related problems. This approach avoids\nusing accidental or unreliable orders and timestamps, creates partial orders to\ncapture uncertainty, and allows for exploiting domain knowledge to (re)order\nevents. Optionally, the approach also generates interleavings to be able to use\nexisting process mining techniques that cannot handle partially ordered event\ndata. The approach has been implemented using ProM and can be applied to any\nevent log.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:08:57 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["van der Aalst", "Wil M. P.", ""], ["Santos", "Luis", ""]]}, {"id": "2107.03997", "submitter": "Giacomo Bergami", "authors": "Giacomo Bergami, Fabrizio Maria Maggi, Marco Montali, Rafael\n  Pe\\~naloza", "title": "Probabilistic Trace Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Alignments provide sophisticated diagnostics that pinpoint deviations in a\ntrace with respect to a process model and their severity. However, approaches\nbased on trace alignments use crisp process models as reference and recent\nprobabilistic conformance checking approaches check the degree of conformance\nof an event log with respect to a stochastic process model instead of finding\ntrace alignments. In this paper, for the first time, we provide a conformance\nchecking approach based on trace alignments using stochastic Workflow nets.\nConceptually, this requires to handle the two possibly contrasting forces of\nthe cost of the alignment on the one hand and the likelihood of the model trace\nwith respect to which the alignment is computed on the other.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:42:57 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Bergami", "Giacomo", ""], ["Maggi", "Fabrizio Maria", ""], ["Montali", "Marco", ""], ["Pe\u00f1aloza", "Rafael", ""]]}, {"id": "2107.04027", "submitter": "Jerome Darmont", "authors": "Etienne Scholly (ERIC), Pegdwend\\'e Sawadogo (ERIC), Pengfei Liu\n  (ERIC), Javier Espinosa-Oviedo (ERIC), C\\'ecile Favre (ERIC), Sabine Loudcher\n  (ERIC), J\\'er\\^ome Darmont (ERIC), Camille No\\^us", "title": "goldMEDAL : une nouvelle contribution {\\`a} la mod{\\'e}lisation\n  g{\\'e}n{\\'e}rique des m{\\'e}tadonn{\\'e}es des lacs de donn{\\'e}es", "comments": "in French. 17e journ{\\'e}es Business Intelligence et Big Data (EDA\n  2021), Jul 2021, Toulouse, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We summarize here a paper published in 2021 in the DOLAP international\nworkshop DOLAP associated with the EDBT and ICDT conferences. We propose\ngoldMEDAL, a generic metadata model for data lakes based on four concepts and a\nthree-level modeling: conceptual, logical and physical.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 07:56:27 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Scholly", "Etienne", "", "ERIC"], ["Sawadogo", "Pegdwend\u00e9", "", "ERIC"], ["Liu", "Pengfei", "", "ERIC"], ["Espinosa-Oviedo", "Javier", "", "ERIC"], ["Favre", "C\u00e9cile", "", "ERIC"], ["Loudcher", "Sabine", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["No\u00fbs", "Camille", ""]]}, {"id": "2107.04071", "submitter": "Erich Schubert", "authors": "Erich Schubert", "title": "A Triangle Inequality for Cosine Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is a fundamental problem for many data analysis techniques.\nMany efficient search techniques rely on the triangle inequality of metrics,\nwhich allows pruning parts of the search space based on transitive bounds on\ndistances. Recently, Cosine similarity has become a popular alternative choice\nto the standard Euclidean metric, in particular in the context of textual data\nand neural network embeddings. Unfortunately, Cosine similarity is not metric\nand does not satisfy the standard triangle inequality. Instead, many search\ntechniques for Cosine rely on approximation techniques such as locality\nsensitive hashing. In this paper, we derive a triangle inequality for Cosine\nsimilarity that is suitable for efficient similarity search with many standard\nsearch structures (such as the VP-tree, Cover-tree, and M-tree); show that this\nbound is tight and discuss fast approximations for it. We hope that this spurs\nnew research on accelerating exact similarity search for cosine similarity, and\npossible other similarity measures beyond the existing work for distance\nmetrics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 19:13:34 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Schubert", "Erich", ""]]}, {"id": "2107.04462", "submitter": "Felix I. Stamm", "authors": "Felix I. Stamm, Martin Becker, Markus Strohmaier, Florian Lemmerich", "title": "Redescription Model Mining", "comments": null, "journal-ref": null, "doi": "10.1145/3447548.3467366", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Redescription Model Mining, a novel approach to\nidentify interpretable patterns across two datasets that share only a subset of\nattributes and have no common instances. In particular, Redescription Model\nMining aims to find pairs of describable data subsets -- one for each dataset\n-- that induce similar exceptional models with respect to a prespecified model\nclass. To achieve this, we combine two previously separate research areas:\nExceptional Model Mining and Redescription Mining. For this new problem\nsetting, we develop interestingness measures to select promising patterns,\npropose efficient algorithms, and demonstrate their potential on synthetic and\nreal-world data. Uncovered patterns can hint at common underlying phenomena\nthat manifest themselves across datasets, enabling the discovery of possible\nassociations between (combinations of) attributes that do not appear in the\nsame dataset.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 14:26:00 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Stamm", "Felix I.", ""], ["Becker", "Martin", ""], ["Strohmaier", "Markus", ""], ["Lemmerich", "Florian", ""]]}, {"id": "2107.04553", "submitter": "Immanuel Trummer Mr.", "authors": "Immanuel Trummer", "title": "Can Deep Neural Networks Predict Data Correlations from Column Names?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For humans, it is often possible to predict data correlations from column\nnames. We conduct experiments to find out whether deep neural networks can\nlearn to do the same. If so, e.g., it would open up the possibility of tuning\ntools that use NLP analysis on schema elements to prioritize their efforts for\ncorrelation detection.\n  We analyze correlations for around 120,000 column pairs, taken from around\n4,000 data sets. We try to predict correlations, based on column names alone.\nFor predictions, we exploit pre-trained language models, based on the recently\nproposed Transformer architecture. We consider different types of correlations,\nmultiple prediction methods, and various prediction scenarios. We study the\nimpact of factors such as column name length or the amount of training data on\nprediction accuracy. Altogether, we find that deep neural networks can predict\ncorrelations with a relatively high accuracy in many scenarios (e.g., with an\naccuracy of 95% for long column names).\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:11:54 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Trummer", "Immanuel", ""]]}, {"id": "2107.04891", "submitter": "S{\\l}awek Staworko", "authors": "Beno\\^it Groz, Aur\\'elien Lemay, S{\\l}awek Staworko, Piotr Wieczorek", "title": "Inference of Shape Expression Schemas Typed RDF Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing a Shape Expression Schema (ShEx) that\ndescribes the structure of a given input RDF graph. We employ the framework of\ngrammatical inference, where the objective is to find an inference algorithm\nthat is both sound i.e., always producing a schema that validates the input RDF\ngraph, and complete i.e., able to produce any schema, within a given class of\nschemas, provided that a sufficiently informative input graph is presented. We\nstudy the case where the input graph is typed i.e., every node is given with\nits types. We limit our attention to a practical fragment ShEx0 of Shape\nExpressions Schemas that has an equivalent graphical representation in the form\nof shape graphs. We investigate the problem of constructing a canonical\nrepresentative of a given shape graph. Finally, we present a sound and complete\nalgorithm for shape graphs thus showing that ShEx0 is learnable from typed\ngraphs.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 19:05:23 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Groz", "Beno\u00eet", ""], ["Lemay", "Aur\u00e9lien", ""], ["Staworko", "S\u0142awek", ""], ["Wieczorek", "Piotr", ""]]}, {"id": "2107.04922", "submitter": "Sepanta Zeighami", "authors": "Sepanta Zeighami, Cyrus Shahabi", "title": "NeuroDB: A Neural Network Framework for Answering Range Aggregate\n  Queries and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Range aggregate queries (RAQs) are an integral part of many real-world\napplications, where, often, fast and approximate answers for the queries are\ndesired. Recent work has studied answering RAQs using machine learning models,\nwhere a model of the data is learned to answer the queries. However, such\nmodelling choices fail to utilize any query specific information. To capture\nsuch information, we observe that RAQs can be represented by query functions,\nwhich are functions that take a query instance (i.e., a specific RAQ) as an\ninput and output its corresponding answer. Using this representation, we\nformulate the problem of learning to approximate the query function, and\npropose NeuroDB, a query specialized neural network framework, that answers\nRAQs efficiently. NeuroDB is query-type agnostic (i.e., it does not make any\nassumption about the underlying query type) and our observation that queries\ncan be represented by functions is not specific to RAQs. Thus, we investigate\nwhether NeuroDB can be used for other query types, by applying it to distance\nto nearest neighbour queries. We experimentally show that NeuroDB outperforms\nthe state-of-the-art for this query type, often by orders of magnitude.\nMoreover, the same neural network architecture as for RAQs is used, bringing to\nlight the possibility of using a generic framework to answer any query type\nefficiently.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 22:36:53 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zeighami", "Sepanta", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "2107.05369", "submitter": "Carsten Lutz", "authors": "Anneke Haga and Carsten Lutz and Leif Sabellek and Frank Wolter", "title": "How to Approximate Ontology-Mediated Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce and study several notions of approximation for ontology-mediated\nqueries based on the description logics ALC and ALCI. Our approximations are of\ntwo kinds: we may (1) replace the ontology with one formulated in a tractable\nontology language such as ELI or certain TGDs and (2) replace the database with\none from a tractable class such as the class of databases whose treewidth is\nbounded by a constant. We determine the computational complexity and the\nrelative completeness of the resulting approximations. (Almost) all of them\nreduce the data complexity from coNP-complete to PTime, in some cases even to\nfixed-parameter tractable and to linear time. While approximations of kind (1)\nalso reduce the combined complexity, this tends to not be the case for\napproximations of kind (2). In some cases, the combined complexity even\nincreases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 12:29:50 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Haga", "Anneke", ""], ["Lutz", "Carsten", ""], ["Sabellek", "Leif", ""], ["Wolter", "Frank", ""]]}, {"id": "2107.05537", "submitter": "Bolong Zheng", "authors": "Bolong Zheng, Xi Zhao, Lianggui Weng, Nguyen Quoc Viet Hung, Hang Liu,\n  Christian S. Jensen", "title": "PM-LSH: a fast and accurate in-memory framework for high-dimensional\n  approximate NN and closest pair search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor (NN) search is inherently computationally expensive in\nhigh-dimensional spaces due to the curse of dimensionality. As a well-known\nsolution, locality-sensitive hashing (LSH) is able to answer c-approximate NN\n(c-ANN) queries in sublinear time with constant probability. Existing LSH\nmethods focus mainly on building hash bucket-based indexing such that the\ncandidate points can be retrieved quickly. However, existing coarse-grained\nstructures fail to offer accurate distance estimation for candidate points,\nwhich translates into additional computational overhead when having to examine\nunnecessary points. This in turn reduces the performance of query processing.\nIn contrast, we propose a fast and accurate in-memory LSH framework, called\nPM-LSH, that aims to compute c-ANN queries on large-scale, high-dimensional\ndatasets. First, we adopt a simple yet effective PM-tree to index the data\npoints. Second, we develop a tunable confidence interval to achieve accurate\ndistance estimation and guarantee high result quality. Third, we propose an\nefficient algorithm on top of the PM-tree to improve the performance of\ncomputing c-ANN queries. In addition, we extend PM-LSH to support closest pair\n(CP) search in high-dimensional spaces. We again adopt the PM-tree to organize\nthe points in a lowdimensional space, and we propose a branch and bound\nalgorithm together with a radius pruning technique to improve the performance\nof computing c-approximate closest pair (c-ACP) queries. Extensive experiments\nwith real-world data offer evidence that PM-LSH is capable of outperforming\nexisting proposals with respect to both efficiency and accuracy for both NN and\nCP search.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 05:54:15 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zheng", "Bolong", ""], ["Zhao", "Xi", ""], ["Weng", "Lianggui", ""], ["Hung", "Nguyen Quoc Viet", ""], ["Liu", "Hang", ""], ["Jensen", "Christian S.", ""]]}, {"id": "2107.05566", "submitter": "Philipp Seifer", "authors": "Philipp Seifer, Ralf L\\\"ammel, Steffen Staab", "title": "ProGS: Property Graph Shapes Language (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property graphs constitute data models for representing knowledge graphs.\nThey allow for the convenient representation of facts, including facts about\nfacts, represented by triples in subject or object position of other triples.\nKnowledge graphs such as Wikidata are created by a diversity of contributors\nand a range of sources leaving them prone to two types of errors. The first\ntype of error, falsity of facts, is addressed by property graphs through the\nrepresentation of provenance and validity, making triples occur as first-order\nobjects in subject position of metadata triples. The second type of error,\nviolation of domain constraints, has not been addressed with regard to property\ngraphs so far. In RDF representations, this error can be addressed by shape\nlanguages such as SHACL or ShEx, which allow for checking whether graphs are\nvalid with respect to a set of domain constraints. Borrowing ideas from the\nsyntax and semantics definitions of SHACL, we design a shape language for\nproperty graphs, ProGS, which allows for formulating shape constraints on\nproperty graphs including their specific constructs, such as edges with\nidentities and key-value annotations to both nodes and edges. We define a\nformal semantics of ProGS, investigate the resulting complexity of validating\nproperty graphs against sets of ProGS shapes, compare with corresponding\nresults for SHACL, and implement a prototypical validator that utilizes answer\nset programming.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:44:21 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Seifer", "Philipp", ""], ["L\u00e4mmel", "Ralf", ""], ["Staab", "Steffen", ""]]}, {"id": "2107.06054", "submitter": "Maxime Buron", "authors": "Maxime Buron, Marie-Laure Mugnier, Micha\\\"el Thomazo", "title": "Parallelisable Existential Rules: a Story of Pieces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider existential rules, an expressive formalism well\nsuited to the representation of ontological knowledge and data-to-ontology\nmappings in the context of ontology-based data integration. The chase is a\nfundamental tool to do reasoning with existential rules as it computes all the\nfacts entailed by the rules from a database instance. We introduce\nparallelisable sets of existential rules, for which the chase can be computed\nin a single breadth-first step from any instance. The question we investigate\nis the characterization of such rule sets. We show that parallelisable rule\nsets are exactly those rule sets both bounded for the chase and belonging to a\nnovel class of rules, called pieceful. The pieceful class includes in\nparticular frontier-guarded existential rules and (plain) datalog. We also give\nanother characterization of parallelisable rule sets in terms of rule\ncomposition based on rewriting.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 13:09:14 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Buron", "Maxime", ""], ["Mugnier", "Marie-Laure", ""], ["Thomazo", "Micha\u00ebl", ""]]}, {"id": "2107.06065", "submitter": "Dirk Riehle", "authors": "Dirk Riehle, Nikolay Harutyunyan, Ann Barcomb", "title": "Pattern Discovery and Validation Using Scientific Research Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pattern discovery, the process of discovering previously unrecognized\npatterns, is often performed as an ad-hoc process with little resulting\ncertainty in the quality of the proposed patterns. Pattern validation, the\nprocess of validating the accuracy of proposed patterns, remains dominated by\nthe simple heuristic of \"the rule of three\". This article shows how to use\nestablished scientific research methods for the purpose of pattern discovery\nand validation. We present a specific approach, called the handbook method,\nthat uses the qualitative survey, action research, and case study research for\npattern discovery and evaluation, and we discuss the underlying principle of\nusing scientific methods in general. We evaluate the handbook method using\nthree exploratory studies and demonstrate its usefulness.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 16:11:56 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Riehle", "Dirk", ""], ["Harutyunyan", "Nikolay", ""], ["Barcomb", "Ann", ""]]}, {"id": "2107.06139", "submitter": "Jacques Chabin", "authors": "Jacques Chabin and Mirian Halfeld-Ferrari and B\\'eatrice Markhoff and\n  Thanh Binh Nguyen", "title": "Querying Linked Data: how to ensure user's quality requirements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the distributed and dynamic framework of the Web, data quality is a big\nchallenge. The Linked Open Data (LOD) provides an enormous amount of data, the\nquality of which is difficult to control. Quality is intrinsically a matter of\nusage, so consumers need ways to specify quality rules that make sense for\ntheir use, in order to get only data conforming to these rules. We propose a\nuser-side query framework equipped with a checker of constraints and confidence\nlevels on data resulting from LOD providers\\' query evaluations. We detail its\ntheoretical foundations and we provide experimental results showing that the\ncheck additional cost is reasonable and that integrating the constraints in the\nqueries further improves it significantly.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:45:40 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Chabin", "Jacques", ""], ["Halfeld-Ferrari", "Mirian", ""], ["Markhoff", "B\u00e9atrice", ""], ["Nguyen", "Thanh Binh", ""]]}, {"id": "2107.06817", "submitter": "Michael Leybovich", "authors": "Michael Leybovich and Oded Shmueli", "title": "Efficient Set of Vectors Search", "comments": "6 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider a similarity measure between two sets $A$ and $B$ of vectors,\nthat balances the average and maximum cosine distance between pairs of vectors,\none from set $A$ and one from set $B$. As a motivation for this measure, we\npresent lineage tracking in a database. To practically realize this measure, we\nneed an approximate search algorithm that given a set of vectors $A$ and sets\nof vectors $B_1,...,B_n$, the algorithm quickly locates the set $B_i$ that\nmaximizes the similarity measure. For the case where all sets are singleton\nsets, essentially each is a single vector, there are known efficient\napproximate search algorithms, e.g., approximated versions of tree search\nalgorithms, locality-sensitive hashing (LSH), vector quantization (VQ) and\nproximity graph algorithms. In this work, we present approximate search\nalgorithms for the general case. The underlying idea in these algorithms is\nencoding a set of vectors via a \"long\" single vector.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:22:20 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Leybovich", "Michael", ""], ["Shmueli", "Oded", ""]]}, {"id": "2107.06835", "submitter": "Ripon Patgiri", "authors": "Sabuzima Nayak, Ripon Patgiri, Lilapati Waikhom, Arif Ahmed", "title": "A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises,\n  Future Directions, and Applications", "comments": "Submitted to Elsevier for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edge technology aims to bring Cloud resources (specifically, the compute,\nstorage, and network) to the closed proximity of the Edge devices, i.e., smart\ndevices where the data are produced and consumed. Embedding computing and\napplication in Edge devices lead to emerging of two new concepts in Edge\ntechnology, namely, Edge computing and Edge analytics. Edge analytics uses some\ntechniques or algorithms to analyze the data generated by the Edge devices.\nWith the emerging of Edge analytics, the Edge devices have become a complete\nset. Currently, Edge analytics is unable to provide full support for the\nexecution of the analytic techniques. The Edge devices cannot execute advanced\nand sophisticated analytic algorithms following various constraints such as\nlimited power supply, small memory size, limited resources, etc. This article\naims to provide a detailed discussion on Edge analytics. A clear explanation to\ndistinguish between the three concepts of Edge technology, namely, Edge\ndevices, Edge computing, and Edge analytics, along with their issues.\nFurthermore, the article discusses the implementation of Edge analytics to\nsolve many problems in various areas such as retail, agriculture, industry, and\nhealthcare. In addition, the research papers of the state-of-the-art edge\nanalytics are rigorously reviewed in this article to explore the existing\nissues, emerging challenges, research opportunities and their directions, and\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 21:48:20 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Nayak", "Sabuzima", ""], ["Patgiri", "Ripon", ""], ["Waikhom", "Lilapati", ""], ["Ahmed", "Arif", ""]]}, {"id": "2107.07304", "submitter": "Javier Lopez-Gomez", "authors": "Javier L\\'opez-G\\'omez and Jakob Blomer", "title": "Exploring Object Stores for High-Energy Physics Data Storage", "comments": "Accepted for Proceedings of 25th International Conference on\n  Computing in High-Energy and Nuclear Physics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, ROOT TTree has been used for storing over one\nexabyte of High-Energy Physics (HEP) events. The TTree columnar on-disk layout\nhas been proved to be ideal for analyses of HEP data that typically require\naccess to many events, but only a subset of the information stored for each of\nthem. Future colliders, and particularly HL-LHC, will bring an increase of at\nleast one order of magnitude in the volume of generated data. Therefore, the\nuse of modern storage hardware, such as low-latency high-bandwidth NVMe devices\nand distributed object stores, becomes more important. However, TTree was not\ndesigned to optimally exploit modern hardware and may become a bottleneck for\ndata retrieval. The ROOT RNTuple I/O system aims at overcoming TTree's\nlimitations and at providing improved efficiency for modern storage systems. In\nthis paper, we extend RNTuple with a backend that uses Intel DAOS as the\nunderlying storage, demonstrating that the RNTuple architecture can accommodate\nhigh-performance object stores. From the user perspective, data can be accessed\nwith minimal changes to the code, that is by replacing a filesystem path by a\nDAOS URI. Our performance evaluation shows that the new backend can be used for\nrealistic analyses, while outperforming the compatibility solution provided by\nthe DAOS project.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:14:21 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["L\u00f3pez-G\u00f3mez", "Javier", ""], ["Blomer", "Jakob", ""]]}, {"id": "2107.07811", "submitter": "David Carral", "authors": "Camille Bourgaux and David Carral and Markus Kr\\\"otzsch and Sebastian\n  Rudolph and Micha\\\"el Thomazo", "title": "Capturing Homomorphism-Closed Decidable Queries with Existential Rules", "comments": "Technical Report of our KR 2021 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existential rules are a very popular ontology-mediated query language for\nwhich the chase represents a generic computational approach for query\nanswering. It is straightforward that existential rule queries exhibiting chase\ntermination are decidable and can only recognize properties that are preserved\nunder homomorphisms. In this paper, we show the converse: every decidable query\nthat is closed under homomorphism can be expressed by an existential rule set\nfor which the standard chase universally terminates. Membership in this\nfragment is not decidable, but we show via a diagonalisation argument that this\nis unavoidable.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 10:41:05 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Bourgaux", "Camille", ""], ["Carral", "David", ""], ["Kr\u00f6tzsch", "Markus", ""], ["Rudolph", "Sebastian", ""], ["Thomazo", "Micha\u00ebl", ""]]}, {"id": "2107.08203", "submitter": "Yiru Chen", "authors": "Yiru Chen and Eugene Wu", "title": "PI2: Generating Visual Analysis Interfaces From Queries", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive visual analysis interfaces are critical in nearly every data\ntask. Yet creating new interfaces is deeply challenging, as it requires the\ndeveloper to understand the queries needed to express the desired analysis\ntask, design the appropriate interface to express those queries for the task,\nand implement the interface using a combination of visualization, browser,\nserver, and database technologies. Although prior work generates a set of\ninteractive widgets that can express an input query log, this paper presents\nPI2, the first system to generate fully functional visual analysis interfaces\nfrom an example sequence of analysis queries. PI2 analyzes queries\nsyntactically, and represents a set of queries using a novel Difftree structure\nthat encodes systematic variations between query abstract syntax trees. PI2\nthen maps each Difftree to a visualization that renders its results, the\nvariations in each Difftree to interactions, and generates a good layout for\nthe interface. We show that PI2 can express data-oriented interactions in\nexisting visualization interaction taxonomies, can reproduce or improve several\nreal-world visual analysis interfaces, generates interfaces in 2-19s (median\n6s), and scales linearly with the number of queries.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 08:41:00 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chen", "Yiru", ""], ["Wu", "Eugene", ""]]}, {"id": "2107.08297", "submitter": "Tin Vu", "authors": "Tin Vu, Sara Migliorini, Ahmed Eldawy, Alberto Belussi", "title": "Spatial Data Generators", "comments": "1st ACM SIGSPATIAL International Workshop on Spatial Gems\n  (SpatialGems 2019), 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This gem describes a standard method for generating synthetic spatial data\nthat can be used in benchmarking and scalability tests. The goal is to improve\nthe reproducibility and increase the trust in experiments on synthetic data by\nusing standard widely acceptable dataset distributions. In addition, this\narticle describes how to assign a unique identifier to each synthetic dataset\nthat can be shared in papers for reproducibility of results. Finally, this gem\nprovides a supplementary material that gives a reference implementation for all\nthe provided distributions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 18:08:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Vu", "Tin", ""], ["Migliorini", "Sara", ""], ["Eldawy", "Ahmed", ""], ["Belussi", "Alberto", ""]]}, {"id": "2107.08588", "submitter": "Yinjun Wu", "authors": "Yinjun Wu, James Weimer, Susan B. Davidson", "title": "CHEF: A Cheap and Fast Pipeline for Iteratively Cleaning Label\n  Uncertainties (Technical Report)", "comments": "Accepted by VLDB 2021", "journal-ref": null, "doi": "10.14778/3476249.3476290", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality labels are expensive to obtain for many machine learning tasks,\nsuch as medical image classification tasks. Therefore, probabilistic (weak)\nlabels produced by weak supervision tools are used to seed a process in which\ninfluential samples with weak labels are identified and cleaned by several\nhuman annotators to improve the model performance. To lower the overall cost\nand computational overhead of this process, we propose a solution called CHEF\n(CHEap and Fast label cleaning), which consists of the following three\ncomponents. First, to reduce the cost of human annotators, we use Infl, which\nprioritizes the most influential training samples for cleaning and provides\ncleaned labels to save the cost of one human annotator. Second, to accelerate\nthe sample selector phase and the model constructor phase, we use Increm-Infl\nto incrementally produce influential samples, and DeltaGrad-L to incrementally\nupdate the model. Third, we redesign the typical label cleaning pipeline so\nthat human annotators iteratively clean smaller batch of samples rather than\none big batch of samples. This yields better over all model performance and\nenables possible early termination when the expected model performance has been\nachieved. Extensive experiments show that our approach gives good model\nprediction performance while achieving significant speed-ups.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 02:42:35 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 20:39:53 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wu", "Yinjun", ""], ["Weimer", "James", ""], ["Davidson", "Susan B.", ""]]}, {"id": "2107.08594", "submitter": "Rathijit Sen", "authors": "Anish Pimpley, Shuo Li, Anubha Srivastava, Vishal Rohra, Yi Zhu,\n  Soundararajan Srinivasan, Alekh Jindal, Hiren Patel, Shi Qiao, Rathijit Sen", "title": "Optimal Resource Allocation for Serverless Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Optimizing resource allocation for analytical workloads is vital for reducing\ncosts of cloud-data services. At the same time, it is incredibly hard for users\nto allocate resources per query in serverless processing systems, and they\nfrequently misallocate by orders of magnitude. Unfortunately, prior work\nfocused on predicting peak allocation while ignoring aggressive trade-offs\nbetween resource allocation and run-time. Additionally, these methods fail to\npredict allocation for queries that have not been observed in the past. In this\npaper, we tackle both these problems. We introduce a system for optimal\nresource allocation that can predict performance with aggressive trade-offs,\nfor both new and past observed queries. We introduce the notion of a\nperformance characteristic curve (PCC) as a parameterized representation that\ncan compactly capture the relationship between resources and performance. To\ntackle training data sparsity, we introduce a novel data augmentation technique\nto efficiently synthesize the entire PCC using a single run of the query.\nLastly, we demonstrate the advantages of a constrained loss function coupled\nwith GNNs, over traditional ML methods, for capturing the domain specific\nbehavior through an extensive experimental evaluation over SCOPE big data\nworkloads at Microsoft.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 02:55:48 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Pimpley", "Anish", ""], ["Li", "Shuo", ""], ["Srivastava", "Anubha", ""], ["Rohra", "Vishal", ""], ["Zhu", "Yi", ""], ["Srinivasan", "Soundararajan", ""], ["Jindal", "Alekh", ""], ["Patel", "Hiren", ""], ["Qiao", "Shi", ""], ["Sen", "Rathijit", ""]]}, {"id": "2107.08662", "submitter": "Peng Cheng", "authors": "Peng Cheng, Jiabao Jin, Lei Chen, Xuemin Lin, Libin Zheng", "title": "A Queueing-Theoretic Framework for Vehicle Dispatching in Dynamic\n  Car-Hailing [technical report]", "comments": "15 pages", "journal-ref": null, "doi": "10.14778/3476249.3476271", "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of smart mobile devices, the car-hailing platforms\n(e.g., Uber or Lyft) have attracted much attention from both the academia and\nthe industry. In this paper, we consider an important dynamic car-hailing\nproblem, namely \\textit{maximum revenue vehicle dispatching} (MRVD), in which\nrider requests dynamically arrive and drivers need to serve as many riders as\npossible such that the entire revenue of the platform is maximized. We prove\nthat the MRVD problem is NP-hard and intractable. In addition, the dynamic\ncar-hailing platforms have no information of the future riders, which makes the\nproblem even harder. To handle the MRVD problem, we propose a queueing-based\nvehicle dispatching framework, which first uses existing machine learning\nalgorithms to predict the future vehicle demand of each region, then estimates\nthe idle time periods of drivers through a queueing model for each region. With\nthe information of the predicted vehicle demands and estimated idle time\nperiods of drivers, we propose two batch-based vehicle dispatching algorithms\nto efficiently assign suitable drivers to riders such that the expected overall\nrevenue of the platform is maximized during each batch processing. Through\nextensive experiments, we demonstrate the efficiency and effectiveness of our\nproposed approaches over both real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 07:51:31 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 12:45:49 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Cheng", "Peng", ""], ["Jin", "Jiabao", ""], ["Chen", "Lei", ""], ["Lin", "Xuemin", ""], ["Zheng", "Libin", ""]]}, {"id": "2107.08677", "submitter": "Mohamed-Amine Baazizi", "authors": "Mohamed-Amine Baazizi, Dario Colazzo, Giorgio Ghelli, Carlo Sartiani,\n  Stefanie Scherzinger", "title": "An Empirical Study on the \"Usage of Not\" in Real-World JSON Schema\n  Documents (Long Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the usage of negation in JSON Schema data modeling.\nNegation is a logical operator that is rarely present in type systems and\nschema description languages, since it complicates decision problems. As a\nconsequence, many software tools, but also formal frameworks for working with\nJSON Schema, do not fully support negation. As of today, the question whether\ncovering negation is practically relevant, or a mainly theoretical exercise\n(albeit challenging), is open. This motivates us to study whether negation is\nreally used in practice, for which aims, and whether it could be - in principle\n- replaced by simpler operators. We have collected the most diverse corpus of\nJSON Schema documents analyzed so far, based on a crawl of 90k open source\nschemas hosted on GitHub. We perform a systematic analysis, quantify usage\npatterns of negation, and also qualitatively analyze schemas. We show that\nnegation is indeed used, following a stable set of patterns, with the potential\nto mature into design patterns.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 08:28:00 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Baazizi", "Mohamed-Amine", ""], ["Colazzo", "Dario", ""], ["Ghelli", "Giorgio", ""], ["Sartiani", "Carlo", ""], ["Scherzinger", "Stefanie", ""]]}, {"id": "2107.08706", "submitter": "Kangfei Zhao", "authors": "Kangfei Zhao, Jeffrey Xu Yu, Zongyan He, Hao Zhang", "title": "Uncertainty-aware Cardinality Estimation by Neural Network Gaussian\n  Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning (DL) has achieved great success in many real applications.\nDespite its success, there are some main problems when deploying advanced DL\nmodels in database systems, such as hyper-parameters tuning, the risk of\noverfitting, and lack of prediction uncertainty. In this paper, we study\ncardinality estimation for SQL queries with a focus on uncertainty, which we\nbelieve is important in database systems when dealing with a large number of\nuser queries on various applications. With uncertainty ensured, instead of\ntrusting an estimator learned as it is, a query optimizer can explore other\noptions when the estimator learned has a large variance, and it also becomes\npossible to update the estimator to improve its prediction in areas with high\nuncertainty. The approach we explore is different from the direction of\ndeploying sophisticated DL models in database systems to build cardinality\nestimators. We employ Bayesian deep learning (BDL), which serves as a bridge\nbetween Bayesian inference and deep learning.The prediction distribution by BDL\nprovides principled uncertainty calibration for the prediction. In addition,\nwhen the network width of a BDL model goes to infinity, the model performs\nequivalent to Gaussian Process (GP). This special class of BDL, known as Neural\nNetwork Gaussian Process (NNGP), inherits the advantages of Bayesian approach\nwhile keeping universal approximation of neural network, and can utilize a much\nlarger model space to model distribution-free data as a nonparametric model. We\nshow that our uncertainty-aware NNGP estimator achieves high accuracy, can be\nbuilt very fast, and is robust to query workload shift, in our extensive\nperformance studies by comparing with the existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:30:52 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhao", "Kangfei", ""], ["Yu", "Jeffrey Xu", ""], ["He", "Zongyan", ""], ["Zhang", "Hao", ""]]}, {"id": "2107.08814", "submitter": "Abdullatif Baba", "authors": "Shadi Al Shehabi and Abdullatif Baba", "title": "MARC: Mining Association Rules from datasets by using Clustering models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Association rules are useful to discover relationships, which are mostly\nhidden, between the different items in large datasets. Symbolic models are the\nprincipal tools to extract association rules. This basic technique is\ntime-consuming, and it generates a big number of associated rules. To overcome\nthis drawback, we suggest a new method, called MARC, to extract the more\nimportant association rules of two important levels: Type I, and Type II. This\napproach relies on a multi-topographic unsupervised neural network model as\nwell as clustering quality measures that evaluate the success of a given\nnumerical classification model to behave as a natural symbolic model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 06:28:42 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Shehabi", "Shadi Al", ""], ["Baba", "Abdullatif", ""]]}, {"id": "2107.09110", "submitter": "Abhinav Mishra", "authors": "Abhinav Mishra, Ram Sriharsha, Sichen Zhong", "title": "OnlineSTL: Scaling Time Series Decomposition by 100x", "comments": "8 pages, 9 figures, vldb submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decomposing a complex time series into trend, seasonality, and remainder\ncomponents is an important primitive that facilitates time series anomaly\ndetection, change point detection and forecasting. Although numerous batch\nalgorithms are known for time series decomposition, none operate well in an\nonline scalable setting where high throughput and real-time response are\nparamount. In this paper, we propose OnlineSTL, a novel online algorithm for\ntime series decomposition which solves the scalability problem and is deployed\nfor real-time metrics monitoring on high resolution, high ingest rate data.\nExperiments on different synthetic and real world time series datasets\ndemonstrate that OnlineSTL achieves orders of magnitude speedups while\nmaintaining quality of decomposition.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 19:03:27 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Mishra", "Abhinav", ""], ["Sriharsha", "Ram", ""], ["Zhong", "Sichen", ""]]}, {"id": "2107.09351", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu, Yanzhe An, Yuan Zi, Yu Feng, Jianmin Wang", "title": "IoTDataBench: Extending TPCx-IoT for Compression and Scalability", "comments": "16 pages, 7 figures, TPCTC accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a record-breaking result and lessons learned in practicing\nTPCx-IoT benchmarking for a real-world use case. We find that more system\ncharacteristics need to be benchmarked for its application to real-world use\ncases. We introduce an extension to the TPCx-IoT benchmark, covering\nfundamental requirements of time-series data management for IoT infrastructure.\nWe characterize them as data compression and system scalability. To evaluate\nthese two important features of IoT databases, we propose IoTDataBench and\nupdate four aspects of TPCx-IoT, i.e., data generation, workloads, metrics and\ntest procedures. Preliminary evaluation results show systems that fail to\neffectively compress data or flexibly scale can negatively affect the\nredesigned metrics, while systems with high compression ratios and linear\nscalability are rewarded in the final metrics. Such systems have the ability to\nscale up computing resources on demand and can thus save dollar costs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:13:08 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Zhu", "Yuqing", ""], ["An", "Yanzhe", ""], ["Zi", "Yuan", ""], ["Feng", "Yu", ""], ["Wang", "Jianmin", ""]]}, {"id": "2107.09480", "submitter": "Giosu\\'e Lo Bosco", "authors": "Domenico Amato and Raffaele Giancarlo and Giosu\\`e Lo Bosco", "title": "Learned Sorted Table Search and Static Indexes in Small Space:\n  Methodological and Practical Insights via an Experimental Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sorted Table Search Procedures are the quintessential query-answering tool,\nstill very useful, e.g, Search Engines (Google Chrome). Speeding them up, in\nsmall additional space with respect to the table being searched into, is still\na quite significant achievement. Static Learned Indexes have been very\nsuccessful in achieving such a speed-up, but leave open a major question: To\nwhat extent one can enjoy the speed-up of Learned Indexes while using constant\nor nearly constant additional space. By generalizing the experimental\nmethodology of a recent benchmarking study on Learned Indexes, we shed light on\nthis question, by considering two scenarios. The first, quite elementary, i.e.,\ntextbook code, and the second using advanced Learned Indexing algorithms and\nthe supporting sophisticated software platforms. Although in both cases one\nwould expect a positive answer, its achievement is not as simple as it seems.\nIndeed, our extensive set of experiments reveal a complex relationship between\nquery time and model space. The findings regarding this relationship and the\ncorresponding quantitative estimates, across memory levels, can be of interest\nto algorithm designers and of use to practitioners as well. As an essential\npart of our research, we introduce two new models that are of interest in their\nown right. The first is a constant space model that can be seen as a\ngeneralization of $k$-ary search, while the second is a synoptic {\\bf RMI}, in\nwhich we can control model space usage.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:06:55 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 13:56:52 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 13:18:02 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Amato", "Domenico", ""], ["Giancarlo", "Raffaele", ""], ["Bosco", "Giosu\u00e8 Lo", ""]]}, {"id": "2107.09558", "submitter": "Hieu Tran", "authors": "Hieu Tran, Son Nguyen, I-Ling Yen, Farokh Bastani", "title": "Into Summarization Techniques for IoT Data Discovery Routing", "comments": "10 pages, 8 figures", "journal-ref": "IEEE International Conference on Cloud Computing 2021 (IEEE CLOUD\n  2021)", "doi": null, "report-no": null, "categories": "cs.NI cs.DB cs.DC cs.IR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the IoT data discovery problem in very large and\ngrowing scale networks. Specifically, we investigate in depth the routing table\nsummarization techniques to support effective and space-efficient IoT data\ndiscovery routing. Novel summarization algorithms, including alphabetical\nbased, hash based, and meaning based summarization and their corresponding\ncoding schemes are proposed. The issue of potentially misleading routing due to\nsummarization is also investigated. Subsequently, we analyze the strategy of\nwhen to summarize in order to balance the tradeoff between the routing table\ncompression rate and the chance of causing misleading routing. For experimental\nstudy, we have collected 100K IoT data streams from various IoT databases as\nthe input dataset. Experimental results show that our summarization solution\ncan reduce the routing table size by 20 to 30 folds with 2-5% increase in\nlatency when compared with similar peer-to-peer discovery routing algorithms\nwithout summarization. Also, our approach outperforms DHT based approaches by 2\nto 6 folds in terms of latency and traffic.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 15:22:16 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 14:52:47 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Tran", "Hieu", ""], ["Nguyen", "Son", ""], ["Yen", "I-Ling", ""], ["Bastani", "Farokh", ""]]}, {"id": "2107.09592", "submitter": "Fritz Laux", "authors": "Fritz Laux, Malcolm Crowe", "title": "Information Integration using the Typed Graph Model", "comments": "8 pages, 8 figures, DBKDA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Schema and data integration have been a challenge for more than 40 years.\nWhile data warehouse technologies are quite a success story, there is still a\nlack of information integration methods, especially if the data sources are\nbased on different data models or do not have a schema. Enterprise Information\nIntegration has to deal with heterogeneous data sources and requires up-to-date\nhigh-quality information to provide a reliable basis for analysis and decision\nmaking. The paper proposes virtual integration using the Typed Graph Model to\nsupport schema mediation. The integration process first converts the structure\nof each source into a typed graph schema, which is then matched to the mediated\nschema. Mapping rules define transformations between the schemata to reconcile\nsemantics. The mapping can be visually validated by experts. It provides\nindicators and rules to achieve a consistent schema mapping, which leads to\nhigh data integrity and quality.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 16:09:52 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Laux", "Fritz", ""], ["Crowe", "Malcolm", ""]]}, {"id": "2107.09886", "submitter": "Quang Minh Nguyen", "authors": "Minh Quang Nguyen, Dumitrel Loghin, Tien Tuan Anh Dinh", "title": "Understanding the Scalability of Hyperledger Fabric", "comments": "10 pages, BCDL 2019 in conjunction with ACM VLDB. Los Angeles, USA,\n  26-30 Aug 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid growth of blockchain systems leads to increasing interest in\nunderstanding and comparing blockchain performance at scale. In this paper, we\nfocus on analyzing the performance of Hyperledger Fabric v1.1 - one of the most\npopular permissioned blockchain systems. Prior works have analyzed Hyperledger\nFabric v0.6 in depth, but newer versions of the system undergo significant\nchanges that warrant new analysis. Existing works on benchmarking the system\nare limited in their scope: some consider only small networks, others consider\nscalability of only parts of the system instead of the whole. We perform a\ncomprehensive performance analysis of Hyperledger Fabric v1.1 at scale. We\nextend an existing benchmarking tool to conduct experiments over many servers\nwhile scaling all important components of the system. Our results demonstrate\nthat Fabric v1.1's scalability bottlenecks lie in the communication overhead\nbetween the execution and ordering phase. Furthermore, we show that scaling the\nKafka cluster that is used for the ordering phase does not affect the overall\nthroughput.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 05:57:31 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Nguyen", "Minh Quang", ""], ["Loghin", "Dumitrel", ""], ["Dinh", "Tien Tuan Anh", ""]]}, {"id": "2107.09952", "submitter": "Huey Eng Chua", "authors": "Zifeng Yuan, Huey Eng Chua, Sourav S Bhowmick, Zekun Ye, Wook-Shin\n  Han, Byron Choi", "title": "Towards Plug-and-Play Visual Graph Query Interfaces: Data-driven Canned\n  Pattern Selection for Large Networks", "comments": "19 pages, 26 figures, 3 tables, VLDB conference", "journal-ref": null, "doi": "10.14778/3476249.3476256", "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canned patterns (i.e. small subgraph patterns) in visual graph query\ninterfaces (a.k.a GUI) facilitate efficient query formulation by enabling\npattern-at-a-time construction mode. However, existing GUIs for querying large\nnetworks either do not expose any canned patterns or if they do then they are\ntypically selected manually based on domain knowledge. Unfortunately, manual\ngeneration of canned patterns is not only labor intensive but may also lack\ndiversity for supporting efficient visual formulation of a wide range of\nsubgraph queries. In this paper, we present a novel generic and extensible\nframework called TATTOO that takes a data-driven approach to automatically\nselecting canned patterns for a GUI from large networks. Specifically, it first\ndecomposes the underlying network into truss-infested and truss-oblivious\nregions. Then candidate canned patterns capturing different real-world query\ntopologies are generated from these regions. Canned patterns based on a\nuser-specified plug are then selected for the GUI from these candidates by\nmaximizing coverage and diversity, and by minimizing the cognitive load of the\npattern set. Experimental studies with real-world datasets demonstrate the\nbenefits of TATTOO. Importantly, this work takes a concrete step towards\nrealizing plug-and-play visual graph query interfaces for large networks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 09:01:00 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Yuan", "Zifeng", ""], ["Chua", "Huey Eng", ""], ["Bhowmick", "Sourav S", ""], ["Ye", "Zekun", ""], ["Han", "Wook-Shin", ""], ["Choi", "Byron", ""]]}, {"id": "2107.09966", "submitter": "Muhammad Aslam Jarwar", "authors": "Muhammad Aslam Jarwar, Adriane Chapman, Mark Elliot, Fatemeh Raji", "title": "Provenance, Anonymisation and Data Environments: a Unifying Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Anonymisation Decision-making Framework (ADF) operationalizes the risk\nmanagement of data exchange between organizations, referred to as \"data\nenvironments\". The second edition of ADF has increased its emphasis on modeling\ndata flows, highlighting a potential new use of provenance information to\nsupport anonymisation decision-making. In this paper, we provide a use case\nthat showcases this functionality more. Based on this use case, we identify how\nprovenance information could be utilized within the ADF framework, and identify\na currently un-met requirement which is the modeling of \\textit{data\nenvironments}. We show how data environments can be implemented within the W3C\nPROV in four different ways. We analyze the costs and benefits of each\napproach, and consider another use case as a partial check for completeness. We\nthen summarize our findings and suggest ways forward.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 09:25:33 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Jarwar", "Muhammad Aslam", ""], ["Chapman", "Adriane", ""], ["Elliot", "Mark", ""], ["Raji", "Fatemeh", ""]]}, {"id": "2107.09993", "submitter": "Chuanwen Li", "authors": "Chuanwen Li, Yu Gu, Jianzhong Qi, Ge Yu", "title": "SkyCell: A Space-Pruning Based Parallel Skyline Algorithm", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Skyline computation is an essential database operation that has many\napplications in multi-criteria decision making scenarios such as recommender\nsystems. Existing algorithms have focused on checking point domination, which\nlack efficiency over large datasets. We propose a grid-based structure that\nenables grid cell domination checks. We show that only a small constant number\nof cells need to be checked which is independent from the number of data\npoints. Our structure also enables parallel processing. We thus obtain a highly\nefficient parallel skyline algorithm named SkyCell, taking advantage of the\nparallelization power of graphics processing units. Experimental results\nconfirm the effectiveness and efficiency of SkyCell -- it outperforms\nstate-of-the-art algorithms consistently and by up to over two orders of\nmagnitude in the computation time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 10:23:58 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Li", "Chuanwen", ""], ["Gu", "Yu", ""], ["Qi", "Jianzhong", ""], ["Yu", "Ge", ""]]}, {"id": "2107.10025", "submitter": "Qi Zhang", "authors": "Minjia Pan, Rong-Hua Li, Qi Zhang, Yongheng Dai, Qun Tian, Guoren Wang", "title": "Fairness-aware Maximal Clique Enumeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cohesive subgraph mining on attributed graphs is a fundamental problem in\ngraph data analysis. Existing cohesive subgraph mining algorithms on attributed\ngraphs do not consider the fairness of attributes in the subgraph. In this\npaper, we for the first time introduce fairness into the widely-used clique\nmodel to mine fairness-aware cohesive subgraphs. In particular, we propose two\nnovel fairness-aware maximal clique models on attributed graphs, called weak\nfair clique and strong fair clique respectively. To enumerate all weak fair\ncliques, we develop an efficient backtracking algorithm called WFCEnum equipped\nwith a novel colorful k-core based pruning technique. We also propose an\nefficient enumeration algorithm called SFCEnum to find all strong fair cliques\nbased on a new attribute-alternatively-selection search technique. To further\nimprove the efficiency, we also present several non-trivial ordering techniques\nfor both weak and strong fair clique enumeration. The results of extensive\nexperiments on four real-world graphs demonstrate the efficiency and\neffectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 11:45:40 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Pan", "Minjia", ""], ["Li", "Rong-Hua", ""], ["Zhang", "Qi", ""], ["Dai", "Yongheng", ""], ["Tian", "Qun", ""], ["Wang", "Guoren", ""]]}, {"id": "2107.10052", "submitter": "Qi Zhang", "authors": "Qi Zhang, Rong-Hua Li, Minjia Pan, Yongheng Dai, Guoren Wang, Ye Yuan", "title": "Efficient Top-k Ego-Betweenness Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Betweenness centrality, measured by the number of times a vertex occurs on\nall shortest paths of a graph, has been recognized as a key indicator for the\nimportance of a vertex in the network. However, the betweenness of a vertex is\noften very hard to compute because it needs to explore all the shortest paths\nbetween the other vertices. Recently, a relaxed concept called ego-betweenness\nwas introduced which focuses on computing the betweenness of a vertex in its\nego network. In this work, we study a problem of finding the top-k vertices\nwith the highest ego-betweennesses. We first develop two novel search\nalgorithms equipped with a basic upper bound and a dynamic upper bound to\nefficiently solve this problem. Then, we propose local-update and lazy-update\nsolutions to maintain the ego-betweennesses for all vertices and the top-k\nresults when the graph is updated, respectively. In addition, we also present\ntwo efficient parallel algorithms to further improve the efficiency. The\nresults of extensive experiments on five large real-life datasets demonstrate\nthe efficiency, scalability, and effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 12:46:52 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Zhang", "Qi", ""], ["Li", "Rong-Hua", ""], ["Pan", "Minjia", ""], ["Dai", "Yongheng", ""], ["Wang", "Guoren", ""], ["Yuan", "Ye", ""]]}, {"id": "2107.10407", "submitter": "Takao Murakami", "authors": "Takao Murakami, Hiromi Arai, Koki Hamada, Takuma Hatano, Makoto\n  Iguchi, Hiroaki Kikuchi, Atsushi Kuromasa, Hiroshi Nakagawa, Yuichi Nakamura,\n  Kenshiro Nishiyama, Ryo Nojima, Hidenobu Oguri, Chiemi Watanabe, Akira\n  Yamada, Takayasu Yamaguchi, Yuji Yamaoka", "title": "Designing a Location Trace Anonymization Contest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location-based services (LBS) are increasingly used in recent years, and\nconsequently a large amount of location traces are accumulating in a data\ncenter. Although these traces can be provided to a data analyst for geo-data\nanalysis, the disclosure of location traces raises serious privacy concerns.\nFinding an appropriate anonymization method for location traces is also\nextremely challenging, especially for long traces. To address this issue, we\nhave designed and held a location trace anonymization contest that deals with a\nlong trace (400 events per user) and fine-grained locations (1024 regions). In\nour contest, each team anonymizes her original traces, and then the other teams\nperform privacy attacks against the anonymized traces (i.e., both defense and\nattack compete together) in a partial-knowledge attacker model where the\nadversary does not know the original traces. To realize such a contest, we\npropose a novel location synthesizer that has diversity in that synthetic\ntraces for each team are different from those for the other teams and utility\nin that synthetic traces preserve various statistical features of real traces.\nWe also show that re-identification alone is insufficient as a privacy risk,\nand that trace inference should be added as an additional risk. Specifically,\nwe show an example of anonymization that is perfectly secure against\nre-identification and is not secure against trace inference. Based on this, our\ncontest evaluates both the re-identification risk and trace inference risk, and\nanalyzes the relation between the two risks. In this paper, we present our\nlocation synthesizer and the design of our contest, and then report our contest\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 00:33:09 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Murakami", "Takao", ""], ["Arai", "Hiromi", ""], ["Hamada", "Koki", ""], ["Hatano", "Takuma", ""], ["Iguchi", "Makoto", ""], ["Kikuchi", "Hiroaki", ""], ["Kuromasa", "Atsushi", ""], ["Nakagawa", "Hiroshi", ""], ["Nakamura", "Yuichi", ""], ["Nishiyama", "Kenshiro", ""], ["Nojima", "Ryo", ""], ["Oguri", "Hidenobu", ""], ["Watanabe", "Chiemi", ""], ["Yamada", "Akira", ""], ["Yamaguchi", "Takayasu", ""], ["Yamaoka", "Yuji", ""]]}, {"id": "2107.10417", "submitter": "Yifan Li", "authors": "Yifan Li, Xiaohui Yu, Nick Koudas", "title": "LES3: Learning-based Exact Set Similarity Search", "comments": "Accepted by VLDB2021", "journal-ref": null, "doi": "10.14778/3476249.3476263", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set similarity search is a problem of central interest to a wide variety of\napplications such as data cleaning and web search. Past approaches on set\nsimilarity search utilize either heavy indexing structures, incurring large\nsearch costs or indexes that produce large candidate sets. In this paper, we\ndesign a learning-based exact set similarity search approach, LES3. Our\napproach first partitions sets into groups, and then utilizes a light-weight\nbitmap-like indexing structure, called token-group matrix (TGM), to organize\ngroups and prune out candidates given a query set. In order to optimize pruning\nusing the TGM, we analytically investigate the optimal partitioning strategy\nunder certain distributional assumptions. Using these results, we then design a\nlearning-based partitioning approach called L2P and an associated data\nrepresentation encoding, PTR, to identify the partitions. We conduct extensive\nexperiments on real and synthetic datasets to fully study LES3, establishing\nthe effectiveness and superiority over other applicable approaches.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 01:59:11 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Li", "Yifan", ""], ["Yu", "Xiaohui", ""], ["Koudas", "Nick", ""]]}, {"id": "2107.10508", "submitter": "Marc E. Sol\\`er", "authors": "Tobias Fankhauser, Marc E. Sol\\`er, Rudolf M. F\\\"uchslin, Kurt\n  Stockinger", "title": "Multiple Query Optimization using a Hybrid Approach of Classical and\n  Quantum Computing", "comments": "18 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum computing promises to solve difficult optimization problems in\nchemistry, physics and mathematics more efficiently than classical computers,\nbut requires fault-tolerant quantum computers with millions of qubits. To\novercome errors introduced by today's quantum computers, hybrid algorithms\ncombining classical and quantum computers are used. In this paper we tackle the\nmultiple query optimization problem (MQO) which is an important NP-hard problem\nin the area of data-intensive problems. We propose a novel hybrid\nclassical-quantum algorithm to solve the MQO on a gate-based quantum computer.\nWe perform a detailed experimental evaluation of our algorithm and compare its\nperformance against a competing approach that employs a quantum annealer --\nanother type of quantum computer. Our experimental results demonstrate that our\nalgorithm currently can only handle small problem sizes due to the limited\nnumber of qubits available on a gate-based quantum computer compared to a\nquantum computer based on quantum annealing. However, our algorithm shows a\nqubit efficiency of close to 99% which is almost a factor of 2 higher compared\nto the state of the art implementation. Finally, we analyze how our algorithm\nscales with larger problem sizes and conclude that our approach shows promising\nresults for near-term quantum computers.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 08:12:49 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Fankhauser", "Tobias", ""], ["Sol\u00e8r", "Marc E.", ""], ["F\u00fcchslin", "Rudolf M.", ""], ["Stockinger", "Kurt", ""]]}, {"id": "2107.10590", "submitter": "Florian Sold", "authors": "Martin Graf, Lukas Laskowski, Florian Papsdorf, Florian Sold, Roland\n  Gremmelspacher, Felix Naumann, Fabian Panse", "title": "Frost: Benchmarking and Exploring Data Matching Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  \"Bad\" data has a direct impact on 88% of companies, with the average company\nlosing 12% of its revenue due to it. Duplicates - multiple but different\nrepresentations of the same real-world entities - are among the main reasons\nfor poor data quality. Therefore, finding and configuring the right\ndeduplication solution is essential. Various data matching benchmarks exist\nwhich address this issue. However, many of them focus on the quality of\nmatching results and neglect other important factors, such as business\nrequirements. Additionally, they often do not specify how to explore benchmark\nresults, which helps understand matching solution behavior. To address this gap\nbetween the mere counting of record pairs vs. a comprehensive means to evaluate\ndata matching approaches, we present the benchmark platform Frost. Frost\ncombines existing benchmarks, established quality metrics, a benchmark\ndimension for soft KPIs, and techniques to systematically explore and\nunderstand matching results. Thus, it can be used to compare multiple matching\nsolutions regarding quality, usability, and economic aspects, but also to\ncompare multiple runs of the same matching solution for understanding its\nbehavior. Frost is implemented and published in the open-source application\nSnowman, which includes the visual exploration of matching results.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 11:39:16 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Graf", "Martin", ""], ["Laskowski", "Lukas", ""], ["Papsdorf", "Florian", ""], ["Sold", "Florian", ""], ["Gremmelspacher", "Roland", ""], ["Naumann", "Felix", ""], ["Panse", "Fabian", ""]]}, {"id": "2107.10659", "submitter": "Ashwin Machanavajjhala", "authors": "Sam Haney and William Sexton and Ashwin Machanavajjhala and Michael\n  Hay and Gerome Miklau", "title": "Differentially Private Algorithms for 2020 Census Detailed DHC Race \\&\n  Ethnicity", "comments": "Presented at Theory and Practice of Differential Privacy Workshop\n  (TPDP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article describes a proposed differentially private (DP) algorithms that\nthe US Census Bureau is considering to release the Detailed Demographic and\nHousing Characteristics (DHC) Race & Ethnicity tabulations as part of the 2020\nCensus. The tabulations contain statistics (counts) of demographic and housing\ncharacteristics of the entire population of the US crossed with detailed races\nand tribes at varying levels of geography. We describe two differentially\nprivate algorithmic strategies, one based on adding noise drawn from a\ntwo-sided Geometric distribution that satisfies \"pure\"-DP, and another based on\nadding noise from a Discrete Gaussian distribution that satisfied a well\nstudied variant of differential privacy, called Zero Concentrated Differential\nPrivacy (zCDP). We analytically estimate the privacy loss parameters ensured by\nthe two algorithms for comparable levels of error introduced in the statistics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 13:35:11 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Haney", "Sam", ""], ["Sexton", "William", ""], ["Machanavajjhala", "Ashwin", ""], ["Hay", "Michael", ""], ["Miklau", "Gerome", ""]]}, {"id": "2107.10669", "submitter": "Ali Almalki", "authors": "Ali Almalki, Pawel Wocjan", "title": "Accuracy analysis of Educational Data Mining using Feature Selection\n  Algorithm", "comments": null, "journal-ref": "Int'l Conf. Artificial Intelligence ICAI2019", "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract - Gathering relevant information to predict student academic\nprogress is a tedious task. Due to the large amount of irrelevant data present\nin databases which provides inaccurate results. Currently, it is not possible\nto accurately measure and analyze student data because there are too many\nirrelevant attributes and features in the data. With the help of Educational\nData Mining (EDM), the quality of information can be improved. This research\ndemonstrates how EDM helps to measure the accuracy of data using relevant\nattributes and machine learning algorithms performed. With EDM, irrelevant\nfeatures are removed without changing the original data. The data set used in\nthis study was taken from Kaggle.com. The results compared on the basis of\nrecall, precision and f-measure to check the accuracy of the student data. The\nimportance of this research is to help improve the quality of educational\nresearch by providing more accurate results for researchers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 01:44:25 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Almalki", "Ali", ""], ["Wocjan", "Pawel", ""]]}, {"id": "2107.10831", "submitter": "Ami Pandat", "authors": "Ami Pandat, Nidhi Gupta, Minal Bhise", "title": "Load Balanced Semantic Aware Distributed RDF Graph", "comments": null, "journal-ref": null, "doi": "10.1145/3472163.347216", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern day semantic applications store data as Resource Description\nFramework (RDF) data.Due to Proliferation of RDF Data, the efficient management\nof huge RDF data has become essential. A number of approaches pertaining to\nboth relational and graph-based have been devised to handle this huge data. As\nthe relational approach suffers from query joins, we propose a semantic aware\ngraph based partitioning method. The partitioned fragments are further\nallocated in a load balanced way. For efficient query processing, partial\nreplication is implemented. It reduces Inter node Communication thereby\naccelerating queries on distributed RDF Graph. This approach has been\ndemonstrated in two phases partitioning and Distribution of Linked Observation\nData (LOD). The time complexity for partitioning and distribution of Load\nBalanced Semantic Aware RDF Graph (LBSD) is O(n) where n is the number of\ntriples which is demonstrated by linear increment in algorithm execution time\n(AET) for LOD data scaled from 1x to 5x. LBSD has been found to behave well\ntill 4x. LBSD is compared with the state of the art relational and graph-based\npartitioning techniques. LBSD records 71% QET gain when averaged over all the\nfour query types. For most frequent query types, Linear and Star, on an average\n65% QET gain is recorded over original configuration for scaling experiments.\nThe optimal replication level has been found to be 12% of original data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:42:27 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Pandat", "Ami", ""], ["Gupta", "Nidhi", ""], ["Bhise", "Minal", ""]]}, {"id": "2107.10836", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Boon Thau Loo, Divyakant Agrawal, Amr El Abbadi", "title": "Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System with\n  Confidentiality Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Today's large-scale data management systems need to address distributed\napplications' confidentiality and scalability requirements among a set of\ncollaborative enterprises. In this paper, we present Qanaat, a scalable\nmulti-enterprise permissioned blockchain system that guarantees\nconfidentiality. Qanaat consists of multiple enterprises where each enterprise\npartitions its data into multiple shards and replicates a data shard on a\ncluster of nodes to provide fault tolerance. Qanaat presents data collections\nthat preserve the confidentiality of transactions and a transaction ordering\nschema that enforces only the necessary and sufficient constraints to guarantee\ndata consistency. Furthermore, Qanaat supports both data consistency and\nconfidentiality across collaboration workflows where an enterprise can\nparticipate in different collaboration workflows with different sets of\nenterprises. Finally, Qanaat presents a suite of centralized and decentralized\nconsensus protocols to support different types of intra-shard and cross-shard\ntransactions within or across enterprises. The experimental results reveal the\nefficiency of Qanaat in processing multi-shard and multi-enterprise\ntransactions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:50:31 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Loo", "Boon Thau", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "2107.11152", "submitter": "Jerome Darmont", "authors": "Pegdwend\\'e Sawadogo (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "On data lake architectures and metadata management", "comments": null, "journal-ref": "Journal of Intelligent Information Systems, Springer Verlag, 2021,\n  56 (1), pp.97-120", "doi": "10.1007/s10844-020-00608-7", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, we have witnessed an exponential increase of data\nproduction in the world. So-called big data generally come from transactional\nsystems, and even more so from the Internet of Things and social media. They\nare mainly characterized by volume, velocity, variety and veracity issues. Big\ndata-related issues strongly challenge traditional data management and analysis\nsystems. The concept of data lake was introduced to address them. A data lake\nis a large, raw data repository that stores and manages all company data\nbearing any format. However, the data lake concept remains ambiguous or fuzzy\nfor many researchers and practitioners, who often confuse it with the Hadoop\ntechnology. Thus, we provide in this paper a comprehensive state of the art of\nthe different approaches to data lake design. We particularly focus on data\nlake architectures and metadata management, which are key issues in successful\ndata lakes. We also discuss the pros and cons of data lakes and their design\nalternatives.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 11:59:28 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Sawadogo", "Pegdwend\u00e9", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "2107.11157", "submitter": "Jerome Darmont", "authors": "Pengfei Liu (ERIC), Sabine Loudcher (ERIC), J\\'er\\^ome Darmont (ERIC),\n  Camille No\\^us (Laboratoire Cogitamus)", "title": "ArchaeoDAL: A Data Lake for Archaeological Data Management and Analytics", "comments": "25th International Database Engineering & Applications Symposium\n  (IDEAS 2021), Jul 2021, Montr\\'eal, Canada", "journal-ref": null, "doi": "10.1145/3472163.3472266", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With new emerging technologies, such as satellites and drones, archaeologists\ncollect data over large areas. However, it becomes difficult to process such\ndata in time. Archaeological data also have many different formats (images,\ntexts, sensor data) and can be structured, semi-structured and unstructured.\nSuch variety makes data difficult to collect, store, manage, search and analyze\neffectively. A few approaches have been proposed, but none of them covers the\nfull data lifecycle nor provides an efficient data management system. Hence, we\npropose the use of a data lake to provide centralized data stores to host\nheterogeneous data, as well as tools for data quality checking, cleaning,\ntransformation, and analysis. In this paper, we propose a generic, flexible and\ncomplete data lake architecture. Our metadata management system exploits\ngoldMEDAL, which is the most complete metadata model currently available.\nFinally, we detail the concrete implementation of this architecture dedicated\nto an archaeological project.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 12:05:52 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Liu", "Pengfei", "", "ERIC"], ["Loudcher", "Sabine", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["No\u00fbs", "Camille", "", "Laboratoire Cogitamus"]]}, {"id": "2107.11347", "submitter": "James Cheney", "authors": "James Cheney and Wilmer Ricciotti", "title": "Comprehending nulls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nested Relational Calculus (NRC) has been an influential high-level query\nlanguage, providing power and flexibility while still allowing translation to\nstandard SQL queries. It has also been used as a basis for language-integrated\nquery in programming languages such as F#, Scala, and Links. However, SQL's\ntreatment of incomplete information, using nulls and three-valued logic, is not\ncompatible with `standard' NRC based on two-valued logic. Nulls are widely used\nin practice for incomplete data, but the question of how to accommodate\nSQL-style nulls and incomplete information in NRC, or integrate such queries\ninto a typed programming language, appears not to have been studied thoroughly.\nIn this paper we consider two approaches: an explicit approach in which option\ntypes are used to represent (possibly) nullable primitive types, and an\nimplicit approach in which types are treated as possibly-null by default. We\ngive translations relating the implicit and explicit approaches, discuss\nhandling nulls in language integration, and sketch extensions of normalization\nand conservativity results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 16:49:21 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Cheney", "James", ""], ["Ricciotti", "Wilmer", ""]]}, {"id": "2107.11378", "submitter": "Thamir Qadah", "authors": "Thamir M. Qadah and Mohammad Sadoghi", "title": "Highly Available Queue-oriented Speculative Transaction Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deterministic database systems have received increasing attention from the\ndatabase research community in recent years. Despite their current limitations,\nrecent proposals of distributed deterministic transaction processing systems\ndemonstrated significant improvements over systems using traditional\ntransaction processing techniques (e.g., two-phase-locking or optimistic\nconcurrency control with two-phase-commit). However, the problem of ensuring\nhigh availability in deterministic distributed transaction processing systems\nhas received less attention from the research community, and this aspect has\nnot been analyzed and evaluated well. This paper proposes a generic framework\nto model the replication process in deterministic transaction processing\nsystems and use it to study three cases. We design and implement QR-Store, a\nqueue-oriented replicated transaction processing system, and extensively\nevaluate it with various workloads based on a transactional version of YCSB.\nOur prototype implementation QR-Store can achieve a throughput of 1.9 million\nreplicated transactions per second in under 200 milliseconds and a replication\noverhead of 8%-25% compared to non-replicated configurations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 17:58:40 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Qadah", "Thamir M.", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "2107.11592", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Mohammad Sadoghi", "title": "Blockchain Transaction Processing", "comments": null, "journal-ref": "Encyclopedia of Big Data Technologies 2019", "doi": "10.1007/978-3-319-77525-8_333", "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A blockchain is a linked list of immutable tamper-proof blocks, which is\nstored at each participating node. Each block records a set of transactions and\nthe associated metadata. Blockchain transactions act on the identical ledger\ndata stored at each node. Blockchain was first perceived by Satoshi Nakamoto,\nas a peer-to-peer money exchange system. Nakamoto referred to the transactional\ntokens exchanged among clients in his system as Bitcoins.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 12:20:36 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gupta", "Suyash", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "2107.11607", "submitter": "Michael Fruth", "authors": "Michael Fruth, Stefanie Scherzinger, Wolfgang Mauerer, Ralf Ramsauer", "title": "Tell-Tale Tail Latencies: Pitfalls and Perils in Database Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of database systems is usually characterised by their\naverage-case (i.e., throughput) behaviour in standardised or de-facto standard\nbenchmarks like TPC-X or YCSB. While tails of the latency (i.e., response time)\ndistribution receive considerably less attention, they have been identified as\na threat to the overall system performance: In large-scale systems, even a\nfraction of requests delayed can build up into delays perceivable by end users.\nTo eradicate large tail latencies from database systems, the ability to\nfaithfully record them, and likewise pinpoint them to the root causes, is\nimminently required. In this paper, we address the challenge of measuring tail\nlatencies using standard benchmarks, and identify subtle perils and pitfalls.\nIn particular, we demonstrate how Java-based benchmarking approaches can\nsubstantially distort tail latency observations, and discuss how the discovery\nof such problems is inhibited by the common focus on throughput performance. We\nmake a case for purposefully re-designing database benchmarking harnesses based\non these observations to arrive at faithful characterisations of database\nperformance from multiple important angles.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 13:54:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Fruth", "Michael", ""], ["Scherzinger", "Stefanie", ""], ["Mauerer", "Wolfgang", ""], ["Ramsauer", "Ralf", ""]]}, {"id": "2107.11967", "submitter": "Tarique Siddiqui", "authors": "Tarique Siddiqui, Surajit Chaudhuri and Vivek Narasayya", "title": "COMPARE: Accelerating Groupwise Comparison in Relational Databases for\n  Data Analytics", "comments": "Extended Version of VLDB 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data analysis often involves comparing subsets of data across many dimensions\nfor finding unusual trends and patterns. While the comparison between subsets\nof data can be expressed using SQL, they tend to be complex to write, and\nsuffer from poor performance over large and high-dimensional datasets. In this\npaper, we propose a new logical operator COMPARE for relational databases that\nconcisely captures the enumeration and comparison between subsets of data and\ngreatly simplifies the expressing of a large class of comparative queries. We\nextend the database engine with optimization techniques that exploit the\nsemantics of COMPARE to significantly improve the performance of such queries.\nWe have implemented these extensions inside Microsoft SQL Server, a commercial\nDBMS engine. Our extensive evaluation on synthetic and real-world datasets\nshows that COMPARE results in a significant speedup over existing approaches,\nincluding physical plans generated by today's database systems, user-defined\nfunction (UDF), as well as middleware solutions that compare subsets outside\nthe databases.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 05:48:32 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 07:11:06 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Siddiqui", "Tarique", ""], ["Chaudhuri", "Surajit", ""], ["Narasayya", "Vivek", ""]]}, {"id": "2107.11983", "submitter": "Shixuan Sun", "authors": "Shixuan Sun and Yuhang Chen and Shengliang Lu and Bingsheng He and\n  Yuchen Li", "title": "ThunderRW: An In-Memory Graph Random Walk Engine (Complete Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As random walk is a powerful tool in many graph processing, mining and\nlearning applications, this paper proposes an efficient in-memory random walk\nengine named ThunderRW. Compared with existing parallel systems on improving\nthe performance of a single graph operation, ThunderRW supports massive\nparallel random walks. The core design of ThunderRW is motivated by our\nprofiling results: common RW algorithms have as high as 73.1% CPU pipeline\nslots stalled due to irregular memory access, which suffers significantly more\nmemory stalls than the conventional graph workloads such as BFS and SSSP. To\nimprove the memory efficiency, we first design a generic step-centric\nprogramming model named Gather-Move-Update to abstract different RW algorithms.\nBased on the programming model, we develop the step interleaving technique to\nhide memory access latency by switching the executions of different random walk\nqueries. In our experiments, we use four representative RW algorithms including\nPPR, DeepWalk, Node2Vec and MetaPath to demonstrate the efficiency and\nprogramming flexibility of ThunderRW. Experimental results show that ThunderRW\noutperforms state-of-the-art approaches by an order of magnitude, and the step\ninterleaving technique significantly reduces the CPU pipeline stall from 73.1%\nto 15.0%.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 06:37:42 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Sun", "Shixuan", ""], ["Chen", "Yuhang", ""], ["Lu", "Shengliang", ""], ["He", "Bingsheng", ""], ["Li", "Yuchen", ""]]}, {"id": "2107.12055", "submitter": "Jerome Darmont", "authors": "Yuzhao Yang (IRIT-SIG), J\\'er\\^ome Darmont (ERIC), Franck Ravat\n  (IRIT-SIG), Olivier Teste (IRIT-SIG)", "title": "An Automatic Schema-Instance Approach for Merging Multidimensional Data\n  Warehouses", "comments": "25th International Database Engineering & Applications Symposium\n  (IDEAS 2021), Jul 2021, Montreal, Canada", "journal-ref": null, "doi": "10.1145/3472163.3472268", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using data warehouses to analyse multidimensional data is a significant task\nin company decision-making.The data warehouse merging process is composed of\ntwo steps: matching multidimensional components and then merging them. Current\napproaches do not take all the particularities of multidimensional data\nwarehouses into account, e.g., only merging schemata, but not instances; or not\nexploiting hierarchies nor fact tables. Thus, in this paper, we propose an\nautomatic merging approach for star schema-modeled data warehouses that works\nat both the schema and instance levels. We also provide algorithms for merging\nhierarchies, dimensions and facts. Eventually, we implement our merging\nalgorithms and validate them with the use of both synthetic and benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:24:44 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yang", "Yuzhao", "", "IRIT-SIG"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Ravat", "Franck", "", "IRIT-SIG"], ["Teste", "Olivier", "", "IRIT-SIG"]]}, {"id": "2107.12239", "submitter": "Brecht Vandevoort", "authors": "Brecht Vandevoort, Bas Ketsman, Christoph Koch, Frank Neven", "title": "Robustness against Read Committed for Transaction Templates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The isolation level Multiversion Read Committed (RC), offered by many\ndatabase systems, is known to trade consistency for increased transaction\nthroughput. Sometimes, transaction workloads can be safely executed under RC\nobtaining the perfect isolation of serializability at the lower cost of RC. To\nidentify such cases, we introduce an expressive model of transaction programs\nto better reason about the serializability of transactional workloads. We\ndevelop tractable algorithms to decide whether any possible schedule of a\nworkload executed under RC is serializable (referred to as the robustness\nproblem). Our approach yields robust subsets that are larger than those\nidentified by previous methods. We provide experimental evidence that workloads\nthat are robust against RC can be evaluated faster under RC compared to\nstronger isolation levels. We discuss techniques for making workloads robust\nagainst RC by promoting selective read operations to updates. Depending on the\nscenario, the performance improvements can be considerable. Robustness testing\nand safely executing transactions under the lower isolation level RC can\ntherefore provide a direct way to increase transaction throughput without\nchanging DBMS internals.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 14:38:38 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Vandevoort", "Brecht", ""], ["Ketsman", "Bas", ""], ["Koch", "Christoph", ""], ["Neven", "Frank", ""]]}, {"id": "2107.12295", "submitter": "Peizhi Wu", "authors": "Peizhi Wu and Gao Cong", "title": "A Unified Deep Model of Learning from both Data and Queries for\n  Cardinality Estimation", "comments": "14 pages, SIGMOD 2021", "journal-ref": null, "doi": "10.1145/3448016.3452830", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinality estimation is a fundamental problem in database systems. To\ncapture the rich joint data distributions of a relational table, most of the\nexisting work either uses data as unsupervised information or uses query\nworkload as supervised information. Very little work has been done to use both\ntypes of information, and cannot fully make use of both types of information to\nlearn the joint data distribution. In this work, we aim to close the gap\nbetween data-driven and query-driven methods by proposing a new unified deep\nautoregressive model, UAE, that learns the joint data distribution from both\nthe data and query workload. First, to enable using the supervised query\ninformation in the deep autoregressive model, we develop differentiable\nprogressive sampling using the Gumbel-Softmax trick. Second, UAE is able to\nutilize both types of information to learn the joint data distribution in a\nsingle model. Comprehensive experimental results demonstrate that UAE achieves\nsingle-digit multiplicative error at tail, better accuracies over\nstate-of-the-art methods, and is both space and time efficient.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 16:09:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wu", "Peizhi", ""], ["Cong", "Gao", ""]]}, {"id": "2107.12373", "submitter": "Sonia Cromp", "authors": "Sonia Cromp, Alireza Samadian, Kirk Pruhs", "title": "Relational Boosted Regression Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks use data housed in relational databases to train boosted\nregression tree models. In this paper, we give a relational adaptation of the\ngreedy algorithm for training boosted regression trees. For the subproblem of\ncalculating the sum of squared residuals of the dataset, which dominates the\nruntime of the boosting algorithm, we provide a $(1 + \\epsilon)$-approximation\nusing the tensor sketch technique. Employing this approximation within the\nrelational boosted regression trees algorithm leads to learning similar model\nparameters, but with asymptotically better runtime.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 20:29:28 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Cromp", "Sonia", ""], ["Samadian", "Alireza", ""], ["Pruhs", "Kirk", ""]]}, {"id": "2107.12525", "submitter": "Daniel Kang", "authors": "Daniel Kang, John Guibas, Peter Bailis, Tatsunori Hashimoto, Yi Sun,\n  Matei Zaharia", "title": "Proof: Accelerating Approximate Aggregation Queries with Expensive\n  Predicates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DB cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dataset $\\mathcal{D}$, we are interested in computing the mean of a\nsubset of $\\mathcal{D}$ which matches a predicate. ABae leverages stratified\nsampling and proxy models to efficiently compute this statistic given a\nsampling budget $N$. In this document, we theoretically analyze ABae and show\nthat the MSE of the estimate decays at rate $O(N_1^{-1} + N_2^{-1} +\nN_1^{1/2}N_2^{-3/2})$, where $N=K \\cdot N_1+N_2$ for some integer constant $K$\nand $K \\cdot N_1$ and $N_2$ represent the number of samples used in Stage 1 and\nStage 2 of ABae respectively. Hence, if a constant fraction of the total sample\nbudget $N$ is allocated to each stage, we will achieve a mean squared error of\n$O(N^{-1})$ which matches the rate of mean squared error of the optimal\nstratified sampling algorithm given a priori knowledge of the predicate\npositive rate and standard deviation per stratum.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 00:18:21 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 18:29:08 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Kang", "Daniel", ""], ["Guibas", "John", ""], ["Bailis", "Peter", ""], ["Hashimoto", "Tatsunori", ""], ["Sun", "Yi", ""], ["Zaharia", "Matei", ""]]}, {"id": "2107.12884", "submitter": "Carlos Diego Nascimento Damasceno", "authors": "Carlos Diego Nascimento Damasceno, Fabio Manoel Fran\\c{c}a Lobato,\n  Elton Rocha Moutinho, Arilene Santos de Fran\\c{c}a, Ivan Ikikame de Oliveira,\n  \\'Adamo Lima de Santana", "title": "SimCleaner -- Sistema de Padroniza\\c{c}\\~ao de Bases de Dados utilizando\n  Fun\\c{c}\\~oes de Similaridade", "comments": "Anais da XIV Semana de Inform\\'atica (SEMINF) e Escola Regional de\n  Inform\\'atica Norte (ERIN), 2011. in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Knowledge Discovery in Database (KDD) process permits the detection of\npattern in databases, where this analysis may be compromised if database is not\nconsistent, making necessary the use of data cleaning techniques. This paper\npresents a tool based in similarity functions to help the preprocessing of\ndatabases and it behaved efficiently in the standardization of a System of\nPublic Security of the State of Par\\'a database and may be reused with other\ndatabases and other data mining projects.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 15:29:29 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Damasceno", "Carlos Diego Nascimento", ""], ["Lobato", "Fabio Manoel Fran\u00e7a", ""], ["Moutinho", "Elton Rocha", ""], ["de Fran\u00e7a", "Arilene Santos", ""], ["de Oliveira", "Ivan Ikikame", ""], ["de Santana", "\u00c1damo Lima", ""]]}, {"id": "2107.12948", "submitter": "Hui Luo", "authors": "Hui Luo, Zhifeng Bao, Gao Cong, J. Shane Culpepper, Nguyen Lu Dang\n  Khoa", "title": "Let Trajectories Speak Out the Traffic Bottlenecks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic bottlenecks are a set of road segments that have an unacceptable\nlevel of traffic caused by a poor balance between road capacity and traffic\nvolume. A huge volume of trajectory data which captures real-time traffic\nconditions in road networks provides promising new opportunities to identify\nthe traffic bottlenecks. In this paper, we define this problem as\ntrajectory-driven traffic bottleneck identification: Given a road network R, a\ntrajectory database T , find a representative set of seed edges of size K of\ntraffic bottlenecks that influence the highest number of road segments not in\nthe seed set. We show that this problem is NP-hard and propose a framework to\nfind the traffic bottlenecks as follows. First, a traffic spread model is\ndefined which represents changes in traffic volume for each road segment over\ntime. Then, the traffic diffusion probability between two connected segments\nand the residual ratio of traffic volume for each segment can be computed using\nhistorical trajectory data. We then propose two different algorithmic\napproaches to solve the problem. The first one is a best-first algorithm BF,\nwith an approximation ratio of 1-1/e. To further accelerate the identification\nprocess in larger datasets, we also propose a sampling-based greedy algorithm\nSG. Finally, comprehensive experiments using three different datasets compare\nand contrast various solutions, and provide insights into important efficiency\nand effectiveness trade-offs among the respective methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:10:20 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Luo", "Hui", ""], ["Bao", "Zhifeng", ""], ["Cong", "Gao", ""], ["Culpepper", "J. Shane", ""], ["Khoa", "Nguyen Lu Dang", ""]]}, {"id": "2107.13047", "submitter": "Suyash Gupta", "authors": "Sajjad Rahnama, Suyash Gupta, Rohan Sogani, Dhruv Krishnan, Mohammad\n  Sadoghi", "title": "RingBFT: Resilient Consensus over Sharded Ring Topology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent surge in federated data-management applications has brought forth\nconcerns about the security of underlying data and the consistency of replicas\nin the presence of malicious attacks. A prominent solution in this direction is\nto employ a permissioned blockchain framework that is modeled around\ntraditional Byzantine Fault-Tolerant (BFT) consensus protocols. Any federated\napplication expects its data to be globally scattered to achieve faster access.\nBut, prior works have shown that traditional BFT protocols are slow and this\nled to the rise of sharded-replicated blockchains. Existing BFT protocols for\nthese sharded blockchains are efficient if client transactions require access\nto a single-shard, but face performance degradation if there is a cross-shard\ntransaction that requires access to multiple shards. However, cross-shard\ntransactions are common, and to resolve this dilemma, we present RingBFT, a\nnovel meta-BFT protocol for sharded blockchains. RingBFT requires shards to\nadhere to the ring order, and follow the principle of process, forward, and\nre-transmit while ensuring the communication between shards is linear. Our\nevaluation of RingBFT against state-of-the-art sharding BFT protocols\nillustrates that RingBFT achieves up to 25x higher throughput, easily scales to\nnearly 500 globally distributed nodes, and achieves a peak throughput of 1.2\nmillion txns/s.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 19:15:23 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Rahnama", "Sajjad", ""], ["Gupta", "Suyash", ""], ["Sogani", "Rohan", ""], ["Krishnan", "Dhruv", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "2107.13066", "submitter": "Wil van der Aalst", "authors": "Wil van der Aalst and Tobias Brockhoff and Anahita Farhang Ghahfarokhi\n  and Mahsa Pourbafrani and Merih Seran Uysal and Sebastiaan van Zelst", "title": "Removing Operational Friction Using Process Mining: Challenges Provided\n  by the Internet of Production (IoP)", "comments": "30 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Operational processes in production, logistics, material handling,\nmaintenance, etc., are supported by cyber-physical systems combining hardware\nand software components. As a result, the digital and the physical world are\nclosely aligned, and it is possible to track operational processes in detail\n(e.g., using sensors). The abundance of event data generated by today's\noperational processes provides opportunities and challenges for process mining\ntechniques supporting process discovery, performance analysis, and conformance\nchecking. Using existing process mining tools, it is already possible to\nautomatically discover process models and uncover performance and compliance\nproblems. In the DFG-funded Cluster of Excellence \"Internet of Production\"\n(IoP), process mining is used to create \"digital shadows\" to improve a wide\nvariety of operational processes. However, operational processes are dynamic,\ndistributed, and complex. Driven by the challenges identified in the IoP\ncluster, we work on novel techniques for comparative process mining (comparing\nprocess variants for different products at different locations at different\ntimes), object-centric process mining (to handle processes involving different\ntypes of objects that interact), and forward-looking process mining (to explore\n\"What if?\" questions). By addressing these challenges, we aim to develop\nvaluable \"digital shadows\" that can be used to remove operational friction.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 20:04:25 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["van der Aalst", "Wil", ""], ["Brockhoff", "Tobias", ""], ["Ghahfarokhi", "Anahita Farhang", ""], ["Pourbafrani", "Mahsa", ""], ["Uysal", "Merih Seran", ""], ["van Zelst", "Sebastiaan", ""]]}, {"id": "2107.13749", "submitter": "Sina Shaham", "authors": "Sina Shaham, Gabriel Ghinita, Ritesh Ahuja, John Krumm, Cyrus Shahabi", "title": "HTF: Homogeneous Tree Framework for Differentially-Private Release of\n  Location Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile apps that use location data are pervasive, spanning domains such as\ntransportation, urban planning and healthcare. Important use cases for location\ndata rely on statistical queries, e.g., identifying hotspots where users work\nand travel. Such queries can be answered efficiently by building histograms.\nHowever, precise histograms can expose sensitive details about individual\nusers. Differential privacy (DP) is a mature and widely-adopted protection\nmodel, but most approaches for DP-compliant histograms work in a\ndata-independent fashion, leading to poor accuracy. The few proposed\ndata-dependent techniques attempt to adjust histogram partitions based on\ndataset characteristics, but they do not perform well due to the addition of\nnoise required to achieve DP. We identify density homogeneity as a main factor\ndriving the accuracy of DP-compliant histograms, and we build a data structure\nthat splits the space such that data density is homogeneous within each\nresulting partition. We show through extensive experiments on large-scale\nreal-world data that the proposed approach achieves superior accuracy compared\nto existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 05:15:07 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Shaham", "Sina", ""], ["Ghinita", "Gabriel", ""], ["Ahuja", "Ritesh", ""], ["Krumm", "John", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "2107.13923", "submitter": "Haozhe Zhang", "authors": "Ahmet Kara, Milos Nikolic, Dan Olteanu, Haozhe Zhang", "title": "Machine Learning over Static and Dynamic Relational Data", "comments": "arXiv admin note: text overlap with arXiv:2008.07864", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This tutorial overviews principles behind recent works on training and\nmaintaining machine learning models over relational data, with an emphasis on\nthe exploitation of the relational data structure to improve the runtime\nperformance of the learning task.\n  The tutorial has the following parts:\n  1) Database research for data science\n  2) Three main ideas to achieve performance improvements\n  2.1) Turn the ML problem into a DB problem\n  2.2) Exploit structure of the data and problem\n  2.3) Exploit engineering tools of a DB researcher\n  3) Avenues for future research\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 12:00:11 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Kara", "Ahmet", ""], ["Nikolic", "Milos", ""], ["Olteanu", "Dan", ""], ["Zhang", "Haozhe", ""]]}, {"id": "2107.13957", "submitter": "Pavlos Fafalios", "authors": "Pavlos Fafalios, Konstantina Konsolaki, Lida Charami, Kostas Petrakis,\n  Manos Paterakis, Dimitris Angelakis, Yannis Tzitzikas, Chrysoula Bekiari,\n  Martin Doerr", "title": "Towards Semantic Interoperability in Historical Research: Documenting\n  Research Data and Knowledge with Synthesis", "comments": "This is a preprint of an article accepted for publication at the 20th\n  International Semantic Web Conference (ISWC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A vast area of research in historical science concerns the documentation and\nstudy of artefacts and related evidence. Current practice mostly uses\nspreadsheets or simple relational databases to organise the information as rows\nwith multiple columns of related attributes. This form offers itself for data\nanalysis and scholarly interpretation, however it also poses problems including\ni) the difficulty for collaborative but controlled documentation by a large\nnumber of users, ii) the lack of representation of the details from which the\ndocumented relations are inferred, iii) the difficulty to extend the underlying\ndata structures as well as to combine and integrate data from multiple and\ndiverse information sources, and iv) the limitation to reuse the data beyond\nthe context of a particular research activity. To support historians to cope\nwith these problems, in this paper we describe the Synthesis documentation\nsystem and its use by a large number of historians in the context of an ongoing\nresearch project in the field of History of Art. The system is Web-based and\ncollaborative, and makes use of existing standards for information\ndocumentation and publication (CIDOC-CRM, RDF), focusing on semantic\ninteroperability and the production of data of high value and long-term\nvalidity.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 13:37:39 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fafalios", "Pavlos", ""], ["Konsolaki", "Konstantina", ""], ["Charami", "Lida", ""], ["Petrakis", "Kostas", ""], ["Paterakis", "Manos", ""], ["Angelakis", "Dimitris", ""], ["Tzitzikas", "Yannis", ""], ["Bekiari", "Chrysoula", ""], ["Doerr", "Martin", ""]]}, {"id": "2107.13987", "submitter": "Tong Zhang", "authors": "Yifan Qiao, Xubin Chen, Ning Zheng, Jiangpeng Li, Yang Liu, and Tong\n  Zhang", "title": "Closing the B-tree vs. LSM-tree Write Amplification Gap on Modern\n  Storage Hardware with Built-in Transparent Compression", "comments": "12 pages, submitted to VLDB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the design of B-tree that can take full advantage of\nmodern storage hardware with built-in transparent compression. Recent years\nhave witnessed significant interest in applying log-structured merge tree\n(LSM-tree) as an alternative to B-tree. The current consensus is that, compared\nwith B-tree, LSM-tree has distinct advantages in terms of storage space\nefficiency and write amplification. This paper argues that one should revisit\nthis belief upon the arrival of storage hardware with built-in transparent\ncompression. Advanced storage appliances~(e.g., all-flash array) and emerging\ncomputational storage drives perform hardware-based lossless data compression,\ntransparent to OS and user applications. Beyond straightforwardly reducing the\nphysical storage cost difference between B-tree and LSM-tree, such modern\nstorage hardware brings new opportunities to innovate B-tree implementation in\norder to largely reduce its write amplification. As the first step to explore\nthe potential, this paper presents three simple design techniques (i.e.,\ndeterministic page shadowing, localized page modification logging, and sparse\nredo logging) that can leverage such modern storage hardware to significantly\nreduce the B-tree write amplification. We implemented these design techniques\nand carried out experiments on a commercial storage drive with built-in\ntransparent compression. The results show that the proposed design techniques\ncan reduce the B-tree write amplification by over 10x. Compared with RocksDB (a\npopular key-value store built upon LSM-tree), the implemented B-tree can\nachieve similar or even smaller write amplification and physical storage space\nusage.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 00:17:54 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Qiao", "Yifan", ""], ["Chen", "Xubin", ""], ["Zheng", "Ning", ""], ["Li", "Jiangpeng", ""], ["Liu", "Yang", ""], ["Zhang", "Tong", ""]]}, {"id": "2107.14122", "submitter": "Muhammad Cheema", "authors": "Punam Biswas, Tanzima Hashem, Muhammad Aamir Cheema", "title": "Safest Nearby Neighbor Queries in Road Networks (Full Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional route planning and $k$ nearest neighbors queries only consider\ndistance or travel time and ignore road safety altogether. However, many\ntravellers prefer to avoid risky or unpleasant road conditions such as roads\nwith high crime rates (e.g., robberies, kidnapping, riots etc.) and bumpy\nroads. To facilitate safe travel, we introduce a novel query for road networks\ncalled the $k$ safest nearby neighbors ($k$SNN) query. Given a query location\n$v_l$, a distance constraint $d_c$ and a point of interest $p_i$, we define the\nsafest path from $v_l$ to $p_i$ as the path with the highest path safety score\namong all the paths from $v_l$ to $p_i$ with length less than $d_c$. The path\nsafety score is computed considering the road safety of each road segment on\nthe path. Given a query location $v_l$, a distance constraint $d_c$ and a set\nof POIs $P$, a $k$SNN query returns $k$ POIs with the $k$ highest path safety\nscores in $P$ along with their respective safest paths from the query location.\nWe develop two novel indexing structures called $Ct$-tree and a safety score\nbased Voronoi diagram (SNVD). We propose two efficient query processing\nalgorithms each exploiting one of the proposed indexes to effectively refine\nthe search space using the properties of the index. Our extensive experimental\nstudy on real datasets demonstrates that our solution is on average an order of\nmagnitude faster than the baselines.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 15:48:12 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Biswas", "Punam", ""], ["Hashem", "Tanzima", ""], ["Cheema", "Muhammad Aamir", ""]]}]