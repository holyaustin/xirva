[{"id": "1912.00055", "submitter": "Grigorios Loukides", "authors": "Grigorios Loukides and George Theodorakopoulos", "title": "Location histogram privacy by sensitive location hiding and target\n  histogram avoidance/resemblance (extended version)", "comments": "A shorter version is to appear in Knowledge and Information Systems\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A location histogram is comprised of the number of times a user has visited\nlocations as they move in an area of interest, and it is often obtained from\nthe user in applications such as recommendation and advertising. However, a\nlocation histogram that leaves a user's computer or device may threaten privacy\nwhen it contains visits to locations that the user does not want to disclose\n(sensitive locations), or when it can be used to profile the user in a way that\nleads to price discrimination and unsolicited advertising. Our work introduces\ntwo privacy notions to protect a location histogram from these threats:\nsensitive location hiding, which aims at concealing all visits to sensitive\nlocations, and target avoidance/resemblance, which aims at concealing the\nsimilarity/dissimilarity of the user's histogram to a target histogram that\ncorresponds to an undesired/desired profile. We formulate an optimization\nproblem around each notion: Sensitive Location Hiding (SLH), which seeks to\nconstruct a histogram that is as similar as possible to the user's histogram\nbut associates all visits with nonsensitive locations, and Target\nAvoidance/Resemblance (TA/TR), which seeks to construct a histogram that is as\ndissimilar/similar as possible to a given target histogram but remains useful\nfor getting a good response from the application that analyzes the histogram.\nWe develop an optimal algorithm for each notion and also develop a greedy\nheuristic for the TA/TR problem. Our experiments demonstrate that all\nalgorithms are effective at preserving the distribution of locations in a\nhistogram and the quality of location recommendation. They also demonstrate\nthat the heuristic produces near-optimal solutions while being orders of\nmagnitude faster than the optimal algorithm for TA/TR.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:59:52 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Loukides", "Grigorios", ""], ["Theodorakopoulos", "George", ""]]}, {"id": "1912.00290", "submitter": "Yue Zhao", "authors": "Yue Zhao and Maciej K. Hryniewicki", "title": "XGBOD: Improving Supervised Outlier Detection with Unsupervised\n  Representation Learning", "comments": "Proceedings of the 2018 International Joint Conference on Neural\n  Networks (IJCNN)", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489605", "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new semi-supervised ensemble algorithm called XGBOD (Extreme Gradient\nBoosting Outlier Detection) is proposed, described and demonstrated for the\nenhanced detection of outliers from normal observations in various practical\ndatasets. The proposed framework combines the strengths of both supervised and\nunsupervised machine learning methods by creating a hybrid approach that\nexploits each of their individual performance capabilities in outlier\ndetection. XGBOD uses multiple unsupervised outlier mining algorithms to\nextract useful representations from the underlying data that augment the\npredictive capabilities of an embedded supervised classifier on an improved\nfeature space. The novel approach is shown to provide superior performance in\ncomparison to competing individual detectors, the full ensemble and two\nexisting representation learning based algorithms across seven outlier\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 00:09:10 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhao", "Yue", ""], ["Hryniewicki", "Maciej K.", ""]]}, {"id": "1912.00323", "submitter": "Vinayak Mathur", "authors": "Vinayak Mathur, Jinesh Mehta, Sanjay Singh", "title": "HCA-DBSCAN: HyperCube Accelerated Density Based Spatial Clustering for\n  Applications with Noise", "comments": "9 pages, Sets and Partitions workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density-based clustering has found numerous applications across various\ndomains. The Density-Based Spatial Clustering of Applications with Noise\n(DBSCAN) algorithm is capable of finding clusters of varied shapes that are not\nlinearly separable, at the same time it is not sensitive to outliers in the\ndata. Combined with the fact that the number of clusters in the data are not\nrequired apriori makes DBSCAN really powerfully. Slower performance (O(n2))\nlimits its applications. In this work, we present a new clustering algorithm,\nthe HyperCube Accelerated DBSCAN(HCA-DBSCAN) which uses a combination of\ndistance-based aggregation by overlaying the data with customized grids. We use\nrepresentative points to reduce the number of comparisons that need to be\ncomputed. Experimental results show that the proposed algorithm achieves a\nsignificant run time speedup of up to 58.27% when compared to other\nimprovements that try to reduce the time complexity of theDBSCAN algorithm\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 05:42:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Mathur", "Vinayak", ""], ["Mehta", "Jinesh", ""], ["Singh", "Sanjay", ""]]}, {"id": "1912.00426", "submitter": "Yang Li", "authors": "Yang Li", "title": "Area Queries Based on Voronoi Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area query, to find all elements contained in a specified area from a\ncertain set of spatial objects, is a very important spatial query widely\nrequired in various fields. A number of approaches have been proposed to\nimplement this query, the best known of which is to obtain a rough candidate\nset through spatial indexes and then refine the candidates through geometric\nvalidations to get the final result. When the shape of the query area is a\nrectangle, this method has very high efficiency. However, when the query area\nis irregular, the candidate set is usually much larger than the final result\nset, which means a lot of redundant detection needs to be done, thus the\nefficiency is greatly limited. In view of this issue, we propose a method of\niteratively generating candidates based on Voronoi diagrams and apply it to\narea queries. The experimental results indicate that with our approach, the\nnumber of candidates in the process of area query is greatly reduced and the\nefficiency of the query is significantly improved.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 15:16:46 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 17:17:09 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Li", "Yang", ""]]}, {"id": "1912.00442", "submitter": "Xinyu Fan", "authors": "Xinyu Fan, Faen Zhang, Jianfei Song, Jingming Guo, Fujie Gao", "title": "PACLP: a fine-grained partition-based access control policy language for\n  provenance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though the idea of partitioning provenance graphs for access control was\npreviously proposed, employing segments of the provenance DAG for fine-grained\naccess control to provenance data has not been thoroughly explored. Hence, we\ntake segments of a provenance graph, based on the extended OPM, and defined use\na variant of regular expressions, and utilize them in our fine-grained access\ncontrol language. It can not only return partial graphs to answer access\nrequests but also introduce segments as restrictions in order to screen\ntargeted data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 17:05:13 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 09:48:38 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Fan", "Xinyu", ""], ["Zhang", "Faen", ""], ["Song", "Jianfei", ""], ["Guo", "Jingming", ""], ["Gao", "Fujie", ""]]}, {"id": "1912.00580", "submitter": "Pulkit Misra", "authors": "Pulkit A. Misra and Jeffrey S. Chase and Johannes Gehrke and Alvin R.\n  Lebeck", "title": "Multi-version Indexing in Flash-based Key-Value Stores", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining multiple versions of data is popular in key-value stores since it\nincreases concurrency and improves performance. However, designing a\nmulti-version key-value store entails several challenges, such as additional\ncapacity for storing extra versions and an indexing mechanism for mapping\nversions of a key to their values. We present SkimpyFTL, a FTL-integrated\nmulti-version key-value store that exploits the remap-on-write property of\nflash-based SSDs for multi-versioning and provides a tradeoff between memory\ncapacity and lookup latency for indexing.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 05:05:39 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Misra", "Pulkit A.", ""], ["Chase", "Jeffrey S.", ""], ["Gehrke", "Johannes", ""], ["Lebeck", "Alvin R.", ""]]}, {"id": "1912.00736", "submitter": "Mohammadreza Fani Sani", "authors": "Mohammadreza Fani Sani, Mathilde Boltenhagen, Wil van der Aalst", "title": "Prototype Selection Based on Clustering and Conformance Metrics for\n  Model Discovery", "comments": "15 pages, 7 Figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process discovery aims at automatically creating process models on the basis\nof event data captured during the execution of business processes. Process\ndiscovery algorithms tend to use all of the event data to discover a process\nmodel. This attitude sometimes leads to discover imprecise and/or complex\nprocess models that may conceal important information of processes. To address\nthis problem, several techniques, from data filtering to model repair, have\nbeen elaborated in the literature. In this paper, we introduce a new\nincremental prototype selection algorithm based on clustering of process\ninstances. The method aims to iteratively compute a unique process model with a\ndifferent set of selected prototypes, i.e., representative of whole event data\nand stops when conformance metrics decrease. The proposed method has been\nimplemented in both the ProM and the RapidProM platforms. We applied the\nproposed method on several real event data with state-of-the-art, process\ndiscovery algorithms. Results show that using the proposed method leads to\nimprove the general quality of discovered process models.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:29:04 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sani", "Mohammadreza Fani", ""], ["Boltenhagen", "Mathilde", ""], ["van der Aalst", "Wil", ""]]}, {"id": "1912.00937", "submitter": "Ingo M\\\"uller", "authors": "Ingo M\\\"uller and Renato Marroqu\\'in and Gustavo Alonso", "title": "Lambada: Interactive Data Analytics on Cold Data using Serverless Cloud\n  Infrastructure", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3389758", "report-no": "https://doi.org/10.3929/ethz-b-000413183", "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The promise of ultimate elasticity and operational simplicity of serverless\ncomputing has recently lead to an explosion of research in this area. In the\ncontext of data analytics, the concept sounds appealing, but due to the\nlimitations of current offerings, there is no consensus yet on whether or not\nthis approach is technically and economically viable. In this paper, we\nidentify interactive data analytics on cold data as a use case where serverless\ncomputing excels. We design and implement Lambada, a system following a purely\nserverless architecture, in order to illustrate when and how serverless\ncomputing should be employed for data analytics. We propose several system\ncomponents that overcome the previously known limitations inherent in the\nserverless paradigm as well as additional ones we identify in this work. We can\nshow that, thanks to careful design, a serverless query processing system can\nbe at the same time one order of magnitude faster and two orders of magnitude\ncheaper compared to commercial Query-as-a-Service systems, the only alternative\nwith similar operational simplicity.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:07:43 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["M\u00fcller", "Ingo", ""], ["Marroqu\u00edn", "Renato", ""], ["Alonso", "Gustavo", ""]]}, {"id": "1912.01051", "submitter": "Zitao Li", "authors": "Zitao Li, Tianhao Wang, Milan Lopuha\\\"a-Zwakenberg, Boris Skoric,\n  Ninghui Li", "title": "Estimating Numerical Distributions under Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When collecting information, local differential privacy (LDP) relieves the\nconcern of privacy leakage from users' perspective, as user's private\ninformation is randomized before sent to the aggregator. We study the problem\nof recovering the distribution over a numerical domain while satisfying LDP.\nWhile one can discretize a numerical domain and then apply the protocols\ndeveloped for categorical domains, we show that taking advantage of the\nnumerical nature of the domain results in better trade-off of privacy and\nutility. We introduce a new reporting mechanism, called the square wave SW\nmechanism, which exploits the numerical nature in reporting. We also develop an\nExpectation Maximization with Smoothing (EMS) algorithm, which is applied to\naggregated histograms from the SW mechanism to estimate the original\ndistributions. Extensive experiments demonstrate that our proposed approach, SW\nwith EMS, consistently outperforms other methods in a variety of utility\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 19:26:11 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Li", "Zitao", ""], ["Wang", "Tianhao", ""], ["Lopuha\u00e4-Zwakenberg", "Milan", ""], ["Skoric", "Boris", ""], ["Li", "Ninghui", ""]]}, {"id": "1912.01059", "submitter": "Patrick Wieschollek", "authors": "Fabian Groh, Lukas Ruppert, Patrick Wieschollek, Hendrik P.A. Lensch", "title": "GGNN: Graph-based GPU Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbor (ANN) search in high dimensions is an integral\npart of several computer vision systems and gains importance in deep learning\nwith explicit memory representations. Since PQT and FAISS started to leverage\nthe massive parallelism offered by GPUs, GPU-based implementations are a\ncrucial resource for today's state-of-the-art ANN methods. While most of these\nmethods allow for faster queries, less emphasis is devoted to accelerate the\nconstruction of the underlying index structures. In this paper, we propose a\nnovel search structure based on nearest neighbor graphs and information\npropagation on graphs. Our method is designed to take advantage of GPU\narchitectures to accelerate the hierarchical building of the index structure\nand for performing the query. Empirical evaluation shows that GGNN\nsignificantly surpasses the state-of-the-art GPU- and CPU-based systems in\nterms of build-time, accuracy and search speed.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 19:46:13 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 08:15:19 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 15:49:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Groh", "Fabian", ""], ["Ruppert", "Lukas", ""], ["Wieschollek", "Patrick", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1912.01519", "submitter": "Martin Thomas Horsch", "authors": "Martin Thomas Horsch and Silvia Chiacchiera and Michael A. Seaton and\n  Ilian T. Todorov and Karel \\v{S}indelka and Martin L\\'isal and Barbara\n  Andreon and Esteban Bayro Kaiser and Gabriele Mogni and Gerhard Goldbeck and\n  Ralf Kunze and Georg Summer and Andreas Fiseni and Hauke Br\\\"uning and Peter\n  Schiffels and Welchy Leite Cavalcanti", "title": "Ontologies for the Virtual Materials Marketplace", "comments": "The Virtual Materials Marketplace (VIMMP) project is funded from the\n  European Union's Horizon 2020 research and innovation programme under grant\n  agreement no. 760907", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Virtual Materials Marketplace (VIMMP) project, which develops an open\nplatform for providing and accessing services related to materials modelling,\nis presented with a focus on its ontology development and data technology\naspects. Within VIMMP, a system of marketplace-level ontologies is developed to\ncharacterize services, models, and interactions between users; the European\nMaterials and Modelling Ontology (EMMO) is employed as a top-level ontology.\nThe ontologies are used to annotate data that are stored in the ZONTAL Space\ncomponent of VIMMP and to support the ingest and retrieval of data and metadata\nat the VIMMP marketplace frontend.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:48:05 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 14:08:24 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Horsch", "Martin Thomas", ""], ["Chiacchiera", "Silvia", ""], ["Seaton", "Michael A.", ""], ["Todorov", "Ilian T.", ""], ["\u0160indelka", "Karel", ""], ["L\u00edsal", "Martin", ""], ["Andreon", "Barbara", ""], ["Kaiser", "Esteban Bayro", ""], ["Mogni", "Gabriele", ""], ["Goldbeck", "Gerhard", ""], ["Kunze", "Ralf", ""], ["Summer", "Georg", ""], ["Fiseni", "Andreas", ""], ["Br\u00fcning", "Hauke", ""], ["Schiffels", "Peter", ""], ["Cavalcanti", "Welchy Leite", ""]]}, {"id": "1912.01668", "submitter": "Jialin Ding", "authors": "Vikram Nathan, Jialin Ding, Mohammad Alizadeh, Tim Kraska", "title": "Learning Multi-dimensional Indexes", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3380579", "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scanning and filtering over multi-dimensional tables are key operations in\nmodern analytical database engines. To optimize the performance of these\noperations, databases often create clustered indexes over a single dimension or\nmulti-dimensional indexes such as R-trees, or use complex sort orders (e.g.,\nZ-ordering). However, these schemes are often hard to tune and their\nperformance is inconsistent across different datasets and queries. In this\npaper, we introduce Flood, a multi-dimensional in-memory index that\nautomatically adapts itself to a particular dataset and workload by jointly\noptimizing the index structure and data storage. Flood achieves up to three\norders of magnitude faster performance for range scans with predicates than\nstate-of-the-art multi-dimensional indexes or sort orders on real-world\ndatasets and workloads. Our work serves as a building block towards an\nend-to-end learned database system.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:10:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Nathan", "Vikram", ""], ["Ding", "Jialin", ""], ["Alizadeh", "Mohammad", ""], ["Kraska", "Tim", ""]]}, {"id": "1912.01861", "submitter": "Siddharth Dawar", "authors": "Anuj S. Saxena, Siddharth Dawar, Vikram Goyal, Debajyoti Bera", "title": "Mining Top-k Trajectory-Patterns from Anonymized Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ubiquity of GPS enabled devices result into the generation of an enormous\namount of user movement data consisting of a sequence of locations together\nwith activities performed at those locations. Such data, commonly known as {\\it\nactivity-trajectory data}, can be analysed to find common user movements and\npreferred activities, which will have tremendous business value. However,\nvarious countries have now introduced data privacy regulations that make it\nmandatory to anonymize any data before releasing it. This makes it difficult to\nlook for patterns as the existing mining techniques may not be directly\napplicable over anonymized data. User locations in an activity-trajectory\ndataset are anonymized to regions of different shapes and sizes making them\nuncomparable; therefore, it is unclear how to define suitable patterns over\nthose regions. In this paper, we propose a top-k pattern mining technique\ncalled TopKMintra that employs a pre-processing step to transform anonymized\nactivity-trajectory into an intermediate representation that address the above\nproblem. Unlike usual sequential data, activity-trajectory data is\n2-dimensional that can lead to generation of duplicate patterns. To handle\nthis, TopKMintra restricts arbitrary extensions in the two dimensions by\nimposing an order over the search space; this also helps in addressing the\ncommon problem of updating the threshold in top-k pattern mining algorithms\nduring various stages. We perform extensive experiments on real datasets to\ndemonstrate the efficiency and the effectiveness of TopKMintra. Our results\nshow that even after anonymization, certain patterns may remain in a dataset\nand those could be mined efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 09:19:07 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Saxena", "Anuj S.", ""], ["Dawar", "Siddharth", ""], ["Goyal", "Vikram", ""], ["Bera", "Debajyoti", ""]]}, {"id": "1912.02127", "submitter": "Harsh Thakkar", "authors": "Renzo Angles, Harsh Thakkar, Dominik Tomaszuk", "title": "Directly Mapping RDF Databases to Property Graph Databases", "comments": "This work has been accepted and published at the IEEE Access Journal\n  DOI: 10.1109/ACCESS.2020.2993117", "journal-ref": "IEEE Access Volume 8, 2020", "doi": "10.1109/ACCESS.2020.2993117", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  RDF triplestores and property graph databases are two approaches for data\nmanagement which are based on modeling, storing, and querying graph-like data.\nIn spite of such common principles, they present special features that\ncomplicate the task of database interoperability. While there exist some\nmethods to transform RDF graphs into property graphs, and vice versa, they lack\ncompatibility and a solid formal foundation. This paper presents three direct\nmappings (schema-dependent and schema-independent) for transforming an RDF\ndatabase into a property graph database, including data and schema. We show\nthat two of the proposed mappings satisfy the properties of semantics\npreservation and information preservation. The existence of both mappings\nallows us to conclude that the property graph data model subsumes the\ninformation capacity of the RDF data model.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 17:20:33 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 18:43:11 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Angles", "Renzo", ""], ["Thakkar", "Harsh", ""], ["Tomaszuk", "Dominik", ""]]}, {"id": "1912.02651", "submitter": "Maede Zolanvari", "authors": "Maede Zolanvari, Marcio A. Teixeira, Raj Jain", "title": "Effect of Imbalanced Datasets on Security of Industrial IoT Using\n  Machine Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.05771", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms have been shown to be suitable for securing\nplatforms for IT systems. However, due to the fundamental differences between\nthe industrial internet of things (IIoT) and regular IT networks, a special\nperformance review needs to be considered. The vulnerabilities and security\nrequirements of IIoT systems demand different considerations. In this paper, we\nstudy the reasons why machine learning must be integrated into the security\nmechanisms of the IIoT, and where it currently falls short in having a\nsatisfactory performance. The challenges and real-world considerations\nassociated with this matter are studied in our experimental design. We use an\nIIoT testbed resembling a real industrial plant to show our proof of concept.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 20:16:47 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Zolanvari", "Maede", ""], ["Teixeira", "Marcio A.", ""], ["Jain", "Raj", ""]]}, {"id": "1912.02947", "submitter": "Zhaoqiang Chen", "authors": "Zhaoqiang Chen, Qun Chen, Boyi Hou, Tianyi Duan, Zhanhuai Li and\n  Guoliang Li", "title": "Towards Interpretable and Learnable Risk Analysis for Entity Resolution", "comments": "14 pages, Accepted to SIGMOD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learning-based entity resolution has been widely studied. However,\nsome entity pairs may be mislabeled by machine learning models and existing\nstudies do not study the risk analysis problem -- predicting and interpreting\nwhich entity pairs are mislabeled. In this paper, we propose an interpretable\nand learnable framework for risk analysis, which aims to rank the labeled pairs\nbased on their risks of being mislabeled. We first describe how to\nautomatically generate interpretable risk features, and then present a\nlearnable risk model and its training technique. Finally, we empirically\nevaluate the performance of the proposed approach on real data. Our extensive\nexperiments have shown that the learning risk model can identify the mislabeled\npairs with considerably higher accuracy than the existing alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 01:59:43 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Chen", "Zhaoqiang", ""], ["Chen", "Qun", ""], ["Hou", "Boyi", ""], ["Duan", "Tianyi", ""], ["Li", "Zhanhuai", ""], ["Li", "Guoliang", ""]]}, {"id": "1912.03417", "submitter": "Wei Zhang", "authors": "Wei Zhang, Hao Wei, Bunyamin Sisman, Xin Luna Dong, Christos\n  Faloutsos, David Page", "title": "AutoBlock: A Hands-off Blocking Framework for Entity Matching", "comments": "In The Thirteenth ACM International Conference on Web Search and Data\n  Mining (WSDM '20), February 3-7, 2020, Houston, TX, USA. ACM, Anchorage,\n  Alaska, USA , 9 pages", "journal-ref": null, "doi": "10.1145/3336191.3371813", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity matching seeks to identify data records over one or multiple data\nsources that refer to the same real-world entity. Virtually every entity\nmatching task on large datasets requires blocking, a step that reduces the\nnumber of record pairs to be matched. However, most of the traditional blocking\nmethods are learning-free and key-based, and their successes are largely built\non laborious human effort in cleaning data and designing blocking keys.\n  In this paper, we propose AutoBlock, a novel hands-off blocking framework for\nentity matching, based on similarity-preserving representation learning and\nnearest neighbor search. Our contributions include: (a) Automation: AutoBlock\nfrees users from laborious data cleaning and blocking key tuning. (b)\nScalability: AutoBlock has a sub-quadratic total time complexity and can be\neasily deployed for millions of records. (c) Effectiveness: AutoBlock\noutperforms a wide range of competitive baselines on multiple large-scale,\nreal-world datasets, especially when datasets are dirty and/or unstructured.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 02:42:48 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhang", "Wei", ""], ["Wei", "Hao", ""], ["Sisman", "Bunyamin", ""], ["Dong", "Xin Luna", ""], ["Faloutsos", "Christos", ""], ["Page", "David", ""]]}, {"id": "1912.03443", "submitter": "Dawei Huang", "authors": "Dawei Huang, Dong Young Yoon, Seth Pettie, Barzan Mozafari", "title": "Joins on Samples: A Theoretical Guide for Practitioners", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite decades of research on approximate query processing (AQP), our\nunderstanding of sample-based joins has remained limited and, to some extent,\neven superficial. The common belief in the community is that joining random\nsamples is futile. This belief is largely based on an early result showing that\nthe join of two uniform samples is not an independent sample of the original\njoin, and that it leads to quadratically fewer output tuples. However,\nunfortunately, this result has little applicability to the key questions\npractitioners face. For example, the success metric is often the final\napproximation's accuracy, rather than output cardinality. Moreover, there are\nmany non-uniform sampling strategies that one can employ. Is sampling for joins\nstill futile in all of these settings? If not, what is the best sampling\nstrategy in each case? To the best of our knowledge, there is no formal study\nanswering these questions.\n  This paper aims to improve our understanding of sample-based joins and offer\na guideline for practitioners building and using real-world AQP systems. We\nstudy limitations of offline samples in approximating join queries: given an\noffline sampling budget, how well can one approximate the join of two tables?\nWe answer this question for two success metrics: output size and estimator\nvariance. We show that maximizing output size is easy, while there is an\ninformation-theoretical lower bound on the lowest variance achievable by any\nsampling strategy. We then define a hybrid sampling scheme that captures all\ncombinations of stratified, universe, and Bernoulli sampling, and show that\nthis scheme with our optimal parameters achieves the theoretical lower bound\nwithin a constant factor. Since computing these optimal parameters requires\nshuffling statistics across the network, we also propose a decentralized\nvariant where each node acts autonomously using minimal statistics.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 05:20:43 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 19:06:42 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 21:52:02 GMT"}, {"version": "v4", "created": "Fri, 24 Jan 2020 21:33:23 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Huang", "Dawei", ""], ["Yoon", "Dong Young", ""], ["Pettie", "Seth", ""], ["Mozafari", "Barzan", ""]]}, {"id": "1912.03854", "submitter": "Ramy Shahin", "authors": "Ramy Shahin, Marsha Chechik", "title": "Variability-aware Datalog", "comments": "PADL'20 paper", "journal-ref": null, "doi": "10.1007/978-3-030-39197-3_14", "report-no": null, "categories": "cs.PL cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variability-aware computing is the efficient application of programs to\ndifferent sets of inputs that exhibit some variability. One example is program\nanalyses applied to Software Product Lines (SPLs). In this paper we present the\ndesign and development of a variability-aware version of the Souffl\\'{e}\nDatalog engine. The engine can take facts annotated with Presence Conditions\n(PCs) as input, and compute the PCs of its inferred facts, eliminating facts\nthat do not exist in any valid configuration. We evaluate our variability-aware\nSouffl\\'{e} implementation on several fact sets annotated with PCs to measure\nthe associated overhead in terms of processing time and database size.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 05:05:07 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Shahin", "Ramy", ""], ["Chechik", "Marsha", ""]]}, {"id": "1912.04648", "submitter": "Jonas Traub", "authors": "Jonas Traub (1 and 2), Julius H\\\"ulsmann (1), Sebastian Bre{\\ss} (1),\n  Tilmann Rabl (3), Volker Markl (1 and 2) ((1) Technische Universit\\\"at\n  Berlin, (2) German Research Center for Artificial Intelligence (DFKI), (3)\n  Hasso-Plattner-Institut Potsdam (HPI))", "title": "SENSE: Scalable Data Acquisition from Distributed Sensors with\n  Guaranteed Time Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis in the Internet of Things (IoT) requires us to combine event\nstreams from a huge amount of sensors. This combination (join) of events is\nusually based on the time stamps associated with the events. We address two\nchallenges in environments which acquire and join events in the IoT: First, due\nto the growing number of sensors, we are facing the performance limits of\ncentral joins with respect to throughput, latency, and network utilization.\nSecond, in the IoT, diverse sensor nodes are operated by different\norganizations and use different time synchronization techniques. Thus, events\nwith the same timestamps are not necessarily recorded at the exact same time\nand joined data tuples have an unknown time incoherence. This can cause\nundetected failures, such as false correlations and wrong predictions. We\npresent SENSE, a system for scalable data acquisition from distributed sensors.\nSENSE introduces time coherence measures as a fundamental data characteristic\nin addition to common time synchronization techniques. The time coherence of a\ndata tuple is the time span in which all values contained in the tuple have\nbeen read from sensors. We explore concepts and algorithms to quantify and\noptimize time coherence and show that SENSE scales to thousands of sensors,\noperates efficiently under latency and coherence constraints, and adapts to\nchanging network conditions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 11:17:15 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Traub", "Jonas", "", "1 and 2"], ["H\u00fclsmann", "Julius", "", "1 and 2"], ["Bre\u00df", "Sebastian", "", "1 and 2"], ["Rabl", "Tilmann", "", "1 and 2"], ["Markl", "Volker", "", "1 and 2"]]}, {"id": "1912.04820", "submitter": "Felix Martin Schuhknecht", "authors": "Felix Martin Schuhknecht and Ankur Sharma and Jens Dittrich and Divya\n  Agrawal", "title": "ChainifyDB: How to Blockchainify any Data Management System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Today's permissioned blockchain systems come in a stand-alone fashion and\nrequire the users to integrate yet another full-fledged transaction processing\nsystem into their already complex data management landscape. This seems odd as\nblockchains and traditional DBMSs share large parts of their processing stack.\nThus, rather than replacing the established data systems altogether, we\nadvocate to simply 'chainify' them with a blockchain layer on top.\n  Unfortunately, this task is far more challenging than it sounds: As we want\nto build upon heterogeneous transaction processing systems, which potentially\nbehave differently, we cannot rely on every organization to execute every\ntransaction deterministically in the same way. Further, as these systems are\nalready filled with data and being used by top-level applications, we also\ncannot rely on every organization being resilient against tampering with its\nlocal data.\n  Therefore, in this work, we will drop these assumptions and introduce a\npowerful processing model that avoids them in the first place: The so-called\nWhatever-LedgerConsensus (WLC) model allows us to create a highly flexible\npermissioned blockchain layer coined ChainifyDB that (a) is centered around\nbullet-proof database technology, (b) makes even stronger guarantees than\nexisting permissioned systems, (c) provides a sophisticated recovery mechanism,\n(d) has an up to 6x higher throughput than the permissioned blockchain system\nFabric, and (e) can easily be integrated into an existing heterogeneous\ndatabase landscape.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 16:55:46 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Schuhknecht", "Felix Martin", ""], ["Sharma", "Ankur", ""], ["Dittrich", "Jens", ""], ["Agrawal", "Divya", ""]]}, {"id": "1912.06110", "submitter": "Dominik D. Freydenberger", "authors": "Dominik D. Freydenberger and Liat Peterfreund", "title": "The theory of concatenation over finite models", "comments": "Update to make this version consistent with conference version (ICALP\n  2021), which renamed Datasplog to FC-Datalog", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose FC, a new logic on words that combines finite model theory with\nthe theory of concatenation - a first-order logic that is based on word\nequations. Like the theory of concatenation, FC is built around word equations;\nin contrast to it, its semantics are defined to only allow finite models, by\nlimiting the universe to a word and all its factors. As a consequence of this,\nFC has many of the desirable properties of FO on finite models, while being far\nmore expressive than FO[<]. Most noteworthy among these desirable properties\nare sufficient criteria for efficient model checking, and capturing various\ncomplexity classes by adding operators for transitive closures or fixed points.\n  Not only does FC allow us to obtain new insights and techniques for\nexpressive power and efficient evaluation of document spanners, but it also\nprovides a general framework for logic on words that also has potential\napplications in other areas.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 18:18:55 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 14:18:54 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 18:23:35 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 13:52:43 GMT"}, {"version": "v5", "created": "Thu, 11 Mar 2021 12:42:24 GMT"}, {"version": "v6", "created": "Thu, 13 May 2021 16:38:17 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Freydenberger", "Dominik D.", ""], ["Peterfreund", "Liat", ""]]}, {"id": "1912.06194", "submitter": "Jens D\\\"orpinghaus", "authors": "Jens D\\\"orpinghaus, Alexander Apke, Vanessa Lage-Rupprecht, Andreas\n  Stefan", "title": "Data Exploration and Validation on dense knowledge graphs for biomedical\n  research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present a holistic approach for data exploration on dense knowledge\ngraphs as a novel approach with a proof-of-concept in biomedical research.\nKnowledge graphs are increasingly becoming a vital factor in knowledge mining\nand discovery as they connect data using technologies from the semantic web. In\nthis paper we extend a basic knowledge graph extracted from biomedical\nliterature by context data like named entities and relations obtained by text\nmining and other linked data sources like ontologies and databases. We will\npresent an overview about this novel network. The aim of this work was to\nextend this current knowledge with approaches from graph theory. This method\nwill build the foundation for quality control, validation of hypothesis,\ndetection of missing data and time series analysis of biomedical knowledge in\ngeneral. In this context we tried to apply multiple-valued decision diagrams to\nthese questions. In addition this knowledge representation of linked data can\nbe used as FAIR approach to answer semantic questions. This paper sheds new\nlights on dense and very large knowledge graphs and the importance of a\ngraph-theoretic understanding of these networks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 12:52:35 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["D\u00f6rpinghaus", "Jens", ""], ["Apke", "Alexander", ""], ["Lage-Rupprecht", "Vanessa", ""], ["Stefan", "Andreas", ""]]}, {"id": "1912.06229", "submitter": "Tao Zhang", "authors": "Tao Zhang, Quanyan Zhu", "title": "Optimal Two-Sided Market Mechanism Design for Large-Scale Data Sharing\n  and Trading in Massive IoT Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of the Internet of Things (IoT) generates a significant\namount of data that contains valuable knowledge for system operations and\nbusiness opportunities. Since the data is the property of the IoT data owners,\nthe access to the data requires permission from the data owners, which gives\nrise to a potential market opportunity for the IoT data sharing and trading to\ncreate economic values and market opportunities for both data owners and\nbuyers. In this work, we leverage optimal mechanism design theory to develop a\nmonopolist matching platform for data trading over massive IoT networks. The\nproposed mechanism is composed of a pair of matching and payment rules for each\nside of the market. We analyze the incentive compatibility of the market and\ncharacterize the optimal mechanism with a class of cut-off matching rules for\nboth welfare-maximization and revenue-maximization mechanisms and study three\nmatching behaviors including complete-matched, bottom-eliminated, and\ntop-reserved.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 21:35:55 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 07:37:54 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zhang", "Tao", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1912.06255", "submitter": "Yiqiu Wang", "authors": "Yiqiu Wang, Yan Gu, Julian Shun", "title": "Theoretically-Efficient and Practical Parallel DBSCAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DBSCAN method for spatial clustering has received significant attention\ndue to its applicability in a variety of data analysis tasks. There are fast\nsequential algorithms for DBSCAN in Euclidean space that take $O(n\\log n)$ work\nfor two dimensions, sub-quadratic work for three or more dimensions, and can be\ncomputed approximately in linear work for any constant number of dimensions.\nHowever, existing parallel DBSCAN algorithms require quadratic work in the\nworst case, making them inefficient for large datasets. This paper bridges the\ngap between theory and practice of parallel DBSCAN by presenting new parallel\nalgorithms for Euclidean exact DBSCAN and approximate DBSCAN that match the\nwork bounds of their sequential counterparts, and are highly parallel\n(polylogarithmic depth). We present implementations of our algorithms along\nwith optimizations that improve their practical performance. We perform a\ncomprehensive experimental evaluation of our algorithms on a variety of\ndatasets and parameter settings. Our experiments on a 36-core machine with\nhyper-threading show that we outperform existing parallel DBSCAN\nimplementations by up to several orders of magnitude, and achieve speedups by\nup to 33x over the best sequential algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:09:20 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 19:41:12 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2020 21:10:23 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2021 23:55:26 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wang", "Yiqiu", ""], ["Gu", "Yan", ""], ["Shun", "Julian", ""]]}, {"id": "1912.06415", "submitter": "Sudhakar Singh", "authors": "Pankaj Singh, Sudhakar Singh, P. K. Mishra, Rakhi Garg", "title": "RDD-Eclat: Approaches to Parallelize Eclat Algorithm on Spark RDD\n  Framework", "comments": "16 pages, 6 figures, ICCNCT 2019", "journal-ref": "ICCNCT 2019, LNDECT 44", "doi": "10.1007/978-3-030-37051-0_85", "report-no": "ICCNCT-171", "categories": "cs.DC cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initially, a number of frequent itemset mining (FIM) algorithms have been\ndesigned on the Hadoop MapReduce, a distributed big data processing framework.\nBut, due to heavy disk I/O, MapReduce is found to be inefficient for such\nhighly iterative algorithms. Therefore, Spark, a more efficient distributed\ndata processing framework, has been developed with in-memory computation and\nresilient distributed dataset (RDD) features to support the iterative\nalgorithms. On the Spark RDD framework, Apriori and FP-Growth based FIM\nalgorithms have been designed, but Eclat-based algorithm has not been explored\nyet. In this paper, RDD-Eclat, a parallel Eclat algorithm on the Spark RDD\nframework is proposed with its five variants. The proposed algorithms are\nevaluated on the various benchmark datasets, which shows that RDD-Eclat\noutperforms the Spark-based Apriori by many times. Also, the experimental\nresults show the scalability of the proposed algorithms on increasing the\nnumber of cores and size of the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:23:47 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Singh", "Pankaj", ""], ["Singh", "Sudhakar", ""], ["Mishra", "P. K.", ""], ["Garg", "Rakhi", ""]]}, {"id": "1912.07001", "submitter": "Xinyi Yu", "authors": "Sai Wu, Xinyi Yu, Xiaojie Feng, Feifei Li, Wei Cao, Gang Chen", "title": "Progressive Neural Index Search for Database System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a key ingredient of the DBMS, index plays an important role in the query\noptimization and processing. However, it is a non-trivial task to apply\nexisting indexes or design new indexes for new applications, where both data\ndistribution and query distribution are unknown. To address the issue, we\npropose a new indexing approach, NIS (Neural Index Search), which searches for\nthe optimal index parameters and structures using a neural network. In\nparticular, NIS is capable for building a tree-like index automatically for an\narbitrary column that can be sorted/partitioned using a customized function.\nThe contributions of NIS are twofold. First, NIS constructs a tree-like index\nin a layer-by-layer way via formalizing the index structure as abstract ordered\nand unordered blocks. Ordered blocks are implemented using B+-tree nodes or\nskip lists, while unordered blocks adopt hash functions with different\nconfigurations. Second, all parameters of the building blocks (e.g., fanout of\nB+-tree node, bucket number of hash function and etc.) are tuned by NIS\nautomatically. We achieve the two goals for a given workload and dataset with\none RNN-powered reinforcement learning model. Experiments show that the\nauto-tuned index built by NIS can achieve a better performance than the\nstate-of-the-art index.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 08:39:27 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 16:17:13 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Wu", "Sai", ""], ["Yu", "Xinyi", ""], ["Feng", "Xiaojie", ""], ["Li", "Feifei", ""], ["Cao", "Wei", ""], ["Chen", "Gang", ""]]}, {"id": "1912.07091", "submitter": "Omid Jafari", "authors": "Omid Jafari, Khandker Mushfiqul Islam, Parth Nagarkar", "title": "Drawbacks and Proposed Solutions for Real-time Processing on Existing\n  State-of-the-art Locality Sensitive Hashing Techniques", "comments": "Accepted and Presented at the 5th International Conference on Signal\n  and Image Processing (SIGI-2019), Dubai, UAE", "journal-ref": null, "doi": "10.5121/csit.2019.91410", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest-neighbor query processing is a fundamental operation for many image\nretrieval applications. Often, images are stored and represented by\nhigh-dimensional vectors that are generated by feature-extraction algorithms.\nSince tree-based index structures are shown to be ineffective for high\ndimensional processing due to the well-known \"Curse of Dimensionality\",\napproximate nearest neighbor techniques are used for faster query processing.\nLocality Sensitive Hashing (LSH) is a very popular and efficient approximate\nnearest neighbor technique that is known for its sublinear query processing\ncomplexity and theoretical guarantees. Nowadays, with the emergence of\ntechnology, several diverse application domains require real-time\nhigh-dimensional data storing and processing capacity. Existing LSH techniques\nare not suitable to handle real-time data and queries. In this paper, we\ndiscuss the challenges and drawbacks of existing LSH techniques for processing\nreal-time high-dimensional image data. Additionally, through experimental\nanalysis, we propose improvements for existing state-of-the-art LSH techniques\nfor efficient processing of high-dimensional image data.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 19:04:19 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Jafari", "Omid", ""], ["Islam", "Khandker Mushfiqul", ""], ["Nagarkar", "Parth", ""]]}, {"id": "1912.07172", "submitter": "Shuyan Zhang", "authors": "Yuming Li, Rong Zhang, Yuchen Li, Ke Shu, Shuyan Zhang, Aoying Zhou", "title": "Lauca: Generating Application-Oriented Synthetic Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The synthetic workload is essential and critical to the performance\nevaluation of database systems. When evaluating the database performance for a\nspecific application, the similarity between synthetic workload and real\napplication workload determines the credibility of evaluation results. However,\nthe workload currently used for performance evaluation is difficult to have the\nsame workload characteristics as the target application, which leads to\ninaccurate evaluation results. To address this problem, we propose a workload\nduplicator (Lauca) that can generate synthetic workloads with highly similar\nperformance metrics for specific applications. To the best of our knowledge,\nLauca is the first application-oriented transactional workload generator. By\ncarefully studying the application-oriented synthetic workload generation\nproblem, we present the key workload characteristics (transaction logic and\ndata access distribution) of online transaction processing (OLTP) applications,\nand propose novel workload characterization and generation algorithms, which\nguarantee the high fidelity of synthetic workloads. We conduct extensive\nexperiments using workloads from TPC-C, SmallBank and micro benchmarks on both\nMySQL and PostgreSQL databases, and experimental results show that Lauca\nconsistently generates high-quality synthetic workloads.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 03:13:17 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Li", "Yuming", ""], ["Zhang", "Rong", ""], ["Li", "Yuchen", ""], ["Shu", "Ke", ""], ["Zhang", "Shuyan", ""], ["Zhou", "Aoying", ""]]}, {"id": "1912.07650", "submitter": "Alexander Hayes", "authors": "Alexander L. Hayes and Mayukh Das and Phillip Odom and Sriraam\n  Natarajan", "title": "User Friendly Automatic Construction of Background Knowledge: Mode\n  Construction from ER Diagrams", "comments": "8 pages. Published in Proceedings of the Knowledge Capture\n  Conference, 2017", "journal-ref": "Proceedings of the Knowledge Capture Conference (2017) 30:1-30:8", "doi": "10.1145/3148011.3148027", "report-no": null, "categories": "cs.AI cs.DB cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  One of the key advantages of Inductive Logic Programming systems is the\nability of the domain experts to provide background knowledge as modes that\nallow for efficient search through the space of hypotheses. However, there is\nan inherent assumption that this expert should also be an ILP expert to provide\neffective modes. We relax this assumption by designing a graphical user\ninterface that allows the domain expert to interact with the system using\nEntity Relationship diagrams. These interactions are used to construct modes\nfor the learning system. We evaluate our algorithm on a probabilistic logic\nlearning system where we demonstrate that the user is able to construct\neffective background knowledge on par with the expert-encoded knowledge on five\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 19:30:57 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Hayes", "Alexander L.", ""], ["Das", "Mayukh", ""], ["Odom", "Phillip", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1912.07777", "submitter": "Laurel Orr", "authors": "Laurel Orr, Samuel Ainsworth, Walter Cai, Kevin Jamieson, Magda\n  Balazinska, Dan Suciu", "title": "Mosaic: A Sample-Based Database System for Open World Query Processing", "comments": "CIDR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data scientists have relied on samples to analyze populations of interest for\ndecades. Recently, with the increase in the number of public data repositories,\nsample data has become easier to access. It has not, however, become easier to\nanalyze. This sample data is arbitrarily biased with an unknown sampling\nprobability, meaning data scientists must manually debias the sample with\ncustom techniques to avoid inaccurate results. In this vision paper, we propose\nMosaic, a database system that treats samples as first-class citizens and\nallows users to ask questions over populations represented by these samples.\nAnswering queries over biased samples is non-trivial as there is no existing,\nstandard technique to answer population queries when the sampling probability\nis unknown. In this paper, we show how our envisioned system solves this\nproblem by having a unique sample-based data model with extensions to the SQL\nlanguage. We propose how to perform population query answering using biased\nsamples and give preliminary results for one of our novel query answering\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 01:34:05 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 19:46:28 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 20:48:14 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Orr", "Laurel", ""], ["Ainsworth", "Samuel", ""], ["Cai", "Walter", ""], ["Jamieson", "Kevin", ""], ["Balazinska", "Magda", ""], ["Suciu", "Dan", ""]]}, {"id": "1912.08010", "submitter": "Gabriela Montoya", "authors": "Gabriela Montoya, Ilkcan Keles, Katja Hose", "title": "Querying Linked Data: An Experimental Evaluation of State-of-the-Art\n  Interfaces", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of Semantic Web technologies, and in particular the Open Data\ninitiative, has contributed to the steady growth of the number of datasets and\ntriples accessible on the Web. Most commonly, queries over RDF data are\nevaluated over SPARQL endpoints. Recently, however, alternatives such as TPF\nhave been proposed with the goal of shifting query processing load from the\nserver running the SPARQL endpoint towards the client that issued the query.\nAlthough these interfaces have been evaluated against standard benchmarks and\ntestbeds that showed their benefits over previous work in general, a\nfine-granular evaluation of what types of queries exploit the strengths of the\ndifferent available interfaces has never been done. In this paper, we present\nthe results of our in-depth evaluation of existing RDF interfaces. In addition,\nwe also examine the influence of the backend on the performance of these\ninterfaces. Using representative and diverse query loads based on the query log\nof a public SPARQL endpoint, we stress test the different interfaces and\nbackends and identify their strengths and weaknesses.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 13:45:10 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Montoya", "Gabriela", ""], ["Keles", "Ilkcan", ""], ["Hose", "Katja", ""]]}, {"id": "1912.08026", "submitter": "Michael R\\\"oder", "authors": "Michael R\\\"oder and Geraldo de Souza and Denis Kuchelev and\n  Abdelmoneim Amer Desouki and Axel-Cyrille Ngonga Ngomo", "title": "ORCA: a Benchmark for Data Web Crawlers", "comments": "8 pages, submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of RDF knowledge graphs available on the Web grows constantly.\nGathering these graphs at large scale for downstream applications hence\nrequires the use of crawlers. Although Data Web crawlers exist, and general Web\ncrawlers could be adapted to focus on the Data Web, there is currently no\nbenchmark to fairly evaluate their performance. Our work closes this gap by\npresenting the Orca benchmark. Orca generates a synthetic Data Web, which is\ndecoupled from the original Web and enables a fair and repeatable comparison of\nData Web crawlers. Our evaluations show that Orca can be used to reveal the\ndifferent advantages and disadvantages of existing crawlers. The benchmark is\nopen-source and available at https://github.com/dice-group/orca.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 14:08:01 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 17:18:08 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["R\u00f6der", "Michael", ""], ["de Souza", "Geraldo", ""], ["Kuchelev", "Denis", ""], ["Desouki", "Abdelmoneim Amer", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1912.08322", "submitter": "Lu Chen", "authors": "Lu Chen, Chengfei Liu, Rui Zhou, Jiajie Xu, Jianxin Li", "title": "Finding Effective Geo-Social Group for Impromptu Activity with Multiple\n  Demands", "comments": "12 Pages with 1 Page References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-social group search aims to find a group of people proximate to a\nlocation while socially related. One of the driven applications for geo-social\ngroup search is organizing an impromptu activity. This is because the social\ncohesiveness of a found geo-social group ensures a good communication\natmosphere and the spatial closeness of the geo-social group reduces the\npreparation time for the activity. Most existing works treat geo-social group\nsearch as a problem that finds a group satisfying a single social constraint\nwhile optimizing the spatial proximity. However, when an impromptu activity has\nadditional demands on attendees, e.g., the activity requires that the attendees\nhave certain set of skills, the existing works cannot find an effective\ngeo-social group efficiently. In this paper, we study how to find a group that\nis most proximate to a query location while satisfying multiple constraints.\nSpecifically, the multiple constraints on which we focus include social\nconstraint, size constraint and keyword constraint. We propose a novel search\nframework which first effectively narrows down the search space with\ntheoretical guarantees and then efficiently finds the optimum result. Although\nour model considers multiple constraints, novel techniques devised in this\npaper ensure that search cost is equivalent to parameterized constant times of\none time social constraint checking on a vastly restricted search space. We\nconduct extensive experiments on both real and semi-synthetic datasets for\ndemonstrating the efficiency of the proposed search algorithm. To evaluate the\neffectiveness, we conduct two case studies on real datasets, demonstrating the\nsuperiority of our proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 00:01:27 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Chen", "Lu", ""], ["Liu", "Chengfei", ""], ["Zhou", "Rui", ""], ["Xu", "Jiajie", ""], ["Li", "Jianxin", ""]]}, {"id": "1912.08352", "submitter": "Witold Litwin", "authors": "Witold Litwin", "title": "Manifesto for Improved Foundations of Relational Model", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized relations extended with inherited attributes can be more faithful\nto reality and support logical navigation free queries, properties available at\npresent only through specific views. Adding inherited attributes can be\nnonetheless always less procedural than to define any such views. Present\nschemes should even typically suffice for relations with foreign keys.\nImplementing extended relations on popular DBSs appears also simple. Relational\nmodel should evolve accordingly, for benefit of likely millions of DBAs,\nclients, developers.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 21:28:44 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Litwin", "Witold", ""]]}, {"id": "1912.08454", "submitter": "Yaxing Chen", "authors": "Yaxing Chen, Qinghua Zheng, Dan Liu, Zheng Yan, Wenhai Sun, Ning\n  Zhang, Wenjing Lou, Y. Thomas Hou", "title": "Enjoy the Untrusted Cloud: A Secure, Scalable and Efficient SQL-like\n  Query Framework for Outsourcing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the security of the cloud remains a concern, a common practice is to\nencrypt data before outsourcing them for utilization. One key challenging issue\nis how to efficiently perform queries over the ciphertext. Conventional\ncrypto-based solutions, e.g. partially/fully homomorphic encryption and\nsearchable encryption, suffer from low performance, poor expressiveness and\nweak compatibility. An alternative method that utilizes hardware-assisted\ntrusted execution environment, i.e., Intel SGX, has emerged recently. On one\nhand, such work lacks of supporting scalable access control over multiple data\nusers. On the other hand, existing solutions are subjected to the key\nrevocation problem and knowledge extractor vulnerability. In this work, we\nleverage the newly hardware-assisted methodology and propose a secure, scalable\nand efficient SQL-like query framework named QShield. Building upon Intel SGX,\nQShield can guarantee the confidentiality and integrity of sensitive data when\nbeing processed on an untrusted cloud platform. Moreover, we present a novel\nlightweight secret sharing method to enable multi-user access control in\nQShield, while tackling the key revocation problem. Furthermore, with an\nadditional trust proof mechanism, QShield guarantees the correctness of queries\nand significantly alleviates the possibility to build a knowledge extractor. We\nimplemented a prototype for QShield and show that QShield incurs minimum\nperformance cost.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:54:51 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Chen", "Yaxing", ""], ["Zheng", "Qinghua", ""], ["Liu", "Dan", ""], ["Yan", "Zheng", ""], ["Sun", "Wenhai", ""], ["Zhang", "Ning", ""], ["Lou", "Wenjing", ""], ["Hou", "Y. Thomas", ""]]}, {"id": "1912.08768", "submitter": "Pradeeban Kathiravelu", "authors": "Pradeeban Kathiravelu, Yusuf Nadir Saghar, Tushar Aggarwal, Ashish\n  Sharma", "title": "Data Services with Bindaas: RESTful Interfaces for Diverse Data Sources", "comments": "Kathiravelu, P., Saghar, Y.N., Aggarwal, T., and Sharma, A. Data\n  Services with Bindaas: RESTful Interfaces for Diverse Data Sources. In The\n  IEEE International Conference on Big Data (BigData'19). pp. 457 - 462. Dec.\n  2019", "journal-ref": "IEEE BigData 2019", "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversity of data management systems affords developers the luxury of\nbuilding systems with heterogeneous systems that address needs that are unique\nto the data. It allows one to mix-n-match systems that can store, query,\nupdate, and process data, based on specific use cases. However, this\nheterogeneity brings with it the burden of developing custom interfaces for\neach data management system. Developers are required to build high-performance\nAPIs for data access while adopting best-practices governing security, data\nprivacy, and access control. These include user authentication, data\nauthorization, role-based access control, and audit mechanisms to avoid\ncompromising the security standards mandated by data providers.\n  In this paper, we present Bindaas, a secure, extensible big data middleware\nthat offers uniform access to diverse data sources. By providing a standard\nRESTful web service interface to the data sources, Bindaas exposes query,\nupdate, store, and delete functionality of the data sources as data service\nAPIs, while providing turn-key support for standard operations involving\nsecurity, access control, and audit-trails. Bindaas consists of optional\nfeatures, such as query and response modifiers as well as plugins that\nimplement composable and reusable data operations on the data. The research\ncommunity has deployed Bindaas in various production environments in\nhealthcare. Our evaluations highlight the efficiency of Bindaas in serving\nconcurrent requests to data source instances. We further observe that the\noverheads caused by Bindaas on the data sources are negligible.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:05:28 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Kathiravelu", "Pradeeban", ""], ["Saghar", "Yusuf Nadir", ""], ["Aggarwal", "Tushar", ""], ["Sharma", "Ashish", ""]]}, {"id": "1912.09018", "submitter": "Cheng Tan", "authors": "Cheng Tan, Changgeng Zhao, Shuai Mu, and Michael Walfish", "title": "Detecting Incorrect Behavior of Cloud Databases as an Outsider", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud DBs offer strong properties, including serializability, sometimes\ncalled the gold standard database correctness property. But cloud DBs are\ncomplicated black boxes, running in a different administrative domain from\ntheir clients; thus, clients might like to know whether the DBs are meeting\ntheir contract. A core difficulty is that the underlying problem here, namely\nverifying serializability, is NP-complete. Nevertheless, we hypothesize that on\nreal-world workloads, verifying serializability is tractable, and we treat the\nquestion as a systems problem, for the first time. We build Cobra, which tames\nthe underlying search problem by blending a new encoding of the problem,\nhardware acceleration, and a careful choice of a suitable SMT solver. cobra\nalso introduces a technique to address the challenge of garbage collection in\nthis context. cobra improves over natural baselines by at least 10x in the\nproblem size it can handle, while imposing modest overhead on clients.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 05:05:51 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 13:39:20 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Tan", "Cheng", ""], ["Zhao", "Changgeng", ""], ["Mu", "Shuai", ""], ["Walfish", "Michael", ""]]}, {"id": "1912.09019", "submitter": "Bikash Chandra", "authors": "Bikash Chandra and Ananyo Banerjee and Udbhas Hazra and Mathew Joseph\n  and S. Sudarshan", "title": "Edit Based Grading of SQL Queries", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grading student SQL queries manually is a tedious and error-prone process.\nEarlier work on testing correctness of student SQL queries, such as the XData\nsystem, can be used to test correctness of a student query. However, in case a\nstudent query is found to be incorrect there is currently no way to\nautomatically assign partial marks. Partial marking is important so that small\nerrors are penalized less than large errors. Manually awarding partial marks is\nnot scalable for classes with large number of students, especially MOOCs, and\nis also prone to human errors.\n  In this paper, we discuss techniques to find a minimum cost set of edits to a\nstudent query that would make it correct, which can help assign partial marks,\nand to help students understand exactly where they went wrong. Given the\nlimitations of current formal methods for checking equivalence, our approach is\nbased on finding nearest query, from a set of instructor provided correct\nqueries, that is found to be equivalent based on query canonicalization. We\nshow that exhaustive techniques are expensive, and propose a greedy heuristic\napproach that works well both in terms of runtime and accuracy on queries in\nreal-world datasets. Our system can also be used in a learning mode where query\nedits can be suggested as feedback to students to guide them towards a correct\nquery. Our partial marking system has been successfully used in courses at IIT\nBombay and IIT Dharwad.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 05:13:47 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Chandra", "Bikash", ""], ["Banerjee", "Ananyo", ""], ["Hazra", "Udbhas", ""], ["Joseph", "Mathew", ""], ["Sudarshan", "S.", ""]]}, {"id": "1912.09127", "submitter": "Kwang Woo Nam", "authors": "Yongmi Lee, Kwang Woo Nam, Keun Ho Ryu", "title": "Fast Mining of Spatial Frequent Wordset from Social Database", "comments": "published in Spatial Information Research, vol.25, pp. 271-280", "journal-ref": "Spatial Information Research, 25(2), 271-280 (2017)", "doi": "10.1007/s41324-017-0094-6", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an algorithm that extracts spatial frequent\npatterns to explain the relative characteristics of a specific location from\nthe available social data. This paper proposes a spatial social data model\nwhich includes spatial social data, spatial support, spatial frequent patterns,\nspatial partition, and spatial clustering; these concepts are used for\ndescribing the exploration algorithm of spatial frequent patterns. With these\ndefined concepts as the foundation, an SFP-tree structure that maintains not\nonly the frequent words but also the frequent cells was proposed, and an\nSFP-growth algorithm that explores the frequent patterns on the basis of this\nSFP-tree was proposed.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:08:18 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 08:01:04 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lee", "Yongmi", ""], ["Nam", "Kwang Woo", ""], ["Ryu", "Keun Ho", ""]]}, {"id": "1912.09326", "submitter": "Markus Schmid", "authors": "Markus L. Schmid", "title": "Conjunctive Regular Path Queries with String Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the class CXRPQ of conjunctive xregex path queries, which are\nobtained from conjunctive regular path queries (CRPQs) by adding string\nvariables (also called backreferences) as found in practical implementations of\nregular expressions. CXRPQs can be considered user-friendly, since they combine\ntwo concepts that are well-established in practice: pattern-based graph queries\nand regular expressions with backreferences. Due to the string variables,\nCXRPQs can express inter-path dependencies, which are not expressible by CRPQs.\nThe evaluation complexity of CXRPQs, if not further restricted, is PSPACE-hard\nin data-complexity. We identify three natural fragments with more acceptable\nevaluation complexity: their data-complexity is in NL, while their combined\ncomplexity varies between EXPSPACE, PSPACE and NP. In terms of expressive\npower, we compare the CXRPQ-fragments with CRPQs and unions of CRPQs, and with\nextended conjunctive regular path queries (ECRPQs) and unions of ECRPQs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:04:58 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Schmid", "Markus L.", ""]]}, {"id": "1912.09747", "submitter": "Malte Sandstede", "authors": "Malte Sandstede", "title": "Online Analysis of Distributed Dataflows with Timely Dataflow", "comments": "92 pages, 44 figures, contact and further information: visit\n  https://www.maltesandstede.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ST2, an end-to-end solution to analyze distributed dataflows in an\nonline setting. It is powered by Timely Dataflow, a low-latency, distributed\ndata-parallel dataflow computational framework, and expands on its predecessor\nSnailTrail 1, a system to run online critical path analysis on program activity\ngraphs derived from dataflow execution traces. ST2 connects to a running Timely\ncomputation, creates the program activity graph representation, and runs\nmultiple analyses on top of it. Analyses include aggregate metrics, progress\nand temporal invariant checking, and graph pattern matching. Through a\ncommand-line interface and a real-time dashboard, users are able to interact\nwith and visualize ST2's analysis results.\n  For ST2's implementation, we discuss Differential Dataflow, a framework that\nuses differential computation to incrementalize even complex relational\ndataflow operators, as an alternative to Timely Dataflow, but ultimately settle\non using Timely. In our performance evaluations, we are able to show that ST2\nis able to comfortably keep up with common streaming computations in offline\nand online settings, even exceeding SnailTrail 1's performance. We also\nshowcase and evaluate ST2 from a functional standpoint in a case study. Using\nthe dashboard to profile a faulty source computation, we manage to successfully\ndetect the issues' root cause. We argue that ST2 is an extendable system that\npaves the way for users to debug, monitor, and optimize online distributed\ndataflows.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 10:42:18 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Sandstede", "Malte", ""]]}, {"id": "1912.10171", "submitter": "Tarek Hamrouni", "authors": "Tarek Hamrouni", "title": "Replication in Data Grids: Metrics and Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus in this report on two main axes. The first is dedicated to the study\nof the effect of replicas distribution on data grid performances. In this\nrespect, our main contributions are as follows: 1) An overview of replication\nstrategies mainly from the viewpoints of the considered parameters in their\nassociated steps as well as the used metrics in the literature for their\nevaluation. 2) A study of the impact of placement strategies on data grid\nperformance which motivated the analysis of the effect of the replicas\ndistribution quality on the performance results of replication strategies. 3)\nThe proposal of new evaluation metrics dedicated to the evaluation of the\ndistribution quality. 4) The setting of an objective evaluation of replication\nstrategies which is based on a beforehand assessment of the distribution\nquality.\n  The second axis is mainly dedicated to exploiting results of data mining\ntechniques to enhance performances of replication strategies. With respect to\nthis axis, we mainly concentrate on the following contributions listed below:\n1) The study of the strengths and the drawbacks of the main replication\nstrategies based on data mining techniques and how these latter are applied in\nthis context. 2) The proposal of a new guideline to data mining application in\nthe context of data grid replication strategies. 3) The proposal of a new\nalgorithm for mining maximal frequent correlated patterns. The input of this\nalgorithm is obtained through a preliminary step focusing on how to adapt the\nrequired grid concepts to the data mining algorithm. 4) The design and the\nimplementation of a new replication strategy based on a data mining technique,\nand more precisely correlated patterns.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 07:45:41 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Hamrouni", "Tarek", ""]]}, {"id": "1912.10704", "submitter": "Nofar Carmeli", "authors": "Nofar Carmeli, Shai Zeevi, Christoph Berkholz, Benny Kimelfeld, Nicole\n  Schweikardt", "title": "Answering (Unions of) Conjunctive Queries using Random Access and\n  Random-Order Enumeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data analytics becomes more crucial to digital systems, so grows the\nimportance of characterizing the database queries that admit a more efficient\nevaluation. We consider the tractability yardstick of answer enumeration with a\npolylogarithmic delay after a linear-time preprocessing phase. Such an\nevaluation is obtained by constructing, in the preprocessing phase, a data\nstructure that supports polylogarithmic-delay enumeration. In this paper, we\nseek a structure that supports the more demanding task of a \"random\npermutation\": polylogarithmic-delay enumeration in truly random order.\nEnumeration of this kind is required if downstream applications assume that the\nintermediate results are representative of the whole result set in a\nstatistically valuable manner. An even more demanding task is that of a \"random\naccess\": polylogarithmic-time retrieval of an answer whose position is given.\n  We establish that the free-connex acyclic CQs are tractable in all three\nsenses: enumeration, random-order enumeration, and random access; and in the\nabsence of self-joins, it follows from past results that every other CQ is\nintractable by each of the three (under some fine-grained complexity\nassumptions). However, the three yardsticks are separated in the case of a\nunion of CQs (UCQ): while a union of free-connex acyclic CQs has a tractable\nenumeration, it may (provably) admit no random access. For such UCQs we devise\na random-order enumeration whose delay is logarithmic in expectation. We also\nidentify a subclass of UCQs for which we can provide random access with\npolylogarithmic access time. Finally, we present an implementation and an\nempirical study that show a considerable practical superiority of our\nrandom-order enumeration approach over state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 09:44:06 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Carmeli", "Nofar", ""], ["Zeevi", "Shai", ""], ["Berkholz", "Christoph", ""], ["Kimelfeld", "Benny", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "1912.11064", "submitter": "Mika\\\"el Monet", "authors": "Marcelo Arenas, Pablo Barcel\\'o, Mika\\\"el Monet", "title": "Counting Problems over Incomplete Databases", "comments": "29 pages, including 12 pages of main text. This is the arXiv version\n  of the PODS'20 paper. Except from minor differences that could be introduced\n  by the publisher, the only difference should be the addition of the appendix,\n  which contains all the proofs that do not appear in the main text", "journal-ref": null, "doi": "10.1145/3375395.3387656", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of various fundamental counting problems that arise\nin the context of incomplete databases, i.e., relational databases that can\ncontain unknown values in the form of labeled nulls. Specifically, we assume\nthat the domains of these unknown values are finite and, for a Boolean query\n$q$, we consider the following two problems: given as input an incomplete\ndatabase $D$, (a) return the number of completions of $D$ that satisfy $q$; or\n(b) return or the number of valuations of the nulls of $D$ yielding a\ncompletion that satisfies $q$. We obtain dichotomies between #P-hardness and\npolynomial-time computability for these problems when $q$ is a self-join--free\nconjunctive query, and study the impact on the complexity of the following two\nrestrictions: (1) every null occurs at most once in $D$ (what is called Codd\ntables); and (2) the domain of each null is the same. Roughly speaking, we show\nthat counting completions is much harder than counting valuations (for\ninstance, while the latter is always in #P, we prove that the former is not in\n#P under some widely believed theoretical complexity assumption). Moreover, we\nfind that both (1) and (2) reduce the complexity of our problems. We also study\nthe approximability of these problems and show that, while counting valuations\nalways has a fully polynomial randomized approximation scheme, in most cases\ncounting completions does not. Finally, we consider more expressive query\nlanguages and situate our problems with respect to known complexity classes.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 19:08:58 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 12:02:02 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Arenas", "Marcelo", ""], ["Barcel\u00f3", "Pablo", ""], ["Monet", "Mika\u00ebl", ""]]}, {"id": "1912.11098", "submitter": "Mika\\\"el Monet", "authors": "Mika\\\"el Monet, Dan Olteanu", "title": "Towards Deterministic Decomposable Circuits for Safe Queries", "comments": "10 pages. Appeared in the workshop AMW'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist two approaches for exact probabilistic inference of UCQs on\ntuple-independent databases. In the extensional approach, query evaluation is\nperformed within a DBMS by exploiting the structure of the query. In the\nintensional approach, one first builds a representation of the lineage of the\nquery on the database, then computes the probability of the lineage. In this\npaper we propose a new technique to construct lineage representations as\ndeterministic decomposable circuits in PTIME. The technique can apply to a\nclass of UCQs that has been conjectured to separate the complexity of the two\napproaches. We test our technique experimentally, and show that it succeeds on\nall the queries of this class up to a certain size parameter, i.e., over $20$\nmillion queries.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 20:37:40 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Monet", "Mika\u00ebl", ""], ["Olteanu", "Dan", ""]]}, {"id": "1912.11417", "submitter": "Sergio Sainz", "authors": "Sergio Sainz-Palacios", "title": "Flat combined Red Black Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flat combining is a concurrency threaded technique whereby one thread\nperforms all the operations in batch by scanning a queue of operations\nto-be-done and performing them together. Flat combining makes sense as long as\nk operations each taking O(n) separately can be batched together and done in\nless than O(k*n). Red black tree is a balanced binary search tree with\npermanent balancing warranties. Operations in red black tree are hard to batch\ntogether: for example inserting nodes in two different branches of the tree\naffect different areas of the tree. In this paper we investigate alternatives\nto making a flat combine approach work for red black trees.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 15:39:46 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Sainz-Palacios", "Sergio", ""]]}, {"id": "1912.11666", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Han-Chieh Chao, Philippe\n  Fournier-Viger, Xuan Wang and Philip S. Yu", "title": "Utility-Driven Mining of Trend Information for Intelligent System", "comments": "Accepted by ACM Trans. on Management Information Systems, 26 pages, 8\n  figures", "journal-ref": "ACM Transactions on Management Information Systems, 2020", "doi": "10.1145/3391251", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Useful knowledge, embedded in a database, is likely to change over time.\nIdentifying recent changes in temporal databases can provide valuable\nup-to-date information to decision-makers. Nevertheless, techniques for mining\nhigh-utility patterns (HUPs) seldom consider recency as a criterion to discover\npatterns. Thus, the traditional utility mining framework is inadequate for\nobtaining up-to-date insights about real world data. In this paper, we address\nthis issue by introducing a novel framework, named utility-driven mining of\nRecent/trend high-Utility Patterns (RUP) in temporal databases for intelligent\nsystems, based on user-specified minimum recency and minimum utility\nthresholds. The utility-driven RUP algorithm is based on novel global and\nconditional downward closure properties, and a recency-utility tree. Moreover,\nit adopts a vertical compact recency-utility list structure to store the\ninformation required by the mining process. The developed RUP algorithm\nrecursively discovers recent HUPs. It is also fast and consumes a small amount\nof memory due to its pattern discovery approach that does not generate\ncandidates. Two improved versions of the algorithm with additional pruning\nstrategies are also designed to speed up the discovery of patterns by reducing\nthe search space. Results of a substantial experimental evaluation show that\nthe proposed algorithm can efficiently identify all recent high-utility\npatterns in large-scale databases, and that the improved algorithm performs\nbest.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 13:29:04 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Chao", "Han-Chieh", ""], ["Fournier-Viger", "Philippe", ""], ["Wang", "Xuan", ""], ["Yu", "Philip S.", ""]]}, {"id": "1912.11670", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Han-Chieh Chao and Philip S. Yu", "title": "Discovering High Utility Episodes in Sequences", "comments": "IEEE Transactions on Big Data, under review, 13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence data, e.g., complex event sequence, is more commonly seen than other\ntypes of data (e.g., transaction data) in real-world applications. For the\nmining task from sequence data, several problems have been formulated, such as\nsequential pattern mining, episode mining, and sequential rule mining. As one\nof the fundamental problems, episode mining has often been studied. The common\nwisdom is that discovering frequent episodes is not useful enough. In this\npaper, we propose an efficient utility mining approach namely UMEpi: Utility\nMining of high-utility Episodes from complex event sequence. We propose the\nconcept of remaining utility of episode, and achieve a tighter upper bound,\nnamely episode-weighted utilization (EWU), which will provide better pruning.\nThus, the optimized EWU-based pruning strategies can achieve better\nimprovements in mining efficiency. The search space of UMEpi w.r.t. a\nprefix-based lexicographic sequence tree is spanned and determined recursively\nfor mining high-utility episodes, by prefix-spanning in a depth-first way.\nFinally, extensive experiments on four real-life datasets demonstrate that\nUMEpi can discover the complete high-utility episodes from complex event\nsequence, while the state-of-the-art algorithms fail to return the correct\nresults. Furthermore, the improved variants of UMEpi significantly outperform\nthe baseline in terms of execution time, memory consumption, and scalability.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 13:59:33 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Chao", "Han-Chieh", ""], ["Yu", "Philip S.", ""]]}, {"id": "1912.11673", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Jiexiong Zhang and Philip S. Yu", "title": "Utility Mining Across Multi-Sequences with Individualized Thresholds", "comments": "Accepted by ACM Trans. on Data Science, 29 pages, 6 figures", "journal-ref": "ACM Transactions on Data Science, 2020", "doi": "10.1145/3362070", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utility-oriented pattern mining has become an emerging topic since it can\nreveal high-utility patterns (e.g., itemsets, rules, sequences) from different\ntypes of data, which provides more information than the traditional\nfrequent/confident-based pattern mining models. The utilities of various items\nare not exactly equal in realistic situations; each item has its own utility or\nimportance. In general, user considers a uniform minimum utility (minutil)\nthreshold to identify the set of high-utility sequential patterns (HUSPs). This\nis unable to find the interesting patterns while the minutil is set extremely\nhigh or low. We first design a new utility mining framework namely USPT for\nmining high-Utility Sequential Patterns across multi-sequences with\nindividualized Thresholds. Each item in the designed framework has its own\nspecified minimum utility threshold. Based on the lexicographic-sequential tree\nand the utility-array structure, the USPT framework is presented to efficiently\ndiscover the HUSPs. With the upper-bounds on utility, several pruning\nstrategies are developed to prune the unpromising candidates early in the\nsearch space. Several experiments are conducted on both real-life and synthetic\ndatasets to show the performance of the designed USPT algorithm, and the\nresults showed that USPT could achieve good effectiveness and efficiency for\nmining HUSPs with individualized minimum utility thresholds.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 14:06:02 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Zhang", "Jiexiong", ""], ["Yu", "Philip S.", ""]]}, {"id": "1912.11859", "submitter": "Fernando Silva-Coira", "authors": "Susana Ladra, Miguel R. Luaces, Jos\\'e R. Param\\'a, Fernando\n  Silva-Coira", "title": "Space- and Time-Efficient Storage of LiDAR Point Clouds", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "In: String Processing and Information Retrieval. SPIRE 2019.\n  Lecture Notes in Computer Science, vol 11811. Springer", "doi": "10.1007/978-3-030-32686-9_36", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR devices obtain a 3D representation of a space. Due to the large size of\nthe resulting datasets, there already exist storage methods that use\ncompression and present some properties that resemble those of compact data\nstructures. Specifically, LAZ format allows accesses to a given datum or\nportion of the data without having to decompress the whole dataset and provides\nindexation of the stored data. However, LAZ format still have some drawbacks\nthat should be faced. In this work, we propose a new compact data structure for\nthe representation of a cloud of LiDAR points that supports efficient queries,\nproviding indexing capabilities that are superior to those of LAZ format.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 13:22:19 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ladra", "Susana", ""], ["Luaces", "Miguel R.", ""], ["Param\u00e1", "Jos\u00e9 R.", ""], ["Silva-Coira", "Fernando", ""]]}, {"id": "1912.11864", "submitter": "Mika\\\"el Monet", "authors": "Mika\\\"el Monet", "title": "Solving a Special Case of the Intensional vs Extensional Conjecture in\n  Probabilistic Databases", "comments": "16 pages, including 12 pages of main text. This is the ArXiv version\n  of the PODS'20 paper. Except from minor differences that could be introduced\n  by the publisher, the only difference should be the addition of the figure in\n  Appendix D", "journal-ref": null, "doi": "10.1145/3375395.3387642", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of exact probabilistic inference for Union of\nConjunctive Queries (UCQs) on tuple-independent databases. For this problem,\ntwo approaches currently coexist. In the extensional method, query evaluation\nis performed by exploiting the structure of the query, and relies heavily on\nthe use of the inclusion-exclusion principle. In the intensional method, one\nfirst builds a representation of the lineage of the query in a tractable\nformalism of knowledge compilation. The chosen formalism should then ensure\nthat the probability can be efficiently computed using simple disjointness and\nindependence assumptions, without the need of performing inclusion-exclusion.\nThe extensional approach has long been thought to be strictly more powerful\nthan the intensional approach, the reason being that for some queries, the use\nof inclusion-exclusion seemed unavoidable. In this paper we introduce a new\ntechnique to construct lineage representations as deterministic decomposable\ncircuits in polynomial time. We prove that this technique applies to a class of\nUCQs that had been conjectured to separate the complexity of the two\napproaches. In essence, we show that relying on the inclusion-exclusion formula\ncan be avoided by using negation. This result brings back hope to prove that\nthe intensional approach can handle all tractable UCQs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 13:46:23 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 16:20:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Monet", "Mika\u00ebl", ""]]}, {"id": "1912.11977", "submitter": "Renzhi Wu", "authors": "Renzhi Wu, Sergey Sukhanov, Christian Debes", "title": "Real Time Pattern Matching with Dynamic Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching in time series data streams is considered to be an essential\ndata mining problem that still stays challenging for many practical scenarios.\nDifferent factors such as noise, varying amplitude scale or shift, signal\nstretches or shrinks in time are all leading to performance degradation of many\nexisting pattern matching algorithms. In this paper, we introduce a dynamic\nz-normalization mechanism allowing for proper signal scaling even under\nsignificant time and amplitude distortions. Based on that, we further propose a\nDynamic Time Warping-based real-time pattern matching method to recover hidden\npatterns that can be distorted in both time and amplitude. We evaluate our\nproposed method on synthetic and real-world scenarios under realistic\nconditions demonstrating its high operational characteristics comparing to\nother state-of-the-art pattern matching methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 04:02:06 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 06:41:29 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wu", "Renzhi", ""], ["Sukhanov", "Sergey", ""], ["Debes", "Christian", ""]]}, {"id": "1912.12338", "submitter": "Szymon Toru\\'nczyk", "authors": "Szymon Toru\\'nczyk", "title": "Aggregate Queries on Sparse Databases", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algebraic framework for studying efficient algorithms for query\nevaluation, aggregation, enumeration, and maintenance under updates, on sparse\ndatabases. Our framework allows to treat those problems in a unified way, by\nconsidering various semirings, depending on the considered problem. As a\nconcrete application, we propose a powerful query language extending\nfirst-order logic by aggregation in multiple semirings. We obtain an optimal\nalgorithm for computing the answers of such queries on sparse databases. More\nprecisely, given a database from a fixed class with bounded expansion, the\nalgorithm computes in linear time a data structure which allows to enumerate\nthe set of answers to the query, with constant delay between two outputs.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 21:15:02 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Toru\u0144czyk", "Szymon", ""]]}, {"id": "1912.12442", "submitter": "Andreas Pieris", "authors": "Pablo Barcelo, Victor Dalmau, Cristina Feier, Carsten Lutz, Andreas\n  Pieris", "title": "The Limits of Efficiency for Open- and Closed-World Query Evaluation\n  Under Guarded TGDs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology-mediated querying and querying in the presence of constraints are\ntwo key database problems where tuple-generating dependencies (TGDs) play a\ncentral role. In ontology-mediated querying, TGDs can formalize the ontology\nand thus derive additional facts from the given data, while in querying in the\npresence of constraints, they restrict the set of admissible databases. In this\nwork, we study the limits of efficient query evaluation in the context of the\nabove two problems, focussing on guarded and frontier-guarded TGDs and on UCQs\nas the actual queries. We show that a class of ontology-mediated queries (OMQs)\nbased on guarded TGDs can be evaluated in FPT iff the OMQs in the class are\nequivalent to OMQs in which the actual query has bounded treewidth, up to some\nreasonable assumptions. For querying in the presence of constraints, we\nconsider classes of constraint-query specifications (CQSs) that bundle a set of\nconstraints with an actual query. We show a dichotomy result for CQSs based on\nguarded TGDs that parallels the one for OMQs except that, additionally, FPT\ncoincides with PTime combined complexity. The proof is based on a novel\nconnection between OMQ and CQS evaluation. Using a direct proof, we also show a\nsimilar dichotomy result, again up to some reasonable assumptions, for CQSs\nbased on frontier-guarded TGDs with a bounded number of atoms in TGD heads. Our\nresults on CQSs can be viewed as extensions of Grohe's well-known\ncharacterization of the tractable classes of CQs (without constraints). Like\nGrohe's characterization, all the above results assume that the arity of\nrelation symbols is bounded by a constant. We also study the associated meta\nproblems, i.e., whether a given OMQ or CQS is equivalent to one in which the\nactual query has bounded treewidth.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 11:08:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Barcelo", "Pablo", ""], ["Dalmau", "Victor", ""], ["Feier", "Cristina", ""], ["Lutz", "Carsten", ""], ["Pieris", "Andreas", ""]]}, {"id": "1912.12531", "submitter": "Giacomo Bergami", "authors": "Giacomo Bergami", "title": "A framework supporting imprecise queries and data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This technical report provides some lightweight introduction and some generic\nuse case scenarios motivating the definition of a database supporting\nuncertainties in both queries and data. This technical report is only providing\nthe logical framework, which implementation is going to be provided in the\nfinal paper.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 21:58:43 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bergami", "Giacomo", ""]]}, {"id": "1912.12610", "submitter": "Ester Livshits", "authors": "Alon Reshef, Benny Kimelfeld, Ester Livshits", "title": "The Impact of Negation on the Complexity of the Shapley Value in\n  Conjunctive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shapley value is a conventional and well-studied function for determining\nthe contribution of a player to the coalition in a cooperative game. Among its\napplications in a plethora of domains, it has recently been proposed to use the\nShapley value for quantifying the contribution of a tuple to the result of a\ndatabase query. In particular, we have a thorough understanding of the\ntractability frontier for the class of Conjunctive Queries (CQs) and aggregate\nfunctions over CQs. It has also been established that a tractable (randomized)\nmultiplicative approximation exists for every union of CQs. Nevertheless, all\nof these results are based on the monotonicity of CQs. In this work, we\ninvestigate the implication of negation on the complexity of Shapley\ncomputation, in both the exact and approximate senses. We generalize a known\ndichotomy to account for negated atoms. We also show that negation\nfundamentally changes the complexity of approximation. We do so by drawing a\nconnection to the problem of deciding whether a tuple is \"relevant\" to a query,\nand by analyzing its complexity.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 08:55:27 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Reshef", "Alon", ""], ["Kimelfeld", "Benny", ""], ["Livshits", "Ester", ""]]}, {"id": "1912.12740", "submitter": "Maciej Besta", "authors": "Maciej Besta, Marc Fischer, Vasiliki Kalavri, Michael Kapralov,\n  Torsten Hoefler", "title": "Practice of Streaming Processing of Dynamic Graphs: Concepts, Models,\n  and Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processing has become an important part of various areas of computing,\nincluding machine learning, medical applications, social network analysis,\ncomputational sciences, and others. A growing amount of the associated graph\nprocessing workloads are dynamic, with millions of edges added or removed per\nsecond. Graph streaming frameworks are specifically crafted to enable the\nprocessing of such highly dynamic workloads. Recent years have seen the\ndevelopment of many such frameworks. However, they differ in their general\narchitectures (with key details such as the support for the concurrent\nexecution of graph updates and queries, or the incorporated graph data\norganization), the types of updates and workloads allowed, and many others. To\nfacilitate the understanding of this growing field, we provide the first\nanalysis and taxonomy of dynamic and streaming graph processing. We focus on\nidentifying the fundamental system designs and on understanding their support\nfor concurrency, and for different graph updates as well as analytics\nworkloads. We also crystallize the meaning of different concepts associated\nwith streaming graph processing, such as dynamic, temporal, online, and\ntime-evolving graphs, edge-centric processing, models for the maintenance of\nupdates, and graph databases. Moreover, we provide a bridge with the very rich\nlandscape of graph streaming theory by giving a broad overview of recent\ntheoretical related advances, and by discussing which graph streaming models\nand settings could be helpful in developing more powerful streaming frameworks\nand designs. We also outline graph streaming workloads and research challenges.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 21:41:14 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 19:24:01 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 13:00:50 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Besta", "Maciej", ""], ["Fischer", "Marc", ""], ["Kalavri", "Vasiliki", ""], ["Kapralov", "Michael", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1912.12747", "submitter": "Bernhard Scholz", "authors": "Alan Fekete and Brody Franks and Herbert Jordan and Bernhard Scholz", "title": "Worst-Case Optimal Radix Triejoin", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relatively recently, the field of join processing has been swayed by the\ndiscovery of a new class of multi-way join algorithms. The new algorithms join\nmultiple relations simultaneously rather than perform a series of pairwise\njoins. The new join algorithms satisfy stronger worst-case runtime complexity\nguarantees than any of the existing approaches based on pairwise joins -- they\nare worst-case optimal in data complexity. These research efforts have resulted\nin a flurry of papers documenting theoretical and some practical contributions.\nHowever, there is still the quest of making the new worst-case optimal join\nalgorithms truly practical in terms of (1) ease of implementation and (2)\nsecondary index efficiency in terms of number of indexes created to answer a\nquery.\n  In this paper, we present a simple worst-case optimal multi-way join\nalgorithm called the radix triejoin. Radix triejoin uses a binary encoding for\nreducing the domain of a database. Our main technical contribution is that\ndomain reduction allows a bit-interleaving of attribute values that gives rise\nto a query-independent relation representation, permitting the computation of\nmultiple queries over the same relations worst-case optimally without having to\nconstruct additional secondary indexes. We also generalise the core algorithm\nto conjunctive queries with inequality constraints and provide a new proof\ntechnique for the worst-case optimal join result.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 22:00:58 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Fekete", "Alan", ""], ["Franks", "Brody", ""], ["Jordan", "Herbert", ""], ["Scholz", "Bernhard", ""]]}, {"id": "1912.13501", "submitter": "Karim Banawan", "authors": "Zhusheng Wang and Karim Banawan and Sennur Ulukus", "title": "Private Set Intersection: A Multi-Message Symmetric Private Information\n  Retrieval Perspective", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of private set intersection (PSI). In this problem,\nthere are two entities $E_i$, for $i=1, 2$, each storing a set $\\mathcal{P}_i$,\nwhose elements are picked from a finite field $\\mathbb{F}_K$, on $N_i$\nreplicated and non-colluding databases. It is required to determine the set\nintersection $\\mathcal{P}_1 \\cap \\mathcal{P}_2$ without leaking any information\nabout the remaining elements to the other entity with the least amount of\ndownloaded bits. We first show that the PSI problem can be recast as a\nmulti-message symmetric private information retrieval (MM-SPIR) problem. Next,\nas a stand-alone result, we derive the information-theoretic sum capacity of\nMM-SPIR, $C_{MM-SPIR}$. We show that with $K$ messages, $N$ databases, and the\nsize of the desired message set $P$, the exact capacity of MM-SPIR is\n$C_{MM-SPIR} = 1 - \\frac{1}{N}$ when $P \\leq K-1$, provided that the entropy of\nthe common randomness $S$ satisfies $H(S) \\geq \\frac{P}{N-1}$ per desired\nsymbol. This result implies that there is no gain for MM-SPIR over successive\nsingle-message SPIR (SM-SPIR). For the MM-SPIR problem, we present a novel\ncapacity-achieving scheme that builds on the near-optimal scheme of\nBanawan-Ulukus originally proposed for the multi-message PIR (MM-PIR) problem\nwithout database privacy constraints. Surprisingly, our scheme here is exactly\noptimal for the MM-SPIR problem for any $P$, in contrast to the scheme for the\nMM-PIR problem, which was proved only to be near-optimal. Our scheme is an\nalternative to the SM-SPIR scheme of Sun-Jafar. Based on this capacity result\nfor MM-SPIR, and after addressing the added requirements in its conversion to\nthe PSI problem, we show that the optimal download cost for the PSI problem is\n$\\min\\left\\{\\left\\lceil\\frac{P_1 N_2}{N_2-1}\\right\\rceil, \\left\\lceil\\frac{P_2\nN_1}{N_1-1}\\right\\rceil\\right\\}$, where $P_i$ is the cardinality of set\n$\\mathcal{P}_i$\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:51:02 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 17:17:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Zhusheng", ""], ["Banawan", "Karim", ""], ["Ulukus", "Sennur", ""]]}]