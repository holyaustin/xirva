[{"id": "0811.0741", "submitter": "Jerome Darmont", "authors": "Hadj Mahboubi (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "Data Mining-based Fragmentation of XML Data Warehouses", "comments": null, "journal-ref": "ACM 11th International Workshop on Data Warehousing and OLAP\n  (CIKM/DOLAP 08), Napa Valley : \\'Etats-Unis d'Am\\'erique (2008)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the multiplication of XML data sources, many XML data warehouse models\nhave been proposed to handle data heterogeneity and complexity in a way\nrelational data warehouses fail to achieve. However, XML-native database\nsystems currently suffer from limited performances, both in terms of manageable\ndata volume and response time. Fragmentation helps address both these issues.\nDerived horizontal fragmentation is typically used in relational data\nwarehouses and can definitely be adapted to the XML context. However, the\nnumber of fragments produced by classical algorithms is difficult to control.\nIn this paper, we propose the use of a k-means-based fragmentation approach\nthat allows to master the number of fragments through its $k$ parameter. We\nexperimentally compare its efficiency to classical derived horizontal\nfragmentation algorithms adapted to XML data warehouses and show its\nsuperiority.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2008 15:00:32 GMT"}], "update_date": "2008-11-06", "authors_parsed": [["Mahboubi", "Hadj", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "0811.1083", "submitter": "George  Fletcher", "authors": "George H. L. Fletcher and Peter W. Beck", "title": "A role-free approach to indexing large RDF data sets in secondary memory\n  for efficient SPARQL evaluation", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive RDF data sets are becoming commonplace. RDF data is typically\ngenerated in social semantic domains (such as personal information management)\nwherein a fixed schema is often not available a priori. We propose a simple\nThree-way Triple Tree (TripleT) secondary-memory indexing technique to\nfacilitate efficient SPARQL query evaluation on such data sets. The novelty of\nTripleT is that (1) the index is built over the atoms occurring in the data\nset, rather than at a coarser granularity, such as whole triples occurring in\nthe data set; and (2) the atoms are indexed regardless of the roles (i.e.,\nsubjects, predicates, or objects) they play in the triples of the data set. We\nshow through extensive empirical evaluation that TripleT exhibits multiple\norders of magnitude improvement over the state of the art on RDF indexing, in\nterms of both storage and query processing costs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2008 05:08:41 GMT"}], "update_date": "2008-11-20", "authors_parsed": [["Fletcher", "George H. L.", ""], ["Beck", "Peter W.", ""]]}, {"id": "0811.2117", "submitter": "Cristian Molinaro", "authors": "Cristian Molinaro, Jan Chomicki, Jerzy Marcinkowski", "title": "Disjunctive Databases for Representing Repairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of representing the set of repairs of a\npossibly inconsistent database by means of a disjunctive database.\nSpecifically, the class of denial constraints is considered. We show that,\ngiven a database and a set of denial constraints, there exists a (unique)\ndisjunctive database, called canonical, which represents the repairs of the\ndatabase w.r.t. the constraints and is contained in any other disjunctive\ndatabase with the same set of minimal models. We propose an algorithm for\ncomputing the canonical disjunctive database. Finally, we study the size of the\ncanonical disjunctive database in the presence of functional dependencies for\nboth repairs and cardinality-based repairs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2008 14:12:57 GMT"}], "update_date": "2008-11-14", "authors_parsed": [["Molinaro", "Cristian", ""], ["Chomicki", "Jan", ""], ["Marcinkowski", "Jerzy", ""]]}, {"id": "0811.2250", "submitter": "Xi Zhang", "authors": "Xi Zhang and Jan Chomicki", "title": "Semantics and Evaluation of Top-k Queries in Probabilistic Databases", "comments": "60 pages, section 4.4 added, section 6 added, typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study here fundamental issues involved in top-k query evaluation in\nprobabilistic databases. We consider simple probabilistic databases in which\nprobabilities are associated with individual tuples, and general probabilistic\ndatabases in which, additionally, exclusivity relationships between tuples can\nbe represented. In contrast to other recent research in this area, we do not\nlimit ourselves to injective scoring functions. We formulate three intuitive\npostulates that the semantics of top-k queries in probabilistic databases\nshould satisfy, and introduce a new semantics, Global-Topk, that satisfies\nthose postulates to a large degree. We also show how to evaluate queries under\nthe Global-Topk semantics. For simple databases we design dynamic-programming\nbased algorithms, and for general databases we show polynomial-time reductions\nto the simple cases. For example, we demonstrate that for a fixed k the time\ncomplexity of top-k query evaluation is as low as linear, under the assumption\nthat probabilistic databases are simple and scoring functions are injective.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2008 01:47:14 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2009 19:47:55 GMT"}], "update_date": "2009-06-08", "authors_parsed": [["Zhang", "Xi", ""], ["Chomicki", "Jan", ""]]}, {"id": "0811.2841", "submitter": "Mukund Sundararajan", "authors": "Arpita Ghosh, Tim Roughgarden, Mukund Sundararajan", "title": "Universally Utility-Maximizing Privacy Mechanisms", "comments": "rewritten for clarity, typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mechanism for releasing information about a statistical database with\nsensitive data must resolve a trade-off between utility and privacy. Privacy\ncan be rigorously quantified using the framework of {\\em differential privacy},\nwhich requires that a mechanism's output distribution is nearly the same\nwhether or not a given database row is included or excluded. The goal of this\npaper is strong and general utility guarantees, subject to differential\nprivacy.\n  We pursue mechanisms that guarantee near-optimal utility to every potential\nuser, independent of its side information (modeled as a prior distribution over\nquery results) and preferences (modeled via a loss function).\n  Our main result is: for each fixed count query and differential privacy\nlevel, there is a {\\em geometric mechanism} $M^*$ -- a discrete variant of the\nsimple and well-studied Laplace mechanism -- that is {\\em simultaneously\nexpected loss-minimizing} for every possible user, subject to the differential\nprivacy constraint. This is an extremely strong utility guarantee: {\\em every}\npotential user $u$, no matter what its side information and preferences,\nderives as much utility from $M^*$ as from interacting with a differentially\nprivate mechanism $M_u$ that is optimally tailored to $u$.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2008 05:59:39 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2009 19:17:20 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2009 16:34:43 GMT"}], "update_date": "2009-03-20", "authors_parsed": [["Ghosh", "Arpita", ""], ["Roughgarden", "Tim", ""], ["Sundararajan", "Mukund", ""]]}, {"id": "0811.2904", "submitter": "Srinivasa Rao Satti", "authors": "Rasmus Pagh and S. Srinivasa Rao", "title": "Secondary Indexing in One Dimension: Beyond B-trees and Bitmap Indexes", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let S be a finite, ordered alphabet, and let x = x_1 x_2 ... x_n be a string\nover S. A \"secondary index\" for x answers alphabet range queries of the form:\nGiven a range [a_l,a_r] over S, return the set I_{[a_l;a_r]} = {i |x_i \\in\n[a_l; a_r]}. Secondary indexes are heavily used in relational databases and\nscientific data analysis. It is well-known that the obvious solution, storing a\ndictionary for the position set associated with each character, does not always\ngive optimal query time. In this paper we give the first theoretically optimal\ndata structure for the secondary indexing problem. In the I/O model, the amount\nof data read when answering a query is within a constant factor of the minimum\nspace needed to represent I_{[a_l;a_r]}, assuming that the size of internal\nmemory is (|S| log n)^{delta} blocks, for some constant delta > 0. The space\nusage of the data structure is O(n log |S|) bits in the worst case, and we\nfurther show how to bound the size of the data structure in terms of the 0-th\norder entropy of x. We show how to support updates achieving various time-space\ntrade-offs.\n  We also consider an approximate version of the basic secondary indexing\nproblem where a query reports a superset of I_{[a_l;a_r]} containing each\nelement not in I_{[a_l;a_r]} with probability at most epsilon, where epsilon >\n0 is the false positive probability. For this problem the amount of data that\nneeds to be read by the query algorithm is reduced to O(|I_{[a_l;a_r]}|\nlog(1/epsilon)) bits.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2008 13:31:05 GMT"}], "update_date": "2008-11-19", "authors_parsed": [["Pagh", "Rasmus", ""], ["Rao", "S. Srinivasa", ""]]}, {"id": "0811.3301", "submitter": "Daniel Lemire", "authors": "Daniel Lemire", "title": "Faster Retrieval with a Two-Pass Dynamic-Time-Warping Lower Bound", "comments": "Accepted in Pattern Recognition on November 20th, 2008", "journal-ref": "Daniel Lemire, Faster Retrieval with a Two-Pass\n  Dynamic-Time-Warping Lower Bound, Pattern Recognition 42(9): 2169-2180 (2009)", "doi": "10.1016/j.patcog.2008.11.030", "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dynamic Time Warping (DTW) is a popular similarity measure between time\nseries. The DTW fails to satisfy the triangle inequality and its computation\nrequires quadratic time. Hence, to find closest neighbors quickly, we use\nbounding techniques. We can avoid most DTW computations with an inexpensive\nlower bound (LB Keogh). We compare LB Keogh with a tighter lower bound (LB\nImproved). We find that LB Improved-based search is faster. As an example, our\napproach is 2-3 times faster over random-walk and shape time series.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2008 16:22:05 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2009 17:37:32 GMT"}], "update_date": "2012-01-16", "authors_parsed": [["Lemire", "Daniel", ""]]}, {"id": "0811.3691", "submitter": "Alejandro Vaisman Prof.", "authors": "Leticia Gomez, Bart Kuijpers, Alejandro Vaisman", "title": "Temporal Support of Regular Expressions in Sequential Pattern Mining", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic algorithms for sequential pattern discovery, return all frequent\nsequences present in a database, but, in general, only a few ones are\ninteresting for the user. Languages based on regular expressions (RE) have been\nproposed to restrict frequent sequences to the ones that satisfy user-specified\nconstraints. Although the support of a sequence is computed as the number of\ndata-sequences satisfying a pattern with respect to the total number of\ndata-sequences in the database, once regular expressions come into play, new\napproaches to the concept of support are needed. For example, users may be\ninterested in computing the support of the RE as a whole, in addition to the\none of a particular pattern. Also, when the items are frequently updated, the\ntraditional way of counting support in sequential pattern mining may lead to\nincorrect (or, at least incomplete), conclusions. The problem gets more\ninvolved if we are interested in categorical sequential patterns. In light of\nthe above, in this paper we propose to revise the classic notion of support in\nsequential pattern mining, introducing the concept of temporal support of\nregular expressions, intuitively defined as the number of sequences satisfying\na target pattern, out of the total number of sequences that could have possibly\nmatched such pattern, where the pattern is defined as a RE over complex items\n(i.e., not only item identifiers, but also attributes and functions).\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2008 15:22:40 GMT"}], "update_date": "2008-11-25", "authors_parsed": [["Gomez", "Leticia", ""], ["Kuijpers", "Bart", ""], ["Vaisman", "Alejandro", ""]]}, {"id": "0811.4346", "submitter": "Ke Yi", "authors": "Ke Yi", "title": "Dynamic Indexability: The Query-Update Tradeoff for One-Dimensional\n  Range Queries", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The B-tree is a fundamental secondary index structure that is widely used for\nanswering one-dimensional range reporting queries. Given a set of $N$ keys, a\nrange query can be answered in $O(\\log_B \\nm + \\frac{K}{B})$ I/Os, where $B$ is\nthe disk block size, $K$ the output size, and $M$ the size of the main memory\nbuffer. When keys are inserted or deleted, the B-tree is updated in $O(\\log_B\nN)$ I/Os, if we require the resulting changes to be committed to disk right\naway. Otherwise, the memory buffer can be used to buffer the recent updates,\nand changes can be written to disk in batches, which significantly lowers the\namortized update cost. A systematic way of batching up updates is to use the\nlogarithmic method, combined with fractional cascading, resulting in a dynamic\nB-tree that supports insertions in $O(\\frac{1}{B}\\log\\nm)$ I/Os and queries in\n$O(\\log\\nm + \\frac{K}{B})$ I/Os. Such bounds have also been matched by several\nknown dynamic B-tree variants in the database literature.\n  In this paper, we prove that for any dynamic one-dimensional range query\nindex structure with query cost $O(q+\\frac{K}{B})$ and amortized insertion cost\n$O(u/B)$, the tradeoff $q\\cdot \\log(u/q) = \\Omega(\\log B)$ must hold if\n$q=O(\\log B)$. For most reasonable values of the parameters, we have $\\nm =\nB^{O(1)}$, in which case our query-insertion tradeoff implies that the bounds\nmentioned above are already optimal. Our lower bounds hold in a dynamic version\nof the {\\em indexability model}, which is of independent interests.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2008 15:36:14 GMT"}], "update_date": "2008-11-27", "authors_parsed": [["Yi", "Ke", ""]]}]