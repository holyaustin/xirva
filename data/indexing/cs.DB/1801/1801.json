[{"id": "1801.00036", "submitter": "Renzo Angles", "authors": "Renzo Angles and Claudio Gutierrez", "title": "An introduction to Graph Data Management", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-96193-4_1", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph database is a database where the data structures for the schema\nand/or instances are modeled as a (labeled)(directed) graph or generalizations\nof it, and where querying is expressed by graph-oriented operations and type\nconstructors. In this article we present the basic notions of graph databases,\ngive an historical overview of its main development, and study the main current\nsystems that implement them.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 21:02:12 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Angles", "Renzo", ""], ["Gutierrez", "Claudio", ""]]}, {"id": "1801.00345", "submitter": "Mehdi Maamar", "authors": "Christian Bessiere and Nadjib Lazaar and Yahia Lebbah and Mehdi Maamar", "title": "Users Constraints in Itemset Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering significant itemsets is one of the fundamental problems in data\nmining. It has recently been shown that constraint programming is a flexible\nway to tackle data mining tasks. With a constraint programming approach, we can\neasily express and efficiently answer queries with users constraints on items.\nHowever, in many practical cases it is possible that queries also express users\nconstraints on the dataset itself. For instance, asking for a particular\nitemset in a particular part of the dataset. This paper presents a general\nconstraint programming model able to handle any kind of query on the items or\nthe dataset for itemset mining.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 19:55:52 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 16:21:54 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Bessiere", "Christian", ""], ["Lazaar", "Nadjib", ""], ["Lebbah", "Yahia", ""], ["Maamar", "Mehdi", ""]]}, {"id": "1801.00783", "submitter": "Jianbin Huang", "authors": "Yu Zhou and Jianbin Huang and Heli Sun", "title": "A Semantic-Rich Similarity Measure in Heterogeneous Information Networks", "comments": "arXiv admin note: text overlap with arXiv:1712.09008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the similarities between objects in information networks has\nfundamental importance in recommendation systems, clustering and web search.\nThe existing metrics depend on the meta path or meta structure specified by\nusers. In this paper, we propose a stratified meta structure based similarity\n$SMSS$ in heterogeneous information networks. The stratified meta structure can\nbe constructed automatically and capture rich semantics. Then, we define the\ncommuting matrix of the stratified meta structure by virtue of the commuting\nmatrices of meta paths and meta structures. As a result, $SMSS$ is defined by\nvirtue of these commuting matrices. Experimental evaluations show that the\nproposed $SMSS$ on the whole outperforms the state-of-the-art metrics in terms\nof ranking and clustering.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 01:22:48 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 05:12:50 GMT"}, {"version": "v3", "created": "Sat, 27 Jan 2018 02:55:33 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Zhou", "Yu", ""], ["Huang", "Jianbin", ""], ["Sun", "Heli", ""]]}, {"id": "1801.00829", "submitter": "Matthias Boehm", "authors": "Matthias Boehm, Berthold Reinwald, Dylan Hutchison, Alexandre V.\n  Evfimievski, Prithviraj Sen", "title": "On Optimizing Operator Fusion Plans for Large-Scale Machine Learning in\n  SystemML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale machine learning (ML) systems allow specifying custom ML\nalgorithms by means of linear algebra programs, and then automatically generate\nefficient execution plans. In this context, optimization opportunities for\nfused operators---in terms of fused chains of basic operators---are ubiquitous.\nThese opportunities include (1) fewer materialized intermediates, (2) fewer\nscans of input data, and (3) the exploitation of sparsity across chains of\noperators. Automatic operator fusion eliminates the need for hand-written fused\noperators and significantly improves performance for complex or previously\nunseen chains of operations. However, existing fusion heuristics struggle to\nfind good fusion plans for complex DAGs or hybrid plans of local and\ndistributed operations. In this paper, we introduce an optimization framework\nfor systematically reason about fusion plans that considers materialization\npoints in DAGs, sparsity exploitation, different fusion template types, as well\nas local and distributed operations. In detail, we contribute algorithms for\n(1) candidate exploration of valid fusion plans, (2) cost-based candidate\nselection, and (3) code generation of local and distributed operations over\ndense, sparse, and compressed data. Our experiments in SystemML show end-to-end\nperformance improvements with optimized fusion plans of up to 21x compared to\nhand-written fused operators, with negligible optimization and code generation\noverhead.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 20:40:19 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Boehm", "Matthias", ""], ["Reinwald", "Berthold", ""], ["Hutchison", "Dylan", ""], ["Evfimievski", "Alexandre V.", ""], ["Sen", "Prithviraj", ""]]}, {"id": "1801.01012", "submitter": "Shuai Ma", "authors": "Shuai Ma and Jia Li and Chunming Hu and Xudong Liu and Jinpeng Huai", "title": "Graph Pattern Matching for Dynamic Team Formation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a list of k teams of experts, referred to as top-k team formation,\nwith the required skills and high collaboration compatibility has been\nextensively studied. However, existing methods have not considered the specific\ncollaboration relationships among different team members, i.e., structural\nconstraints, which are typically needed in practice. In this study, we first\npropose a novel graph pattern matching approach for top-k team formation, which\nincorporates both structural constraints and capacity bounds. Second, we\nformulate and study the dynamic top-k team formation problem due to the growing\nneed of a dynamic environment. Third, we develop an unified incremental\napproach, together with an optimization technique, to handle continuous pattern\nand data updates, separately and simultaneously, which has not been explored\nbefore. Finally, using real-life and synthetic data, we conduct an extensive\nexperimental study to show the effectiveness and efficiency of our graph\npattern matching approach for (dynamic) top-k team formation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 14:24:08 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Ma", "Shuai", ""], ["Li", "Jia", ""], ["Hu", "Chunming", ""], ["Liu", "Xudong", ""], ["Huai", "Jinpeng", ""]]}, {"id": "1801.01618", "submitter": "Kai Mast", "authors": "Kai Mast and Lequn Chen and Emin G\\\"un Sirer", "title": "Enabling Strong Database Integrity using Trusted Execution Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many applications require the immutable and consistent sharing of data across\norganizational boundaries. Because conventional datastores cannot provide this\nfunctionality, blockchains have been proposed as one possible solution. Yet\npublic blockchains are energy inefficient, hard to scale and suffer from\nlimited throughput and high latencies, while permissioned blockchains depend on\nspecially designated nodes, potentially leak meta-information, and also suffer\nfrom scale and performance bottlenecks.\n  This paper presents CreDB, a datastore that provides blockchain-like\nguarantees of integrity using trusted execution environments. CreDB employs\nfour novel mechanisms to support a new class of applications. First, it creates\na permanent record of every transaction, known as a witness, that clients can\nthen use not only to audit the database but to prove to third parties that\ndesired actions took place. Second, it associates with every object an\ninseparable and inviolable policy, which not only performs access control but\nenables the datastore to implement state machines whose behavior is amenable to\nanalysis. Third, timeline inspection allows authorized parties to inspect and\nreason about the history of changes made to the data. Finally, CreDB provides a\nprotected function evaluation mechanism that allows integrity-protected\ncomputation over private data. The paper describes these mechanisms, and the\napplications they collectively enable, in detail. We have fully implemented a\nprototype of CreDB on Intel SGX. Evaluation shows that CreDB can serve as a\ndrop-in replacement for other NoSQL stores, such as MongoDB while providing\nstronger integrity guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 03:04:50 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 14:44:25 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Mast", "Kai", ""], ["Chen", "Lequn", ""], ["Sirer", "Emin G\u00fcn", ""]]}, {"id": "1801.02476", "submitter": "Meysam Golmohammadi", "authors": "Scott Yang, Silvia Lopez, Meysam Golmohammadi, Iyad Obeid and Joseph\n  Picone", "title": "Semi-automated Annotation of Signal Events in Clinical EEG Data", "comments": "Published in IEEE Signal Processing in Medicine and Biology\n  Symposium. Philadelphia, Pennsylvania, USA", "journal-ref": "S. Yang, S. Lopez, M. Golmohammadi, I. Obeid and J. Picone,\n  \"Semi-automated annotation of signal events in clinical EEG data,\" 2016 IEEE\n  Signal Processing in Medicine and Biology Symposium (SPMB), Philadelphia, PA,\n  2016, pp. 1-5", "doi": "10.1109/SPMB.2016.7846855", "report-no": null, "categories": "eess.SP cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To be effective, state of the art machine learning technology needs large\namounts of annotated data. There are numerous compelling applications in\nhealthcare that can benefit from high performance automated decision support\nsystems provided by deep learning technology, but they lack the comprehensive\ndata resources required to apply sophisticated machine learning models.\nFurther, for economic reasons, it is very difficult to justify the creation of\nlarge annotated corpora for these applications. Hence, automated annotation\ntechniques become increasingly important. In this study, we investigated the\neffectiveness of using an active learning algorithm to automatically annotate a\nlarge EEG corpus. The algorithm is designed to annotate six types of EEG\nevents. Two model training schemes, namely threshold-based and volume-based,\nare evaluated. In the threshold-based scheme the threshold of confidence scores\nis optimized in the initial training iteration, whereas for the volume-based\nscheme only a certain amount of data is preserved after each iteration.\nRecognition performance is improved 2% absolute and the system is capable of\nautomatically annotating previously unlabeled data. Given that the\ninterpretation of clinical EEG data is an exceedingly difficult task, this\nstudy provides some evidence that the proposed method is a viable alternative\nto expensive manual annotation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 03:47:20 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Yang", "Scott", ""], ["Lopez", "Silvia", ""], ["Golmohammadi", "Meysam", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1801.02911", "submitter": "Harsh Thakkar", "authors": "Harsh Thakkar and Dharmen Punjani and Yashwant Keswani and Jens\n  Lehmann and S\\\"oren Auer", "title": "A Stitch in Time Saves Nine -- SPARQL querying of Property Graphs using\n  Gremlin Traversals", "comments": "Author's draft -- submitted to SWJ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graphs have become popular over the past years and frequently rely\non the Resource Description Framework (RDF) or Property Graphs (PG) as\nunderlying data models. However, the query languages for these two data models\n-- SPARQL for RDF and Gremlin for property graph traversal -- are lacking\ninteroperability. We present Gremlinator, a novel SPARQL to Gremlin translator.\nGremlinator translates SPARQL queries to Gremlin traversals for executing graph\npattern matching queries over graph databases. This allows to access and query\na wide variety of Graph Data Management Systems (DMS) using the W3C\nstandardized SPARQL query language and avoid the learning curve of a new Graph\nQuery Language. Gremlin is a system-agnostic traversal language covering both\nOLTP graph database or OLAP graph processors, thus making it a desirable choice\nfor supporting interoperability wrt. querying Graph DMSs. We present a\ncomprehensive empirical evaluation of Gremlinator and demonstrate its validity\nand applicability by executing SPARQL queries on top of the leading graph\nstores Neo4J, Sparksee, and Apache TinkerGraph and compare the performance with\nthe RDF stores Virtuoso, 4Store and JenaTDB. Our evaluation demonstrates the\nsubstantial performance gain obtained by the Gremlin counterparts of the SPARQL\nqueries, especially for star-shaped and complex queries.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 12:25:19 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 14:53:00 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Thakkar", "Harsh", ""], ["Punjani", "Dharmen", ""], ["Keswani", "Yashwant", ""], ["Lehmann", "Jens", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1801.02940", "submitter": "Robert Rovetto", "authors": "Robert J. Rovetto", "title": "An Ontology for Satellite Databases", "comments": null, "journal-ref": "Earth Sci Inform (2017) 10: 417", "doi": "10.1007/s12145-017-0290-x", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the development of ontology for satellite databases.\nFirst, I create a computational ontology for the Union of Concerned Scientists\n(UCS) Satellite Database (UCSSD for short), called the UCS Satellite Ontology\n(or UCSSO). Second, in developing UCSSO I show that The Space Situational\nAwareness Ontology (SSAO) (Rovetto and Kelso 2016)--an existing space domain\nreference ontology--and related ontology work by the author (Rovetto 2015,\n2016) can be used either (i) with a database-specific local ontology such as\nUCSSO, or (ii) in its stead. In case (i), local ontologies such as UCSSO can\nreuse SSAO terms, perform term mappings, or extend it. In case (ii), the\nauthor's orbital space ontology work, such as the SSAO, is usable by the UCSSD\nand organizations with other space object catalogs, as a reference ontology\nsuite providing a common semantically-rich domain model. The SSAO, UCSSO, and\nthe broader Orbital Space Environment Domain Ontology project is online at\nhttp://purl.org/space-ontology and GitHub. This ontology effort aims, in part,\nto provide accurate formal representations of the domain for various\napplications. Ontology engineering has the potential to facilitate the sharing\nand integration of satellite data from federated databases and sensors for\nsafer spaceflight.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 16:59:49 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Rovetto", "Robert J.", ""]]}, {"id": "1801.03190", "submitter": "Charalampos Tsourakakis", "authors": "Charalampos E. Tsourakakis, Shreyas Sekar, Johnson Lam, Liu Yang", "title": "Risk-Averse Matchings over Uncertain Graph Databases", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of applications such as querying sensor networks, and\nanalyzing protein-protein interaction (PPI) networks, rely on mining uncertain\ngraph and hypergraph databases. In this work we study the following problem:\ngiven an uncertain, weighted (hyper)graph, how can we efficiently find a\n(hyper)matching with high expected reward, and low risk?\n  This problem naturally arises in the context of several important\napplications, such as online dating, kidney exchanges, and team formation. We\nintroduce a novel formulation for finding matchings with maximum expected\nreward and bounded risk under a general model of uncertain weighted\n(hyper)graphs that we introduce in this work. Our model generalizes\nprobabilistic models used in prior work, and captures both continuous and\ndiscrete probability distributions, thus allowing to handle privacy related\napplications that inject appropriately distributed noise to (hyper)edge\nweights. Given that our optimization problem is NP-hard, we turn our attention\nto designing efficient approximation algorithms. For the case of uncertain\nweighted graphs, we provide a $\\frac{1}{3}$-approximation algorithm, and a\n$\\frac{1}{5}$-approximation algorithm with near optimal run time. For the case\nof uncertain weighted hypergraphs, we provide a\n$\\Omega(\\frac{1}{k})$-approximation algorithm, where $k$ is the rank of the\nhypergraph (i.e., any hyperedge includes at most $k$ nodes), that runs in\nalmost (modulo log factors) linear time.\n  We complement our theoretical results by testing our approximation algorithms\non a wide variety of synthetic experiments, where we observe in a controlled\nsetting interesting findings on the trade-off between reward, and risk. We also\nprovide an application of our formulation for providing recommendations of\nteams that are likely to collaborate, and have high impact.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 23:41:28 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Tsourakakis", "Charalampos E.", ""], ["Sekar", "Shreyas", ""], ["Lam", "Johnson", ""], ["Yang", "Liu", ""]]}, {"id": "1801.03233", "submitter": "Mohammadreza Esfandiari", "authors": "Mohammadreza Esfandiari, Senjuti Basu Roy, Sihem Amer-Yahia", "title": "Eliciting Worker Preference for Task Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current crowdsourcing platforms provide little support for worker feedback.\nWorkers are sometimes invited to post free text describing their experience and\npreferences in completing tasks. They can also use forums such as Turker\nNation1 to exchange preferences on tasks and requesters. In fact, crowdsourcing\nplatforms rely heavily on observing workers and inferring their preferences\nimplicitly. In this work, we believe that asking workers to indicate their\npreferences explicitly improve their experience in task completion and hence,\nthe quality of their contributions. Explicit elicitation can indeed help to\nbuild more accurate worker models for task completion that captures the\nevolving nature of worker preferences. We design a worker model whose accuracy\nis improved iteratively by requesting preferences for task factors such as\nrequired skills, task payment, and task relevance. We propose a generic\nframework, develop efficient solutions in realistic scenarios, and run\nextensive experiments that show the benefit of explicit preference elicitation\nover implicit ones with statistical significance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 03:55:09 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Esfandiari", "Mohammadreza", ""], ["Roy", "Senjuti Basu", ""], ["Amer-Yahia", "Sihem", ""]]}, {"id": "1801.03493", "submitter": "Kevin Hsieh", "authors": "Kevin Hsieh, Ganesh Ananthanarayanan, Peter Bodik, Paramvir Bahl,\n  Matthai Philipose, Phillip B. Gibbons, Onur Mutlu", "title": "Focus: Querying Large Video Datasets with Low Latency and Low Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large volumes of videos are continuously recorded from cameras deployed for\ntraffic control and surveillance with the goal of answering \"after the fact\"\nqueries: identify video frames with objects of certain classes (cars, bags)\nfrom many days of recorded video. While advancements in convolutional neural\nnetworks (CNNs) have enabled answering such queries with high accuracy, they\nare too expensive and slow. We build Focus, a system for low-latency and\nlow-cost querying on large video datasets. Focus uses cheap ingestion\ntechniques to index the videos by the objects occurring in them. At\ningest-time, it uses compression and video-specific specialization of CNNs.\nFocus handles the lower accuracy of the cheap CNNs by judiciously leveraging\nexpensive CNNs at query-time. To reduce query time latency, we cluster similar\nobjects and hence avoid redundant processing. Using experiments on video\nstreams from traffic, surveillance and news channels, we see that Focus uses\n58X fewer GPU cycles than running expensive ingest processors and is 37X faster\nthan processing all the video at query time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 18:52:25 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Hsieh", "Kevin", ""], ["Ananthanarayanan", "Ganesh", ""], ["Bodik", "Peter", ""], ["Bahl", "Paramvir", ""], ["Philipose", "Matthai", ""], ["Gibbons", "Phillip B.", ""], ["Mutlu", "Onur", ""]]}, {"id": "1801.03644", "submitter": "Stefan Sprenger", "authors": "Stefan Sprenger, Patrick Sch\\\"afer, Ulf Leser", "title": "Multidimensional Range Queries on Modern Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Range queries over multidimensional data are an important part of database\nworkloads in many applications. Their execution may be accelerated by using\nmultidimensional index structures (MDIS), such as kd-trees or R-trees. As for\nmost index structures, the usefulness of this approach depends on the\nselectivity of the queries, and common wisdom told that a simple scan beats\nMDIS for queries accessing more than 15%-20% of a dataset. However, this wisdom\nis largely based on evaluations that are almost two decades old, performed on\ndata being held on disks, applying IO-optimized data structures, and using\nsingle-core systems. The question is whether this rule of thumb still holds\nwhen multidimensional range queries (MDRQ) are performed on modern\narchitectures with large main memories holding all data, multi-core CPUs and\ndata-parallel instruction sets. In this paper, we study the question whether\nand how much modern hardware influences the performance ratio between index\nstructures and scans for MDRQ. To this end, we conservatively adapted three\npopular MDIS, namely the R*-tree, the kd-tree, and the VA-file, to exploit\nfeatures of modern servers and compared their performance to different flavors\nof parallel scans using multiple (synthetic and real-world) analytical\nworkloads over multiple (synthetic and real-world) datasets of varying size,\ndimensionality, and skew. We find that all approaches benefit considerably from\nusing main memory and parallelization, yet to varying degrees. Our evaluation\nindicates that, on current machines, scanning should be favored over parallel\nversions of classical MDIS even for very selective queries.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 07:03:23 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 08:44:05 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Sprenger", "Stefan", ""], ["Sch\u00e4fer", "Patrick", ""], ["Leser", "Ulf", ""]]}, {"id": "1801.03645", "submitter": "Jiangwei Zhang", "authors": "J.W. Zhang and Y.C. Tay", "title": "A tool framework for tweaking features in synthetic datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers and developers use benchmarks to compare their algorithms and\nproducts. A database benchmark must have a dataset D. To be\napplication-specific, this dataset D should be empirical. However, D may be too\nsmall, or too large, for the benchmarking experiments. D must, therefore, be\nscaled to the desired size.\n  To ensure the scaled D' is similar to D, previous work typically specifies or\nextracts a fixed set of features F = {F_1, F_2, . . . , F_n} from D, then uses\nF to generate synthetic data for D'. However, this approach (D -> F -> D')\nbecomes increasingly intractable as F gets larger, so a new solution is\nnecessary.\n  Different from existing approaches, this paper proposes ASPECT to scale D to\nenforce similarity. ASPECT first uses a size-scaler (S0) to scale D to D'. Then\nthe user selects a set of desired features F'_1, . . . , F'_n. For each desired\nfeature F'_k, there is a tweaking tool T_k that tweaks D' to make sure D' has\nthe required feature F'_k. ASPECT coordinates the tweaking of T_1,...,T_n to\nD', so T_n(...(T_1(D'))...) has the required features F'_1,...,F'_n.\n  By shifting from D -> F -> D' to D -> D' -> F', data scaling becomes\nflexible. The user can customise the scaled dataset with their own interested\nfeatures. Extensive experiments on real datasets show that ASPECT can enforce\nsimilarity in the dataset effectively and efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 07:18:16 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Zhang", "J. W.", ""], ["Tay", "Y. C.", ""]]}, {"id": "1801.03915", "submitter": "Luiz Gadelha Jr.", "authors": "Maria Luiza Mondelli, Thiago Magalh\\~aes, Guilherme Loss, Michael\n  Wilde, Ian Foster, Marta Mattoso, Daniel S. Katz, Helio J. C. Barbosa, Ana\n  Tereza R. Vasconcelos, Kary Oca\\~na, Luiz M. R. Gadelha Jr", "title": "BioWorkbench: A High-Performance Framework for Managing and Analyzing\n  Bioinformatics Experiments", "comments": null, "journal-ref": "PeerJ, 6 (2018), e5551", "doi": "10.7717/peerj.5551", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in sequencing techniques have led to exponential growth in\nbiological data, demanding the development of large-scale bioinformatics\nexperiments. Because these experiments are computation- and data-intensive,\nthey require high-performance computing (HPC) techniques and can benefit from\nspecialized technologies such as Scientific Workflow Management Systems (SWfMS)\nand databases. In this work, we present BioWorkbench, a framework for managing\nand analyzing bioinformatics experiments. This framework automatically collects\nprovenance data, including both performance data from workflow execution and\ndata from the scientific domain of the workflow application. Provenance data\ncan be analyzed through a web application that abstracts a set of queries to\nthe provenance database, simplifying access to provenance information. We\nevaluate BioWorkbench using three case studies: SwiftPhylo, a phylogenetic tree\nassembly workflow; SwiftGECKO, a comparative genomics workflow; and RASflow, a\nRASopathy analysis workflow. We analyze each workflow from both computational\nand scientific domain perspectives, by using queries to a provenance and\nannotation database. Some of these queries are available as a pre-built feature\nof the BioWorkbench web application. Through the provenance data, we show that\nthe framework is scalable and achieves high-performance, reducing up to 98% of\nthe case studies execution time. We also show how the application of machine\nlearning techniques can enrich the analysis process.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 18:35:50 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Mondelli", "Maria Luiza", ""], ["Magalh\u00e3es", "Thiago", ""], ["Loss", "Guilherme", ""], ["Wilde", "Michael", ""], ["Foster", "Ian", ""], ["Mattoso", "Marta", ""], ["Katz", "Daniel S.", ""], ["Barbosa", "Helio J. C.", ""], ["Vasconcelos", "Ana Tereza R.", ""], ["Oca\u00f1a", "Kary", ""], ["Gadelha", "Luiz M. R.", "Jr"]]}, {"id": "1801.04101", "submitter": "Fuat Basik", "authors": "Fuat Bas{\\i}k, Bu\\u{g}ra Gedik, \\c{C}a\\u{g}r{\\i} Etemo\\u{g}lu, Hakan\n  Ferhatosmano\\u{g}lu", "title": "Spatio-Temporal Linkage over Location Enhanced Services", "comments": "IEEE Transactions on Mobile Computing ( Volume: 17, Issue: 2, Feb. 1\n  2018 ) http://ieeexplore.ieee.org/document/7937913/", "journal-ref": "F. Bas{\\i}k, B. Gedik, \\c{C}. Etemo\\u{g}lu and H.\n  Ferhatosmano\\u{g}lu, \"Spatio-Temporal Linkage over Location-Enhanced\n  Services,\" in IEEE Transactions on Mobile Computing, vol. 17, no. 2, pp.\n  447-460, Feb. 1 2018", "doi": "10.1109/TMC.2017.2711027", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are witnessing an enormous growth in the volume of data generated by\nvarious online services. An important portion of this data contains geographic\nreferences, since many of these services are \\emph{location-enhanced} and thus\nproduce spatio-temporal records of their usage. We postulate that the\nspatio-temporal usage records belonging to the same real-world entity can be\nmatched across records from different location-enhanced services. Linking\nspatio-temporal records enables data analysts and service providers to obtain\ninformation that they cannot derive by analyzing only one set of usage records.\nIn this paper, we develop a new \\emph{linkage model} that can be used to match\nentities from two sets of spatio-temporal usage records belonging to two\ndifferent location-enhanced services. This linkage model is based on the\nconcept of $k$-$l$ \\emph{diversity} --- that we developed to capture both\nspatial and temporal aspects of the linkage. To realize this linkage model in\npractice, we develop a scalable linking algorithm called \\emph{ST-Link}, which\nmakes use of effective spatial and temporal filtering mechanisms that\nsignificantly reduce the search space for matching users. Furthermore,\n\\emph{ST-Link} utilizes sequential scan procedures to avoid random disk access\nand thus scales to large datasets. We evaluated our work with respect to\naccuracy and performance using several datasets. Experiments show that\n\\emph{ST-Link} is effective in practice for performing spatio-temporal linkage\nand can scale to large datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 09:32:45 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Bas\u0131k", "Fuat", ""], ["Gedik", "Bu\u011fra", ""], ["Etemo\u011flu", "\u00c7a\u011fr\u0131", ""], ["Ferhatosmano\u011flu", "Hakan", ""]]}, {"id": "1801.04891", "submitter": "K Venkatesh Emani", "authors": "K. Venkatesh Emani, S. Sudarshan", "title": "Cobra: A Framework for Cost Based Rewriting of Database Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database applications are typically written using a mixture of imperative\nlanguages and declarative frameworks for data processing. Application logic\ngets distributed across the declarative and imperative parts of a program.\nOften, there is more than one way to implement the same program, whose\nefficiency may depend on a number of parameters. In this paper, we propose a\nframework that automatically generates all equivalent alternatives of a given\nprogram using a given set of program transformations, and chooses the least\ncost alternative. We use the concept of program regions as an algebraic\nabstraction of a program and extend the Volcano/Cascades framework for\noptimization of algebraic expressions, to optimize programs. We illustrate the\nuse of our framework for optimizing database applications. We show through\nexperimental results, that our framework has wide applicability in real world\napplications and provides significant performance benefits.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 17:58:18 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 07:49:58 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 11:26:39 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Emani", "K. Venkatesh", ""], ["Sudarshan", "S.", ""]]}, {"id": "1801.05055", "submitter": "Edward Raff", "authors": "Edward Raff and Charles Nicholas", "title": "Toward Metric Indexes for Incremental Insertion and Querying", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the use of metric index structures, which accelerate\nnearest neighbor queries, in the scenario where we need to interleave\ninsertions and queries during deployment. This use-case is inspired by a\nreal-life need in malware analysis triage, and is surprisingly understudied.\nExisting literature tends to either focus on only final query efficiency, often\ndoes not support incremental insertion, or does not support arbitrary distance\nmetrics. We modify and improve three algorithms to support our scenario of\nincremental insertion and querying with arbitrary metrics, and evaluate them on\nmultiple datasets and distance metrics while varying the value of $k$ for the\ndesired number of nearest neighbors. In doing so we determine that our improved\nVantage-Point tree of Minimum-Variance performs best for this scenario.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 16:25:16 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Raff", "Edward", ""], ["Nicholas", "Charles", ""]]}, {"id": "1801.05064", "submitter": "Mohammad Roohitavaf", "authors": "Mohammad Roohitavaf and Sandeep Kulkarni", "title": "DKVF: A Framework for Rapid Prototyping and Evaluating Distributed\n  Key-value Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our framework DKVF that enables one to quickly prototype and\nevaluate new protocols for key-value stores and compare them with existing\nprotocols based on selected benchmarks. Due to limitations of CAP theorem, new\nprotocols must be developed that achieve the desired trade-off between\nconsistency and availability for the given application at hand. Hence, both\nacademic and industrial communities focus on developing new protocols that\nidentify a different (and hopefully better in one or more aspect) point on this\ntrade-off curve. While these protocols are often based on a simple intuition,\nevaluating them to ensure that they indeed provide increased availability,\nconsistency, or performance is a tedious task. Our framework, DKVF, enables one\nto quickly prototype a new protocol as well as identify how it performs\ncompared to existing protocols for pre-specified benchmarks. Our framework\nrelies on YCSB (Yahoo! Cloud Servicing Benchmark) for benchmarking. We\ndemonstrate DKVF by implementing four existing protocols --eventual\nconsistency, COPS, GentleRain and CausalSpartan-- with it. We compare the\nperformance of these protocols against different loading conditions. We find\nthat the performance is similar to our implementation of these protocols from\nscratch. And, the comparison of these protocols is consistent with what has\nbeen reported in the literature. Moreover, implementation of these protocols\nwas much more natural as we only needed to translate the pseudocode into Java\n(and add the necessary error handling). Hence, it was possible to achieve this\nin just 1-2 days per protocol. Finally, our framework is extensible. It is\npossible to replace individual components in the framework (e.g., the storage\ncomponent).\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 23:09:59 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Roohitavaf", "Mohammad", ""], ["Kulkarni", "Sandeep", ""]]}, {"id": "1801.05161", "submitter": "Sergi Nadal", "authors": "Sergi Nadal, Oscar Romero, Alberto Abell\\'o, Panos Vassiliadis and\n  Stijn Vansummeren", "title": "An Integration-Oriented Ontology to Govern Evolution in Big Data\n  Ecosystems", "comments": "Preprint submitted to Information Systems. 35 pages", "journal-ref": null, "doi": "10.1016/j.is.2018.01.006", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data architectures allow to flexibly store and process heterogeneous\ndata, from multiple sources, in their original format. The structure of those\ndata, commonly supplied by means of REST APIs, is continuously evolving. Thus\ndata analysts need to adapt their analytical processes after each API release.\nThis gets more challenging when performing an integrated or historical\nanalysis. To cope with such complexity, in this paper, we present the Big Data\nIntegration ontology, the core construct to govern the data integration process\nunder schema evolution by systematically annotating it with information\nregarding the schema of the sources. We present a query rewriting algorithm\nthat, using the annotated ontology, converts queries posed over the ontology to\nqueries over the sources. To cope with syntactic evolution in the sources, we\npresent an algorithm that semi-automatically adapts the ontology upon new\nreleases. This guarantees ontology-mediated queries to correctly retrieve data\nfrom the most recent schema version as well as correctness in historical\nqueries. A functional and performance evaluation on real-world APIs is\nperformed to validate our approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 08:55:41 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Nadal", "Sergi", ""], ["Romero", "Oscar", ""], ["Abell\u00f3", "Alberto", ""], ["Vassiliadis", "Panos", ""], ["Vansummeren", "Stijn", ""]]}, {"id": "1801.05206", "submitter": "Sebastian Herbst", "authors": "Sebastian Herbst and Johannes Tenschert and Andreas M. Wahl and Klaus\n  Meyer-Wegener", "title": "Sequences, yet Functions: The Dual Nature of Data-Stream Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-stream processing has continuously risen in importance as the amount of\navailable data has been steadily increas- ing over the last decade. Besides\ntraditional domains such as data-center monitoring and click analytics, there\nis an increasing number of network-enabled production machines that generate\ncontinuous streams of data. Due to their continuous nature, queries on\ndata-streams can be more complex, and distinctly harder to understand then\ndatabase queries. As users have to consider operational details, maintenance\nand debugging become challenging. Current approaches model data-streams as\nsequences, be- cause this is the way they are physically received. These models\nresult in an implementation-focused perspective. We explore an alternate way of\nmodeling data-streams by focusing on time-slicing semantics. This focus results\nin a model based on functions, which is better suited for reasoning about query\nsemantics. By adapting the definitions of relevant concepts in stream\nprocessing to our model, we illustrate the practical useful- ness of our\napproach. Thereby, we link data-streams and query primitives to concepts in\nfunctional programming and mathematics. Most noteworthy, we prove that\ndata-streams are monads, and show how to derive monad definitions for current\ndata-stream models. We provide an abstract, yet practical perspective on data-\nstream related subjects based on a sound, consistent query model. Our work can\nserve as solid foundation for future data-stream query-languages.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 11:05:12 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Herbst", "Sebastian", ""], ["Tenschert", "Johannes", ""], ["Wahl", "Andreas M.", ""], ["Meyer-Wegener", "Klaus", ""]]}, {"id": "1801.05360", "submitter": "Shuai Ma", "authors": "Xuelian Lin, Jiahao Jiang, Shuai Ma, Yimeng Zuo, Chunming Hu", "title": "One-Pass Trajectory Simplification Using the Synchronous Euclidean\n  Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various mobile devices have been used to collect, store and transmit\ntremendous trajectory data, and it is known that raw trajectory data seriously\nwastes the storage, network band and computing resource. To attack this issue,\none-pass line simplification (LS) algorithms have are been developed, by\ncompressing data points in a trajectory to a set of continuous line segments.\nHowever, these algorithms adopt the perpendicular Euclidean distance, and none\nof them uses the synchronous Euclidean distance (SED), and cannot support\nspatio-temporal queries. To do this, we develop two one-pass error bounded\ntrajectory simplification algorithms (CISED-S and CISED-W) using SED, based on\na novel spatio-temporal cone intersection technique. Using four real-life\ntrajectory datasets, we experimentally show that our approaches are both\nefficient and effective. In terms of running time, algorithms CISED-S and\nCISED-W are on average 3 times faster than SQUISH-E (the most efficient\nexisting LS algorithm using SED). In terms of compression ratios, algorithms\nCISED-S and CISED-W are comparable with and 19.6% better than DPSED (the most\neffective existing LS algorithm using SED) on average, respectively, and are\n21.1% and 42.4% better than SQUISH-E on average, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 09:36:09 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Lin", "Xuelian", ""], ["Jiang", "Jiahao", ""], ["Ma", "Shuai", ""], ["Zuo", "Yimeng", ""], ["Hu", "Chunming", ""]]}, {"id": "1801.05613", "submitter": "Shrainik Jain", "authors": "Shrainik Jain, Bill Howe, Jiaqi Yan, Thierry Cruanes", "title": "Query2Vec: An Evaluation of NLP Techniques for Generalized Workload\n  Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider methods for learning vector representations of SQL queries to\nsupport generalized workload analytics tasks, including workload summarization\nfor index selection and predicting queries that will trigger memory errors. We\nconsider vector representations of both raw SQL text and optimized query plans,\nand evaluate these methods on synthetic and real SQL workloads. We find that\ngeneral algorithms based on vector representations can outperform existing\napproaches that rely on specialized features. For index recommendation, we\ncluster the vector representations to compress large workloads with no loss in\nperformance from the recommended index. For error prediction, we train a\nclassifier over learned vectors that can automatically relate subtle syntactic\npatterns with specific errors raised during query execution. Surprisingly, we\nalso find that these methods enable transfer learning, where a model trained on\none SQL corpus can be applied to an unrelated corpus and still enable good\nperformance. We find that these general approaches, when trained on a large\ncorpus of SQL queries, provides a robust foundation for a variety of workload\nanalysis tasks and database features, without requiring application-specific\nfeature engineering.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 10:21:49 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 22:07:06 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Jain", "Shrainik", ""], ["Howe", "Bill", ""], ["Yan", "Jiaqi", ""], ["Cruanes", "Thierry", ""]]}, {"id": "1801.05643", "submitter": "Felix Martin Schuhknecht", "authors": "Ankur Sharma, Felix Martin Schuhknecht, Jens Dittrich", "title": "The Case for Automatic Database Administration using Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like any large software system, a full-fledged DBMS offers an overwhelming\namount of configuration knobs. These range from static initialisation\nparameters like buffer sizes, degree of concurrency, or level of replication to\ncomplex runtime decisions like creating a secondary index on a particular\ncolumn or reorganising the physical layout of the store. To simplify the\nconfiguration, industry grade DBMSs are usually shipped with various advisory\ntools, that provide recommendations for given workloads and machines. However,\nreality shows that the actual configuration, tuning, and maintenance is usually\nstill done by a human administrator, relying on intuition and experience.\nRecent work on deep reinforcement learning has shown very promising results in\nsolving problems, that require such a sense of intuition. For instance, it has\nbeen applied very successfully in learning how to play complicated games with\nenormous search spaces. Motivated by these achievements, in this work we\nexplore how deep reinforcement learning can be used to administer a DBMS.\nFirst, we will describe how deep reinforcement learning can be used to\nautomatically tune an arbitrary software system like a DBMS by defining a\nproblem environment. Second, we showcase our concept of NoDBA at the concrete\nexample of index selection and evaluate how well it recommends indexes for\ngiven workloads.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 12:51:01 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Sharma", "Ankur", ""], ["Schuhknecht", "Felix Martin", ""], ["Dittrich", "Jens", ""]]}, {"id": "1801.05800", "submitter": "R\\'emi Cura", "authors": "Remi Cura, Julien Perret, Nicolas Paparoditis", "title": "Interactive in-base street model edit: how common GIS software and a\n  database can serve as a custom Graphical User Interface", "comments": "this article is an extract from PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our modern world produces an increasing quantity of data, and especially\ngeospatial data, with advance of sensing technologies, and growing complexity\nand organisation of vector data. Tools are needed to efficiently create and\nedit those vector geospatial data. Procedural generation has been a tool of\nchoice to generate strongly organised data, yet it may be hard to control.\nBecause those data may be involved to take consequence-full real life\ndecisions, user interactions are required to check data and edit it. The\nclassical process to do so would be to build an adhoc Graphical User Interface\n(GUI) tool adapted for the model and method being used. This task is difficult,\ntakes a large amount of resources, and is very specific to one model, making it\nhard to share and re-use.\n  Besides, many common generic GUI already exists to edit vector data, each\nhaving its specialities. We propose a change of paradigm; instead of building a\nspecific tool for one task, we use common GIS software as GUIs, and deport the\nspecific interactions from the software to within the database. In this\nparadigm, GIS software simply modify geometry and attributes of database\nlayers, and those changes are used by the database to perform automated task.\n  This new paradigm has many advantages. The first one is genericity. With\nin-base interaction, any GIS software can be used to perform edition, whatever\nthe software is a Desktop sofware or a web application. The second is\nconcurrency and coherency. Because interaction is in-base, use of database\nfeatures allows seamless multi-user work, and can guarantee that the data is in\na coherent state. Last we propose tools to facilitate multi-user edits, both\nduring the edit phase (each user knows what areas are edited by other users),\nand before and after edit (planning of edit, analyse of edited areas).\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 18:55:43 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Cura", "Remi", ""], ["Perret", "Julien", ""], ["Paparoditis", "Nicolas", ""]]}, {"id": "1801.06027", "submitter": "Divya Mahajan", "authors": "Divya Mahajan, Joon Kyung Kim, Jacob Sacks, Adel Ardalan, Arun Kumar,\n  Hadi Esmaeilzadeh", "title": "In-RDBMS Hardware Acceleration of Advanced Analytics", "comments": null, "journal-ref": "Divya Mahajan, Joon Kyung Kim, Jacob Sacks, Adel Ardalan, Arun\n  Kumar, and Hadi Esmaeilzadeh. In-RDBMS Hardware Acceleration of Advanced\n  Analytics. PVLDB, 11(11): 1317-1331, 2018", "doi": "10.14778/3236187.3236188", "report-no": null, "categories": "cs.DB cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data revolution is fueled by advances in machine learning, databases, and\nhardware design. Programmable accelerators are making their way into each of\nthese areas independently. As such, there is a void of solutions that enables\nhardware acceleration at the intersection of these disjoint fields. This paper\nsets out to be the initial step towards a unifying solution for in-Database\nAcceleration of Advanced Analytics (DAnA). Deploying specialized hardware, such\nas FPGAs, for in-database analytics currently requires hand-designing the\nhardware and manually routing the data. Instead, DAnA automatically maps a\nhigh-level specification of advanced analytics queries to an FPGA accelerator.\nThe accelerator implementation is generated for a User Defined Function (UDF),\nexpressed as a part of an SQL query using a Python-embedded Domain-Specific\nLanguage (DSL). To realize an efficient in-database integration, DAnA\naccelerators contain a novel hardware structure, Striders, that directly\ninterface with the buffer pool of the database. Striders extract, cleanse, and\nprocess the training data tuples that are consumed by a multi-threaded FPGA\nengine that executes the analytics algorithm. We integrate DAnA with PostgreSQL\nto generate hardware accelerators for a range of real-world and synthetic\ndatasets running diverse ML algorithms. Results show that DAnA-enhanced\nPostgreSQL provides, on average, 8.3x end-to-end speedup for real datasets,\nwith a maximum of 28.2x. Moreover, DAnA-enhanced PostgreSQL is, on average,\n4.0x faster than the multi-threaded Apache MADLib running on Greenplum. DAnA\nprovides these benefits while hiding the complexity of hardware design from\ndata scientists and allowing them to express the algorithm in =30-60 lines of\nPython.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 19:04:13 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 13:55:56 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Mahajan", "Divya", ""], ["Kim", "Joon Kyung", ""], ["Sacks", "Jacob", ""], ["Ardalan", "Adel", ""], ["Kumar", "Arun", ""], ["Esmaeilzadeh", "Hadi", ""]]}, {"id": "1801.06258", "submitter": "Tana Wattanawaroon", "authors": "Tana Wattanawaroon, Stephen Macke, Aditya Parameswaran", "title": "Towards a Theory of Data-Diff: Optimal Synthesis of Succinct Data\n  Modification Scripts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the Data-Diff problem: given a dataset and a subsequent\nversion of the dataset, find the shortest sequence of operations that\ntransforms the dataset to the subsequent version, under a restricted family of\noperations. We consider operations similar to SQL UPDATE, each with a condition\n(WHERE) that matches a subset of tuples and a modifier (SET) that makes changes\nto those matched tuples. We characterize the problem based on different\nconstraints on the attributes and the allowed conditions and modifiers,\nproviding complexity classification and algorithms in each case.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 00:02:34 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Wattanawaroon", "Tana", ""], ["Macke", "Stephen", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1801.06340", "submitter": "Marc Shapiro", "authors": "Marc Shapiro (1), Annette Bieniusa, Nuno Pregui\\c{c}a (2), Valter\n  Balegas (2), Christopher Meiklejohn (3) ((1) DELYS, (2) NOVA-LINCS, (3) UCL)", "title": "Just-Right Consistency: reconciling availability and safety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By the CAP Theorem, a distributed data storage system can ensure either\nConsistency under Partition (CP) or Availability under Partition (AP), but not\nboth. This has led to a split between CP databases, in which updates are\nsynchronous, and AP databases, where they are asynchronous. However, there is\nno inherent reason to treat all updates identically: simply, the system should\nbe as available as possible, and synchronised just enough for the application\nto be correct. We offer a principled Just-Right Consistency approach to\ndesigning such applications, reconciling correctness with availability and\nperformance, based on the following insights:(i) The Conflict-Free Replicated\nData Type (CRDTs) data model supports asynchronous updates in an intuitive and\nprincipled way.(ii) Invariants involving joint or mutually-ordered updates are\ncompatible with AP and can be guaranteed by Transactional Causal Consistency,\nthe strongest consistency model that does not compromise availability.\nRegarding the remaining, \"CAP-sensitive\" invariants:(iii) For the common\npattern of Bounded Counters, we provide encapsulated data type that is proven\ncorrect and is efficient; (iv) in the general case, static analysis can\nidentify when synchronisation is not necessary for correctness.Our Antidote\ncloud database system supports CRDTs, Transactional Causal Consistency and the\nBounded Counter data type. Support tools help design applications by static\nanalysis and proof of CAP-sensitive invariants. This system supports\nindustrial-grade applications and has been tested experimentally with hundreds\nof servers across several geo-distributed data centres.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 09:32:54 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Shapiro", "Marc", "", "DELYS"], ["Bieniusa", "Annette", "", "NOVA-LINCS"], ["Pregui\u00e7a", "Nuno", "", "NOVA-LINCS"], ["Balegas", "Valter", "", "NOVA-LINCS"], ["Meiklejohn", "Christopher", "", "UCL"]]}, {"id": "1801.06396", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Mouhamadou Lamine Ba, Daniel Deutch, Pierre\n  Senellart", "title": "Computing Possible and Certain Answers over Order-Incomplete Data", "comments": "55 pages, 56 references. Extended journal version of\n  arXiv:1707.07222. Up to the stylesheet, page/environment numbering, and\n  possible minor publisher-induced changes, this is the exact content of the\n  journal paper that will appear in Theoretical Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the complexity of query evaluation for databases whose\nrelations are partially ordered; the problem commonly arises when combining or\ntransforming ordered data from multiple sources. We focus on queries in a\nuseful fragment of SQL, namely positive relational algebra with aggregates,\nwhose bag semantics we extend to the partially ordered setting. Our semantics\nleads to the study of two main computational problems: the possibility and\ncertainty of query answers. We show that these problems are respectively\nNP-complete and coNP-complete, but identify tractable cases depending on the\nquery operators or input partial orders. We further introduce a duplicate\nelimination operator and study its effect on the complexity results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 13:20:56 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 15:10:45 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Amarilli", "Antoine", ""], ["Ba", "Mouhamadou Lamine", ""], ["Deutch", "Daniel", ""], ["Senellart", "Pierre", ""]]}, {"id": "1801.06402", "submitter": "Arnab Bhattacharya", "authors": "Jithin Vachery, Akhil Arora, Sayan Ranu, Arnab Bhattacharya", "title": "RAQ: Relationship-Aware Graph Querying in Large Networks", "comments": null, "journal-ref": "WWW 2019", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenal growth of graph data from a wide variety of real-world\napplications has rendered graph querying to be a problem of paramount\nimportance. Traditional techniques use structural as well as node similarities\nto find matches of a given query graph in a (large) target graph. However,\nalmost all existing techniques have tacitly ignored the presence of\nrelationships in graphs, which are usually encoded through interactions between\nnode and edge labels. In this paper, we propose RAQ -- Relationship-Aware Graph\nQuerying, to mitigate this gap. Given a query graph, RAQ identifies the $k$\nbest matching subgraphs of the target graph that encode similar relationships\nas in the query graph. To assess the utility of RAQ as a graph querying\nparadigm for knowledge discovery and exploration tasks, we perform a user\nsurvey on the Internet Movie Database (IMDb), where an overwhelming 86% of the\n170 surveyed users preferred the relationship-aware match over traditional\ngraph querying. The need to perform subgraph isomorphism renders RAQ NP-hard.\nThe querying is made practical through beam stack search. Extensive experiments\non multiple real-world graph datasets demonstrate RAQ to be effective,\nefficient, and scalable.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 13:50:47 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 17:18:32 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 14:46:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Vachery", "Jithin", ""], ["Arora", "Akhil", ""], ["Ranu", "Sayan", ""], ["Bhattacharya", "Arnab", ""]]}, {"id": "1801.06408", "submitter": "Xin Wang", "authors": "Xin Wang, Eugene Siow, Aastha Madaan, Thanassis Tiropanis", "title": "PRESTO: Probabilistic Cardinality Estimation for RDF Queries Based on\n  Subgraph Overlapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In query optimisation accurate cardinality estimation is essential for\nfinding optimal query plans. It is especially challenging for RDF due to the\nlack of explicit schema and the excessive occurrence of joins in RDF queries.\nExisting approaches typically collect statistics based on the counts of triples\nand estimate the cardinality of a query as the product of its join components,\nwhere errors can accumulate even when the estimation of each component is\naccurate. As opposed to existing methods, we propose PRESTO, a cardinality\nestimation method that is based on the counts of subgraphs instead of triples\nand uses a probabilistic method to estimate cardinalities of RDF queries as a\nwhole. PRESTO avoids some major issues of existing approaches and is able to\naccurately estimate arbitrary queries under a bound memory constraint. We\nevaluate PRESTO with YAGO and show that PRESTO is more accurate for both simple\nand complex queries.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 14:11:45 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Wang", "Xin", ""], ["Siow", "Eugene", ""], ["Madaan", "Aastha", ""], ["Tiropanis", "Thanassis", ""]]}, {"id": "1801.06750", "submitter": "Theodoros Rekatsinas", "authors": "Christopher De Sa, Ihab F. Ilyas, Benny Kimelfeld, Christopher Re,\n  Theodoros Rekatsinas", "title": "A Formal Framework For Probabilistic Unclean Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most theoretical frameworks that focus on data errors and inconsistencies\nfollow logic-based reasoning. Yet, practical data cleaning tools need to\nincorporate statistical reasoning to be effective in real-world data cleaning\ntasks. Motivated by these empirical successes, we propose a formal framework\nfor unclean databases, where two types of statistical knowledge are\nincorporated: The first represents a belief of how intended (clean) data is\ngenerated, and the second represents a belief of how noise is introduced in the\nactual observed database instance. To capture this noisy channel model, we\nintroduce the concept of a Probabilistic Unclean Database (PUD), a triple that\nconsists of a probabilistic database that we call the intention, a\nprobabilistic data transformator that we call the realization and captures how\nnoise is introduced, and a dirty observed database instance that we call the\nobservation. We define three computational problems in the PUD framework:\ncleaning (infer the most probable clean instance given a PUD), probabilistic\nquery answering (compute the probability of an answer tuple over the unclean\nobserved instance), and learning (estimate the most likely intention and\nrealization models of a PUD given a collection of training data). We illustrate\nthe PUD framework on concrete representations of the intention and realization,\nshow that they generalize traditional concepts of repairs such as cardinality\nand value repairs, draw connection to consistent query answering, and prove\ntractability results. We further show that parameters can be learned in\npractical instantiations, and in fact, prove that under certain conditions we\ncan learn a PUD directly from a single dirty database instance without any need\nfor clean examples.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 01:56:07 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 01:43:45 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 01:05:07 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["De Sa", "Christopher", ""], ["Ilyas", "Ihab F.", ""], ["Kimelfeld", "Benny", ""], ["Re", "Christopher", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "1801.06766", "submitter": "F A Rezaur Rahman Chowdhury", "authors": "Mohammad Hossain Namaki, F A Rezaur Rahman Chowdhury, Md Rakibul\n  Islam, Janardhan Rao Doppa, Yinghui Wu", "title": "Learning to Speed Up Query Planning in Graph Databases", "comments": "Published in the Proceedings of the 27th International Conference on\n  Automated Planning and Scheduling (ICAPS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying graph structured data is a fundamental operation that enables\nimportant applications including knowledge graph search, social network\nanalysis, and cyber-network security. However, the growing size of real-world\ndata graphs poses severe challenges for graph databases to meet the\nresponse-time requirements of the applications. Planning the computational\nsteps of query processing - Query Planning - is central to address these\nchallenges. In this paper, we study the problem of learning to speedup query\nplanning in graph databases towards the goal of improving the\ncomputational-efficiency of query processing via training queries.We present a\nLearning to Plan (L2P) framework that is applicable to a large class of query\nreasoners that follow the Threshold Algorithm (TA) approach. First, we define a\ngeneric search space over candidate query plans, and identify target search\ntrajectories (query plans) corresponding to the training queries by performing\nan expensive search. Subsequently, we learn greedy search control knowledge to\nimitate the search behavior of the target query plans. We provide a concrete\ninstantiation of our L2P framework for STAR, a state-of-the-art graph query\nreasoner. Our experiments on benchmark knowledge graphs including DBpedia,\nYAGO, and Freebase show that using the query plans generated by the learned\nsearch control knowledge, we can significantly improve the speed of STAR with\nnegligible loss in accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 04:49:23 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Namaki", "Mohammad Hossain", ""], ["Chowdhury", "F A Rezaur Rahman", ""], ["Islam", "Md Rakibul", ""], ["Doppa", "Janardhan Rao", ""], ["Wu", "Yinghui", ""]]}, {"id": "1801.06965", "submitter": "Thapana Boonchoo", "authors": "Thapana Boonchoo, Xiang Ao, Qing He", "title": "An Efficient Density-based Clustering Algorithm for Higher-Dimensional\n  Data", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DBSCAN is a typically used clustering algorithm due to its clustering ability\nfor arbitrarily-shaped clusters and its robustness to outliers. Generally, the\ncomplexity of DBSCAN is O(n^2) in the worst case, and it practically becomes\nmore severe in higher dimension. Grid-based DBSCAN is one of the recent\nimproved algorithms aiming at facilitating efficiency. However, the performance\nof grid-based DBSCAN still suffers from two problems: neighbour explosion and\nredundancies in merging, which make the algorithms infeasible in\nhigh-dimensional space. In this paper, we propose a novel algorithm named GDPAM\nattempting to extend Grid-based DBSCAN to higher data dimension. In GDPAM, a\nbitmap indexing is utilized to manage non-empty grids so that the neighbour\ngrid queries can be performed efficiently. Furthermore, we adopt an efficient\nunion-find algorithm to maintain the clustering information in order to reduce\nredundancies in the merging. The experimental results on both real-world and\nsynthetic datasets demonstrate that the proposed algorithm outperforms the\nstate-of-the-art exact/approximate DBSCAN and suggests a good scalability.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 06:35:17 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Boonchoo", "Thapana", ""], ["Ao", "Xiang", ""], ["He", "Qing", ""]]}, {"id": "1801.07005", "submitter": "Annette Bieniusa", "authors": "Mathias Weber, Annette Bieniusa", "title": "ACGreGate: A Framework for Practical Access Control for Applications\n  using Weakly Consistent Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable and highly available systems often require data stores that offer\nweaker consistency guarantees than traditional relational databases systems.\nThe correctness of these applications highly depends on the resilience of the\napplication model against data inconsistencies. In particular regarding\napplication security, it is difficult to determine which inconsistencies can be\ntolerated and which might lead to security breaches.\n  In this paper, we discuss the problem of how to develop an access control\nlayer for applications using weakly consistent data stores without loosing the\nperformance benefits gained by using weaker consistency models. We present\nACGreGate, a Java framework for implementing correct access control layers for\napplications using weakly consistent data stores. Under certain requirements on\nthe data store, ACGreGate ensures that the access control layer operates\ncorrectly with respect to dynamically adaptable security policies. We used\nACGreGate to implement the access control layer of a student management system.\nThis case study shows that practically useful security policies can be\nimplemented with the framework incurring little overhead. A comparison with a\nsetup using a centralized server shows the benefits of using ACGreGate for\nscalability of the service to geo-scale.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 09:23:42 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Weber", "Mathias", ""], ["Bieniusa", "Annette", ""]]}, {"id": "1801.07215", "submitter": "Chao Yan", "authors": "Chao Yan, Bo Li, Yevgeniy Vorobeychik, Aron Laszka, Daniel Fabbri,\n  Bradley Malin", "title": "Get Your Workload in Order: Game Theoretic Prioritization of Database\n  Auditing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.DB cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For enhancing the privacy protections of databases, where the increasing\namount of detailed personal data is stored and processed, multiple mechanisms\nhave been developed, such as audit logging and alert triggers, which notify\nadministrators about suspicious activities; however, the two main limitations\nin common are: 1) the volume of such alerts is often substantially greater than\nthe capabilities of resource-constrained organizations, and 2) strategic\nattackers may disguise their actions or carefully choosing which records they\ntouch, making incompetent the statistical detection models. For solving them,\nwe introduce a novel approach to database auditing that explicitly accounts for\nadversarial behavior by 1) prioritizing the order in which types of alerts are\ninvestigated and 2) providing an upper bound on how much resource to allocate\nfor each type. We model the interaction between a database auditor and\npotential attackers as a Stackelberg game in which the auditor chooses an\nauditing policy and attackers choose which records to target. A corresponding\napproach combining linear programming, column generation, and heuristic search\nis proposed to derive an auditing policy. For testing the policy-searching\nperformance, a publicly available credit card application dataset are adopted,\non which it shows that our methods produce high-quality mixed strategies as\ndatabase audit policies, and our general approach significantly outperforms\nnon-game-theoretic baselines.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 17:42:32 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Yan", "Chao", ""], ["Li", "Bo", ""], ["Vorobeychik", "Yevgeniy", ""], ["Laszka", "Aron", ""], ["Fabbri", "Daniel", ""], ["Malin", "Bradley", ""]]}, {"id": "1801.07237", "submitter": "Eugene Wu", "authors": "Fotis Psallidas, Eugene Wu", "title": "Smoke: Fine-grained Lineage at Interactive Speed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data lineage describes the relationship between individual input and output\ndata items of a workflow, and has served as an integral ingredient for both\ntraditional (e.g., debugging, auditing, data integration, and security) and\nemergent (e.g., interactive visualizations, iterative analytics, explanations,\nand cleaning) applications. The core, long-standing problem that lineage\nsystems need to address---and the main focus of this paper---is to capture the\nrelationships between input and output data items across a workflow with the\ngoal to streamline queries over lineage. Unfortunately, current lineage systems\neither incur high lineage capture overheads, or lineage query processing costs,\nor both. As a result, applications, that in principle can express their logic\ndeclaratively in lineage terms, resort to hand-tuned implementations. To this\nend, we introduce Smoke, an in-memory database engine that neither lineage\ncapture overhead nor lineage query processing needs to be compromised. To do\nso, Smoke introduces tight integration of the lineage capture logic into\nphysical database operators; efficient, write-optimized lineage representations\nfor storage; and optimizations when future lineage queries are known up-front.\nOur experiments on microbenchmarks and realistic workloads show that Smoke\nreduces the lineage capture overhead and streamlines lineage queries by\nmultiple orders of magnitude compared to state-of-the-art alternatives. Our\nexperiments on real-world applications highlight that Smoke can meet the\nlatency requirements of interactive visualizations (e.g., <150ms) and\noutperform hand-written implementations of data profiling primitives.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 18:39:18 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Psallidas", "Fotis", ""], ["Wu", "Eugene", ""]]}, {"id": "1801.07653", "submitter": "Daniel Hornung", "authors": "Timm Fitschen, Alexander Schlemmer, Daniel Hornung, Henrik tom\n  W\\\"orden, Ulrich Parlitz, Stefan Luther", "title": "CaosDB - Research Data Management for Complex, Changing, and Automated\n  Research Workflows", "comments": null, "journal-ref": "Data 2019, 4(2), 83", "doi": "10.3390/data4020083", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Here we present CaosDB, a Research Data Management System (RDMS) designed to\nensure seamless integration of inhomogeneous data sources and repositories of\nlegacy data. Its primary purpose is the management of data from biomedical\nsciences, both from simulations and experiments during the complete research\ndata lifecycle. An RDMS for this domain faces particular challenges: Research\ndata arise in huge amounts, from a wide variety of sources, and traverse a\nhighly branched path of further processing. To be accepted by its users, an\nRDMS must be built around workflows of the scientists and practices and thus\nsupport changes in workflow and data structure. Nevertheless it should\nencourage and support the development and observation of standards and\nfurthermore facilitate the automation of data acquisition and processing with\nspecialized software. The storage data model of an RDMS must reflect these\ncomplexities with appropriate semantics and ontologies while offering simple\nmethods for finding, retrieving, and understanding relevant data. We show how\nCaosDB responds to these challenges and give an overview of the CaosDB Server,\nits data model and its easy-to-learn CaosDB Query Language. We briefly discuss\nthe status of the implementation, how we currently use CaosDB, and how we plan\nto use and extend it.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:46:18 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 12:46:10 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Fitschen", "Timm", ""], ["Schlemmer", "Alexander", ""], ["Hornung", "Daniel", ""], ["W\u00f6rden", "Henrik tom", ""], ["Parlitz", "Ulrich", ""], ["Luther", "Stefan", ""]]}, {"id": "1801.07789", "submitter": "Abdul Rahman Sherzad", "authors": "Abdul Rahman Sherzad", "title": "Data is the Fuel of Organizations: Opportunities and Challenges in\n  Afghanistan", "comments": "This paper consists of 14 pages, and it includes 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, the author at first briefly outlines the value of data in\norganizations and the opportunities and challenges in Afghanistan. Then the\nauthor takes the Kankor (National University Entrance Exam) data, particularly\nnames of participants, locations, high schools and higher education\ninstitutions into account and explains how these data, that organizations in\nAfghanistan do not use for anything, can be useful in several cases and areas.\nThe application of these data is shown through cases such as Auto filling\nmissing values, identifying names of people, locations, and institutions from\nunstructured text, generating fake data to benchmark the database and web\napplication performance and appearance, comparing and matching high school data\nwith Kankor data, producing the top-n male and female names very common in\nAfghanistan or province-wise, and the data mining application in education and\nhigher education institutions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 13:23:54 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Sherzad", "Abdul Rahman", ""]]}, {"id": "1801.07947", "submitter": "Eugene Siow", "authors": "Eugene Siow, Thanassis Tiropanis, Xin Wang and Wendy Hall", "title": "TritanDB: Time-series Rapid Internet of Things Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient management of data is an important prerequisite for realising\nthe potential of the Internet of Things (IoT). Two issues given the large\nvolume of structured time-series IoT data are, addressing the difficulties of\ndata integration between heterogeneous Things and improving ingestion and query\nperformance across databases on both resource-constrained Things and in the\ncloud. In this paper, we examine the structure of public IoT data and discover\nthat the majority exhibit unique flat, wide and numerical characteristics with\na mix of evenly and unevenly-spaced time-series. We investigate the advances in\ntime-series databases for telemetry data and combine these findings with\nmicrobenchmarks to determine the best compression techniques and storage data\nstructures to inform the design of a novel solution optimised for IoT data. A\nquery translation method with low overhead even on resource-constrained Things\nallows us to utilise rich data models like the Resource Description Framework\n(RDF) for interoperability and data integration on top of the optimised\nstorage. Our solution, TritanDB, shows an order of magnitude performance\nimprovement across both Things and cloud hardware on many state-of-the-art\ndatabases within IoT scenarios. Finally, we describe how TritanDB supports\nvarious analyses of IoT time-series data like forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 12:10:46 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Siow", "Eugene", ""], ["Tiropanis", "Thanassis", ""], ["Wang", "Xin", ""], ["Hall", "Wendy", ""]]}, {"id": "1801.08052", "submitter": "Daniel Kaltenthaler", "authors": "Daniel Kaltenthaler and Johannes-Y. Lohrer", "title": "The Historic Development of the Zooarchaeological Database OssoBook and\n  the xBook Framework for Scientific Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we describe the historic development of the\nzooarchaeological database OssoBook and the resulting framework xBook, a\ngeneric infrastructure for distributed, relational data management that is\nmainly designed for the needs of scientific data. We describe the concepts of\nthe architecture and its most important features. We especially point out the\nServer-Client architecture, the synchronization process, the Launcher\napplication, and the structure and features of the application.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 16:07:39 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Kaltenthaler", "Daniel", ""], ["Lohrer", "Johannes-Y.", ""]]}, {"id": "1801.08336", "submitter": "Nikolaos Bikakis", "authors": "Nikos Bikakis", "title": "Big Data Visualization Tools", "comments": "This article appears in Encyclopedia of Big Data Technologies,\n  Springer, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization is the presentation of data in a pictorial or graphical\nformat, and a data visualization tool is the software that generates this\npresentation. Data visualization provides users with intuitive means to\ninteractively explore and analyze data, enabling them to effectively identify\ninteresting patterns, infer correlations and causalities, and supports\nsense-making activities.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 10:16:48 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 22:03:28 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Bikakis", "Nikos", ""]]}, {"id": "1801.08588", "submitter": "Ilya Kolchinsky", "authors": "Ilya Kolchinsky and Assaf Schuster", "title": "Efficient Adaptive Detection of Complex Event Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex event processing (CEP) is widely employed to detect occurrences of\npredefined combinations (patterns) of events in massive data streams. As new\nevents are accepted, they are matched using some type of evaluation structure,\ncommonly optimized according to the statistical properties of the data items in\nthe input stream. However, in many real-life scenarios the data characteristics\nare never known in advance or are subject to frequent on-the-fly changes. To\nmodify the evaluation structure as a reaction to such changes, adaptation\nmechanisms are employed. These mechanisms typically function by monitoring a\nset of properties and applying a new evaluation plan when significant deviation\nfrom the initial values is observed. This strategy often leads to missing\nimportant input changes or it may incur substantial computational overhead by\nover-adapting. In this paper, we present an efficient and precise method for\ndynamically deciding whether and how the evaluation structure should be\nreoptimized. This method is based on a small set of constraints to be satisfied\nby the monitored values, defined such that a better evaluation plan is\nguaranteed if any of the constraints is violated. To the best of our knowledge,\nour proposed mechanism is the first to provably avoid false positives on\nreoptimization decisions. We formally prove this claim and demonstrate how our\nmethod can be applied on known algorithms for evaluation plan generation. Our\nextensive experimental evaluation on real-world datasets confirms the\nsuperiority of our strategy over existing methods in terms of performance and\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 20:17:18 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2018 14:04:58 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kolchinsky", "Ilya", ""], ["Schuster", "Assaf", ""]]}, {"id": "1801.09039", "submitter": "Ming-Hung Shih", "authors": "Trong Duc Nguyen, Ming-Hung Shih, Divesh Srivastava, Srikanta\n  Tirthapura, Bojian Xu", "title": "Variance-Optimal Offline and Streaming Stratified Random Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stratified random sampling (SRS) is a fundamental sampling technique that\nprovides accurate estimates for aggregate queries using a small size sample,\nand has been used widely for approximate query processing. A key question in\nSRS is how to partition a target sample size among different strata. While\nNeyman allocation provides a solution that minimizes the variance of an\nestimate using this sample, it works under the assumption that each stratum is\nabundant, i.e., has a large number of data points to choose from. This\nassumption may not hold in general: one or more strata may be bounded, and may\nnot contain a large number of data points, even though the total data size may\nbe large.\n  We first present VOILA, an offline method for allocating sample sizes to\nstrata in a variance-optimal manner, even for the case when one or more strata\nmay be bounded. We next consider SRS on streaming data that are continuously\narriving. We show a lower bound, that any streaming algorithm for SRS must have\n(in the worst case) a variance that is {\\Omega}(r) factor away from the\noptimal, where r is the number of strata. We present S-VOILA, a practical\nstreaming algorithm for SRS that is locally variance-optimal in its allocation\nof sample sizes to different strata. Our result from experiments on real and\nsynthetic data show that VOILA can have significantly (1.4 to 50.0 times)\nsmaller variance than Neyman allocation. The streaming algorithm S-VOILA\nresults in a variance that is typically close to VOILA, which was given the\nentire input beforehand.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 05:28:48 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 03:32:49 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 05:49:10 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Nguyen", "Trong Duc", ""], ["Shih", "Ming-Hung", ""], ["Srivastava", "Divesh", ""], ["Tirthapura", "Srikanta", ""], ["Xu", "Bojian", ""]]}, {"id": "1801.09240", "submitter": "Youhuan Li", "authors": "Youhuan Li and Lei Zou and M. Tamer Ozsu and Dongyan Zhao", "title": "Time Constrained Continuous Subgraph Search over Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing popularity of dynamic applications such as social networks\nprovides a promising way to detect valuable information in real time. Efficient\nanalysis over high-speed data from dynamic applications is of great\nsignificance. Data from these dynamic applications can be easily modeled as\nstreaming graph. In this paper, we study the subgraph (isomorphism) search over\nstreaming graph data that obeys timing order constraints over the occurrence of\nedges in the stream. We propose a data structure and algorithm to efficiently\nanswer subgraph search and introduce optimizations to greatly reduce the space\ncost, and propose concurrency management to improve system throughput.\nExtensive experiments on real network traffic data and synthetic social\nstreaming data confirms the efficiency and effectiveness of our solution.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 14:43:04 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 14:39:49 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 03:09:37 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Li", "Youhuan", ""], ["Zou", "Lei", ""], ["Ozsu", "M. Tamer", ""], ["Zhao", "Dongyan", ""]]}, {"id": "1801.09413", "submitter": "Ilya Kolchinsky", "authors": "Ilya Kolchinsky and Assaf Schuster", "title": "Join Query Optimization Techniques for Complex Event Processing\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex event processing (CEP) is a prominent technology used in many modern\napplications for monitoring and tracking events of interest in massive data\nstreams. CEP engines inspect real-time information flows and attempt to detect\ncombinations of occurrences matching predefined patterns. This is done by\ncombining basic data items, also called primitive events, according to a\npattern detection plan, in a manner similar to the execution of multi-join\nqueries in traditional data management systems. Despite this similarity, little\nwork has been done on utilizing existing join optimization methods to improve\nthe performance of CEP-based systems. In this paper, we provide the first\ntheoretical and experimental study of the relationship between these two\nresearch areas. We formally prove that the CEP Plan Generation problem is\nequivalent to the Join Query Plan Generation problem for a restricted class of\npatterns and can be reduced to it for a considerably wider range of classes.\nThis result implies the NP-completeness of the CEP Plan Generation problem. We\nfurther show how join query optimization techniques developed over the last\ndecades can be adapted and utilized to provide practically efficient solutions\nfor complex event detection. Our experiments demonstrate the superiority of\nthese techniques over existing strategies for CEP optimization in terms of\nthroughput, latency, and memory consumption.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 09:23:36 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2018 13:16:15 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kolchinsky", "Ilya", ""], ["Schuster", "Assaf", ""]]}, {"id": "1801.09556", "submitter": "Harsh Thakkar", "authors": "Harsh Thakkar and Dharmen Punjani and Jens Lehmann and S\\\"oren Auer", "title": "Killing Two Birds with One Stone -- Querying Property Graphs using\n  SPARQL via GREMLINATOR", "comments": "4 pages, 8 figures, DEMO paper submission. arXiv admin note: text\n  overlap with arXiv:1801.02911", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graphs have become popular over the past decade and frequently rely\non the Resource Description Framework (RDF) or Property Graph (PG) databases as\ndata models. However, the query languages for these two data models -- SPARQL\nfor RDF and the PG traversal language Gremlin -- are lacking interoperability.\nWe present Gremlinator, the first translator from SPARQL -- the W3C\nstandardized language for RDF -- and Gremlin -- a popular property graph\ntraversal language. Gremlinator translates SPARQL queries to Gremlin path\ntraversals for executing graph pattern matching queries over graph databases.\nThis allows a user, who is well versed in SPARQL, to access and query a wide\nvariety of Graph Data Management Systems (DMSs) avoiding the steep learning\ncurve for adapting to a new Graph Query Language (GQL). Gremlin is a graph\ncomputing system-agnostic traversal language (covering both OLTP graph database\nor OLAP graph processors), making it a desirable choice for supporting\ninteroperability for querying Graph DMSs. Gremlinator currently supports the\ntranslation of a subset of SPARQL 1.0, specifically the SPARQL SELECT queries.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 23:15:36 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Thakkar", "Harsh", ""], ["Punjani", "Dharmen", ""], ["Lehmann", "Jens", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1801.09619", "submitter": "Boris Motik", "authors": "Giorgio Stefanoni and Boris Motik and Egor V. Kostylev", "title": "Estimating the Cardinality of Conjunctive Queries over RDF Data Using\n  Graph Summarisation", "comments": null, "journal-ref": null, "doi": "10.1145/3178876.3186003", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the cardinality (i.e., the number of answers) of conjunctive\nqueries is particularly difficult in RDF systems: queries over RDF data are\nnavigational and thus tend to involve many joins. We present a new, principled\ncardinality estimation technique based on graph summarisation. We interpret a\nsummary of an RDF graph using a possible world semantics and formalise the\nestimation problem as computing the expected cardinality over all RDF graphs\nrepresented by the summary, and we present a closed-form formula for computing\nthe expectation of arbitrary queries. We also discuss approaches to RDF graph\nsummarisation. Finally, we show empirically that our cardinality technique is\nmore accurate and more consistent, often by orders of magnitude, than the state\nof the art.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 16:48:33 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 11:52:02 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Stefanoni", "Giorgio", ""], ["Motik", "Boris", ""], ["Kostylev", "Egor V.", ""]]}, {"id": "1801.09639", "submitter": "Hui Li", "authors": "Hui Li and Sizhe Peng and Jian Li and Jingjing Li and Jiangtao Cui and\n  Jianfeng Ma", "title": "ONCE and ONCE+: Counting the Frequency of Time-constrained Serial\n  Episodes in a Streaming Sequence", "comments": "14 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a representative sequential pattern mining problem, counting the frequency\nof serial episodes from a streaming sequence has drawn continuous attention in\nacademia due to its wide application in practice, e.g., telecommunication\nalarms, stock market, transaction logs, bioinformatics, etc. Although a number\nof serial episodes mining algorithms have been developed recently, most of them\nare neither stream-oriented, as they require multi-pass of dataset, nor\ntime-aware, as they fail to take into account the time constraint of serial\nepisodes. In this paper, we propose two novel one-pass algorithms, ONCE and\nONCE+, each of which can respectively compute two popular frequencies of given\nepisodes satisfying predefined time-constraint as signals in a stream arrives\none-after-another. ONCE is only used for non-overlapped frequency where the\noccurrences of a serial episode in sequence are not intersected. ONCE+ is\ndesigned for the distinct frequency where the occurrences of a serial episode\ndo not share any event. Theoretical study proves that our algorithm can\ncorrectly mine the frequency of target time constraint serial episodes in a\ngiven stream. Experimental study over both real-world and synthetic datasets\ndemonstrates that the proposed algorithm can work, with little time and space,\nin signal-intensive streams where millions of signals arrive within a single\nsecond. Moreover, the algorithm has been applied in a real stream processing\nsystem, where the efficacy and efficiency of this work is tested in practical\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 17:26:07 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Li", "Hui", ""], ["Peng", "Sizhe", ""], ["Li", "Jian", ""], ["Li", "Jingjing", ""], ["Cui", "Jiangtao", ""], ["Ma", "Jianfeng", ""]]}, {"id": "1801.09709", "submitter": "Yuanyuan Tian", "authors": "Brian Hentschel, Peter J. Haas, Yuanyuan Tian", "title": "Temporally-Biased Sampling for Online Model Management", "comments": "17 pages, 14 figures, extended version of an EDBT'18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To maintain the accuracy of supervised learning models in the presence of\nevolving data streams, we provide temporally-biased sampling schemes that\nweight recent data most heavily, with inclusion probabilities for a given data\nitem decaying exponentially over time. We then periodically retrain the models\non the current sample. This approach speeds up the training process relative to\ntraining on all of the data. Moreover, time-biasing lets the models adapt to\nrecent changes in the data while -- unlike in a sliding-window approach --\nstill keeping some old data to ensure robustness in the face of temporary\nfluctuations and periodicities in the data values. In addition, the\nsampling-based approach allows existing analytic algorithms for static data to\nbe applied to dynamic streaming data essentially without change. We provide and\nanalyze both a simple sampling scheme (T-TBS) that probabilistically maintains\na target sample size and a novel reservoir-based scheme (R-TBS) that is the\nfirst to provide both complete control over the decay rate and a guaranteed\nupper bound on the sample size, while maximizing both expected sample size and\nsample-size stability. The latter scheme rests on the notion of a \"fractional\nsample\" and, unlike T-TBS, allows for data arrival rates that are unknown and\ntime varying. R-TBS and T-TBS are of independent interest, extending the known\nset of unequal-probability sampling schemes. We discuss distributed\nimplementation strategies; experiments in Spark illuminate the performance and\nscalability of the algorithms, and show that our approach can increase machine\nlearning robustness in the face of evolving data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 19:10:15 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Hentschel", "Brian", ""], ["Haas", "Peter J.", ""], ["Tian", "Yuanyuan", ""]]}, {"id": "1801.09802", "submitter": "Maaz Bin Safeer Ahmad", "authors": "Maaz Bin Safeer Ahmad and Alvin Cheung", "title": "Automatically Leveraging MapReduce Frameworks for Data-Intensive\n  Applications", "comments": "12 pages, additional 4 pages of references and appendix", "journal-ref": "SIGMOD '18 Proceedings of the 2018 International Conference on\n  Management of Data, Pages 1205-1220", "doi": "10.1145/3183713.3196891", "report-no": null, "categories": "cs.DB cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce is a popular programming paradigm for developing large-scale,\ndata-intensive computation. Many frameworks that implement this paradigm have\nrecently been developed. To leverage these frameworks, however, developers must\nbecome familiar with their APIs and rewrite existing code. Casper is a new tool\nthat automatically translates sequential Java programs into the MapReduce\nparadigm. Casper identifies potential code fragments to rewrite and translates\nthem in two steps: (1) Casper uses program synthesis to search for a program\nsummary (i.e., a functional specification) of each code fragment. The summary\nis expressed using a high-level intermediate language resembling the MapReduce\nparadigm and verified to be semantically equivalent to the original using a\ntheorem prover. (2) Casper generates executable code from the summary, using\neither the Hadoop, Spark, or Flink API. We evaluated Casper by automatically\nconverting real-world, sequential Java benchmarks to MapReduce. The resulting\nbenchmarks perform up to 48.2x faster compared to the original.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 00:02:57 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 05:01:08 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Ahmad", "Maaz Bin Safeer", ""], ["Cheung", "Alvin", ""]]}, {"id": "1801.09973", "submitter": "Nikolaos Bikakis", "authors": "Nikos Bikakis, Vana Kalogeraki, Dimitrios Gunopulos", "title": "Social Event Scheduling", "comments": "This paper appears in 34th IEEE International Conference on Data\n  Engineering (ICDE 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for social event organizers (e.g., event planning and\nmarketing companies, venues) is attracting the maximum number of participants,\nsince it has great impact on the success of the event, and, consequently, the\nexpected gains (e.g., revenue, artist/brand publicity). In this paper, we\nintroduce the Social Event Scheduling (SES) problem, which schedules a set of\nsocial events considering user preferences and behavior, events' spatiotemporal\nconflicts, and competing vents, in order to maximize the overall number of\nattendees. We show that SES is strongly NP-hard, even in highly restricted\ninstances. To cope with the hardness of the SES problem we design a greedy\napproximation algorithm. Finally, we evaluate our method experimentally using a\ndataset from the Meetup event-based social network.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 13:16:51 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 23:09:54 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Bikakis", "Nikos", ""], ["Kalogeraki", "Vana", ""], ["Gunopulos", "Dimitrios", ""]]}, {"id": "1801.10207", "submitter": "Alex Galakatos", "authors": "Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca,\n  Tim Kraska", "title": "FITing-Tree: A Data-aware Index Structure", "comments": "18 pages", "journal-ref": "SIGMOD (2019) 1189-1206", "doi": "10.1145/3299869.3319860", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Index structures are one of the most important tools that DBAs leverage to\nimprove the performance of analytics and transactional workloads. However,\nbuilding several indexes over large datasets can often become prohibitive and\nconsume valuable system resources. In fact, a recent study showed that indexes\ncreated as part of the TPC-C benchmark can account for 55% of the total memory\navailable in a modern DBMS. This overhead consumes valuable and expensive main\nmemory, and limits the amount of space available to store new data or process\nexisting data.\n  In this paper, we present FITing-Tree, a novel form of a learned index which\nuses piece-wise linear functions with a bounded error specified at construction\ntime. This error knob provides a tunable parameter that allows a DBA to FIT an\nindex to a dataset and workload by being able to balance lookup performance and\nspace consumption. To navigate this tradeoff, we provide a cost model that\nhelps determine an appropriate error parameter given either (1) a lookup\nlatency requirement (e.g., 500ns) or (2) a storage budget (e.g., 100MB). Using\na variety of real-world datasets, we show that our index is able to provide\nperformance that is comparable to full index structures while reducing the\nstorage footprint by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 20:22:53 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 14:24:15 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Galakatos", "Alex", ""], ["Markovitch", "Michael", ""], ["Binnig", "Carsten", ""], ["Fonseca", "Rodrigo", ""], ["Kraska", "Tim", ""]]}, {"id": "1801.10323", "submitter": "Shantanu Sharma", "authors": "Shlomi Dolev, Peeyush Gupta, Yin Li, Sharad Mehrotra, Shantanu Sharma", "title": "Privacy-Preserving Secret Shared Computations using MapReduce", "comments": "IEEE Transactions on Dependable and Secure Computing, Accepted 01\n  Aug. 2019", "journal-ref": null, "doi": "10.1109/TDSC.2019.2933844", "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data outsourcing allows data owners to keep their data at \\emph{untrusted}\nclouds that do not ensure the privacy of data and/or computations. One useful\nframework for fault-tolerant data processing in a distributed fashion is\nMapReduce, which was developed for \\emph{trusted} private clouds. This paper\npresents algorithms for data outsourcing based on Shamir's secret-sharing\nscheme and for executing privacy-preserving SQL queries such as count,\nselection including range selection, projection, and join while using MapReduce\nas an underlying programming model. Our proposed algorithms prevent an\nadversary from knowing the database or the query while also preventing\noutput-size and access-pattern attacks. Interestingly, our algorithms do not\ninvolve the database owner, which only creates and distributes secret-shares\nonce, in answering any query, and hence, the database owner also cannot learn\nthe query. Logically and experimentally, we evaluate the efficiency of the\nalgorithms on the following parameters: (\\textit{i}) the number of\ncommunication rounds (between a user and a server), (\\textit{ii}) the total\namount of bit flow (between a user and a server), and (\\textit{iii}) the\ncomputational load at the user and the server.\\B\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 07:02:10 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 05:23:30 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 03:15:04 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Dolev", "Shlomi", ""], ["Gupta", "Peeyush", ""], ["Li", "Yin", ""], ["Mehrotra", "Sharad", ""], ["Sharma", "Shantanu", ""]]}]