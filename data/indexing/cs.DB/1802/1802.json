[{"id": "1802.00230", "submitter": "Daniel Kondratyuk", "authors": "Dan Kondratyuk, Jake Rodden, Elmer Duran", "title": "Integrity Coded Databases: An Evaluation of Performance, Efficiency, and\n  Practicality", "comments": "11 pages, 7 figures. Research Experience for Undergraduates in\n  Software Security, Boise State University, July 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, cloud database storage has become an inexpensive and\nconvenient option for businesses and individuals to store information. While\nits positive aspects make the cloud extremely attractive for data storage, it\nis a relatively new area of service, making it vulnerable to cyber-attacks and\nsecurity breaches. Storing data in a foreign location also requires the owner\nto relinquish control of their information to system administrators of these\nonline database services. This opens the possibility for malicious, internal\nattacks on the data that may involve the manipulation, omission, or addition of\ndata. The retention of the data as it was intended to be stored is referred to\nas the database's integrity. Our research tests a potential solution for\nmaintaining the integrity of these cloud-storage databases by converting the\noriginal databases to Integrity Coded Databases (ICDB). ICDBs utilize Integrity\nCodes: cryptographic codes created alongside the data by a private key that\nonly the data owner has access to. When the database is queried, an integrity\ncode is returned along with the queried information. The owner is then able to\nverify that the information is correct, complete, and fresh. Consequently,\nICDBs also incur performance and memory penalties. In our research, we explore,\ntest, and benchmark ICDBs to determine the costs and benefits of maintaining an\nICDB versus a standard database.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 10:30:20 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Kondratyuk", "Dan", ""], ["Rodden", "Jake", ""], ["Duran", "Elmer", ""]]}, {"id": "1802.00259", "submitter": "Timo Schindler", "authors": "Timo Schindler", "title": "Anomaly Detection in Log Data using Graph Databases and Machine Learning\n  to Defend Advanced Persistent Threats", "comments": "Lecture Notes in Informatics (LNI), Gesellschaft f\\\"ur Informatik,\n  Bonn 2017 2371", "journal-ref": null, "doi": "10.18420/in2017_241", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Advanced Persistent Threats (APTs) are a main impendence in cyber security of\ncomputer networks. In 2015, a successful breach remains undetected 146 days on\naverage, reported by [Fi16].With our work we demonstrate a feasible and fast\nway to analyse real world log data to detect breaches or breach attempts. By\nadapting well-known kill chain mechanisms and a combine of a time series\ndatabase and an abstracted graph approach, it is possible to create flexible\nattack profiles. Using this approach, it can be demonstrated that the graph\nanalysis successfully detects simulated attacks by analysing the log data of a\nsimulated computer network. Considering another source for log data, the\nframework is capable to deliver sufficient performance for analysing real-world\ndata in short time. By using the computing power of the graph database it is\npossible to identify the attacker and furthermore it is feasible to detect\nother affected system components. We believe to significantly reduce the\ndetection time of breaches with this approach and react fast to new attack\nvectors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 12:17:54 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Schindler", "Timo", ""]]}, {"id": "1802.00304", "submitter": "Malika Bendechache", "authors": "Malika Bendechache and M-Tahar Kechadi", "title": "Distributed Clustering Algorithm for Spatial Data Mining", "comments": "6 pages. arXiv admin note: text overlap with arXiv:1704.03421", "journal-ref": "Spatial Data Mining and Geographical Knowledge Services (ICSDM),\n  2015 2nd IEEE International Conference on, pages 60--65, 2015", "doi": "10.1109/ICSDM.2015.7298026", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data mining techniques and mainly distributed clustering are\nwidely used in the last decade because they deal with very large and\nheterogeneous datasets which cannot be gathered centrally. Current distributed\nclustering approaches are normally generating global models by aggregating\nlocal results that are obtained on each site. While this approach mines the\ndatasets on their locations the aggregation phase is complex, which may produce\nincorrect and ambiguous global clusters and therefore incorrect knowledge. In\nthis paper we propose a new clustering approach for very large spatial datasets\nthat are heterogeneous and distributed. The approach is based on K-means\nAlgorithm but it generates the number of global clusters dynamically. Moreover,\nthis approach uses an elaborated aggregation phase. The aggregation phase is\ndesigned in such a way that the overall process is efficient in time and memory\nallocation. Preliminary results show that the proposed approach produces high\nquality results and scales up well. We also compared it to two popular\nclustering algorithms and show that this approach is much more efficient.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 14:41:33 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Bendechache", "Malika", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1802.00688", "submitter": "Malika Bendechache", "authors": "Malika Bendechache, Nhien-An Le-Khac and M-Tahar Kechadi", "title": "Hierarchical Aggregation Approach for Distributed clustering of spatial\n  datasets", "comments": "6 pages. arXiv admin note: substantial text overlap with\n  arXiv:1704.03421", "journal-ref": null, "doi": "10.1109/ICDMW.2016.0158", "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new approach of distributed clustering for\nspatial datasets, based on an innovative and efficient aggregation technique.\nThis distributed approach consists of two phases: 1) local clustering phase,\nwhere each node performs a clustering on its local data, 2) aggregation phase,\nwhere the local clusters are aggregated to produce global clusters. This\napproach is characterised by the fact that the local clusters are represented\nin a simple and efficient way. And The aggregation phase is designed in such a\nway that the final clusters are compact and accurate while the overall process\nis efficient in both response time and memory allocation. We evaluated the\napproach with different datasets and compared it to well-known clustering\ntechniques. The experimental results show that our approach is very promising\nand outperforms all those algorithms\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 12:50:59 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Bendechache", "Malika", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1802.00696", "submitter": "Diego Didona Dr", "authors": "Diego Didona, Willy Zwaenepoel", "title": "Size-aware Sharding For Improving Tail Latencies in In-memory Key-value\n  Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the concept of size-aware sharding to improve tail\nlatencies for in-memory key-value stores, and describes its implementation in\nthe Minos key-value store. Tail latencies are crucial in distributed\napplications with high fan-out ratios, because overall response time is\ndetermined by the slowest response. Size-aware sharding distributes requests\nfor keys to cores according to the size of the item associated with the key. In\nparticular, requests for small and large items are sent to disjoint subsets of\ncores. Size-aware sharding improves tail latencies by avoiding head-of-line\nblocking, in which a request for a small item gets queued behind a request for\na large item. Alternative size-unaware approaches to sharding, such as\nkeyhash-based sharding, request dispatching and stealing do not avoid\nhead-of-line blocking, and therefore exhibit worse tail latencies. The\nchallenge in implementing size-aware sharding is to maintain high throughput by\navoiding the cost of software dispatching and by achieving load balancing\nbetween different cores. Minos uses hardware dispatch for all requests for\nsmall items, which form the very large majority of all requests. It achieves\nload balancing by adapting the number of cores handling requests for small and\nlarge items to their relative presence in the workload. We compare Minos to\nthree state-of-the-art designs of in-memory KV stores. Compared to its closest\ncompetitor, Minos achieves a 99th percentile latency that is up to two orders\nof magnitude lower. Put differently, for a given value for the 99th percentile\nlatency equal to 10 times the mean service time, Minos achieves a throughput\nthat is up to 7.4 times higher.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 14:23:00 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Didona", "Diego", ""], ["Zwaenepoel", "Willy", ""]]}, {"id": "1802.01065", "submitter": "Kijung Shin", "authors": "Kijung Shin, Bryan Hooi, Jisu Kim, and Christos Faloutsos", "title": "Detecting Group Anomalies in Tera-Scale Multi-Aspect Data via\n  Dense-Subtensor Mining", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we detect fraudulent lockstep behavior in large-scale multi-aspect\ndata (i.e., tensors)? Can we detect it when data are too large to fit in memory\nor even on a disk? Past studies have shown that dense subtensors in real-world\ntensors (e.g., social media, Wikipedia, TCP dumps, etc.) signal anomalous or\nfraudulent behavior such as retweet boosting, bot activities, and network\nattacks. Thus, various approaches, including tensor decomposition and search,\nhave been proposed for detecting dense subtensors rapidly and accurately.\nHowever, existing methods have low accuracy, or they assume that tensors are\nsmall enough to fit in main memory, which is unrealistic in many real-world\napplications such as social media and web. To overcome these limitations, we\npropose D-CUBE, a disk-based dense-subtensor detection method, which also can\nrun in a distributed manner across multiple machines. Compared to\nstate-of-the-art methods, D-CUBE is (1) Memory Efficient: requires up to 1,561X\nless memory and handles 1,000X larger data (2.6TB), (2) Fast: up to 7X faster\ndue to its near-linear scalability, (3) Provably Accurate: gives a guarantee on\nthe densities of the detected subtensors, and (4) Effective: spotted network\nattacks from TCP dumps and synchronized behavior in rating data most\naccurately.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 03:06:53 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 03:56:00 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 09:03:33 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Shin", "Kijung", ""], ["Hooi", "Bryan", ""], ["Kim", "Jisu", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1802.01508", "submitter": "Markus Schmid", "authors": "Dominik D. Freydenberger and Markus L. Schmid", "title": "Deterministic Regular Expressions With Back-References", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern libraries for regular expression matching allow back-references\n(i.e., repetition operators) that substantially increase expressive power, but\nalso lead to intractability. In order to find a better balance between\nexpressiveness and tractability, we combine these with the notion of\ndeterminism for regular expressions used in XML DTDs and XML Schema. This\nincludes the definition of a suitable automaton model, and a generalization of\nthe Glushkov construction. We demonstrate that, compared to their\nnon-deterministic superclass, these deterministic regular expressions with\nback-references have desirable algorithmic properties (i.e., efficiently\nsolvable membership problem and some decidable problems in static analysis),\nwhile, at the same time, their expressive power exceeds that of deterministic\nregular expressions without back-references.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 16:47:45 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Freydenberger", "Dominik D.", ""], ["Schmid", "Markus L.", ""]]}, {"id": "1802.01554", "submitter": "Piotr Piotr-Ostropolski", "authors": "Grzegorz G{\\l}uch, Jerzy Marcinkowski, Piotr Ostropolski-Nalewaja", "title": "Can One Escape Red Chains? Regular Path Queries Determinacy is\n  Undecidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given set of queries (which are expressions in some query language)\n$\\mathcal{Q}=\\{Q_1$, $Q_2, \\ldots Q_k\\}$ and for another query $Q_0$ we say\nthat $\\mathcal{Q}$ determines $Q_0$ if -- informally speaking -- for every\ndatabase $\\mathbb D$, the information contained in the views\n$\\mathcal{Q}({\\mathbb D})$ is sufficient to compute $Q_0({\\mathbb D})$. Query\nDeterminacy Problem is the problem of deciding, for given $\\mathcal{Q}$ and\n$Q_0$, whether $\\mathcal{Q}$ determines $Q_0$. Many versions of this problem,\nfor different query languages, were studied in database theory. In this paper\nwe solve a problem stated in [CGLV02] and show that Query Determinacy Problem\nis undecidable for the Regular Path Queries -- the paradigmatic query language\nof graph databases.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 18:33:16 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["G\u0142uch", "Grzegorz", ""], ["Marcinkowski", "Jerzy", ""], ["Ostropolski-Nalewaja", "Piotr", ""]]}, {"id": "1802.02229", "submitter": "Shumo Chu", "authors": "Shumo Chu, Brendan Murphy, Jared Roesch, Alvin Cheung, and Dan Suciu", "title": "Axiomatic Foundations and Algorithms for Deciding Semantic Equivalences\n  of SQL Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deciding the equivalence of SQL queries is a fundamental problem in data\nmanagement. As prior work has mainly focused on studying the theoretical\nlimitations of the problem, very few implementations for checking such\nequivalences exist. In this paper, we present a new formalism and\nimplementation for reasoning about the equivalences of SQL queries. Our\nformalism, U-semiring, extends SQL's semiring semantics with unbounded\nsummation and duplicate elimination. U-semiring is defined using only very few\naxioms and can thus be easily implemented using proof assistants such as Coq\nfor automated query reasoning. Yet, they are sufficient enough to enable us\nreason about sophisticated SQL queries that are evaluated over bags and sets,\nalong with various integrity constraints. To evaluate the effectiveness of\nU-semiring, we have used it to formally verify 39 query rewrite rules from both\nclassical data management research papers and real-world SQL engines, where\nmany of them have never been proven correct before.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 21:40:50 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 01:01:38 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 01:20:14 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Chu", "Shumo", ""], ["Murphy", "Brendan", ""], ["Roesch", "Jared", ""], ["Cheung", "Alvin", ""], ["Suciu", "Dan", ""]]}, {"id": "1802.02254", "submitter": "Ping Zhang", "authors": "Ping Zhang, Zhifeng Bao, Yuchen Li, Guoliang Li, Yipeng Zhang, Zhiyong\n  Peng", "title": "Trajectory-driven Influential Billboard Placement", "comments": null, "journal-ref": null, "doi": "10.1145/3219819.3219946", "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study the problem of trajectory-driven\ninfluential billboard placement: given a set of billboards $U$ (each with a\nlocation and a cost), a database of trajectories $\\mathcal{T}$ and a budget\n$L$, find a set of billboards within the budget to influence the largest number\nof trajectories. One core challenge is to identify and reduce the overlap of\nthe influence from different billboards to the same trajectories, while keeping\nthe budget constraint into consideration. We show that this problem is NP-hard\nand present an enumeration based algorithm with $(1-1/e)$ approximation ratio.\nHowever, the enumeration should be very costly when $|U|$ is large. By\nexploiting the locality property of billboards' influence, we propose a\npartition-based framework PartSel. PartSel partitions $U$ into a set of small\nclusters, computes the locally influential billboards for each cluster, and\nmerges them to generate the global solution. Since the local solutions can be\nobtained much more efficient than the global one, PartSel should reduce the\ncomputation cost greatly; meanwhile it achieves a non-trivial approximation\nratio guarantee. Then we propose a LazyProbe method to further prune billboards\nwith low marginal influence, while achieving the same approximation ratio as\nPartSel. Experiments on real datasets verify the efficiency and effectiveness\nof our methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 23:05:02 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 11:13:46 GMT"}, {"version": "v3", "created": "Sun, 9 Sep 2018 14:43:37 GMT"}, {"version": "v4", "created": "Sat, 15 Sep 2018 07:43:40 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Zhang", "Ping", ""], ["Bao", "Zhifeng", ""], ["Li", "Yuchen", ""], ["Li", "Guoliang", ""], ["Zhang", "Yipeng", ""], ["Peng", "Zhiyong", ""]]}, {"id": "1802.02301", "submitter": "Kyung Joong Kim", "authors": "EunJo Lee, Yoonjae Jang, DuMim Yoon, JiHoon Jeon, Seong-il Yang,\n  Sang-Kwang Lee, Dae-Wook Kim, Pei Pei Chen, Anna Guitart, Paul Bertens,\n  \\'Africa Peri\\'a\\~nez, Fabian Hadiji, Marc M\\\"uller, Youngjun Joo, Jiyeon\n  Lee, Inchon Hwang and Kyung-Joong Kim", "title": "Game Data Mining Competition on Churn Prediction and Survival Analysis\n  using Commercial Game Log Data", "comments": "IEEE Transactions on Games", "journal-ref": "IEEE Transactions on Games, 2018", "doi": "10.1109/TG.2018.2888863", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game companies avoid sharing their game data with external researchers. Only\na few research groups have been granted limited access to game data so far. The\nreluctance of these companies to make data publicly available limits the wide\nuse and development of data mining techniques and artificial intelligence\nresearch specific to the game industry. In this work, we developed and\nimplemented an international competition on game data mining using commercial\ngame log data from one of the major game companies in South Korea: NCSOFT. Our\napproach enabled researchers to develop and apply state-of-the-art data mining\ntechniques to game log data by making the data open. For the competition, data\nwere collected from Blade & Soul, an action role-playing game, from NCSOFT. The\ndata comprised approximately 100 GB of game logs from 10,000 players. The main\naim of the competition was to predict whether a player would churn and when the\nplayer would churn during two periods between which the business model was\nchanged to a free-to-play model from a monthly subscription. The results of the\ncompetition revealed that highly ranked competitors used deep learning, tree\nboosting, and linear regression.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 04:20:24 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 06:11:21 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 13:47:38 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 06:36:26 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Lee", "EunJo", ""], ["Jang", "Yoonjae", ""], ["Yoon", "DuMim", ""], ["Jeon", "JiHoon", ""], ["Yang", "Seong-il", ""], ["Lee", "Sang-Kwang", ""], ["Kim", "Dae-Wook", ""], ["Chen", "Pei Pei", ""], ["Guitart", "Anna", ""], ["Bertens", "Paul", ""], ["Peri\u00e1\u00f1ez", "\u00c1frica", ""], ["Hadiji", "Fabian", ""], ["M\u00fcller", "Marc", ""], ["Joo", "Youngjun", ""], ["Lee", "Jiyeon", ""], ["Hwang", "Inchon", ""], ["Kim", "Kyung-Joong", ""]]}, {"id": "1802.02827", "submitter": "Thed Leeuwen van", "authors": "Thed van Leeuwen, Ingeborg Meijer, Alfredo Yegros-Yegros and Rodrigo\n  Costas", "title": "Developing indicators on Open Access by combining evidence from diverse\n  data sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last couple of years, the role of Open Access (OA) publishing has\nbecome central in science management and research policy. In the UK and the\nNetherlands, national OA mandates require the scientific community to seriously\nconsider publishing research outputs in OA forms. At the same time, other\nelements of Open Science are becoming also part of the debate, thus including\nnot only publishing research outputs but also other related aspects of the\nchain of scientific knowledge production such as open peer review and open\ndata. From a research management point of view, it is important to keep track\nof the progress made in the OA publishing debate. Until now, this has been\nquite problematic, given the fact that OA as a topic is hard to grasp by\nbibliometric methods, as most databases supporting bibliometric data lack\nexhaustive and accurate open access labelling of scientific publications. In\nthis study, we present a methodology that systematically creates OA labels for\nlarge sets of publications processed in the Web of Science database. The\nmethodology is based on the combination of diverse data sources that provide\nevidence of publications being OA\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 12:36:57 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["van Leeuwen", "Thed", ""], ["Meijer", "Ingeborg", ""], ["Yegros-Yegros", "Alfredo", ""], ["Costas", "Rodrigo", ""]]}, {"id": "1802.02872", "submitter": "Marie Le Guilly", "authors": "Marie Le Guilly (BD), Jean-Marc Petit (BD), Vasile-Marian Scuturici\n  (BD)", "title": "SQL Query Completion for Data Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the big data tsunami, relational databases and SQL are still there and\nremain mandatory in most of cases for accessing data. On the one hand, SQL is\neasy-to-use by non specialists and allows to identify pertinent initial data at\nthe very beginning of the data exploration process. On the other hand, it is\nnot always so easy to formulate SQL queries: nowadays, it is more and more\nfrequent to have several databases available for one application domain, some\nof them with hundreds of tables and/or attributes. Identifying the pertinent\nconditions to select the desired data, or even identifying relevant attributes\nis far from trivial. To make it easier to write SQL queries, we propose the\nnotion of SQL query completion: given a query, it suggests additional\nconditions to be added to its WHERE clause. This completion is semantic, as it\nrelies on the data from the database, unlike current completion tools that are\nmostly syntactic. Since the process can be repeated over and over again --\nuntil the data analyst reaches her data of interest --, SQL query completion\nfacilitates the exploration of databases. SQL query completion has been\nimplemented in a SQL editor on top of a database management system. For the\nevaluation, two questions need to be studied: first, does the completion speed\nup the writing of SQL queries? Second , is the completion easily adopted by\nusers? A thorough experiment has been conducted on a group of 70 computer\nscience students divided in two groups (one with the completion and the other\none without) to answer those questions. The results are positive and very\npromising.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 14:20:30 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Guilly", "Marie Le", "", "BD"], ["Petit", "Jean-Marc", "", "BD"], ["Scuturici", "Vasile-Marian", "", "BD"]]}, {"id": "1802.02914", "submitter": "George Christodoulides", "authors": "George Christodoulides (ILC)", "title": "Praaline: Integrating Tools for Speech Corpus Research", "comments": null, "journal-ref": "Proceedings of the 9th International Conference on Language\n  Resources and Evaluation (LREC), May 2014, Reykjavik, Iceland", "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Praaline, an open-source software system for managing,\nannotating, analysing and visualising speech corpora. Researchers working with\nspeech corpora are often faced with multiple tools and formats, and they need\nto work with ever-increasing amounts of data in a collaborative way. Praaline\nintegrates and extends existing time-proven tools for spoken corpora analysis\n(Praat, Sonic Visualiser and a bridge to the R statistical package) in a\nmodular system, facilitating automation and reuse. Users are exposed to an\nintegrated, user-friendly interface from which to access multiple tools. Corpus\nmetadata and annotations may be stored in a database, locally or remotely, and\nusers can define the metadata and annotation structure. Users may run a\ncustomisable cascade of analysis steps, based on plug-ins and scripts, and\nupdate the database with the results. The corpus database may be queried, to\nproduce aggregated data-sets. Praaline is extensible using Python or C++\nplug-ins, while Praat and R scripts may be executed against the corpus data. A\nseries of visualisations, editors and plug-ins are provided. Praaline is free\nsoftware, released under the GPL license.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 15:15:51 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Christodoulides", "George", "", "ILC"]]}, {"id": "1802.03057", "submitter": "Jinho Lee", "authors": "Gabriel Tanase, Toyotaro Suzumura, Jinho Lee, Chun-Fu Chen, Jason\n  Crawford, Hiroki Kanezashi, Song Zhang, Warut D.Vijitbenjaronk", "title": "System G Distributed Graph Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need to extract knowledge and value from interconnected\ndata, graph analytics on big data is a very active area of research in both\nindustry and academia. To support graph analytics efficiently a large number of\nin memory graph libraries, graph processing systems and graph databases have\nemerged. Projects in each of these categories focus on particular aspects such\nas static versus dynamic graphs, off line versus on line processing, small\nversus large graphs, etc. While there has been much advance in graph processing\nin the past decades, there is still a need for a fast graph processing, using a\ncluster of machines with distributed storage. In this paper, we discuss a novel\ndistributed graph database called System G designed for efficient graph data\nstorage and processing on modern computing architectures. In particular we\ndescribe a single node graph database and a runtime and communication layer\nthat allows us to compose a distributed graph database from multiple single\nnode instances. From various industry requirements, we find that fast\ninsertions and large volume concurrent queries are critical parts of the graph\ndatabases and we optimize our database for such features. We experimentally\nshow the efficiency of System G for storing data and processing graph queries\non state-of-the-art platforms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 21:42:28 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Tanase", "Gabriel", ""], ["Suzumura", "Toyotaro", ""], ["Lee", "Jinho", ""], ["Chen", "Chun-Fu", ""], ["Crawford", "Jason", ""], ["Kanezashi", "Hiroki", ""], ["Zhang", "Song", ""], ["Vijitbenjaronk", "Warut D.", ""]]}, {"id": "1802.03074", "submitter": "An Yan", "authors": "An Yan, Nicholas Weber", "title": "Mining Open Government Data Used in Scientific Research", "comments": "Accepted to iConference 2018", "journal-ref": "Transforming Digital Worlds. iConference 2018. Lecture Notes in\n  Computer Science, vol 10766. Springer, Cham", "doi": "10.1007/978-3-319-78105-1_34", "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following paper, we describe results from mining citations, mentions,\nand links to open government data (OGD) in peer-reviewed literature. We\ninductively develop a method for categorizing how OGD are used by different\nresearch communities, and provide descriptive statistics about the publication\nyears, publication outlets, and OGD sources. Our results demonstrate that, 1.\nThe use of OGD in research is steadily increasing from 2009 to 2016; 2.\nResearchers use OGD from 96 different open government data portals, with\ndata.gov.uk and data.gov being the most frequent sources; and, 3.Contrary to\nprevious findings, we provide evidence suggesting that OGD from developing\nnations, notably India and Kenya, are being frequently used to fuel scientific\ndiscoveries. The findings of this paper contribute to ongoing research agendas\naimed at tracking the impact of open government data initiatives, and provides\nan initial description of how open government data are valuable to diverse\nscientific research communities.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 23:18:44 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 06:44:10 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 10:04:59 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yan", "An", ""], ["Weber", "Nicholas", ""]]}, {"id": "1802.03638", "submitter": "Tommaso Soru", "authors": "Tommaso Soru and Andr\\'e Valdestilhas and Edgard Marx and Axel-Cyrille\n  Ngonga Ngomo", "title": "Beyond Markov Logic: Efficient Mining of Prediction Rules in Large\n  Graphs", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representations of large knowledge bases may comprise billions of\nedges. Usually built upon human-generated ontologies, several knowledge bases\ndo not feature declared ontological rules and are far from being complete.\nCurrent rule mining approaches rely on schemata or store the graph in-memory,\nwhich can be unfeasible for large graphs. In this paper, we introduce\nHornConcerto, an algorithm to discover Horn clauses in large graphs without the\nneed of a schema. Using a standard fact-based confidence score, we can mine\nclose Horn rules having an arbitrary body size. We show that our method can\noutperform existing approaches in terms of runtime and memory consumption and\nmine high-quality rules for the link prediction task, achieving\nstate-of-the-art results on a widely-used benchmark. Moreover, we find that\nrules alone can perform inference significantly faster than embedding-based\nmethods and achieve accuracies on link prediction comparable to\nresource-demanding approaches such as Markov Logic Networks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 18:46:54 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 13:48:30 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Soru", "Tommaso", ""], ["Valdestilhas", "Andr\u00e9", ""], ["Marx", "Edgard", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1802.03760", "submitter": "Khaled Ammar", "authors": "Khaled Ammar, Frank McSherry, Semih Salihoglu, Manas Joglekar", "title": "Distributed Evaluation of Subgraph Queries Using Worstcase Optimal\n  LowMemory Dataflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding and monitoring fixed-size subgraphs in a\ncontinually changing large-scale graph. We present the first approach that (i)\nperforms worst-case optimal computation and communication, (ii) maintains a\ntotal memory footprint linear in the number of input edges, and (iii) scales\ndown per-worker computation, communication, and memory requirements linearly as\nthe number of workers increases, even on adversarially skewed inputs.\n  Our approach is based on worst-case optimal join algorithms, recast as a\ndata-parallel dataflow computation. We describe the general algorithm and\nmodifications that make it robust to skewed data, prove theoretical bounds on\nits resource requirements in the massively parallel computing model, and\nimplement and evaluate it on graphs containing as many as 64 billion edges. The\nunderlying algorithm and ideas generalize from finding and monitoring subgraphs\nto the more general problem of computing and maintaining relational equi-joins\nover dynamic relations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 16:08:47 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ammar", "Khaled", ""], ["McSherry", "Frank", ""], ["Salihoglu", "Semih", ""], ["Joglekar", "Manas", ""]]}, {"id": "1802.03855", "submitter": "Feichen Shen PhD", "authors": "Feichen Shen, Yugyung Lee", "title": "MedTQ: Dynamic Topic Discovery and Query Generation for Medical\n  Ontologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biomedical ontology refers to a shared conceptualization for a biomedical\ndomain of interest that has vastly improved data management and data sharing\nthrough the open data movement. The rapid growth and availability of biomedical\ndata make it impractical and computationally expensive to perform manual\nanalysis and query processing with the large scale ontologies. The lack of\nability in analyzing ontologies from such a variety of sources, and supporting\nknowledge discovery for clinical practice and biomedical research should be\novercome with new technologies. In this study, we developed a Medical Topic\ndiscovery and Query generation framework (MedTQ), which was composed by a\nseries of approaches and algorithms. A predicate neighborhood pattern-based\napproach introduced has the ability to compute the similarity of predicates\n(relations) in ontologies. Given a predicate similarity metric, machine\nlearning algorithms have been developed for automatic topic discovery and query\ngeneration. The topic discovery algorithm, called the hierarchical K-Means\nalgorithm was designed by extending an existing supervised algorithm (K-means\nclustering) for the construction of a topic hierarchy. In the hierarchical\nK-Means algorithm, a level-by-level optimization strategy was selected for\nconsistent with the strongly association between elements within a topic.\nAutomatic query generation was facilitated for discovered topic that could be\nguided users for interactive query design and processing. Evaluation was\nconducted to generate topic hierarchy for DrugBank ontology as a case study.\nResults demonstrated that the MedTQ framework can enhance knowledge discovery\nby capturing underlying structures from domain specific data and ontologies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 01:22:10 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Shen", "Feichen", ""], ["Lee", "Yugyung", ""]]}, {"id": "1802.04030", "submitter": "Giacomo Kahn", "authors": "Giacomo Kahn (LIMOS), Alexandre Bazin (LIMOS)", "title": "Introducer Concepts in n-Dimensional Contexts", "comments": "ICCS, Jun 2018, Edinburgh, United Kingdom", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept lattices are well-known conceptual structures that organise\ninteresting patterns-the concepts-extracted from data. In some applications,\nsuch as software engineering or data mining, the size of the lattice can be a\nproblem, as it is often too large to be efficiently computed, and too complex\nto be browsed. For this reason, the Galois Sub-Hierarchy, a restriction of the\nconcept lattice to introducer concepts, has been introduced as a smaller\nalternative. In this paper, we generalise the Galois Sub-Hierarchy to\nn-lattices, conceptual structures obtained from multidimensional data in the\nsame way that concept lattices are obtained from binary relations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 13:29:46 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Kahn", "Giacomo", "", "LIMOS"], ["Bazin", "Alexandre", "", "LIMOS"]]}, {"id": "1802.04032", "submitter": "Giacomo Kahn", "authors": "Giacomo Kahn (LIMOS), Alexandre Bazin (Le2i)", "title": "Average Size of Implicational Bases", "comments": "CLA, Jun 2018, Olomouc, Czech Republic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DB math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicational bases are objects of interest in formal concept analysis and\nits applications. Unfortunately, even the smallest base, the Duquenne-Guigues\nbase, has an exponential size in the worst case. In this paper, we use results\non the average number of minimal transversals in random hypergraphs to show\nthat the base of proper premises is, on average, of quasi-polynomial size.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 13:31:27 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Kahn", "Giacomo", "", "LIMOS"], ["Bazin", "Alexandre", "", "Le2i"]]}, {"id": "1802.04060", "submitter": "Davide Mottin", "authors": "Davide Mottin, Bastian Grasnick, Axel Kroschk, Patrick Siegler,\n  Emmanuel Mueller", "title": "Notable Characteristics Search through Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query answering routinely employs knowledge graphs to assist the user in the\nsearch process. Given a knowledge graph that represents entities and\nrelationships among them, one aims at complementing the search with intuitive\nbut effective mechanisms. In particular, we focus on the comparison of two or\nmore entities and the detection of unexpected, surprising properties, called\nnotable characteristics. Such characteristics provide intuitive explanations of\nthe peculiarities of the selected entities with respect to similar entities. We\npropose a solid probabilistic approach that first retrieves entity nodes\nsimilar to the query nodes provided by the user, and then exploits\ndistributional properties to understand whether a certain attribute is\ninteresting or not. Our preliminary experiments demonstrate the solidity of our\napproach and show that we are able to discover notable characteristics that are\nindeed interesting and relevant for the user.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:24:55 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Mottin", "Davide", ""], ["Grasnick", "Bastian", ""], ["Kroschk", "Axel", ""], ["Siegler", "Patrick", ""], ["Mueller", "Emmanuel", ""]]}, {"id": "1802.04249", "submitter": "Kijung Shin", "authors": "Kijung Shin, Euiwoong Lee, Jinoh Oh, Mohammad Hammoud, Christos\n  Faloutsos", "title": "CoCoS: Fast and Accurate Distributed Triangle Counting in Graph Streams", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph stream, how can we estimate the number of triangles in it using\nmultiple machines with limited storage? Specifically, how should edges be\nprocessed and sampled across the machines for rapid and accurate estimation?\n  The count of triangles (i.e., cliques of size three) has proven useful in\nnumerous applications, including anomaly detection, community detection, and\nlink recommendation. For triangle counting in large and dynamic graphs, recent\nwork has focused largely on streaming algorithms and distributed algorithms but\nlittle on their combinations for \"the best of both worlds\".\n  In this work, we propose CoCoS, a fast and accurate distributed streaming\nalgorithm for estimating the counts of global triangles (i.e., all triangles)\nand local triangles incident to each node. Making one pass over the input\nstream, COCOS carefully processes and stores the edges across multiple machines\nso that the redundant use of computational and storage resources is minimized.\nCompared to baselines, CoCoS is (a) Accurate: giving up to 39X smaller\nestimation error, (b) Fast: up to 10.4X faster, scaling linearly with the size\nof the input stream, and (c) Theoretically sound: yielding unbiased estimates.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:57:57 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 06:28:17 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 22:05:29 GMT"}, {"version": "v4", "created": "Sun, 6 Dec 2020 07:48:15 GMT"}, {"version": "v5", "created": "Sat, 27 Feb 2021 06:23:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Shin", "Kijung", ""], ["Lee", "Euiwoong", ""], ["Oh", "Jinoh", ""], ["Hammoud", "Mohammad", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1802.04613", "submitter": "Luc Segoufin", "authors": "Wojtek Kazana and Luc Segoufin", "title": "First-order queries on classes of structures with bounded expansion", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 1 (February\n  25, 2020) lmcs:6156", "doi": "10.23638/LMCS-16(1:25)2020", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the evaluation of first-order queries over classes of databases\nwith bounded expansion. The notion of bounded expansion is fairly broad and\ngeneralizes bounded degree, bounded treewidth and exclusion of at least one\nminor. It was known that over a class of databases with bounded expansion,\nfirst-order sentences could be evaluated in time linear in the size of the\ndatabase. We give a different proof of this result. Moreover, we show that\nanswers to first-order queries can be enumerated with constant delay after a\nlinear time preprocessing. We also show that counting the number of answers to\na query can be done in time linear in the size of the database.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 13:29:50 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 13:25:42 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 10:00:36 GMT"}, {"version": "v4", "created": "Mon, 24 Feb 2020 09:35:57 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Kazana", "Wojtek", ""], ["Segoufin", "Luc", ""]]}, {"id": "1802.04780", "submitter": "David Dao", "authors": "David Dao, Dan Alistarh, Claudiu Musat, Ce Zhang", "title": "DataBright: Towards a Global Exchange for Decentralized Data Ownership\n  and Trusted Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is safe to assume that, for the foreseeable future, machine learning,\nespecially deep learning will remain both data- and computation-hungry. In this\npaper, we ask: Can we build a global exchange where everyone can contribute\ncomputation and data to train the next generation of machine learning\napplications?\n  We present an early, but running prototype of DataBright, a system that turns\nthe creation of training examples and the sharing of computation into an\ninvestment mechanism. Unlike most crowdsourcing platforms, where the\ncontributor gets paid when they submit their data, DataBright pays dividends\nwhenever a contributor's data or hardware is used by someone to train a machine\nlearning model. The contributor becomes a shareholder in the dataset they\ncreated. To enable the measurement of usage, a computation platform that\ncontributors can trust is also necessary. DataBright thus merges both a data\nmarket and a trusted computation market.\n  We illustrate that trusted computation can enable the creation of an AI\nmarket, where each data point has an exact value that should be paid to its\ncreator. DataBright allows data creators to retain ownership of their\ncontribution and attaches to it a measurable value. The value of the data is\ngiven by its utility in subsequent distributed computation done on the\nDataBright computation market. The computation market allocates tasks and\nsubsequent payments to pooled hardware. This leads to the creation of a\ndecentralized AI cloud. Our experiments show that trusted hardware such as\nIntel SGX can be added to the usual ML pipeline with no additional costs. We\nuse this setting to orchestrate distributed computation that enables the\ncreation of a computation market. DataBright is available for download at\nhttps://github.com/ds3lab/databright.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 18:20:07 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Dao", "David", ""], ["Alistarh", "Dan", ""], ["Musat", "Claudiu", ""], ["Zhang", "Ce", ""]]}, {"id": "1802.04949", "submitter": "Sheng Wang", "authors": "Sheng Wang, Tien Tuan Anh Dinh, Qian Lin, Zhongle Xie, Meihui Zhang,\n  Qingchao Cai, Gang Chen, Wanzeng Fu, Beng Chin Ooi, Pingcheng Ruan", "title": "ForkBase: An Efficient Storage Engine for Blockchain and Forkable\n  Applications", "comments": "15 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing data storage systems offer a wide range of functionalities to\naccommodate an equally diverse range of applications. However, new classes of\napplications have emerged, e.g., blockchain and collaborative analytics,\nfeaturing data versioning, fork semantics, tamper-evidence or any combination\nthereof. They present new opportunities for storage systems to efficiently\nsupport such applications by embedding the above requirements into the storage.\n  In this paper, we present ForkBase, a storage engine specifically designed to\nprovide efficient support for blockchain and forkable applications. By\nintegrating the core application properties into the storage, ForkBase not only\ndelivers high performance but also reduces development effort. Data in ForkBase\nis multi-versioned, and each version uniquely identifies the data content and\nits history. Two variants of fork semantics are supported in ForkBase to\nfacilitate any collaboration workflows. A novel index structure is introduced\nto efficiently identify and eliminate duplicate content across data objects.\nConsequently, ForkBase is not only efficient in performance, but also in space\nrequirement. We demonstrate the performance of ForkBase using three\napplications: a blockchain platform, a wiki engine and a collaborative\nanalytics application. We conduct extensive experimental evaluation of these\napplications against respective state-of-the-art system. The results show that\nForkBase achieves superior performance while significantly lowering the\ndevelopment cost.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 04:07:34 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Wang", "Sheng", ""], ["Dinh", "Tien Tuan Anh", ""], ["Lin", "Qian", ""], ["Xie", "Zhongle", ""], ["Zhang", "Meihui", ""], ["Cai", "Qingchao", ""], ["Chen", "Gang", ""], ["Fu", "Wanzeng", ""], ["Ooi", "Beng Chin", ""], ["Ruan", "Pingcheng", ""]]}, {"id": "1802.05305", "submitter": "Xin Yang", "authors": "Xin Yang and Ju Fan", "title": "Influential User Subscription on Time-Decaying Social Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence maximization which asks for $k$-size seed set from a social network\nsuch that maximizing the influence over all other users (called influence\nspread) has widely attracted attention due to its significant applications in\nviral marketing and rumor control. In real world scenarios, people are\ninterested in the most influential users in particular topics, and want to\nsubscribe the topics-of-interests over social networks. In this paper, we\nformulate the problem of influential users subscription on time-decaying social\nstream, which asks for maintaining the $k$-size influential users sets for each\ntopic-aware subscription queries. We first analyze the widely adopted sliding\nwindow model and propose a newly time-decaying influence model to overcome the\nshortages when calculating the influence over social stream. Developed from\nsieve based streaming algorithm, we propose an efficient algorithm to support\nthe calculation of time-decaying influence over dynamically updating social\nnetworks. Using information among subscriptions, we then construct the Prefix\nTree Structure to allow us minimizing the times of calculating influence of\neach update and easily maintained. Pruning techniques are also applied to the\nPrefix Tree to optimize the performance of social stream update. Our approach\nensures a $\\frac{1}{2}-\\epsilon$ approximation ratio. Experimental results show\nthat our approach significantly outperforms the baseline approaches in\nefficiency and result quality.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 20:08:35 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Yang", "Xin", ""], ["Fan", "Ju", ""]]}, {"id": "1802.05898", "submitter": "Matteo Cossu", "authors": "Matteo Cossu, Michael F\\\"arber and Georg Lausen", "title": "PRoST: Distributed Execution of SPARQL Queries Using Mixed Partitioning\n  Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapidly growing size of RDF graphs in recent years necessitates\ndistributed storage and parallel processing strategies. To obtain efficient\nquery processing using computer clusters a wide variety of different approaches\nhave been proposed. Related to the approach presented in the current paper are\nsystems built on top of Hadoop HDFS, for example using Apache Accumulo or using\nApache Spark. We present a new RDF store called PRoST (Partitioned RDF on Spark\nTables) based on Apache Spark. PRoST introduces an innovative strategy that\ncombines the Vertical Partitioning approach with the Property Table, two\npreexisting models for storing RDF datasets. We demonstrate that our proposal\noutperforms state-of-the-art systems w.r.t. the runtime for a wide range of\nquery types and without any extensive precomputing phase.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 11:25:15 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Cossu", "Matteo", ""], ["F\u00e4rber", "Michael", ""], ["Lausen", "Georg", ""]]}, {"id": "1802.06060", "submitter": "Xiaofeng Yang", "authors": "Xiaofeng Yang, Deepak Ajwani, Wolfgang Gatterbauer, Patrick K.\n  Nicholson, Mirek Riedewald, Alessandra Sala", "title": "Any-k: Anytime Top-k Tree Pattern Retrieval in Labeled Graphs", "comments": "To appear in WWW 2018", "journal-ref": null, "doi": "10.1145/3178876.3186115", "report-no": null, "categories": "cs.SI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in areas as diverse as recommendation systems, social network\nanalysis, semantic search, and distributed root cause analysis can be modeled\nas pattern search on labeled graphs (also called \"heterogeneous information\nnetworks\" or HINs). Given a large graph and a query pattern with node and edge\nlabel constraints, a fundamental challenge is to nd the top-k matches ac-\ncording to a ranking function over edge and node weights. For users, it is di\ncult to select value k . We therefore propose the novel notion of an any-k\nranking algorithm: for a given time budget, re- turn as many of the top-ranked\nresults as possible. Then, given additional time, produce the next lower-ranked\nresults quickly as well. It can be stopped anytime, but may have to continues\nuntil all results are returned. This paper focuses on acyclic patterns over\narbitrary labeled graphs. We are interested in practical algorithms that\neffectively exploit (1) properties of heterogeneous networks, in particular\nselective constraints on labels, and (2) that the users often explore only a\nfraction of the top-ranked results. Our solution, KARPET, carefully integrates\naggressive pruning that leverages the acyclic nature of the query, and\nincremental guided search. It enables us to prove strong non-trivial time and\nspace guarantees, which is generally considered very hard for this type of\ngraph search problem. Through experimental studies we show that KARPET achieves\nrunning times in the order of milliseconds for tree patterns on large networks\nwith millions of nodes and edges.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 18:21:54 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 19:21:00 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 21:29:10 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Yang", "Xiaofeng", ""], ["Ajwani", "Deepak", ""], ["Gatterbauer", "Wolfgang", ""], ["Nicholson", "Patrick K.", ""], ["Riedewald", "Mirek", ""], ["Sala", "Alessandra", ""]]}, {"id": "1802.06183", "submitter": "Ikechukwu Maduako", "authors": "Maduako N. Ikechukwu, Francis I. Okeke", "title": "Towards Realisation of Heterogeneous Earth-Observation Sensor Database\n  Framework for the Sensor Observation Service based on PostGIS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental monitoring and management systems in most cases deal with\nmodels and spatial analytics that involve the integration of in-situ and remote\nGeosensor observations. In-situ sensor observations and those gathered by\nremote sensors are usually provided by different databases and services in\nreal-time dynamic services such as the Geo-Web Services. Thus, data have to be\npulled from different databases and transferred over the network before they\nare fused and processed on the service middleware. This process is very massive\nand unnecessary communication-work load on the service middleware. Massive work\nload in large raster downloads from flat-file raster data sources each time a\nrequest is made and huge integration and geo-processing work load on the\nservice middleware which could actually be better leveraged at the database\nThis paper therefore proposes the realization of heterogeneous sensor database\nframework based on PostGIS for integration, geo-processing and spatial analysis\nof remote and in-situ sensor observations at the database level. Also discussed\nin this paper is how the framework can be integrated in the Sensor Observation\nService (SOS) to reduce communication and massive workload on the Geospatial\nWeb Services and as well make query request from the user end a lot more\nflexible. Keywords: Earth-Observation, Heterogeneous Earth-Observation Sensor\nDatabase, PostGIS , Sensor Observation Service.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 03:52:21 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Ikechukwu", "Maduako N.", ""], ["Okeke", "Francis I.", ""]]}, {"id": "1802.07284", "submitter": "Yanhong Annie Liu", "authors": "Yanhong A. Liu", "title": "Logic Programming Applications: What Are the Abstractions and\n  Implementations?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an overview of applications of logic programming,\nclassifying them based on the abstractions and implementations of logic\nlanguages that support the applications. The three key abstractions are join,\nrecursion, and constraint. Their essential implementations are for-loops, fixed\npoints, and backtracking, respectively. The corresponding kinds of applications\nare database queries, inductive analysis, and combinatorial search,\nrespectively. We also discuss language extensions and programming paradigms,\nsummarize example application problems by application areas, and touch on\nexample systems that support variants of the abstractions with different\nimplementations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 19:04:14 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Liu", "Yanhong A.", ""]]}, {"id": "1802.07693", "submitter": "Souvik Bhattacherjee", "authors": "Souvik Bhattacherjee and Amol Deshpande", "title": "RStore: A Distributed Multi-version Document Store", "comments": "A shorter version of the paper is to appear in ICDE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of compactly storing a large number of versions\n(snapshots) of a collection of keyed documents or records in a distributed\nenvironment, while efficiently answering a variety of retrieval queries over\nthose, including retrieving full or partial versions, and evolution histories\nfor specific keys. We motivate the increasing need for such a system in a\nvariety of application domains, carefully explore the design space for building\nsuch a system and the various storage-computation-retrieval trade-offs, and\ndiscuss how different storage layouts influence those trade-offs. We propose a\nnovel system architecture that satisfies the key desiderata for such a system,\nand offers simple tuning knobs that allow adapting to a specific data and query\nworkload. Our system is intended to act as a layer on top of a distributed\nkey-value store that houses the raw data as well as any indexes. We design\nnovel off-line storage layout algorithms for efficiently partitioning the data\nto minimize the storage costs while keeping the retrieval costs low. We also\npresent an online algorithm to handle new versions being added to system. Using\nextensive experiments on large datasets, we demonstrate that our system\noperates at the scale required in most practical scenarios and often\noutperforms standard baselines, including a delta-based storage engine, by\norders-of-magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 17:50:44 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 01:01:00 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Bhattacherjee", "Souvik", ""], ["Deshpande", "Amol", ""]]}, {"id": "1802.08014", "submitter": "Bin Yang", "authors": "Huiping Liu, Cheqing Jin, Bin Yang, Aoying Zhou", "title": "Finding Top-k Optimal Sequenced Routes -- Full Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by many practical applications in logistics and\nmobility-as-a-service, we study the top-k optimal sequenced routes (KOSR)\nquerying on large, general graphs where the edge weights may not satisfy the\ntriangle inequality, e.g., road network graphs with travel times as edge\nweights. The KOSR querying strives to find the top-k optimal routes (i.e., with\nthe top-k minimal total costs) from a given source to a given destination,\nwhich must visit a number of vertices with specific vertex categories (e.g.,\ngas stations, restaurants, and shopping malls) in a particular order (e.g.,\nvisiting gas stations before restaurants and then shopping malls).\n  To efficiently find the top-k optimal sequenced routes, we propose two\nalgorithms PruningKOSR and StarKOSR. In PruningKOSR, we define a dominance\nrelationship between two partially-explored routes. The partially-explored\nroutes that can be dominated by other partially-explored routes are postponed\nbeing extended, which leads to a smaller searching space and thus improves\nefficiency. In StarKOSR, we further improve the efficiency by extending routes\nin an A* manner. With the help of a judiciously designed heuristic estimation\nthat works for general graphs, the cost of partially explored routes to the\ndestination can be estimated such that the qualified complete routes can be\nfound early. In addition, we demonstrate the high extensibility of the proposed\nalgorithms by incorporating Hop Labeling, an effective label indexing technique\nfor shortest path queries, to further improve efficiency. Extensive experiments\non multiple real-world graphs demonstrate that the proposed methods\nsignificantly outperform the baseline method. Furthermore, when k=1, StarKOSR\nalso outperforms the state-of-the-art method for the optimal sequenced route\nqueries.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 12:46:07 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Liu", "Huiping", ""], ["Jin", "Cheqing", ""], ["Yang", "Bin", ""], ["Zhou", "Aoying", ""]]}, {"id": "1802.08052", "submitter": "Nazim Faour", "authors": "Nazim Faour", "title": "Data Consistency Simulation Tool for NoSQL Database Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Various data consistency levels have an important part in the integrity of\ndata and also affect performance especially the data that is replicated many\ntimes across or over the cluster. Based on BASE and the theorem of CAP\ntradeoffs, most systems of NoSQL have more relaxed consistency guarantees than\nanother kind of databases which implement ACID. Most systems of NoSQL gave\ndifferent methods to adjust a required level of consistency to ensure the\nminimal numbering of the replicas accepted in each operation. Simulations are\nalways depending on a simplified model and ignore many details and facts about\nthe real system. Therefore, a simulation can only work as an estimation or an\nexplanation vehicle for observed behavior. So to create simulation tool, I have\nto characterize a model, identify influence factors and simply implement that\ndepending on a (modeled) workload. In this paper, I have a model of simulation\nto measure the consistency of the data and to detect the data consistency\nviolations in simulated network partition settings. So workloads are needed\nwith the set of users who make requests and then put the results for analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 14:24:00 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Faour", "Nazim", ""]]}, {"id": "1802.08496", "submitter": "Jeyhun Karimov", "authors": "Jeyhun Karimov, Tilmann Rabl, Asterios Katsifodimos, Roman Samarev,\n  Henri Heiskanen, Volker Markl", "title": "Benchmarking Distributed Stream Data Processing Systems", "comments": "Published at ICDE 2018", "journal-ref": "2018 IEEE 34th International Conference on Data Engineering\n  (ICDE), pp. 1507-1518, IEEE, 2018", "doi": "10.1109/ICDE.2018.00169", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for scalable and efficient stream analysis has led to the\ndevelopment of many open-source streaming data processing systems (SDPSs) with\nhighly diverging capabilities and performance characteristics. While first\ninitiatives try to compare the systems for simple workloads, there is a clear\ngap of detailed analyses of the systems' performance characteristics. In this\npaper, we propose a framework for benchmarking distributed stream processing\nengines. We use our suite to evaluate the performance of three widely used\nSDPSs in detail, namely Apache Storm, Apache Spark, and Apache Flink. Our\nevaluation focuses in particular on measuring the throughput and latency of\nwindowed operations, which are the basic type of operations in stream\nanalytics. For this benchmark, we design workloads based on real-life,\nindustrial use-cases inspired by the online gaming industry. The contribution\nof our work is threefold. First, we give a definition of latency and throughput\nfor stateful operators. Second, we carefully separate the system under test and\ndriver, in order to correctly represent the open world model of typical stream\nprocessing deployments and can, therefore, measure system performance under\nrealistic conditions. Third, we build the first benchmarking framework to\ndefine and test the sustainable performance of streaming systems.\n  Our detailed evaluation highlights the individual characteristics and\nuse-cases of each system.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 12:09:08 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 18:11:29 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Karimov", "Jeyhun", ""], ["Rabl", "Tilmann", ""], ["Katsifodimos", "Asterios", ""], ["Samarev", "Roman", ""], ["Heiskanen", "Henri", ""], ["Markl", "Volker", ""]]}, {"id": "1802.08586", "submitter": "Umberto Grandi", "authors": "Francesco Belardinelli and Umberto Grandi", "title": "Database Aggregation", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge can be represented compactly in a multitude ways, from a set of\npropositional formulas, to a Kripke model, to a database. In this paper we\nstudy the aggregation of information coming from multiple sources, each source\nsubmitting a database modelled as a first-order relational structure. In the\npresence of an integrity constraint, we identify classes of aggregators that\nrespect it in the aggregated database, provided all individual databases\nsatisfy it. We also characterise languages for first-order queries on which the\nanswer to queries on the aggregated database coincides with the aggregation of\nthe answers to the query obtained on each individual database. This\ncontribution is meant to be a first step on the application of techniques from\nrational choice theory to knowledge representation in databases.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:15:53 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Belardinelli", "Francesco", ""], ["Grandi", "Umberto", ""]]}, {"id": "1802.08800", "submitter": "Florin Rusu", "authors": "Yujing Ma, Florin Rusu, Martin Torres", "title": "Stochastic Gradient Descent on Highly-Parallel Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increased interest in building data analytics frameworks with\nadvanced algebraic capabilities both in industry and academia. Many of these\nframeworks, e.g., TensorFlow and BIDMach, implement their compute-intensive\nprimitives in two flavors---as multi-thread routines for multi-core CPUs and as\nhighly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is\nthe most popular optimization method for model training implemented extensively\non modern data analytics platforms. While the data-intensive properties of SGD\nare well-known, there is an intense debate on which of the many SGD variants is\nbetter in practice. In this paper, we perform a comprehensive study of parallel\nSGD for training generalized linear models. We consider the impact of three\nfactors -- computing architecture (multi-core CPU or GPU), synchronous or\nasynchronous model updates, and data sparsity -- on three measures---hardware\nefficiency, statistical efficiency, and time to convergence. In the process, we\ndesign an optimized asynchronous SGD algorithm for GPU that leverages warp\nshuffling and cache coalescing for data and model access. We draw several\ninteresting findings from our extensive experiments with logistic regression\n(LR) and support vector machines (SVM) on five real datasets. For synchronous\nSGD, GPU always outperforms parallel CPU---they both outperform a sequential\nCPU solution by more than 400X. For asynchronous SGD, parallel CPU is the\nsafest choice while GPU with data replication is better in certain situations.\nThe choice between synchronous GPU and asynchronous CPU depends on the task and\nthe characteristics of the data. As a reference, our best implementation\noutperforms TensorFlow and BIDMach consistently. We hope that our insights\nprovide a useful guide for applying parallel SGD to generalized linear models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 05:27:04 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Ma", "Yujing", ""], ["Rusu", "Florin", ""], ["Torres", "Martin", ""]]}, {"id": "1802.09180", "submitter": "Tomer Kaftan", "authors": "Tomer Kaftan, Magdalena Balazinska, Alvin Cheung, Johannes Gehrke", "title": "Cuttlefish: A Lightweight Primitive for Adaptive Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data processing applications execute increasingly sophisticated\nanalysis that requires operations beyond traditional relational algebra. As a\nresult, operators in query plans grow in diversity and complexity. Designing\nquery optimizer rules and cost models to choose physical operators for all of\nthese novel logical operators is impractical. To address this challenge, we\ndevelop Cuttlefish, a new primitive for adaptively processing online query\nplans that explores candidate physical operator instances during query\nexecution and exploits the fastest ones using multi-armed bandit reinforcement\nlearning techniques. We prototype Cuttlefish in Apache Spark and adaptively\nchoose operators for image convolution, regular expression matching, and\nrelational joins. Our experiments show Cuttlefish-based adaptive convolution\nand regular expression operators can reach 72-99% of the throughput of an\nall-knowing oracle that always selects the optimal algorithm, even when\nindividual physical operators are up to 105x slower than the optimal.\nAdditionally, Cuttlefish achieves join throughput improvements of up to 7.5x\ncompared with Spark SQL's query optimizer.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 06:50:43 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Kaftan", "Tomer", ""], ["Balazinska", "Magdalena", ""], ["Cheung", "Alvin", ""], ["Gehrke", "Johannes", ""]]}, {"id": "1802.09488", "submitter": "Andreas Kipf", "authors": "Andreas Kipf, Harald Lang, Varun Pandey, Raul Alexandru Persa, Peter\n  Boncz, Thomas Neumann, Alfons Kemper", "title": "Adaptive Geospatial Joins for Modern Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial joins are a core building block of connected mobility\napplications. An especially challenging problem are joins between streaming\npoints and static polygons. Since points are not known beforehand, they cannot\nbe indexed. Nevertheless, points need to be mapped to polygons with low\nlatencies to enable real-time feedback.\n  We present an adaptive geospatial join that uses true hit filtering to avoid\nexpensive geometric computations in most cases. Our technique uses a\nquadtree-based hierarchical grid to approximate polygons and stores these\napproximations in a specialized radix tree. We emphasize on an approximate\nversion of our algorithm that guarantees a user-defined precision. The exact\nversion of our algorithm can adapt to the expected point distribution by\nrefining the index. We optimized our implementation for modern hardware\narchitectures with wide SIMD vector processing units, including Intel's brand\nnew Knights Landing. Overall, our approach can perform up to two orders of\nmagnitude faster than existing techniques.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 18:11:36 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Kipf", "Andreas", ""], ["Lang", "Harald", ""], ["Pandey", "Varun", ""], ["Persa", "Raul Alexandru", ""], ["Boncz", "Peter", ""], ["Neumann", "Thomas", ""], ["Kemper", "Alfons", ""]]}, {"id": "1802.09594", "submitter": "Ali Karami", "authors": "Nasrin Mazaheri Soudani, Ali Karami", "title": "All nearest neighbor calculation based on Delaunay graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we have two data sets and want to find the nearest neighbour of each\npoint in the first dataset among points in the second one, we need the all\nnearest neighbour operator. This is an operator in spatial databases that has\nmany application in different fields such as GIS and VLSI circuit design.\nExisting algorithms for calculating this operator assume that there is no pre\ncomputation on these data sets. These algorithms has o(n*m*d) time complexity\nwhere n and m are the number of points in two data sets and d is the dimension\nof data points. With assumption of some pre computation on data sets algorithms\nwith lower time complexity can be obtained. One of the most common pre\ncomputation on spatial data is Delaunay graphs. In the Delaunay graph of a data\nset each point is linked to its nearest neighbours. In this paper, we introduce\nan algorithm for computing the all nearest neighbour operator on spatial data\nsets based on their Delaunay graphs. The performance of this algorithm is\ncompared with one of the best existing algorithms for computing ANN operator in\nterms of CPU time and the number of IOs. The experimental results show that\nthis algorithm has better performance than the other.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:32:05 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Soudani", "Nasrin Mazaheri", ""], ["Karami", "Ali", ""]]}, {"id": "1802.09883", "submitter": "Ingo M\\\"uller", "authors": "Ingo M\\\"uller (1), Andrea Arteaga (2), Torsten Hoefler (1), Gustavo\n  Alonso (1) ((1) Systems Group, Dept. of Computer Science, ETH Zurich, (2)\n  Federal Institute of Meteorology and Climatology MeteoSwiss)", "title": "Reproducible Floating-Point Aggregation in RDBMSs", "comments": "This document is the extended version of an article in the\n  Proceedings of the 34th IEEE International Conference on Data Engineering\n  (ICDE) 2018", "journal-ref": "34th IEEE International Conference on Data Engineering (ICDE)\n  2018, Paris, 2018, pp. 1049-1060", "doi": "10.1109/ICDE.2018.00098", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Industry-grade database systems are expected to produce the same result if\nthe same query is repeatedly run on the same input. However, the numerous\nsources of non-determinism in modern systems make reproducible results\ndifficult to achieve. This is particularly true if floating-point numbers are\ninvolved, where the order of the operations affects the final result.\n  As part of a larger effort to extend database engines with data\nrepresentations more suitable for machine learning and scientific applications,\nin this paper we explore the problem of making relational GroupBy over\nfloating-point formats bit-reproducible, i.e., ensuring any execution of the\noperator produces the same result up to every single bit. To that aim, we first\npropose a numeric data type that can be used as drop-in replacement for other\nnumber formats and is---unlike standard floating-point formats---associative.\nWe use this data type to make state-of-the-art GroupBy operators reproducible,\nbut this approach incurs a slowdown between 4x and 12x compared to the same\noperator using conventional database number formats. We thus explore how to\nmodify existing GroupBy algorithms to make them bit-reproducible and efficient.\nBy using vectorized summation on batches and carefully balancing batch size,\ncache footprint, and preprocessing costs, we are able to reduce the slowdown\ndue to reproducibility to a factor between 1.9x and 2.4x of aggregation in\nisolation and to a mere 2.7% of end-to-end query performance even on\naggregation-intensive queries in MonetDB. We thereby provide a solid basis for\nsupporting more reproducible operations directly in relational engines.\n  This document is an extended version of an article currently in print for the\nproceedings of ICDE'18 with the same title and by the same authors. The main\nadditions are more implementation details and experiments.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 13:51:05 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["M\u00fcller", "Ingo", ""], ["Arteaga", "Andrea", ""], ["Hoefler", "Torsten", ""], ["Alonso", "Gustavo", ""]]}, {"id": "1802.09984", "submitter": "Victor Marsault", "authors": "Nadime Francis, Alastair Green, Paolo Guagliardo, Leonid Libkin,\n  Tobias Lindaaker, Victor Marsault, Stefan Plantikow, Mats Rydberg, Martin\n  Schuster, Petra Selmer, and Andr\\'es Taylor", "title": "Formal Semantics of the Language Cypher", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cypher is a query language for property graphs. It was originally designed\nand implemented as part of the Neo4j graph database, and it is currently used\nin a growing number of commercial systems, industrial applications and research\nprojects. In this work, we provide denotational semantics of the core fragment\nof the read-only part of Cypher, which features in particular pattern matching,\nfiltering, and most relational operations on tables.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 16:01:36 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 18:27:52 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Francis", "Nadime", ""], ["Green", "Alastair", ""], ["Guagliardo", "Paolo", ""], ["Libkin", "Leonid", ""], ["Lindaaker", "Tobias", ""], ["Marsault", "Victor", ""], ["Plantikow", "Stefan", ""], ["Rydberg", "Mats", ""], ["Schuster", "Martin", ""], ["Selmer", "Petra", ""], ["Taylor", "Andr\u00e9s", ""]]}, {"id": "1802.10233", "submitter": "Daniel Lemire", "authors": "Edmon Begoli, Jes\\'us Camacho Rodr\\'iguez, Julian Hyde, Michael J.\n  Mior, Daniel Lemire", "title": "Apache Calcite: A Foundational Framework for Optimized Query Processing\n  Over Heterogeneous Data Sources", "comments": "SIGMOD'18", "journal-ref": null, "doi": "10.1145/3183713.3190662", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Apache Calcite is a foundational software framework that provides query\nprocessing, optimization, and query language support to many popular\nopen-source data processing systems such as Apache Hive, Apache Storm, Apache\nFlink, Druid, and MapD. Calcite's architecture consists of a modular and\nextensible query optimizer with hundreds of built-in optimization rules, a\nquery processor capable of processing a variety of query languages, an adapter\narchitecture designed for extensibility, and support for heterogeneous data\nmodels and stores (relational, semi-structured, streaming, and geospatial).\nThis flexible, embeddable, and extensible architecture is what makes Calcite an\nattractive choice for adoption in big-data frameworks. It is an active project\nthat continues to introduce support for the new types of data sources, query\nlanguages, and approaches to query processing and optimization.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 02:10:36 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Begoli", "Edmon", ""], ["Rodr\u00edguez", "Jes\u00fas Camacho", ""], ["Hyde", "Julian", ""], ["Mior", "Michael J.", ""], ["Lemire", "Daniel", ""]]}, {"id": "1802.10303", "submitter": "Abolfazl Asudeh", "authors": "Abolfazl Asudeh and Azade Nazi and Nan Zhang and Gautam Das and H. V.\n  Jagadish", "title": "RRR: Rank-Regret Representative", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the best items in a dataset is a common task in data exploration.\nHowever, the concept of \"best\" lies in the eyes of the beholder: different\nusers may consider different attributes more important, and hence arrive at\ndifferent rankings. Nevertheless, one can remove \"dominated\" items and create a\n\"representative\" subset of the data set, comprising the \"best items\" in it. A\nPareto-optimal representative is guaranteed to contain the best item of each\npossible ranking, but it can be almost as big as the full data. Representative\ncan be found if we relax the requirement to include the best item for every\npossible user, and instead just limit the users' \"regret\". Existing work\ndefines regret as the loss in score by limiting consideration to the\nrepresentative instead of the full data set, for any chosen ranking function.\n  However, the score is often not a meaningful number and users may not\nunderstand its absolute value. Sometimes small ranges in score can include\nlarge fractions of the data set. In contrast, users do understand the notion of\nrank ordering. Therefore, alternatively, we consider the position of the items\nin the ranked list for defining the regret and propose the {\\em rank-regret\nrepresentative} as the minimal subset of the data containing at least one of\nthe top-$k$ of any possible ranking function. This problem is NP-complete. We\nuse the geometric interpretation of items to bound their ranks on ranges of\nfunctions and to utilize combinatorial geometry notions for developing\neffective and efficient approximation algorithms for the problem. Experiments\non real datasets demonstrate that we can efficiently find small subsets with\nsmall rank-regrets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 08:24:02 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 17:14:31 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Asudeh", "Abolfazl", ""], ["Nazi", "Azade", ""], ["Zhang", "Nan", ""], ["Das", "Gautam", ""], ["Jagadish", "H. V.", ""]]}, {"id": "1802.10543", "submitter": "Elias Stavropoulos", "authors": "Vasileios Kagklis, Elias C. Stavropoulos, Vassilios S. Verykios", "title": "A Frequent Itemset Hiding Toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in data collection and data storage technologies have given way to\nthe establishment of transactional databases among companies and organizations,\nas they allow enormous amounts of data to be stored efficiently. Useful\nknowledge can be mined from these data, which can be used in several ways\ndepending on the nature of the data. Quite often companies and organizations\nare willing to share data for the sake of mutual benefit. However, the sharing\nof such data comes with risks, as problems with privacy may arise. Sensitive\ndata, along with sensitive knowledge inferred from this data, must be protected\nfrom unintentional exposure to unauthorized parties. One form of the inferred\nknowledge is frequent patterns mined in the form of frequent itemsets from\ntransactional databases. The problem of protecting such patterns is known as\nthe frequent itemset hiding problem.\n  In this paper we present a toolbox, which provides several implementations of\nfrequent itemset hiding algorithms. Firstly, we summarize the most important\naspects of each algorithm. We then introduce the architecture of the toolbox\nand its novel features. Finally, we provide experimental results on real world\ndatasets, demonstrating the efficiency of the toolbox and the convenience it\noffers in comparing different algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:23:33 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Kagklis", "Vasileios", ""], ["Stavropoulos", "Elias C.", ""], ["Verykios", "Vassilios S.", ""]]}]