[{"id": "1206.0021", "submitter": "Casey Bennett", "authors": "Casey C. Bennett", "title": "Clinical Productivity System - A Decision Support Model", "comments": "Keywords: Decision support systems, clinical; Efficiency,\n  organizational; Clinical productivity; Healthcare; Electronic Health Records", "journal-ref": "International Journal of Productivity and Performance Management.\n  60(3): 311-319 (2010)", "doi": "10.1108/17410401111112014", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: This goal of this study was to evaluate the effects of a data-driven\nclinical productivity system that leverages Electronic Health Record (EHR) data\nto provide productivity decision support functionality in a real-world clinical\nsetting. The system was implemented for a large behavioral health care provider\nseeing over 75,000 distinct clients a year. Design/methodology/approach: The\nkey metric in this system is a \"VPU\", which simultaneously optimizes multiple\naspects of clinical care. The resulting mathematical value of clinical\nproductivity was hypothesized to tightly link the organization's performance to\nits expectations and, through transparency and decision support tools at the\nclinician level, affect significant changes in productivity, quality, and\nconsistency relative to traditional models of clinical productivity. Findings:\nIn only 3 months, every single variable integrated into the VPU system showed\nsignificant improvement, including a 30% rise in revenue, 10% rise in clinical\npercentage, a 25% rise in treatment plan completion, a 20% rise in case rate\neligibility, along with similar improvements in compliance/audit issues,\noutcomes collection, access, etc. Practical implications: A data-driven\nclinical productivity system employing decision support functionality is\neffective because of the impact on clinician behavior relative to traditional\nclinical productivity systems. Critically, the model is also extensible to\nintegration with outcomes-based productivity. Originality/Value: EHR's are only\na first step - the problem is turning that data into useful information.\nTechnology can leverage the data in order to produce actionable information\nthat can inform clinical practice and decision-making. Without additional\ntechnology, EHR's are essentially just copies of paper-based records stored in\nelectronic form.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2012 20:15:35 GMT"}], "update_date": "2012-06-04", "authors_parsed": [["Bennett", "Casey C.", ""]]}, {"id": "1206.0051", "submitter": "Florin Rusu", "authors": "Chengjie Qin, Florin Rusu", "title": "PF-OLA: A High-Performance Framework for Parallel On-Line Aggregation", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online aggregation provides estimates to the final result of a computation\nduring the actual processing. The user can stop the computation as soon as the\nestimate is accurate enough, typically early in the execution. This allows for\nthe interactive data exploration of the largest datasets. In this paper we\nintroduce the first framework for parallel online aggregation in which the\nestimation virtually does not incur any overhead on top of the actual\nexecution. We define a generic interface to express any estimation model that\nabstracts completely the execution details. We design a novel estimator\nspecifically targeted at parallel online aggregation. When executed by the\nframework over a massive $8\\text{TB}$ TPC-H instance, the estimator provides\naccurate confidence bounds early in the execution even when the cardinality of\nthe final result is seven orders of magnitude smaller than the dataset size and\nwithout incurring overhead.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2012 23:38:36 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2013 07:10:04 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Qin", "Chengjie", ""], ["Rusu", "Florin", ""]]}, {"id": "1206.0104", "submitter": "Dian Pratiwi", "authors": "Dian Pratiwi", "title": "The Use of Self Organizing Map Method and Feature Selection in Image\n  Database Classification System", "comments": "5 pages, 5 figures, 2 tables. citation in IJCSI volume 9 issue 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique in classifying the images into a number of\nclasses or clusters desired by means of Self Organizing Map (SOM) Artificial\nNeural Network method. A number of 250 color images to be classified as\npreviously done some processing, such as RGB to grayscale color conversion,\ncolor histogram, feature vector selection, and then classifying by the SOM\nFeature vector selection in this paper will use two methods, namely by PCA\n(Principal Component Analysis) and LSA (Latent Semantic Analysis) in which each\nof these methods would have taken the characteristic vector of 50, 100, and 150\nfrom 256 initial feature vector into the process of color histogram. Then the\nselection will be processed into the SOM network to be classified into five\nclasses using a learning rate of 0.5 and calculated accuracy. Classification of\nsome of the test results showed that the highest percentage of accuracy\nobtained when using PCA and the selection of 100 feature vector that is equal\nto 88%, compared to when using LSA selection that only 74%. Thus it can be\nconcluded that the method fits the PCA feature selection methods are applied in\nconjunction with SOM and has an accuracy rate better than the LSA feature\nselection methods. Keywords: Color Histogram, Feature Selection, LSA, PCA, SOM.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2012 07:16:18 GMT"}], "update_date": "2012-06-04", "authors_parsed": [["Pratiwi", "Dian", ""]]}, {"id": "1206.0217", "submitter": "Mohamed El-Zawawy Dr.", "authors": "Mohamed A. El-Zawawy", "title": "Efficient techniques for mining spatial databases", "comments": "112 pages; M.Sc. thesis, Department of Mathematics, Faculty of\n  Science, Cairo University, 2002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the major tasks in data mining. In the last few years,\nClustering of spatial data has received a lot of research attention. Spatial\ndatabases are components of many advanced information systems like geographic\ninformation systems VLSI design systems. In this thesis, we introduce several\nefficient algorithms for clustering spatial data. First, we present a\ngrid-based clustering algorithm that has several advantages and comparable\nperformance to the well known efficient clustering algorithm. The algorithm has\nseveral advantages. The algorithm does not require many input parameters. It\nrequires only three parameters, the number of the points in the data space, the\nnumber of the cells in the grid and a percentage. The number of the cells in\nthe grid reflects the accuracy that should be achieved by the algorithm. The\nalgorithm is capable of discovering clusters of arbitrary shapes. The\ncomputational complexity of the algorithm is comparable to the complexity of\nthe most efficient clustering algorithm. The algorithm has been implemented and\ntested against different ranges of database sizes. The performance results show\nthat the running time of the algorithm is superior to the most well known\nalgorithms (CLARANS [23]). The results show also that the performance of the\nalgorithm do not degrade as the number of the data points increases.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2012 15:06:58 GMT"}], "update_date": "2012-06-04", "authors_parsed": [["El-Zawawy", "Mohamed A.", ""]]}, {"id": "1206.1032", "submitter": "Zarrouk Manel", "authors": "Manel Zarrouk, Med Salah Gouider", "title": "Frequent Patterns mining in time-sensitive Data Stream", "comments": "8pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent itemsets through static Databases has been extensively\nstudied and used and is always considered a highly challenging task. For this\nreason it is interesting to extend it to data streams field. In the streaming\ncase, the frequent patterns' mining has much more information to track and much\ngreater complexity to manage. Infrequent items can become frequent later on and\nhence cannot be ignored. The output structure needs to be dynamically\nincremented to reflect the evolution of itemset frequencies over time. In this\npaper, we study this problem and specifically the methodology of mining\ntime-sensitive data streams. We tried to improve an existing algorithm by\nincreasing the temporal accuracy and discarding the out-of-date data by adding\na new concept called the \"Shaking Point\". We presented as well some experiments\nillustrating the time and space required.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 19:26:24 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Zarrouk", "Manel", ""], ["Gouider", "Med Salah", ""]]}, {"id": "1206.1134", "submitter": "Rachit Agarwal", "authors": "Rachit Agarwal, Matthew Caesar, P. Brighten Godfrey, Ben Y. Zhao", "title": "Shortest Paths in Less Than a Millisecond", "comments": "6 pages; to appear in SIGCOMM WOSN 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of answering point-to-point shortest path queries on\nmassive social networks. The goal is to answer queries within tens of\nmilliseconds while minimizing the memory requirements. We present a technique\nthat achieves this goal for an extremely large fraction of path queries by\nexploiting the structure of the social networks.\n  Using evaluations on real-world datasets, we argue that our technique offers\na unique trade-off between latency, memory and accuracy. For instance, for the\nLiveJournal social network (roughly 5 million nodes and 69 million edges), our\ntechnique can answer 99.9% of the queries in less than a millisecond. In\ncomparison to storing all pair shortest paths, our technique requires at least\n550x less memory; the average query time is roughly 365 microseconds --- 430x\nfaster than the state-of-the-art shortest path algorithm. Furthermore, the\nrelative performance of our technique improves with the size (and density) of\nthe network. For the Orkut social network (3 million nodes and 220 million\nedges), for instance, our technique is roughly 2588x faster than the\nstate-of-the-art algorithm for computing shortest paths.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 07:13:37 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Agarwal", "Rachit", ""], ["Caesar", "Matthew", ""], ["Godfrey", "P. Brighten", ""], ["Zhao", "Ben Y.", ""]]}, {"id": "1206.1430", "submitter": "Yogita Khatri", "authors": "Yogita Khatri", "title": "Distance Based Asynchronous Recovery Approach in Mobile Computing\n  Environment", "comments": "7 pages, 1figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile computing system is a distributed system in which at least one of\nthe processes is mobile. They are constrained by lack of stable storage, low\nnetwork bandwidth, mobility, frequent disconnection and limited battery life.\nCheckpointing is one of the commonly used techniques to provide fault tolerance\nin mobile computing environment. In order to suit the mobile environment a\ndistance based recovery scheme is proposed which is based on checkpointing and\nmessage logging. After the system recovers from failures, only the failed\nprocesses rollback and restart from their respective recent checkpoints,\nindependent of the others. The salient feature of this scheme is to reduce the\ntransfer and recovery cost. While the mobile host moves with in a specific\nrange, recovery information is not moved and thus only be transferred nearby if\nthe mobile host moves out of certain range.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 09:44:13 GMT"}], "update_date": "2012-06-08", "authors_parsed": [["Khatri", "Yogita", ""]]}, {"id": "1206.4555", "submitter": "Jarek Duda dr", "authors": "Jarek Duda", "title": "Optimal compression of hash-origin prefix trees", "comments": "13 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB cs.DS math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a common problem of operating on hash values of elements of some\ndatabase. In this paper there will be analyzed informational content of such\ngeneral task and how to practically approach such found lower boundaries.\nMinimal prefix tree which distinguish elements turns out to require\nasymptotically only about 2.77544 bits per element, while standard approaches\nuse a few times more. While being certain of working inside the database, the\ncost of distinguishability can be reduced further to about 2.33275 bits per\nelements. Increasing minimal depth of nodes to reduce probability of false\npositives leads to simple relation with average depth of such random tree,\nwhich is asymptotically larger by about 1.33275 bits than lg(n) of the perfect\nbinary tree. This asymptotic case can be also seen as a way to optimally encode\nn large unordered numbers - saving lg(n!) bits of information about their\nordering, which can be the major part of contained information. This ability\nitself allows to reduce memory requirements even to about 0.693 of required in\nBloom filter for the same false positive probability.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 16:53:04 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2012 15:27:48 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2012 12:06:42 GMT"}, {"version": "v4", "created": "Sun, 8 Jul 2012 15:05:35 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Duda", "Jarek", ""]]}, {"id": "1206.4952", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Jennifer Neville, Ramana Kompella", "title": "Space-Efficient Sampling from Social Activity Streams", "comments": "BigMine 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to efficiently study the characteristics of network domains and\nsupport development of network systems (e.g. algorithms, protocols that operate\non networks), it is often necessary to sample a representative subgraph from a\nlarge complex network. Although recent subgraph sampling methods have been\nshown to work well, they focus on sampling from memory-resident graphs and\nassume that the sampling algorithm can access the entire graph in order to\ndecide which nodes/edges to select. Many large-scale network datasets, however,\nare too large and/or dynamic to be processed using main memory (e.g., email,\ntweets, wall posts). In this work, we formulate the problem of sampling from\nlarge graph streams. We propose a streaming graph sampling algorithm that\ndynamically maintains a representative sample in a reservoir based setting. We\nevaluate the efficacy of our proposed methods empirically using several\nreal-world data sets. Across all datasets, we found that our method produce\nsamples that preserve better the original graph distributions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 04:55:20 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Neville", "Jennifer", ""], ["Kompella", "Ramana", ""]]}, {"id": "1206.5021", "submitter": "L\\'aszl\\'o Dobos", "authors": "L\\'aszl\\'o Dobos, Tam\\'as Budav\\'ari, Nolan Li, Alexander S. Szalay\n  and Istv\\'an Csabai", "title": "SkyQuery: An Implementation of a Parallel Probabilistic Join Engine for\n  Cross-Identification of Multiple Astronomical Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-wavelength astronomical studies require cross-identification of\ndetections of the same celestial objects in multiple catalogs based on\nspherical coordinates and other properties. Because of the large data volumes\nand spherical geometry, the symmetric N-way association of astronomical\ndetections is a computationally intensive problem, even when sophisticated\nindexing schemes are used to exclude obviously false candidates. Legacy\nastronomical catalogs already contain detections of more than a hundred million\nobjects while the ongoing and future surveys will produce catalogs of billions\nof objects with multiple detections of each at different times. The varying\nstatistical error of position measurements, moving and extended objects, and\nother physical properties make it necessary to perform the cross-identification\nusing a mathematically correct, proper Bayesian probabilistic algorithm,\ncapable of including various priors. One time, pair-wise cross-identification\nof these large catalogs is not sufficient for many astronomical scenarios.\nConsequently, a novel system is necessary that can cross-identify multiple\ncatalogs on-demand, efficiently and reliably. In this paper, we present our\nsolution based on a cluster of commodity servers and ordinary relational\ndatabases. The cross-identification problems are formulated in a language based\non SQL, but extended with special clauses. These special queries are\npartitioned spatially by coordinate ranges and compiled into a complex workflow\nof ordinary SQL queries. Workflows are then executed in a parallel framework\nusing a cluster of servers hosting identical mirrors of the same data sets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2012 21:50:44 GMT"}], "update_date": "2012-06-25", "authors_parsed": [["Dobos", "L\u00e1szl\u00f3", ""], ["Budav\u00e1ri", "Tam\u00e1s", ""], ["Li", "Nolan", ""], ["Szalay", "Alexander S.", ""], ["Csabai", "Istv\u00e1n", ""]]}, {"id": "1206.5637", "submitter": "Edith Cohen", "authors": "Edith Cohen and Haim Kaplan", "title": "What you can do with Coordinated Samples", "comments": "4 figures, 21 pages, Extended Abstract appeared in RANDOM 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample coordination, where similar instances have similar samples, was\nproposed by statisticians four decades ago as a way to maximize overlap in\nrepeated surveys. Coordinated sampling had been since used for summarizing\nmassive data sets.\n  The usefulness of a sampling scheme hinges on the scope and accuracy within\nwhich queries posed over the original data can be answered from the sample. We\naim here to gain a fundamental understanding of the limits and potential of\ncoordination. Our main result is a precise characterization, in terms of simple\nproperties of the estimated function, of queries for which estimators with\ndesirable properties exist. We consider unbiasedness, nonnegativity, finite\nvariance, and bounded estimates.\n  Since generally a single estimator can not be optimal (minimize variance\nsimultaneously) for all data, we propose {\\em variance competitiveness}, which\nmeans that the expectation of the square on any data is not too far from the\nminimum one possible for the data. Surprisingly perhaps, we show how to\nconstruct, for any function for which an unbiased nonnegative estimator exists,\na variance competitive estimator.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2012 10:35:28 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2012 07:14:14 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2012 06:42:00 GMT"}, {"version": "v4", "created": "Wed, 12 Jun 2013 11:04:47 GMT"}, {"version": "v5", "created": "Wed, 24 Jul 2013 18:27:33 GMT"}, {"version": "v6", "created": "Fri, 2 Aug 2013 17:24:24 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Cohen", "Edith", ""], ["Kaplan", "Haim", ""]]}, {"id": "1206.5930", "submitter": "Klara Stokes", "authors": "Klara Stokes and Oriol Farr\\`as", "title": "Linear spaces and transversal designs: k-anonymous combinatorial\n  configurations for anonymous database search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymous database search protocols allow users to query a database\nanonymously. This can be achieved by letting the users form a peer-to-peer\ncommunity and post queries on behalf of each other. In this article we discuss\nan application of combinatorial configurations (also known as regular and\nuniform partial linear spaces) to a protocol for anonymous database search, as\ndefining the key-distribution within the user community that implements the\nprotocol. The degree of anonymity that can be provided by the protocol is\ndetermined by properties of the neighborhoods and the closed neighborhoods of\nthe points in the combinatorial configuration that is used. Combinatorial\nconfigurations with unique neighborhoods or unique closed neighborhoods are\ndescribed and we show how to attack the protocol if such configurations are\nused. We apply k-anonymity arguments and present the combinatorial\nconfigurations with k-anonymous neighborhoods and with k-anonymous closed\nneighborhoods. The transversal designs and the linear spaces are presented as\noptimal configurations among the configurations with k-anonymous neighborhoods\nand k-anonymous closed neighborhoods, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2012 09:18:50 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2012 18:23:29 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Stokes", "Klara", ""], ["Farr\u00e0s", "Oriol", ""]]}, {"id": "1206.6196", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (IRISA), Nicolas Bonnel (IRISA), Gilbas\n  M\\'enier (IRISA)", "title": "Discrete Elastic Inner Vector Spaces with Application in Time Series and\n  Sequence Mining", "comments": "arXiv admin note: substantial text overlap with arXiv:1101.4318", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering (2012) pp 1-14", "doi": "10.1109/TKDE.2012.131", "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a framework dedicated to the construction of what we call\ndiscrete elastic inner product allowing one to embed sets of non-uniformly\nsampled multivariate time series or sequences of varying lengths into inner\nproduct space structures. This framework is based on a recursive definition\nthat covers the case of multiple embedded time elastic dimensions. We prove\nthat such inner products exist in our general framework and show how a simple\ninstance of this inner product class operates on some prospective applications,\nwhile generalizing the Euclidean inner product. Classification experimentations\non time series and symbolic sequences datasets demonstrate the benefits that we\ncan expect by embedding time series or sequences into elastic inner spaces\nrather than into classical Euclidean spaces. These experiments show good\naccuracy when compared to the euclidean distance or even dynamic programming\nalgorithms while maintaining a linear algorithmic complexity at exploitation\nstage, although a quadratic indexing phase beforehand is required.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 07:44:15 GMT"}], "update_date": "2012-06-28", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "IRISA"], ["Bonnel", "Nicolas", "", "IRISA"], ["M\u00e9nier", "Gilbas", "", "IRISA"]]}, {"id": "1206.6293", "submitter": "Alexander Sch\\\"atzle", "authors": "Martin Przyjaciel-Zablocki, Alexander Sch\\\"atzle, Thomas Hornung,\n  Christopher Dorner, Georg Lausen", "title": "Cascading map-side joins over HBase for scalable join processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in large-scale data processing with MapReduce is\nthe smart computation of joins. Since Semantic Web datasets published in RDF\nhave increased rapidly over the last few years, scalable join techniques become\nan important issue for SPARQL query processing as well. In this paper, we\nintroduce the Map-Side Index Nested Loop Join (MAPSIN join) which combines\nscalable indexing capabilities of NoSQL storage systems like HBase, that suffer\nfrom an insufficient distributed processing layer, with MapReduce, which in\nturn does not provide appropriate storage structures for efficient large-scale\njoin processing. While retaining the flexibility of commonly used reduce-side\njoins, we leverage the effectiveness of map-side joins without any changes to\nthe underlying framework. We demonstrate the significant benefits of MAPSIN\njoins for the processing of SPARQL basic graph patterns on large RDF datasets\nby an evaluation with the LUBM and SP2Bench benchmarks. For most queries,\nMAPSIN join based query execution outperforms reduce-side join based execution\nby an order of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 15:05:05 GMT"}], "update_date": "2012-06-28", "authors_parsed": [["Przyjaciel-Zablocki", "Martin", ""], ["Sch\u00e4tzle", "Alexander", ""], ["Hornung", "Thomas", ""], ["Dorner", "Christopher", ""], ["Lausen", "Georg", ""]]}, {"id": "1206.6322", "submitter": "Nabendu Chaki Dr.", "authors": "Soumya Sen, Anjan Dutta, Agostino Cortesi, Nabendu Chaki", "title": "A New Scale for Attribute Dependency in Large Database Systems", "comments": "12 pages - paper accepted for presentation and publication in CISIM\n  2012 International Confrence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, data centric applications are characterized by its different\nattributes. In modern day, a huge majority of the large data centric\napplications are based on relational model. The databases are collection of\ntables and every table consists of numbers of attributes. The data is accessed\ntypically through SQL queries. The queries that are being executed could be\nanalyzed for different types of optimizations. Analysis based on different\nattributes used in a set of query would guide the database administrators to\nenhance the speed of query execution. A better model in this context would help\nin predicting the nature of upcoming query set. An effective prediction model\nwould guide in different applications of database, data warehouse, data mining\netc. In this paper, a numeric scale has been proposed to enumerate the strength\nof associations between independent data attributes. The proposed scale is\nbuilt based on some probabilistic analysis of the usage of the attributes in\ndifferent queries. Thus this methodology aims to predict future usage of\nattributes based on the current usage.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:09:46 GMT"}], "update_date": "2012-06-28", "authors_parsed": [["Sen", "Soumya", ""], ["Dutta", "Anjan", ""], ["Cortesi", "Agostino", ""], ["Chaki", "Nabendu", ""]]}, {"id": "1206.6411", "submitter": "Junfeng He", "authors": "Junfeng He (Columbia University), Sanjiv Kumar (Google Research),\n  Shih-Fu Chang (Columbia University)", "title": "On the Difficulty of Nearest Neighbor Search", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast approximate nearest neighbor (NN) search in large databases is becoming\npopular. Several powerful learning-based formulations have been proposed\nrecently. However, not much attention has been paid to a more fundamental\nquestion: how difficult is (approximate) nearest neighbor search in a given\ndata set? And which data properties affect the difficulty of nearest neighbor\nsearch and how? This paper introduces the first concrete measure called\nRelative Contrast that can be used to evaluate the influence of several crucial\ndata characteristics such as dimensionality, sparsity, and database size\nsimultaneously in arbitrary normed metric spaces. Moreover, we present a\ntheoretical analysis to prove how the difficulty measure (relative contrast)\ndetermines/affects the complexity of Local Sensitive Hashing, a popular\napproximate NN search method. Relative contrast also provides an explanation\nfor a family of heuristic hashing algorithms with good practical performance\nbased on PCA. Finally, we show that most of the previous works in measuring NN\nsearch meaningfulness/difficulty can be derived as special asymptotic cases for\ndense vectors of the proposed measure.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["He", "Junfeng", "", "Columbia University"], ["Kumar", "Sanjiv", "", "Google Research"], ["Chang", "Shih-Fu", "", "Columbia University"]]}, {"id": "1206.6557", "submitter": "Tingyu Liu", "authors": "Tingyu Liu, Yalong Cheng, Zhonghua Ni", "title": "Mining Event Logs to Support Workflow Resource Allocation", "comments": "T. Liu et al., Mining event logs to support workflow resource\n  allocation, Knowl. Based Syst. (2012), http://dx.doi.org/\n  10.1016/j.knosys.2012.05.010", "journal-ref": "Knowledge-based Systems 35(2012) 320-331", "doi": "10.1016/j.knosys.2012.05.010", "report-no": null, "categories": "cs.SE cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Workflow technology is widely used to facilitate the business process in\nenterprise information systems (EIS), and it has the potential to reduce design\ntime, enhance product quality and decrease product cost. However, significant\nlimitations still exist: as an important task in the context of workflow, many\npresent resource allocation operations are still performed manually, which are\ntime-consuming. This paper presents a data mining approach to address the\nresource allocation problem (RAP) and improve the productivity of workflow\nresource management. Specifically, an Apriori-like algorithm is used to find\nthe frequent patterns from the event log, and association rules are generated\naccording to predefined resource allocation constraints. Subsequently, a\ncorrelation measure named lift is utilized to annotate the negatively\ncorrelated resource allocation rules for resource reservation. Finally, the\nrules are ranked using the confidence measures as resource allocation rules.\nComparative experiments are performed using C4.5, SVM, ID3, Na\\\"ive Bayes and\nthe presented approach, and the results show that the presented approach is\neffective in both accuracy and candidate resource recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2012 03:36:28 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Liu", "Tingyu", ""], ["Cheng", "Yalong", ""], ["Ni", "Zhonghua", ""]]}, {"id": "1206.6646", "submitter": "Arnab Bhattacharya", "authors": "Arnab Bhattacharya and B. Palvali Teja", "title": "Aggregate Skyline Join Queries: Skylines with Aggregate Operations over\n  Multiple Relations", "comments": "Best student paper award; COMAD 2010 (International Conference on\n  Management of Data)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-criteria decision making, which is possible with the advent of\nskyline queries, has been applied in many areas. Though most of the existing\nresearch is concerned with only a single relation, several real world\napplications require finding the skyline set of records over multiple\nrelations. Consequently, the join operation over skylines where the preferences\nare local to each relation, has been proposed. In many of those cases, however,\nthe join often involves performing aggregate operations among some of the\nattributes from the different relations. In this paper, we introduce such\nqueries as \"aggregate skyline join queries\". Since the naive algorithm is\nimpractical, we propose three algorithms to efficiently process such queries.\nThe algorithms utilize certain properties of skyline sets, and processes the\nskylines as much as possible locally before computing the join. Experiments\nwith real and synthetic datasets exhibit the practicality and scalability of\nthe algorithms with respect to the cardinality and dimensionality of the\nrelations.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2012 12:06:51 GMT"}], "update_date": "2012-06-29", "authors_parsed": [["Bhattacharya", "Arnab", ""], ["Teja", "B. Palvali", ""]]}, {"id": "1206.6864", "submitter": "Zhao Xu", "authors": "Zhao Xu, Volker Tresp, Kai Yu, Hans-Peter Kriegel", "title": "Infinite Hidden Relational Models", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-544-551", "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases it makes sense to model a relationship symmetrically, not\nimplying any particular directionality. Consider the classical example of a\nrecommendation system where the rating of an item by a user should\nsymmetrically be dependent on the attributes of both the user and the item. The\nattributes of the (known) relationships are also relevant for predicting\nattributes of entities and for predicting attributes of new relations. In\nrecommendation systems, the exploitation of relational attributes is often\nreferred to as collaborative filtering. Again, in many applications one might\nprefer to model the collaborative effect in a symmetrical way. In this paper we\npresent a relational model, which is completely symmetrical. The key innovation\nis that we introduce for each entity (or object) an infinite-dimensional latent\nvariable as part of a Dirichlet process (DP) model. We discuss inference in the\nmodel, which is based on a DP Gibbs sampler, i.e., the Chinese restaurant\nprocess. We extend the Chinese restaurant process to be applicable to\nrelational modeling. Our approach is evaluated in three applications. One is a\nrecommendation system based on the MovieLens data set. The second application\nconcerns the prediction of the function of yeast genes/proteins on the data set\nof KDD Cup 2001 using a multi-relational model. The third application involves\na relational medical domain. The experimental results show that our model gives\nsignificantly improved estimates of attributes describing relationships or\nentities in complex relational models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:28:29 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Xu", "Zhao", ""], ["Tresp", "Volker", ""], ["Yu", "Kai", ""], ["Kriegel", "Hans-Peter", ""]]}]