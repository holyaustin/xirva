[{"id": "1612.00151", "submitter": "Vijendra Singh", "authors": "Singh Vijendra, Hemjyotsana Parashar and Nisha Vasudeva", "title": "A New Method for Classification of Datasets for Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Decision tree is an important method for both induction research and data\nmining, which is mainly used for model classification and prediction. ID3\nalgorithm is the most widely used algorithm in the decision tree so far. In\nthis paper, the shortcoming of ID3's inclining to choose attributes with many\nvalues is discussed, and then a new decision tree algorithm which is improved\nversion of ID3. In our proposed algorithm attributes are divided into groups\nand then we apply the selection measure 5 for these groups. If information gain\nis not good then again divide attributes values into groups. These steps are\ndone until we get good classification/misclassification ratio. The proposed\nalgorithms classify the data sets more accurately and efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:24:36 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Vijendra", "Singh", ""], ["Parashar", "Hemjyotsana", ""], ["Vasudeva", "Nisha", ""]]}, {"id": "1612.00476", "submitter": "Vinay Deolalikar", "authors": "Vinay Deolalikar and Hernan Laffitte", "title": "Extensive Large-Scale Study of Error in Samping-Based Distinct Value\n  Estimators for Databases", "comments": "This is the full-length version of a shorter published paper, and\n  includes supplementary material for the published paper. Please cite as\n  \"Vinay Deolalikar and Hernan Laffitte: Extensive Large-Scale Study of Error\n  in Samping-Based Distinct Value Estimators for Databases, IEEE Big Data\n  Conference, Washington DC, December 2016.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of distinct value estimation has many applications. Being a\ncritical component of query optimizers in databases, it also has high\ncommercial impact. Many distinct value estimators have been proposed, using\nvarious statistical approaches. However, characterizing the errors incurred by\nthese estimators is an open problem: existing analytical approaches are not\npowerful enough, and extensive empirical studies at large scale do not exist.\nWe conduct an extensive large-scale empirical study of 11 distinct value\nestimators from four different approaches to the problem over families of\nZipfian distributions whose parameters model real-world applications. Our study\nis the first that \\emph{scales to the size of a billion-rows} that today's\nlarge commercial databases have to operate in. This allows us to characterize\nthe error that is encountered in real-world applications of distinct value\nestimation. By mining the generated data, we show that estimator error depends\non a key latent parameter --- the average uniform class size --- that has not\nbeen studied previously. This parameter also allows us to unearth error\npatterns that were previously unknown. Importantly, ours is the first approach\nthat provides a framework for \\emph{visualizing the error patterns} in distinct\nvalue estimation, facilitating discussion of this problem in enterprise\nsettings. Our characterization of errors can be used for several problems in\ndistinct value estimation, such as the design of hybrid estimators. This work\naims at the practitioner and the researcher alike, and addresses questions\nfrequently asked by both audiences.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 21:33:25 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Deolalikar", "Vinay", ""], ["Laffitte", "Hernan", ""]]}, {"id": "1612.00531", "submitter": "Cigdem Aslay", "authors": "Cigdem Aslay, Francesco Bonchi, Laks V.S. Lakshmanan, Wei Lu", "title": "Revenue Maximization in Incentivized Social Advertising", "comments": "15 pages; Provided a detailed theoretical analysis for budget\n  feasibility issue in Section 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incentivized social advertising, an emerging marketing model, provides\nmonetization opportunities not only to the owners of the social networking\nplatforms but also to their influential users by offering a \"cut\" on the\nadvertising revenue. We consider a social network (the host) that sells\nad-engagements to advertisers by inserting their ads, in the form of promoted\nposts, into the feeds of carefully selected \"initial endorsers\" or seed users:\nthese users receive monetary incentives in exchange for their endorsements. The\nendorsements help propagate the ads to the feeds of their followers. In this\ncontext, the problem for the host is is to allocate ads to influential users,\ntaking into account the propensity of ads for viral propagation, and carefully\napportioning the monetary budget of each of the advertisers between incentives\nto influential users and ad-engagement costs, with the rational goal of\nmaximizing its own revenue. We consider a monetary incentive for the\ninfluential users, which is proportional to their influence potential. We show\nthat revenue maximization in incentivized social advertising corresponds to the\nproblem of monotone submodular function maximization, subject to a partition\nmatroid constraint on the ads-to-seeds allocation, and submodular knapsack\nconstraints on the advertisers' budgets. This problem is NP-hard and we devise\n2 greedy algorithms with provable approximation guarantees, which differ in\ntheir sensitivity to seed user incentive costs. Our approximation algorithms\nrequire repeatedly estimating the expected marginal gain in revenue as well as\nin advertiser payment. By exploiting a connection to the recent advances made\nin scalable estimation of expected influence spread, we devise efficient and\nscalable versions of the greedy algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 01:11:12 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 17:46:04 GMT"}, {"version": "v3", "created": "Sat, 17 Dec 2016 09:45:26 GMT"}, {"version": "v4", "created": "Sun, 16 Apr 2017 15:52:21 GMT"}, {"version": "v5", "created": "Wed, 19 Apr 2017 10:42:00 GMT"}, {"version": "v6", "created": "Tue, 22 Jun 2021 14:25:50 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Aslay", "Cigdem", ""], ["Bonchi", "Francesco", ""], ["Lakshmanan", "Laks V. S.", ""], ["Lu", "Wei", ""]]}, {"id": "1612.00623", "submitter": "Vijendra Singh", "authors": "Singh Vijendra and Priyanka Trikha", "title": "Density Based Algorithm With Automatic Parameters Generation", "comments": "2011 IEEE 3rd International Conference on Machine Learning and\n  Computing (ICMLC 2011), Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The traditional algorithms do not meet the latest multiple requirements\nsimultaneously for objects. Density-based method is one of the methodologies,\nwhich can detect arbitrary shaped clusters where clusters are defined as dense\nregions separated by low density regions. In this paper, we present a new\nclustering algorithm to enhance the density-based algorithm DBSCAN. This\nenables an automatic parameter generation strategy to create clusters with\ndifferent densities and enables noises recognition, and generates arbitrary\nshaped clusters. The kdtree is used for increasing the memory efficiency.\nExperimental result shows that proposed algorithm is capable of handling\ncomplex objects with good memory efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 10:38:49 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Vijendra", "Singh", ""], ["Trikha", "Priyanka", ""]]}, {"id": "1612.01040", "submitter": "Tim Kraska", "authors": "Zheguang Zhao, Lorenzo De Stefani, Emanuel Zgraggen, Carsten Binnig,\n  Eli Upfal, Tim Kraska", "title": "Controlling False Discoveries During Interactive Data Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent tools for interactive data exploration significantly increase the\nchance that users make false discoveries. The crux is that these tools\nimplicitly allow the user to test a large body of different hypotheses with\njust a few clicks thus incurring in the issue commonly known in statistics as\nthe multiple hypothesis testing error. In this paper, we propose solutions to\nintegrate multiple hypothesis testing control into interactive data exploration\ntools. A key insight is that existing methods for controlling the false\ndiscovery rate (such as FDR) are not directly applicable for interactive data\nexploration. We therefore discuss a set of new control procedures that are\nbetter suited and integrated them in our system called Aware. By means of\nextensive experiments using both real-world and synthetic data sets we\ndemonstrate how Aware can help experts and novice users alike to efficiently\ncontrol false discoveries.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 00:24:17 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhao", "Zheguang", ""], ["De Stefani", "Lorenzo", ""], ["Zgraggen", "Emanuel", ""], ["Binnig", "Carsten", ""], ["Upfal", "Eli", ""], ["Kraska", "Tim", ""]]}, {"id": "1612.01090", "submitter": "Eid AlDikanji Mr.", "authors": "Eid Aldikanji and Khalil Ajami", "title": "Studying Academic Indicators within Virtual Learning Environment Using\n  Educational Data Mining", "comments": null, "journal-ref": null, "doi": "10.5121/ijdkp.2016.6603", "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our main goal is to discover the main factors influencing students' academic\ntrajectory and students' academic evolution within such environment. Our\nresults indicate strong correlation in this virtual learning environment\nbetween student average and some factors like: student's English level (despite\nthe fact that Arabic language is the teaching language), student's age,\nstudent's gender, student's over-stay and student's place of residence (inside\nor outside Syria). Our results indicate also a need to modify the academic\ntrajectory of students by changing the prerequisites of few courses delivered\nas a part of BIT diploma like Advanced DBA II, Data Security. In this research,\nthe results also highlight the effect of the Syrian Crisis on students.\nFinally, we've suggested some future recommendations based on our observations\nand results to develop the current information system in SVU in order to help\nus to deduce some indicators more easily.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 09:38:20 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Aldikanji", "Eid", ""], ["Ajami", "Khalil", ""]]}, {"id": "1612.01641", "submitter": "EPTCS", "authors": "Thomas Beyhl (Hasso Plattner Institute at the University of Potsdam),\n  Holger Giese (Hasso Plattner Institute at the University of Potsdam)", "title": "Incremental View Maintenance for Deductive Graph Databases Using\n  Generalized Discrimination Networks", "comments": "In Proceedings GaM 2016, arXiv:1612.01053", "journal-ref": "EPTCS 231, 2016, pp. 57-71", "doi": "10.4204/EPTCS.231.5", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, graph databases are employed when relationships between entities\nare in the scope of database queries to avoid performance-critical join\noperations of relational databases. Graph queries are used to query and modify\ngraphs stored in graph databases. Graph queries employ graph pattern matching\nthat is NP-complete for subgraph isomorphism. Graph database views can be\nemployed that keep ready answers in terms of precalculated graph pattern\nmatches for often stated and complex graph queries to increase query\nperformance. However, such graph database views must be kept consistent with\nthe graphs stored in the graph database.\n  In this paper, we describe how to use incremental graph pattern matching as\ntechnique for maintaining graph database views. We present an incremental\nmaintenance algorithm for graph database views, which works for imperatively\nand declaratively specified graph queries. The evaluation shows that our\nmaintenance algorithm scales when the number of nodes and edges stored in the\ngraph database increases. Furthermore, our evaluation shows that our approach\ncan outperform existing approaches for the incremental maintenance of graph\nquery results.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 02:36:47 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Beyhl", "Thomas", "", "Hasso Plattner Institute at the University of Potsdam"], ["Giese", "Holger", "", "Hasso Plattner Institute at the University of Potsdam"]]}, {"id": "1612.01835", "submitter": "M Sadegh Riazi", "authors": "M. Sadegh Riazi, Beidi Chen, Anshumali Shrivastava, Dan Wallach,\n  Farinaz Koushanfar", "title": "Sub-Linear Privacy-Preserving Near-Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Near-Neighbor Search (NNS), a new client queries a database (held by a\nserver) for the most similar data (near-neighbors) given a certain similarity\nmetric. The Privacy-Preserving variant (PP-NNS) requires that neither server\nnor the client shall learn information about the other party's data except what\ncan be inferred from the outcome of NNS. The overwhelming growth in the size of\ncurrent datasets and the lack of a truly secure server in the online world\nrender the existing solutions impractical; either due to their high\ncomputational requirements or non-realistic assumptions which potentially\ncompromise privacy. PP-NNS having query time {\\it sub-linear} in the size of\nthe database has been suggested as an open research direction by Li et al.\n(CCSW'15). In this paper, we provide the first such algorithm, called Secure\nLocality Sensitive Indexing (SLSI) which has a sub-linear query time and the\nability to handle honest-but-curious parties. At the heart of our proposal lies\na secure binary embedding scheme generated from a novel probabilistic\ntransformation over locality sensitive hashing family. We provide information\ntheoretic bound for the privacy guarantees and support our theoretical claims\nusing substantial empirical evidence on real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 14:53:06 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 08:41:48 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 16:46:39 GMT"}, {"version": "v4", "created": "Thu, 17 Oct 2019 17:36:02 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Riazi", "M. Sadegh", ""], ["Chen", "Beidi", ""], ["Shrivastava", "Anshumali", ""], ["Wallach", "Dan", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1612.02130", "submitter": "Niek Tax", "authors": "Niek Tax, Ilya Verenich, Marcello La Rosa, Marlon Dumas", "title": "Predictive Business Process Monitoring with LSTM Neural Networks", "comments": "Accepted at the International Conference on Advanced Information\n  Systems Engineering (CAiSE) 2017", "journal-ref": "Lecture Notes in Computer Science, 10253 (2017) 477-492", "doi": "10.1007/978-3-319-59536-8_30", "report-no": null, "categories": "stat.AP cs.DB cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive business process monitoring methods exploit logs of completed\ncases of a process in order to make predictions about running cases thereof.\nExisting methods in this space are tailor-made for specific prediction tasks.\nMoreover, their relative accuracy is highly sensitive to the dataset at hand,\nthus requiring users to engage in trial-and-error and tuning when applying them\nin a specific setting. This paper investigates Long Short-Term Memory (LSTM)\nneural networks as an approach to build consistently accurate models for a wide\nrange of predictive process monitoring tasks. First, we show that LSTMs\noutperform existing techniques to predict the next event of a running case and\nits timestamp. Next, we show how to use models for predicting the next task in\norder to predict the full continuation of a running case. Finally, we apply the\nsame approach to predict the remaining time, and show that this approach\noutperforms existing tailor-made methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 07:04:17 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 18:51:41 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Tax", "Niek", ""], ["Verenich", "Ilya", ""], ["La Rosa", "Marcello", ""], ["Dumas", "Marlon", ""]]}, {"id": "1612.02485", "submitter": "Parmita Mehta", "authors": "Parmita Mehta, Sven Dorkenwald, Dongfang Zhao, Tomer Kaftan, Alvin\n  Cheung, Magdalena Balazinska, Ariel Rokem, Andrew Connolly, Jacob Vanderplas,\n  Yusra AlSayyad", "title": "Comparative Evaluation of Big-Data Systems on Scientific Image Analytics\n  Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific discoveries are increasingly driven by analyzing large volumes of\nimage data. Many new libraries and specialized database management systems\n(DBMSs) have emerged to support such tasks. It is unclear, however, how well\nthese systems support real-world image analysis use cases, and how performant\nare the image analytics tasks implemented on top of such systems. In this\npaper, we present the first comprehensive evaluation of large-scale image\nanalysis systems using two real-world scientific image data processing use\ncases. We evaluate five representative systems (SciDB, Myria, Spark, Dask, and\nTensorFlow) and find that each of them has shortcomings that complicate\nimplementation or hurt performance. Such shortcomings lead to new research\nopportunities in making large-scale image analysis both efficient and easy to\nuse.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 23:29:43 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Mehta", "Parmita", ""], ["Dorkenwald", "Sven", ""], ["Zhao", "Dongfang", ""], ["Kaftan", "Tomer", ""], ["Cheung", "Alvin", ""], ["Balazinska", "Magdalena", ""], ["Rokem", "Ariel", ""], ["Connolly", "Andrew", ""], ["Vanderplas", "Jacob", ""], ["AlSayyad", "Yusra", ""]]}, {"id": "1612.02503", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Dan Suciu", "title": "What do Shannon-type Inequalities, Submodular Width, and Disjunctive\n  Datalog have to do with one another?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on bounding the output size of a conjunctive query with\nfunctional dependencies and degree constraints have shown a deep connection\nbetween fundamental questions in information theory and database theory. We\nprove analogous output bounds for disjunctive datalog rules, and answer several\nopen questions regarding the tightness and looseness of these bounds along the\nway. Our bounds are intimately related to Shannon-type information\ninequalities. We devise the notion of a \"proof sequence\" of a specific class of\nShannon-type information inequalities called \"Shannon flow inequalities\". We\nthen show how such a proof sequence can be interpreted as symbolic instructions\nguiding an algorithm called \"PANDA\", which answers disjunctive datalog rules\nwithin the time that the size bound predicted. We show that PANDA can be used\nas a black-box to devise algorithms matching precisely the fractional hypertree\nwidth and the submodular width runtimes for aggregate and conjunctive queries\nwith functional dependencies and degree constraints.\n  Our results improve upon known results in three ways. First, our bounds and\nalgorithms are for the much more general class of disjunctive datalog rules, of\nwhich conjunctive queries are a special case. Second, the runtime of PANDA\nmatches precisely the submodular width bound, while the previous algorithm by\nMarx has a runtime that is polynomial in this bound. Third, our bounds and\nalgorithms work for queries with input cardinality bounds, functional\ndependencies, and degree constraints.\n  Overall, our results show a deep connection between three seemingly unrelated\nlines of research; and, our results on proof sequences for Shannon flow\ninequalities might be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 01:06:40 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2016 08:10:36 GMT"}, {"version": "v3", "created": "Sat, 25 Mar 2017 23:18:03 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 07:13:09 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Suciu", "Dan", ""]]}, {"id": "1612.02564", "submitter": "Zhida Chen", "authors": "Zhida Chen, Gao Cong, Zhenjie Zhang, Tom Z.J. Fu, Lisi Chen", "title": "Distributed Publish/Subscribe Query Processing on the Spatio-Textual\n  Data Stream", "comments": "13 pages, 16 figures, this paper has been accepted by ICDE2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Huge amount of data with both space and text information, e.g., geo-tagged\ntweets, is flooding on the Internet. Such spatio-textual data stream contains\nvaluable information for millions of users with various interests on different\nkeywords and locations. Publish/subscribe systems enable efficient and\neffective information distribution by allowing users to register continuous\nqueries with both spatial and textual constraints. However, the explosive\ngrowth of data scale and user base has posed challenges to the existing\ncentralized publish/subscribe systems for spatio-textual data streams.\n  In this paper, we propose our distributed publish/subscribe system, called\nPS2Stream, which digests a massive spatio-textual data stream and directs the\nstream to target users with registered interests. Compared with existing\nsystems, PS2Stream achieves a better workload distribution in terms of both\nminimizing the total amount of workload and balancing the load of workers. To\nachieve this, we propose a new workload distribution algorithm considering both\nspace and text properties of the data. Additionally, PS2Stream supports dynamic\nload adjustments to adapt to the change of the workload, which makes PS2Stream\nadaptive. Extensive empirical evaluation, on commercial cloud computing\nplatform with real data, validates the superiority of our system design and\nadvantages of our techniques on system performance improvement.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 08:51:23 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 13:55:53 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 08:37:36 GMT"}, {"version": "v4", "created": "Wed, 23 Aug 2017 05:36:07 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Chen", "Zhida", ""], ["Cong", "Gao", ""], ["Zhang", "Zhenjie", ""], ["Fu", "Tom Z. J.", ""], ["Chen", "Lisi", ""]]}, {"id": "1612.02701", "submitter": "Andrei Sorin Sabau", "authors": "Andrei Sorin Sabau", "title": "Stream Clustering using Probabilistic Data Structures", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most density based stream clustering algorithms separate the clustering\nprocess into an online and offline component. Exact summarized statistics are\nbeing employed for defining micro-clusters or grid cells during the online\nstage followed by macro-clustering during the offline stage. This paper\nproposes a novel alternative to the traditional two phase stream clustering\nscheme, introducing sketch-based data structures for assessing both stream\ndensity and cluster membership with probabilistic accuracy guarantees. A\ncount-min sketch using a damped window model estimates stream density. Bloom\nfilters employing a variation of active-active buffering estimate cluster\nmembership. Instances of both types of sketches share the same set of hash\nfunctions. The resulting stream clustering algorithm is capable of detecting\narbitrarily shaped clusters while correctly handling outliers and making no\nassumption on the total number of clusters. Experimental results over a number\nof real and synthetic datasets illustrate the proposed algorithm quality and\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 15:43:54 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Sabau", "Andrei Sorin", ""]]}, {"id": "1612.04094", "submitter": "Fernando Silva-Coira", "authors": "Nieves R. Brisaboa, Ana Cerdeira-Pena, Narciso L\\'opez-L\\'opez,\n  Gonzalo Navarro, Miguel R. Penabad, Fernando Silva-Coira", "title": "Efficient Representation of Multidimensional Data over Hierarchical\n  Domains", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "String Processing and Information Retrieval: 23rd International\n  Symposium, SPIRE 2016, Beppu, Japan, October 18-20, 2016, Proceedings.\n  Springer International Publishing. pp 191-203. ISBN: 9783319460482", "doi": "10.1007/978-3-319-46049-9_19", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of representing multidimensional data where the\ndomain of each dimension is organized hierarchically, and the queries require\nsummary information at a different node in the hierarchy of each dimension.\nThis is the typical case of OLAP databases. A basic approach is to represent\neach hierarchy as a one-dimensional line and recast the queries as\nmultidimensional range queries. This approach can be implemented compactly by\ngeneralizing to more dimensions the $k^2$-treap, a compact representation of\ntwo-dimensional points that allows for efficient summarization queries along\ngeneric ranges. Instead, we propose a more flexible generalization, which\ninstead of a generic quadtree-like partition of the space, follows the domain\nhierarchies across each dimension to organize the partitioning. The resulting\nstructure is much more efficient than a generic multidimensional structure,\nsince queries are resolved by aggregating much fewer nodes of the tree.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 10:52:03 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Cerdeira-Pena", "Ana", ""], ["L\u00f3pez-L\u00f3pez", "Narciso", ""], ["Navarro", "Gonzalo", ""], ["Penabad", "Miguel R.", ""], ["Silva-Coira", "Fernando", ""]]}, {"id": "1612.04203", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Pierre Bourhis, Mika\\\"el Monet, Pierre Senellart", "title": "Combined Tractability of Query Evaluation via Tree Automata and Cycluits\n  (Extended Version)", "comments": "69 pages, accepted at ICDT'17. Appendix F contains results from an\n  independent upcoming journal paper by Michael Benedikt, Pierre Bourhis, Georg\n  Gottlob, and Pierre Senellart", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2017.6", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate parameterizations of both database instances and queries that\nmake query evaluation fixed-parameter tractable in combined complexity. We\nintroduce a new Datalog fragment with stratified negation,\nintensional-clique-guarded Datalog (ICG-Datalog), with linear-time evaluation\non structures of bounded treewidth for programs of bounded rule size. Such\nprograms capture in particular conjunctive queries with simplicial\ndecompositions of bounded width, guarded negation fragment queries of bounded\nCQ-rank, or two-way regular path queries. Our result proceeds via compilation\nto alternating two-way automata, whose semantics is defined via cyclic\nprovenance circuits (cycluits) that can be tractably evaluated. Last, we prove\nthat probabilistic query evaluation remains intractable in combined complexity\nunder this parameterization.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 14:33:48 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 20:14:13 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Monet", "Mika\u00ebl", ""], ["Senellart", "Pierre", ""]]}, {"id": "1612.04286", "submitter": "Peter Christen", "authors": "Peter Christen", "title": "Application of Advanced Record Linkage Techniques for Complex Population\n  Reconstruction", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is the process of identifying records that refer to the same\nentities from several databases. This process is challenging because commonly\nno unique entity identifiers are available. Linkage therefore has to rely on\npartially identifying attributes, such as names and addresses of people. Recent\nyears have seen the development of novel techniques for linking data from\ndiverse application areas, where a major focus has been on linking complex data\nthat contain records about different types of entities. Advanced approaches\nthat exploit both the similarities between record attributes as well as the\nrelationships between entities to identify clusters of matching records have\nbeen developed.\n  In this application paper we study the novel problem where rather than\ndifferent types of entities we have databases where the same entity can have\ndifferent roles, and where these roles change over time. We specifically\ndevelop novel techniques for linking historical birth, death, marriage and\ncensus records with the aim to reconstruct the population covered by these\nrecords over a period of several decades. Our experimental evaluation on real\nScottish data shows that even with advanced linkage techniques that consider\ngroup, relationship, and temporal aspects it is challenging to achieve high\nquality linkage from such complex data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 17:10:11 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Christen", "Peter", ""]]}, {"id": "1612.04883", "submitter": "Tara Safavi", "authors": "Yike Liu, Tara Safavi, Abhilash Dighe, Danai Koutra", "title": "Graph Summarization Methods and Applications: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While advances in computing resources have made processing enormous amounts\nof data possible, human ability to identify patterns in such data has not\nscaled accordingly. Efficient computational methods for condensing and\nsimplifying data are thus becoming vital for extracting actionable insights. In\nparticular, while data summarization techniques have been studied extensively,\nonly recently has summarizing interconnected data, or graphs, become popular.\nThis survey is a structured, comprehensive overview of the state-of-the-art\nmethods for summarizing graph data. We first broach the motivation behind, and\nthe challenges of, graph summarization. We then categorize summarization\napproaches by the type of graphs taken as input and further organize each\ncategory by core methodology. Finally, we discuss applications of summarization\non real-world graphs and conclude by describing some open problems in the\nfield.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 23:39:45 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 03:32:02 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 18:10:26 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Liu", "Yike", ""], ["Safavi", "Tara", ""], ["Dighe", "Abhilash", ""], ["Koutra", "Danai", ""]]}, {"id": "1612.05110", "submitter": "Ilya Kolchinsky", "authors": "Ilya Kolchinsky, Assaf Schuster and Danny Keren", "title": "Efficient Detection of Complex Event Patterns Using Lazy Chain Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex Event Processing (CEP) is an emerging field with important\napplications in many areas. CEP systems collect events arriving from input data\nstreams and use them to infer more complex events according to predefined\npatterns. The Non-deterministic Finite Automaton (NFA) is one of the most\npopular mechanisms on which such systems are based. During the event detection\nprocess, NFAs incrementally extend previously observed partial matches until a\nfull match for the query is found. As a result, each arriving event needs to be\nprocessed to determine whether a new partial match is to be initiated or an\nexisting one extended. This method may be highly inefficient when many of the\nevents do not result in output matches. We present a lazy evaluation mechanism\nthat defers processing of frequent event types and stores them internally upon\narrival. Events are then matched in ascending order of frequency, thus\nminimizing potentially redundant computations. We introduce a Lazy Chain NFA,\nwhich utilizes the above principle, and does not depend on the underlying\npattern structure. An algorithm for constructing a Lazy Chain NFA for common\npattern types is presented, including conjunction, negation and iteration.\nFinally, we experimentally evaluate our mechanism on real-world stock trading\ndata. The results demonstrate a performance gain of two orders of magnitude\nover traditional NFA-based approaches, with significantly reduced memory\nresource requirements.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 15:27:37 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 11:56:20 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kolchinsky", "Ilya", ""], ["Schuster", "Assaf", ""], ["Keren", "Danny", ""]]}, {"id": "1612.05566", "submitter": "Yannis Klonatos", "authors": "Amir Shaikhha, Yannis Klonatos, Christoph Koch", "title": "Building Efficient Query Engines in a High-Level Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstraction without regret refers to the vision of using high-level\nprogramming languages for systems development without experiencing a negative\nimpact on performance. A database system designed according to this vision\noffers both increased productivity and high performance, instead of sacrificing\nthe former for the latter as is the case with existing, monolithic\nimplementations that are hard to maintain and extend. In this article, we\nrealize this vision in the domain of analytical query processing. We present\nLegoBase, a query engine written in the high-level language Scala. The key\ntechnique to regain efficiency is to apply generative programming: LegoBase\nperforms source-to-source compilation and optimizes the entire query engine by\nconverting the high-level Scala code to specialized, low-level C code. We show\nhow generative programming allows to easily implement a wide spectrum of\noptimizations, such as introducing data partitioning or switching from a row to\na column data layout, which are difficult to achieve with existing low-level\nquery compilers that handle only queries. We demonstrate that sufficiently\npowerful abstractions are essential for dealing with the complexity of the\noptimization effort, shielding developers from compiler internals and\ndecoupling individual optimizations from each other. We evaluate our approach\nwith the TPC-H benchmark and show that: (a) With all optimizations enabled,\nLegoBase significantly outperforms a commercial database and an existing query\ncompiler. (b) Programmers need to provide just a few hundred lines of\nhigh-level code for implementing the optimizations, instead of complicated\nlow-level code that is required by existing query compilation approaches. (c)\nThe compilation overhead is low compared to the overall execution time, thus\nmaking our approach usable in practice for compiling query engines.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 17:32:40 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Shaikhha", "Amir", ""], ["Klonatos", "Yannis", ""], ["Koch", "Christoph", ""]]}, {"id": "1612.05665", "submitter": "Yihan Sun", "authors": "Yihan Sun and Daniel Ferizovic and Guy E. Blelloch", "title": "PAM: Parallel Augmented Maps", "comments": null, "journal-ref": "Yihan Sun, Daniel Ferizovic, and Guy E. Belloch. 2018. PAM:\n  parallel augmented maps. In Proceedings of the 23rd ACM SIGPLAN Symposium on\n  Principles and Practice of Parallel Programming (PPoPP '18). ACM, New York,\n  NY, USA, 290-304", "doi": "10.1145/3178487.3178509", "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordered (key-value) maps are an important and widely-used data type for\nlarge-scale data processing frameworks. Beyond simple search, insertion and\ndeletion, more advanced operations such as range extraction, filtering, and\nbulk updates form a critical part of these frameworks.\n  We describe an interface for ordered maps that is augmented to support fast\nrange queries and sums, and introduce a parallel and concurrent library called\nPAM (Parallel Augmented Maps) that implements the interface. The interface\nincludes a wide variety of functions on augmented maps ranging from basic\ninsertion and deletion to more interesting functions such as union,\nintersection, filtering, extracting ranges, splitting, and range-sums. We\ndescribe algorithms for these functions that are efficient both in theory and\npractice.\n  As examples of the use of the interface and the performance of PAM, we apply\nthe library to four applications: simple range sums, interval trees, 2D range\ntrees, and ranked word index searching. The interface greatly simplifies the\nimplementation of these data structures over direct implementations.\nSequentially the code achieves performance that matches or exceeds existing\nlibraries designed specially for a single application, and in parallel our\nimplementation gets speedups ranging from 40 to 90 on 72 cores with 2-way\nhyperthreading.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 22:02:49 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 22:36:15 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 18:31:41 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sun", "Yihan", ""], ["Ferizovic", "Daniel", ""], ["Blelloch", "Guy E.", ""]]}, {"id": "1612.05786", "submitter": "Antoine Amarilli", "authors": "Luis Gal\\'arraga, Simon Razniewski, Antoine Amarilli, Fabian M.\n  Suchanek", "title": "Predicting Completeness in Knowledge Bases", "comments": "21 pages, 19 references, 1 figure, 5 tables. Complete version of the\n  article accepted at WSDM'17", "journal-ref": null, "doi": "10.1145/3018661.3018739", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases such as Wikidata, DBpedia, or YAGO contain millions of\nentities and facts. In some knowledge bases, the correctness of these facts has\nbeen evaluated. However, much less is known about their completeness, i.e., the\nproportion of real facts that the knowledge bases cover. In this work, we\ninvestigate different signals to identify the areas where a knowledge base is\ncomplete. We show that we can combine these signals in a rule mining approach,\nwhich allows us to predict where facts may be missing. We also show that\ncompleteness predictions can help other applications such as fact prediction.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 16:08:04 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Gal\u00e1rraga", "Luis", ""], ["Razniewski", "Simon", ""], ["Amarilli", "Antoine", ""], ["Suchanek", "Fabian M.", ""]]}, {"id": "1612.05858", "submitter": "Afsin Akdogan", "authors": "Afsin Akdogan", "title": "Partitioning, Indexing and Querying Spatial Data on Cloud", "comments": "PhD Dissertation - University of Southern California", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of mobile devices (e.g., smartphones, wearable technologies) is\nrapidly growing. In line with this trend, a massive amount of spatial data is\nbeing collected since these devices allow users to geo-tag user-generated\ncontent. Clearly, a scalable computing infrastructure is needed to manage such\nlarge datasets. Meanwhile, Cloud Computing service providers (e.g., Amazon,\nGoogle, and Microsoft) allow users to lease computing resources. However, most\nof the existing spatial indexing techniques are designed for the centralized\nparadigm which is limited to the capabilities of a single sever. To address the\nscalability shortcomings of existing approaches, we provide a study that focus\non generating a distributed spatial index structure that not only scales out to\nmultiple servers but also scales up since it fully exploits the multi-core CPUs\navailable on each server using Voronoi diagram as the partitioning and indexing\ntechnique which we also use to process spatial queries effectively. More\nspecifically, since the data objects continuously move and issue position\nupdates to the index structure, we collect the latest positions of objects and\nperiodically generate a read-only index to eliminate costly distributed\nupdates. Our approach scales near-linearly in index construction and query\nprocessing, and can efficiently construct an index for millions of objects\nwithin a few seconds. In addition to scalability and efficiency, we also aim to\nmaximize the server utilization that can support the same workload with less\nnumber of servers. Server utilization is a crucial point while using Cloud\nComputing because users are charged based on the total amount of time they\nreserve each server, with no consideration of utilization.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 06:24:06 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Akdogan", "Afsin", ""]]}, {"id": "1612.06195", "submitter": "Jerome Darmont", "authors": "Ciprian-Octavian Truic\\u{a}, J\\'er\\^ome Darmont (ERIC), Julien Velcin\n  (ERIC)", "title": "A Scalable Document-based Architecture for Text Analysis", "comments": null, "journal-ref": "12th International Conference on Advanced Data Mining and\n  Applications (ADMA 2016), Dec 2016, Gold Coast, Australia. Springer, 10086,\n  pp.481-494, 2016, Lecture Notes in Artificial Intelligence", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing textual data is a very challenging task because of the huge volume\nof data generated daily. Fundamental issues in text analysis include the lack\nof structure in document datasets, the need for various preprocessing steps\n%(e.g., stem or lemma extraction, part-of-speech tagging, named entities\nrecognition...), and performance and scaling issues. Existing text analysis\narchitectures partly solve these issues, providing restrictive data schemas,\naddressing only one aspect of text preprocessing and focusing on one single\ntask when dealing with performance optimization. %As a result, no definite\nsolution is currently available. Thus, we propose in this paper a new generic\ntext analysis architecture, where document structure is flexible, many\npreprocessing techniques are integrated and textual datasets are indexed for\nefficient access. We implement our conceptual architecture using both a\nrelational and a document-oriented database. Our experiments demonstrate the\nfeasibility of our approach and the superiority of the document-oriented\nlogical and physical implementation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 14:24:23 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Truic\u0103", "Ciprian-Octavian", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Velcin", "Julien", "", "ERIC"]]}, {"id": "1612.07404", "submitter": "Arijit Khan", "authors": "Arijit Khan", "title": "Vertex-Centric Graph Processing: The Good, the Bad, and the Ugly", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed graph algorithms that adopt an iterative vertex-centric\nframework for graph processing, popularized by the Google's Pregel system.\nSince then, there are several attempts to implement many graph algorithms in a\nvertex-centric framework, as well as efforts to design optimization techniques\nfor improving the efficiency. However, to the best of our knowledge, there has\nnot been any systematic study to compare these vertex-centric implementations\nwith their sequential counterparts. Our paper addresses this gap in two ways.\n(1) We analyze the computational complexity of such implementations with the\nnotion of time-processor product, and benchmark several vertex-centric graph\nalgorithms whether they perform more work with respect to their best-known\nsequential solutions. (2) Employing the concept of balanced practical Pregel\nalgorithms, we study if these implementations suffer from imbalanced workload\nand large number of iterations. Our findings illustrate that with the exception\nof Euler tour tree algorithm, all other algorithms either perform more work\nthan their best-known sequential approach, or suffer from imbalanced workload/\nlarge number of iterations, or even both. We also emphasize on graph algorithms\nthat are fundamentally difficult to be expressed in vertex-centric frameworks,\nand conclude by discussing the road ahead for distributed graph processing.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 00:53:41 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Khan", "Arijit", ""]]}, {"id": "1612.07448", "submitter": "Lingjiao Chen", "authors": "Lingjiao Chen, Arun Kumar, Jeffrey Naughton, Jignesh M. Patel", "title": "Towards Linear Algebra over Normalized Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing machine learning (ML) over relational data is a mainstream\nrequirement for data analytics systems. While almost all the ML tools require\nthe input data to be presented as a single table, many datasets are\nmulti-table, which forces data scientists to join those tables first, leading\nto data redundancy and runtime waste. Recent works on \"factorized\" ML mitigate\nthis issue for a few specific ML algorithms by pushing ML through joins. But\ntheir approaches require a manual rewrite of ML implementations. Such piecemeal\nmethods create a massive development overhead when extending such ideas to\nother ML algorithms. In this paper, we show that it is possible to mitigate\nthis overhead by leveraging a popular formal algebra to represent the\ncomputations of many ML algorithms: linear algebra. We introduce a new logical\ndata type to represent normalized data and devise a framework of algebraic\nrewrite rules to convert a large set of linear algebra operations over\ndenormalized data into operations over normalized data. We show how this\nenables us to automatically \"factorize\" several popular ML algorithms, thus\nunifying and generalizing several prior works. We prototype our framework in\nthe popular ML environment R and an industrial R-over-RDBMS tool. Experiments\nwith both synthetic and real normalized data show that our framework also\nyields significant speed-ups, up to 36x on real data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 05:41:27 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 04:28:16 GMT"}, {"version": "v3", "created": "Thu, 29 Dec 2016 01:25:30 GMT"}, {"version": "v4", "created": "Sun, 1 Jan 2017 01:25:21 GMT"}, {"version": "v5", "created": "Sat, 15 Apr 2017 04:20:30 GMT"}, {"version": "v6", "created": "Tue, 27 Jun 2017 02:05:09 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Chen", "Lingjiao", ""], ["Kumar", "Arun", ""], ["Naughton", "Jeffrey", ""], ["Patel", "Jignesh M.", ""]]}, {"id": "1612.07514", "submitter": "Gaetan de Rassenfosse", "authors": "Gaetan de Rassenfosse, Martin Kracker, Gianluca Tarasconi", "title": "Getting Started with PATSTAT Register", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a technical introduction to the PATSTAT Register\ndatabase, which contains bibliographical, procedural and legal status data on\npatent applications handled by the European Patent Office. It presents eight\nMySQL queries that cover some of the most relevant aspects of the database for\nresearch purposes. It targets academic researchers and practitioners who are\nfamiliar with the PATSTAT database and the MySQL language.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 10:05:45 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["de Rassenfosse", "Gaetan", ""], ["Kracker", "Martin", ""], ["Tarasconi", "Gianluca", ""]]}, {"id": "1612.08050", "submitter": "Pengfei Xu", "authors": "Jiaheng Lu, Zhen Hua Liu, Pengfei Xu, Chao Zhang", "title": "UDBMS: Road to Unification for Multi-model Data Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A traditional database systems is organized around a single data model that\ndetermines how data can be organized, stored and manipulated. But the vision of\nthis paper is to develop new principles and techniques to manage multiple data\nmodels against a single, integrated backend. For example, semi-structured,\ngraph and relational models are examples of data models that may be supported\nby a new system. Having a single data platform for managing both\nwell-structured data and NoSQL data is beneficial to users; this approach\nsignificantly reduces integration, migration, development, maintenance and\noperational issues. The problem is challenging: the existing database\nprinciples mainly work for a single model and the research on multi-model data\nmanagement is still at an early stage. In this paper, we envision a UDBMS\n(Unified Database Management System) for multi-model data management in one\nplatform. UDBMS will provide several new features such as unified data model\nand flexible schema, unified query processing, unified index structure and\ncross-model transaction guarantees. We discuss our vision as well as present\nmultiple research challenges that we need to address.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 17:47:15 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Lu", "Jiaheng", ""], ["Liu", "Zhen Hua", ""], ["Xu", "Pengfei", ""], ["Zhang", "Chao", ""]]}, {"id": "1612.08543", "submitter": "Amir Hossein Akhavan Rahnama", "authors": "Amir Hossein Akhavan Rahnama", "title": "Distributed Real-Time Sentiment Analysis for Big Data Social Streams", "comments": null, "journal-ref": "IEEE 2014 International Conference on Control, Decision and\n  Information Technologies (CoDIT)", "doi": "10.1109/CoDIT.2014.6996998", "report-no": null, "categories": "stat.ML cs.CL cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data trend has enforced the data-centric systems to have continuous fast\ndata streams. In recent years, real-time analytics on stream data has formed\ninto a new research field, which aims to answer queries about\nwhat-is-happening-now with a negligible delay. The real challenge with\nreal-time stream data processing is that it is impossible to store instances of\ndata, and therefore online analytical algorithms are utilized. To perform\nreal-time analytics, pre-processing of data should be performed in a way that\nonly a short summary of stream is stored in main memory. In addition, due to\nhigh speed of arrival, average processing time for each instance of data should\nbe in such a way that incoming instances are not lost without being captured.\nLastly, the learner needs to provide high analytical accuracy measures.\nSentinel is a distributed system written in Java that aims to solve this\nchallenge by enforcing both the processing and learning process to be done in\ndistributed form. Sentinel is built on top of Apache Storm, a distributed\ncomputing platform. Sentinels learner, Vertical Hoeffding Tree, is a parallel\ndecision tree-learning algorithm based on the VFDT, with ability of enabling\nparallel classification in distributed environments. Sentinel also uses\nSpaceSaving to keep a summary of the data stream and stores its summary in a\nsynopsis data structure. Application of Sentinel on Twitter Public Stream API\nis shown and the results are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:10:18 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Rahnama", "Amir Hossein Akhavan", ""]]}, {"id": "1612.08835", "submitter": "Dinusha Vatsalan", "authors": "Dinusha Vatsalan and Peter Christen", "title": "Multi-Party Privacy-Preserving Record Linkage using Bloom Filters", "comments": "Extended version of the poster paper published in proceedings of ACM\n  Conference in Information and Knowledge Management (CIKM) 2014\n  (http://dl.acm.org/citation.cfm?id=2661875)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-preserving record linkage (PPRL), the problem of identifying records\nthat correspond to the same real-world entity across several data sources held\nby different parties without revealing any sensitive information about these\nrecords, is increasingly being required in many real-world application areas.\nExamples range from public health surveillance to crime and fraud detection,\nand national security. Various techniques have been developed to tackle the\nproblem of PPRL, with the majority of them considering linking data from only\ntwo sources. However, in many real-world applications data from more than two\nsources need to be linked. In this paper we propose a viable solution for\nmulti-party PPRL using two efficient privacy techniques: Bloom filter encoding\nand distributed secure summation. Our proposed protocol efficiently identifies\nmatching sets of records held by all data sources that have a similarity above\na certain minimum threshold. While being efficient, our protocol is also secure\nunder the semi-honest adversary model in that no party can learn any sensitive\ninformation about any other parties' data, but all parties learn which of their\nrecords have a high similarity with records held by the other parties. We\nevaluate our protocol on a large real voter registration database showing the\nscalability, linkage quality, and privacy of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 09:30:00 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Vatsalan", "Dinusha", ""], ["Christen", "Peter", ""]]}, {"id": "1612.08872", "submitter": "Semyon Grigorev", "authors": "Semyon Grigorev, Anastasiya Ragozina", "title": "Context-Free Path Querying with Structural Representation of Result", "comments": "Evaluation extended", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph data model and graph databases are very popular in various areas such\nas bioinformatics, semantic web, and social networks. One specific problem in\nthe area is a path querying with constraints formulated in terms of formal\ngrammars. The query in this approach is written as grammar, and paths querying\nis graph parsing with respect to given grammar. There are several solutions to\nit, but how to provide structural representation of query result which is\npractical for answer processing and debugging is still an open problem. In this\npaper we propose a graph parsing technique which allows one to build such\nrepresentation with respect to given grammar in polynomial time and space for\narbitrary context-free grammar and graph. Proposed algorithm is based on\ngeneralized LL parsing algorithm, while previous solutions are based mostly on\nCYK or Earley algorithms, which reduces time complexity in some cases.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 12:50:49 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 10:53:14 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Grigorev", "Semyon", ""], ["Ragozina", "Anastasiya", ""]]}, {"id": "1612.09155", "submitter": "Xiaoyang Chen", "authors": "Xiaoyang Chen, Hongwei Huo, Jun Huan and Jeffrey Scott Vitter", "title": "MSQ-Index: A Succinct Index for Fast Graph Similarity Search", "comments": "prepare to submit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph similarity search has received considerable attention in many\napplications, such as bioinformatics, data mining, pattern recognition, and\nsocial networks. Existing methods for this problem have limited scalability\nbecause of the huge amount of memory they consume when handling very large\ngraph databases with millions or billions of graphs.\n  In this paper, we study the problem of graph similarity search under the\ngraph edit distance constraint. We present a space-efficient index structure\nbased upon the q-gram tree that incorporates succinct data structures and\nhybrid encoding to achieve improved query time performance with minimal space\nusage. Specifically, the space usage of our index requires only 5%-15% of the\nprevious state-of-the-art indexing size on the tested data while at the same\ntime achieving 2-3 times acceleration in query time with small data sets. We\nalso boost the query performance by augmenting the global filter with range\nsearch, which allows us to perform a query in a reduced region. In addition, we\npropose two effective filters that combine degree structures and label\nstructures. Extensive experiments demonstrate that our proposed approach is\nsuperior in space and competitive in filtering to the state-of-the-art\napproaches. To the best of our knowledge, our index is the first in-memory\nindex for this problem that successfully scales to cope with the large dataset\nof 25 million chemical structure graphs from the PubChem dataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 14:23:46 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Chen", "Xiaoyang", ""], ["Huo", "Hongwei", ""], ["Huan", "Jun", ""], ["Vitter", "Jeffrey Scott", ""]]}, {"id": "1612.09251", "submitter": "Evgenia (Eugenia) Ternovska", "authors": "Eugenia Ternovska", "title": "Lifted Relational Algebra with Recursion and Connections to Modal Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new formalism for specifying and reasoning about problems that\ninvolve heterogeneous \"pieces of information\" -- large collections of data,\ndecision procedures of any kind and complexity and connections between them.\nThe essence of our proposal is to lift Codd's relational algebra from\noperations on relational tables to operations on classes of structures (with\nrecursion), and to add a direction of information propagation. We observe the\npresence of information propagation in several formalisms for efficient\nreasoning and use it to express unary negation and operations used in graph\ndatabases. We carefully analyze several reasoning tasks and establish a precise\nconnection between a generalized query evaluation and temporal logic model\nchecking. Our development allows us to reveal a general correspondence between\nclassical and modal logics and may shed a new light on the good computational\nproperties of modal logics and related formalisms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 19:17:31 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Ternovska", "Eugenia", ""]]}]