[{"id": "1109.0003", "submitter": "Harry  Enke", "authors": "Kristin Riebe, Adrian M. Partl, Harry Enke, Jaime Forero-Romero,\n  Stefan Gottloeber, Anatoly Klypin, Gerard Lemson, Francisco Prada, Joel R.\n  Primack, Matthias Steinmetz, Victor Turchaninov", "title": "The MultiDark Database: Release of the Bolshoi and MultiDark\n  Cosmological Simulations", "comments": "28 pages, 9 figures, submitted to New Astronomy", "journal-ref": null, "doi": "10.1002/asna.201211900", "report-no": null, "categories": "astro-ph.CO astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the online MultiDark Database -- a Virtual Observatory-oriented,\nrelational database for hosting various cosmological simulations. The data is\naccessible via an SQL (Structured Query Language) query interface, which also\nallows users to directly pose scientific questions, as shown in a number of\nexamples in this paper. Further examples for the usage of the database are\ngiven in its extensive online documentation (www.multidark.org). The database\nis based on the same technology as the Millennium Database, a fact that will\ngreatly facilitate the usage of both suites of cosmological simulations. The\nfirst release of the MultiDark Database hosts two 8.6 billion particle\ncosmological N-body simulations: the Bolshoi (250/h Mpc simulation box, 1/h kpc\nresolution) and MultiDark Run1 simulation (MDR1, or BigBolshoi, 1000/h Mpc\nsimulation box, 7/h kpc resolution). The extraction methods for halos/subhalos\nfrom the raw simulation data, and how this data is structured in the database\nare explained in this paper. With the first data release, users get full access\nto halo/subhalo catalogs, various profiles of the halos at redshifts z=0-15,\nand raw dark matter data for one time-step of the Bolshoi and four time-steps\nof the MultiDark simulation. Later releases will also include galaxy mock\ncatalogs and additional merging trees for both simulations as well as new large\nvolume simulations with high resolution. This project is further proof of the\nviability to store and present complex data using relational database\ntechnology. We encourage other simulators to publish their results in a similar\nmanner.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 20:00:00 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2011 09:29:57 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Riebe", "Kristin", ""], ["Partl", "Adrian M.", ""], ["Enke", "Harry", ""], ["Forero-Romero", "Jaime", ""], ["Gottloeber", "Stefan", ""], ["Klypin", "Anatoly", ""], ["Lemson", "Gerard", ""], ["Prada", "Francisco", ""], ["Primack", "Joel R.", ""], ["Steinmetz", "Matthias", ""], ["Turchaninov", "Victor", ""]]}, {"id": "1109.0086", "submitter": "Qiang Zeng", "authors": "Qiang Zeng and Hai Zhuge", "title": "Comments on \"Stack-based Algorithms for Pattern Matching on DAGs\"", "comments": "This paper has been withdrawn by the author, since this is an\n  obsolete version. Please refer to the version published in PVLDB Volume 5\n  Issue 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The paper \"Stack-based Algorithms for Pattern Matching on DAGs\" generalizes\nthe classical holistic twig join algorithms and proposes PathStackD, TwigStackD\nand DagStackD to respectively evaluate path, twig and DAG pattern queries on\ndirected acyclic graphs. In this paper, we investigate the major results of\nthat paper, pointing out several discrepancies and proposing solutions to\nresolving them. We show that the original algorithms do not find particular\ntypes of query solutions that are common in practice. We also analyze the\neffect of an underlying assumption on the correctness of the algorithms and\ndiscuss the pre-filtering process that the original work proposes to prune\nredundant nodes. Our experimental study on both real and synthetic data\nsubstantiates our conclusions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 04:14:34 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2011 11:24:33 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2012 02:36:00 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Zeng", "Qiang", ""], ["Zhuge", "Hai", ""]]}, {"id": "1109.0181", "submitter": "J\\\"urgen Umbrich", "authors": "J\\\"urgen Umbrich, Aidan Hogan and Axel Polleres", "title": "Improving the recall of decentralised linked data querying through\n  implicit knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aside from crawling, indexing, and querying RDF data centrally, Linked Data\nprinciples allow for processing SPARQL queries on-the-fly by dereferencing\nURIs. Proposed link-traversal query approaches for Linked Data have the\nbenefits of up-to-date results and decentralised (i.e., client-side) execution,\nbut operate on incomplete knowledge available in dereferenced documents, thus\naffecting recall. In this paper, we investigate how implicit knowledge -\nspecifically that found through owl:sameAs and RDFS reasoning - can improve the\nrecall in this setting. We start with an empirical analysis of a large crawl\nfeaturing 4 m Linked Data sources and 1.1 g quadruples: we (1) measure expected\nrecall by only considering dereferenceable information, (2) measure the\nimprovement in recall given by considering rdfs:seeAlso links as previous\nproposals did. We further propose and measure the impact of additionally\nconsidering (3) owl:sameAs links, and (4) applying lightweight RDFS reasoning\n(specifically {\\rho}DF) for finding more results, relying on static schema\ninformation. We evaluate our methods for live queries over our crawl.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 13:14:13 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Umbrich", "J\u00fcrgen", ""], ["Hogan", "Aidan", ""], ["Polleres", "Axel", ""]]}, {"id": "1109.0617", "submitter": "Komalavalli Chakravarthi", "authors": "C.Komalavalli (Jagan Institute of Management Studies, Rohini, New\n  Delhi) Chetna Laroiya (Jagan Insitute of Management Studies, Rohini, New\n  Delhi)", "title": "Metadata Challenge for Query Processing Over Heterogeneous Wireless\n  Sensor Network", "comments": "15 Pages", "journal-ref": "International Journal of Wireless & Mobile Networks (IJWMN) Vol.\n  3, No. 4, August 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless sensor networks become integral part of our life. These networks can\nbe used for monitoring the data in various domain due to their flexibility and\nfunctionality. Query processing and optimization in the WSN is a very\nchallenging task because of their energy and memory constraint. In this paper,\nfirst our focus is to review the different approaches that have significant\nimpacts on the development of query processing techniques for WSN. Finally, we\naim to illustrate the existing approach in popular query processing engines\nwith future research challenges in query optimization.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2011 12:04:00 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Komalavalli", "C.", "", "Jagan Institute of Management Studies, Rohini, New\n  Delhi"], ["Laroiya", "Chetna", "", "Jagan Insitute of Management Studies, Rohini, New\n  Delhi"]]}, {"id": "1109.0736", "submitter": "Hideaki Kimura", "authors": "Hideaki Kimura (Brown University), Vivek Narasayya (Microsoft\n  Research), Manoj Syamala (Microsoft Research)", "title": "Compression Aware Physical Database Design", "comments": "VLDB2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern RDBMSs support the ability to compress data using methods such as null\nsuppression and dictionary encoding. Data compression offers the promise of\nsignificantly reducing storage requirements and improving I/O performance for\ndecision support queries. However, compression can also slow down update and\nquery performance due to the CPU costs of compression and decompression. In\nthis paper, we study how data compression affects choice of appropriate\nphysical database design, such as indexes, for a given workload. We observe\nthat approaches that decouple the decision of whether or not to choose an index\nfrom whether or not to compress the index can result in poor solutions. Thus,\nwe focus on the novel problem of integrating compression into physical database\ndesign in a scalable manner. We have implemented our techniques by modifying\nMicrosoft SQL Server and the Database Engine Tuning Advisor (DTA) physical\ndesign tool. Our techniques are general and are potentially applicable to DBMSs\nthat support other compression methods. Our experimental results on real world\nas well as TPC-H benchmark workloads demonstrate the effectiveness of our\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2011 18:11:43 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Kimura", "Hideaki", "", "Brown University"], ["Narasayya", "Vivek", "", "Microsoft\n  Research"], ["Syamala", "Manoj", "", "Microsoft Research"]]}, {"id": "1109.1087", "submitter": "Martin Aruldoss Mr", "authors": "A.Martin, M.Manjula, Dr.V.Prasanna Venkatesan", "title": "A Business Intelligence Model to Predict Bankruptcy using Financial\n  Domain Ontology with Association Rule Mining Algorithm", "comments": "Bankruptcy, Financial domain Ontology, Data Mining, Z-Score Model,\n  Business Intelligence, Altman Bankruptcy model", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 3, No. 2, May 2011 ISSN (Online): 1694-0814", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today in every organization financial analysis provides the basis for\nunderstanding and evaluating the results of business operations and delivering\nhow well a business is doing. This means that the organizations can control the\noperational activities primarily related to corporate finance. One way that\ndoing this is by analysis of bankruptcy prediction. This paper develops an\nontological model from financial information of an organization by analyzing\nthe Semantics of the financial statement of a business. One of the best\nbankruptcy prediction models is Altman Z-score model. Altman Z-score method\nuses financial rations to predict bankruptcy. From the financial ontological\nmodel the relation between financial data is discovered by using data mining\nalgorithm. By combining financial domain ontological model with association\nrule mining algorithm and Zscore model a new business intelligence model is\ndeveloped to predict the bankruptcy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 07:02:35 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Martin", "A.", ""], ["Manjula", "M.", ""], ["Venkatesan", "Dr. V. Prasanna", ""]]}, {"id": "1109.1144", "submitter": "Venkatesan Sundaram", "authors": "M. Venkatesan, Arunkumar Thangavelu, P. Prabhavathy", "title": "Event Centric Modeling Approach in Colocation Pattern Snalysis from\n  Spatial Data", "comments": "9 pages", "journal-ref": null, "doi": "10.5121/ijdms.2011.3311", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial co-location patterns are the subsets of Boolean spatial features\nwhose instances are often located in close geographic proximity. Co-location\nrules can be identified by spatial statistics or data mining approaches. In\ndata mining method, Association rule-based approaches can be used which are\nfurther divided into transaction-based approaches and distance-based\napproaches. Transaction-based approaches focus on defining transactions over\nspace so that an Apriori algorithm can be used. The natural notion of\ntransactions is absent in spatial data sets which are embedded in continuous\ngeographic space. A new distance -based approach is developed to mine\nco-location patterns from spatial data by using the concept of proximity\nneighborhood. A new interest measure, a participation index, is used for\nspatial co-location patterns as it possesses an anti-monotone property. An\nalgorithm to discover co-location patterns are designed which generates\ncandidate locations and their table instances. Finally the co-location rules\nare generated to identify the patterns.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 11:12:42 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Venkatesan", "M.", ""], ["Thangavelu", "Arunkumar", ""], ["Prabhavathy", "P.", ""]]}, {"id": "1109.1168", "submitter": "Arezoo Rajaei", "authors": "Arezoo Rajaei, Ahmad Baraani Dastjerdi and Nasser Ghasem Aghaee", "title": "An Extension of Semantic Proximity for Fuzzy Multivalued Dependencies in\n  Fuzzy Relational Database", "comments": "13 pages, 2 tables, Journal", "journal-ref": "International Journal of Database Management Systems (IJDMS),\n  Vol.3, No.3, August 2011, 157-169", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the development of fuzzy logic theory by Lotfi Zadeh, its\napplications were investigated by researchers in different fields. Presenting\nand working with uncertain data is a complex problem. To solve for such a\ncomplex problem, the structure of relationships and operators dependent on such\nrelationships must be repaired. The fuzzy database has integrity limitations\nincluding data dependencies. In this paper, first fuzzy multivalued dependency\nbased semantic proximity and its problems are studied. To solve these problems,\nthe semantic proximity's formula is modified, and fuzzy multivalued dependency\nbased on the concept of extension of semantic proximity with \\alpha degree is\ndefined in fuzzy relational database which includes Crisp, NULL and fuzzy\nvalues, and also inference rules for this dependency are defined, and their\ncompleteness is proved. Finally, we will show that fuzzy functional dependency\nbased on this concept is a special case of fuzzy multivalued dependency in\nfuzzy relational database.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 13:04:41 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Rajaei", "Arezoo", ""], ["Dastjerdi", "Ahmad Baraani", ""], ["Aghaee", "Nasser Ghasem", ""]]}, {"id": "1109.1202", "submitter": "Abhijit Raorane Aravind", "authors": "Abhijit Raorane and R.V.Kulkarni", "title": "Data Mining Techniques: A Source for Consumer Behavior Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various studies on consumer purchasing behaviors have been presented and used\nin real problems. Data mining techniques are expected to be a more effective\ntool for analyzing consumer behaviors. However, the data mining method has\ndisadvantages as well as advantages. Therefore, it is important to select\nappropriate techniques to mine databases. The objective of this paper is to\nknow consumer behavior, his psychological condition at the time of purchase and\nhow suitable data mining method apply to improve conventional method. Moreover,\nin an experiment, association rule is employed to mine rules for trusted\ncustomers using sales data in a super market industry\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 14:32:05 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Raorane", "Abhijit", ""], ["Kulkarni", "R. V.", ""]]}, {"id": "1109.1302", "submitter": "Hakik Paci", "authors": "Hakik Paci, Elinda Kajo, Igli Tafa and Aleksander Xhuvani", "title": "Adding a new site in an existing Oracle Multimaster replication without\n  quiescing the replication", "comments": "9 pages, 4 figures, in International Journal of Database Management\n  Systems (IJDMS) (2011)", "journal-ref": "International Journal of Database Management Systems (IJDMS),\n  Vol.3, No.3, (2011) 58-67", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new solution, which adds a new database server on an\nexisting Oracle Multimaster Data replication system with Online Instantiation\nmethod. During this time the system is down, because we cannot execute DML\nstatements on replication objects but we can only make queries. The time for\nadding the new database server depends on the number of objects, on the\nreplication group and on the network conditions. We propose to add a new layer\nbetween replication objects and the database sessions, which contain DML\nstatements. The layer eliminates the system down time exploiting our developed\npackages. The packages will be active only during the addition of a new site\nprocess and will modify all DML statements and queries based on replication\nobjects.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 21:12:33 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Paci", "Hakik", ""], ["Kajo", "Elinda", ""], ["Tafa", "Igli", ""], ["Xhuvani", "Aleksander", ""]]}, {"id": "1109.1325", "submitter": "Edith Cohen", "authors": "Edith Cohen and Haim Kaplan", "title": "Get the Most out of Your Sample: Optimal Unbiased Estimators using\n  Partial Information", "comments": "This is a full version of a PODS 2011 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.NI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sampling is an essential tool in the processing and transmission of\ndata. It is used to summarize data too large to store or manipulate and meet\nresource constraints on bandwidth or battery power. Estimators that are applied\nto the sample facilitate fast approximate processing of queries posed over the\noriginal data and the value of the sample hinges on the quality of these\nestimators.\n  Our work targets data sets such as request and traffic logs and sensor\nmeasurements, where data is repeatedly collected over multiple {\\em instances}:\ntime periods, locations, or snapshots.\n  We are interested in queries that span multiple instances, such as distinct\ncounts and distance measures over selected records. These queries are used for\napplications ranging from planning to anomaly and change detection.\n  Unbiased low-variance estimators are particularly effective as the relative\nerror decreases with the number of selected record keys.\n  The Horvitz-Thompson estimator, known to minimize variance for sampling with\n\"all or nothing\" outcomes (which reveals exacts value or no information on\nestimated quantity), is not optimal for multi-instance operations for which an\noutcome may provide partial information.\n  We present a general principled methodology for the derivation of (Pareto)\noptimal unbiased estimators over sampled instances and aim to understand its\npotential. We demonstrate significant improvement in estimate accuracy of\nfundamental queries for common sampling schemes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 23:42:06 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Cohen", "Edith", ""], ["Kaplan", "Haim", ""]]}, {"id": "1109.1359", "submitter": "Agus Pratondo", "authors": "Agus Pratondo", "title": "Representation for alphanumeric data type based on space and speed case\n  study: Student ID of X university", "comments": null, "journal-ref": null, "doi": "10.5121/ijdms.2011.3303", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  ID is derived from the word identity, derived from the first two characters\nin the word. ID is used to distinguish between an entity to another entity.\nStudent ID (SID) is the key differentiator between a student with other\nstudents. On the concept of database, the differentiator is unique. SID can be\nnumbers, letters, or a combination of both (alphanumeric). Viewed from the\ndaily context, it is not important to determine which a SID belongs to the type\nof data. However, when reviewed on database design, determining the type of\ndata, including SID in this case, is important. Problems arise because there is\na contradiction between the data type viewed from the data characteristic and\npractical needs. Type of data for SID is a string, if it is evaluated from the\nbasic concepts and its characteristic. It is acceptable because SID consists of\na set of numbers which will not be meaningful if applied arithmetic operations\nlike addition, subtraction, multiplication and division. But in terms of\ncomputer organization, data representation type will determine how much data\nspace requirements, speed of access, and speed of operation. By considering the\nconstraints of space and speed on the experiments conducted, SID is better\nexpressed as an integer rather than a set of characters.\n  KEYWORDS aphanumeric,representation, string, integer, space, speed\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2011 05:57:20 GMT"}], "update_date": "2011-09-08", "authors_parsed": [["Pratondo", "Agus", ""]]}, {"id": "1109.2425", "submitter": "Anastasia Analyti", "authors": "Carlo Meghini, Yannis Tzitzikas, Veronica Coltella, Anastasia Analyti", "title": "Query processing in distributed, taxonomy-based information sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of answering queries over a distributed information\nsystem, storing objects indexed by terms organized in a taxonomy. The taxonomy\nconsists of subsumption relationships between negation-free DNF formulas on\nterms and negation-free conjunctions of terms. In the first part of the paper,\nwe consider the centralized case, deriving a hypergraph-based algorithm that is\nefficient in data complexity. In the second part of the paper, we consider the\ndistributed case, presenting alternative ways implementing the centralized\nalgorithm. These ways descend from two basic criteria: direct vs. query\nre-writing evaluation, and centralized vs. distributed data or taxonomy\nallocation. Combinations of these criteria allow to cover a wide spectrum of\narchitectures, ranging from client-server to peer-to-peer. We evaluate the\nperformance of the various architectures by simulation on a network with\nO(10^4) nodes, and derive final results. An extensive review of the relevant\nliterature is finally included.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 10:20:31 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Meghini", "Carlo", ""], ["Tzitzikas", "Yannis", ""], ["Coltella", "Veronica", ""], ["Analyti", "Anastasia", ""]]}, {"id": "1109.2427", "submitter": "Rajalakshmi Nedunchezhian", "authors": "M.Rajalakshmi, Dr.T.Purusothaman, Dr.R.Nedunchezhian", "title": "Maximal frequent itemset generation using segmentation approach", "comments": "14 pages", "journal-ref": null, "doi": "10.5121/ijdms.2011.3302", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding frequent itemsets in a data source is a fundamental operation behind\nAssociation Rule Mining. Generally, many algorithms use either the bottom-up or\ntop-down approaches for finding these frequent itemsets. When the length of\nfrequent itemsets to be found is large, the traditional algorithms find all the\nfrequent itemsets from 1-length to n-length, which is a difficult process. This\nproblem can be solved by mining only the Maximal Frequent Itemsets (MFS).\nMaximal Frequent Itemsets are frequent itemsets which have no proper frequent\nsuperset. Thus, the generation of only maximal frequent itemsets reduces the\nnumber of itemsets and also time needed for the generation of all frequent\nitemsets as each maximal itemset of length m implies the presence of 2m-2\nfrequent itemsets. Furthermore, mining only maximal frequent itemset is\nsufficient in many data mining applications like minimal key discovery and\ntheory extraction. In this paper, we suggest a novel method for finding the\nmaximal frequent itemset from huge data sources using the concept of\nsegmentation of data source and prioritization of segments. Empirical\nevaluation shows that this method outperforms various other known methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 10:37:53 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Rajalakshmi", "M.", ""], ["Purusothaman", "Dr. T.", ""], ["Nedunchezhian", "Dr. R.", ""]]}, {"id": "1109.3119", "submitter": "Peter van Gemmeren", "authors": "Peter van Gemmeren, David Malon", "title": "Persistent Data Layout and Infrastructure for Efficient Selective\n  Retrieval of Event Data in ATLAS", "comments": "Proceedings of the DPF-2011 Conference, Providence, RI, August 8-13,\n  2011 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CE cs.DB hep-ex", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The ATLAS detector at CERN has completed its first full year of recording\ncollisions at 7 TeV, resulting in billions of events and petabytes of data. At\nthese scales, physicists must have the capability to read only the data of\ninterest to their analyses, with the importance of efficient selective access\nincreasing as data taking continues. ATLAS has developed a sophisticated\nevent-level metadata infrastructure and supporting I/O framework allowing event\nselections by explicit specification, by back navigation, and by selection\nqueries to a TAG database via an integrated web interface. These systems and\ntheir performance have been reported on elsewhere. The ultimate success of such\na system, however, depends significantly upon the efficiency of selective event\nretrieval. Supporting such retrieval can be challenging, as ATLAS stores its\nevent data in column-wise orientation using ROOT trees for a number of reasons,\nincluding compression considerations, histogramming use cases, and more. For\n2011 data, ATLAS will utilize new capabilities in ROOT to tune the persistent\nstorage layout of event data, and to significantly speed up selective event\nreading. The new persistent layout strategy and its implications for I/O\nperformance are described in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2011 16:04:31 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["van Gemmeren", "Peter", ""], ["Malon", "David", ""]]}, {"id": "1109.3555", "submitter": "Francesco Pagano", "authors": "Francesco Pagano and Davide Pagano", "title": "Using In-Memory Encrypted Databases on the Cloud", "comments": "8 pages, 8 figures", "journal-ref": "2011 1st International Workshop on Securing Ser vices on the Cloud\n  IWSSC 2011", "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Storing data in the cloud poses a number of privacy issues. A way to handle\nthem is supporting data replication and distribution on the cloud via a local,\ncentrally synchronized storage. In this paper we propose to use an in-memory\nRDBMS with row-level data encryption for granting and revoking access rights to\ndistributed data. This type of solution is rarely adopted in conventional\nRDBMSs because it requires several complex steps. In this paper we focus on\nimplementation and benchmarking of a test system, which shows that our simple\nyet effective solution overcomes most of the problems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2011 09:20:23 GMT"}], "update_date": "2011-09-19", "authors_parsed": [["Pagano", "Francesco", ""], ["Pagano", "Davide", ""]]}, {"id": "1109.4104", "submitter": "Marco Castellani", "authors": "Marco Castellani, Massimo Brescia, Ettore Mancini, Luca Pellecchia,\n  Giuseppe Longo", "title": "VOGCLUSTERS: an example of DAME web application", "comments": "4 pages, 1 figure. Proceedings of \"Advances in Computational\n  Astrophysics: methods, tools and outcomes\" (Cefal\\`u, Sicily, June 2011). To\n  be published on ASP Conference Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the alpha release of the VOGCLUSTERS web application, specialized\nfor data and text mining on globular clusters. It is one of the web2.0\ntechnology based services of Data Mining & Exploration (DAME) Program, devoted\nto mine and explore heterogeneous information related to globular clusters\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 17:33:39 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2011 10:57:57 GMT"}], "update_date": "2011-09-23", "authors_parsed": [["Castellani", "Marco", ""], ["Brescia", "Massimo", ""], ["Mancini", "Ettore", ""], ["Pellecchia", "Luca", ""], ["Longo", "Giuseppe", ""]]}, {"id": "1109.4288", "submitter": "Qiang Zeng", "authors": "Qiang Zeng and Xiaorui Jiang and Hai Zhuge", "title": "Adding Logical Operators to Tree Pattern Queries on Graph-Structured\n  Data", "comments": "16 pages", "journal-ref": "PVLDB 5(8):728-739, 2012", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  As data are increasingly modeled as graphs for expressing complex\nrelationships, the tree pattern query on graph-structured data becomes an\nimportant type of queries in real-world applications. Most practical query\nlanguages, such as XQuery and SPARQL, support logical expressions using\nlogical-AND/OR/NOT operators to define structural constraints of tree patterns.\nIn this paper, (1) we propose generalized tree pattern queries (GTPQs) over\ngraph-structured data, which fully support propositional logic of structural\nconstraints. (2) We make a thorough study of fundamental problems including\nsatisfiability, containment and minimization, and analyze the computational\ncomplexity and the decision procedures of these problems. (3) We propose a\ncompact graph representation of intermediate results and a pruning approach to\nreduce the size of intermediate results and the number of join operations --\ntwo factors that often impair the efficiency of traditional algorithms for\nevaluating tree pattern queries. (4) We present an efficient algorithm for\nevaluating GTPQs using 3-hop as the underlying reachability index. (5)\nExperiments on both real-life and synthetic data sets demonstrate the\neffectiveness and efficiency of our algorithm, from several times to orders of\nmagnitude faster than state-of-the-art algorithms in terms of evaluation time,\neven for traditional tree pattern queries with only conjunctive operations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2011 13:30:32 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2012 16:22:22 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2012 02:03:20 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2012 07:41:16 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zeng", "Qiang", ""], ["Jiang", "Xiaorui", ""], ["Zhuge", "Hai", ""]]}, {"id": "1109.6299", "submitter": "Vilem Vychodil", "authors": "Radim Belohlavek, Lucie Urbanova, Vilem Vychodil", "title": "Sensitivity Analysis for Declarative Relational Query Languages with\n  Ordinal Ranks", "comments": "The paper will appear in Proceedings of the 19th International\n  Conference on Applications of Declarative Programming and Knowledge\n  Management (INAP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present sensitivity analysis for results of query executions in a\nrelational model of data extended by ordinal ranks. The underlying model of\ndata results from the ordinary Codd's model of data in which we consider\nordinal ranks of tuples in data tables expressing degrees to which tuples match\nqueries. In this setting, we show that ranks assigned to tuples are insensitive\nto small changes, i.e., small changes in the input data do not yield large\nchanges in the results of queries.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 19:04:18 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Belohlavek", "Radim", ""], ["Urbanova", "Lucie", ""], ["Vychodil", "Vilem", ""]]}, {"id": "1109.6880", "submitter": "Daniel Fabbri", "authors": "Daniel Fabbri, Kristen LeFevre", "title": "Explanation-Based Auditing", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp. 1-12\n  (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To comply with emerging privacy laws and regulations, it has become common\nfor applications like electronic health records systems (EHRs) to collect\naccess logs, which record each time a user (e.g., a hospital employee) accesses\na piece of sensitive data (e.g., a patient record). Using the access log, it is\neasy to answer simple queries (e.g., Who accessed Alice's medical record?), but\nthis often does not provide enough information. In addition to learning who\naccessed their medical records, patients will likely want to understand why\neach access occurred. In this paper, we introduce the problem of generating\nexplanations for individual records in an access log. The problem is motivated\nby user-centric auditing applications, and it also provides a novel approach to\nmisuse detection. We develop a framework for modeling explanations which is\nbased on a fundamental observation: For certain classes of databases, including\nEHRs, the reason for most data accesses can be inferred from data stored\nelsewhere in the database. For example, if Alice has an appointment with Dr.\nDave, this information is stored in the database, and it explains why Dr. Dave\nlooked at Alice's record. Large numbers of data accesses can be explained using\ngeneral forms called explanation templates. Rather than requiring an\nadministrator to manually specify explanation templates, we propose a set of\nalgorithms for automatically discovering frequent templates from the database\n(i.e., those that explain a large number of accesses). We also propose\ntechniques for inferring collaborative user groups, which can be used to\nenhance the quality of the discovered explanations. Finally, we have evaluated\nour proposed techniques using an access log and data from the University of\nMichigan Health System. Our results demonstrate that in practice we can provide\nexplanations for over 94% of data accesses in the log.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 16:24:41 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Fabbri", "Daniel", ""], ["LeFevre", "Kristen", ""]]}, {"id": "1109.6881", "submitter": "Adam Marcus", "authors": "Adam Marcus, Eugene Wu, David Karger, Samuel Madden, Robert Miller", "title": "Human-powered Sorts and Joins", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.\n  13-24 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible\nto task people with small jobs, such as labeling images or looking up phone\nnumbers, via a programmatic interface. MTurk tasks for processing datasets with\nhumans are currently designed with significant reimplementation of common\nworkflows and ad-hoc selection of parameters such as price to pay per task. We\ndescribe how we have integrated crowds into a declarative workflow engine\ncalled Qurk to reduce the burden on workflow designers. In this paper, we focus\non how to use humans to compare items for sorting and joining data, two of the\nmost common operations in DBMSs. We describe our basic query interface and the\nuser interface of the tasks we post to MTurk. We also propose a number of\noptimizations, including task batching, replacing pairwise comparisons with\nnumerical ratings, and pre-filtering tables before joining them, which\ndramatically reduce the overall cost of running sorts and joins on the crowd.\nIn an experiment joining two sets of images, we reduce the overall cost from\n$67 in a naive implementation to about $3, without substantially affecting\naccuracy or latency. In an end-to-end experiment, we reduced cost by a factor\nof 14.5.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 16:24:47 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Marcus", "Adam", ""], ["Wu", "Eugene", ""], ["Karger", "David", ""], ["Madden", "Samuel", ""], ["Miller", "Robert", ""]]}, {"id": "1109.6882", "submitter": "Graham Cormode", "authors": "Graham Cormode, Justin Thaler, Ke Yi", "title": "Verifying Computations with Streaming Interactive Proofs", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.\n  25-36 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When computation is outsourced, the data owner would like to be assured that\nthe desired computation has been performed correctly by the service provider.\nIn theory, proof systems can give the necessary assurance, but prior work is\nnot sufficiently scalable or practical. In this paper, we develop new proof\nprotocols for verifying computations which are streaming in nature: the\nverifier (data owner) needs only logarithmic space and a single pass over the\ninput, and after observing the input follows a simple protocol with a prover\n(service provider) that takes logarithmic communication spread over a\nlogarithmic number of rounds. These ensure that the computation is performed\ncorrectly: that the service provider has not made any errors or missed out some\ndata. The guarantee is very strong: even if the service provider deliberately\ntries to cheat, there is only vanishingly small probability of doing so\nundetected, while a correct computation is always accepted. We first observe\nthat some theoretical results can be modified to work with streaming verifiers,\nshowing that there are efficient protocols for problems in the complexity\nclasses NP and NC. Our main results then seek to bridge the gap between theory\nand practice by developing usable protocols for a variety of problems of\ncentral importance in streaming and database processing. All these problems\nrequire linear space in the traditional streaming model, and therefore our\nprotocols demonstrate that adding a prover can exponentially reduce the effort\nneeded by the verifier. Our experimental results show that our protocols are\npractical and scalable.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 16:24:53 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Cormode", "Graham", ""], ["Thaler", "Justin", ""], ["Yi", "Ke", ""]]}, {"id": "1109.6883", "submitter": "Dan Lin", "authors": "Dan Lin, Christian S. Jensen, Rui Zhang, Lu Xiao, Jiaheng Lu", "title": "A MovingObject Index for Efficient Query Processing with Peer-Wise\n  Location Privacy", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.\n  37-48 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing use of location-based services, location privacy attracts\nincreasing attention from users, industry, and the research community. While\nconsiderable effort has been devoted to inventing techniques that prevent\nservice providers from knowing a user's exact location, relatively little\nattention has been paid to enabling so-called peer-wise privacy--the protection\nof a user's location from unauthorized peer users. This paper identifies an\nimportant efficiency problem in existing peer-privacy approaches that simply\napply a filtering step to identify users that are located in a query range, but\nthat do not want to disclose their location to the querying peer. To solve this\nproblem, we propose a novel, privacy-policy enabled index called the PEB-tree\nthat seamlessly integrates location proximity and policy compatibility. We\npropose efficient algorithms that use the PEB-tree for processing privacy-aware\nrange and kNN queries. Extensive experiments suggest that the PEB-tree enables\nefficient query processing.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 16:24:58 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Lin", "Dan", ""], ["Jensen", "Christian S.", ""], ["Zhang", "Rui", ""], ["Xiao", "Lu", ""], ["Lu", "Jiaheng", ""]]}, {"id": "1109.6884", "submitter": "Essam Mansour", "authors": "Essam Mansour, Amin Allam, Spiros Skiadopoulos, Panos Kalnis", "title": "ERA: Efficient Serial and Parallel Suffix Tree Construction for Very\n  Long Strings", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.\n  49-60 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suffix tree is a data structure for indexing strings. It is used in a\nvariety of applications such as bioinformatics, time series analysis,\nclustering, text editing and data compression. However, when the string and the\nresulting suffix tree are too large to fit into the main memory, most existing\nconstruction algorithms become very inefficient. This paper presents a\ndisk-based suffix tree construction method, called Elastic Range (ERa), which\nworks efficiently with very long strings that are much larger than the\navailable memory. ERa partitions the tree construction process horizontally and\nvertically and minimizes I/Os by dynamically adjusting the horizontal\npartitions independently for each vertical partition, based on the evolving\nshape of the tree and the available memory. Where appropriate, ERa also groups\nvertical partitions together to amortize the I/O cost. We developed a serial\nversion; a parallel version for shared-memory and shared-disk multi-core\nsystems; and a parallel version for shared-nothing architectures. ERa indexes\nthe entire human genome in 19 minutes on an ordinary desktop computer. For\ncomparison, the fastest existing method needs 15 minutes using 1024 CPUs on an\nIBM BlueGene supercomputer.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 16:25:02 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Mansour", "Essam", ""], ["Allam", "Amin", ""], ["Skiadopoulos", "Spiros", ""], ["Kalnis", "Panos", ""]]}, {"id": "1109.6885", "submitter": "Martin Grund", "authors": "Jens Krueger, Changkyu Kim, Martin Grund, Nadathur Satish, David\n  Schwalb, Jatin Chhugani, Hasso Plattner, Pradeep Dubey, Alexander Zeier", "title": "Fast Updates on Read-Optimized Databases Using Multi-Core CPUs", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.\n  61-72 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Read-optimized columnar databases use differential updates to handle writes\nby maintaining a separate write-optimized delta partition which is periodically\nmerged with the read-optimized and compressed main partition. This merge\nprocess introduces significant overheads and unacceptable downtimes in update\nintensive systems, aspiring to combine transactional and analytical workloads\ninto one system. In the first part of the paper, we report data analyses of 12\nSAP Business Suite customer systems. In the second half, we present an\noptimized merge process reducing the merge overhead of current systems by a\nfactor of 30. Our linear-time merge algorithm exploits the underlying high\ncompute and bandwidth resources of modern multi-core CPUs with\narchitecture-aware optimizations and efficient parallelization. This enables\ncompressed in-memory column stores to handle the transactional update rate\nrequired by enterprise applications, while keeping properties of read-optimized\ndatabases for analytic-style queries.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 16:25:08 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Krueger", "Jens", ""], ["Kim", "Changkyu", ""], ["Grund", "Martin", ""], ["Satish", "Nadathur", ""], ["Schwalb", "David", ""], ["Chhugani", "Jatin", ""], ["Plattner", "Hasso", ""], ["Dubey", "Pradeep", ""], ["Zeier", "Alexander", ""]]}, {"id": "1109.6886", "submitter": "Amit Goyal", "authors": "Amit Goyal, Francesco Bonchi, Laks V. S. Lakshmanan", "title": "A Data-Based Approach to Social Influence Maximization", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.\n  73-84 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence maximization is the problem of finding a set of users in a social\nnetwork, such that by targeting this set, one maximizes the expected spread of\ninfluence in the network. Most of the literature on this topic has focused\nexclusively on the social graph, overlooking historical data, i.e., traces of\npast action propagations. In this paper, we study influence maximization from a\nnovel data-based perspective. In particular, we introduce a new model, which we\ncall credit distribution, that directly leverages available propagation traces\nto learn how influence flows in the network and uses this to estimate expected\ninfluence spread. Our approach also learns the different levels of\ninfluenceability of users, and it is time-aware in the sense that it takes the\ntemporal nature of influence into account. We show that influence maximization\nunder the credit distribution model is NP-hard and that the function that\ndefines expected spread under our model is submodular. Based on these, we\ndevelop an approximation algorithm for solving the influence maximization\nproblem that at once enjoys high accuracy compared to the standard approach,\nwhile being several orders of magnitude faster and more scalable.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 16:25:13 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Goyal", "Amit", ""], ["Bonchi", "Francesco", ""], ["Lakshmanan", "Laks V. S.", ""]]}]