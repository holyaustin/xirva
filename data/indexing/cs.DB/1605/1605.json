[{"id": "1605.00060", "submitter": "Puneet Agarwal", "authors": "Puneet Agarwal and Maya Ramanath and Gautam Shroff", "title": "Relationship Queries on Large graphs using Pregel", "comments": "19 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale graph-structured data arising from social networks, databases,\nknowledge bases, web graphs, etc. is now available for analysis and mining.\nGraph-mining often involves 'relationship queries', which seek a ranked list of\ninteresting interconnections among a given set of entities, corresponding to\nnodes in the graph. While relationship queries have been studied for many\nyears, using various terminologies, e.g., keyword-search, Steiner-tree in a\ngraph etc., the solutions proposed in the literature so far have not focused on\nscaling relationship queries to large graphs having billions of nodes and\nedges, such are now publicly available in the form of 'linked-open-data'. In\nthis paper, we present an algorithm for distributed keyword search (DKS) on\nlarge graphs, based on the graph-parallel computing paradigm Pregel. We also\npresent an analytical proof that our algorithm produces an optimally ranked\nlist of answers if run to completion. Even if terminated early, our algorithm\nproduces approximate answers along with bounds. We describe an optimized\nimplementation of our DKS algorithm along with time-complexity analysis.\nFinally, we report and analyze experiments using an implementation of DKS on\nGiraph the graph-parallel computing framework based on Pregel, and demonstrate\nthat we can efficiently process relationship queries on large-scale subsets of\nlinked-open-data.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 03:15:08 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Agarwal", "Puneet", ""], ["Ramanath", "Maya", ""], ["Shroff", "Gautam", ""]]}, {"id": "1605.00677", "submitter": "Shantanu Sharma", "authors": "Philip Derbeko, Shlomi Dolev, Ehud Gudes, Shantanu Sharma", "title": "Security and Privacy Aspects in MapReduce on Clouds: A Survey", "comments": "Accepted in Elsevier Computer Science Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce is a programming system for distributed processing large-scale data\nin an efficient and fault tolerant manner on a private, public, or hybrid\ncloud. MapReduce is extensively used daily around the world as an efficient\ndistributed computation tool for a large class of problems, e.g., search,\nclustering, log analysis, different types of join operations, matrix\nmultiplication, pattern matching, and analysis of social networks. Security and\nprivacy of data and MapReduce computations are essential concerns when a\nMapReduce computation is executed in public or hybrid clouds. In order to\nexecute a MapReduce job in public and hybrid clouds, authentication of\nmappers-reducers, confidentiality of data-computations, integrity of\ndata-computations, and correctness-freshness of the outputs are required.\nSatisfying these requirements shield the operation from several types of\nattacks on data and MapReduce computations. In this paper, we investigate and\ndiscuss security and privacy challenges and requirements, considering a variety\nof adversarial capabilities, and characteristics in the scope of MapReduce. We\nalso provide a review of existing security and privacy protocols for MapReduce\nand discuss their overhead issues.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 20:31:52 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Derbeko", "Philip", ""], ["Dolev", "Shlomi", ""], ["Gudes", "Ehud", ""], ["Sharma", "Shantanu", ""]]}, {"id": "1605.00686", "submitter": "Mayank Kejriwal", "authors": "Mayank Kejriwal", "title": "Adaptive Candidate Generation for Scalable Edge-discovery Tasks on Data\n  Graphs", "comments": "8 pages,published at MLG workshop at KDD'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several `edge-discovery' applications over graph-based data models are known\nto have worst-case quadratic time complexity in the nodes, even if the\ndiscovered edges are sparse. One example is the generic link discovery problem\nbetween two graphs, which has invited research interest in several communities.\nSpecific versions of this problem include link prediction in social networks,\nontology alignment between metadata-rich RDF data, approximate joins, and\nentity resolution between instance-rich data. As large datasets continue to\nproliferate, reducing quadratic complexity to make the task practical is an\nimportant research problem. Within the entity resolution community, the problem\nis commonly referred to as blocking. A particular class of learnable blocking\nschemes is known as Disjunctive Normal Form (DNF) blocking schemes, and has\nemerged as state-of-the art for homogeneous (i.e. same-schema) tabular data.\nDespite the promise of these schemes, a formalism or learning framework has not\nbeen developed for them when input data instances are generic, attributed\ngraphs possessing both node and edge heterogeneity. With such a development,\nthe complexity-reducing scope of DNF schemes becomes applicable to a variety of\nproblems, including entity resolution and type alignment between heterogeneous\ngraphs, and link prediction in networks represented as attributed graphs. This\npaper presents a graph-theoretic formalism for DNF schemes, and investigates\ntheir learnability in an optimization framework. We also briefly describe an\nempirical case study encapsulating some of the principles in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 20:51:43 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 00:47:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Kejriwal", "Mayank", ""]]}, {"id": "1605.01010", "submitter": "UshaRani Yelipe", "authors": "Yelipe UshaRani, P. Sammulal", "title": "A Novel Approach for Imputation of Missing Attribute Values for\n  Efficient Mining of Medical Datasets - Class Based Cluster Approach", "comments": "Journal Published by University of Zulia, Venezuela and Indexed by\n  Web of Science and Scopus , H.index-5, SJR 0.11 (2014 Elsevier SJR Report),\n  12 Pages", "journal-ref": "Revista Tecnica de la Facultad de Ingeniera, Vol. 39, No 2, 184 -\n  195, 2016", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing attribute values are quite common in the datasets available in the\nliterature. Missing values are also possible because all attributes values may\nnot be recorded and hence unavailable due to several practical reasons. For all\nthese one must fix missing attribute vales if the analysis has to be done.\nImputation is the first step in analyzing medical datasets. Hence this has\nachieved significant contribution from several medical domain researchers.\nSeveral data mining researchers have proposed various methods and approaches to\nimpute missing values. However very few of them concentrate on dimensionality\nreduction. In this paper, we discuss a novel imputation framework for missing\nvalues imputation. Our approach of filling missing values is rooted on class\nbased clustering approach and essentially aims at medical records\ndimensionality reduction. We use these dimensionality records for carrying\nprediction and classification analysis. A case study is discussed which shows\nhow imputation is performed using proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 18:18:57 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["UshaRani", "Yelipe", ""], ["Sammulal", "P.", ""]]}, {"id": "1605.01092", "submitter": "Kapil Vaswani", "authors": "Kapil Vaswani, Ravi Ramamurthy, Ramarathnam Venkatesan", "title": "Information Flows in Encrypted Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In encrypted databases, sensitive data is protected from an untrusted server\nby encrypting columns using partially homomorphic encryption schemes, and\nstoring encryption keys in a trusted client. However, encrypting columns and\nprotecting encryption keys does not ensure confidentiality - sensitive data can\nleak during query processing due to information flows through the trusted\nclient. In this paper, we propose SecureSQL, an encrypted database that\npartitions query processing between an untrusted server and a trusted client\nwhile ensuring the absence of information flows. Our evaluation based on OLTP\nbenchmarks suggests that SecureSQL can protect against explicit flows with low\noverheads (< 30%). However, protecting against implicit flows can be expensive\nbecause it precludes the use of key databases optimizations and introduces\nadditional round trips between client and server.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 21:13:27 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Vaswani", "Kapil", ""], ["Ramamurthy", "Ravi", ""], ["Venkatesan", "Ramarathnam", ""]]}, {"id": "1605.01207", "submitter": "Stanislav Kikot", "authors": "Meghyn Bienvenu, Stanislav Kikot, Roman Kontchakov, Vladimir Podolskii\n  and Michael Zakharyaschev", "title": "Ontology-Mediated Queries: Combined Complexity and Succinctness of\n  Rewritings via Circuit Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give solutions to two fundamental computational problems in ontology-based\ndata access with the W3C standard ontology language OWL 2 QL: the succinctness\nproblem for first-order rewritings of ontology-mediated queries (OMQs), and the\ncomplexity problem for OMQ answering. We classify OMQs according to the shape\nof their conjunctive queries (treewidth, the number of leaves) and the\nexistential depth of their ontologies. For each of these classes, we determine\nthe combined complexity of OMQ answering, and whether all OMQs in the class\nhave polynomial-size first-order, positive existential, and nonrecursive\ndatalog rewritings. We obtain the succinctness results using hypergraph\nprograms, a new computational model for Boolean functions, which makes it\npossible to connect the size of OMQ rewritings and circuit complexity.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 10:10:37 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Bienvenu", "Meghyn", ""], ["Kikot", "Stanislav", ""], ["Kontchakov", "Roman", ""], ["Podolskii", "Vladimir", ""], ["Zakharyaschev", "Michael", ""]]}, {"id": "1605.01229", "submitter": "Paolo Missier", "authors": "Paolo Missier", "title": "The lifecycle of provenance metadata and its associated challenges and\n  opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter outlines some of the challenges and opportunities associated\nwith adopting provenance principles and standards in a variety of disciplines,\nincluding data publication and reuse, and information sciences.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 11:27:45 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Missier", "Paolo", ""]]}, {"id": "1605.01429", "submitter": "PhridviRaj M.S.B", "authors": "M.S.B. PhridviRaja, C.V. GuruRao", "title": "Data mining : past present and future - a typical survey on data streams", "comments": "The 7th International Conference Interdisciplinary in Engineering\n  (INTER-ENG 2013). in Procedia Technology, Volume 12, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data Stream Mining is one of the area gaining lot of practical significance\nand is progressing at a brisk pace with new methods, methodologies and findings\nin various applications related to medicine, computer science, bioinformatics\nand stock market prediction, weather forecast, text, audio and video processing\nto name a few. Data happens to be the key concern in data mining. With the huge\nonline data generated from several sensors, Internet Relay Chats, Twitter, Face\nbook, Online Bank or ATM Transactions, the concept of dynamically changing data\nis becoming a key challenge, what we call as data streams. In this paper, we\ngive the algorithm for finding frequent patterns from data streams with a case\nstudy and identify the research issues in handling data streams.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 20:46:36 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["PhridviRaja", "M. S. B.", ""], ["GuruRao", "C. V.", ""]]}, {"id": "1605.01435", "submitter": "Daniel Waddington", "authors": "Daniel G. Waddington, Changhui Lin", "title": "A Fast Lightweight Time-Series Store for IoT Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of the Internet-of-Things (IoT), handling large volumes of\ntime-series data has become a growing concern. Data, generated from millions of\nInternet-connected sensors, will drive new IoT applications and services. A key\nrequirement is the ability to aggregate, preprocess, index, store and analyze\ndata with minimal latency so that time-to-insight can be reduced. In the\nfuture, we expect real-time data collection and analysis to be performed both\non small devices (e.g., in hubs and appliances) as well in server-based\ninfrastructure. The ability to localize sensitive data to the home, and thus\npreserve privacy, is a key driver for small-device deployment.\n  In this paper, we present an efficient architecture for time-series data\nmanagement that provides a high data ingestion rate, while still being\nsufficiently lightweight that it can be deployed in embedded environments or\nsmall virtual machines. Our solution strives to minimize overhead and explores\nwhat can be done without complex indexing schemes that typically, for\nperformance reasons, must be held in main memory. We combine a simple in-memory\nhierarchical index, log-structured store and in-flight sort, with a\nhigh-performance data pipeline architecture that is optimized for multicore\nplatforms. We show that our solution is able to handle streaming insertions at\nover 4 million records per second (on a single x86 server) while still\nretaining SQL query performance better than or comparable to existing RDBMS.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 21:00:36 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 16:15:23 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Waddington", "Daniel G.", ""], ["Lin", "Changhui", ""]]}, {"id": "1605.01664", "submitter": "Brandon Haynes", "authors": "Brandon Haynes, Alvin Cheung and Magdalena Balazinska", "title": "PipeGen: Data Pipe Generator for Hybrid Analytics", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a tool called PipeGen for efficient data transfer between database\nmanagement systems (DBMSs). PipeGen targets data analytics workloads on\nshared-nothing engines. It supports scenarios where users seek to perform\ndifferent parts of an analysis in different DBMSs or want to combine and\nanalyze data stored in different systems. The systems may be colocated in the\nsame cluster or may be in different clusters. To achieve high performance,\nPipeGen leverages the ability of all DBMSs to export, possibly in parallel,\ndata into a common data format, such as CSV or JSON. It automatically extends\nthese import and export functions with efficient binary data transfer\ncapabilities that avoid materializing the transmitted data on the file system.\nWe implement a prototype of PipeGen and evaluate it by automatically generating\ndata pipes between five different DBMSs. Our experiments show that PipeGen\ndelivers speedups up to 3.8x compared with manually exporting and importing\ndata across systems using CSV.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 17:43:38 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 19:37:55 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Haynes", "Brandon", ""], ["Cheung", "Alvin", ""], ["Balazinska", "Magdalena", ""]]}, {"id": "1605.02210", "submitter": "Adrian Onet", "authors": "Adrian Onet", "title": "Inference-based semantics in Data Exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Exchange is an old problem that was firstly studied from a theoretical\npoint of view only in 2003. Since then many approaches were considered when it\ncame to the language describing the relationship between the source and the\ntarget schema. These approaches focus on what it makes a target instance a\n\"good\" solution for data-exchange. In this paper we propose the inference-based\nsemantics that solves many certain-answer anomalies existing in current\ndata-exchange semantics. To this we introduce a new mapping language between\nthe source and the target schema based on annotated bidirectional dependencies\n(abd) and, consequently define the semantics for this new language. It is shown\nthat the ABD-semantics can properly represent the inference-based semantics,\nfor any source-to-target mappings. We discovered three dichotomy results under\nthe new semantics for solution-existence, solution-check and UCQ evaluation\nproblems. These results rely on two factors describing the annotation used in\nthe mappings (density and cardinality). Finally we also investigate the\ncertain-answers evaluation problem under ABD-semantics and discover many\ntractable classes for non-UCQ queries even for a subclass of CQ with negation.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 16:28:40 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Onet", "Adrian", ""]]}, {"id": "1605.02824", "submitter": "Xiaowang Zhang", "authors": "Zhihui Liu and Zhiyong Feng and Xiaowang Zhang and Xin Wang and\n  Guozheng Rao", "title": "RORS: Enhanced Rule-based OWL Reasoning on Spark", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rule-based OWL reasoning is to compute the deductive closure of an\nontology by applying RDF/RDFS and OWL entailment rules. The performance of the\nrule-based OWL reasoning is often sensitive to the rule execution order. In\nthis paper, we present an approach to enhancing the performance of the\nrule-based OWL reasoning on Spark based on a locally optimal executable\nstrategy. Firstly, we divide all rules (27 in total) into four main classes,\nnamely, SPO rules (5 rules), type rules (7 rules), sameAs rules (7 rules), and\nschema rules (8 rules) since, as we investigated, those triples corresponding\nto the first three classes of rules are overwhelming (e.g., over 99% in the\nLUBM dataset) in our practical world. Secondly, based on the interdependence\namong those entailment rules in each class, we pick out an optimal rule\nexecutable order of each class and then combine them into a new rule execution\norder of all rules. Finally, we implement the new rule execution order on Spark\nin a prototype called RORS. The experimental results show that the running time\nof RORS is improved by about 30% as compared to Kim & Park's algorithm (2015)\nusing the LUBM200 (27.6 million triples).\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 02:27:09 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Liu", "Zhihui", ""], ["Feng", "Zhiyong", ""], ["Zhang", "Xiaowang", ""], ["Wang", "Xin", ""], ["Rao", "Guozheng", ""]]}, {"id": "1605.04035", "submitter": "Jimmy Lin", "authors": "Kareem El Gebaly and Jimmy Lin", "title": "Afterburner: The Case for In-Browser Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the novel and unconventional idea of implementing an\nanalytical RDBMS in pure JavaScript so that it runs completely inside a browser\nwith no external dependencies. Our prototype, called Afterburner, generates\ncompiled query plans that exploit typed arrays and asm.js, two relatively\nrecent advances in JavaScript. On a few simple queries, we show that\nAfterburner achieves comparable performance to MonetDB running natively on the\nsame machine. This is an interesting finding in that it shows how far\nJavaScript has come as an efficient execution platform. Beyond a mere technical\ncuriosity, we discuss how our techniques could support ubiquitous in-browser\ninteractive analytics (potentially integrating with browser-based notebooks)\nand also present interesting opportunities for split-execution strategies where\nquery operators are distributed between the browser and backend servers.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 02:50:24 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Gebaly", "Kareem El", ""], ["Lin", "Jimmy", ""]]}, {"id": "1605.04227", "submitter": "Madhav Nimishakavi Mr", "authors": "Madhav Nimishakavi, Uday Singh Saini and Partha Talukdar", "title": "Relation Schema Induction using Tensor Factorization with Side\n  Information", "comments": "Proceedings of the 2016 Conference on Empirical Methods in Natural\n  Language Processing, November 2016. Austin, TX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of documents from a specific domain (e.g., medical research\njournals), how do we automatically build a Knowledge Graph (KG) for that\ndomain? Automatic identification of relations and their schemas, i.e., type\nsignature of arguments of relations (e.g., undergo(Patient, Surgery)), is an\nimportant first step towards this goal. We refer to this problem as Relation\nSchema Induction (RSI). In this paper, we propose Schema Induction using\nCoupled Tensor Factorization (SICTF), a novel tensor factorization method for\nrelation schema induction. SICTF factorizes Open Information Extraction\n(OpenIE) triples extracted from a domain corpus along with additional side\ninformation in a principled way to induce relation schemas. To the best of our\nknowledge, this is the first application of tensor factorization for the RSI\nproblem. Through extensive experiments on multiple real-world datasets, we find\nthat SICTF is not only more accurate than state-of-the-art baselines, but also\nsignificantly faster (about 14x faster).\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 19:44:04 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 04:57:09 GMT"}, {"version": "v3", "created": "Wed, 16 Nov 2016 04:53:42 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nimishakavi", "Madhav", ""], ["Saini", "Uday Singh", ""], ["Talukdar", "Partha", ""]]}, {"id": "1605.04263", "submitter": "Guohui Xiao", "authors": "Dag Hovland and Davide Lanti and Martin Rezk and Guohui Xiao", "title": "OBDA Constraints for Effective Query Answering (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Ontology Based Data Access (OBDA) users pose SPARQL queries over an\nontology that lies on top of relational datasources. These queries are\ntranslated on-the-fly into SQL queries by OBDA systems. Standard SPARQL-to-SQL\ntranslation techniques in OBDA often produce SQL queries containing redundant\njoins and unions, even after a number of semantic and structural optimizations.\nThese redundancies are detrimental to the performance of query answering,\nespecially in complex industrial OBDA scenarios with large enterprise\ndatabases. To address this issue, we introduce two novel notions of OBDA\nconstraints and show how to exploit them for efficient query answering. We\nconduct an extensive set of experiments on large datasets using real world data\nand queries, showing that these techniques strongly improve the performance of\nquery answering up to orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 17:29:28 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 09:21:26 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Hovland", "Dag", ""], ["Lanti", "Davide", ""], ["Rezk", "Martin", ""], ["Xiao", "Guohui", ""]]}, {"id": "1605.04271", "submitter": "Mar\\'ia Emilia Descotte", "authors": "Sergio Abriola, Mar\\'ia Emilia Descotte, Raul Fervari, Santiago\n  Figueira", "title": "Axiomatizations for downward XPath on Data Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give sound and complete axiomatizations for XPath with data tests by\n\"equality\" or \"inequality\", and containing the single \"child\" axis. This\ndata-aware logic predicts over data trees, which are tree-like structures whose\nevery node contains a label from a finite alphabet and a data value from an\ninfinite domain. The language allows us to compare data values of two nodes but\ncannot access the data values themselves (i.e. there is no comparison by\nconstants). Our axioms are in the style of equational logic, extending the\naxiomatization of data-oblivious XPath, by B. ten Cate, T. Litak and M. Marx.\nWe axiomatize the full logic with tests by \"equality\" and \"inequality\", and\nalso a simpler fragment with \"equality\" tests only. Our axiomatizations apply\nboth to node expressions and path expressions. The proof of completeness relies\non a novel normal form theorem for XPath with data tests.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 17:49:22 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 12:30:00 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Abriola", "Sergio", ""], ["Descotte", "Mar\u00eda Emilia", ""], ["Fervari", "Raul", ""], ["Figueira", "Santiago", ""]]}, {"id": "1605.04292", "submitter": "Tianzheng Wang", "authors": "Tianzheng Wang and Ryan Johnson and Alan Fekete and Ippokratis Pandis", "title": "Efficiently making (almost) any concurrency control mechanism\n  serializable", "comments": null, "journal-ref": null, "doi": "10.1007/s00778-017-0463-8", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrency control (CC) algorithms must trade off strictness for\nperformance. Serializable CC schemes generally pay higher cost to prevent\nanomalies, both in runtime overhead and in efforts wasted by aborting\ntransactions. We propose the serial safety net (SSN), a\nserializability-enforcing certifier which can be applied with minimal overhead\non top of various CC schemes that offer higher performance but admit anomalies,\nsuch as snapshot isolation and read committed. The underlying CC retains\ncontrol of scheduling and transactional accesses, while SSN tracks the\nresulting dependencies. At commit time, SSN performs an efficient validation\ntest by examining only direct dependencies of the committing transaction to\ndetermine whether it can commit safely or must abort to avoid a potential\ndependency cycle.\n  SSN performs robustly for various workloads. It maintains the characteristics\nof the underlying CC without biasing toward certain types of transactions,\nthough the underlying CC might. Besides traditional OLTP workloads, SSN also\nallows efficient handling of heterogeneous workloads with long, read-mostly\ntransactions. SSN can avoid tracking the majority of reads (thus reducing the\noverhead of serializability certification) and still produce serializable\nexecutions with little overhead. The dependency tracking and validation tests\ncan be done efficiently, fully parallel and latch-free, for multi-version\nsystems on modern hardware with substantial core count and large main memory.\n  We demonstrate the efficiency, accuracy and robustness of SSN using extensive\nsimulations and an implementation that overlays snapshot isolation in ERMIA, a\nmemory-optimized OLTP engine that is capable of running different CC schemes.\nEvaluation results confirm that SSN is a promising approach to serializability\nwith robust performance and low overhead for various workloads.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 19:14:52 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2016 17:56:31 GMT"}, {"version": "v3", "created": "Fri, 9 Dec 2016 20:43:23 GMT"}, {"version": "v4", "created": "Tue, 7 Feb 2017 02:26:09 GMT"}, {"version": "v5", "created": "Thu, 4 May 2017 14:54:53 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Wang", "Tianzheng", ""], ["Johnson", "Ryan", ""], ["Fekete", "Alan", ""], ["Pandis", "Ippokratis", ""]]}, {"id": "1605.04672", "submitter": "Pushpendre Rastogi", "authors": "Pushpendre Rastogi, Benjamin Van Durme", "title": "A Critical Examination of RESCAL for Completion of Knowledge Bases with\n  Transitive Relations", "comments": "Four and a half page", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction in large knowledge graphs has received a lot of attention\nrecently because of its importance for inferring missing relations and for\ncompleting and improving noisily extracted knowledge graphs. Over the years a\nnumber of machine learning researchers have presented various models for\npredicting the presence of missing relations in a knowledge base. Although all\nthe previous methods are presented with empirical results that show high\nperformance on select datasets, there is almost no previous work on\nunderstanding the connection between properties of a knowledge base and the\nperformance of a model. In this paper we analyze the RESCAL method and prove\nthat it can not encode asymmetric transitive relations in knowledge bases.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 07:43:28 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Rastogi", "Pushpendre", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1605.05219", "submitter": "Jonny Daenen", "authors": "Jonny Daenen, Frank Neven, Tony Tan, Stijn Vansummeren", "title": "Parallel Evaluation of Multi-Semi-Joins", "comments": "added Gumbo code reference, added Subset Sum reference, adjusted\n  alignment in Figure 1, adjusted Figure 5 (remove redundant units, larger\n  font), removed capitals in Table 2, boxes for environment ends, clarified\n  proof in appendix, reference cleanup (pages, capitalization), uncapitalized\n  \"REQUEST\" and \"ASSERT\" when used in text, small rewordings (no results\n  affected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While services such as Amazon AWS make computing power abundantly available,\nadding more computing nodes can incur high costs in, for instance,\npay-as-you-go plans while not always significantly improving the net running\ntime (aka wall-clock time) of queries. In this work, we provide algorithms for\nparallel evaluation of SGF queries in MapReduce that optimize total time, while\nretaining low net time. Not only can SGF queries specify all semi-join\nreducers, but also more expressive queries involving disjunction and negation.\nSince SGF queries can be seen as Boolean combinations of (potentially nested)\nsemi-joins, we introduce a novel multi-semi-join (MSJ) MapReduce operator that\nenables the evaluation of a set of semi-joins in one job. We use this operator\nto obtain parallel query plans for SGF queries that outvalue sequential plans\nw.r.t. net time and provide additional optimizations aimed at minimizing total\ntime without severely affecting net time. Even though the latter optimizations\nare NP-hard, we present effective greedy algorithms. Our experiments, conducted\nusing our own implementation Gumbo on top of Hadoop, confirm the usefulness of\nparallel query plans, and the effectiveness and scalability of our\noptimizations, all with a significant improvement over Pig and Hive.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 15:53:27 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 11:22:44 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Daenen", "Jonny", ""], ["Neven", "Frank", ""], ["Tan", "Tony", ""], ["Vansummeren", "Stijn", ""]]}, {"id": "1605.05566", "submitter": "Joseph M. Hellerstein", "authors": "Joseph M. Hellerstein", "title": "Naughton's Wisconsin Bibliography: A Brief Guide", "comments": "Presented at the Wisconsin Database Group 40 Year Event, on the\n  occasion of Jeff Naughton's retirement from the University of Wisconsin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over nearly three decades at the University of Wisconsin, Jeff Naughton has\nleft an indelible mark on computer science. He has been a global leader of the\ndatabase research field, deepening its core and pushing its boundaries. Many of\nNaughton's ideas were translated directly into practice in commercial and\nopen-source systems. But software comes and goes. In the end, it is the ideas\nthemselves that have had impact, ideas written down in papers.\n  Naughton has been a prolific scholar over the last thirty years, with over\n175 publications in his bibliography, covering a wide range of topics. This\ndocument does not attempt to enumerate or even summarize the wealth of ideas\nthat Naughton has published over the course of his academic career--the task is\ntoo daunting. Instead, the best this short note aims to do is to serve as a\nrough map of the territory: something to help other researchers navigate the\nwide spaces of Naughton's work.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 05:54:01 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Hellerstein", "Joseph M.", ""]]}, {"id": "1605.05826", "submitter": "Matthias Boehm", "authors": "Matthias Boehm, Alexandre V. Evfimievski, Niketan Pansare, Berthold\n  Reinwald", "title": "Declarative Machine Learning - A Classification of Basic Properties and\n  Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Declarative machine learning (ML) aims at the high-level specification of ML\ntasks or algorithms, and automatic generation of optimized execution plans from\nthese specifications. The fundamental goal is to simplify the usage and/or\ndevelopment of ML algorithms, which is especially important in the context of\nlarge-scale computations. However, ML systems at different abstraction levels\nhave emerged over time and accordingly there has been a controversy about the\nmeaning of this general definition of declarative ML. Specification\nalternatives range from ML algorithms expressed in domain-specific languages\n(DSLs) with optimization for performance, to ML task (learning problem)\nspecifications with optimization for performance and accuracy. We argue that\nthese different types of declarative ML complement each other as they address\ndifferent users (data scientists and end users). This paper makes an attempt to\ncreate a taxonomy for declarative ML, including a definition of essential basic\nproperties and types of declarative ML. Along the way, we provide insights into\nimplications of these properties. We also use this taxonomy to classify\nexisting systems. Finally, we draw conclusions on defining appropriate\nbenchmarks and specification languages for declarative ML.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 06:39:28 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Boehm", "Matthias", ""], ["Evfimievski", "Alexandre V.", ""], ["Pansare", "Niketan", ""], ["Reinwald", "Berthold", ""]]}, {"id": "1605.06143", "submitter": "Philip Derbeko", "authors": "Philip Derbeko, Shlomi Dolev, Ehud Gudes, Jeffrey D. Ullman", "title": "Efficient and Private Approximations of Distributed Databases\n  Calculations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, an increasing amount of data is collected in different and\noften, not cooperative, databases. The problem of privacy-preserving,\ndistributed calculations over separated databases and, a relative to it, issue\nof private data release were intensively investigated. However, despite a\nconsiderable progress, computational complexity, due to an increasing size of\ndata, remains a limiting factor in real-world deployments, especially in case\nof privacy-preserving computations.\n  In this paper, we present a general method for trade off between performance\nand accuracy of distributed calculations by performing data sampling. Sampling\nwas a topic of extensive research that recently received a boost of interest.\nWe provide a sampling method targeted at separate, non-collaborating,\nvertically partitioned datasets. The method is exemplified and tested on\napproximation of intersection set both without and with privacy-preserving\nmechanism. An analysis of the bound on error as a function of the sample size\nis discussed and heuristic algorithm is suggested to further improve the\nperformance. The algorithms were implemented and experimental results confirm\nthe validity of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 21:11:01 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Derbeko", "Philip", ""], ["Dolev", "Shlomi", ""], ["Gudes", "Ehud", ""], ["Ullman", "Jeffrey D.", ""]]}, {"id": "1605.06424", "submitter": "Zeeshan Lakhani", "authors": "Russell Brown, Zeeshan Lakhani, and Paul Place", "title": "Big(ger) Sets: decomposed delta CRDT Sets in Riak", "comments": "PaPoC '16 Proceedings of the 2nd Workshop on the Principles and\n  Practice of Consistency for Distributed Data, Article No. 5, Publication\n  Date: 2016-04-18", "journal-ref": null, "doi": "10.1145/2911151.2911156", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CRDT[24] Sets as implemented in Riak[6] perform poorly for writes, both as\ncardinality grows, and for sets larger than 500KB[25]. Riak users wish to\ncreate high cardinality CRDT sets, and expect better than O(n) performance for\nindividual insert and remove operations. By decomposing a CRDT set on disk, and\nemploying delta-replication[2], we can achieve far better performance than just\ndelta replication alone: relative to the size of causal metadata, not the\ncardinality of the set, and we can support sets that are 100s times the size of\nRiak sets, while still providing the same level of consistency. There is a\ntrade-off in read performance but we expect it is mitigated by enabling queries\non sets.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:27:13 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Brown", "Russell", ""], ["Lakhani", "Zeeshan", ""], ["Place", "Paul", ""]]}, {"id": "1605.06523", "submitter": "William Cohen", "authors": "William W. Cohen", "title": "TensorLog: A Differentiable Deductive Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large knowledge bases (KBs) are useful in many tasks, but it is unclear how\nto integrate this sort of knowledge into \"deep\" gradient-based learning\nsystems. To address this problem, we describe a probabilistic deductive\ndatabase, called TensorLog, in which reasoning uses a differentiable process.\nIn TensorLog, each clause in a logical theory is first converted into certain\ntype of factor graph. Then, for each type of query to the factor graph, the\nmessage-passing steps required to perform belief propagation (BP) are\n\"unrolled\" into a function, which is differentiable. We show that these\nfunctions can be composed recursively to perform inference in non-trivial\nlogical theories containing multiple interrelated clauses and predicates. Both\ncompilation and inference in TensorLog are efficient: compilation is linear in\ntheory size and proof depth, and inference is linear in database size and the\nnumber of message-passing steps used in BP. We also present experimental\nresults with TensorLog and discuss its relationship to other first-order\nprobabilistic logics.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 20:10:46 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 21:03:55 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Cohen", "William W.", ""]]}, {"id": "1605.06856", "submitter": "Chengkai Li", "authors": "Nandish Jayaram, Rohit Bhoopalam, Chengkai Li, Vassilis Athitsos", "title": "Orion: Enabling Suggestions in a Visual Query Builder for\n  Ultra-Heterogeneous Graphs", "comments": "This version is just to correct the title with a missing space", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The database community has long recognized the importance of graphical query\ninterface to the usability of data management systems. Yet, relatively less has\nbeen done. We present Orion, a visual interface for querying\nultra-heterogeneous graphs. It iteratively assists users in query graph\nconstruction by making suggestions via machine learning methods. In its active\nmode, Orion automatically suggests top-k edges to be added to a query graph. In\nits passive mode, the user adds a new edge manually, and Orion suggests a\nranked list of labels for the edge. Orion's edge ranking algorithm, Random\nDecision Paths (RDP), makes use of a query log to rank candidate edges by how\nlikely they will match the user's query intent. Extensive user studies using\nFreebase demonstrated that Orion users have a 70% success rate in constructing\ncomplex query graphs, a significant improvement over the 58% success rate by\nthe users of a baseline system that resembles existing visual query builders.\nFurthermore, using active mode only, the RDP algorithm was compared with\nseveral methods adapting other machine learning algorithms such as random\nforests and naive Bayes classifier, as well as class association rules and\nrecommendation systems based on singular value decomposition. On average, RDP\nrequired 40 suggestions to correctly reach a target query graph (using only its\nactive mode of suggestion) while other methods required 1.5--4 times as many\nsuggestions.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 21:29:04 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 19:55:26 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Jayaram", "Nandish", ""], ["Bhoopalam", "Rohit", ""], ["Li", "Chengkai", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "1605.06865", "submitter": "Lei Gai", "authors": "Lei Gai, Wei Chen, Tengjiao Wang", "title": "ROSIE: Runtime Optimization of SPARQL Queries Using Incremental\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational databases are wildly adopted in RDF (Resource Description\nFramework) data management. For efficient SPARQL query evaluation, the legacy\nquery optimizer needs reconsiderations. One vital problem is how to tackle the\nsuboptimal query plan caused by error-prone cardinality estimation. Consider\nthe schema-free nature of RDF data and the Join-intensive characteristic of\nSPARQL query, determine an optimal execution order before the query actually\nevaluated is costly or even infeasible, especially for complex queries on\nlarge-scale data. In this paper, we propose ROSIE, a Runtime Optimization\nframework that iteratively re-optimize SPARQL query plan according to the\nactual cardinality derived from Incremental partial query Evaluation. By\nintroducing an approach for heuristic-based plan generation, as well as a\nmechanism to detect cardinality estimation error at runtime, ROSIE relieves the\nproblem of biased cardinality propagation in an efficient way, and thus is more\nresilient to complex query evaluation. Extensive experiments on real and\nbenchmark data show that compared to the state-of-the-arts, ROSIE consistently\noutperformed on complex queries by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 00:00:35 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Gai", "Lei", ""], ["Chen", "Wei", ""], ["Wang", "Tengjiao", ""]]}, {"id": "1605.07159", "submitter": "Leopoldo Bertossi", "authors": "Andrei Lopatenko and Leopoldo Bertossi", "title": "Complexity of Consistent Query Answering in Databases under\n  Cardinality-Based and Incremental Repair Semantics (extended version)", "comments": "This paper, without the proofs provided here, arXiv:cs/0604002,\n  appeared in the Proc. of ICDT 2007. This version contains all the proofs in\n  correlation with the results reported in the ICDT paper (as opposed to a\n  previous Arkiv Corr posting related to the same paper). One proof was\n  corrected, and a corollary was added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A database D may be inconsistent wrt a given set IC of integrity constraints.\nConsistent Query Answering (CQA) is the problem of computing from D the answers\nto a query that are consistent wrt IC . Consistent answers are invariant under\nall the repairs of D, i.e. the consistent instances that minimally depart from\nD. Three classes of repair have been considered in the literature: those that\nminimize set-theoretically the set of tuples in the symmetric difference; those\nthat minimize the changes of attribute values, and those that minimize the\ncardinality of the set of tuples in the symmetric difference. The latter class\nhas not been systematically investigated. In this paper we obtain algorithmic\nand complexity theoretic results for CQA under this cardinality-based repair\nsemantics. We do this in the usual, static setting, but also in a dynamic\nframework where a consistent database is affected by a sequence of updates,\nwhich may make it inconsistent. We also establish comparative results with the\nother two kinds of repairs in the dynamic case.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 19:50:29 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Lopatenko", "Andrei", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1605.07865", "submitter": "Konstantin Golenberg", "authors": "Konstantin Golenberg and Yehoshua Sagiv", "title": "Constructing Data Graphs for Keyword Search", "comments": "Full version of DEXA'16 paper", "journal-ref": null, "doi": "10.1007/978-3-319-44406-2_33", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data graph is a convenient paradigm for supporting keyword search that\ntakes into account available semantic structure and not just textual relevance.\nHowever, the problem of constructing data graphs that facilitate both\nefficiency and effectiveness of the underlying system has hardly been\naddressed. A conceptual model for this task is proposed. Principles for\nconstructing good data graphs are explained. Transformations for generating\ndata graphs from RDB and XML are developed. The results obtained from these\ntransformations are analyzed. It is shown that XML is a better starting point\nfor getting a good data graph.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 12:59:37 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 20:34:28 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Golenberg", "Konstantin", ""], ["Sagiv", "Yehoshua", ""]]}, {"id": "1605.09675", "submitter": "Peng Cheng", "authors": "Peng Cheng, Xun Jian, Lei Chen", "title": "Task Assignment on Spatial Crowdsourcing [Experiments and Analyses]\n  (Technical Report)", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the rapid development of mobile devices and the crowdsourcing\nplatforms, the spatial crowdsourcing has attracted much attention from the\ndatabase community. Specifically, spatial crowdsourcing refers to sending a\nlocation-based request to workers according to their positions, and workers\nneed to physically move to specified locations to conduct tasks. Many works\nhave studied task assignment problems in spatial crowdsourcing, however, their\nproblem settings are different from each other. Thus, it is hard to compare the\nperformances of existing algorithms on task assignment in spatial\ncrowdsourcing. In this paper, we present a comprehensive experimental\ncomparison of most existing algorithms on task assignment in spatial\ncrowdsourcing. Specifically, we first give general definitions about spatial\nworkers and spatial tasks based on definitions in the existing works such that\nthe existing algorithms can be applied on the same synthetic and real data\nsets. Then, we provide an uniform implementation for all the tested algorithms\nof task assignment problems in spatial crowdsourcing (open sourced). Finally,\nbased on the results on both synthetic and real data sets, we discuss the\nstrengths and weaknesses of tested algorithms, which can guide future research\non the same area and practical implementations of spatial crowdsourcing\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 15:35:18 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 15:28:49 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 13:06:08 GMT"}, {"version": "v4", "created": "Wed, 16 May 2018 02:38:52 GMT"}, {"version": "v5", "created": "Thu, 6 Sep 2018 05:29:20 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Cheng", "Peng", ""], ["Jian", "Xun", ""], ["Chen", "Lei", ""]]}, {"id": "1605.09753", "submitter": "Jennifer Ortiz", "authors": "Jennifer Ortiz, Brendan Lee, Magdalena Balazinska, Joseph L.\n  Hellerstein", "title": "PerfEnforce: A Dynamic Scaling Engine for Analytics with Performance\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present PerfEnforce, a scaling engine designed to enable\ncloud providers to sell performance levels for data analytics cloud services.\nPerfEnforce scales a cluster of virtual machines allocated to a user in a way\nthat minimizes cost while probabilistically meeting the query runtime\nguarantees offered by a service level agreement. With PerfEnforce, we show how\nto scale a cluster in a way that minimally disrupts a user's query session. We\nfurther show when to scale the cluster using one of three methods: feedback\ncontrol, reinforcement learning, or perceptron learning. We find that\nperceptron learning outperforms the other two methods when making cluster\nscaling decisions.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 18:20:36 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ortiz", "Jennifer", ""], ["Lee", "Brendan", ""], ["Balazinska", "Magdalena", ""], ["Hellerstein", "Joseph L.", ""]]}]