[{"id": "1501.00255", "submitter": "Florin Rusu", "authors": "Chengjie Qin and Florin Rusu", "title": "Speculative Approximations for Terascale Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model calibration is a major challenge faced by the plethora of statistical\nanalytics packages that are increasingly used in Big Data applications.\nIdentifying the optimal model parameters is a time-consuming process that has\nto be executed from scratch for every dataset/model combination even by\nexperienced data scientists. We argue that the incapacity to evaluate multiple\nparameter configurations simultaneously and the lack of support to quickly\nidentify sub-optimal configurations are the principal causes. In this paper, we\ndevelop two database-inspired techniques for efficient model calibration.\nSpeculative parameter testing applies advanced parallel multi-query processing\nmethods to evaluate several configurations concurrently. The number of\nconfigurations is determined adaptively at runtime, while the configurations\nthemselves are extracted from a distribution that is continuously learned\nfollowing a Bayesian process. Online aggregation is applied to identify\nsub-optimal configurations early in the processing by incrementally sampling\nthe training dataset and estimating the objective function corresponding to\neach configuration. We design concurrent online aggregation estimators and\ndefine halting conditions to accurately and timely stop the execution. We apply\nthe proposed techniques to distributed gradient descent optimization -- batch\nand incremental -- for support vector machines and logistic regression models.\nWe implement the resulting solutions in GLADE PF-OLA -- a state-of-the-art Big\nData analytics system -- and evaluate their performance over terascale-size\nsynthetic and real datasets. The results confirm that as many as 32\nconfigurations can be evaluated concurrently almost as fast as one, while\nsub-optimal configurations are detected accurately in as little as a\n$1/20^{\\text{th}}$ fraction of the time.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 07:07:44 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Qin", "Chengjie", ""], ["Rusu", "Florin", ""]]}, {"id": "1501.00354", "submitter": "Sang-Pil Kim", "authors": "Sang-Pil Kim, Myeong-Sun Gil, Yang-Sae Moon, and Hee-Sun Won", "title": "Efficient 2-Step Protocol and Its Discriminative Feature Selections in\n  Secure Similar Document Detection", "comments": "25 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure similar document detection (SSDD) identifies similar documents of two\nparties while each party does not disclose its own sensitive documents to\nanother party. In this paper, we propose an efficient 2-step protocol that\nexploits a feature selection as the lower-dimensional transformation and\npresents discriminative feature selections to maximize the performance of the\nprotocol. For this, we first analyze that the existing 1-step protocol causes\nserious computation and communication overhead for high dimensional document\nvectors. To alleviate the overhead, we next present the feature selection-based\n2-step protocol and formally prove its correctness. The proposed 2-step\nprotocol works as follows: (1) in the filtering step, it uses low dimensional\nvectors obtained by the feature selection to filter out non-similar documents;\n(2) in the post-processing step, it identifies similar documents only from the\nnon-filtered documents by using the 1-step protocol. As the feature selection,\nwe first consider the simplest one, random projection (RP), and propose its\n2-step solution SSDD-RP. We then present two discriminative feature selections\nand their solutions: SSDD-LF (local frequency) which selects a few dimensions\nlocally frequent in the current querying vector and SSDD-GF (global frequency)\nwhich selects ones globally frequent in the set of all document vectors. We\nfinally propose a hybrid one, SSDD-HF (hybrid frequency), that takes advantage\nof both SSDD-LF and SSDD-GF. We empirically show that the proposed 2-step\nprotocol outperforms the 1-step protocol by three or four orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 07:32:06 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Kim", "Sang-Pil", ""], ["Gil", "Myeong-Sun", ""], ["Moon", "Yang-Sae", ""], ["Won", "Hee-Sun", ""]]}, {"id": "1501.00405", "submitter": "Puneet Agarwal", "authors": "Puneet Agarwal, Gautam Shroff, Sarmimala Saikia, and Zaigham Khan", "title": "Efficiently Discovering Frequent Motifs in Large-scale Sensor Data", "comments": "13 pages, 8 figures, Technical Report", "journal-ref": null, "doi": null, "report-no": "TR-DAIF-2015-1", "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  While analyzing vehicular sensor data, we found that frequently occurring\nwaveforms could serve as features for further analysis, such as rule mining,\nclassification, and anomaly detection. The discovery of waveform patterns, also\nknown as time-series motifs, has been studied extensively; however, available\ntechniques for discovering frequently occurring time-series motifs were found\nlacking in either efficiency or quality: Standard subsequence clustering\nresults in poor quality, to the extent that it has even been termed\n'meaningless'. Variants of hierarchical clustering using techniques for\nefficient discovery of 'exact pair motifs' find high-quality frequent motifs,\nbut at the cost of high computational complexity, making such techniques\nunusable for our voluminous vehicular sensor data. We show that good quality\nfrequent motifs can be discovered using bounded spherical clustering of\ntime-series subsequences, which we refer to as COIN clustering, with near\nlinear complexity in time-series size. COIN clustering addresses many of the\nchallenges that previously led to subsequence clustering being viewed as\nmeaningless. We describe an end-to-end motif-discovery procedure using a\nsequence of pre and post-processing techniques that remove trivial-matches and\nshifted-motifs, which also plagued previous subsequence-clustering approaches.\nWe demonstrate that our technique efficiently discovers frequent motifs in\nvoluminous vehicular sensor data as well as in publicly available data sets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 14:09:46 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""], ["Saikia", "Sarmimala", ""], ["Khan", "Zaigham", ""]]}, {"id": "1501.00666", "submitter": "Evgeny Nikulchev", "authors": "Oleg Lukyanchikov, Evgeniy Pluzhnik, Simon Payain, Evgeny Nikulchev", "title": "Using Object-Relational Mapping to Create the Distributed Databases in a\n  Hybrid Cloud Infrastructure", "comments": null, "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, 2014, 5(12):61-64", "doi": "10.14569/IJACSA.2014.051208", "report-no": null, "categories": "cs.DB cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges currently problems in the use of cloud services is the\ntask of designing of specialized data management systems. This is especially\nimportant for hybrid systems in which the data are located in public and\nprivate clouds. Implementation monitoring functions querying, scheduling and\nprocessing software must be properly implemented and is an integral part of the\nsystem. To provide these functions is proposed to use an object-relational\nmapping (ORM). The article devoted to presenting the approach of designing\ndatabases for information systems hosted in a hybrid cloud infrastructure. It\nalso provides an example of the development of ORM library.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 11:23:57 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Lukyanchikov", "Oleg", ""], ["Pluzhnik", "Evgeniy", ""], ["Payain", "Simon", ""], ["Nikulchev", "Evgeny", ""]]}, {"id": "1501.01070", "submitter": "Herald Kllapi", "authors": "Herald Kllapi, Panos Sakkos, Alex Delis, Dimitrios Gunopulos, Yannis\n  Ioannidis", "title": "Elastic Processing of Analytical Query Workloads on IaaS Clouds", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern applications require the evaluation of analytical queries on\nlarge amounts of data. Such queries entail joins and heavy aggregations that\noften include user-defined functions (UDFs). The most efficient way to process\nthese specific type of queries is using tree execution plans. In this work, we\ndevelop an engine for analytical query processing and a suite of specialized\ntechniques that collectively take advantage of the tree form of such plans. The\nengine executes these tree plans in an elastic IaaS cloud infrastructure and\ndynamically adapts by allocating and releasing pertinent resources based on the\nquery workload monitored over a sliding time window. The engine offers its\nservices for a fee according to service-level agreements (SLAs) associated with\nthe incoming queries; its management of cloud resources aims at maximizing the\nprofit after removing the costs of using these resources. We have fully\nimplemented our algorithms in the Exareme dataflow processing system. We\npresent an extensive evaluation that demonstrates that our approach is very\nefficient (exhibiting fast response times), elastic (successfully adjusting the\ncloud resources it uses as the engine continually adapts to query workload\nchanges), and profitable (approximating very well the maximum difference\nbetween SLA-based income and cloud-based expenses).\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 03:38:45 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Kllapi", "Herald", ""], ["Sakkos", "Panos", ""], ["Delis", "Alex", ""], ["Gunopulos", "Dimitrios", ""], ["Ioannidis", "Yannis", ""]]}, {"id": "1501.01149", "submitter": "Thorsten Wuest", "authors": "Thorsten Wuest, Rainer Tinscher, Robert Porzel, Klaus-Dieter Thoben", "title": "Experimental Research Data Quality In Materials Science", "comments": null, "journal-ref": "International Journal of Advanced Information Technology, Vol. 4,\n  No. 6, Dec. 2014", "doi": "10.5121/ijait.2014.4601", "report-no": null, "categories": "cs.DB cond-mat.mtrl-sci cs.DL", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In materials sciences, a large amount of research data is generated through a\nbroad spectrum of different experiments. As of today, experimental research\ndata including meta-data in materials science is often stored decentralized by\nthe researcher(s) conducting the experiments without generally accepted\nstandards on what and how to store data. The conducted research and experiments\noften involve a considerable investment from public funding agencies that\ndesire the results to be made available in order to increase their impact. In\norder to achieve the goal of citable and (openly) accessible materials science\nexperimental research data in the future, not only an adequate infrastructure\nneeds to be established but the question of how to measure the quality of the\nexperimental research data also to be addressed. In this publication, the\nauthors identify requirements and challenges towards a systematic methodology\nto measure experimental research data quality prior to publication and derive\ndifferent approaches on that basis. These methods are critically discussed and\nassessed by their contribution and limitations towards the set goals.\nConcluding, a combination of selected methods is presented as a systematic,\nfunctional and practical quality measurement and assurance approach for\nexperimental research data in materials science with the goal of supporting the\naccessibility and dissemination of existing data sets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 11:29:22 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Wuest", "Thorsten", ""], ["Tinscher", "Rainer", ""], ["Porzel", "Robert", ""], ["Thoben", "Klaus-Dieter", ""]]}, {"id": "1501.01694", "submitter": "Mayank Kejriwal", "authors": "Mayank Kejriwal, Daniel P. Miranker", "title": "A DNF Blocking Scheme Learner for Heterogeneous Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Resolution concerns identifying co-referent entity pairs across\ndatasets. A typical workflow comprises two steps. In the first step, a blocking\nmethod uses a one-many function called a blocking scheme to map entities to\nblocks. In the second step, entities sharing a block are paired and compared.\nCurrent DNF blocking scheme learners (DNF-BSLs) apply only to structurally\nhomogeneous tables. We present an unsupervised algorithmic pipeline for\nlearning DNF blocking schemes on RDF graph datasets, as well as structurally\nheterogeneous tables. Previous DNF-BSLs are admitted as special cases. We\nevaluate the pipeline on six real-world dataset pairs. Unsupervised results are\nshown to be competitive with supervised and semi-supervised baselines. To the\nbest of our knowledge, this is the first unsupervised DNF-BSL that admits RDF\ngraphs and structurally heterogeneous tables as inputs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 00:37:09 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Kejriwal", "Mayank", ""], ["Miranker", "Daniel P.", ""]]}, {"id": "1501.01817", "submitter": "Tomasz Gogacz", "authors": "Tomasz Gogacz, Jerzy Marcinkowski", "title": "The Hunt for a Red Spider: Conjunctive Query Determinacy Is Undecidable", "comments": null, "journal-ref": null, "doi": "10.1109/LICS.2015.35", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve a well known, long-standing open problem in relational databases\ntheory, showing that the conjunctive query determinacy problem (in its\n\"unrestricted\" version) is undecidable.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 12:28:45 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 10:34:27 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Gogacz", "Tomasz", ""], ["Marcinkowski", "Jerzy", ""]]}, {"id": "1501.01924", "submitter": "Leman Akoglu", "authors": "Shebuti Rayana and Leman Akoglu", "title": "Less is More: Building Selective Anomaly Ensembles", "comments": "14 pages, 5 pages Appendix, 10 Figures, 15 Tables, to appear at SDM\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble techniques for classification and clustering have long proven\neffective, yet anomaly ensembles have been barely studied. In this work, we tap\ninto this gap and propose a new ensemble approach for anomaly mining, with\napplication to event detection in temporal graphs. Our method aims to combine\nresults from heterogeneous detectors with varying outputs, and leverage the\nevidence from multiple sources to yield better performance. However, trusting\nall the results may deteriorate the overall ensemble accuracy, as some\ndetectors may fall short and provide inaccurate results depending on the nature\nof the data in hand. This suggests that being selective in which results to\ncombine is vital in building effective ensembles---hence \"less is more\".\n  In this paper we propose SELECT; an ensemble approach for anomaly mining that\nemploys novel techniques to automatically and systematically select the results\nto assemble in a fully unsupervised fashion. We apply our method to event\ndetection in temporal graphs, where SELECT successfully utilizes five base\ndetectors and seven consensus methods under a unified ensemble framework. We\nprovide extensive quantitative evaluation of our approach on five real-world\ndatasets (four with ground truth), including Enron email communications, New\nYork Times news corpus, and World Cup 2014 Twitter news feed. Thanks to its\nselection mechanism, SELECT yields superior performance compared to individual\ndetectors alone, the full ensemble (naively combining all results), and an\nexisting diversity-based ensemble.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 18:54:09 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Rayana", "Shebuti", ""], ["Akoglu", "Leman", ""]]}, {"id": "1501.01941", "submitter": "Daniel Lemire", "authors": "Adina Crainiceanu and Daniel Lemire", "title": "Bloofi: Multidimensional Bloom Filters", "comments": null, "journal-ref": "Information Systems Volume 54, December 2015, Pages 311-324", "doi": "10.1016/j.is.2015.01.002", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bloom filters are probabilistic data structures commonly used for approximate\nmembership problems in many areas of Computer Science (networking, distributed\nsystems, databases, etc.). With the increase in data size and distribution of\ndata, problems arise where a large number of Bloom filters are available, and\nall them need to be searched for potential matches. As an example, in a\nfederated cloud environment, each cloud provider could encode the information\nusing Bloom filters and share the Bloom filters with a central coordinator. The\nproblem of interest is not only whether a given element is in any of the sets\nrepresented by the Bloom filters, but which of the existing sets contain the\ngiven element. This problem cannot be solved by just constructing a Bloom\nfilter on the union of all the sets. Instead, we effectively have a\nmultidimensional Bloom filter problem: given an element, we wish to receive a\nlist of candidate sets where the element might be.\n  To solve this problem, we consider 3 alternatives. Firstly, we can naively\ncheck many Bloom filters. Secondly, we propose to organize the Bloom filters in\na hierarchical index structure akin to a B+ tree, that we call Bloofi. Finally,\nwe propose another data structure that packs the Bloom filters in such a way as\nto exploit bit-level parallelism, which we call Flat-Bloofi.\n  Our theoretical and experimental results show that Bloofi and Flat-Bloofi\nprovide scalable and efficient solutions alternatives to search through a large\nnumber of Bloom filters.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 20:04:46 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 15:31:25 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 18:37:56 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Crainiceanu", "Adina", ""], ["Lemire", "Daniel", ""]]}, {"id": "1501.02033", "submitter": "EPTCS", "authors": "Jes\\'us M. Almendros-Jim\\'enez (Universidad de Almer\\'ia)", "title": "XQOWL: An Extension of XQuery for OWL Querying and Reasoning", "comments": "In Proceedings PROLE 2014, arXiv:1501.01693", "journal-ref": "EPTCS 173, 2015, pp. 41-55", "doi": "10.4204/EPTCS.173.4", "report-no": null, "categories": "cs.PL cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main aims of the so-called Web of Data is to be able to handle\nheterogeneous resources where data can be expressed in either XML or RDF. The\ndesign of programming languages able to handle both XML and RDF data is a key\ntarget in this context. In this paper we present a framework called XQOWL that\nmakes possible to handle XML and RDF/OWL data with XQuery. XQOWL can be\nconsidered as an extension of the XQuery language that connects XQuery with\nSPARQL and OWL reasoners. XQOWL embeds SPARQL queries (via Jena SPARQL engine)\nin XQuery and enables to make calls to OWL reasoners (HermiT, Pellet and\nFaCT++) from XQuery. It permits to combine queries against XML and RDF/OWL\nresources as well as to reason with RDF/OWL data. Therefore input data can be\neither XML or RDF/OWL and output data can be formatted in XML (also using\nRDF/OWL XML serialization).\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 03:59:54 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Almendros-Jim\u00e9nez", "Jes\u00fas M.", "", "Universidad de Almer\u00eda"]]}, {"id": "1501.02036", "submitter": "EPTCS", "authors": "Fernando S\\'aenz-P\\'erez (Universidad Complutense de Madrid)", "title": "Improving the Deductive System DES with Persistence by Using SQL DBMS's", "comments": "In Proceedings PROLE 2014, arXiv:1501.01693", "journal-ref": "EPTCS 173, 2015, pp. 100-114", "doi": "10.4204/EPTCS.173.8", "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents how persistent predicates have been included in the\nin-memory deductive system DES by relying on external SQL database management\nsystems. We introduce how persistence is supported from a user-point of view\nand the possible applications the system opens up, as the deductive expressive\npower is projected to relational databases. Also, we describe how it is\npossible to intermix computations of the deductive engine and the external\ndatabase, explaining its implementation and some optimizations. Finally, a\nperformance analysis is undertaken, comparing the system with current\nrelational database systems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 04:00:41 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["S\u00e1enz-P\u00e9rez", "Fernando", "", "Universidad Complutense de Madrid"]]}, {"id": "1501.02143", "submitter": "Morten St\\\"ockel", "authors": "Rasmus Pagh, Morten St\\\"ockel", "title": "Association Rule Mining using Maximum Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendations based on behavioral data may be faced with ambiguous\nstatistical evidence. We consider the case of association rules, relevant\ne.g.~for query and product recommendations. For example: Suppose that a\ncustomer belongs to categories A and B, each of which is known to have positive\ncorrelation with buying product C, how do we estimate the probability that she\nwill buy product C?\n  For rare terms or products there may not be enough data to directly produce\nsuch an estimate --- perhaps we never directly observed a connection between A,\nB, and C. What can we do when there is no support for estimating the\nprobability by simply computing the observed frequency? In particular, what is\nthe right thing to do when A and B give rise to very different estimates of the\nprobability of C?\n  We consider the use of maximum entropy probability estimates, which give a\nprincipled way of extrapolating probabilities of events that do not even occur\nin the data set! Focusing on the basic case of three variables, our main\ntechnical contributions are that (under mild assumptions): 1) There exists a\nsimple, explicit formula that gives a good approximation of maximum entropy\nestimates, and 2) Maximum entropy estimates based on a small number of samples\nare provably tightly concentrated around the true maximum entropy frequency\nthat arises if we let the number of samples go to infinity.\n  Our empirical work demonstrates the surprising precision of maximum entropy\nestimates, across a range of real-life transaction data sets. In particular we\nobserve the average absolute error on maximum entropy estimates is a factor\n$3$--$14$ less compared to using independence or extrapolation estimates, when\nthe data used to make the estimates has low support. We believe that the same\nprinciple can be used to synthesize probability estimates in many settings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 13:58:13 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Pagh", "Rasmus", ""], ["St\u00f6ckel", "Morten", ""]]}, {"id": "1501.02309", "submitter": "Haitao Wang", "authors": "Jian Li and Haitao Wang", "title": "Range Queries on Uncertain Data", "comments": "26 pages. A preliminary version of this paper appeared in ISAAC 2014.\n  In this full version, we also present solutions to the most general case of\n  the problem (i.e., the histogram bounded case), which were left as open\n  problems in the preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $P$ of $n$ uncertain points on the real line, each represented by\nits one-dimensional probability density function, we consider the problem of\nbuilding data structures on $P$ to answer range queries of the following three\ntypes for any query interval $I$: (1) top-$1$ query: find the point in $P$ that\nlies in $I$ with the highest probability, (2) top-$k$ query: given any integer\n$k\\leq n$ as part of the query, return the $k$ points in $P$ that lie in $I$\nwith the highest probabilities, and (3) threshold query: given any threshold\n$\\tau$ as part of the query, return all points of $P$ that lie in $I$ with\nprobabilities at least $\\tau$. We present data structures for these range\nqueries with linear or nearly linear space and efficient query time.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 04:18:08 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Li", "Jian", ""], ["Wang", "Haitao", ""]]}, {"id": "1501.02431", "submitter": "Rashmi Paithankar Ms", "authors": "Rashmi Paithankar and Bharat Tidke", "title": "A H-K Clustering Algorithm For High Dimensional Data Using Ensemble\n  Learning", "comments": "9 pages, 1 table, 2 figures, International Journal of Information\n  Technology Convergence and Services (IJITCS) Vol.4, No.5/6, December 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances made to the traditional clustering algorithms solves the various\nproblems such as curse of dimensionality and sparsity of data for multiple\nattributes. The traditional H-K clustering algorithm can solve the randomness\nand apriority of the initial centers of K-means clustering algorithm. But when\nwe apply it to high dimensional data it causes the dimensional disaster problem\ndue to high computational complexity. All the advanced clustering algorithms\nlike subspace and ensemble clustering algorithms improve the performance for\nclustering high dimension dataset from different aspects in different extent.\nStill these algorithms will improve the performance form a single perspective.\nThe objective of the proposed model is to improve the performance of\ntraditional H-K clustering and overcome the limitations such as high\ncomputational complexity and poor accuracy for high dimensional data by\ncombining the three different approaches of clustering algorithm as subspace\nclustering algorithm and ensemble clustering algorithm with H-K clustering\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 08:30:15 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Paithankar", "Rashmi", ""], ["Tidke", "Bharat", ""]]}, {"id": "1501.02560", "submitter": "Kuang Zhou", "authors": "Wiem Maalel (IRISA), Kuang Zhou (IRISA), Arnaud Martin (IRISA), Zied\n  Elouedi", "title": "Belief Hierarchical Clustering", "comments": null, "journal-ref": "3rd International Conference on Belief Functions, Sep 2014,\n  Oxford, United Kingdom. pp.68 - 76", "doi": "10.1007/978-3-319-11191-9_8", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the data mining field many clustering methods have been proposed, yet\nstandard versions do not take into account uncertain databases. This paper\ndeals with a new approach to cluster uncertain data by using a hierarchical\nclustering defined within the belief function framework. The main objective of\nthe belief hierarchical clustering is to allow an object to belong to one or\nseveral clusters. To each belonging, a degree of belief is associated, and\nclusters are combined based on the pignistic properties. Experiments with real\nuncertain data show that our proposed method can be considered as a propitious\ntool.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 07:55:41 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Maalel", "Wiem", "", "IRISA"], ["Zhou", "Kuang", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Elouedi", "Zied", ""]]}, {"id": "1501.02652", "submitter": "Kostas Stefanidis", "authors": "Yannis Roussakis, Ioannis Chrysakis, Kostas Stefanidis, Giorgos\n  Flouris, Yannis Stavrakas", "title": "A Flexible Framework for Defining, Representing and Detecting Changes on\n  the Data Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic nature of Web data gives rise to a multitude of problems related\nto the identification, computation and management of the evolving versions and\nthe related changes. In this paper, we consider the problem of change\nrecognition in RDF datasets, i.e., the problem of identifying, and when\npossible give semantics to, the changes that led from one version of an RDF\ndataset to another. Despite our RDF focus, our approach is sufficiently general\nto engulf different data models that can be encoded in RDF, such as relational\nor multi-dimensional. In fact, we propose a flexible, extendible and\ndata-model-independent methodology of defining changes that can capture the\npeculiarities and needs of different data models and applications, while being\nformally robust due to the satisfaction of the properties of completeness and\nunambiguity. Further, we propose an ontology of changes for storing the\ndetected changes that allows automated processing and analysis of changes,\ncross-snapshot queries (spanning across different versions), as well as queries\ninvolving both changes and data. To detect changes and populate said ontology,\nwe propose a customizable detection algorithm, which is applicable to different\ndata models and applications requiring the detection of custom, user-defined\nchanges. Finally, we provide a proof-of-concept application and evaluation of\nour framework for different data models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 14:15:35 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Roussakis", "Yannis", ""], ["Chrysakis", "Ioannis", ""], ["Stefanidis", "Kostas", ""], ["Flouris", "Giorgos", ""], ["Stavrakas", "Yannis", ""]]}, {"id": "1501.03935", "submitter": "Fedor Chernogorov Mr.", "authors": "Fedor Chernogorov, Sergey Chernov, Kimmo Brigatti, Tapani Ristaniemi", "title": "Sequence-based Detection of Sleeping Cell Failures in Mobile Networks", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an automatic malfunction detection framework based on\ndata mining approach to analysis of network event sequences. The considered\nenvironment is Long Term Evolution (LTE) for Universal Mobile Telecommunication\nSystem (UMTS) with sleeping cell caused by random access channel failure.\nSleeping cell problem means unavailability of network service without triggered\nalarm. The proposed detection framework uses N-gram analysis for identification\nof abnormal behavior in sequences of network events. These events are collected\nwith Minimization of Drive Tests (MDT) functionality standardized in LTE.\nFurther processing applies dimensionality reduction, anomaly detection with\nk-nearest neighbor, cross-validation, post-processing techniques and efficiency\nevaluation. Different anomaly detection approaches proposed in this paper are\ncompared against each other with both classic data mining metrics, such as\nF-score and receiver operating characteristic curves, and a newly proposed\nheuristic approach. Achieved results demonstrate that the suggested method can\nbe used in modern performance monitoring systems for reliable, timely and\nautomatic detection of random access channel sleeping cells.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 10:21:14 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 07:09:50 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Chernogorov", "Fedor", ""], ["Chernov", "Sergey", ""], ["Brigatti", "Kimmo", ""], ["Ristaniemi", "Tapani", ""]]}, {"id": "1501.04038", "submitter": "Benjamin McCamish", "authors": "Ben McCamish, Rich Meier, Jordan Landford, Robert Bass, Eduardo\n  Cotilla-Sanchez, David Chiu", "title": "A Backend Framework for the Efficient Management of Power System\n  Measurements", "comments": "Published in Electric Power Systems Research (2016), not available\n  yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased adoption and deployment of phasor measurement units (PMU) has\nprovided valuable fine-grained data over the grid. Analysis over these data can\nprovide insight into the health of the grid, thereby improving control over\noperations. Realizing this data-driven control, however, requires validating,\nprocessing and storing massive amounts of PMU data. This paper describes a PMU\ndata management system that supports input from multiple PMU data streams,\nfeatures an event-detection algorithm, and provides an efficient method for\nretrieving archival data. The event-detection algorithm rapidly correlates\nmultiple PMU data streams, providing details on events occurring within the\npower system. The event-detection algorithm feeds into a visualization\ncomponent, allowing operators to recognize events as they occur. The indexing\nand data retrieval mechanism facilitates fast access to archived PMU data.\nUsing this method, we achieved over 30x speedup for queries with high\nselectivity. With the development of these two components, we have developed a\nsystem that allows efficient analysis of multiple time-aligned PMU data\nstreams.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 20:16:52 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 17:07:40 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["McCamish", "Ben", ""], ["Meier", "Rich", ""], ["Landford", "Jordan", ""], ["Bass", "Robert", ""], ["Cotilla-Sanchez", "Eduardo", ""], ["Chiu", "David", ""]]}, {"id": "1501.04186", "submitter": "Josep Domingo-Ferrer", "authors": "Josep Domingo-Ferrer, Krishnamurty Muralidhar", "title": "New Directions in Anonymization: Permutation Paradigm, Verifiability by\n  Subjects and Intruders, Transparency to Users", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are currently two approaches to anonymization: \"utility first\" (use an\nanonymization method with suitable utility features, then empirically evaluate\nthe disclosure risk and, if necessary, reduce the risk by possibly sacrificing\nsome utility) or \"privacy first\" (enforce a target privacy level via a privacy\nmodel, e.g., k-anonymity or epsilon-differential privacy, without regard to\nutility). To get formal privacy guarantees, the second approach must be\nfollowed, but then data releases with no utility guarantees are obtained. Also,\nin general it is unclear how verifiable is anonymization by the data subject\n(how safely released is the record she has contributed?), what type of intruder\nis being considered (what does he know and want?) and how transparent is\nanonymization towards the data user (what is the user told about methods and\nparameters used?).\n  We show that, using a generally applicable reverse mapping transformation,\nany anonymization for microdata can be viewed as a permutation plus (perhaps) a\nsmall amount of noise; permutation is thus shown to be the essential principle\nunderlying any anonymization of microdata, which allows giving simple utility\nand privacy metrics. From this permutation paradigm, a new privacy model\nnaturally follows, which we call (d,v)-permuted privacy. The privacy ensured by\nthis method can be verified by each subject contributing an original record\n(subject-verifiability) and also at the data set level by the data protector.\nWe then proceed to define a maximum-knowledge intruder model, which we argue\nshould be the one considered in anonymization. Finally, we make the case for\nanonymization transparent to the data user, that is, compliant with Kerckhoff's\nassumption (only the randomness used, if any, must stay secret).\n", "versions": [{"version": "v1", "created": "Sat, 17 Jan 2015 10:25:33 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Domingo-Ferrer", "Josep", ""], ["Muralidhar", "Krishnamurty", ""]]}, {"id": "1501.04826", "submitter": "Thorsten Wissmann", "authors": "Albert Atserias and Jos\\'e L. Balc\\'azar and Marie Ely Piceno", "title": "Relative Entailment Among Probabilistic Implications", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 1 (February\n  6, 2019) lmcs:5171", "doi": "10.23638/LMCS-15(1:10)2019", "report-no": null, "categories": "cs.LO cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a natural variant of the implicational fragment of propositional\nlogic. Its formulas are pairs of conjunctions of positive literals, related\ntogether by an implicational-like connective; the semantics of this sort of\nimplication is defined in terms of a threshold on a conditional probability of\nthe consequent, given the antecedent: we are dealing with what the data\nanalysis community calls confidence of partial implications or association\nrules. Existing studies of redundancy among these partial implications have\ncharacterized so far only entailment from one premise and entailment from two\npremises, both in the stand-alone case and in the case of presence of\nadditional classical implications (this is what we call \"relative entailment\").\nBy exploiting a previously noted alternative view of the entailment in terms of\nlinear programming duality, we characterize exactly the cases of entailment\nfrom arbitrary numbers of premises, again both in the stand-alone case and in\nthe case of presence of additional classical implications. As a result, we\nobtain decision algorithms of better complexity; additionally, for each\npotential case of entailment, we identify a critical confidence threshold and\nshow that it is, actually, intrinsic to each set of premises and antecedent of\nthe conclusion.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 14:41:36 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 12:02:40 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 09:37:28 GMT"}, {"version": "v4", "created": "Mon, 3 Sep 2018 17:02:21 GMT"}, {"version": "v5", "created": "Tue, 5 Feb 2019 11:44:54 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Atserias", "Albert", ""], ["Balc\u00e1zar", "Jos\u00e9 L.", ""], ["Piceno", "Marie Ely", ""]]}, {"id": "1501.04832", "submitter": "Robert Jeansoulin", "authors": "Robert Jeansoulin", "title": "Big Data: How Geo-information Helped Shape the Future of Data\n  Engineering", "comments": "Conference \"AutoCarto 6\", revisited 30 years later in a\n  \"Retrospective book\", edited by Barry Wellar, the same chair as the original\n  conference. 12 pages, 6 figures. see: AutoCarto Six Retrospective, 2013.\n  ISBN: 978-0-9921435-0-3, pages 190-201", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Very large data sets are the common rule in automated mapping, GIS, remote\nsensing, and what we can name geo-information. Indeed, in 1983 Landsat was\nalready delivering gigabytes of data, and other sensors were in orbit or ready\nfor launch, and a tantamount of cartographic data was being digitized. The\nretrospective paper revisits several issues that geo-information sciences had\nto face from the early stages on, including: structure ( to bring some\nstructure to the data registered from a sampled signal, metadata); processing\n(huge amounts of data for big computers and fast algorithms); uncertainty (the\nkinds of errors, their quantification); consistency (when merging different\nsources of data is logically allowed, and meaningful); ontologies (clear and\nagreed shared definitions, if any kind of decision should be based upon them).\nAll these issues are the background of Internet queries, and the underlying\ntechnology has been shaped during those years when geo-information engineering\nemerged.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 14:57:11 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Jeansoulin", "Robert", ""]]}, {"id": "1501.05039", "submitter": "Yun Xiong", "authors": "Yangyong Zhu and Yun Xiong", "title": "Defining Data Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science is gaining more and more and widespread attention, but no\nconsensus viewpoint on what data science is has emerged. As a new science, its\nobjects of study and scientific issues should not be covered by established\nsciences. Data in cyberspace have formed what we call datanature. In the\npresent paper, data science is defined as the science of exploring datanature.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 02:41:55 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Zhu", "Yangyong", ""], ["Xiong", "Yun", ""]]}, {"id": "1501.05265", "submitter": "Micha\\\"el Marcozzi", "authors": "Micha\\\"el Marcozzi and Wim Vanhoof and Jean-Luc Hainaut", "title": "A Direct Symbolic Execution of SQL Code for Testing of Data-Oriented\n  Applications", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic execution is a technique which enables automatically generating test\ninputs (and outputs) exercising a set of execution paths within a program to be\ntested. If the paths cover a sufficient part of the code under test, the test\ndata offer a representative view of the program's actual behaviour, which\nnotably enables detecting errors and correcting faults. Relational databases\nare ubiquitous in software, but symbolic execution of pieces of code that\nmanipulate them remains a non-trivial problem, particularly because of the\ncomplex structure of such databases and the complex behaviour of SQL\nstatements. In this work, we define a direct symbolic execution for database\nmanipulation code and integrate it with a more traditional symbolic execution\nof normal program code. The database tables are represented by relational\nsymbols and the SQL statements by relational constraints over these symbols and\nthe symbols representing the normal variables of the program. An algorithm\nbased on these principles is presented for the symbolic execution of Java\nmethods that implement business use cases by reading and writing in a\nrelational database, the latter subject to data integrity constraints. The\nalgorithm is integrated in a test generation tool and experimented over sample\ncode. The target language for the constraints produced by the tool is the\nSMT-Lib standard and the used solver is Microsoft Z3. The results show that the\nproposed approach enables generating meaningful test data, including valid\ndatabase content, in reasonable time. In particular, the Z3 solver is shown to\nbe more scalable than the Alloy solver, used in our previous work, for solving\nrelational constraints.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 19:12:12 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Marcozzi", "Micha\u00ebl", ""], ["Vanhoof", "Wim", ""], ["Hainaut", "Jean-Luc", ""]]}, {"id": "1501.05290", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves", "title": "Managing large-scale scientific hypotheses as uncertain and\n  probabilistic data", "comments": "145 pages, 61 figures, 1 table. PhD thesis, National Laboratory for\n  Scientific Computing (LNCC), Brazil, February 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view of the paradigm shift that makes science ever more data-driven, in\nthis thesis we propose a synthesis method for encoding and managing large-scale\ndeterministic scientific hypotheses as uncertain and probabilistic data.\n  In the form of mathematical equations, hypotheses symmetrically relate\naspects of the studied phenomena. For computing predictions, however,\ndeterministic hypotheses can be abstracted as functions. We build upon Simon's\nnotion of structural equations in order to efficiently extract the (so-called)\ncausal ordering between variables, implicit in a hypothesis structure (set of\nmathematical equations).\n  We show how to process the hypothesis predictive structure effectively\nthrough original algorithms for encoding it into a set of functional\ndependencies (fd's) and then performing causal reasoning in terms of acyclic\npseudo-transitive reasoning over fd's. Such reasoning reveals important causal\ndependencies implicit in the hypothesis predictive data and guide our synthesis\nof a probabilistic database. Like in the field of graphical models in AI, such\na probabilistic database should be normalized so that the uncertainty arisen\nfrom competing hypotheses is decomposed into factors and propagated properly\nonto predictive data by recovering its joint probability distribution through a\nlossless join. That is motivated as a design-theoretic principle for\ndata-driven hypothesis management and predictive analytics.\n  The method is applicable to both quantitative and qualitative deterministic\nhypotheses and demonstrated in realistic use cases from computational science.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 20:46:23 GMT"}, {"version": "v2", "created": "Thu, 12 Feb 2015 20:52:29 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""]]}, {"id": "1501.05546", "submitter": "Ashley Conard", "authors": "Ashley Mae Conard, Stephanie Dodson, Jeremy Kepner, Darrell Ricke", "title": "Using a Big Data Database to Identify Pathogens in Protein Data Space", "comments": "2 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current metagenomic analysis algorithms require significant computing\nresources, can report excessive false positives (type I errors), may miss\norganisms (type II errors / false negatives), or scale poorly on large\ndatasets. This paper explores using big data database technologies to\ncharacterize very large metagenomic DNA sequences in protein space, with the\nultimate goal of rapid pathogen identification in patient samples. Our approach\nuses the abilities of a big data databases to hold large sparse associative\narray representations of genetic data to extract statistical patterns about the\ndata that can be used in a variety of ways to improve identification\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 15:58:42 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Conard", "Ashley Mae", ""], ["Dodson", "Stephanie", ""], ["Kepner", "Jeremy", ""], ["Ricke", "Darrell", ""]]}, {"id": "1501.05709", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Julian Chaidez, Vijay Gadepally, Hayden Jansen", "title": "Associative Arrays: Unified Mathematics for Spreadsheets, Databases,\n  Matrices, and Graphs", "comments": "4 pages, 6 figures; New England Database Summit 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data processing systems impose multiple views on data as it is processed by\nthe system. These views include spreadsheets, databases, matrices, and graphs.\nThe common theme amongst these views is the need to store and operate on data\nas whole sets instead of as individual data elements. This work describes a\ncommon mathematical representation of these data sets (associative arrays) that\napplies across a wide range of applications and technologies. Associative\narrays unify and simplify these different approaches for representing and\nmanipulating data into common two-dimensional view of data. Specifically,\nassociative arrays (1) reduce the effort required to pass data between steps in\na data processing system, (2) allow steps to be interchanged with full\nconfidence that the results will be unchanged, and (3) make it possible to\nrecognize when steps can be simplified or eliminated. Most database system\nnaturally support associative arrays via their tabular interfaces. The D4M\nimplementation of associative arrays uses this feature to provide a common\ninterface across SQL, NoSQL, and NewSQL databases.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 04:16:04 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Kepner", "Jeremy", ""], ["Chaidez", "Julian", ""], ["Gadepally", "Vijay", ""], ["Jansen", "Hayden", ""]]}, {"id": "1501.05916", "submitter": "Nafees Qamar", "authors": "Nafees Qamar, Yilong Yang, Andras Nadas, Zhiming Liu, and Janos\n  Sztipanovits", "title": "Anonymously Analyzing Clinical Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper takes on the problem of automatically identifying\nclinically-relevant patterns in medical datasets without compromising patient\nprivacy. To achieve this goal, we treat datasets as a black box for both\ninternal and external users of data that lets us handle clinical data queries\ndirectly and far more efficiently. The novelty of the approach lies in avoiding\nthe data de-identification process often used as a means of preserving patient\nprivacy. The implemented toolkit combines software engineering technologies\nsuch as Java EE and RESTful web services, to allow exchanging medical data in\nan unidentifiable XML format as well as restricting users to the need-to-know\nprinciple. Our technique also inhibits retrospective processing of data, such\nas attacks by an adversary on a medical dataset using advanced computational\nmethods to reveal Protected Health Information (PHI). The approach is validated\non an endoscopic reporting application based on openEHR and MST standards. From\nthe usability perspective, the approach can be used to query datasets by\nclinical researchers, governmental or non-governmental organizations in\nmonitoring health care services to improve quality of care.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 07:53:34 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Qamar", "Nafees", ""], ["Yang", "Yilong", ""], ["Nadas", "Andras", ""], ["Liu", "Zhiming", ""], ["Sztipanovits", "Janos", ""]]}, {"id": "1501.06206", "submitter": "Radhakrishnan Delhibabu", "authors": "Radhakrishnan Delhibabu", "title": "Dynamics of Belief: Abduction, Horn Knowledge Base And Database Updates", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.2499,\n  arXiv:1405.2642, arXiv:1407.3512, arXiv:1301.5154", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The dynamics of belief and knowledge is one of the major components of any\nautonomous system that should be able to incorporate new pieces of information.\nIn order to apply the rationality result of belief dynamics theory to various\npractical problems, it should be generalized in two respects: first it should\nallow a certain part of belief to be declared as immutable; and second, the\nbelief state need not be deductively closed. Such a generalization of belief\ndynamics, referred to as base dynamics, is presented in this paper, along with\nthe concept of a generalized revision algorithm for knowledge bases (Horn or\nHorn logic with stratified negation). We show that knowledge base dynamics has\nan interesting connection with kernel change via hitting set and abduction. In\nthis paper, we show how techniques from disjunctive logic programming can be\nused for efficient (deductive) database updates. The key idea is to transform\nthe given database together with the update request into a disjunctive\n(datalog) logic program and apply disjunctive techniques (such as minimal model\nreasoning) to solve the original update problem. The approach extends and\nintegrates standard techniques for efficient query answering and integrity\nchecking. The generation of a hitting set is carried out through a hyper\ntableaux calculus and magic set that is focused on the goal of minimality. The\npresent paper provides a comparative study of view update algorithms in\nrational approach. For, understand the basic concepts with abduction, we\nprovide an abductive framework for knowledge base dynamics. Finally, we\ndemonstrate how belief base dynamics can provide an axiomatic characterization\nfor insertion a view atom to the database. We give a quick overview of the main\noperators for belief change, in particular, belief update versus database\nupdate.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 20:48:53 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 09:59:49 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Delhibabu", "Radhakrishnan", ""]]}, {"id": "1501.06456", "submitter": "Sanjay Chakraborty", "authors": "Ratul Dey Sanjay Chakraborty Lopamudra Dey", "title": "Weather forecasting using Convex hull & K-Means Techniques An Approach", "comments": "1st International Science & Technology Congress(IEMCON-2015) Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is a popular concept of mined necessary data from a large set of\ndata. Data mining using clustering is a powerful way to analyze data and gives\nprediction. In this paper non structural time series data is used to forecast\ndaily average temperature, humidity and overall weather conditions of Kolkata\ncity. The air pollution data have been taken from West Bengal Pollution Control\nBoard to build the original dataset on which the prediction approach of this\npaper is studied and applied. This paper describes a new technique to predict\nthe weather conditions using convex hull which gives structural data and then\napply incremental K-means to define the appropriate clusters. It splits the\ntotal database into four separate databases with respect to different weather\nconditions. In the final step, the result will be calculated on the basis of\npriority based protocol which is defined based on some mathematical deduction.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 16:12:01 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Dey", "Ratul Dey Sanjay Chakraborty Lopamudra", ""]]}, {"id": "1501.06689", "submitter": "Daniel Zinn", "authors": "Daniel Zinn", "title": "General-Purpose Join Algorithms for Listing Triangles in Large Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate applying general-purpose join algorithms to the triangle\nlisting problem in an out-of-core context. In particular, we focus on Leapfrog\nTriejoin (LFTJ) by Veldhuizen 2014, a recently proposed, worst-case optimal\nalgorithm. We present \"boxing\": a novel, yet conceptually simple, approach for\nfeeding input data to LFTJ. Our extensive analysis shows that this approach is\nI/O efficient, being worst-case optimal (in a certain sense). Furthermore, if\ninput data is only a constant factor larger than the available memory, then a\nboxed LFTJ essentially maintains the CPU data-complexity of the vanilla LFTJ.\nNext, focusing on LFTJ applied to the triangle query, we show that for many\ngraphs boxed LFTJ matches the I/O complexity of the recently by Hu, Tao and\nYufei proposed specialized algorithm MGT for listing tiangles in an out-of-core\nsetting. We also strengthen the analysis of LFTJ's computational complexity for\nthe triangle query by considering families of input graphs that are\ncharacterized not only by the number of edges but also by a measure of their\ndensity. E.g., we show that LFTJ achieves a CPU complexity of O(|E|log|E|) for\nplanar graphs, while on general graphs, no algorithm can be faster than\nO(|E|^{1.5}). Finally, we perform an experimental evaluation for the triangle\nlisting problem confirming our theoretical results and showing the overall\neffectiveness of our approach. On all our real-world and synthetic data sets\n(some of which containing more than 1.2 billion edges) LFTJ in single-threaded\nmode is within a factor of 3 of the specialized MGT; a penalty that---as we\ndemonstrate---can be alleviated by parallelization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 08:47:25 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Zinn", "Daniel", ""]]}, {"id": "1501.06758", "submitter": "Shantanu Sharma", "authors": "Foto Afrati, Shlomi Dolev, Ephraim Korach, Shantanu Sharma, Jeffrey D.\n  Ullman", "title": "Assignment of Different-Sized Inputs in MapReduce", "comments": "Brief announcement in International Symposium on Distributed\n  Computing (DISC), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A MapReduce algorithm can be described by a mapping schema, which assigns\ninputs to a set of reducers, such that for each required output there exists a\nreducer that receives all the inputs that participate in the computation of\nthis output. Reducers have a capacity, which limits the sets of inputs that\nthey can be assigned. However, individual inputs may vary in terms of size. We\nconsider, for the first time, mapping schemas where input sizes are part of the\nconsiderations and restrictions. One of the significant parameters to optimize\nin any MapReduce job is communication cost between the map and reduce phases.\nThe communication cost can be optimized by minimizing the number of copies of\ninputs sent to the reducers. The communication cost is closely related to the\nnumber of reducers of constrained capacity that are used to accommodate\nappropriately the inputs, so that the requirement of how the inputs must meet\nin a reducer is satisfied. In this work, we consider a family of problems where\nit is required that each input meets with each other input in at least one\nreducer. We also consider a slightly different family of problems in which,\neach input of a set, X, is required to meet each input of another set, Y, in at\nleast one reducer. We prove that finding an optimal mapping schema for these\nfamilies of problem is NP-hard, and present several approximation algorithms\nfor finding a near optimal mapping schema.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 12:37:21 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Afrati", "Foto", ""], ["Dolev", "Shlomi", ""], ["Korach", "Ephraim", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""]]}, {"id": "1501.06964", "submitter": "Usha Keshavamurthy", "authors": "Usha Keshavamurthy, H. S. Guruprasad", "title": "Learning Analytics: A Survey", "comments": null, "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  Volume 18 Number 6 Dec 2014 Page 260 - 264", "doi": "10.14445/22312803/IJCTT-V18P155", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning analytics is a research topic that is gaining increasing popularity\nin recent time. It analyzes the learning data available in order to make aware\nor improvise the process itself and/or the outcome such as student performance.\nIn this survey paper, we look at the recent research work that has been\nconducted around learning analytics, framework and integrated models, and\napplication of various models and data mining techniques to identify students\nat risk and to predict student performance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 11:03:12 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Keshavamurthy", "Usha", ""], ["Guruprasad", "H. S.", ""]]}, {"id": "1501.07020", "submitter": "Alireza Khoshkbarforoushha", "authors": "Alireza Khoshkbarforoushha, Rajiv Ranjan, Raj Gaire, Prem P.\n  Jayaraman, John Hosking, Ehsan Abbasnejad", "title": "Resource Usage Estimation of Data Stream Processing Workloads in\n  Datacenter Clouds", "comments": "Working Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time computation of data streams over affordable virtualized\ninfrastructure resources is an important form of data in motion processing\narchitecture. However, processing such data streams while ensuring strict\nguarantees on quality of services is problematic due to: (i) uncertain stream\narrival pattern; (ii) need of processing different types of continuous queries;\nand (iii) variable resource consumption behavior of continuous queries. Recent\nwork has explored the use of statistical techniques for resource estimation of\nSQL queries and OLTP workloads. All these techniques approximate resource usage\nfor each query as a single point value. However, in data stream processing\nworkloads in which data flows through the graph of operators endlessly and\nposes performance and resource demand fluctuations, the single point resource\nestimation is inadequate. Because it is neither expressive enough nor does it\ncapture the multi-modal nature of the target data. To this end, we present a\nnovel technique which uses mixture density networks, a combined structure of\nneural networks and mixture models, to estimate the whole spectrum of resource\nusage as probability density functions. The proposed approach is a flexible and\nconvenient means of modeling unknown distribution models. We have validated the\nmodels using both the linear road benchmark and the TPC-H, observing high\naccuracy under a number of error metrics: mean-square error, continuous ranked\nprobability score, and negative log predictive density.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 08:11:10 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Khoshkbarforoushha", "Alireza", ""], ["Ranjan", "Rajiv", ""], ["Gaire", "Raj", ""], ["Jayaraman", "Prem P.", ""], ["Hosking", "John", ""], ["Abbasnejad", "Ehsan", ""]]}, {"id": "1501.07184", "submitter": "Zehra Meral Ozsoyoglu", "authors": "Shi Qiao, Z. Meral Ozsoyoglu", "title": "One Size Does not Fit All: When to Use Signature-based Pruning to\n  Improve Template Matching for RDF graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signature-based pruning is broadly accepted as an effective way to improve\nquery performance of graph template matching on general labeled graphs. Most\nexisting techniques which utilize signature-based pruning claim its benefits on\nall datasets and queries. However, the effectiveness of signature-based pruning\nvaries greatly among different RDF datasets and highly related with their\ndataset characteristics. We observe that the performance benefits from\nsignature-based pruning depend not only on the size of the RDF graphs, but also\nthe underlying graph structure and the complexity of queries. This motivates us\nto propose a flexible RDF querying framework, called RDF-h, which selectively\nutilizes signature-based pruning by evaluating the characteristics of RDF\ndatasets and query templates. Scalability and efficiency of RDF-h is\ndemonstrated in experimental results using both real and synthetic datasets.\n  Keywords: RDF, Graph Template Matching, Signature-based Pruning\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 16:49:18 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Qiao", "Shi", ""], ["Ozsoyoglu", "Z. Meral", ""]]}, {"id": "1501.07864", "submitter": "Paraschos Koutris", "authors": "Paraschos Koutris and Jef Wijsen", "title": "A Trichotomy in the Data Complexity of Certain Query Answering for\n  Conjunctive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A relational database is said to be uncertain if primary key constraints can\npossibly be violated. A repair (or possible world) of an uncertain database is\nobtained by selecting a maximal number of tuples without ever selecting two\ndistinct tuples with the same primary key value. For any Boolean query q,\nCERTAINTY(q) is the problem that takes an uncertain database db on input, and\nasks whether q is true in every repair of db. The complexity of this problem\nhas been particularly studied for q ranging over the class of self-join-free\nBoolean conjunctive queries. A research challenge is to determine, given q,\nwhether CERTAINTY(q) belongs to complexity classes FO, P, or coNP-complete. In\nthis paper, we combine existing techniques for studying the above complexity\nclassification task. We show that for any self-join-free Boolean conjunctive\nquery q, it can be decided whether or not CERTAINTY(q) is in FO. Further, for\nany self-join-free Boolean conjunctive query q, CERTAINTY(q) is either in P or\ncoNP-complete, and the complexity dichotomy is effective. This settles a\nresearch question that has been open for ten years, since [9].\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 17:56:11 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Koutris", "Paraschos", ""], ["Wijsen", "Jef", ""]]}]