[{"id": "1701.00072", "submitter": "Ashish Sureka", "authors": "Jeevan Joishi, Ashish Sureka", "title": "Graph or Relational Databases: A Speed Comparison for Process Mining\n  Algorithm", "comments": "Extended and Detailed Version of the IDEAS '15 paper by Jeevan Joishi\n  and Ashish Sureka", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process-Aware Information System (PAIS) are IT systems that manages, supports\nbusiness processes and generate large event logs from execution of business\nprocesses. An event log is represented as a tuple of the form CaseID,\nTimeStamp, Activity and Actor. Process Mining is an emerging area of research\nthat deals with the study and analysis of business processes based on event\nlogs. Process Mining aims at analyzing event logs and discover business process\nmodels, enhance them or check for conformance with an a priori model. The large\nvolume of event logs generated are stored in databases. Relational databases\nperform well for certain class of applications. However, there are certain\nclass of applications for which relational databases are not able to scale. A\nnumber of NoSQL databases have emerged to encounter the challenges of\nscalability. Discovering social network from event logs is one of the most\nchallenging and important Process Mining task. Similar-Task and Sub-Contract\nalgorithms are some of the most widely used Organizational Mining techniques.\nOur objective is to investigate which of the databases (Relational or Graph)\nperform better for Organizational Mining under Process Mining. An intersection\nof Process Mining and Graph Databases can be accomplished by modelling these\nOrganizational Mining metrics with graph databases. We implement Similar-Task\nand Sub-Contract algorithms on relational and NoSQL (graph-oriented) databases\nusing only query language constructs. We conduct empirical analysis on a large\nreal world data set to compare the performance of row-oriented database and\nNoSQL graph-oriented database. We benchmark performance factors like query\nexecution time, CPU usage and disk/memory space usage for NoSQL graph-oriented\ndatabase against row-oriented database.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 08:00:07 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Joishi", "Jeevan", ""], ["Sureka", "Ashish", ""]]}, {"id": "1701.00398", "submitter": "Jerome Darmont", "authors": "Omar Boussaid (ERIC), Jerome Darmont (ERIC), Fadila Bentayeb (ERIC),\n  Sabine Loudcher (ERIC)", "title": "Warehousing complex data from the Web", "comments": null, "journal-ref": "Int. J. Web Engineering and Technology, 2008, 4 (4), pp.408-433", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data warehousing and OLAP technologies are now moving onto handling\ncomplex data that mostly originate from the Web. However, intagrating such data\ninto a decision-support process requires their representation under a form\nprocessable by OLAP and/or data mining techniques. We present in this paper a\ncomplex data warehousing methodology that exploits XML as a pivot language. Our\napproach includes the integration of complex data in an ODS, under the form of\nXML documents; their dimensional modeling and storage in an XML data warehouse;\nand their analysis with combined OLAP and data mining techniques. We also\naddress the crucial issue of performance in XML warehouses.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 14:18:01 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Boussaid", "Omar", "", "ERIC"], ["Darmont", "Jerome", "", "ERIC"], ["Bentayeb", "Fadila", "", "ERIC"], ["Loudcher", "Sabine", "", "ERIC"]]}, {"id": "1701.00399", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (ERIC), Fadila Bentayeb (ERIC), Omar Boussa\\\"id\n  (ERIC)", "title": "Benchmarking data warehouses", "comments": "arXiv admin note: text overlap with arXiv:0705.1453", "journal-ref": "International Journal of Business Intelligence and Data Mining,\n  Inderscience, 2007, 2 (1), pp.79-104", "doi": "10.1504/IJBIDM.2007.012947", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data warehouse architectural choices and optimization techniques are critical\nto decision support query performance. To facilitate these choices, the\nperformance of the designed data warehouse must be assessed, usually with\nbenchmarks. These tools can either help system users comparing the performances\nof different systems, or help system engineers testing the effect of various\ndesign choices. While the Transaction Processing Performance Council's standard\nbenchmarks address the first point, they are not tunable enough to address the\nsecond one and fail to model different data warehouse schemas. By contrast, our\nData Warehouse Engineering Benchmark (DWEB) allows generating various ad-hoc\nsynthetic data warehouses and workloads. DWEB is implemented as a Java free\nsoftware that can be interfaced with most existing relational database\nmanagement systems. The full specifications of DWEB, as well as experiments we\nperformed to illustrate how our benchmark may be used, are provided in this\npaper.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 14:19:24 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Bentayeb", "Fadila", "", "ERIC"], ["Boussa\u00efd", "Omar", "", "ERIC"]]}, {"id": "1701.00400", "submitter": "Jerome Darmont", "authors": "Zhen He, J\\'er\\^ome Darmont (ERIC)", "title": "Evaluating the Dynamic Behavior of Database Applications", "comments": "arXiv admin note: text overlap with arXiv:0705.1454", "journal-ref": "Journal of Database Management, IGI Global, 2005, 16 (2), pp.21 -\n  45", "doi": "10.4018/jdm.2005040102", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the effect that changing access patterns has on the\nperformance of database management systems. Changes in access patterns play an\nimportant role in determining the efficiency of key performance optimization\ntechniques, such as dynamic clustering, prefetching, and buffer replacement.\nHowever, all existing benchmarks or evaluation frameworks produce static access\npatterns in which objects are always accessed in the same order repeatedly.\nHence, we have proposed the Dynamic Evaluation Framework (DEF) that simulates\naccess pattern changes using configurable styles of change. DEF has been\ndesigned to be open and fully extensible (e.g., new access pattern change\nmodels can be added easily). In this paper, we instantiate DEF into the Dynamic\nobject Evaluation Framework (DoEF) which is designed for object databases,\ni.e., object-oriented or object-relational databases such as multi-media\ndatabases or most XML databases.The capabilities of DoEF have been evaluated by\nsimulating the execution of four different dynamic clustering algorithms. The\nresults confirm our analysis that flexible conservative re-clustering is the\nkey in determining a clustering algorithm's ability to adapt to changes in\naccess pattern. These results show the effectiveness of DoEF at determining the\nadaptability of each dynamic clustering algorithm to changes in access pattern\nin a simulation environment. In a second set of experiments, we have used DoEF\nto compare the performance of two real-life object stores : Platypus and SHORE.\nDoEF has helped to reveal the poor swapping performance of Platypus.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 14:20:12 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["He", "Zhen", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.00622", "submitter": "EPTCS", "authors": "Dietmar Seipel (University of W\\\"urzburg)", "title": "Knowledge Engineering for Hybrid Deductive Databases", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 1-12", "doi": "10.4204/EPTCS.234.1", "report-no": null, "categories": "cs.DB cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern knowledge base systems frequently need to combine a collection of\ndatabases in different formats: e.g., relational databases, XML databases, rule\nbases, ontologies, etc. In the deductive database system DDBASE, we can manage\nthese different formats of knowledge and reason about them. Even the file\nsystems on different computers can be part of the knowledge base. Often, it is\nnecessary to handle different versions of a knowledge base. E.g., we might want\nto find out common parts or differences of two versions of a relational\ndatabase.\n  We will examine the use of abstractions of rule bases by predicate dependency\nand rule predicate graphs. Also the proof trees of derived atoms can help to\ncompare different versions of a rule base. Moreover, it might be possible to\nhave derivations joining rules with other formalisms of knowledge\nrepresentation.\n  Ontologies have shown their benefits in many applications of intelligent\nsystems, and there have been many proposals for rule languages compatible with\nthe semantic web stack, e.g., SWRL, the semantic web rule language. Recently,\nontologies are used in hybrid systems for specifying the provenance of the\ndifferent components.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:30:37 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Seipel", "Dietmar", "", "University of W\u00fcrzburg"]]}, {"id": "1701.00623", "submitter": "EPTCS", "authors": "Stefan Brass (University of Halle), Heike Stephan (University of\n  Halle)", "title": "Bottom-Up Evaluation of Datalog: Preliminary Report", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 13-26", "doi": "10.4204/EPTCS.234.2", "report-no": null, "categories": "cs.DB cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottom-up evaluation of Datalog has been studied for a long time, and is\nstandard material in textbooks. However, if one actually wants to develop a\ndeductive database system, it turns out that there are many implementation\noptions. For instance, the sequence in which rule instances are applied is not\ngiven. In this paper, we study a method that immediately uses a derived tuple\nto derive more tuples (called the Push method). In this way, storage space for\nintermediate results can be reduced. The main contribution of our method is the\nway in which we minimize the copying of values at runtime, and do much work\nalready at compile-time.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:30:51 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Brass", "Stefan", "", "University of Halle"], ["Stephan", "Heike", "", "University of\n  Halle"]]}, {"id": "1701.00626", "submitter": "EPTCS", "authors": "Falco Nogatz, Dietmar Seipel", "title": "Implementing GraphQL as a Query Language for Deductive Databases in\n  SWI-Prolog Using DCGs, Quasi Quotations, and Dicts", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 42-56", "doi": "10.4204/EPTCS.234.4", "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The methods to access large relational databases in a distributed system are\nwell established: the relational query language SQL often serves as a language\nfor data access and manipulation, and in addition public interfaces are exposed\nusing communication protocols like REST. Similarly to REST, GraphQL is the\nquery protocol of an application layer developed by Facebook. It provides a\nunified interface between the client and the server for data fetching and\nmanipulation. Using GraphQL's type system, it is possible to specify data\nhandling of various sources and to combine, e.g., relational with NoSQL\ndatabases. In contrast to REST, GraphQL provides a single API endpoint and\nsupports flexible queries over linked data.\n  GraphQL can also be used as an interface for deductive databases. In this\npaper, we give an introduction of GraphQL and a comparison to REST. Using\nlanguage features recently added to SWI-Prolog 7, we have developed the Prolog\nlibrary GraphQL.pl, which implements the GraphQL type system and query syntax\nas a domain-specific language with the help of definite clause grammars (DCG),\nquasi quotations, and dicts. Using our library, the type system created for a\ndeductive database can be validated, while the query system provides a unified\ninterface for data access and introspection.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:31:26 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Nogatz", "Falco", ""], ["Seipel", "Dietmar", ""]]}, {"id": "1701.00627", "submitter": "EPTCS", "authors": "Stefan Brass (University of Halle), Heike Stephan (University of\n  Halle)", "title": "Experiences with Some Benchmarks for Deductive Databases and\n  Implementations of Bottom-Up Evaluation", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 57-72", "doi": "10.4204/EPTCS.234.5", "report-no": null, "categories": "cs.DB cs.LO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenRuleBench is a large benchmark suite for rule engines, which includes\ndeductive databases. We previously proposed a translation of Datalog to C++\nbased on a method that \"pushes\" derived tuples immediately to places where they\nare used. In this paper, we report performance results of various\nimplementation variants of this method compared to XSB, YAP and DLV. We study\nonly a fraction of the OpenRuleBench problems, but we give a quite detailed\nanalysis of each such task and the factors which influence performance. The\nresults not only show the potential of our method and implementation approach,\nbut could be valuable for anybody implementing systems which should be able to\nexecute tasks of the discussed types.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:31:41 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Brass", "Stefan", "", "University of Halle"], ["Stephan", "Heike", "", "University of\n  Halle"]]}, {"id": "1701.00631", "submitter": "EPTCS", "authors": "Michael Hanus (CAU Kiel), Julia Krone (CAU Kiel)", "title": "A Typeful Integration of SQL into Curry", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 104-119", "doi": "10.4204/EPTCS.234.8", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension of the declarative programming language Curry to\nsupport the access to data stored in relational databases via SQL. Since Curry\nis statically typed, our emphasis on this SQL integration is on type safety.\nOur extension respects the type system of Curry so that run-time errors due to\nill-typed data are avoided. This is obtained by preprocessing SQL statements at\ncompile time and translating them into type-safe database access operations. As\na consequence, the type checker of the Curry system can spot type errors in SQL\nstatements at compile time. To generate appropriately typed access operations,\nthe preprocessor uses an entity-relationship (ER) model describing the\nstructure of the relational data. In addition to standard SQL, SQL statements\nembedded in Curry can include program expressions and also relationships\nspecified in the ER model. The latter feature is useful to avoid the\nerror-prone use of foreign keys. As a result, our SQL integration supports a\nhigh-level and type-safe access to databases in Curry programs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:32:32 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Hanus", "Michael", "", "CAU Kiel"], ["Krone", "Julia", "", "CAU Kiel"]]}, {"id": "1701.01094", "submitter": "Karamjit Singh", "authors": "Karamjit Singh, Garima Gupta, Gautam Shroff, and Puneet Agarwal", "title": "Minimally-Supervised Attribute Fusion for Data Lakes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregate analysis, such as comparing country-wise sales versus global market\nshare across product categories, is often complicated by the unavailability of\ncommon join attributes, e.g., category, across diverse datasets from different\ngeographies or retail chains, even after disparate data is technically ingested\ninto a common data lake. Sometimes this is a missing data issue, while in other\ncases it may be inherent, e.g., the records in different geographical databases\nmay actually describe different product 'SKUs', or follow different norms for\ncategorization. Record linkage techniques can be used to automatically map\nproducts in different data sources to a common set of global attributes,\nthereby enabling federated aggregation joins to be performed. Traditional\nrecord-linkage techniques are typically unsupervised, relying textual\nsimilarity features across attributes to estimate matches. In this paper, we\npresent an ensemble model combining minimal supervision using Bayesian network\nmodels together with unsupervised textual matching for automating such\n'attribute fusion'. We present results of our approach on a large volume of\nreal-life data from a market-research scenario and compare with a standard\nrecord matching algorithm. Finally we illustrate how attribute fusion using\nmachine learning could be included as a data-lake management feature,\nespecially as our approach also provides confidence values for matches,\nenabling human intervention, if required.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 18:19:19 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Singh", "Karamjit", ""], ["Gupta", "Garima", ""], ["Shroff", "Gautam", ""], ["Agarwal", "Puneet", ""]]}, {"id": "1701.01232", "submitter": "Dinusha Vatsalan", "authors": "Dinusha Vatsalan, Peter Christen, and Erhard Rahm", "title": "Scalable Multi-Database Privacy-Preserving Record Linkage using Counting\n  Bloom Filters", "comments": "This is an extended version of an article published in IEEE ICDM\n  International Workshop on Privacy and Discrimination in Data Mining (PDDM)\n  2016 - Scalable privacy-preserving linking of multiple databases using\n  counting Bloom filters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-preserving record linkage (PPRL) aims at integrating sensitive\ninformation from multiple disparate databases of different organizations. PPRL\napproaches are increasingly required in real-world application areas such as\nhealthcare, national security, and business. Previous approaches have mostly\nfocused on linking only two databases as well as the use of a dedicated linkage\nunit. Scaling PPRL to more databases (multi-party PPRL) is an open challenge\nsince privacy threats as well as the computation and communication costs for\nrecord linkage increase significantly with the number of databases. We thus\npropose the use of a new encoding method of sensitive data based on Counting\nBloom Filters (CBF) to improve privacy for multi-party PPRL. We also\ninvestigate optimizations to reduce communication and computation costs for\nCBF-based multi-party PPRL with and without the use of a dedicated linkage\nunit. Empirical evaluations conducted with real datasets show the viability of\nthe proposed approaches and demonstrate their scalability, linkage quality, and\nprivacy protection.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 07:57:55 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Vatsalan", "Dinusha", ""], ["Christen", "Peter", ""], ["Rahm", "Erhard", ""]]}, {"id": "1701.02190", "submitter": "Jerome Darmont", "authors": "Alfredo Cuzzocrea (ICAR-CNR), J\\'er\\^ome Darmont (ERIC), Hadj Mahboubi\n  (ERIC)", "title": "Fragmenting very large XML data warehouses via K-means clustering\n  algorithm", "comments": null, "journal-ref": "International Journal of Business Intelligence and Data Mining,\n  Inderscience, 2009, 4 (3/4), pp.301-328", "doi": "10.1504/IJBIDM.2009.029076", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML data sources are more and more gaining popularity in the context of a\nwide family of Business Intelligence (BI) and On-Line Analytical Processing\n(OLAP) applications, due to the amenities of XML in representing and managing\nsemi-structured and complex multidimensional data. As a consequence, many XML\ndata warehouse models have been proposed during past years in order to handle\nhetero-geneity and complexity of multidimensional data in a way traditional\nrelational data warehouse approaches fail to achieve. However, XML-native\ndatabase systems currently suffer from limited performance, both in terms of\nvolumes of manageable data and query response time. Therefore , recent research\nefforts are focusing the attention on fragmentation techniques, which are able\nto overcome the limitations above. Derived horizontal fragmentation is already\nused in relational data warehouses, and can definitely be adapted to the XML\ncontext. However, classical fragmentation algorithms are not suitable to\ncontrol the number of originated fragments, which instead plays a critical role\nin data warehouses, and, with more emphasis, distributed data warehouse\narchitectures. Inspired by this research challenge, in this paper we propose\nthe use of K-means clustering algorithm for effectively and efficiently\nsupporting the fragmentation of very large XML data warehouses, and, at the\nsame time, completely controlling and determining the number of originated\nfragments via adequately setting the parameter K. We complete our analytical\ncontribution by means of a comprehensive experimental assessment where we\ncompare the efficiency of our proposed XML data warehouse fragmentation\ntechnique against those of classical derived horizontal fragmentation\nalgorithms adapted to XML data warehouses.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 14:26:35 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Cuzzocrea", "Alfredo", "", "ICAR-CNR"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Mahboubi", "Hadj", "", "ERIC"]]}, {"id": "1701.02221", "submitter": "Domagoj Vrgo\\v{c}", "authors": "Pierre Bourhis and Juan L. Reutter and Fernando Su\\'arez and Domagoj\n  Vrgo\\v{c}", "title": "JSON: data model, query languages and schema specification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that JSON is currently one of the most popular formats for\nexchanging data on the Web, there are very few studies on this topic and there\nare no agreement upon theoretical framework for dealing with JSON. There- fore\nin this paper we propose a formal data model for JSON documents and, based on\nthe common features present in available systems using JSON, we define a\nlightweight query language allowing us to navigate through JSON documents. We\nalso introduce a logic capturing the schema proposal for JSON and study the\ncomplexity of basic computational tasks associated with these two formalisms.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 15:53:41 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Bourhis", "Pierre", ""], ["Reutter", "Juan L.", ""], ["Su\u00e1rez", "Fernando", ""], ["Vrgo\u010d", "Domagoj", ""]]}, {"id": "1701.02231", "submitter": "Thorsten Wissmann", "authors": "Cristina Feier and Antti Kuusisto and Carsten Lutz", "title": "Rewritability in Monadic Disjunctive Datalog, MMSNP, and Expressive\n  Description Logics", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 2 (May 23,\n  2019) lmcs:5502", "doi": "10.23638/LMCS-15(2:15)2019", "report-no": null, "categories": "cs.LO cs.CC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study rewritability of monadic disjunctive Datalog programs, (the\ncomplements of) MMSNP sentences, and ontology-mediated queries (OMQs) based on\nexpressive description logics of the ALC family and on conjunctive queries. We\nshow that rewritability into FO and into monadic Datalog (MDLog) are decidable,\nand that rewritability into Datalog is decidable when the original query\nsatisfies a certain condition related to equality. We establish\n2NExpTime-completeness for all studied problems except rewritability into MDLog\nfor which there remains a gap between 2NExpTime and 3ExpTime. We also analyze\nthe shape of rewritings, which in the MMSNP case correspond to obstructions,\nand give a new construction of canonical Datalog programs that is more\nelementary than existing ones and also applies to formulas with free variables.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 18:17:15 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 13:13:21 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 17:23:18 GMT"}, {"version": "v4", "created": "Wed, 22 May 2019 06:03:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Feier", "Cristina", ""], ["Kuusisto", "Antti", ""], ["Lutz", "Carsten", ""]]}, {"id": "1701.02344", "submitter": "Andriy Miranskyy", "authors": "Andriy V. Miranskyy and Zainab Al-zanbouri and David Godwin and Ayse\n  Basar Bener", "title": "Database Engines: Evolution of Greenness", "comments": null, "journal-ref": null, "doi": "10.1002/smr.1915", "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Information Technology consumes up to 10\\% of the world's\nelectricity generation, contributing to CO2 emissions and high energy costs.\nData centers, particularly databases, use up to 23% of this energy. Therefore,\nbuilding an energy-efficient (green) database engine could reduce energy\nconsumption and CO2 emissions.\n  Goal: To understand the factors driving databases' energy consumption and\nexecution time throughout their evolution.\n  Method: We conducted an empirical case study of energy consumption by two\nMySQL database engines, InnoDB and MyISAM, across 40 releases. We examined the\nrelationships of four software metrics to energy consumption and execution time\nto determine which metrics reflect the greenness and performance of a database.\n  Results: Our analysis shows that database engines' energy consumption and\nexecution time increase as databases evolve. Moreover, the Lines of Code metric\nis correlated moderately to strongly with energy consumption and execution time\nin 88% of cases.\n  Conclusions: Our findings provide insights to both practitioners and\nresearchers. Database administrators may use them to select a fast, green\nrelease of the MySQL database engine. MySQL database-engine developers may use\nthe software metric to assess products' greenness and performance. Researchers\nmay use our findings to further develop new hypotheses or build models to\npredict greenness and performance of databases.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 20:47:21 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Miranskyy", "Andriy V.", ""], ["Al-zanbouri", "Zainab", ""], ["Godwin", "David", ""], ["Bener", "Ayse Basar", ""]]}, {"id": "1701.02494", "submitter": "Nils Vortmeier", "authors": "Thomas Schwentick, Nils Vortmeier, Thomas Zeume", "title": "Dynamic Complexity under Definable Changes", "comments": "Full version of an article to be published in ICDT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies dynamic complexity under definable change operations in\nthe DynFO framework by Patnaik and Immerman. It is shown that for changes\ndefinable by parameter-free first-order formulas, all (uniform) $AC^1$ queries\ncan be maintained by first-order dynamic programs. Furthermore, many\nmaintenance results for single-tuple changes are extended to more powerful\nchange operations: (1) The reachability query for undirected graphs is\nfirst-order maintainable under single tuple changes and first-order defined\ninsertions, likewise the reachability query for directed acyclic graphs under\nquantifier-free insertions. (2) Context-free languages are first-order\nmaintainable under $\\Sigma_1$-defined changes. These results are complemented\nby several inexpressibility results, for example, that the reachability query\ncannot be maintained by quantifier-free programs under definable,\nquantifier-free deletions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 09:43:10 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Schwentick", "Thomas", ""], ["Vortmeier", "Nils", ""], ["Zeume", "Thomas", ""]]}, {"id": "1701.02634", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Yael Amsterdamer, Tova Milo, Pierre Senellart", "title": "Top-k Querying of Unknown Values under Order Constraints (Extended\n  Version)", "comments": "32 pages, 1 figure, 1 algorithm, 51 references. Extended version of\n  paper at ICDT'17", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2017.5", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many practical scenarios make it necessary to evaluate top-k queries over\ndata items with partially unknown values. This paper considers a setting where\nthe values are taken from a numerical domain, and where some partial order\nconstraints are given over known and unknown values: under these constraints,\nwe assume that all possible worlds are equally likely. Our work is the first to\npropose a principled scheme to derive the value distributions and expected\nvalues of unknown items in this setting, with the goal of computing estimated\ntop-k results by interpolating the unknown values from the known ones. We study\nthe complexity of this general task, and show tight complexity bounds, proving\nthat the problem is intractable, but can be tractably approximated. We then\nconsider the case of tree-shaped partial orders, where we show a constructive\nPTIME solution. We also compare our problem setting to other top-k definitions\non uncertain data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 15:21:11 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Amsterdamer", "Yael", ""], ["Milo", "Tova", ""], ["Senellart", "Pierre", ""]]}, {"id": "1701.03091", "submitter": "Besat Kassaie", "authors": "Besat Kassaie", "title": "SPARQL over GraphX", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of the RDF data model to link data from heterogeneous domains has\nled to an explosive growth of RDF data. So, evaluating SPARQL queries over\nlarge RDF data has been crucial for the semantic web community. However, due to\nthe graph nature of RDF data, evaluating SPARQL queries in relational databases\nand common data-parallel systems needs a lot of joins and is inefficient. On\nthe other hand, the enormity of datasets that are graph in nature such as\nsocial network data, has led the database community to develop graph-parallel\nprocessing systems to support iterative graph computations efficiently. In this\nwork we take advantage of the graph representation of RDF data and exploit\nGraphX, a new graph processing system based on Spark. We propose a subgraph\nmatching algorithm, compatible with the GraphX programming model to evaluate\nSPARQL queries. Some experiments are performed to show the system scalability\nto handle large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 18:38:16 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Kassaie", "Besat", ""]]}, {"id": "1701.03854", "submitter": "Xiaowang Zhang", "authors": "Qiong Li and Xiaowang Zhang and Zhiyong Feng", "title": "PRSP: A Plugin-based Framework for RDF Stream Processing", "comments": "2 pages and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a plugin-based framework for RDF stream processing\nnamed PRSP. Within this framework, we can employ SPARQL query engines to\nprocess C-SPARQL queries with maintaining the high performance of those engines\nin a simple way. Taking advantage of PRSP, we can process large-scale RDF\nstreams in a distributed context via distributed SPARQL engines. Besides, we\ncan evaluate the performance and correctness of existing SPARQL query engines\nin handling RDF streams in a united way, which amends the evaluation of them\nranging from static RDF (i.e., RDF graph) to dynamic RDF (i.e., RDF stream).\nFinally, within PRSP, we experimently evaluate the correctness and the\nperformance on YABench. The experiments show that PRSP can still maintain the\nhigh performance of those engines in RDF stream processing although there are\nsome slight differences among them.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 00:17:53 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Li", "Qiong", ""], ["Zhang", "Xiaowang", ""], ["Feng", "Zhiyong", ""]]}, {"id": "1701.04182", "submitter": "Xiaowang Zhang", "authors": "Xiaowang Zhang, Jiahui Zhang, Zhiyong Feng", "title": "hMDAP: A Hybrid Framework for Multi-paradigm Data Analytical Processing\n  on Spark", "comments": "4 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose hMDAP, a hybrid framework for large-scale data analytical\nprocessing on Spark, to support multi-paradigm process (incl. OLAP, machine\nlearning, and graph analysis etc.) in distributed environments. The framework\nfeatures a three-layer data process module and a business process module which\ncontrols the former. We will demonstrate the strength of hMDAP by using traffic\nscenarios in a real world.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 06:22:24 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Zhang", "Xiaowang", ""], ["Zhang", "Jiahui", ""], ["Feng", "Zhiyong", ""]]}, {"id": "1701.04339", "submitter": "Vivek Shah", "authors": "Vivek Shah", "title": "Transactional Partitioning: A New Abstraction for Main-Memory Databases", "comments": "Appeared in VLDB 2014 PhD workshop proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth in variety and volume of OLTP (Online Transaction Processing)\napplications poses a challenge to OLTP systems to meet performance and cost\ndemands in the existing hardware landscape. These applications are highly\ninteractive (latency sensitive) and require update consistency. They target\ncommodity hardware for deployment and demand scalability in throughput with\nincreasing clients and data. Currently, OLTP systems used by these applications\nprovide trade-offs in performance and ease of development over a variety of\napplications. In order to bridge the gap between performance and ease of\ndevelopment, we propose an intuitive, high-level programming model which allows\nOLTP applications to be modeled as a cluster of application logic units. By\nextending transactions guaranteeing full ACID semantics to provide the proposed\nmodel, we maintain ease of application development. The model allows the\napplication developer to reason about program performance, and to influence it\nwithout the involvement of OLTP system designers (database designers) and/or\nDBAs. As a result, the database designer is free to focus on efficient running\nof programs to ensure optimal cluster resource utilization.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 15:56:47 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Shah", "Vivek", ""]]}, {"id": "1701.04652", "submitter": "Jerome Darmont", "authors": "Marouane Hachicha (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "A Survey of XML Tree Patterns", "comments": null, "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, Institute of\n  Electrical and Electronics Engineers, 2013, 25 (1), pp.29 - 46", "doi": "10.1109/TKDE.2011.209", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With XML becoming an ubiquitous language for data interoperability purposes\nin various domains, efficiently querying XML data is a critical issue. This has\nlead to the design of algebraic frameworks based on tree-shaped patterns akin\nto the tree-structured data model of XML. Tree patterns are graphic\nrepresentations of queries over data trees. They are actually matched against\nan input data tree to answer a query. Since the turn of the twenty-first\ncentury, an astounding research effort has been focusing on tree pattern models\nand matching optimization (a primordial issue). This paper is a comprehensive\nsurvey of these topics, in which we outline and compare the various features of\ntree patterns. We also review and discuss the two main families of approaches\nfor optimizing tree pattern matching, namely pattern tree minimization and\nholistic matching. We finally present actual tree pattern-based developments,\nto provide a global overview of this significant research topic.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 12:50:56 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Hachicha", "Marouane", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.05099", "submitter": "Jerome Darmont", "authors": "Romain Perriot (LIMOS), J\\'er\\'emy Pfeifer (LIMOS), Laurent D 'Orazio\n  (LIMOS), Bruno Bachelet (LIMOS), Sandro Bimonte (IRSTEA), J\\'er\\^ome Darmont\n  (ERIC)", "title": "Cost Models for Selecting Materialized Views in Public Clouds", "comments": null, "journal-ref": "International Journal of Data Warehousing and Mining (JDWM), IGI\n  Global, 2014, 10 (4), pp.1-25", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data warehouse performance is usually achieved through physical data\nstructures such as indexes or materialized views. In this context, cost models\ncan help select a relevant set ofsuch performance optimization structures.\nNevertheless, selection becomes more complex in the cloud. The criterion to\noptimize is indeed at least two-dimensional, with monetary cost balancing\noverall query response time. This paper introduces new cost models that fit\ninto the pay-as-you-go paradigm of cloud computing. Based on these cost models,\nan optimization problem is defined to discover, among candidate views, those to\nbe materialized to minimize both the overall cost of using and maintaining the\ndatabase in a public cloud and the total response time ofa given query\nworkload. We experimentally show that maintaining materialized views is always\nadvantageous, both in terms of performance and cost.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 15:25:07 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Perriot", "Romain", "", "LIMOS"], ["Pfeifer", "J\u00e9r\u00e9my", "", "LIMOS"], ["'Orazio", "Laurent D", "", "LIMOS"], ["Bachelet", "Bruno", "", "LIMOS"], ["Bimonte", "Sandro", "", "IRSTEA"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.05397", "submitter": "Vivek Shah", "authors": "Vivek Shah, Marcos Antonio Vaz Salles", "title": "Reactors: A Case for Predictable, Virtualized Actor Database Systems", "comments": "Extended version of SIGMOD 2018 paper with an expanded appendix\n  consisting of additional experiments, figures and code examples", "journal-ref": null, "doi": "10.1145/3183713.3183752", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The requirements for OLTP database systems are becoming ever more demanding.\nDomains such as finance and computer games increasingly mandate that developers\nbe able to encode complex application logic and control transaction latencies\nin in-memory databases. At the same time, infrastructure engineers in these\ndomains need to experiment with and deploy OLTP database architectures that\nensure application scalability and maximize resource utilization in modern\nmachines. In this paper, we propose a relational actor programming model for\nin-memory databases as a novel, holistic approach towards fulfilling these\nchallenging requirements. Conceptually, relational actors, or reactors for\nshort, are application-defined, isolated logical actors that encapsulate\nrelations and process function calls asynchronously. Reactors ease reasoning\nabout correctness by guaranteeing serializability of application-level function\ncalls. In contrast to classic transactional models, however, reactors allow\ndevelopers to take advantage of intra-transaction parallelism and state\nencapsulation in their applications to reduce latency and improve locality.\nMoreover, reactors enable a new degree of flexibility in database deployment.\nWe present ReactDB, a system design exposing reactors that allows for flexible\nvirtualization of database architecture between the extremes of shared-nothing\nand shared-everything without changes to application code. Our experiments\nillustrate latency control, low overhead, and asynchronicity trade-offs with\nReactDB in OLTP benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 12:51:46 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 12:12:23 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 12:35:32 GMT"}, {"version": "v4", "created": "Fri, 11 May 2018 13:34:10 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Shah", "Vivek", ""], ["Salles", "Marcos Antonio Vaz", ""]]}, {"id": "1701.05449", "submitter": "Jerome Darmont", "authors": "Varunya Attasena (ERIC), Nouria Harbi (ERIC), J\\'er\\^ome Darmont\n  (ERIC)", "title": "A Novel Multi-Secret Sharing Approach for Secure Data Warehousing and\n  On-Line Analysis Processing in the Cloud", "comments": null, "journal-ref": "International Journal of Data Warehousing and Mining, 11 (2),\n  pp.22 - 43 (2015)", "doi": "10.4018/ijdwm.2015040102", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing helps reduce costs, increase business agility and deploy\nsolutions with a high return on investment for many types of applications,\nincluding data warehouses and on-line analytical processing. However, storing\nand transferring sensitive data into the cloud raises legitimate security\nconcerns. In this paper, we propose a new multi-secret sharing approach for\ndeploying data warehouses in the cloud and allowing on-line analysis\nprocessing, while enforcing data privacy, integrity and availability. We first\nvalidate the relevance of our approach theoretically and then experimentally\nwith both a simple random dataset and the Star Schema Benchmark. We also\ndemonstrate its superiority to related methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 14:54:21 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Attasena", "Varunya", "", "ERIC"], ["Harbi", "Nouria", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.05513", "submitter": "Xing Niu", "authors": "Xing Niu and Boris Glavic", "title": "Optimizing Provenance Computations", "comments": "Illinois Institute of Technique Database Group Technical Report\n  IIT/CS-DB-2016-02", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data provenance is essential for debugging query results, auditing data in\ncloud environments, and explaining outputs of Big Data analytics. A\nwell-established technique is to represent provenance as annotations on data\nand to instrument queries to propagate these annotations to produce results\nannotated with provenance. However, even sophisticated optimizers are often\nincapable of producing efficient execution plans for instrumented queries,\nbecause of their inherent complexity and unusual structure. Thus, while\ninstrumentation enables provenance support for databases without requiring any\nmodification to the DBMS, the performance of this approach is far from optimal.\nIn this work, we develop provenance specific optimizations to address this\nproblem. Specifically, we introduce algebraic equivalences targeted at\ninstrumented queries and discuss alternative, equivalent ways of instrumenting\na query for provenance capture. Furthermore, we present an extensible heuristic\nand cost-based optimization (CBO) framework that governs the application of\nthese optimizations and implement this framework in our GProM provenance\nsystem. Our CBO is agnostic to the plan space shape, uses a DBMS for cost\nestimation, and enables retrofitting of optimization choices into existing code\nby adding a few LOC. Our experiments confirm that these optimizations are\nhighly effective, often improving performance by several orders of magnitude\nfor diverse provenance tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 17:12:27 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Niu", "Xing", ""], ["Glavic", "Boris", ""]]}, {"id": "1701.05699", "submitter": "Seokki Lee", "authors": "Seokki Lee, Sven Koehler, Bertram Ludaescher, and Boris Glavic", "title": "Efficiently Computing Provenance Graphs for Queries with Negation", "comments": "Illinois Institute of Technology, IIT/CS-DB-2016-03", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining why an answer is in the result of a query or why it is missing\nfrom the result is important for many applications including auditing,\ndebugging data and queries, and answering hypothetical questions about data.\nBoth types of questions, i.e., why and why-not provenance, have been studied\nextensively. In this work, we present the first practical approach for\nanswering such questions for queries with negation (first-order queries). Our\napproach is based on a rewriting of Datalog rules (called firing rules) that\ncaptures successful rule derivations within the context of a Datalog query. We\nextend this rewriting to support negation and to capture failed derivations\nthat explain missing answers. Given a (why or why-not) provenance question, we\ncompute an explanation, i.e., the part of the provenance that is relevant to\nanswer the question. We introduce optimizations that prune parts of a\nprovenance graph early on if we can determine that they will not be part of the\nexplanation for a given question. We present an implementation that runs on top\nof a relational database using SQL to compute explanations. Our experiments\ndemonstrate that our approach scales to large instances and significantly\noutperforms an earlier approach which instantiates the full provenance to\ncompute explanations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 05:46:00 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Lee", "Seokki", ""], ["Koehler", "Sven", ""], ["Ludaescher", "Bertram", ""], ["Glavic", "Boris", ""]]}, {"id": "1701.05724", "submitter": "Vinh Nguyen", "authors": "Vinh Nguyen and Amit Sheth", "title": "Logical Inferences with Contexts of RDF Triples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logical inference, an integral feature of the Semantic Web, is the process of\nderiving new triples by applying entailment rules on knowledge bases. The\nentailment rules are determined by the model-theoretic semantics. Incorporating\ncontext of an RDF triple (e.g., provenance, time, and location) into the\ninferencing process requires the formal semantics to be capable of describing\nthe context of RDF triples also in the form of triples, or in other words, RDF\ncontextual triples about triples. The formal semantics should also provide the\nrules that could entail new contextual triples about triples. In this paper, we\npropose the first inferencing mechanism that allows context of RDF triples,\nrepresented in the form of RDF triples about triples, to be the first-class\ncitizens in the model-theoretic semantics and in the logical rules. Our\ninference mechanism is well-formalized with all new concepts being captured in\nthe model-theoretic semantics. This formal semantics also allows us to derive a\nnew set of entailment rules that could entail new contextual triples about\ntriples. To demonstrate the feasibility and the scalability of the proposed\nmechanism, we implement a new tool in which we transform the existing knowledge\nbases to our representation of RDF triples about triples and provide the option\nfor this tool to compute the inferred triples for the proposed rules. We\nevaluate the computation of the proposed rules on a large scale using various\nreal-world knowledge bases such as Bio2RDF NCBI Genes and DBpedia. The results\nshow that the computation of the inferred triples can be highly scalable. On\naverage, one billion inferred triples adds 5-6 minutes to the overall\ntransformation process. NCBI Genes, with 20 billion triples in total, took only\n232 minutes for the transformation of 12 billion triples and added 42 minutes\nfor inferring 8 billion triples to the overall process.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 08:51:41 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Nguyen", "Vinh", ""], ["Sheth", "Amit", ""]]}, {"id": "1701.05799", "submitter": "Vijay Gadepally", "authors": "Kyle OBrien, Vijay Gadepally, Jennie Duggan, Adam Dziedzic, Aaron\n  Elmore, Jeremy Kepner, Samuel Madden, Tim Mattson, Zuohao She, Michael\n  Stonebraker", "title": "BigDAWG Polystore Release and Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Intel Science and Technology Center for Big Data is developing a\nreference implementation of a Polystore database. The BigDAWG (Big Data Working\nGroup) system supports \"many sizes\" of database engines, multiple programming\nlanguages and complex analytics for a variety of workloads. Our recent efforts\ninclude application of BigDAWG to an ocean metagenomics problem and\ncontainerization of BigDAWG. We intend to release an open source BigDAWG v1.0\nin the Spring of 2017. In this article, we will demonstrate a number of\npolystore applications developed with oceanographic researchers at MIT and\ndescribe our forthcoming open source release of the BigDAWG system.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 00:29:31 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["OBrien", "Kyle", ""], ["Gadepally", "Vijay", ""], ["Duggan", "Jennie", ""], ["Dziedzic", "Adam", ""], ["Elmore", "Aaron", ""], ["Kepner", "Jeremy", ""], ["Madden", "Samuel", ""], ["Mattson", "Tim", ""], ["She", "Zuohao", ""], ["Stonebraker", "Michael", ""]]}, {"id": "1701.05982", "submitter": "Sudhakar Singh", "authors": "Sudhakar Singh, Rakhi Garg, P. K. Mishra", "title": "Observations on Factors Affecting Performance of MapReduce based Apriori\n  on Hadoop Cluster", "comments": "8 pages, 8 figures, International Conference on Computing,\n  Communication and Automation (ICCCA2016)", "journal-ref": "2016 International Conference on Computing, Communication and\n  Automation (ICCCA), Greater Noida, India, 2016, pp. 87-94", "doi": "10.1109/CCAA.2016.7813695", "report-no": "466", "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing fast and scalable algorithm for mining frequent itemsets is always\nbeing a most eminent and promising problem of data mining. Apriori is one of\nthe most broadly used and popular algorithm of frequent itemset mining.\nDesigning efficient algorithms on MapReduce framework to process and analyze\nbig datasets is contemporary research nowadays. In this paper, we have focused\non the performance of MapReduce based Apriori on homogeneous as well as on\nheterogeneous Hadoop cluster. We have investigated a number of factors that\nsignificantly affects the execution time of MapReduce based Apriori running on\nhomogeneous and heterogeneous Hadoop Cluster. Factors are specific to both\nalgorithmic and non-algorithmic improvements. Considered factors specific to\nalgorithmic improvements are filtered transactions and data structures.\nExperimental results show that how an appropriate data structure and filtered\ntransactions technique drastically reduce the execution time. The\nnon-algorithmic factors include speculative execution, nodes with poor\nperformance, data locality & distribution of data blocks, and parallelism\ncontrol with input split size. We have applied strategies against these factors\nand fine tuned the relevant parameters in our particular application.\nExperimental results show that if cluster specific parameters are taken care of\nthen there is a significant reduction in execution time. Also we have discussed\nthe issues regarding MapReduce implementation of Apriori which may\nsignificantly influence the performance.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 05:12:13 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Singh", "Sudhakar", ""], ["Garg", "Rakhi", ""], ["Mishra", "P. K.", ""]]}, {"id": "1701.06093", "submitter": "Alekh Jindal", "authors": "Alekh Jindal, Jorge-Arnulfo Quiane-Ruiz, Samuel Madden", "title": "INGESTBASE: A Declarative Data Ingestion System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data applications have fast arriving data that must be quickly ingested.\nAt the same time, they have specific needs to preprocess and transform the data\nbefore it could be put to use. The current practice is to do these preparatory\ntransformations once the data is already ingested, however, this is expensive\nto run and cumbersome to manage. As a result, there is a need to push data\npreprocessing down to the ingestion itself. In this paper, we present a\ndeclarative data ingestion system, called INGESTBASE, to allow application\ndevelopers to plan and specify their data ingestion logic in a more systematic\nmanner. We introduce the notion of ingestions plans, analogous to query plans,\nand present a declarative ingestion language to help developers easily build\nsophisticated ingestion plans. INGESTBASE provides an extensible ingestion\noptimizer to rewrite and optimize ingestion plans by applying rules such as\noperator reordering and pipelining. Finally, the INGESTBASE runtime engine runs\nthe optimized ingestion plan in a distributed and fault-tolerant manner. Later,\nat query processing time, INGESTBASE supports ingestion-aware data access and\ninterfaces with upstream query processors, such as Hadoop MapReduce and Spark,\nto post- process the ingested data. We demonstrate through a number of\nexperiments that INGESTBASE: (i) is flexible enough to express a variety of\ningestion techniques, (ii) incurs a low ingestion overhead, (iii) provides\nefficient access to the ingested data, and (iv) has much better performance, up\nto 6 times, than preparing data as an afterthought, via a query processor.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 22:34:47 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Jindal", "Alekh", ""], ["Quiane-Ruiz", "Jorge-Arnulfo", ""], ["Madden", "Samuel", ""]]}, {"id": "1701.06454", "submitter": "Domagoj Vrgo\\v{c}", "authors": "Jorge Baier, Dietrich Daroch, Juan Reutter, Domagoj Vrgo\\v{c}", "title": "Evaluating navigational RDF queries over the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Web, and its underlying data format RDF, lend themselves naturally\nto navigational querying due to their graph-like structure. This is\nparticularly evident when considering RDF data on the Web, where various\nseparately published datasets reference each other and form a giant graph known\nas the Web of Linked Data. And while navigational queries over singular RDF\ndatasets are supported through SPARQL property paths, not much is known about\nevaluating them over Linked Data. In this paper we propose a method for\nevaluating property path queries over the Web based on the classical AI search\nalgorithm A*, show its optimality in the open world setting of the Web, and\ntest it using real world queries which access a variety of RDF datasets\navailable online.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 15:31:17 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 20:24:41 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Baier", "Jorge", ""], ["Daroch", "Dietrich", ""], ["Reutter", "Juan", ""], ["Vrgo\u010d", "Domagoj", ""]]}, {"id": "1701.07388", "submitter": "Konstantinos Xirogiannopoulos", "authors": "Konstantinos Xirogiannopoulos, Amol Deshpande", "title": "Extracting and Analyzing Hidden Graphs from Relational Databases", "comments": "A shorter version of the paper is to appear in SIGMOD 2017", "journal-ref": null, "doi": "10.1145/3035918.3035949", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing interconnection structures among underlying entities or objects in\na dataset through the use of graph analytics has been shown to provide\ntremendous value in many application domains. However, graphs are not the\nprimary representation choice for storing most data today, and in order to have\naccess to these analyses, users are forced to extract data from their data\nstores, construct the requisite graphs, and then load them into some graph\nengine in order to execute their graph analysis task. Moreover, these graphs\ncan be significantly larger than the initial input stored in the database,\nmaking it infeasible to construct or analyze such graphs in memory. In this\npaper we address both of these challenges by building a system that enables\nusers to declaratively specify graph extraction tasks over a relational\ndatabase schema and then execute graph algorithms on the extracted graphs. We\npropose a declarative domain-specific language for this purpose, and pair it up\nwith a novel condensed, in-memory representation that significantly reduces the\nmemory footprint of these graphs, permitting analysis of larger-than-memory\ngraphs. We present a general algorithm for creating this condensed\nrepresentation for a large class of graph extraction queries against arbitrary\nschemas. We observe that the condensed representation suffers from a\nduplication issue, that results in inaccuracies for most graph algorithms. We\nthen present a suite of in-memory representations that handle this duplication\nin different ways and allow trading off the memory required and the\ncomputational cost for executing different graph algorithms. We introduce novel\ndeduplication algorithms for removing this duplication in the graph, which are\nof independent interest for graph compression, and provide a comprehensive\nexperimental evaluation over several real-world and synthetic datasets\nillustrating these trade-offs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 17:25:56 GMT"}, {"version": "v2", "created": "Sat, 11 Feb 2017 04:25:01 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Xirogiannopoulos", "Konstantinos", ""], ["Deshpande", "Amol", ""]]}, {"id": "1701.07473", "submitter": "Jimmy Dobler", "authors": "Jimmy Dobler, Atri Rudra", "title": "Implementation of Tetris as a Model Counter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving SharpSAT problems is an important area of work. In this paper, we\ndiscuss implementing Tetris, an algorithm originally designed for handling\nnatural joins, as an exact model counter for the SharpSAT problem. Tetris uses\na simple geometric framework, yet manages to achieve the fractional\nhypertree-width bound. Its design allows it to handle complex problems\ninvolving extremely large numbers of clauses on which other state-of-the-art\nmodel counters do not perform well, yet still performs strongly on standard SAT\nbenchmarks.\n  We have achieved the following objectives. First, we have found a natural set\nof model counting benchmarks on which Tetris outperforms other model counters.\nSecond, we have constructed a data structure capable of efficiently handling\nand caching all of the data Tetris needs to work on over the course of the\nalgorithm. Third, we have modified Tetris in order to move from a theoretical,\nasymptotic-time-focused environment to one that performs well in practice. In\nparticular, we have managed to produce results keeping us within a single order\nof magnitude as compared to other solvers on most benchmarks, and outperform\nthose solvers by multiple orders of magnitude on others.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:23:58 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Dobler", "Jimmy", ""], ["Rudra", "Atri", ""]]}, {"id": "1701.07696", "submitter": "Mario Boley", "authors": "Mario Boley and Bryan R. Goldsmith and Luca M. Ghiringhelli and Jilles\n  Vreeken", "title": "Identifying Consistent Statements about Numerical Data with\n  Dispersion-Corrected Subgroup Discovery", "comments": "significance of empirical results tested; additional illustrations;\n  table of used notations", "journal-ref": null, "doi": "10.1007/s10618-017-0520-3", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing algorithms for subgroup discovery with numerical targets do not\noptimize the error or target variable dispersion of the groups they find. This\noften leads to unreliable or inconsistent statements about the data, rendering\npractical applications, especially in scientific domains, futile. Therefore, we\nhere extend the optimistic estimator framework for optimal subgroup discovery\nto a new class of objective functions: we show how tight estimators can be\ncomputed efficiently for all functions that are determined by subgroup size\n(non-decreasing dependence), the subgroup median value, and a dispersion\nmeasure around the median (non-increasing dependence). In the important special\ncase when dispersion is measured using the average absolute deviation from the\nmedian, this novel approach yields a linear time algorithm. Empirical\nevaluation on a wide range of datasets shows that, when used within\nbranch-and-bound search, this approach is highly efficient and indeed discovers\nsubgroups with much smaller errors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 13:36:43 GMT"}, {"version": "v2", "created": "Sun, 23 Apr 2017 09:34:35 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Boley", "Mario", ""], ["Goldsmith", "Bryan R.", ""], ["Ghiringhelli", "Luca M.", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1701.07723", "submitter": "Georgia Kougka", "authors": "Georgia Kougka, Anastasios Gounaris and Alkis Simitsis", "title": "The Many Faces of Data-centric Workflow Optimization: A Survey", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workflow technology is rapidly evolving and, rather than being limited to\nmodeling the control flow in business processes, is becoming a key mechanism to\nperform advanced data management, such as big data analytics. This survey\nfocuses on data-centric workflows (or workflows for data analytics or data\nflows), where a key aspect is data passing through and getting manipulated by a\nsequence of steps. The large volume and variety of data, the complexity of\noperations performed, and the long time such workflows take to compute give\nrise to the need for optimization. In general, data-centric workflow\noptimization is a technology in evolution. This survey focuses on techniques\napplicable to workflows comprising arbitrary types of data manipulation steps\nand semantic inter-dependencies between such steps. Further, it serves a\ntwofold purpose. Firstly, to present the main dimensions of the relevant\noptimization problems and the types of optimizations that occur before flow\nexecution. Secondly, to provide a concise overview of the existing approaches\nwith a view to highlighting key observations and areas deserving more attention\nfrom the community.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 14:45:28 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Kougka", "Georgia", ""], ["Gounaris", "Anastasios", ""], ["Simitsis", "Alkis", ""]]}, {"id": "1701.07739", "submitter": "Jerome Darmont", "authors": "Jerome Darmont (ERIC)", "title": "Object Database Benchmarks", "comments": null, "journal-ref": "Encyclopedia of Information Science and Technology, I-III, Idea\n  Group Publishing, pp.2146-2149, 2005", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for performance measurement tools appeared soon after the emergence\nof the first Object-Oriented Database Management Systems (OODBMSs), and proved\nimportant for both designers and users (Atkinson \\& Maier, 1990). Performance\nevaluation is useful to designers to determine elements of architecture and\nmore generally to validate or refute hypotheses regarding the actual behavior\nof an OODBMS. Thus, performance evaluation is an essential component in the\ndevelopment process of well-designed and efficient systems. Users may also\nemploy performance evaluation, either to compare the efficiency of different\ntechnologies before selecting an OODBMS or to tune a system.Performance\nevaluation by experimentation on a real system is generally referred to as\nbenchmarking. It consists in performing a series of tests on a given OODBMS to\nestimate its performance in a given setting. Benchmarks are generally used to\ncompare the global performance of OODBMSs, but they can also be exploited to\nillustrate the advantages of one system or another in a given situation, or to\ndetermine an optimal hardware configuration. Typically, a benchmark is\nconstituted of two main elements: a workload model constituted of a database\nand a set of read and write operations to apply on this database, and a set of\nperformance metrics.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 15:29:51 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Darmont", "Jerome", "", "ERIC"]]}, {"id": "1701.08028", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (ERIC), Emerson Olivier (ERIC)", "title": "Biomedical Data Warehouses", "comments": "arXiv admin note: substantial text overlap with arXiv:0809.2688", "journal-ref": "Encyclopaedia of Healthcare Information Systems, IGI Publishing,\n  pp.149-156, 2008", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to present an overview of the existing biomedical\ndata warehouses and to discuss the issues and future trends in this area. We\nillustrate this topic by presenting the design of an innovative, complex data\nwarehouse for personal, anticipative medicine.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:30:07 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Olivier", "Emerson", "", "ERIC"]]}, {"id": "1701.08029", "submitter": "Jerome Darmont", "authors": "Kamel Aouiche (Centre LICEF - T\\'ELUQ), J\\'er\\^ome Darmont (ERIC)", "title": "Index and Materialized View Selection in Data Warehouses", "comments": null, "journal-ref": "Handbook of Research on Innovations in Database Technologies and\n  Applications, II, pp.693-700, 2009", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to present an overview of the major families of\nstate-of-the-art index and materialized view selection methods, and to discuss\nthe issues and future trends in data warehouse performance optimization. We\nparticularly focus on data mining-based heuristics we developed to reduce the\nselection problem complexity and target the most pertinent candidate indexes\nand materialized views.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:30:49 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Aouiche", "Kamel", "", "Centre LICEF - T\u00c9LUQ"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.08033", "submitter": "Jerome Darmont", "authors": "Hadj Mahboubi (ERIC), Jean-Christian Ralaivao (ERIC), Sabine Loudcher\n  (ERIC), Omar Boussa\\\"id (ERIC), Fadila Bentayeb (ERIC), J\\'er\\^ome Darmont\n  (ERIC)", "title": "X-WACoDa: An XML-based approach for Warehousing and Analyzing Complex\n  Data", "comments": null, "journal-ref": "Advances in Data Warehousing and Mining, 3, IGI Publishing,\n  pp.38-54, 2009, Data Warehousing Design and Advanced Engineering\n  Applications: Methods for Complex Construction", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data warehousing and OLAP applications must nowadays handle complex data that\nare not only numerical or symbolic. The XML language is well-suited to\nlogically and physically represent complex data. However, its usage induces new\ntheoretical and practical challenges at the modeling, storage and analysis\nlevels, and a new trend toward XML warehousing has been emerging for a couple\nof years. Unfortunately, no standard XML data warehouse architecture emerges.\nIn this paper, we propose a unified XML warehouse reference model that\nsynthesizes and enhances related work, and fits into a global XML warehousing\nand analysis approach we have developed. We also present a software platform\nthat is based on this model, as well as a case study that illustrates its\nusage.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:39:39 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Mahboubi", "Hadj", "", "ERIC"], ["Ralaivao", "Jean-Christian", "", "ERIC"], ["Loudcher", "Sabine", "", "ERIC"], ["Boussa\u00efd", "Omar", "", "ERIC"], ["Bentayeb", "Fadila", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.08052", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (ERIC)", "title": "Database Benchmarks", "comments": null, "journal-ref": "Encyclopedia of Information Science and Technology, Second\n  Edition, IGI Publishing, pp.950-954, 2009", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to present an overview of the major families of\nstate-of-the-art data-base benchmarks, namely: relational benchmarks, object\nand object-relational benchmarks, XML benchmarks, and decision-support\nbenchmarks, and to discuss the issues, tradeoffs and future trends in database\nbenchmarking. We particularly focus on XML and decision-support benchmarks,\nwhich are currently the most innovative tools that are developed in this area.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 13:42:54 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.08053", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (ERIC)", "title": "Data Warehouse Benchmarking with DWEB", "comments": "arXiv admin note: substantial text overlap with arXiv:1701.00399;\n  text overlap with arXiv:0705.1453", "journal-ref": "Advances in Data Warehousing and Mining, 3, IGI Publishing,\n  pp.302-323, 2009, Progressive Methods in Data Warehousing and Business\n  Intelligence: Concepts and Competitive Analytics", "doi": "10.4018/978-1-60566-232-9.ch015", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance evaluation is a key issue for designers and users of Database\nManagement Systems (DBMSs). Performance is generally assessed with software\nbenchmarks that help, e.g., test architectural choices, compare different\ntechnologies or tune a system. In the particular context of data warehousing\nand On-Line Analytical Processing (OLAP), although the Transaction Processing\nPerformance Council (TPC) aims at issuing standard decision-support benchmarks,\nfew benchmarks do actually exist. We present in this chapter the Data Warehouse\nEngineering Benchmark (DWEB), which allows generating various ad-hoc synthetic\ndata warehouses and workloads. DWEB is fully parameterized to fulfill various\ndata warehouse design needs. However, two levels of parameterization keep it\nrelatively easy to tune. We also expand on our previous work on DWEB by\npresenting its new Extract, Transform, and Load (ETL) feature as well as its\nnew execution protocol. A Java implementation of DWEB is freely available\non-line, which can be interfaced with most existing relational DMBSs. To the\nbest of our knowledge, DWEB is the only easily available, up-to-date benchmark\nfor data warehouses.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 13:56:37 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.08054", "submitter": "Jerome Darmont", "authors": "Hadj Mahboubi (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "Indices in XML Databases", "comments": null, "journal-ref": "Handbook of Research on Innovations in Database Technologies and\n  Applications, II, IGI Global, pp.674-681, 2009", "doi": "10.4018/978-1-60566-242-8.ch072", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With XML becoming a standard for business information representation and\nexchange, stor-ing, indexing, and querying XML documents have rapidly become\nmajor issues in database research. In this context, query processing and\noptimization are primordial, native-XML data-bases not being mature yet. Data\nstructures such as indices, which help enhance performances substantially, are\nextensively researched, especially since XML data bear numerous specifici-ties\nwith respect to relational data. In this paper, we survey state-of-the-art XML\nindices and discuss the main issues, tradeoffs and future trends in XML\nindexing. We also present an in-dex that we specifically designed for the\nparticular architecture of XML data warehouses.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 14:00:53 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Mahboubi", "Hadj", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.08084", "submitter": "Patrick P. C. Lee", "authors": "Matt M. T. Yiu, Helen H. W. Chan, Patrick P. C. Lee", "title": "Erasure Coding for Small Objects in In-Memory KV Storage", "comments": "Accepted by SYSTOR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MemEC, an erasure-coding-based in-memory key-value (KV) store that\nachieves high availability and fast recovery while keeping low data redundancy\nacross storage servers. MemEC is specifically designed for workloads dominated\nby small objects. By encoding objects in entirety, MemEC is shown to incur 60%\nless storage redundancy for small objects than existing replication- and\nerasure-coding-based approaches. It also supports graceful transitions between\ndecentralized requests in normal mode (i.e., no failures) and coordinated\nrequests in degraded mode (i.e., with failures). We evaluate our MemEC\nprototype via testbed experiments under read-heavy and update-heavy YCSB\nworkloads. We show that MemEC achieves high throughput and low latency in both\nnormal and degraded modes, and supports fast transitions between the two modes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 15:39:50 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 15:24:05 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yiu", "Matt M. T.", ""], ["Chan", "Helen H. W.", ""], ["Lee", "Patrick P. C.", ""]]}, {"id": "1701.08088", "submitter": "Jerome Darmont", "authors": "Hadj Mahboubi (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "Query Performance Optimization in XML Data Warehouses", "comments": "arXiv admin note: substantial text overlap with arXiv:0809.1981,\n  arXiv:0809.1963", "journal-ref": "E-Strategies for Resource Management Systems: Planning and\n  Implementation, IGI Global, pp.232-253, 2010", "doi": "10.4018/978-1-61692-016-6.ch014", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML data warehouses form an interesting basis for decision-support\napplications that exploit complex data. However, native-XML database management\nsystems (DBMSs) currently bear limited performances and it is necessary to\nresearch for ways to optimize them. In this chapter, we present two such\ntechniques. First, we propose a join index that is specifically adapted to the\nmultidimensional architecture of XML warehouses. It eliminates join operations\nwhile preserving the information contained in the original warehouse. Second,\nwe present a strategy for selecting XML materialized views by clustering the\nquery workload. To validate these proposals, we measure the response time of a\nset of decision-support XQueries over an XML data warehouse, with and without\nusing our optimization techniques. Our experimental results demonstrate their\nefficiency, even when queries are complex and data are voluminous.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 15:53:04 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Mahboubi", "Hadj", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.08096", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Jilles Vreeken", "title": "Efficiently Summarising Event Sequences with Rich Interleaving Patterns", "comments": null, "journal-ref": null, "doi": "10.1137/1.9781611974973", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the key structure of a database is one of the main goals of data\nmining. In pattern set mining we do so by discovering a small set of patterns\nthat together describe the data well. The richer the class of patterns we\nconsider, and the more powerful our description language, the better we will be\nable to summarise the data. In this paper we propose \\ourmethod, a novel greedy\nMDL-based method for summarising sequential data using rich patterns that are\nallowed to interleave. Experiments show \\ourmethod is orders of magnitude\nfaster than the state of the art, results in better models, as well as\ndiscovers meaningful semantics in the form patterns that identify multiple\nchoices of values.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 16:02:54 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1701.08190", "submitter": "Mohamed Anis Bach Tobji Dr.", "authors": "Mohamed Anis Bach Tobji", "title": "Comparative Study Of Data Mining Query Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Since formulation of Inductive Database (IDB) problem, several Data Mining\n(DM) languages have been proposed, confirming that KDD process could be\nsupported via inductive queries (IQ) answering. This paper reviews the existing\nDM languages. We are presenting important primitives of the DM language and\nclassifying our languages according to primitives' satisfaction. In addition,\nwe presented languages' syntaxes and tried to apply each one to a database\nsample to test a set of KDD operations. This study allows us to highlight\nlanguages capabilities and limits, which is very useful for future work and\nperspectives.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 21:00:19 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Tobji", "Mohamed Anis Bach", ""]]}, {"id": "1701.08191", "submitter": "Mohamed Anis Bach Tobji Dr.", "authors": "Mohamed Anis Bach Tobji, Mohamed Salah Gouider", "title": "Incremental Maintenance Of Association Rules Under Support Threshold\n  Change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Maintenance of association rules is an interesting problem. Several\nincremental maintenance algorithms were proposed since the work of (Cheung et\nal, 1996). The majority of these algorithms maintain rule bases assuming that\nsupport threshold doesn't change. In this paper, we present incremental\nmaintenance algorithm under support threshold change. This solution allows user\nto maintain its rule base under any support threshold.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 21:02:52 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Tobji", "Mohamed Anis Bach", ""], ["Gouider", "Mohamed Salah", ""]]}, {"id": "1701.08288", "submitter": "Arijit Khan", "authors": "Vijaya Krishna Yalavarthi, Xiangyu Ke, Arijit Khan", "title": "Select Your Questions Wisely: For Entity Resolution With Crowd Errors", "comments": "10 Pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is becoming increasingly important in entity resolution tasks\ndue to their inherent complexity such as clustering of images and natural\nlanguage processing. Humans can provide more insightful information for these\ndifficult problems compared to machine-based automatic techniques.\nNevertheless, human workers can make mistakes due to lack of domain expertise\nor seriousness, ambiguity, or even due to malicious intents. The\nstate-of-the-art literature usually deals with human errors via majority voting\nor by assigning a universal error rate over crowd workers. However, such\napproaches are incomplete, and often inconsistent, because the expertise of\ncrowd workers are diverse with possible biases, thereby making it largely\ninappropriate to assume a universal error rate for all workers over all\ncrowdsourcing tasks.\n  To this end, we mitigate the above challenges by considering an uncertain\ngraph model, where the edge probability between two records A and B denotes the\nratio of crowd workers who voted Yes on the question if A and B are same\nentity. In order to reflect independence across different crowdsourcing tasks,\nwe apply the well-established notion of possible worlds, and develop\nparameter-free algorithms both for next crowdsourcing, as well as for entity\nresolution problems. In particular, using our framework, the problem of entity\nresolution becomes equivalent to finding the maximum-likelihood clustering;\nwhereas for the next crowdsourcing, we identify the record pair that maximally\nincreases the reliability of the maximum-likelihood clustering. Based on\ndetailed empirical analysis over real-world datasets, we find that our proposed\nsolution, PERC (probabilistic entity resolution with imperfect crowd) improves\nthe quality by 15% and reduces the overall cost by 50% for the\ncrowdsourcing-based entity resolution problem.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 13:14:00 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 04:33:45 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Yalavarthi", "Vijaya Krishna", ""], ["Ke", "Xiangyu", ""], ["Khan", "Arijit", ""]]}, {"id": "1701.08475", "submitter": "Wan-Lei Zhao", "authors": "Wan-Lei Zhao, Jie Yang, Cheng-Hao Deng", "title": "Scalable Nearest Neighbor Search based on kNN Graph", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor search is known as a challenging issue that has been studied\nfor several decades. Recently, this issue becomes more and more imminent in\nviewing that the big data problem arises from various fields. In this paper, a\nscalable solution based on hill-climbing strategy with the support of k-nearest\nneighbor graph (kNN) is presented. Two major issues have been considered in the\npaper. Firstly, an efficient kNN graph construction method based on two means\ntree is presented. For the nearest neighbor search, an enhanced hill-climbing\nprocedure is proposed, which sees considerable performance boost over original\nprocedure. Furthermore, with the support of inverted indexing derived from\nresidue vector quantization, our method achieves close to 100% recall with high\nspeed efficiency in two state-of-the-art evaluation benchmarks. In addition, a\ncomparative study on both the compressional and traditional nearest neighbor\nsearch methods is presented. We show that our method achieves the best\ntrade-off between search quality, efficiency and memory complexity.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:51:28 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 09:37:53 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Zhao", "Wan-Lei", ""], ["Yang", "Jie", ""], ["Deng", "Cheng-Hao", ""]]}, {"id": "1701.08612", "submitter": "Jerome Darmont", "authors": "Hadj Mahboubi (ERIC), Marouane Hachicha (ERIC), J\\'er\\^ome Darmont\n  (ERIC)", "title": "XML Warehousing and OLAP", "comments": "arXiv admin note: substantial text overlap with arXiv:1701.08033", "journal-ref": "Encyclopedia of Data Warehousing and Mining, Second Edition, IV,\n  IGI Publishing, pp.2109-2116, 2009", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to present an overview of the major XML\nwarehousing approaches from the literature, as well as the existing approaches\nfor performing OLAP analyses over XML data (which is termed XML-OLAP or XOLAP;\nWang et al., 2005). We also discuss the issues and future trends in this area\nand illustrate this topic by presenting the design of a unified, XML data\nwarehouse architecture and a set of XOLAP operators expressed in an XML\nalgebra.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 14:27:07 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Mahboubi", "Hadj", "", "ERIC"], ["Hachicha", "Marouane", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.08634", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (ERIC)", "title": "Data Processing Benchmarks", "comments": "arXiv admin note: substantial text overlap with arXiv:1701.08052", "journal-ref": "Encyclopedia of Information Science and Technology, Third Edition,\n  pp.146-152, 2014", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to present an overview of the major families of\nstate-of-the-art data processing benchmarks, namely transaction processing\nbenchmarks and decision support benchmarks. We also address the newer trends in\ncloud benchmarking. Finally, we discuss the issues, tradeoffs and future trends\nfor data processing benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 15:10:07 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.08643", "submitter": "Jerome Darmont", "authors": "Fadila Bentayeb (ERIC), Nora Ma\\\"iz (ERIC), Hadj Mahboubi, C\\'ecile\n  Favre (ERIC), Sabine Loudcher (ERIC), Nouria Harbi (ERIC), Omar Boussa\\\"id\n  (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "Innovative Approaches for efficiently Warehousing Complex Data from the\n  Web", "comments": "Business Intelligence Applications and the Web: Models, Systems and\n  Technologies, Business Science Reference, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in data warehousing and OLAP has produced important technologies for\nthe design, management and use of information systems for decision support.\nWith the development of Internet, the availability of various types of data has\nincreased. Thus, users require applications to help them obtaining knowledge\nfrom the Web. One possible solution to facilitate this task is to extract\ninformation from the Web, transform and load it to a Web Warehouse, which\nprovides uniform access methods for automatic processing of the data. In this\nchapter, we present three innovative researches recently introduced to extend\nthe capabilities of decision support systems, namely (1) the use of XML as a\nlogical and physical model for complex data warehouses, (2) associating data\nmining to OLAP to allow elaborated analysis tasks for complex data and (3)\nschema evolution in complex data warehouses for personalized analyses. Our\ncontributions cover the main phases of the data warehouse design process: data\nintegration and modeling and user driven-OLAP analysis.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 15:17:05 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Bentayeb", "Fadila", "", "ERIC"], ["Ma\u00efz", "Nora", "", "ERIC"], ["Mahboubi", "Hadj", "", "ERIC"], ["Favre", "C\u00e9cile", "", "ERIC"], ["Loudcher", "Sabine", "", "ERIC"], ["Harbi", "Nouria", "", "ERIC"], ["Boussa\u00efd", "Omar", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1701.08924", "submitter": "Jose Emilio Labra Gayo", "authors": "Jose-Emilio Labra-Gayo, Eric Prud'hommeaux, Harold Solbrig, Iovka\n  Boneva", "title": "Validating and describing linked data portals using shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linked data portals need to be able to advertise and describe the structure\nof their content. A sufficiently expressive and intuitive schema language will\nallow portals to communicate these structures. Validation tools will aid in the\npublication and maintenance of linked data and increase their quality.\n  Two schema language proposals have recently emerged for describing the\nstructures of RDF graphs: Shape Expressions (ShEx) and Shapes Constraint\nLanguage (SHACL). In this paper we describe how these formalisms can be used in\nthe development of a linked data portal to describe and validate its contents.\nAs a use case, we specify a data model inspired by the WebIndex data model, a\nmedium size linked data portal, using both ShEx and SHACL, and we propose a\nbenchmark that can generate compliant test data structures of any size. We then\nperform some preliminary experiments showing performance of one validation\nengine based on ShEx.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 05:51:09 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Labra-Gayo", "Jose-Emilio", ""], ["Prud'hommeaux", "Eric", ""], ["Solbrig", "Harold", ""], ["Boneva", "Iovka", ""]]}, {"id": "1701.09007", "submitter": "Wim Martens", "authors": "Serge Abiteboul, Marcelo Arenas, Pablo Barcel\\'o, Meghyn Bienvenu,\n  Diego Calvanese, Claire David, Richard Hull, Eyke H\\\"ullermeier, Benny\n  Kimelfeld, Leonid Libkin, Wim Martens, Tova Milo, Filip Murlak, Frank Neven,\n  Magdalena Ortiz, Thomas Schwentick, Julia Stoyanovich, Jianwen Su, Dan Suciu,\n  Victor Vianu, Ke Yi", "title": "Research Directions for Principles of Data Management (Dagstuhl\n  Perspectives Workshop 16151)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In April 2016, a community of researchers working in the area of Principles\nof Data Management (PDM) joined in a workshop at the Dagstuhl Castle in\nGermany. The workshop was organized jointly by the Executive Committee of the\nACM Symposium on Principles of Database Systems (PODS) and the Council of the\nInternational Conference on Database Theory (ICDT). The mission of this\nworkshop was to identify and explore some of the most important research\ndirections that have high relevance to society and to Computer Science today,\nand where the PDM community has the potential to make significant\ncontributions. This report describes the family of research directions that the\nworkshop focused on from three perspectives: potential practical relevance,\nresults already obtained, and research questions that appear surmountable in\nthe short and medium term.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 12:15:31 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Abiteboul", "Serge", ""], ["Arenas", "Marcelo", ""], ["Barcel\u00f3", "Pablo", ""], ["Bienvenu", "Meghyn", ""], ["Calvanese", "Diego", ""], ["David", "Claire", ""], ["Hull", "Richard", ""], ["H\u00fcllermeier", "Eyke", ""], ["Kimelfeld", "Benny", ""], ["Libkin", "Leonid", ""], ["Martens", "Wim", ""], ["Milo", "Tova", ""], ["Murlak", "Filip", ""], ["Neven", "Frank", ""], ["Ortiz", "Magdalena", ""], ["Schwentick", "Thomas", ""], ["Stoyanovich", "Julia", ""], ["Su", "Jianwen", ""], ["Suciu", "Dan", ""], ["Vianu", "Victor", ""], ["Yi", "Ke", ""]]}, {"id": "1701.09042", "submitter": "Jeff Heaton", "authors": "Jeff Heaton", "title": "Comparing Dataset Characteristics that Favor the Apriori, Eclat or\n  FP-Growth Frequent Itemset Mining Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent itemset mining is a popular data mining technique. Apriori, Eclat,\nand FP-Growth are among the most common algorithms for frequent itemset mining.\nConsiderable research has been performed to compare the relative performance\nbetween these three algorithms, by evaluating the scalability of each algorithm\nas the dataset size increases. While scalability as data size increases is\nimportant, previous papers have not examined the performance impact of\nsimilarly sized datasets that contain different itemset characteristics. This\npaper explores the effects that two dataset characteristics can have on the\nperformance of these three frequent itemset algorithms. To perform this\nempirical analysis, a dataset generator is created to measure the effects of\nfrequent item density and the maximum transaction size on performance. The\ngenerated datasets contain the same number of rows. This provides some insight\ninto dataset characteristics that are conducive to each algorithm. The results\nof this paper's research demonstrate Eclat and FP-Growth both handle increases\nin maximum transaction size and frequent itemset density considerably better\nthan the Apriori algorithm.\n  This paper explores the effects that two dataset characteristics can have on\nthe performance of these three frequent itemset algorithms. To perform this\nempirical analysis, a dataset generator is created to measure the effects of\nfrequent item density and the maximum transaction size on performance. The\ngenerated datasets contain the same number of rows. This provides some insight\ninto dataset characteristics that are conducive to each algorithm. The results\nof this paper's research demonstrate Eclat and FP-Growth both handle increases\nin maximum transaction size and frequent itemset density considerably better\nthan the Apriori algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 12:34:02 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Heaton", "Jeff", ""]]}, {"id": "1701.09045", "submitter": "Hao Li", "authors": "Hao Li", "title": "Big Data Technology Accelerate Genomics Precision Medicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During genomics life science research, the data volume of whole genomics and\nlife science algorithm is going bigger and bigger, which is calculated as TB,\nPB or EB etc. The key problem will be how to store and analyze the data with\noptimized way. This paper demonstrates how Intel Big Data Technology and\nArchitecture help to facilitate and accelerate the genomics life science\nresearch in data store and utilization. Intel defines high performance\nGenomicsDB for variant call data query and Lustre filesystem with Hierarchal\nStorage Management for genomics data store. Based on these great technology,\nIntel defines genomics knowledge share and exchange architecture, which is\nlanded and validated in BGI China and Shanghai Children Hospital with very\npositive feedback. And these big data technology can definitely be scaled to\nmuch more genomics life science partners in the world.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 14:21:52 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Li", "Hao", ""]]}, {"id": "1701.09049", "submitter": "Amit Awekar", "authors": "Panthadeep Bhattacharjee and Amit Awekar", "title": "Batch Incremental Shared Nearest Neighbor Density Based Clustering\n  Algorithm for Dynamic Datasets", "comments": "6 pages, Accepted at ECIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental data mining algorithms process frequent updates to dynamic\ndatasets efficiently by avoiding redundant computation. Existing incremental\nextension to shared nearest neighbor density based clustering (SNND) algorithm\ncannot handle deletions to dataset and handles insertions only one point at a\ntime. We present an incremental algorithm to overcome both these bottlenecks by\nefficiently identifying affected parts of clusters while processing updates to\ndataset in batch mode. We show effectiveness of our algorithm by performing\nexperiments on large synthetic as well as real world datasets. Our algorithm is\nup to four orders of magnitude faster than SNND and requires up to 60% extra\nmemory than SNND while providing output identical to SNND.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 14:19:18 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Bhattacharjee", "Panthadeep", ""], ["Awekar", "Amit", ""]]}]