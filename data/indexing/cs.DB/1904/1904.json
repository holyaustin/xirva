[{"id": "1904.00058", "submitter": "Andrey Rivkin", "authors": "Marco Montali, Andrey Rivkin", "title": "From DB-nets to Coloured Petri Nets with Priorities (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced formalism of DB-nets has brought in a new conceptual\nway of modelling complex dynamic systems that equally account for the process\nand data dimensions, considering local data as well as persistent,\ntransactional data. DB-nets combine a coloured variant of Petri nets with name\ncreation and management (which we call nu-CPN), with a relational database. The\nintegration of these two components is realized by equipping the net with\nspecial ``view'' places that query the database and expose the resulting\nanswers to the net, with actions that allow transitions to update the content\nof the database, and with special arcs capturing compensation in case of\ntransaction failure. In this work, we study whether this sophisticated model\ncan be encoded back into nu-CPNs. In particular, we show that the meaningful\nfragment of DB-nets where database queries are expressed using unions of\nconjunctive queries with inequalities can be faithfully encoded into $\\nu$-CPNs\nwith transition priorities. This allows us to directly exploit state-of-the-art\ntechnologies such as CPN Tools to simulate and analyse this relevant class of\nDB-nets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 19:11:42 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Montali", "Marco", ""], ["Rivkin", "Andrey", ""]]}, {"id": "1904.00079", "submitter": "Cigdem Aslay", "authors": "Cigdem Aslay, Martino Ciaperoni, Aristides Gionis, Michael\n  Mathioudakis", "title": "Query the model: precomputations for efficient inference with Bayesian\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable Elimination is a fundamental algorithm for probabilistic inference\nover Bayesian networks. In this paper, we propose a novel materialization\nmethod for Variable Elimination, which can lead to significant efficiency gains\nwhen answering inference queries. We evaluate our technique using real-world\nBayesian networks. Our results show that a modest amount of materialization can\nlead to significant improvements in the running time of queries. Furthermore,\nin comparison with junction tree methods that also rely on materialization, our\napproach achieves comparable efficiency during inference using significantly\nlighter materialization.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 20:17:48 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 15:25:57 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 10:14:11 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2021 22:47:22 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Aslay", "Cigdem", ""], ["Ciaperoni", "Martino", ""], ["Gionis", "Aristides", ""], ["Mathioudakis", "Michael", ""]]}, {"id": "1904.00234", "submitter": "Su Feng", "authors": "Su Feng, Aaron Huber, Boris Glavic, Oliver Kennedy", "title": "Uncertainty Annotated Databases - A Lightweight Approach for\n  Approximating Certain Answers (extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain answers are a principled method for coping with uncertainty that\narises in many practical data management tasks. Unfortunately, this method is\nexpensive and may exclude useful (if uncertain) answers. Thus, users frequently\nresort to less principled approaches to resolve the uncertainty. In this paper,\nwe propose Uncertainty Annotated Databases (UA-DBs), which combine an under-\nand over-approximation of certain answers to achieve the reliability of certain\nanswers, with the performance of a classical database system. Furthermore, in\ncontrast to prior work on certain answers, UA-DBs achieve a higher utility by\nincluding some (explicitly marked) answers that are not certain. UA-DBs are\nbased on incomplete K-relations, which we introduce to generalize the classical\nset-based notions of incomplete databases and certain answers to a much larger\nclass of data models. Using an implementation of our approach, we demonstrate\nexperimentally that it efficiently produces tight approximations of certain\nanswers that are of high utility.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 15:23:05 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Feng", "Su", ""], ["Huber", "Aaron", ""], ["Glavic", "Boris", ""], ["Kennedy", "Oliver", ""]]}, {"id": "1904.00411", "submitter": "Madhav Suresh", "authors": "Madhav Suresh, Zuohao She, William Wallace, Adel Lahlou, and Jennie\n  Rogers", "title": "KloakDB: A Platform for Analyzing Sensitive Data with $K$-anonymous\n  Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A private data federation enables data owners to pool their information for\nquerying without disclosing their secret tuples to one another. Here, a client\nqueries the union of the records of all data owners. The data owners work\ntogether to answer the query using privacy-preserving algorithms that prevent\nthem from learning unauthorized information about the inputs of their peers.\nOnly the client, and a federation coordinator, learn the query's output.\nKloakDB is a private data federation that uses trusted hardware to process SQL\nqueries over the inputs of two or more parties. Currently private data\nfederations compute their queries fully-obliviously, guaranteeing that no\ninformation is revealed about the sensitive inputs of a data owner to their\npeers by observing the query's instruction traces and memory access patterns.\nOblivious querying almost always exacts multiple orders of magnitude slowdown\nin query runtimes compared to plaintext execution, making it impractical for\nmany applications. KloakDB offers a semi-oblivious computing framework,\n$k$-anonymous query processing. We make the query's observable transcript\n$k$-anonymous because it is a popular standard for data release in many domains\nincluding medicine, educational research, and government data. KloakDB's\nqueries run such that each data owner may deduce information about no fewer\nthan $k$ individuals in the data of their peers. In addition, stakeholders set\n$k$, creating a novel trade-off between privacy and performance. Our results\nshow that KloakDB enjoys speedups of up to $117$X using k-anonymous query\nprocessing over full-oblivious evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 13:34:26 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 20:36:32 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Suresh", "Madhav", ""], ["She", "Zuohao", ""], ["Wallace", "William", ""], ["Lahlou", "Adel", ""], ["Rogers", "Jennie", ""]]}, {"id": "1904.00850", "submitter": "Miguel Romero", "authors": "Pablo Barcel\\'o, Diego Figueira, Miguel Romero", "title": "Boundedness of Conjunctive Regular Path Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DM cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the boundedness problem for unions of conjunctive regular path\nqueries with inverses (UC2RPQs). This is the problem of, given a UC2RPQ,\nchecking whether it is equivalent to a union of conjunctive queries (UCQ). We\nshow the problem to be ExpSpace-complete, thus coinciding with the complexity\nof containment for UC2RPQs. As a corollary, when a UC2RPQ is bounded, it is\nequivalent to a UCQ of at most triple-exponential size, and in fact we show\nthat this bound is optimal. We also study better behaved classes of UC2RPQs,\nnamely acyclic UC2RPQs of bounded thickness, and strongly connected UCRPQs,\nwhose boundedness problem are, respectively, PSpace-complete and\n$\\Pi^p_2$-complete. Most upper bounds exploit results on limitedness for\ndistance automata, in particular extending the model with alternation and\ntwo-wayness, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 13:47:58 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Barcel\u00f3", "Pablo", ""], ["Figueira", "Diego", ""], ["Romero", "Miguel", ""]]}, {"id": "1904.00934", "submitter": "Miguel Romero", "authors": "Pablo Barcel\\'o, Miguel Romero, Thomas Zeume", "title": "A More General Theory of Static Approximations for Conjunctive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DM cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjunctive query (CQ) evaluation is NP-complete, but becomes tractable for\nfragments of bounded hypertreewidth. Approximating a hard CQ by a query from\nsuch a fragment can thus allow for an efficient approximate evaluation. While\nunderapproximations (i.e., approximations that return correct answers only) are\nwell-understood, the dual notion of overapproximations (i.e, approximations\nthat return complete - but not necessarily sound - answers), and also a more\ngeneral notion of approximation based on the symmetric difference of query\nresults, are almost unexplored. In fact, the decidability of the basic problems\nof evaluation, identification, and existence of those approximations has been\nopen.\n  This article establishes a connection between overapproximations and\nexistential pebble games that allows for studying such problems systematically.\nBuilding on this connection, it is shown that the evaluation and identification\nproblem for overapproximations can be solved in polynomial time. While the\ngeneral existence problem remains open, the problem is shown to be decidable in\n2EXPTIME over the class of acyclic CQs and in PTIME for Boolean CQs over binary\nschemata. Additionally we propose a more liberal notion of overapproximations\nto remedy the known shortcoming that queries might not have an\noverapproximation, and study how queries can be overapproximated in the\npresence of tuple generating and equality generating dependencies.\n  The techniques are then extended to symmetric difference approximations and\nused to provide several complexity results for the identification, existence,\nand evaluation problem for this type of approximations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:12:04 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Barcel\u00f3", "Pablo", ""], ["Romero", "Miguel", ""], ["Zeume", "Thomas", ""]]}, {"id": "1904.01134", "submitter": "Amro Alasta", "authors": "Amro F. Alasta and Muftah A. Enaba", "title": "Data warehouse on Manpower Employment for Decision Support System", "comments": "nine pages; http://dx.doi.org/10.15242/ IJCCIE.E0913010", "journal-ref": "Int'l Journal of Computing, Communications & Instrumentation Engg.\n  (IJCCIE) Vol. 1, Issue 1 (2014) ISSN 2349-1469 EISSN 2349-1477", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the use of computers in the business world, data collection has become\none of the most important issues due to the available knowledge in the data;\nsuch data has been stored in the database. The database system was developed\nwhich led to the evolvement of hierarchical and relational database followed by\nStandard Query Language (SQL). As data size increases, the need for more\ncontrol and information retrieval increase. These increases lead to the\ndevelopment of data mining systems and data warehouses. This paper focuses on\nthe use of a data warehouse as a supporting tool in decision making. We to\nstudy the effectiveness of data warehouse techniques in the sense of time and\nflexibility in our case study (Manpower Employment). The study will conclude\nwith a comparison of traditional relational database and the use of data\nwarehouse. The fundamental role of a data warehouse is to provide data for\nsupporting the decision-making process. Data in a data warehouse environment is\na multidimensional data store. We can simply say that data warehouse is a\nprocess, not a product, for assembling and managing data from various sources\nfor the purpose of gaining a single detailed view of part or all an\nestablishment. The data warehouse concept has changed the nature of the\ndecision support system, by adding new benefits for improving and expanding the\nscope, accuracy, and accessibility of data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 22:42:21 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Alasta", "Amro F.", ""], ["Enaba", "Muftah A.", ""]]}, {"id": "1904.01279", "submitter": "Benjamin Hilprecht", "authors": "Benjamin Hilprecht, Carsten Binnig and Uwe Roehm", "title": "Learning a Partitioning Advisor with Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial data analytics products such as Microsoft Azure SQL Data Warehouse\nor Amazon Redshift provide ready-to-use scale-out database solutions for\nOLAP-style workloads in the cloud. While the provisioning of a database cluster\nis usually fully automated by cloud providers, customers typically still have\nto make important design decisions which were traditionally made by the\ndatabase administrator such as selecting the partitioning schemes.\n  In this paper we introduce a learned partitioning advisor for analytical\nOLAP-style workloads based on Deep Reinforcement Learning (DRL). The main idea\nis that a DRL agent learns its decisions based on experience by monitoring the\nrewards for different workloads and partitioning schemes. We evaluate our\nlearned partitioning advisor in an experimental evaluation with different\ndatabases schemata and workloads of varying complexity. In the evaluation, we\nshow that our advisor is not only able to find partitionings that outperform\nexisting approaches for automated partitioning design but that it also can\neasily adjust to different deployments. This is especially important in cloud\nsetups where customers can easily migrate their cluster to a new set of\n(virtual) machines.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:29:55 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Hilprecht", "Benjamin", ""], ["Binnig", "Carsten", ""], ["Roehm", "Uwe", ""]]}, {"id": "1904.01614", "submitter": "Alexander van Renen", "authors": "Alexander van Renen, Lukas Vogel, Viktor Leis, Thomas Neumann, Alfons\n  Kemper", "title": "Persistent Memory I/O Primitives", "comments": "7 pages, 6 figures, DaMoN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I/O latency and throughput is one of the major performance bottlenecks for\ndisk-based database systems. Upcoming persistent memory (PMem) technologies,\nlike Intel's Optane DC Persistent Memory Modules, promise to bridge the gap\nbetween NAND-based flash (SSD) and DRAM, and thus eliminate the I/O bottleneck.\nIn this paper, we provide one of the first performance evaluations of PMem in\nterms of bandwidth and latency. Based on the results, we develop guidelines for\nefficient PMem usage and two essential I/O primitives tuned for PMem: log\nwriting and block flushing.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 18:39:16 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 13:09:07 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 08:38:41 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["van Renen", "Alexander", ""], ["Vogel", "Lukas", ""], ["Leis", "Viktor", ""], ["Neumann", "Thomas", ""], ["Kemper", "Alfons", ""]]}, {"id": "1904.01831", "submitter": "Cai Shaofeng", "authors": "Shaofeng Cai, Gang Chen, Beng Chin Ooi, Jinyang Gao", "title": "Model Slicing for Supporting Complex Analytics with Elastic Inference\n  Cost and Resource Constraints", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": "10.14778/3364324.3364325", "report-no": null, "categories": "cs.LG cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have been used to support analytics beyond simple\naggregation, where deeper and wider models have been shown to yield great\nresults. These models consume a huge amount of memory and computational\noperations. However, most of the large-scale industrial applications are often\ncomputational budget constrained. In practice, the peak workload of inference\nservice could be 10x higher than the average cases, with the presence of\nunpredictable extreme cases. Lots of computational resources could be wasted\nduring off-peak hours and the system may crash when the workload exceeds system\ncapacity. How to support deep learning services with a dynamic workload\ncost-efficiently remains a challenging problem. In this paper, we address the\nchallenge with a general and novel training scheme called model slicing, which\nenables deep learning models to provide predictions within the prescribed\ncomputational resource budget dynamically. Model slicing could be viewed as an\nelastic computation solution without requiring more computational resources.\nSuccinctly, each layer in the model is divided into groups of a contiguous\nblock of basic components (i.e. neurons in dense layers and channels in\nconvolutional layers), and then partially ordered relation is introduced to\nthese groups by enforcing that groups participated in each forward pass always\nstarts from the first group to the dynamically-determined rightmost group.\nTrained by dynamically indexing the rightmost group with a single parameter\nslice rate, the network is engendered to build up group-wise and residual\nrepresentation. Then during inference, a sub-model with fewer groups can be\nreadily deployed for efficiency whose computation is roughly quadratic to the\nwidth controlled by the slice rate. Extensive experiments show that models\ntrained with model slicing can effectively support on-demand workload with\nelastic inference cost.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 08:16:24 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 08:49:08 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 07:06:20 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Shaofeng", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""], ["Gao", "Jinyang", ""]]}, {"id": "1904.01863", "submitter": "Xixi Lu", "authors": "Seyed Amin Tabatabaei, Xixi Lu, Mark Hoogendoorn, and Hajo A. Reijers", "title": "Identifying Patient Groups based on Frequent Patterns of Patient Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grouping patients meaningfully can give insights about the different types of\npatients, their needs, and the priorities. Finding groups that are meaningful\nis however very challenging as background knowledge is often required to\ndetermine what a useful grouping is. In this paper we propose an approach that\nis able to find groups of patients based on a small sample of positive examples\ngiven by a domain expert. Because of that, the approach relies on very limited\nefforts by the domain experts. The approach groups based on the activities and\ndiagnostic/billing codes within health pathways of patients. To define such a\ngrouping based on the sample of patients efficiently, frequent patterns of\nactivities are discovered and used to measure the similarity between the care\npathways of other patients to the patients in the sample group. This approach\nresults in an insightful definition of the group. The proposed approach is\nevaluated using several datasets obtained from a large university medical\ncenter. The evaluation shows F1-scores of around 0.7 for grouping kidney injury\nand around 0.6 for diabetes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 09:10:29 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Tabatabaei", "Seyed Amin", ""], ["Lu", "Xixi", ""], ["Hoogendoorn", "Mark", ""], ["Reijers", "Hajo A.", ""]]}, {"id": "1904.02033", "submitter": "Ilya Razenshteyn", "authors": "Hao Chen and Ilaria Chillotti and Yihe Dong and Oxana Poburinnaya and\n  Ilya Razenshteyn and M. Sadegh Riazi", "title": "SANNS: Scaling Up Secure Approximate k-Nearest Neighbors Search", "comments": "18 pages, to appear at USENIX Security Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-Nearest Neighbor Search ($k$-NNS) is the backbone of several\ncloud-based services such as recommender systems, face recognition, and\ndatabase search on text and images. In these services, the client sends the\nquery to the cloud server and receives the response in which case the query and\nresponse are revealed to the service provider. Such data disclosures are\nunacceptable in several scenarios due to the sensitivity of data and/or privacy\nlaws.\n  In this paper, we introduce SANNS, a system for secure $k$-NNS that keeps\nclient's query and the search result confidential. SANNS comprises two\nprotocols: an optimized linear scan and a protocol based on a novel sublinear\ntime clustering-based algorithm. We prove the security of both protocols in the\nstandard semi-honest model. The protocols are built upon several\nstate-of-the-art cryptographic primitives such as lattice-based additively\nhomomorphic encryption, distributed oblivious RAM, and garbled circuits. We\nprovide several contributions to each of these primitives which are applicable\nto other secure computation tasks. Both of our protocols rely on a new circuit\nfor the approximate top-$k$ selection from $n$ numbers that is built from $O(n\n+ k^2)$ comparators.\n  We have implemented our proposed system and performed extensive experimental\nresults on four datasets in two different computation environments,\ndemonstrating more than $18-31\\times$ faster response time compared to\noptimally implemented protocols from the prior work. Moreover, SANNS is the\nfirst work that scales to the database of 10 million entries, pushing the limit\nby more than two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:38:11 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 08:53:48 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 15:41:33 GMT"}, {"version": "v4", "created": "Wed, 20 Nov 2019 08:15:50 GMT"}, {"version": "v5", "created": "Sun, 8 Mar 2020 23:59:18 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Hao", ""], ["Chillotti", "Ilaria", ""], ["Dong", "Yihe", ""], ["Poburinnaya", "Oxana", ""], ["Razenshteyn", "Ilya", ""], ["Riazi", "M. Sadegh", ""]]}, {"id": "1904.02285", "submitter": "Theodoros Rekatsinas", "authors": "Alireza Heidari, Joshua McGrath, Ihab F. Ilyas, Theodoros Rekatsinas", "title": "HoloDetect: Few-Shot Learning for Error Detection", "comments": "18 pages,", "journal-ref": "ACM SIGMOD 2019", "doi": "10.1145/3299869.3319888", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a few-shot learning framework for error detection. We show that\ndata augmentation (a form of weak supervision) is key to training high-quality,\nML-based error detection models that require minimal human involvement. Our\nframework consists of two parts: (1) an expressive model to learn rich\nrepresentations that capture the inherent syntactic and semantic heterogeneity\nof errors; and (2) a data augmentation model that, given a small seed of clean\nrecords, uses dataset-specific transformations to automatically generate\nadditional training data. Our key insight is to learn data augmentation\npolicies from the noisy input dataset in a weakly supervised manner. We show\nthat our framework detects errors with an average precision of ~94% and an\naverage recall of ~93% across a diverse array of datasets that exhibit\ndifferent types and amounts of errors. We compare our approach to a\ncomprehensive collection of error detection methods, ranging from traditional\nrule-based methods to ensemble-based and active learning approaches. We show\nthat data augmentation yields an average improvement of 20 F1 points while it\nrequires access to 3x fewer labeled examples compared to other ML approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 00:38:59 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Heidari", "Alireza", ""], ["McGrath", "Joshua", ""], ["Ilyas", "Ihab F.", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "1904.02344", "submitter": "Eugene Wu", "authors": "Qianrui Zhang, Haoci Zhang, Thibault Sellam, Eugene Wu", "title": "Mining Precision Interfaces From Query Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive tools make data analysis more efficient and more accessible to\nend-users by hiding the underlying query complexity and exposing interactive\nwidgets for the parts of the query that matter to the analysis. However,\ncreating custom tailored (i.e., precise) interfaces is very costly, and\nautomated approaches are desirable. We propose a syntactic approach that uses\nqueries from an analysis to generate a tailored interface. We model interface\nwidgets as functions I(q) -> q' that modify the current analysis query $q$, and\ninterfaces as the set of queries that its widgets can express. Our system,\nPrecision Interfaces, analyzes structural changes between input queries from an\nanalysis, and generates an output interface with widgets to express those\nchanges. Our experiments on the Sloan Digital Sky Survey query log suggest that\nPrecision Interfaces can generate useful interfaces for simple unanticipated\ntasks, and our optimizations can generate interfaces from logs of up to 10,000\nqueries in <10s.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 04:40:38 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 17:44:24 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Zhang", "Qianrui", ""], ["Zhang", "Haoci", ""], ["Sellam", "Thibault", ""], ["Wu", "Eugene", ""]]}, {"id": "1904.02707", "submitter": "Renchi Yang", "authors": "Renchi Yang, Xiaokui Xiao, Zhewei Wei, Sourav S Bhowmick, Jun Zhao,\n  Rong-Hua Li", "title": "Efficient Estimation of Heat Kernel PageRank for Local Clustering", "comments": "The technical report for the full research paper accepted in the\n  SIGMOD 2019", "journal-ref": null, "doi": "10.1145/3299869.3319886", "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph G and a seed node s, the local clustering problem\naims to identify a high-quality cluster containing s in time roughly\nproportional to the size of the cluster, regardless of the size of G. This\nproblem finds numerous applications on large-scale graphs. Recently, heat\nkernel PageRank (HKPR), which is a measure of the proximity of nodes in graphs,\nis applied to this problem and found to be more efficient compared with prior\nmethods. However, existing solutions for computing HKPR either are\nprohibitively expensive or provide unsatisfactory error approximation on HKPR\nvalues, rendering them impractical especially on billion-edge graphs.\n  In this paper, we present TEA and TEA+, two novel local graph clustering\nalgorithms based on HKPR, to address the aforementioned limitations.\nSpecifically, these algorithms provide non-trivial theoretical guarantees in\nrelative error of HKPR values and the time complexity. The basic idea is to\nutilize deterministic graph traversal to produce a rough estimation of exact\nHKPR vector, and then exploit Monte-Carlo random walks to refine the results in\nan optimized and non-trivial way. In particular, TEA+ offers practical\nefficiency and effectiveness due to non-trivial optimizations. Extensive\nexperiments on real-world datasets demonstrate that TEA+ outperforms the\nstate-of-the-art algorithm by more than four times on most benchmark datasets\nin terms of computational time when achieving the same clustering quality, and\nin particular, is an order of magnitude faster on large graphs including the\nwidely studied Twitter and Friendster datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 06:16:50 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Yang", "Renchi", ""], ["Xiao", "Xiaokui", ""], ["Wei", "Zhewei", ""], ["Bhowmick", "Sourav S", ""], ["Zhao", "Jun", ""], ["Li", "Rong-Hua", ""]]}, {"id": "1904.03112", "submitter": "Nancy Awad", "authors": "Nancy Awad, Bechara Al Bouna, Jean-Francois Couchot, Laurent Philippe", "title": "Safe Disassociation of Set-Valued Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disassociation introduced by Terrovitis et al. is a bucketization based\nanonimyzation technique that divides a set-valued dataset into several clusters\nto hide the link between individuals and their complete set of items. It\nincreases the utility of the anonymized dataset, but on the other side, it\nraises many privacy concerns, one in particular, is when the items are tightly\ncoupled to form what is called, a cover problem. In this paper, we present safe\ndisassociation, a technique that relies on partial-suppression, to overcome the\naforementioned privacy breach encountered when disassociating set-valued\ndatasets. Safe disassociation allows the $k^m$-anonymity privacy constraint to\nbe extended to a bucketized dataset and copes with the cover problem. We\ndescribe our algorithm that achieves the safe disassociation and we provide a\nset of experiments to demonstrate its efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:11:23 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Awad", "Nancy", ""], ["Bouna", "Bechara Al", ""], ["Couchot", "Jean-Francois", ""], ["Philippe", "Laurent", ""]]}, {"id": "1904.03257", "submitter": "Alexander Ratner", "authors": "Alexander Ratner, Dan Alistarh, Gustavo Alonso, David G. Andersen,\n  Peter Bailis, Sarah Bird, Nicholas Carlini, Bryan Catanzaro, Jennifer Chayes,\n  Eric Chung, Bill Dally, Jeff Dean, Inderjit S. Dhillon, Alexandros Dimakis,\n  Pradeep Dubey, Charles Elkan, Grigori Fursin, Gregory R. Ganger, Lise Getoor,\n  Phillip B. Gibbons, Garth A. Gibson, Joseph E. Gonzalez, Justin Gottschlich,\n  Song Han, Kim Hazelwood, Furong Huang, Martin Jaggi, Kevin Jamieson, Michael\n  I. Jordan, Gauri Joshi, Rania Khalaf, Jason Knight, Jakub Kone\\v{c}n\\'y, Tim\n  Kraska, Arun Kumar, Anastasios Kyrillidis, Aparna Lakshmiratan, Jing Li,\n  Samuel Madden, H. Brendan McMahan, Erik Meijer, Ioannis Mitliagkas, Rajat\n  Monga, Derek Murray, Kunle Olukotun, Dimitris Papailiopoulos, Gennady\n  Pekhimenko, Theodoros Rekatsinas, Afshin Rostamizadeh, Christopher R\\'e,\n  Christopher De Sa, Hanie Sedghi, Siddhartha Sen, Virginia Smith, Alex Smola,\n  Dawn Song, Evan Sparks, Ion Stoica, Vivienne Sze, Madeleine Udell, Joaquin\n  Vanschoren, Shivaram Venkataraman, Rashmi Vinayak, Markus Weimer, Andrew\n  Gordon Wilson, Eric Xing, Matei Zaharia, Ce Zhang, Ameet Talwalkar", "title": "MLSys: The New Frontier of Machine Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DC cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) techniques are enjoying rapidly increasing adoption.\nHowever, designing and implementing the systems that support ML models in\nreal-world deployments remains a significant obstacle, in large part due to the\nradically different development and deployment profile of modern ML methods,\nand the range of practical concerns that come with broader adoption. We propose\nto foster a new systems machine learning research community at the intersection\nof the traditional systems and ML communities, focused on topics such as\nhardware systems for ML, software systems for ML, and ML optimized for metrics\nbeyond predictive accuracy. To do this, we describe a new conference, MLSys,\nthat explicitly targets research at the intersection of systems and machine\nlearning with a program committee split evenly between experts in systems and\nML, and an explicit focus on topics at the intersection of the two.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 12:43:36 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 04:55:56 GMT"}, {"version": "v3", "created": "Sun, 1 Dec 2019 20:27:06 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ratner", "Alexander", ""], ["Alistarh", "Dan", ""], ["Alonso", "Gustavo", ""], ["Andersen", "David G.", ""], ["Bailis", "Peter", ""], ["Bird", "Sarah", ""], ["Carlini", "Nicholas", ""], ["Catanzaro", "Bryan", ""], ["Chayes", "Jennifer", ""], ["Chung", "Eric", ""], ["Dally", "Bill", ""], ["Dean", "Jeff", ""], ["Dhillon", "Inderjit S.", ""], ["Dimakis", "Alexandros", ""], ["Dubey", "Pradeep", ""], ["Elkan", "Charles", ""], ["Fursin", "Grigori", ""], ["Ganger", "Gregory R.", ""], ["Getoor", "Lise", ""], ["Gibbons", "Phillip B.", ""], ["Gibson", "Garth A.", ""], ["Gonzalez", "Joseph E.", ""], ["Gottschlich", "Justin", ""], ["Han", "Song", ""], ["Hazelwood", "Kim", ""], ["Huang", "Furong", ""], ["Jaggi", "Martin", ""], ["Jamieson", "Kevin", ""], ["Jordan", "Michael I.", ""], ["Joshi", "Gauri", ""], ["Khalaf", "Rania", ""], ["Knight", "Jason", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Kraska", "Tim", ""], ["Kumar", "Arun", ""], ["Kyrillidis", "Anastasios", ""], ["Lakshmiratan", "Aparna", ""], ["Li", "Jing", ""], ["Madden", "Samuel", ""], ["McMahan", "H. Brendan", ""], ["Meijer", "Erik", ""], ["Mitliagkas", "Ioannis", ""], ["Monga", "Rajat", ""], ["Murray", "Derek", ""], ["Olukotun", "Kunle", ""], ["Papailiopoulos", "Dimitris", ""], ["Pekhimenko", "Gennady", ""], ["Rekatsinas", "Theodoros", ""], ["Rostamizadeh", "Afshin", ""], ["R\u00e9", "Christopher", ""], ["De Sa", "Christopher", ""], ["Sedghi", "Hanie", ""], ["Sen", "Siddhartha", ""], ["Smith", "Virginia", ""], ["Smola", "Alex", ""], ["Song", "Dawn", ""], ["Sparks", "Evan", ""], ["Stoica", "Ion", ""], ["Sze", "Vivienne", ""], ["Udell", "Madeleine", ""], ["Vanschoren", "Joaquin", ""], ["Venkataraman", "Shivaram", ""], ["Vinayak", "Rashmi", ""], ["Weimer", "Markus", ""], ["Wilson", "Andrew Gordon", ""], ["Xing", "Eric", ""], ["Zaharia", "Matei", ""], ["Zhang", "Ce", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1904.03336", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Han-Chieh Chao, Hamido Fujita and\n  Philip S. Yu", "title": "Correlated Utility-based Pattern Mining", "comments": "Elsevier Information Science, 15 pages", "journal-ref": "Information Sciences, 2019", "doi": "10.1016/j.ins.2019.07.005", "report-no": "504: 470-486", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of data mining and analytics, the utility theory from Economic\ncan bring benefits in many real-life applications. In recent decade, a new\nresearch field called utility-oriented mining has already attracted great\nattention. Previous studies have, however, the limitation that they rarely\nconsider the inherent correlation of items among patterns. Consider the\npurchase behaviors of consumer, a high-utility group of products (w.r.t.\nmulti-products) may contain several very high-utility products with some\nlow-utility products. However, it is considered as a valuable pattern even if\nthis behavior/pattern may be not highly correlated, or even happen by chance.\nIn this paper, in light of these challenges, we propose an efficient utility\nmining approach namely non-redundant Correlated high-Utility Pattern Miner\n(CoUPM) by taking positive correlation and profitable value into account. The\nderived patterns with high utility and strong positive correlation can lead to\nmore insightful availability than those patterns only have high profitable\nvalues. The utility-list structure is revised and applied to store necessary\ninformation of both correlation and utility. Several pruning strategies are\nfurther developed to improve the efficiency for discovering the desired\npatterns. Experimental results show that the non-redundant correlated\nhigh-utility patterns have more effectiveness than some other kinds of\ninteresting patterns. Moreover, efficiency of the proposed CoUPM algorithm\nsignificantly outperforms the state-of-the-art algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 01:54:57 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 03:23:29 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Chao", "Han-Chieh", ""], ["Fujita", "Hamido", ""], ["Yu", "Philip S.", ""]]}, {"id": "1904.03403", "submitter": "Francesco Parisi", "authors": "Francesco Parisi, John Grant", "title": "Inconsistency Measures for Relational Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, building on work done on measuring inconsistency in knowledge\nbases, we introduce inconsistency measures for databases. In particular,\nfocusing on databases with denial constraints, we first consider the natural\napproach of virtually transforming a database into a propositional knowledge\nbase and then applying well-known measures. However, using this method, tuples\nand constraints are equally considered in charge of inconsistencies. Then, we\nintroduce a version of inconsistency measures blaming database tuples only,\ni.e., treating integrity constraints as irrefutable statements.\n  We analyze the compliance of database inconsistency measures with standard\nrationality postulates and find interesting relationships between measures.\nFinally, we investigate the complexity of the inconsistency measurement problem\nas well as of the problems of deciding whether the inconsistency is lower than,\ngreater than, or equal to a given threshold.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:48:57 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Parisi", "Francesco", ""], ["Grant", "John", ""]]}, {"id": "1904.03587", "submitter": "Yongli Zhu", "authors": "Yongli Zhu, Lingpeng Shi, Renchang Dai, Guangyi Liu", "title": "Fast Grid Splitting Detection for N-1 Contingency Analysis by Graph\n  Computing", "comments": "This paper has been accepted by the IEEE ISGT-ASIA 2019 conference,\n  Chengdu, China, May.21-24, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a graph-computing based grid splitting detection algorithm is\nproposed for contingency analysis in a graph-based EMS (Energy Management\nSystem). The graph model of a power system is established by storing its\nbus-branch information into the corresponding vertex objects and edge objects\nof the graph database. Numerical comparison to an up-to-date serial computing\nalgorithm is also investigated. Online tests on a real power system of China\nState Grid with 2752 buses and 3290 branches show that a 6 times speedup can be\nachieved, which lays a good foundation for advanced contingency analysis.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 05:37:03 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhu", "Yongli", ""], ["Shi", "Lingpeng", ""], ["Dai", "Renchang", ""], ["Liu", "Guangyi", ""]]}, {"id": "1904.03604", "submitter": "Shuhao Zhang", "authors": "Shuhao Zhang and Jiong He and Amelie Chi Zhou and Bingsheng He", "title": "BriskStream: Scaling Data Stream Processing on Shared-Memory Multicore\n  Architectures", "comments": "To appear in SIGMOD'19", "journal-ref": "ACM SIGMOD/PODS International Conference on Management of Data\n  2019", "doi": "10.1145/3299869.3300067", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BriskStream, an in-memory data stream processing system (DSPSs)\nspecifically designed for modern shared-memory multicore architectures.\nBriskStream's key contribution is an execution plan optimization paradigm,\nnamely RLAS, which takes relative-location (i.e., NUMA distance) of each pair\nof producer-consumer operators into consideration. We propose a branch and\nbound based approach with three heuristics to resolve the resulting nontrivial\noptimization problem. The experimental evaluations demonstrate that BriskStream\nyields much higher throughput and better scalability than existing DSPSs on\nmulti-core architectures when processing different types of workloads.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 08:22:53 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zhang", "Shuhao", ""], ["He", "Jiong", ""], ["Zhou", "Amelie Chi", ""], ["He", "Bingsheng", ""]]}, {"id": "1904.03711", "submitter": "Ryan Marcus", "authors": "Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad\n  Alizadeh, Tim Kraska, Olga Papaemmanouil, Nesime Tatbul", "title": "Neo: A Learned Query Optimizer", "comments": null, "journal-ref": null, "doi": "10.14778/3342263.3342644", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query optimization is one of the most challenging problems in database\nsystems. Despite the progress made over the past decades, query optimizers\nremain extremely complex components that require a great deal of hand-tuning\nfor specific workloads and datasets. Motivated by this shortcoming and inspired\nby recent advances in applying machine learning to data management challenges,\nwe introduce Neo (Neural Optimizer), a novel learning-based query optimizer\nthat relies on deep neural networks to generate query executions plans. Neo\nbootstraps its query optimization model from existing optimizers and continues\nto learn from incoming queries, building upon its successes and learning from\nits failures. Furthermore, Neo naturally adapts to underlying data patterns and\nis robust to estimation errors. Experimental results demonstrate that Neo, even\nwhen bootstrapped from a simple optimizer like PostgreSQL, can learn a model\nthat offers similar performance to state-of-the-art commercial optimizers, and\nin some cases even surpass them.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 19:00:45 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Marcus", "Ryan", ""], ["Negi", "Parimarjan", ""], ["Mao", "Hongzi", ""], ["Zhang", "Chi", ""], ["Alizadeh", "Mohammad", ""], ["Kraska", "Tim", ""], ["Papaemmanouil", "Olga", ""], ["Tatbul", "Nesime", ""]]}, {"id": "1904.03800", "submitter": "Shuhao Zhang", "authors": "Shuhao Zhang, Yingjun Wu, Feng Zhang, and Bingsheng He", "title": "Towards Concurrent Stateful Stream Processing on Multicore Processors\n  (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent data stream processing systems (DSPSs) can achieve excellent\nperformance when processing large volumes of data under tight latency\nconstraints. However, they sacrifice support for concurrent state access that\neases the burden of developing stateful stream applications. Recently, some\nhave proposed managing concurrent state access during stream processing by\nmodeling state accesses as transactions. However, these are realized with locks\ninvolving serious contention overhead. Their coarse-grained processing paradigm\nfurther magnifies contention issues and tends to poorly utilize modern\nmulticore architectures. This paper introduces TStream , a novel DSPS\nsupporting efficient concurrent state access on multicore processors.\nTransactional semantics is employed like previous work, but scalability is\ngreatly improved due to two novel designs: 1) dual-mode scheduling, which\nexposes more parallelism opportunities, 2) dynamic restructuring execution,\nwhich aggressively exploits the parallelism opportunities from dual-mode\nscheduling without centralized lock contentions. To validate our proposal, we\nevaluate TStream with a benchmark of four applications on a modern multicore\nmachine. The experimental results show that 1) TStream achieves up to 4.8 times\nhigher throughput with similar processing latency compared to the\nstate-of-the-art and 2) unlike prior solutions, TStream is highly tolerant of\nvarying application workloads such as key skewness and multi-partition state\naccesses.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 02:10:03 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 04:00:55 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2019 02:40:31 GMT"}, {"version": "v4", "created": "Thu, 16 Jan 2020 06:17:26 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Zhang", "Shuhao", ""], ["Wu", "Yingjun", ""], ["Zhang", "Feng", ""], ["He", "Bingsheng", ""]]}, {"id": "1904.03934", "submitter": "Robert Brijder", "authors": "Robert Brijder and Marc Gyssens and Jan Van den Bussche", "title": "On matrices and $K$-relations", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the matrix query language $\\mathsf{MATLANG}$ corresponds to a\nnatural fragment of the positive relational algebra on $K$-relations. The\nfragment is defined by introducing a composition operator and restricting\n$K$-relation arities to two. We then proceed to show that $\\mathsf{MATLANG}$\ncan express all matrix queries expressible in the positive relational algebra\non $K$-relations, when intermediate arities are restricted to three. Thus we\noffer an analogue, in a model with numerical data, to the situation in\nclassical logic, where the algebra of binary relations is equivalent to\nfirst-order logic with three variables.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 10:26:35 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Brijder", "Robert", ""], ["Gyssens", "Marc", ""], ["Bussche", "Jan Van den", ""]]}, {"id": "1904.04045", "submitter": "Thomas Dybdahl Ahle", "authors": "Thomas Dybdahl Ahle, Jakob B{\\ae}k Tejs Knudsen", "title": "Subsets and Supermajorities: Optimal Hashing-based Set Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and optimally solve a new generalized Set Similarity Search\nproblem, which assumes the size of the database and query sets are known in\nadvance. By creating polylog copies of our data-structure, we optimally solve\nany symmetric Approximate Set Similarity Search problem, including approximate\nversions of Subset Search, Maximum Inner Product Search (MIPS), Jaccard\nSimilarity Search and Partial Match.\n  Our algorithm can be seen as a natural generalization of previous work on Set\nas well as Euclidean Similarity Search, but conceptually it differs by\noptimally exploiting the information present in the sets as well as their\ncomplements, and doing so asymmetrically between queries and stored sets. Doing\nso we improve upon the best previous work: MinHash [J. Discrete Algorithms\n1998], SimHash [STOC 2002], Spherical LSF [SODA 2016, 2017] and Chosen Path\n[STOC 2017] by as much as a factor $n^{0.14}$ in both time and space; or in the\nnear-constant time regime, in space, by an arbitrarily large polynomial factor.\n  Turning the geometric concept, based on Boolean supermajority functions, into\na practical algorithm requires ideas from branching random walks on $\\mathbb\nZ^2$, for which we give the first non-asymptotic near tight analysis.\n  Our lower bounds follow from new hypercontractive arguments, which can be\nseen as characterizing the exact family of similarity search problems for which\nsupermajorities are optimal. The optimality holds for among all hashing based\ndata structures in the random setting, and by reductions, for 1 cell and 2 cell\nprobe data structures. As a side effect, we obtain new hypercontractive bounds\non the directed noise operator $T^{p_1 \\to p_2}_\\rho$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 13:23:03 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 11:14:54 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ahle", "Thomas Dybdahl", ""], ["Knudsen", "Jakob B\u00e6k Tejs", ""]]}, {"id": "1904.04467", "submitter": "Zhengjie Miao", "authors": "Zhengjie Miao, Sudeepa Roy, Jun Yang", "title": "Explaining Wrong Queries Using Small Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For testing the correctness of SQL queries, e.g., evaluating student\nsubmissions in a database course, a standard practice is to execute the query\nin question on some test database instance and compare its result with that of\nthe correct query. Given two queries $Q_1$ and $Q_2$, we say that a database\ninstance $D$ is a counterexample (for $Q_1$ and $Q_2$) if $Q_1(D)$ differs from\n$Q_2(D)$; such a counterexample can serve as an explanation of why $Q_1$ and\n$Q_2$ are not equivalent. While the test database instance may serve as a\ncounterexample, it may be too large or complex to read and understand where the\ninequivalence comes from. Therefore, in this paper, given a known\ncounterexample $D$ for $Q_1$ and $Q_2$, we aim to find the smallest\ncounterexample $D' \\subseteq D$ where $Q_1(D') \\neq Q_2(D')$. The problem in\ngeneral is NP-hard. We give a suite of algorithms for finding the smallest\ncounterexample for different classes of queries, some more tractable than\nothers. We also present an efficient provenance-based algorithm for SPJUD\nqueries that uses a constraint solver, and extend it to more complex queries\nwith aggregation, group-by, and nested queries. We perform extensive\nexperiments indicating the effectiveness and scalability of our solution on\nstudent queries from an undergraduate database course and on queries from the\nTPC-H benchmark. We also report a user study from the course where we deployed\nour tool to help students with an assignment on relational algebra.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 04:56:47 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Miao", "Zhengjie", ""], ["Roy", "Sudeepa", ""], ["Yang", "Jun", ""]]}, {"id": "1904.04475", "submitter": "Xianrui Meng", "authors": "Xianrui Meng, Dimitrios Papadopoulos, Alina Oprea, Nikos Triandopoulos", "title": "Private Two-Party Cluster Analysis Made Formal & Scalable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) is widely used for predictive tasks in numerous\nimportant applications---most successfully, in the context of collaborative\nlearning, where a plurality of entities contribute their own datasets to\njointly deduce global ML models. Despite its efficacy, this new learning\nparadigm fails to encompass critical application domains, such as healthcare\nand security analytics, that involve learning over highly sensitive data,\nwherein privacy risks limit entities to individually deduce local models using\nsolely their own datasets.\n  In this work, we present the first comprehensive study for privacy-preserving\ncollaborative hierarchical clustering, overall featuring scalable cryptographic\nprotocols that allow two parties to safely perform cluster analysis over their\ncombined sensitive datasets. For this problem at hand, we introduce a formal\nsecurity notion that achieves the required balance between intended accuracy\nand privacy and presents a class of two-party hierarchical clustering protocols\nthat guarantee strong privacy protection, provable in our new security model.\nCrucially, our solution employs modular design and judicious use of\ncryptography to achieve high degrees of efficiency and extensibility.\nSpecifically, we extend our core protocol to obtain two secure variants that\nsignificantly improve performance, an optimized variant for single-linkage\nclustering and a scalable approximate variant. Finally, we provide a prototype\nimplementation of our approach and experimentally evaluate its feasibility and\nefficiency on synthetic and real datasets, obtaining encouraging results. For\nexample, end-to-end execution of our secure approximate protocol, over 1M\n10-dimensional records, completes in 35 sec, transferring only 896KB and\nachieving 97.09% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 05:58:28 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 04:09:45 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Meng", "Xianrui", ""], ["Papadopoulos", "Dimitrios", ""], ["Oprea", "Alina", ""], ["Triandopoulos", "Nikos", ""]]}, {"id": "1904.04702", "submitter": "Paul Ezhilchelvan", "authors": "Jim Webber, Paul Ezhilchelvan and Isi Mitrani", "title": "Modeling Corruption in Eventually-Consistent Graph Databases", "comments": "6 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a model and analysis of an eventually consistent graph database\nwhere loosely cooperating servers accept concurrent updates to a partitioned,\ndistributed graph. The model is high-fidelity and preserves design choices from\ncontemporary graph database management systems. To explore the problem space,\nwe use two common graph topologies as data models for realistic\nexperimentation. The analysis reveals, even assuming completely fault-free\nhardware and bug-free software, that if it is possible for updates to interfere\nwith one-another, corruption will occur and spread significantly through the\ngraph within the production database lifetime. Using our model, database\ndesigners and operators can compute the rate of corruption for their systems\nand determine whether they are sufficiently dependable for their intended use.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:35:42 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Webber", "Jim", ""], ["Ezhilchelvan", "Paul", ""], ["Mitrani", "Isi", ""]]}, {"id": "1904.04736", "submitter": "Marcus Paradies", "authors": "Bunjamin Memishi, Raja Appuswamy, Marcus Paradies", "title": "Cold Storage Data Archives: More Than Just a Bunch of Tapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of available sensor and derived data from large scientific\nexperiments, such as earth observation programs, radio astronomy sky surveys,\nand high-energy physics already exceeds the storage hardware globally\nfabricated per year. To that end, cold storage data archives are the---often\noverlooked---spearheads of modern big data analytics in scientific,\ndata-intensive application domains. While high-performance data analytics has\nreceived much attention from the research community, the growing number of\nproblems in designing and deploying cold storage archives has only received\nvery little attention.\n  In this paper, we take the first step towards bridging this gap in knowledge\nby presenting an analysis of four real-world cold storage archives from three\ndifferent application domains. In doing so, we highlight (i) workload\ncharacteristics that differentiate these archives from traditional,\nperformance-sensitive data analytics, (ii) design trade-offs involved in\nbuilding cold storage systems for these archives, and (iii) deployment\ntrade-offs with respect to migration to the public cloud. Based on our\nanalysis, we discuss several other important research challenges that need to\nbe addressed by the data management community.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:36:06 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Memishi", "Bunjamin", ""], ["Appuswamy", "Raja", ""], ["Paradies", "Marcus", ""]]}, {"id": "1904.05300", "submitter": "Xiangyu Ke", "authors": "Xiangyu Ke, Arijit Khan, Leroy Lim Hong Quan", "title": "An In-Depth Comparison of s-t Reliability Algorithms over Uncertain\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertain, or probabilistic, graphs have been increasingly used to represent\nnoisy linked data in many emerging applications, and have recently attracted\nthe attention of the database research community. A fundamental problem on\nuncertain graphs is the s-t reliability, which measures the probability that a\ntarget node t is reachable from a source node s in a probabilistic (or\nuncertain) graph, i.e., a graph where every edge is assigned a probability of\nexistence.\n  Due to the inherent complexity of the s-t reliability estimation problem\n(#P-hard), various sampling and indexing based efficient algorithms were\nproposed in the literature. However, since they have not been thoroughly\ncompared with each other, it is not clear whether the later algorithm\noutperforms the earlier ones. More importantly, the comparison framework,\ndatasets, and metrics were often not consistent (e.g., different convergence\ncriteria were employed to find the optimal number of samples) across these\nworks. We address this serious concern by re-implementing six state-of-the-art\ns-t reliability estimation methods in a common system and code base, using\nseveral medium and large-scale, real-world graph datasets, identical evaluation\nmetrics, and query workloads.\n  Through our systematic and in-depth analysis of experimental results, we\nreport surprising findings, such as many follow-up algorithms can actually be\nseveral orders of magnitude inefficient, less accurate, and more memory\nintensive compared to the ones that were proposed earlier. We conclude by\ndiscussing our recommendations on the road ahead.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:03:55 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Ke", "Xiangyu", ""], ["Khan", "Arijit", ""], ["Quan", "Leroy Lim Hong", ""]]}, {"id": "1904.05353", "submitter": "Mostafa Mirzaie", "authors": "Mostafa Mirzaie, Behshid Behkamal, Samad Paydar", "title": "Big Data Quality: A systematic literature review and future research\n  directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most significant problems of Big Data is to extract knowledge\nthrough the huge amount of data. The usefulness of the extracted information\ndepends strongly on data quality. In addition to the importance, data quality\nhas recently been taken into consideration by the big data community and there\nis not any comprehensive review conducted in this area. Therefore, the purpose\nof this study is to review and present the state of the art on the quality of\nbig data research through a hierarchical framework. The dimensions of the\nproposed framework cover various aspects in the quality assessment of Big Data\nincluding 1) the processing types of big data, i.e. stream, batch, and hybrid,\n2) the main task, and 3) the method used to conduct the task. We compare and\ncritically review all of the studies reported during the last ten years through\nour proposed framework to identify which of the available data quality\nassessment methods have been successfully adopted by the big data community.\nFinally, we provide a critical discussion on the limitations of existing\nmethods and offer suggestions on potential valuable research directions that\ncan be taken in future research in this domain.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 13:27:53 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 06:25:59 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 19:29:10 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Mirzaie", "Mostafa", ""], ["Behkamal", "Behshid", ""], ["Paydar", "Samad", ""]]}, {"id": "1904.06484", "submitter": "Michael Mireku Kwakye", "authors": "Michael Mireku Kwakye", "title": "Semantic Data Warehouse Modelling for Trajectories", "comments": "30 pages, 17 figures", "journal-ref": "University of Calgary Science Research & Publications, 2017-01-26", "doi": "10.11575/PRISM/31331", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trajectory patterns of a moving object in a spatio-temporal domain offers\nvaried information in terms of the management of the data generated from the\nmovement. A trajectory data warehouse is a data repository for the data and\ninformation of trajectory objects and their associated spatial objects for\ndefined temporal periods. The query results of trajectory objects from the data\nwarehouse are usually not enough to answer certain trend behaviours and\nmeaningful inferences without the associated semantic information of the\ntrajectory object or the geospatial environment within a specified purpose or\ncontext. This paper formulates and designs a generic ontology modelling\nframework that serves as the background model platform for the design of a\nsemantic data warehouse for trajectories. This semantic trajectory data\nwarehouse can be adaptable for trajectory data processing and analytics on any\nchosen spatio-temporal application domain. The methodology underpins on higher\ngranularity of data as a result of pre-processed and transformed ETL data so as\nto offer efficient semantic inference to the underlying trajectory data.\nMoreover, the approach outlines the thematic dimensions that serve as necessary\nentities for extracting semantic information. Additionally, the modelling\napproach offers a design platform for effective predictive trend analysis and\nknowledge discovery in the trajectory dynamics and data processing for moving\nobjects.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 05:11:17 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kwakye", "Michael Mireku", ""]]}, {"id": "1904.06492", "submitter": "Ester Livshits", "authors": "Ester Livshits, Rina Kochirgan, Segev Tsur, Ihab F. Ilyas, Benny\n  Kimelfeld, and Sudeepa Roy", "title": "Properties of Inconsistency Measures for Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should we quantify the inconsistency of a database that violates\nintegrity constraints? Proper measures are important for various tasks, such as\nprogress indication and action prioritization in cleaning systems, and\nreliability estimation for new datasets. To choose an appropriate inconsistency\nmeasure, it is important to identify the desired properties in the application\nand understand which of these is guaranteed or at least expected in practice.\nFor example, in some use cases the inconsistency should reduce if constraints\nare eliminated; in others it should be stable and avoid jitters and jumps in\nreaction to small changes in the database. We embark on a systematic\ninvestigation of properties for database inconsistency measures. We investigate\na collection of basic measures that have been proposed in the past in both the\nKnowledge Representation and Database communities, analyze their theoretical\nproperties, and empirically observe their behaviour in an experimental study.\nWe also demonstrate how the framework can lead to new inconsistency measures by\nintroducing a new measure that, in contrast to the rest, satisfies all of the\nproperties we consider and can be computed in polynomial time.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 06:38:02 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 07:01:01 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 19:52:12 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 17:03:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Livshits", "Ester", ""], ["Kochirgan", "Rina", ""], ["Tsur", "Segev", ""], ["Ilyas", "Ihab F.", ""], ["Kimelfeld", "Benny", ""], ["Roy", "Sudeepa", ""]]}, {"id": "1904.06766", "submitter": "Peter Lindner", "authors": "Martin Grohe and Peter Lindner", "title": "Infinite Probabilistic Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic databases (PDBs) are used to model uncertainty in data in a\nquantitative way. In the standard formal framework, PDBs are finite probability\nspaces over relational database instances. It has been argued convincingly that\nthis is not compatible with an open world semantics (Ceylan et al., KR 2016)\nand with application scenarios that are modeled by continuous probability\ndistributions (Dalvi et al., CACM 2009).\n  We recently introduced a model of PDBs as infinite probability spaces that\naddresses these issues (Grohe and Lindner, PODS 2019). While that work was\nmainly concerned with countably infinite probability spaces, our focus here is\non uncountable spaces. Such an extension is necessary to model typical\ncontinuous probability distributions that appear in many applications. However,\nan extension beyond countable probability spaces raises nontrivial foundational\nissues concerned with the measurability of events and queries and ultimately\nwith the question whether queries have a well-defined semantics.\n  It turns out that so-called finite point processes are the appropriate model\nfrom probability theory for dealing with probabilistic databases. This model\nallows us to construct suitable (uncountable) probability spaces of database\ninstances in a systematic way. Our main technical results are measurability\nstatements for relational algebra queries as well as aggregate queries and\ndatalog queries.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 21:47:17 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 13:22:08 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 11:45:23 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Grohe", "Martin", ""], ["Lindner", "Peter", ""]]}, {"id": "1904.07124", "submitter": "Wei Bi", "authors": "Wei Bi, Xiangyu Liu, Maolin Zheng", "title": "$\\varepsilon$-differential agreement: A Parallel Data Sorting Mechanism\n  for Distributed Information Processing System", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The order of the input information plays a very important role in a\ndistributed information processing system (DIPS). This paper proposes a novel\ndata sorting mechanism named the {\\epsilon}-differential agreement (EDA) that\ncan support parallel data sorting. EDA adopts the collaborative consensus\nmechanism which is different from the traditional consensus mechanisms using\nthe competition mechanism, such as PoS, PoW, etc. In the system that employs\nthe EDA mechanism, all participants work together to compute the order of the\ninput information by using statistical and probability principles on a\nproportion of participants. Preliminary results show variable fault-tolerant\nrates and consensus delay for systems that have different configurations when\nreaching consensus, thus it suggests that it is possible to use EDA in a system\nand customize these parameters based on different requirements. With the unique\nmechanism, EDA can be used in DIPS of multi-center decision cluster, not just\nthe rotating center decision cluster.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 13:09:46 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Bi", "Wei", ""], ["Liu", "Xiangyu", ""], ["Zheng", "Maolin", ""]]}, {"id": "1904.07421", "submitter": "Sheng Zha", "authors": "Sheng Zha, Ziheng Jiang, Haibin Lin, Zhi Zhang", "title": "Just-in-Time Dynamic-Batching", "comments": "NeurIPS 2018 Systems for ML Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batching is an essential technique to improve computation efficiency in deep\nlearning frameworks. While batch processing for models with static feed-forward\ncomputation graphs is straightforward to implement, batching for dynamic\ncomputation graphs such as syntax trees or social network graphs is challenging\ndue to variable computation graph structure across samples. Through simulation\nand analysis of a Tree-LSTM model, we show the key trade-off between graph\nanalysis time and batching effectiveness in dynamic batching. Based on this\nfinding, we propose a dynamic batching method as an extension to MXNet Gluon's\njust-in-time compilation (JIT) framework. We show empirically that our method\nyields up to 6.25 times speed-up on a common dynamic workload, a tree-LSTM\nmodel for the semantic relatedness task.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 03:00:51 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zha", "Sheng", ""], ["Jiang", "Ziheng", ""], ["Lin", "Haibin", ""], ["Zhang", "Zhi", ""]]}, {"id": "1904.07619", "submitter": "Giulio Ermanno Pibiri", "authors": "Raffaele Perego, Giulio Ermanno Pibiri, Rossano Venturini", "title": "Compressed Indexes for Fast Search of Semantic Data", "comments": "Published in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE), 14 January 2020", "journal-ref": null, "doi": "10.1109/TKDE.2020.2966609", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sheer increase in volume of RDF data demands efficient solutions for the\ntriple indexing problem, that is devising a compressed data structure to\ncompactly represent RDF triples by guaranteeing, at the same time, fast pattern\nmatching operations. This problem lies at the heart of delivering good\npractical performance for the resolution of complex SPARQL queries on large RDF\ndatasets. In this work, we propose a trie-based index layout to solve the\nproblem and introduce two novel techniques to reduce its space of\nrepresentation for improved effectiveness. The extensive experimental analysis\nconducted over a wide range of publicly available real-world datasets, reveals\nthat our best space/time trade-off configuration substantially outperforms\nexisting solutions at the state-of-the-art, by taking 30-60% less space and\nspeeding up query execution by a factor of 2-81x.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 12:27:24 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 07:42:26 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 09:28:13 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Perego", "Raffaele", ""], ["Pibiri", "Giulio Ermanno", ""], ["Venturini", "Rossano", ""]]}, {"id": "1904.07693", "submitter": "Jonas N\\\"u{\\ss}lein", "authors": "Jonas N\\\"u{\\ss}lein", "title": "Most Frequent Itemset Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are dealing with the frequent itemset mining. We concentrate\non the special case that we only want to identify the most frequent itemset of\nlength N. To do that, we present a pattern on how to consider this search as an\noptimization problem. First, we extract the frequency of all possible\n2-item-sets. Then the optimization problem is to find the N objects, for which\nthe minimal frequency of all containing 2-item-sets is maximal. This\ncombinatorial optimization problem can be solved by any optimization algorithm.\nWe will solve them with Quantum Annealing and QUBO with QbSolv by D-Wave. The\nadvantages of MFIO in comparison to the state-of-the-art-approach are the\nenormous reduction of time need, reduction of memory need and the omission of a\nthreshold. The disadvantage is that there is no guaranty for accuracy of the\nresult. The evaluation indicates good results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 14:09:51 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 17:00:35 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["N\u00fc\u00dflein", "Jonas", ""]]}, {"id": "1904.07764", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Jiexiong Zhang, Han-Chieh Chao,\n  Hamido Fujita and Philip S. Yu", "title": "ProUM: Projection-based Utility Mining on Sequence Data", "comments": "Elsevier Information Science, 17 pages, 4 figures", "journal-ref": "Information Science, 2020", "doi": "10.1016/j.ins.2019.10.033", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utility is an important concept in economics. A variety of applications\nconsider utility in real-life situations, which has lead to the emergence of\nutility-oriented mining (also called utility mining) in the recent decade.\nUtility mining has attracted a great amount of attention, but most of the\nexisting studies have been developed to deal with itemset-based data.\nTime-ordered sequence data is more commonly seen in real-world situations,\nwhich is different from itemset-based data. Since they are time-consuming and\nrequire large amount of memory usage, current utility mining algorithms still\nhave limitations when dealing with sequence data. In addition, the mining\nefficiency of utility mining on sequence data still needs to be improved,\nespecially for long sequences or when there is a low minimum utility threshold.\nIn this paper, we propose an efficient Projection-based Utility Mining (ProUM)\napproach to discover high-utility sequential patterns from sequence data. The\nutility-array structure is designed to store the necessary information of the\nsequence-order and utility. ProUM can significantly improve the mining\nefficiency by utilizing the projection technique in generating utility-array,\nand it effectively reduces the memory consumption. Furthermore, a new upper\nbound named sequence extension utility is proposed and several pruning\nstrategies are further applied to improve the efficiency of ProUM. By taking\nutility theory into account, the derived high-utility sequential patterns have\nmore insightful and interesting information than other kinds of patterns.\nExperimental results showed that the proposed ProUM algorithm significantly\noutperformed the state-of-the-art algorithms in terms of execution time, memory\nusage, and scalability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:34:40 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 09:18:50 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Zhang", "Jiexiong", ""], ["Chao", "Han-Chieh", ""], ["Fujita", "Hamido", ""], ["Yu", "Philip S.", ""]]}, {"id": "1904.08010", "submitter": "Mathieu Roche", "authors": "Mathieu Roche", "title": "How to define co-occurrence in different domains of study?", "comments": "CICLING'2018 (International Conference on Computational Linguistics\n  and Intelligent Text Processing) - March 18 to 24, 2018 - Hanoi, Vietnam (not\n  published in CICLING proceedings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper presents a comparative study of co-occurrences. Some\nsimilarities and differences in the definition exist depending on the research\ndomain (e.g. linguistics, NLP, computer science). This paper discusses these\npoints, and deals with the methodological aspects in order to identify\nco-occurrences in a multidisciplinary paradigm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 23:16:56 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Roche", "Mathieu", ""]]}, {"id": "1904.08119", "submitter": "Sho Nakazono", "authors": "Sho Nakazono, Hiroyuki Uchiyama, Yasuhiro Fujiwara, Yasuhiro Nakamura,\n  Hideyuki Kawashima", "title": "NWR: Rethinking Thomas Write Rule for Omittable Write Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrency control protocols are the key to scaling current DBMS\nperformances. They efficiently interleave read and write operations in\ntransactions, but occasionally they restrict concurrency by using coordination\nsuch as exclusive lockings. Although exclusive lockings ensure the correctness\nof DBMS, it incurs serious performance penalties on multi-core environments. In\nparticular, existing protocols generally suffer from emerging highly write\ncontended workloads, since they use innumerable lockings for write operations.\nIn this paper, we rethink the Thomas write rule (TWR), which allows the\ntimestamp ordering (T/O) protocol to omit write operations without any\nlockings. We formalize the notion of omitting and decouple it from the T/O\nprotocol implementation, in order to define a new rule named non-visible write\nrule (NWR). When the rules of NWR are satisfied, any protocol can in theory\ngenerate omittable write operations with preserving the correctness without any\nlockings. In the experiments, we implement three NWR-extended protocols:\nSilo+NWR, TicToc+NWR, and MVTO+NWR. Experimental results demonstrate the\nefficiency and the low-overhead property of the extended protocols. We confirm\nthat NWR-extended protocols achieve more than 11x faster than the originals in\nthe best case of highly write contended YCSB-A and comparable performance with\nthe originals in the other workloads.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:13:16 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 10:04:14 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 08:37:25 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Nakazono", "Sho", ""], ["Uchiyama", "Hiroyuki", ""], ["Fujiwara", "Yasuhiro", ""], ["Nakamura", "Yasuhiro", ""], ["Kawashima", "Hideyuki", ""]]}, {"id": "1904.08147", "submitter": "Yuean Zhu", "authors": "Huaibing Jian and Yuean Zhu and Yongchao Long and Bin Li and Shu Wang\n  and Xiliang Wu and Zhichu Zhong", "title": "In Search of a Key Value Store with High Performance and High\n  Availability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent year, the write-heavy applications is more and more prevalent. How\nto efficiently handle this sort of workload is one of intensive research\ndirection in the field of database system. The overhead caused by write\noperation is mainly issued by two reasons: 1) the hardware level, i.e., the IO\ncost caused by logging. We can't remove this cost in short period 2) the\ndual-copy software architecture and serial replay. The born of log as database\narchitecture is originated to overcome the software defect. But existing\nsystems treating log as database either are built on top of special\ninfrastructure such as infiniband or NVRam (Non-Volatile Random access memory)\nwhich is far from widely available or are constructed with the help of other\nsystem such as Dynamo which is lack of flexibility. In this paper we build only\nwrite-once key-value system called LogStore from scratch to support our instant\nmessenger business. The key features of LogStore include: 1) a single thread\nper partition executing mode, which eliminates the concurrency overhead; 2) log\nas database to enable write-once feature and freshness on the standby. We\nachieve high availability by embedding replication protocol other than\ndependent on other infrastructure; 3) fine-grained and low overhead data buffer\npool management to effectively minimize IO cost. According to our empirical\nevaluations LogStore has good performance in write operation, recovery and\nreplication\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:15:22 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Jian", "Huaibing", ""], ["Zhu", "Yuean", ""], ["Long", "Yongchao", ""], ["Li", "Bin", ""], ["Wang", "Shu", ""], ["Wu", "Xiliang", ""], ["Zhong", "Zhichu", ""]]}, {"id": "1904.08223", "submitter": "Andreas Kipf", "authors": "Andreas Kipf, Dimitri Vorona, Jonas M\\\"uller, Thomas Kipf, Bernhard\n  Radke, Viktor Leis, Peter Boncz, Thomas Neumann, Alfons Kemper", "title": "Estimating Cardinalities with Deep Sketches", "comments": "To appear in SIGMOD'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep Sketches, which are compact models of databases that allow\nus to estimate the result sizes of SQL queries. Deep Sketches are powered by a\nnew deep learning approach to cardinality estimation that can capture\ncorrelations between columns, even across tables. Our demonstration allows\nusers to define such sketches on the TPC-H and IMDb datasets, monitor the\ntraining process, and run ad-hoc queries against trained sketches. We also\nestimate query cardinalities with HyPer and PostgreSQL to visualize the gains\nover traditional cardinality estimators.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:29:28 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Kipf", "Andreas", ""], ["Vorona", "Dimitri", ""], ["M\u00fcller", "Jonas", ""], ["Kipf", "Thomas", ""], ["Radke", "Bernhard", ""], ["Leis", "Viktor", ""], ["Boncz", "Peter", ""], ["Neumann", "Thomas", ""], ["Kemper", "Alfons", ""]]}, {"id": "1904.08679", "submitter": "Ester Livshits", "authors": "Ester Livshits, Leopoldo Bertossi, Benny Kimelfeld and Moshe Sebag", "title": "The Shapley Value of Tuples in Query Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the application of the Shapley value to quantifying the\ncontribution of a tuple to a query answer. The Shapley value is a widely known\nnumerical measure in cooperative game theory and in many applications of game\ntheory for assessing the contribution of a player to a coalition game. It has\nbeen established already in the 1950s, and is theoretically justified by being\nthe very single wealth-distribution measure that satisfies some natural axioms.\nWhile this value has been investigated in several areas, it received little\nattention in data management. We study this measure in the context of\nconjunctive and aggregate queries by defining corresponding coalition games. We\nprovide algorithmic and complexity-theoretic results on the computation of\nShapley-based contributions to query answers; and for the hard cases we present\napproximation algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 10:43:12 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 10:22:31 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 18:59:51 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Livshits", "Ester", ""], ["Bertossi", "Leopoldo", ""], ["Kimelfeld", "Benny", ""], ["Sebag", "Moshe", ""]]}, {"id": "1904.08741", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti, Boris Cule", "title": "Mining Closed Episodes with Simultaneous Events", "comments": null, "journal-ref": null, "doi": "10.1145/2020408.2020589", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential pattern discovery is a well-studied field in data mining. Episodes\nare sequential patterns describing events that often occur in the vicinity of\neach other. Episodes can impose restrictions to the order of the events, which\nmakes them a versatile technique for describing complex patterns in the\nsequence. Most of the research on episodes deals with special cases such as\nserial, parallel, and injective episodes, while discovering general episodes is\nunderstudied.\n  In this paper we extend the definition of an episode in order to be able to\nrepresent cases where events often occur simultaneously. We present an\nefficient and novel miner for discovering frequent and closed general episodes.\nSuch a task presents unique challenges. Firstly, we cannot define closure based\non frequency. We solve this by computing a more conservative closure that we\nuse to reduce the search space and discover the closed episodes as a\npostprocessing step. Secondly, episodes are traditionally presented as directed\nacyclic graphs. We argue that this representation has drawbacks leading to\nredundancy in the output. We solve these drawbacks by defining a subset\nrelationship in such a way that allows us to remove the redundant episodes. We\ndemonstrate the efficiency of our algorithm and the need for using closed\nepisodes empirically on synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 22:53:04 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Cule", "Boris", ""]]}, {"id": "1904.08819", "submitter": "Mosab Hassaan", "authors": "Mosab Hassaan and Karam Gouda", "title": "New Subgraph Isomorphism Algorithms: Vertex versus Path-at-a-time\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are widely used to model complicated data semantics in many\napplication domains. In this paper, two novel and efficient algorithms Fast-ON\nand Fast-P are proposed for solving the subgraph isomorphism problem. The two\nalgorithms are based on Ullman algorithm [Ullmann 1976], apply vertex-at-a-time\nmatching manner and path-at-a-time matching manner respectively, and use\neffective heuristics to cut the search space. Comparing to the well-known\nalgorithms, Fast-ON and Fast-P achieve up to 1-4 orders of magnitude speed-up\nfor both dense and sparse graph data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 14:50:00 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Hassaan", "Mosab", ""], ["Gouda", "Karam", ""]]}, {"id": "1904.08835", "submitter": "Dongjun Lee", "authors": "Dongjun Lee", "title": "Clause-Wise and Recursive Decoding for Complex and Cross-Domain\n  Text-to-SQL Generation", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most deep learning approaches for text-to-SQL generation are limited to the\nWikiSQL dataset, which only supports very simple queries over a single table.\nWe focus on the Spider dataset, a complex and cross-domain text-to-SQL task,\nwhich includes complex queries over multiple tables. In this paper, we propose\na SQL clause-wise decoding neural architecture with a self-attention based\ndatabase schema encoder to address the Spider task. Each of the clause-specific\ndecoders consists of a set of sub-modules, which is defined by the syntax of\neach clause. Additionally, our model works recursively to support nested\nqueries. When evaluated on the Spider dataset, our approach achieves 4.6\\% and\n9.8\\% accuracy gain in the test and dev sets, respectively. In addition, we\nshow that our model is significantly more effective at predicting complex and\nnested queries than previous work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:20:45 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 08:58:05 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Lee", "Dongjun", ""]]}, {"id": "1904.09231", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti, Boris Cule", "title": "Mining Closed Strict Episodes", "comments": "Journal version. The previous version is the conference version", "journal-ref": null, "doi": "10.1007/s10618-011-0232-z", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering patterns in a sequence is an important aspect of data mining. One\npopular choice of such patterns are episodes, patterns in sequential data\ndescribing events that often occur in the vicinity of each other. Episodes also\nenforce in which order the events are allowed to occur.\n  In this work we introduce a technique for discovering closed episodes.\nAdopting existing approaches for discovering traditional patterns, such as\nclosed itemsets, to episodes is not straightforward. First of all, we cannot\ndefine a unique closure based on frequency because an episode may have several\nclosed superepisodes. Moreover, to define a closedness concept for episodes we\nneed a subset relationship between episodes, which is not trivial to define.\n  We approach these problems by introducing strict episodes. We argue that this\nclass is general enough, and at the same time we are able to define a natural\nsubset relationship within it and use it efficiently. In order to mine closed\nepisodes we define an auxiliary closure operator. We show that this closure\nsatisfies the needed properties so that we can use the existing framework for\nmining closed patterns. Discovering the true closed episodes can be done as a\npost-processing step. We combine these observations into an efficient mining\nalgorithm and demonstrate empirically its performance in practice.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 00:24:47 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 23:19:21 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Cule", "Boris", ""]]}, {"id": "1904.09262", "submitter": "Hagit Shatkay", "authors": "Hagit Shatkay and Stanley B. Zdonik", "title": "Approximate Queries and Representations for Large Data Sequences", "comments": "One of the earliest papers on similarity queries over time series\n  data, and of symbolic representation of signals (cardio-signals and ECG in\n  particular). Presents the idea of feature conservation under transformation,\n  and uses piecewise linear or simple-polynomial approximation", "journal-ref": "Proceedings of the International Conference on Data Engineering\n  (ICDE), 1996, pp. 536-545", "doi": "10.1109/ICDE.1996.492204", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many new database application domains such as experimental sciences and\nmedicine are characterized by large sequences as their main form of data. Using\napproximate representation can significantly reduce the required storage and\nsearch space. A good choice of representation, can support a broad new class of\napproximate queries, needed in these domains. These queries are concerned with\napplication dependent features of the data as opposed to the actual sampled\npoints. We introduce a new notion of generalized approximate queries and a\ngeneral divide and conquer approach that supports them. This approach uses\nfamilies of real-valued functions as an approximate representation. We present\nan algorithm for realizing our technique, and the results of applying it to\nmedical cardiology data.\n  (Extended version is available in Tech Report CS-95-03, Dept of Computer\nScience, Brown University.\nhttp://cs.brown.edu/research/pubs/techreports/reports/CS-95-03.html)\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 16:45:20 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Shatkay", "Hagit", ""], ["Zdonik", "Stanley B.", ""]]}, {"id": "1904.09399", "submitter": "Ali Sadeghian", "authors": "Xiaofeng Zhou, Ali Sadeghian, Daisy Zhe Wang", "title": "Mining Rules Incrementally over Large Knowledge Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple web-scale Knowledge Bases, e.g., Freebase, YAGO, NELL, have been\nconstructed using semi-supervised or unsupervised information extraction\ntechniques and many of them, despite their large sizes, are continuously\ngrowing. Much research effort has been put into mining inference rules from\nknowledge bases. To address the task of rule mining over evolving web-scale\nknowledge bases, we propose a parallel incremental rule mining framework. Our\napproach is able to efficiently mine rules based on the relational model and\napply updates to large knowledge bases; we propose an alternative metric that\nreduces computation complexity without compromising quality; we apply multiple\noptimization techniques that reduce runtime by more than 2 orders of magnitude.\nExperiments show that our approach efficiently scales to web-scale knowledge\nbases and saves over 90% time compared to the state-of-the-art batch rule\nmining system. We also apply our optimization techniques to the batch rule\nmining algorithm, reducing runtime by more than half compared to the\nstate-of-the-art. To the best of our knowledge, our incremental rule mining\nsystem is the first that handles updates to web-scale knowledge bases.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 04:09:01 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhou", "Xiaofeng", ""], ["Sadeghian", "Ali", ""], ["Wang", "Daisy Zhe", ""]]}, {"id": "1904.09483", "submitter": "Peng Li", "authors": "Peng Li, Xi Rao, Jennifer Blase, Yue Zhang, Xu Chu, Ce Zhang", "title": "CleanML: A Study for Evaluating the Impact of Data Cleaning on ML\n  Classification Tasks", "comments": "published in ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quality affects machine learning (ML) model performances, and data\nscientists spend considerable amount of time on data cleaning before model\ntraining. However, to date, there does not exist a rigorous study on how\nexactly cleaning affects ML -- ML community usually focuses on developing ML\nalgorithms that are robust to some particular noise types of certain\ndistributions, while database (DB) community has been mostly studying the\nproblem of data cleaning alone without considering how data is consumed by\ndownstream ML analytics. We propose a CleanML study that systematically\ninvestigates the impact of data cleaning on ML classification tasks. The\nopen-source and extensible CleanML study currently includes 14 real-world\ndatasets with real errors, five common error types, seven different ML models,\nand multiple cleaning algorithms for each error type (including both commonly\nused algorithms in practice as well as state-of-the-art solutions in academic\nliterature). We control the randomness in ML experiments using statistical\nhypothesis testing, and we also control false discovery rate in our experiments\nusing the Benjamini-Yekutieli (BY) procedure. We analyze the results in a\nsystematic way to derive many interesting and nontrivial observations. We also\nput forward multiple research directions for researchers.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 19:12:03 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 00:17:24 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 23:35:41 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Li", "Peng", ""], ["Rao", "Xi", ""], ["Blase", "Jennifer", ""], ["Zhang", "Yue", ""], ["Chu", "Xu", ""], ["Zhang", "Ce", ""]]}, {"id": "1904.09610", "submitter": "Mengsu Ding", "authors": "Mengsu Ding, Muqiao Yang, Shimin Chen", "title": "Storing and Querying Large-Scale Spatio-Temporal Graphs with\n  High-Throughput Edge Insertions", "comments": "Section 5.1.1 is modified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world graphs often contain spatio-temporal information and evolve over\ntime. Compared with static graphs, spatio-temporal graphs have very different\ncharacteristics, presenting more significant challenges in data volume, data\nvelocity, and query processing. In this paper, we describe three representative\napplications to understand the features of spatio-temporal graphs. Based on the\ncommonalities of the applications, we define a formal spatio-temporal graph\nmodel, where a graph consists of location vertices, object vertices, and event\nedges. Then we discuss a set of design goals to meet the requirements of the\napplications: (i) supporting up to 10 billion object vertices, 10 million\nlocation vertices, and 100 trillion edges in the graph, (ii) supporting up to 1\ntrillion new edges that are streamed in daily, and (iii) minimizing\ncross-machine communication for query processing. We propose and evaluate PAST,\na framework for efficient PArtitioning and query processing of Spatio-Temporal\ngraphs. Experimental results show that PAST successfully achieves the above\ngoals. It improves query performance by orders of magnitude compared with\nstate-of-the-art solutions, including JanusGraph, Greenplum, Spark and\nST-Hadoop.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 14:52:00 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 10:35:11 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Ding", "Mengsu", ""], ["Yang", "Muqiao", ""], ["Chen", "Shimin", ""]]}, {"id": "1904.10217", "submitter": "Woohwan Jung", "authors": "Woohwan Jung, Younghoon Kim and Kyuseok Shim", "title": "Crowdsourced Truth Discovery in the Presence of Hierarchies for\n  Knowledge Fusion", "comments": null, "journal-ref": "Proceedings of the 22nd International Conference on Extending\n  Database Technology, 2019. pp. 205-216", "doi": "10.5441/002/edbt.2019.19", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing works for truth discovery in categorical data usually assume that\nclaimed values are mutually exclusive and only one among them is correct.\nHowever, many claimed values are not mutually exclusive even for functional\npredicates due to their hierarchical structures. Thus, we need to consider the\nhierarchical structure to effectively estimate the trustworthiness of the\nsources and infer the truths. We propose a probabilistic model to utilize the\nhierarchical structures and an inference algorithm to find the truths. In\naddition, in the knowledge fusion, the step of automatically extracting\ninformation from unstructured data (e.g., text) generates a lot of false\nclaims. To take advantages of the human cognitive abilities in understanding\nunstructured data, we utilize crowdsourcing to refine the result of the truth\ndiscovery. We propose a task assignment algorithm to maximize the accuracy of\nthe inferred truths. The performance study with real-life datasets confirms the\neffectiveness of our truth inference and task assignment algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 09:23:47 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Jung", "Woohwan", ""], ["Kim", "Younghoon", ""], ["Shim", "Kyuseok", ""]]}, {"id": "1904.10578", "submitter": "Yang Cao", "authors": "Maho Asada, Masatoshi Yoshikawa, and Yang Cao", "title": "When and where do you want to hide? Recommendation of location privacy\n  preferences with local differential privacy", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-22479-0_9", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, it has become easy to obtain location information quite\nprecisely. However, the acquisition of such information has risks such as\nindividual identification and leakage of sensitive information, so it is\nnecessary to protect the privacy of location information. For this purpose,\npeople should know their location privacy preferences, that is, whether or not\nhe/she can release location information at each place and time. However, it is\nnot easy for each user to make such decisions and it is troublesome to set the\nprivacy preference at each time. Therefore, we propose a method to recommend\nlocation privacy preferences for decision making. Comparing to existing method,\nour method can improve the accuracy of recommendation by using matrix\nfactorization and preserve privacy strictly by local differential privacy,\nwhereas the existing method does not achieve formal privacy guarantee. In\naddition, we found the best granularity of a location privacy preference, that\nis, how to express the information in location privacy protection. To evaluate\nand verify the utility of our method, we have integrated two existing datasets\nto create a rich information in term of user number. From the results of the\nevaluation using this dataset, we confirmed that our method can predict\nlocation privacy preferences accurately and that it provides a suitable method\nto define the location privacy preference.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 00:18:43 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Asada", "Maho", ""], ["Yoshikawa", "Masatoshi", ""], ["Cao", "Yang", ""]]}, {"id": "1904.10606", "submitter": "Chunmiao Li", "authors": "Chunmiao Li and Yang Cao and Zhenjiang Hu and Masatoshi Yoshikawa", "title": "Blockchain-based Bidirectional Updates on Fine-grained Medical Data", "comments": null, "journal-ref": null, "doi": "10.1109/ICDEW.2019.00010", "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic medical data sharing between stakeholders, such as patients,\ndoctors, and researchers, can promote more effective medical treatment\ncollaboratively. These sensitive and private data should only be accessed by\nauthorized users. Given a total medical data, users may care about parts of\nthem and other unrelated information might interfere with the user interested\ndata search and increase the risk of exposure. Besides accessing these data,\nusers may want to update them and propagate to other sharing peers so that all\npeers keep identical data after each update. To satisfy these requirements, in\nthis paper we propose a medical data sharing architecture that addresses the\npermission control using smart contracts on the blockchain and splits data into\nfined grained pieces shared with different peers then synchronize full data and\nthese pieces with bidirectional transformations. Medical data reside on each\nuser\\'s local database and permission related data are stored on smart\ncontracts. Only all peers have gained the newest shared data after updates can\nthey start to do next operations on it, which are enforced by smart contracts.\nBlockchain based immutable shared ledge enables users to trace data updates\nhistory. This paper can provide a new perspective to view full medical data as\ndifferent slices to be shared with various peers but consistency after updates\nbetween them are still promised, which can protect the privacy and improve data\nsearch efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 02:08:19 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Li", "Chunmiao", ""], ["Cao", "Yang", ""], ["Hu", "Zhenjiang", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "1904.10632", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Maximum Entropy Based Significance of Itemsets", "comments": "Journal version. The previous version is the conference paper", "journal-ref": null, "doi": "10.1007/s10115-008-0128-4", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of defining the significance of an itemset. We say\nthat the itemset is significant if we are surprised by its frequency when\ncompared to the frequencies of its sub-itemsets. In other words, we estimate\nthe frequency of the itemset from the frequencies of its sub-itemsets and\ncompute the deviation between the real value and the estimate. For the\nestimation we use Maximum Entropy and for measuring the deviation we use\nKullback-Leibler divergence.\n  A major advantage compared to the previous methods is that we are able to use\nricher models whereas the previous approaches only measure the deviation from\nthe independence model.\n  We show that our measure of significance goes to zero for derivable itemsets\nand that we can use the rank as a statistical test. Our empirical results\ndemonstrate that for our real datasets the independence assumption is too\nstrong but applying more flexible models leads to good results.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 03:46:23 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 01:44:29 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1904.10761", "submitter": "Steven Whang", "authors": "Ki Hyun Tae, Yuji Roh, Young Hun Oh, Hyunsu Kim, Steven Euijong Whang", "title": "Data Cleaning for Accurate, Fair, and Robust Models: A Big Data - AI\n  Integration Approach", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide use of machine learning is fundamentally changing the software\ndevelopment paradigm (a.k.a. Software 2.0) where data becomes a first-class\ncitizen, on par with code. As machine learning is used in sensitive\napplications, it becomes imperative that the trained model is accurate, fair,\nand robust to attacks. While many techniques have been proposed to improve the\nmodel training process (in-processing approach) or the trained model itself\n(post-processing), we argue that the most effective method is to clean the root\ncause of error: the data the model is trained on (pre-processing).\nHistorically, there are at least three research communities that have been\nseparately studying this problem: data management, machine learning (model\nfairness), and security. Although a significant amount of research has been\ndone by each community, ultimately the same datasets must be preprocessed, and\nthere is little understanding how the techniques relate to each other and can\npossibly be integrated. We contend that it is time to extend the notion of data\ncleaning for modern machine learning needs. We identify dependencies among the\ndata preprocessing techniques and propose MLClean, a unified data cleaning\nframework that integrates the techniques and helps train accurate and fair\nmodels. This work is part of a broader trend of Big data -- Artificial\nIntelligence (AI) integration.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 23:57:07 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Tae", "Ki Hyun", ""], ["Roh", "Yuji", ""], ["Oh", "Young Hun", ""], ["Kim", "Hyunsu", ""], ["Whang", "Steven Euijong", ""]]}, {"id": "1904.10889", "submitter": "Chuan-Chi Lai", "authors": "Chuan-Chi Lai, Zulhaydar Fairozal Akbar, Chuan-Ming Liu, Van-Dai Ta,\n  Li-Chun Wang", "title": "Distributed Continuous Range-Skyline Query Monitoring over the Internet\n  of Mobile Things", "comments": "Accepted by IEEE Internet of Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2019.2909393", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Range-Skyline Query (RSQ) is the combination of range query and skyline\nquery. It is one of the practical query types in multi-criteria decision\nservices, which may include the spatial and non-spatial information as well as\nmake the resulting information more useful than skyline search when the\nlocation is concerned. Furthermore, Continuous Range-Skyline Query (CRSQ) is an\nextension of Range-Skyline Query (RSQ) that the system continuously reports the\nskyline results to a query within a given search range. This work focuses on\nthe RSQ and CRSQ within a specific range on Internet of Mobile Things (IoMT)\napplications. Many server-client approaches for CRSQ have been proposed but are\nsensitive to the number of moving objects. We propose an effective and\nnon-centralized approach, Distributed Continuous Range-Skyline Query process\n(DCRSQ process), for supporting RSQ and CRSQ in mobile environments. By\nconsidering the mobility, the proposed approach can predict the time when an\nobject falls in the query range and ignore more irrelevant information when\nderiving the results, thus saving the computation overhead. The proposed\napproach, DCRSQ process, is analyzed on cost and validated with extensive\nsimulated experiments. The results show that DCRSQ process outperforms the\nexisting approaches in different scenarios and aspects.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:55:34 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Lai", "Chuan-Chi", ""], ["Akbar", "Zulhaydar Fairozal", ""], ["Liu", "Chuan-Ming", ""], ["Ta", "Van-Dai", ""], ["Wang", "Li-Chun", ""]]}, {"id": "1904.11121", "submitter": "Dimitrije Jankov", "authors": "Dimitrije Jankov, Shangyu Luo, Binhang Yuan, Zhuhua Cai, Jia Zou,\n  Chris Jermaine, Zekai J. Gao", "title": "Declarative Recursive Computation on an RDBMS, or, Why You Should Use a\n  Database For Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": "10.14778/3317315.3317323", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of popular systems, most notably Google's TensorFlow, have been\nimplemented from the ground up to support machine learning tasks. We consider\nhow to make a very small set of changes to a modern relational database\nmanagement system (RDBMS) to make it suitable for distributed learning\ncomputations. Changes include adding better support for recursion, and\noptimization and execution of very large compute plans. We also show that there\nare key advantages to using an RDBMS as a machine learning platform. In\nparticular, learning based on a database management system allows for trivial\nscaling to large data sets and especially large models, where different\ncomputational units operate on different parts of a model that may be too large\nto fit into RAM.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 01:50:52 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Jankov", "Dimitrije", ""], ["Luo", "Shangyu", ""], ["Yuan", "Binhang", ""], ["Cai", "Zhuhua", ""], ["Zou", "Jia", ""], ["Jermaine", "Chris", ""], ["Gao", "Zekai J.", ""]]}, {"id": "1904.11134", "submitter": "Nikolaj Tatti", "authors": "Michael Mampaey, Jilles Vreeken, Nikolaj Tatti", "title": "Summarizing Data Succinctly with the Most Informative Itemsets", "comments": "Journal version. The previous version is the conference version", "journal-ref": null, "doi": "10.1145/2382577.2382580", "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge discovery from data is an inherently iterative process. That is,\nwhat we know about the data greatly determines our expectations, and therefore,\nwhat results we would find interesting and/or surprising. Given new knowledge\nabout the data, our expectations will change. Hence, in order to avoid\nredundant results, knowledge discovery algorithms ideally should follow such an\niterative updating procedure.\n  With this in mind, we introduce a well-founded approach for succinctly\nsummarizing data with the most informative itemsets; using a probabilistic\nmaximum entropy model, we iteratively find the itemset that provides us the\nmost novel information--that is, for which the frequency in the data surprises\nus the most---and in turn we update our model accordingly. As we use the\nMaximum Entropy principle to obtain unbiased probabilistic models, and only\ninclude those itemsets that are most informative with regard to the current\nmodel, the summaries we construct are guaranteed to be both descriptive and\nnon-redundant.\n  The algorithm that we present, called MTV, can either discover the top-$k$\nmost informative itemsets, or we can employ either the Bayesian Information\nCriterion (BIC) or the Minimum Description Length (MDL) principle to\nautomatically identify the set of itemsets that together summarize the data\nwell. In other words, our method will `tell you what you need to know' about\nthe data. Importantly, it is a one-phase algorithm: rather than picking\nitemsets from a user-provided candidate set, itemsets and their supports are\nmined on-the-fly. To further its applicability, we provide an efficient method\nto compute the maximum entropy distribution using Quick Inclusion-Exclusion.\n  Experiments on our method, using synthetic, benchmark, and real data, show\nthat the discovered summaries are succinct, and correctly identify the key\npatterns in the data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 02:59:53 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 01:16:11 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Mampaey", "Michael", ""], ["Vreeken", "Jilles", ""], ["Tatti", "Nikolaj", ""]]}, {"id": "1904.11201", "submitter": "Hongzhi Wang", "authors": "Hongzhi Wang, Ning Li, Zheng Wang, Jianing Li", "title": "GPU-based Efficient Join Algorithms on Hadoop", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing data has brought tremendous pressure for query processing and\nstorage, so there are many studies that focus on using GPU to accelerate join\noperation, which is one of the most important operations in modern database\nsystems. However, existing GPU acceleration join operation researches are not\nvery suitable for the join operation on big data. Based on this, this paper\nspeeds up nested loop join, hash join and theta join, combining Hadoop with\nGPU, which is also the first to use GPU to accelerate theta join. At the same\ntime, after the data pre-filtering and pre-processing, using Map-Reduce and\nHDFS in Hadoop proposed in this paper, the larger data table can be handled,\ncompared to existing GPU acceleration methods. Also with Map-Reduce in Hadoop,\nthe algorithm proposed in this paper can estimate the number of results more\naccurately and allocate the appropriate storage space without unnecessary\ncosts, making it more efficient. The rigorous experiments show that the\nproposed method can obtain 1.5 to 2 times the speedup, compared to the\ntraditional GPU acceleration equi join algorithm. And in the synthetic data\nset, the GPU version of the proposed method can get 1.3 to 2 times the speedup,\ncompared to CPU version.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 08:20:05 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Wang", "Hongzhi", ""], ["Li", "Ning", ""], ["Wang", "Zheng", ""], ["Li", "Jianing", ""]]}, {"id": "1904.11327", "submitter": "Saverio Giallorenzo", "authors": "Saverio Giallorenzo, Fabrizio Montesi, Larisa Safina, Stefano Pio\n  Zingaro", "title": "Ephemeral Data Handling in Microservices - Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern application areas for software systems --- like eHealth, the\nInternet-of-Things, and Edge Computing --- data is encoded in heterogeneous,\ntree-shaped data-formats, it must be processed in real-time, and it must be\nephemeral, i.e., not persist in the system. While it is preferable to use a\nquery language to express complex data-handling logic, their typical execution\nengine, a database external from the main application, is unfit in scenarios of\nephemeral data-handling. A better option is represented by integrated query\nframeworks, which benefit from existing development support tools (e.g., syntax\nand type checkers) and execute within the application memory. In this paper, we\npropose one such framework that, for the first time, targets tree-shaped,\ndocument-oriented queries. We formalise an instantiation of MQuery, a sound\nvariant of the widely-used MongoDB query language, which we implemented in the\nJolie language. Jolie programs are microservices, the building blocks of modern\nsoftware systems. Moreover, since Jolie supports native tree data-structures\nand automatic management of heterogeneous data-encodings, we can provide a\nuniform way to use MQuery on any data-format supported by the language. We\npresent a non-trivial use case from eHealth, use it to concretely evaluate our\nmodel, and to illustrate our formalism.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 13:31:33 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Giallorenzo", "Saverio", ""], ["Montesi", "Fabrizio", ""], ["Safina", "Larisa", ""], ["Zingaro", "Stefano Pio", ""]]}, {"id": "1904.11498", "submitter": "Mansaf Alam Dr", "authors": "Samiya Khan, Xiufeng Liu, Syed Arshad Ali, Mansaf Alam", "title": "Storage Solutions for Big Data Systems: A Qualitative Study and\n  Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data systems development is full of challenges in view of the variety of\napplication areas and domains that this technology promises to serve.\nTypically, fundamental design decisions involved in big data systems design\ninclude choosing appropriate storage and computing infrastructures. In this age\nof heterogeneous systems that integrate different technologies for optimized\nsolution to a specific real world problem, big data system are not an exception\nto any such rule. As far as the storage aspect of any big data system is\nconcerned, the primary facet in this regard is a storage infrastructure and\nNoSQL seems to be the right technology that fulfills its requirements. However,\nevery big data application has variable data characteristics and thus, the\ncorresponding data fits into a different data model. This paper presents\nfeature and use case analysis and comparison of the four main data models\nnamely document oriented, key value, graph and wide column. Moreover, a feature\nanalysis of 80 NoSQL solutions has been provided, elaborating on the criteria\nand points that a developer must consider while making a possible choice.\nTypically, big data storage needs to communicate with the execution engine and\nother processing and visualization technologies to create a comprehensive\nsolution. This brings forth second facet of big data storage, big data file\nformats, into picture. The second half of the research paper compares the\nadvantages, shortcomings and possible use cases of available big data file\nformats for Hadoop, which is the foundation for most big data computing\ntechnologies. Decentralized storage and blockchain are seen as the next\ngeneration of big data storage and its challenges and future prospects have\nalso been discussed.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 08:31:50 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Khan", "Samiya", ""], ["Liu", "Xiufeng", ""], ["Ali", "Syed Arshad", ""], ["Alam", "Mansaf", ""]]}, {"id": "1904.11653", "submitter": "Hongzhi Wang", "authors": "Hongzhi Wang, Jiabao Han, Bin Shao, Jianzhong Li", "title": "Regular Expression Matching on billion-nodes Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, it is necessary to retrieve pairs of vertices with the\npath between them satisfying certain constraints, since regular expression is a\npowerful tool to describe patterns of a sequence. To meet such requirements, in\nthis paper, we define regular expression (RE) query on graphs to use regular\nexpression to represent the constraints between vertices. To process RE queries\non large graphs such as social networks, we propose the RE query processing\nmethod with the index size sublinear to the graph size. Considering that large\ngraphs may be randomly distributed in multiple machines, the parallel RE\nprocessing algorithms are presented without the assumption of graph\ndistribution. To achieve high efficiency for complex RE query processing, we\ndevelop cost-based query optimization strategies with only a small size\nstatistical information which is suitable for querying large graphs.\nComprehensive experimental results show that this approach works scale well for\nlarge graphs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 02:31:53 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Wang", "Hongzhi", ""], ["Han", "Jiabao", ""], ["Shao", "Bin", ""], ["Li", "Jianzhong", ""]]}, {"id": "1904.11827", "submitter": "Eugene Wu", "authors": "Sanjay Krishnan, Eugene Wu", "title": "AlphaClean: Automatic Generation of Data Cleaning Pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analyst effort in data cleaning is gradually shifting away from the\ndesign of hand-written scripts to building and tuning complex pipelines of\nautomated data cleaning libraries. Hyper-parameter tuning for data cleaning is\nvery different than hyper-parameter tuning for machine learning since the\npipeline components and objective functions have structure that tuning\nalgorithms can exploit. This paper proposes a framework, called AlphaClean,\nthat rethinks parameter tuning for data cleaning pipelines. AlphaClean provides\nusers with a rich library to define data quality measures with weighted sums of\nSQL aggregate queries. AlphaClean applies generate-then-search framework where\neach pipelined cleaning operator contributes candidate transformations to a\nshared pool. Asynchronously, in separate threads, a search algorithm sequences\nthem into cleaning pipelines that maximize the user-defined quality measures.\nThis architecture allows AlphaClean to apply a number of optimizations\nincluding incremental evaluation of the quality measures and learning dynamic\npruning rules to reduce the search space. Our experiments on real and synthetic\nbenchmarks suggest that AlphaClean finds solutions of up-to 9x higher quality\nthan naively applying state-of-the-art parameter tuning methods, is\nsignificantly more robust to straggling data cleaning methods and redundancy in\nthe data cleaning library, and can incorporate state-of-the-art cleaning\nsystems such as HoloClean as cleaning operators.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 13:07:53 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 13:32:10 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Krishnan", "Sanjay", ""], ["Wu", "Eugene", ""]]}, {"id": "1904.12068", "submitter": "Kai Li", "authors": "Yuzhe Tang, Ju Chen, Kai Li, Jianliang Xu, Qi Zhang", "title": "Authenticated Key-Value Stores with Hardware Enclaves", "comments": "eLSM, Enclave, key-value store, ADS, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authenticated data storage on an untrusted platform is an important computing\nparadigm for cloud applications ranging from big-data outsourcing, to\ncryptocurrency and certificate transparency log. These modern applications\nincreasingly feature update-intensive workloads, whereas existing authenticated\ndata structures (ADSs) designed with in-place updates are inefficient to handle\nsuch workloads. In this paper, we address this issue and propose a novel\nauthenticated log-structured merge tree (eLSM) based key-value store by\nleveraging Intel SGX enclaves.\n  We present a system design that runs the code of eLSM store inside enclave.\nTo circumvent the limited enclave memory (128 MB with the latest Intel CPUs),\nwe propose to place the memory buffer of the eLSM store outside the enclave and\nprotect the buffer using a new authenticated data structure by digesting\nindividual LSM-tree levels. We design protocols to support query authentication\nin data integrity, completeness (under range queries), and freshness. The proof\nin our protocol is made small by including only the Merkle proofs at selective\nlevels.\n  We implement eLSM on top of Google LevelDB and Facebook RocksDB with minimal\ncode change and performance interference. We evaluate the performance of eLSM\nunder the YCSB workload benchmark and show a performance advantage of up to\n4.5X speedup.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 22:45:04 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 19:28:39 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 23:38:02 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Tang", "Yuzhe", ""], ["Chen", "Ju", ""], ["Li", "Kai", ""], ["Xu", "Jianliang", ""], ["Zhang", "Qi", ""]]}, {"id": "1904.12217", "submitter": "Eyal Rozenberg", "authors": "Eyal Rozenberg", "title": "A computational model for analytic column stores", "comments": "A monograph. 99 pages (+ title page) with 27 figures. Full title\n  including subtitle: \"A computational model for analytic column stores with\n  focus on columnar representations of data and composite compression schemes\".\n  ACM classes given in lexicographic order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an abstract model for the computations performed by\nanalytic column stores or columnar query processors. The model is based on\ncircuits whose wires carry columns rather than scalar values, and whose nodes\napply operators with column inputs and outputs. This model allows expression of\nmost of the architectural features of existing column-store DBMSes through\ncolumnar execution plans, rather than such features being implemented\nsui-generis, and without the column store maintaining significant out-of-plan\ndata. A strict adherence to columnarity allows for a relatively simple and\nrobust model; enabling extensive and intensive optimization of almost all\naspects of query processing; and also enabling massive uniform parallelization\nof query process on modern hardware. Moreover, the computational model's\nexpressivity makes it useful also as an \\emph{analytical} tool for considering\ndesign aspects and features of existing column stores, individually and\ncomparatively.\n  To achieve the model's wide expressiveness, much of this work develops\nrepresentation schemes of relevant data structures as combinations of plain\ncolumns, with columnar circuits used as scheme encoders and decoders. A\nparticular focus is given to schemes which also compress the data, and their\nuse in query execution --- as an integral part of the computation: Subcircuits\nof larger columnar circuits, not black boxes. Decoder and encoder circuits are\nthus also composed to form more elaborate schemes. Such formulation allows both\nfor an alternative view of well-known compression schemes, and for the\ndevelopment of new columnar compression schemes with useful features; these\nshould be of independent interest irrespective of column store systems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 21:59:11 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 19:52:53 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Rozenberg", "Eyal", ""]]}, {"id": "1904.12242", "submitter": "Yachen Tang", "authors": "Yachen Tang, Tingting Liu, Guangyi Liu, Jie Li, Renchang Dai, and Chen\n  Yuan", "title": "Enhancement of Power Equipment Management Using Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate retrieval of the power equipment information plays an important role\nin guiding the full-lifecycle management of power system assets. Because of\ndata duplication, database decentralization, weak data relations, and sluggish\ndata updates, the power asset management system eager to adopt a new strategy\nto avoid the information losses, bias, and improve the data storage efficiency\nand extraction process. Knowledge graph has been widely developed in large part\nowing to its schema-less nature. It enables the knowledge graph to grow\nseamlessly and allows new relations addition and entities insertion when\nneeded. This study proposes an approach for constructing power equipment\nknowledge graph by merging existing multi-source heterogeneous power equipment\nrelated data. A graph-search method to illustrate exhaustive results to the\ndesired information based on the constructed knowledge graph is proposed. A\ncase of a 500 kV station example is then demonstrated to show relevant search\nresults and to explain that the knowledge graph can improve the efficiency of\npower equipment management.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 02:02:57 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Tang", "Yachen", ""], ["Liu", "Tingting", ""], ["Liu", "Guangyi", ""], ["Li", "Jie", ""], ["Dai", "Renchang", ""], ["Yuan", "Chen", ""]]}, {"id": "1904.12248", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Jiexiong Zhang, Philippe\n  Fournier-Viger, Han-Chieh Chao, and Philip S. Yu", "title": "Fast Utility Mining on Complex Sequences", "comments": "Under review in IEEE TKDE, 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-utility sequential pattern mining is an emerging topic in the field of\nKnowledge Discovery in Databases. It consists of discovering subsequences\nhaving a high utility (importance) in sequences, referred to as high-utility\nsequential patterns (HUSPs). HUSPs can be applied to many real-life\napplications, such as market basket analysis, E-commerce recommendation,\nclick-stream analysis and scenic route planning. For example, in economics and\ntargeted marketing, understanding economic behavior of consumers is quite\nchallenging, such as finding credible and reliable information on product\nprofitability. Several algorithms have been proposed to address this problem by\nefficiently mining utility-based useful sequential patterns. Nevertheless, the\nperformance of these algorithms can be unsatisfying in terms of runtime and\nmemory usage due to the combinatorial explosion of the search space for low\nutility threshold and large databases. Hence, this paper proposes a more\nefficient algorithm for the task of high-utility sequential pattern mining,\ncalled HUSP-ULL. It utilizes a lexicographic sequence (LS)-tree and a\nutility-linked (UL)-list structure to fast discover HUSPs. Furthermore, two\npruning strategies are introduced in HUSP-ULL to obtain tight upper-bounds on\nthe utility of candidate sequences, and reduce the search space by pruning\nunpromising candidates early. Substantial experiments both on real-life and\nsynthetic datasets show that the proposed algorithm can effectively and\nefficiently discover the complete set of HUSPs and outperforms the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 03:16:56 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Zhang", "Jiexiong", ""], ["Fournier-Viger", "Philippe", ""], ["Chao", "Han-Chieh", ""], ["Yu", "Philip S.", ""]]}, {"id": "1904.12342", "submitter": "Tiantu Xu", "authors": "Mengwei Xu, Tiantu Xu, Yunxin Liu, Felix Xiaozhu Lin", "title": "Video Analytics with Zero-streaming Cameras", "comments": "Mengwei Xu and Tiantu Xu contributed equally to the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-cost cameras enable powerful analytics. An unexploited opportunity is\nthat most captured videos remain \"cold\" without being queried. For efficiency,\nwe advocate for these cameras to be zero streaming: capturing videos to local\nstorage and communicating with the cloud only when analytics is requested. How\nto query zero-streaming cameras efficiently? Our response is a camera/cloud\nruntime system called DIVA. It addresses two key challenges: to best use\nlimited camera resource during video capture; to rapidly explore massive videos\nduring query execution. DIVA contributes two unconventional techniques. (1)\nWhen capturing videos, a camera builds sparse yet accurate landmark frames,\nfrom which it learns reliable knowledge for accelerating future queries. (2)\nWhen executing a query, a camera processes frames in multiple passes with\nincreasingly more expensive operators. As such, DIVA presents and keeps\nrefining inexact query results throughout the query's execution. On diverse\nqueries over 15 videos lasting 720 hours in total, DIVA runs at more than 100x\nvideo realtime and outperforms competitive alternative designs. To our\nknowledge, DIVA is the first system for querying large videos stored on\nlow-cost remote cameras.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 16:35:02 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 21:55:59 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 18:32:15 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 06:03:35 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xu", "Mengwei", ""], ["Xu", "Tiantu", ""], ["Liu", "Yunxin", ""], ["Lin", "Felix Xiaozhu", ""]]}, {"id": "1904.12344", "submitter": "Minyar Sassi Hidri", "authors": "Ines Benali-Sougui, Minyar Sassi Hidri, Amel Grissa-Touzi", "title": "Towards a New Extracting and Querying Approach of Fuzzy Summaries", "comments": "22 pages, 6 figures, 8 tables. Multidisciplinary Approaches to\n  Service-Oriented Engineering, 2018. arXiv admin note: text overlap with\n  arXiv:1401.0494", "journal-ref": null, "doi": "10.4018/978-1-5225-5951-1.ch015", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversification of DB applications highlighted the limitations of relational\ndatabase management system (RDBMS) particularly on the modeling plan. In fact,\nin the real world, we are increasingly faced with the situation where\napplications need to handle imprecise data and to offer a flexible querying to\ntheir users. Several theoretical solutions have been proposed. However, the\nimpact of this work in practice remained negligible with the exception of a few\nresearch prototypes based on the formal model GEFRED. In this chapter, the\nauthors propose a new approach for exploitation of fuzzy relational databases\n(FRDB) described by the model GEFRED. This approach consists of 1) a new\ntechnique for extracting summary fuzzy data, Fuzzy SAINTETIQ, based on the\nclassification of fuzzy data and formal concepts analysis; 2) an approach of\nassessing flexible queries in the context of FDB based on the set of fuzzy\nsummaries generated by our fuzzy SAINTETIQ system; 3) an approach of repairing\nand substituting unanswered query.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 16:49:59 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Benali-Sougui", "Ines", ""], ["Hidri", "Minyar Sassi", ""], ["Grissa-Touzi", "Amel", ""]]}, {"id": "1904.12535", "submitter": "Ping Li", "authors": "Mingming Sun, Xu Li, Xin Wang, Miao Fan, Yue Feng, Ping Li", "title": "Logician: A Unified End-to-End Neural Approach for Open-Domain\n  Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of open information extraction (OIE)\nfor extracting entity and relation level intermediate structures from sentences\nin open-domain. We focus on four types of valuable intermediate structures\n(Relation, Attribute, Description, and Concept), and propose a unified\nknowledge expression form, SAOKE, to express them. We publicly release a data\nset which contains more than forty thousand sentences and the corresponding\nfacts in the SAOKE format labeled by crowd-sourcing. To our knowledge, this is\nthe largest publicly available human labeled data set for open information\nextraction tasks. Using this labeled SAOKE data set, we train an end-to-end\nneural model using the sequenceto-sequence paradigm, called Logician, to\ntransform sentences into facts. For each sentence, different to existing\nalgorithms which generally focus on extracting each single fact without\nconcerning other possible facts, Logician performs a global optimization over\nall possible involved facts, in which facts not only compete with each other to\nattract the attention of words, but also cooperate to share words. An\nexperimental study on various types of open domain relation extraction tasks\nreveals the consistent superiority of Logician to other states-of-the-art\nalgorithms. The experiments verify the reasonableness of SAOKE format, the\nvaluableness of SAOKE data set, the effectiveness of the proposed Logician\nmodel, and the feasibility of the methodology to apply end-to-end learning\nparadigm on supervised data sets for the challenging tasks of open information\nextraction.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:37:31 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sun", "Mingming", ""], ["Li", "Xu", ""], ["Wang", "Xin", ""], ["Fan", "Miao", ""], ["Feng", "Yue", ""], ["Li", "Ping", ""]]}, {"id": "1904.12539", "submitter": "Xin Huang", "authors": "Yixiang Fang, Xin Huang, Lu Qin, Ying Zhang, Wenjie Zhang, Reynold\n  Cheng, Xuemin Lin", "title": "A Survey of Community Search Over Big Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of information technologies, various big graphs\nare prevalent in many real applications (e.g., social media and knowledge\nbases). An important component of these graphs is the network community.\nEssentially, a community is a group of vertices which are densely connected\ninternally. Community retrieval can be used in many real applications, such as\nevent organization, friend recommendation, and so on. Consequently, how to\nefficiently find high-quality communities from big graphs is an important\nresearch topic in the era of big data. Recently a large group of research\nworks, called community search, have been proposed. They aim to provide\nefficient solutions for searching high-quality communities from large networks\nin real-time. Nevertheless, these works focus on different types of graphs and\nformulate communities in different manners, and thus it is desirable to have a\ncomprehensive review of these works.\n  In this survey, we conduct a thorough review of existing community search\nworks. Moreover, we analyze and compare the quality of communities under their\nmodels, and the performance of different solutions. Furthermore, we point out\nnew research directions. This survey does not only help researchers to have a\nbetter understanding of existing community search solutions, but also provides\npractitioners a better judgment on choosing the proper solutions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:46:09 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 08:03:39 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Fang", "Yixiang", ""], ["Huang", "Xin", ""], ["Qin", "Lu", ""], ["Zhang", "Ying", ""], ["Zhang", "Wenjie", ""], ["Cheng", "Reynold", ""], ["Lin", "Xuemin", ""]]}, {"id": "1904.12626", "submitter": "Francisco Bischoff", "authors": "Francisco Bischoff, Pedro Pereira Rodrigues", "title": "tsmp: An R Package for Time Series with Matrix Profile", "comments": null, "journal-ref": null, "doi": "10.32614/RJ-2020-021", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This article describes tsmp, an R package that implements the matrix profile\nconcept for time series. The tsmp package is a toolkit that allows all-pairs\nsimilarity joins, motif, discords and chains discovery, semantic segmentation,\netc. Here we describe how the tsmp package may be used by showing some of the\nuse-cases from the original articles and evaluate the algorithm speed in the R\nenvironment. This package can be downloaded at\nhttps://CRAN.R-project.org/package=tsmp.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 05:27:09 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bischoff", "Francisco", ""], ["Rodrigues", "Pedro Pereira", ""]]}, {"id": "1904.12636", "submitter": "Amrith Kumar", "authors": "Amrith Kumar, Kenneth Rugg", "title": "Brewers Conjecture and a characterization of the limits, and\n  relationships between Consistency, Availability and Partition Tolerance in a\n  distributed service", "comments": "This paper was originally published in October 2011 on the ParElastic\n  blog that can now be found archived on the Internet Wayback machine at\n  http://bit.ly/2Xv5kh4. The PDF document that was part of that six part blog\n  post is at http://bit.ly/2GBJ8fC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In designing a distributed service, three desirable attributes are\nConsistency, Availability and Partition Tolerance. In this note we explore a\nframework for characterizing these three in a manner that establishes definite\nlimits and relationships between them, and explore some implications of this\ncharacterization.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 10:01:16 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Kumar", "Amrith", ""], ["Rugg", "Kenneth", ""]]}, {"id": "1904.12773", "submitter": "Zeyu Ding", "authors": "Zeyu Ding, Yuxin Wang, Danfeng Zhang, Daniel Kifer", "title": "Free Gap Information from the Differentially Private Sparse Vector and\n  Noisy Max Mechanisms", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy Max and Sparse Vector are selection algorithms for differential privacy\nand serve as building blocks for more complex algorithms. In this paper we show\nthat both algorithms can release additional information for free (i.e., at no\nadditional privacy cost). Noisy Max is used to return the approximate maximizer\namong a set of queries. We show that it can also release for free the noisy gap\nbetween the approximate maximizer and runner-up. This free information can\nimprove the accuracy of certain subsequent counting queries by up to 50%.\nSparse Vector is used to return a set of queries that are approximately larger\nthan a fixed threshold. We show that it can adaptively control its privacy\nbudget (use less budget for queries that are likely to be much larger than the\nthreshold) in order to increase the amount of queries it can process. These\nresults follow from a careful privacy analysis.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 15:31:13 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 02:30:23 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 16:34:49 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Ding", "Zeyu", ""], ["Wang", "Yuxin", ""], ["Zhang", "Danfeng", ""], ["Kifer", "Daniel", ""]]}, {"id": "1904.13164", "submitter": "Yeting Li", "authors": "Chunmei Dong and Yeting Li and Haiming Chen", "title": "Learning Restricted Regular Expressions with Interleaving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advantages for the presence of an XML schema for XML documents are\nnumerous. However, many XML documents in practice are not accompanied by a\nschema or by a valid schema. Relax NG is a popular and powerful schema\nlanguage, which supports the unconstrained interleaving operator. Focusing on\nthe inference of Relax NG, we propose a new subclass of regular expressions\nwith interleaving and design a polynomial inference algorithm. Then we\nconducted a series of experiments based on large-scale real data and on three\nXML data corpora, and experimental results show that our subclass has a better\npracticality than previous ones, and the regular expressions inferred by our\nalgorithm are more precise.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 11:29:00 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Dong", "Chunmei", ""], ["Li", "Yeting", ""], ["Chen", "Haiming", ""]]}, {"id": "1904.13251", "submitter": "Leonard Heilig", "authors": "Leonard Heilig, Robert Stahlbock, Stefan Vo{\\ss}", "title": "From Digitalization to Data-Driven Decision Making in Container\n  Terminals", "comments": "20 pages, 5 figures, book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the new opportunities emerging from the current wave of digitalization,\nterminal planning and management need to be revisited by taking a data-driven\nperspective. Business analytics, as a practice of extracting insights from\noperational data, assists in reducing uncertainties using predictions and helps\nto identify and understand causes of inefficiencies, disruptions, and anomalies\nin intra- and inter-organizational terminal operations. Despite the growing\ncomplexity of data within and around container terminals, a lack of data-driven\napproaches in the context of container terminals can be identified. In this\nchapter, the concept of business analytics for supporting terminal planning and\nmanagement is introduced. The chapter specifically focuses on data mining\napproaches and provides a comprehensive overview on applications in container\nterminals and related research. As such, we aim to establish a data-driven\nperspective on terminal planning and management, complementing the traditional\noptimization perspective.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 14:37:28 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Heilig", "Leonard", ""], ["Stahlbock", "Robert", ""], ["Vo\u00df", "Stefan", ""]]}]