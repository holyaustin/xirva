[{"id": "1201.0226", "submitter": "Ning Zhang", "authors": "Ning Zhang, Junichi Tatemura, Jignesh M. Patel, Hakan\n  Hac{\\i}g\\\"um\\\"u\\c{s}", "title": "Towards Cost-Effective Storage Provisioning for DBMSs", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  274-285 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data center operators face a bewildering set of choices when considering how\nto provision resources on machines with complex I/O subsystems. Modern I/O\nsubsystems often have a rich mix of fast, high performing, but expensive SSDs\nsitting alongside with cheaper but relatively slower (for random accesses)\ntraditional hard disk drives. The data center operators need to determine how\nto provision the I/O resources for specific workloads so as to abide by\nexisting Service Level Agreements (SLAs), while minimizing the total operating\ncost (TOC) of running the workload, where the TOC includes the amortized\nhardware costs and the run time energy costs. The focus of this paper is on\nintroducing this new problem of TOC-based storage allocation, cast in a\nframework that is compatible with traditional DBMS query optimization and query\nprocessing architecture. We also present a heuristic-based solution to this\nproblem, called DOT. We have implemented DOT in PostgreSQL, and experiments\nusing TPC-H and TPC-C demonstrate significant TOC reduction by DOT in various\nsettings.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 05:32:48 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Zhang", "Ning", ""], ["Tatemura", "Junichi", ""], ["Patel", "Jignesh M.", ""], ["Hac\u0131g\u00fcm\u00fc\u015f", "Hakan", ""]]}, {"id": "1201.0227", "submitter": "Hongchan Roh", "authors": "Hongchan Roh, Sanghyun Park, Sungho Kim, Mincheol Shin, Sang-Won Lee", "title": "B+-tree Index Optimization by Exploiting Internal Parallelism of\n  Flash-based Solid State Drives", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  286-297 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research addressed the potential problems of the hard-disk oriented\ndesign of DBMSs of flashSSDs. In this paper, we focus on exploiting potential\nbenefits of flashSSDs. First, we examine the internal parallelism issues of\nflashSSDs by conducting benchmarks to various flashSSDs. Then, we suggest\nalgorithm-design principles in order to best benefit from the internal\nparallelism. We present a new I/O request concept, called psync I/O that can\nexploit the internal parallelism of flashSSDs in a single process. Based on\nthese ideas, we introduce B+-tree optimization methods in order to utilize\ninternal parallelism. By integrating the results of these methods, we present a\nB+-tree variant, PIO B-tree. We confirmed that each optimization method\nsubstantially enhances the index performance. Consequently, PIO B-tree enhanced\nB+-tree's insert performance by a factor of up to 16.3, while improving\npoint-search performance by a factor of 1.2. The range search of PIO B-tree was\nup to 5 times faster than that of the B+-tree. Moreover, PIO B-tree\noutperformed other flash-aware indexes in various synthetic workloads. We also\nconfirmed that PIO B-tree outperforms B+-tree in index traces collected inside\nthe Postgresql DBMS with TPC-C benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 05:33:08 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Roh", "Hongchan", ""], ["Park", "Sanghyun", ""], ["Kim", "Sungho", ""], ["Shin", "Mincheol", ""], ["Lee", "Sang-Won", ""]]}, {"id": "1201.0228", "submitter": "Spyros Blanas", "authors": "Per-{\\AA}ke Larson, Spyros Blanas, Cristian Diaconu, Craig Freedman,\n  Jignesh M. Patel, Mike Zwilling", "title": "High-Performance Concurrency Control Mechanisms for Main-Memory\n  Databases", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  298-309 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A database system optimized for in-memory storage can support much higher\ntransaction rates than current systems. However, standard concurrency control\nmethods used today do not scale to the high transaction rates achievable by\nsuch systems. In this paper we introduce two efficient concurrency control\nmethods specifically designed for main-memory databases. Both use\nmultiversioning to isolate read-only transactions from updates but differ in\nhow atomicity is ensured: one is optimistic and one is pessimistic. To avoid\nexpensive context switching, transactions never block during normal processing\nbut they may have to wait before commit to ensure correct serialization\nordering. We also implemented a main-memory optimized version of single-version\nlocking. Experimental results show that while single-version locking works well\nwhen transactions are short and contention is low performance degrades under\nmore demanding conditions. The multiversion schemes have higher overhead but\nare much less sensitive to hotspots and the presence of long-running\ntransactions.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 05:34:34 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Larson", "Per-\u00c5ke", ""], ["Blanas", "Spyros", ""], ["Diaconu", "Cristian", ""], ["Freedman", "Craig", ""], ["Patel", "Jignesh M.", ""], ["Zwilling", "Mike", ""]]}, {"id": "1201.0229", "submitter": "Shuai Ma", "authors": "Shuai Ma, Yang Cao, Wenfei Fan, Jinpeng Huai, Tianyu Wo", "title": "Capturing Topology in Graph Pattern Matching", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  310-321 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pattern matching is often defined in terms of subgraph isomorphism, an\nNP-complete problem. To lower its complexity, various extensions of graph\nsimulation have been considered instead. These extensions allow pattern\nmatching to be conducted in cubic-time. However, they fall short of capturing\nthe topology of data graphs, i.e., graphs may have a structure drastically\ndifferent from pattern graphs they match, and the matches found are often too\nlarge to understand and analyze. To rectify these problems, this paper proposes\na notion of strong simulation, a revision of graph simulation, for graph\npattern matching. (1) We identify a set of criteria for preserving the topology\nof graphs matched. We show that strong simulation preserves the topology of\ndata graphs and finds a bounded number of matches. (2) We show that strong\nsimulation retains the same complexity as earlier extensions of simulation, by\nproviding a cubic-time algorithm for computing strong simulation. (3) We\npresent the locality property of strong simulation, which allows us to\neffectively conduct pattern matching on distributed graphs. (4) We\nexperimentally verify the effectiveness and efficiency of these algorithms,\nusing real-life data and synthetic data.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 05:34:57 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Ma", "Shuai", ""], ["Cao", "Yang", ""], ["Fan", "Wenfei", ""], ["Huai", "Jinpeng", ""], ["Wo", "Tianyu", ""]]}, {"id": "1201.0230", "submitter": "Mateusz Pawlik", "authors": "Mateusz Pawlik, Nikolaus Augsten", "title": "RTED: A Robust Algorithm for the Tree Edit Distance", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  334-345 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical tree edit distance between ordered labeled trees,\nwhich is defined as the minimum-cost sequence of node edit operations that\ntransform one tree into another. The state-of-the-art solutions for the tree\nedit distance are not satisfactory. The main competitors in the field either\nhave optimal worst-case complexity, but the worst case happens frequently, or\nthey are very efficient for some tree shapes, but degenerate for others. This\nleads to unpredictable and often infeasible runtimes. There is no obvious way\nto choose between the algorithms. In this paper we present RTED, a robust tree\nedit distance algorithm. The asymptotic complexity of RTED is smaller or equal\nto the complexity of the best competitors for any input instance, i.e., RTED is\nboth efficient and worst-case optimal. We introduce the class of LRH\n(Left-Right-Heavy) algorithms, which includes RTED and the fastest tree edit\ndistance algorithms presented in literature. We prove that RTED outperforms all\npreviously proposed LRH algorithms in terms of runtime complexity. In our\nexperiments on synthetic and real world data we empirically evaluate our\nsolution and compare it to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 05:35:26 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Pawlik", "Mateusz", ""], ["Augsten", "Nikolaus", ""]]}, {"id": "1201.0231", "submitter": "Julia Stoyanovich", "authors": "Yael Amsterdamer, Susan B. Davidson, Daniel Deutch, Tova Milo, Julia\n  Stoyanovich, Val Tannen", "title": "Putting Lipstick on Pig: Enabling Database-style Workflow Provenance", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  346-357 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workflow provenance typically assumes that each module is a \"black-box\", so\nthat each output depends on all inputs (coarse-grained dependencies).\nFurthermore, it does not model the internal state of a module, which can change\nbetween repeated executions. In practice, however, an output may depend on only\na small subset of the inputs (fine-grained dependencies) as well as on the\ninternal state of the module. We present a novel provenance framework that\nmarries database-style and workflow-style provenance, by using Pig Latin to\nexpose the functionality of modules, thus capturing internal state and\nfine-grained dependencies. A critical ingredient in our solution is the use of\na novel form of provenance graph that models module invocations and yields a\ncompact representation of fine-grained workflow provenance. It also enables a\nnumber of novel graph transformation operations, allowing to choose the desired\nlevel of granularity in provenance querying (ZoomIn and ZoomOut), and\nsupporting \"what-if\" workflow analytic queries. We implemented our approach in\nthe Lipstick system and developed a benchmark in support of a systematic\nperformance evaluation. Our results demonstrate the feasibility of tracking and\nquerying fine-grained workflow provenance.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 05:35:52 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Amsterdamer", "Yael", ""], ["Davidson", "Susan B.", ""], ["Deutch", "Daniel", ""], ["Milo", "Tova", ""], ["Stoyanovich", "Julia", ""], ["Tannen", "Val", ""]]}, {"id": "1201.0232", "submitter": "Jun Gao", "authors": "Jun Gao, Ruoming Jin, Jiashuai Zhou, Jeffrey Xu Yu, Xiao Jiang,\n  Tengjiao Wang", "title": "Relational Approach for Shortest Path Discovery over Large Graphs", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  358-369 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of large graphs, we cannot assume that graphs can still\nbe fully loaded into memory, thus the disk-based graph operation is inevitable.\nIn this paper, we take the shortest path discovery as an example to investigate\nthe technique issues when leveraging existing infrastructure of relational\ndatabase (RDB) in the graph data management. Based on the observation that a\nvariety of graph search queries can be implemented by iterative operations\nincluding selecting frontier nodes from visited nodes, making expansion from\nthe selected frontier nodes, and merging the expanded nodes into the visited\nones, we introduce a relational FEM framework with three corresponding\noperators to implement graph search tasks in the RDB context. We show new\nfeatures such as window function and merge statement introduced by recent SQL\nstandards can not only simplify the expression but also improve the performance\nof the FEM framework. In addition, we propose two optimization strategies\nspecific to shortest path discovery inside the FEM framework. First, we take a\nbi-directional set Dijkstra's algorithm in the path finding. The bi-directional\nstrategy can reduce the search space, and set Dijkstra's algorithm finds the\nshortest path in a set-at-a-time fashion. Second, we introduce an index named\nSegTable to preserve the local shortest segments, and exploit SegTable to\nfurther improve the performance. The final extensive experimental results\nillustrate our relational approach with the optimization strategies achieves\nhigh scalability and performance.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 05:36:12 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Gao", "Jun", ""], ["Jin", "Ruoming", ""], ["Zhou", "Jiashuai", ""], ["Yu", "Jeffrey Xu", ""], ["Jiang", "Xiao", ""], ["Wang", "Tengjiao", ""]]}, {"id": "1201.0233", "submitter": "Marina Barsky", "authors": "Marina Barsky, Sangkyum Kim, Tim Weninger, Jiawei Han", "title": "Mining Flipping Correlations from Large Datasets with Taxonomies", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  370-381 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new type of pattern -- a flipping correlation\npattern. The flipping patterns are obtained from contrasting the correlations\nbetween items at different levels of abstraction. They represent surprising\ncorrelations, both positive and negative, which are specific for a given\nabstraction level, and which \"flip\" from positive to negative and vice versa\nwhen items are generalized to a higher level of abstraction. We design an\nefficient algorithm for finding flipping correlations, the Flipper algorithm,\nwhich outperforms naive pattern mining methods by several orders of magnitude.\nWe apply Flipper to real-life datasets and show that the discovered patterns\nare non-redundant, surprising and actionable. Flipper finds strong contrasting\ncorrelations in itemsets with low-to-medium support, while existing techniques\ncannot handle the pattern discovery in this frequency range.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 05:36:29 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Barsky", "Marina", ""], ["Kim", "Sangkyum", ""], ["Weninger", "Tim", ""], ["Han", "Jiawei", ""]]}, {"id": "1201.0234", "submitter": "Arnd Christian K\\\"onig", "authors": "Arnd Christian K\\\"onig, Bolin Ding, Surajit Chaudhuri, Vivek Narasayya", "title": "A Statistical Approach Towards Robust Progress Estimation", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  382-393 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for accurate SQL progress estimation in the context of decision\nsupport administration has led to a number of techniques proposed for this\ntask. Unfortunately, no single one of these progress estimators behaves\nrobustly across the variety of SQL queries encountered in practice, meaning\nthat each technique performs poorly for a significant fraction of queries. This\npaper proposes a novel estimator selection framework that uses a statistical\nmodel to characterize the sets of conditions under which certain estimators\noutperform others, leading to a significant increase in estimation robustness.\nThe generality of this framework also enables us to add a number of novel\n\"special purpose\" estimators which increase accuracy further. Most importantly,\nthe resulting model generalizes well to queries very different from the ones\nused to train it. We validate our findings using a large number of industrial\nreal-life and benchmark workloads.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 05:36:46 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["K\u00f6nig", "Arnd Christian", ""], ["Ding", "Bolin", ""], ["Chaudhuri", "Surajit", ""], ["Narasayya", "Vivek", ""]]}, {"id": "1201.1340", "submitter": "Robert Seaman", "authors": "William Pence, Rob Seaman, Richard L. White", "title": "A Tiled-Table Convention for Compressing FITS Binary Tables", "comments": "Proposed FITS Convention:\n  http://fits.gsfc.nasa.gov/registry/tiletablecompression/tiletable.pdf, v1.0,\n  28 October 2010, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes a convention for compressing FITS binary tables that\nis modeled after the FITS tiled-image compression method (White et al. 2009)\nthat has been in use for about a decade. The input table is first optionally\nsubdivided into tiles, each containing an equal number of rows, then every\ncolumn of data within each tile is compressed and stored as a variable-length\narray of bytes in the output FITS binary table. All the header keywords from\nthe input table are copied to the header of the output table and remain\nuncompressed for efficient access. The output compressed table contains the\nsame number and order of columns as in the input uncompressed binary table.\nThere is one row in the output table corresponding to each tile of rows in the\ninput table. In principle, each column of data can be compressed using a\ndifferent algorithm that is optimized for the type of data within that column,\nhowever in the prototype implementation described here, the gzip algorithm is\nused to compress every column.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 03:01:12 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Pence", "William", ""], ["Seaman", "Rob", ""], ["White", "Richard L.", ""]]}, {"id": "1201.1345", "submitter": "Robert Seaman", "authors": "Rob Seaman, William Pence, Arnold Rots", "title": "FITS Checksum Proposal", "comments": "Registered FITS Convention:\n  http://fits.gsfc.nasa.gov/registry/checksum.html, 23 May 2002, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The checksum keywords described here provide an integrity check on the\ninformation contained in FITS HDUs. (Header and Data Units are the basic\ncomponents of FITS files, consisting of header keyword records followed by\noptional associated data records). The CHECKSUM keyword is defined to have a\nvalue that forces the 32-bit 1's complement checksum accumulated over all the\n2880-byte FITS logical records in the HDU to equal negative 0. (Note that 1's\ncomplement arithmetic has both positive and negative zero elements). Verifying\nthat the accumulated checksum is still equal to -0 provides a fast and fairly\nreliable way to determine that the HDU has not been modified by subsequent data\nprocessing operations or corrupted while copying or storing the file on\nphysical media.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 03:53:32 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Seaman", "Rob", ""], ["Pence", "William", ""], ["Rots", "Arnold", ""]]}, {"id": "1201.1829", "submitter": "Robert Seaman", "authors": "Nelson Zarate, Rob Seaman, Doug Tody", "title": "FITS Foreign File Encapsulation Convention", "comments": "Registered FITS Convention:\n  http://fits.gsfc.nasa.gov/registry/foreign.html, 10 September 2007, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes a FITS convention developed by the IRAF Group (D.\nTody, R. Seaman, and N. Zarate) at the National Optical Astronomical\nObservatory (NOAO). This convention is implemented by the fgread/fgwrite tasks\nin the IRAF fitsutil package. It was first used in May 1999 to encapsulate\npreview PNG-format graphics files into FITS files in the NOAO High Performance\nPipeline System. A FITS extension of type 'FOREIGN' provides a mechanism for\nstoring an arbitrary file or tree of files in FITS, allowing it to be restored\nto disk at a later time.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 04:24:22 GMT"}], "update_date": "2012-01-10", "authors_parsed": [["Zarate", "Nelson", ""], ["Seaman", "Rob", ""], ["Tody", "Doug", ""]]}, {"id": "1201.2564", "submitter": "Linh Anh Nguyen D.Sc.", "authors": "Linh Anh Nguyen and Son Thanh Cao", "title": "Query-Subquery Nets", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate query-subquery nets and use them to create the first framework\nfor developing algorithms for evaluating queries to Horn knowledge bases with\nthe properties that: the approach is goal-directed; each subquery is processed\nonly once and each supplement tuple, if desired, is transferred only once;\noperations are done set-at-a-time; and any control strategy can be used. Our\nintention is to increase efficiency of query processing by eliminating\nredundant computation, increasing flexibility and reducing the number of\naccesses to the secondary storage. The framework forms a generic evaluation\nmethod called QSQN. To deal with function symbols, we use a term-depth bound\nfor atoms and substitutions occurring in the computation and propose to use\niterative deepening search which iteratively increases the term-depth bound. We\nprove soundness and completeness of our generic evaluation method and show\nthat, when the term-depth bound is fixed, the method has PTIME data complexity.\nWe also present how tail recursion elimination can be incorporated into our\nframework and propose two exemplary control strategies, one is to reduce the\nnumber of accesses to the secondary storage, while the other is depth-first\nsearch.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2012 14:00:51 GMT"}], "update_date": "2012-01-13", "authors_parsed": [["Nguyen", "Linh Anh", ""], ["Cao", "Son Thanh", ""]]}, {"id": "1201.2766", "submitter": "Spyros Sioutas SS", "authors": "Spyros Sioutas, Peter Triantafillou, George Papaloukopoulos, Evangelos\n  Sakkopoulos, Kostas Tsichlas, Yannis Manolopoulos", "title": "ART : Sub-Logarithmic Decentralized Range Query Processing with\n  Probabilistic Guarantees", "comments": "Submitted to Distributed and Parallel Databases (DAPD) Journal,\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on range query processing on large-scale, typically distributed\ninfrastructures, such as clouds of thousands of nodes of shared-datacenters, of\np2p distributed overlays, etc. In such distributed environments, efficient\nrange query processing is the key for managing the distributed data sets per\nse, and for monitoring the infrastructure's resources. We wish to develop an\narchitecture that can support range queries in such large-scale decentralized\nenvironments and can scale in terms of the number of nodes as well as in terms\nof the data items stored. Of course, in the last few years there have been a\nnumber of solutions (mostly from researchers in the p2p domain) for designing\nsuch large-scale systems. However, these are inadequate for our purposes, since\nat the envisaged scales the classic logarithmic complexity (for point queries)\nis still too expensive while for range queries it is even more disappointing.\nIn this paper we go one step further and achieve a sub-logarithmic complexity.\nWe contribute the ART, which outperforms the most popular decentralized\nstructures, including Chord (and some of its successors), BATON (and its\nsuccessor) and Skip-Graphs. We contribute theoretical analysis, backed up by\ndetailed experimental results, showing that the communication cost of query and\nupdate operations is $O(\\log_{b}^2 \\log N)$ hops, where the base $b$ is a\ndouble-exponentially power of two and $N$ is the total number of nodes.\nMoreover, ART is a fully dynamic and fault-tolerant structure, which supports\nthe join/leave node operations in $O(\\log \\log N)$ expected w.h.p number of\nhops. Our experimental performance studies include a detailed performance\ncomparison which showcases the improved performance, scalability, and\nrobustness of ART.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2012 08:54:37 GMT"}], "update_date": "2012-01-16", "authors_parsed": [["Sioutas", "Spyros", ""], ["Triantafillou", "Peter", ""], ["Papaloukopoulos", "George", ""], ["Sakkopoulos", "Evangelos", ""], ["Tsichlas", "Kostas", ""], ["Manolopoulos", "Yannis", ""]]}, {"id": "1201.2925", "submitter": "Geetha  Manjunath", "authors": "Geetha Manjunatha, M Narasimha Murty, Dinkar Sitaram", "title": "Combining Heterogeneous Classifiers for Relational Databases", "comments": "Withdrawn - as that was a trial upload only. Non public information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Most enterprise data is distributed in multiple relational databases with\nexpert-designed schema. Using traditional single-table machine learning\ntechniques over such data not only incur a computational penalty for converting\nto a 'flat' form (mega-join), even the human-specified semantic information\npresent in the relations is lost. In this paper, we present a practical,\ntwo-phase hierarchical meta-classification algorithm for relational databases\nwith a semantic divide and conquer approach. We propose a recursive, prediction\naggregation technique over heterogeneous classifiers applied on individual\ndatabase tables. The proposed algorithm was evaluated on three diverse\ndatasets, namely TPCH, PKDD and UCI benchmarks and showed considerable\nreduction in classification time without any loss of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2012 19:54:27 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2012 20:23:24 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Manjunatha", "Geetha", ""], ["Murty", "M Narasimha", ""], ["Sitaram", "Dinkar", ""]]}, {"id": "1201.2969", "submitter": "Ghazi Al-Naymat", "authors": "Ghazi Al-Naymat, Sanjay Chawla and Javid Taheri", "title": "SparseDTW: A Novel Approach to Speed up Dynamic Time Warping", "comments": "17 pages", "journal-ref": "Al-Naymat, G., S. Chawla, and J. Taheri, \"SparseDTW: A Novel\n  Approach to Speed up Dynamic Time Warping\", The 2009 Australasian Data\n  Mining, vol. 101, Melbourne, Australia, ACM Digital Library, pp. 117-127,\n  12/2009", "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present a new space-efficient approach, (SparseDTW), to compute the\nDynamic Time Warping (DTW) distance between two time series that always yields\nthe optimal result. This is in contrast to other known approaches which\ntypically sacrifice optimality to attain space efficiency. The main idea behind\nour approach is to dynamically exploit the existence of similarity and/or\ncorrelation between the time series. The more the similarity between the time\nseries the less space required to compute the DTW between them. To the best of\nour knowledge, all other techniques to speedup DTW, impose apriori constraints\nand do not exploit similarity characteristics that may be present in the data.\nWe conduct experiments and demonstrate that SparseDTW outperforms previous\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2012 23:26:27 GMT"}], "update_date": "2012-01-17", "authors_parsed": [["Al-Naymat", "Ghazi", ""], ["Chawla", "Sanjay", ""], ["Taheri", "Javid", ""]]}, {"id": "1201.3458", "submitter": "Jeffrey Yu", "authors": "Di Wu, Yiping Ke, Jeffrey Xu Yu, Zheng Liu", "title": "Detecting Priming News Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a problem of detecting priming events based on a time series index\nand an evolving document stream. We define a priming event as an event which\ntriggers abnormal movements of the time series index, i.e., the Iraq war with\nrespect to the president approval index of President Bush. Existing solutions\neither focus on organizing coherent keywords from a document stream into events\nor identifying correlated movements between keyword frequency trajectories and\nthe time series index. In this paper, we tackle the problem in two major steps.\n(1) We identify the elements that form a priming event. The element identified\nis called influential topic which consists of a set of coherent keywords. And\nwe extract them by looking at the correlation between keyword trajectories and\nthe interested time series index at a global level. (2) We extract priming\nevents by detecting and organizing the bursty influential topics at a micro\nlevel. We evaluate our algorithms on a real-world dataset and the result\nconfirms that our method is able to discover the priming events effectively.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2012 08:59:57 GMT"}], "update_date": "2012-01-18", "authors_parsed": [["Wu", "Di", ""], ["Ke", "Yiping", ""], ["Yu", "Jeffrey Xu", ""], ["Liu", "Zheng", ""]]}, {"id": "1201.4479", "submitter": "Saber Jafarizadeh", "authors": "Saber Jafarizadeh, Abbas Jamalipour", "title": "Distributed Data Storage in Large-Scale Sensor Networks Based on LT\n  Codes", "comments": "9 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an algorithm for increasing data persistency in\nlarge-scale sensor networks. In the scenario considered here, k out of n nodes\nsense the phenomenon and produced ? information packets. Due to usually\nhazardous environment and limited resources, e.g. energy, sensors in the\nnetwork are vulnerable. Also due to the large size of the network, gathering\ninformation from a few central hopes is not feasible. Flooding is not a desired\noption either due to limited memory of each node. Therefore the best approach\nto increase data persistency is propagating data throughout the network by\nrandom walks. The algorithm proposed here is based on distributed LT (Luby\nTransform) codes and it benefits from the low complexity of encoding and\ndecoding of LT codes. In previous algorithms the essential global information\n(e.g., n and k) are estimated based on graph statistics, which requires\nexcessive transmissions. In our proposed algorithm, these values are obtained\nwithout additional transmissions. Also the mixing time of random walk is\nenhanced by proposing a new scheme for generating the probabilistic forwarding\ntable of random walk. The proposed method uses only local information and it is\nscalable to any network topology. By simulations the improved performance of\ndeveloped algorithm compared to previous ones has been verified.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2012 14:51:31 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Jafarizadeh", "Saber", ""], ["Jamalipour", "Abbas", ""]]}, {"id": "1201.6112", "submitter": "Seyed Aliakbar Mousavi aslarzanagh Seyed Aliakbar Mousavi aslarz", "authors": "Seyed Aliakbar Mousavi, Muhammad Rafie Hj Arshad, Hasimah Hj Mohamed\n  and Saleh Ali Alomari", "title": "An Efficient Method for Mining Event-Related Potential Patterns", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 6, No 1, November 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we propose a Neuroelectromagnetic Ontology Framework\n(NOF) for mining Event-related Potentials (ERP) patterns as well as the\nprocess. The aim for this research is to develop an infrastructure for mining,\nanalysis and sharing the ERP domain ontologies. The outcome of this research is\na Neuroelectromagnetic knowledge-based system. The framework has 5 stages: 1)\nData pre-processing and preparation; 2) Data mining application; 3) Rule\nComparison and Evaluation; 4) Association rules Post-processing 5) Domain\nOntologies. In 5th stage a new set of hidden rules can be discovered base on\ncomparing association rules by domain ontologies and expert rules.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2012 06:31:06 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Mousavi", "Seyed Aliakbar", ""], ["Arshad", "Muhammad Rafie Hj", ""], ["Mohamed", "Hasimah Hj", ""], ["Alomari", "Saleh Ali", ""]]}, {"id": "1201.6402", "submitter": "Neil J. Gunther", "authors": "Neil J. Gunther", "title": "A Note on Disk Drag Dynamics", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DB physics.class-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electrical power consumed by typical magnetic hard disk drives (HDD) not\nonly increases linearly with the number of spindles but, more significantly, it\nincreases as very fast power-laws of speed (RPM) and diameter. Since the\ntheoretical basis for this relationship is neither well-known nor readily\naccessible in the literature, we show how these exponents arise from\naerodynamic disk drag and discuss their import for green storage capacity\nplanning.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2012 23:03:45 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Gunther", "Neil J.", ""]]}, {"id": "1201.6563", "submitter": "Yizhou Sun", "authors": "Yizhou Sun, Charu C. Aggarwal, Jiawei Han", "title": "Relation Strength-Aware Clustering of Heterogeneous Information Networks\n  with Incomplete Attributes", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 5, pp.\n  394-405 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of online social media, online shopping sites and\ncyber-physical systems, heterogeneous information networks have become\nincreasingly popular and content-rich over time. In many cases, such networks\ncontain multiple types of objects and links, as well as different kinds of\nattributes. The clustering of these objects can provide useful insights in many\napplications. However, the clustering of such networks can be challenging since\n(a) the attribute values of objects are often incomplete, which implies that an\nobject may carry only partial attributes or even no attributes to correctly\nlabel itself; and (b) the links of different types may carry different kinds of\nsemantic meanings, and it is a difficult task to determine the nature of their\nrelative importance in helping the clustering for a given purpose. In this\npaper, we address these challenges by proposing a model-based clustering\nalgorithm. We design a probabilistic model which clusters the objects of\ndifferent types into a common hidden space, by using a user-specified set of\nattributes, as well as the links from different relations. The strengths of\ndifferent types of links are automatically learned, and are determined by the\ngiven purpose of clustering. An iterative algorithm is designed for solving the\nclustering problem, in which the strengths of different types of links and the\nquality of clustering results mutually enhance each other. Our experimental\nresults on real and synthetic data sets demonstrate the effectiveness and\nefficiency of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 15:08:41 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Sun", "Yizhou", ""], ["Aggarwal", "Charu C.", ""], ["Han", "Jiawei", ""]]}, {"id": "1201.6564", "submitter": "Xiaokui Xiao", "authors": "Lingkun Wu, Xiaokui Xiao, Dingxiong Deng, Gao Cong, Andy Diwen Zhu,\n  Shuigeng Zhou", "title": "Shortest Path and Distance Queries on Road Networks: An Experimental\n  Evaluation", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 5, pp.\n  406-417 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the shortest path between two given locations in a road network is\nan important problem that finds applications in various map services and\ncommercial navigation products. The state-of-the-art solutions for the problem\ncan be divided into two categories: spatial-coherence-based methods and\nvertex-importance-based approaches. The two categories of techniques, however,\nhave not been compared systematically under the same experimental framework, as\nthey were developed from two independent lines of research that do not refer to\neach other. This renders it difficult for a practitioner to decide which\ntechnique should be adopted for a specific application. Furthermore, the\nexperimental evaluation of the existing techniques, as presented in previous\nwork, falls short in several aspects. Some methods were tested only on small\nroad networks with up to one hundred thousand vertices; some approaches were\nevaluated using distance queries (instead of shortest path queries), namely,\nqueries that ask only for the length of the shortest path; a state-of-the-art\ntechnique was examined based on a faulty implementation that led to incorrect\nquery results. To address the above issues, this paper presents a comprehensive\ncomparison of the most advanced spatial-coherence-based and\nvertex-importance-based approaches. Using a variety of real road networks with\nup to twenty million vertices, we evaluated each technique in terms of its\npreprocessing time, space consumption, and query efficiency (for both shortest\npath and distance queries). Our experimental results reveal the characteristics\nof different techniques, based on which we provide guidelines on selecting\nappropriate methods for various scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 15:08:53 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Wu", "Lingkun", ""], ["Xiao", "Xiaokui", ""], ["Deng", "Dingxiong", ""], ["Cong", "Gao", ""], ["Zhu", "Andy Diwen", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "1201.6565", "submitter": "Dora Erdos", "authors": "D\\'ora Erd\\\"os, Vatche Ishakian, Andrei Lapets, Evimaria Terzi, Azer\n  Bestavros", "title": "The Filter-Placement Problem and its Application to Minimizing\n  Information Multiplicity", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 5, pp.\n  418-429 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many information networks, data items -- such as updates in social\nnetworks, news flowing through interconnected RSS feeds and blogs, measurements\nin sensor networks, route updates in ad-hoc networks -- propagate in an\nuncoordinated manner: nodes often relay information they receive to neighbors,\nindependent of whether or not these neighbors received the same information\nfrom other sources. This uncoordinated data dissemination may result in\nsignificant, yet unnecessary communication and processing overheads, ultimately\nreducing the utility of information networks. To alleviate the negative impacts\nof this information multiplicity phenomenon, we propose that a subset of nodes\n(selected at key positions in the network) carry out additional information\nfiltering functionality. Thus, nodes are responsible for the removal (or\nsignificant reduction) of the redundant data items relayed through them. We\nrefer to such nodes as filters. We formally define the Filter Placement problem\nas a combinatorial optimization problem, and study its computational complexity\nfor different types of graphs. We also present polynomial-time approximation\nalgorithms and scalable heuristics for the problem. Our experimental results,\nwhich we obtained through extensive simulations on synthetic and real-world\ninformation flow networks, suggest that in many settings a relatively small\nnumber of filters are fairly effective in removing a large fraction of\nredundant information.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 15:09:10 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Erd\u00f6s", "D\u00f3ra", ""], ["Ishakian", "Vatche", ""], ["Lapets", "Andrei", ""], ["Terzi", "Evimaria", ""], ["Bestavros", "Azer", ""]]}, {"id": "1201.6566", "submitter": "Yasuhiro Fujiwara", "authors": "Yasuhiro Fujiwara, Makoto Nakatsuji, Makoto Onizuka, Masaru\n  Kitsuregawa", "title": "Fast and Exact Top-k Search for Random Walk with Restart", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 5, pp.\n  442-453 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are fundamental data structures and have been employed for centuries\nto model real-world systems and phenomena. Random walk with restart (RWR)\nprovides a good proximity score between two nodes in a graph, and it has been\nsuccessfully used in many applications such as automatic image captioning,\nrecommender systems, and link prediction. The goal of this work is to find\nnodes that have top-k highest proximities for a given node. Previous approaches\nto this problem find nodes efficiently at the expense of exactness. The main\nmotivation of this paper is to answer, in the affirmative, the question, `Is it\npossible to improve the search time without sacrificing the exactness?'. Our\nsolution, {it K-dash}, is based on two ideas: (1) It computes the proximity of\na selected node efficiently by sparse matrices, and (2) It skips unnecessary\nproximity computations when searching for the top-k nodes. Theoretical analyses\nshow that K-dash guarantees result exactness. We perform comprehensive\nexperiments to verify the efficiency of K-dash. The results show that K-dash\ncan find top-k nodes significantly faster than the previous approaches while it\nguarantees exactness.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 15:09:50 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Fujiwara", "Yasuhiro", ""], ["Nakatsuji", "Makoto", ""], ["Onizuka", "Makoto", ""], ["Kitsuregawa", "Masaru", ""]]}, {"id": "1201.6567", "submitter": "Ravi Kumar", "authors": "Bahman Bahmani, Ravi Kumar, Sergei Vassilvitskii", "title": "Densest Subgraph in Streaming and MapReduce", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 5, pp.\n  454-465 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding locally dense components of a graph is an important\nprimitive in data analysis, with wide-ranging applications from community\nmining to spam detection and the discovery of biological network modules. In\nthis paper we present new algorithms for finding the densest subgraph in the\nstreaming model. For any epsilon>0, our algorithms make O((log n)/log\n(1+epsilon)) passes over the input and find a subgraph whose density is\nguaranteed to be within a factor 2(1+epsilon) of the optimum. Our algorithms\nare also easily parallelizable and we illustrate this by realizing them in the\nMapReduce model. In addition we perform extensive experimental evaluation on\nmassive real-world graphs showing the performance and scalability of our\nalgorithms in practice.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 15:10:03 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Bahmani", "Bahman", ""], ["Kumar", "Ravi", ""], ["Vassilvitskii", "Sergei", ""]]}, {"id": "1201.6568", "submitter": "Arlei Silva", "authors": "Arlei Silva, Wagner Meira Jr., Mohammed J. Zaki", "title": "Mining Attribute-structure Correlated Patterns in Large Attributed\n  Graphs", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 5, pp.\n  466-477 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the correlation between attribute sets and the\noccurrence of dense subgraphs in large attributed graphs, a task we call\nstructural correlation pattern mining. A structural correlation pattern is a\ndense subgraph induced by a particular attribute set. Existing methods are not\nable to extract relevant knowledge regarding how vertex attributes interact\nwith dense subgraphs. Structural correlation pattern mining combines aspects of\nfrequent itemset and quasi-clique mining problems. We propose statistical\nsignificance measures that compare the structural correlation of attribute sets\nagainst their expected values using null models. Moreover, we evaluate the\ninterestingness of structural correlation patterns in terms of size and\ndensity. An efficient algorithm that combines search and pruning strategies in\nthe identification of the most relevant structural correlation patterns is\npresented. We apply our method for the analysis of three real-world attributed\ngraphs: a collaboration, a music, and a citation network, verifying that it\nprovides valuable knowledge in a feasible time.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 15:10:13 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Silva", "Arlei", ""], ["Meira", "Wagner", "Jr."], ["Zaki", "Mohammed J.", ""]]}, {"id": "1201.6569", "submitter": "Robert Fink", "authors": "Robert Fink, Larisa Han, Dan Olteanu", "title": "Aggregation in Probabilistic Databases via Knowledge Compilation", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 5, pp.\n  490-501 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a query evaluation technique for positive relational\nalgebra queries with aggregates on a representation system for probabilistic\ndata based on the algebraic structures of semiring and semimodule. The core of\nour evaluation technique is a procedure that compiles semimodule and semiring\nexpressions into so-called decomposition trees, for which the computation of\nthe probability distribution can be done in time linear in the product of the\nsizes of the probability distributions represented by its nodes. We give\nsyntactic characterisations of tractable queries with aggregates by exploiting\nthe connection between query tractability and polynomial-time decomposition\ntrees. A prototype of the technique is incorporated in the probabilistic\ndatabase engine SPROUT. We report on performance experiments with custom\ndatasets and TPC-H data.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 15:10:34 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Fink", "Robert", ""], ["Han", "Larisa", ""], ["Olteanu", "Dan", ""]]}]