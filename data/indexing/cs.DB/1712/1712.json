[{"id": "1712.00078", "submitter": "Thibault Sellam", "authors": "Haoci Zhang and Thibault Sellam and Eugene Wu", "title": "Mining Precision Interfaces From Query Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive tools make data analysis both more efficient and more accessible\nto a broad population. Simple interfaces such as Google Finance as well as\ncomplex visual exploration interfaces such as Tableau are effective because\nthey are tailored to the desired user tasks. Yet, designing interactive\ninterfaces requires technical expertise and domain knowledge. Experts are\nscarce and expensive, and therefore it is currently infeasible to provide\ntailored (or precise) interfaces for every user and every task.\n  We envision a data-driven approach to generate tailored interactive\ninterfaces. We observe that interactive interfaces are designed to express sets\nof programs; thus, samples of programs-increasingly collected by data\nsystems-may help us build interactive interfaces. Based on this idea, Precision\nInterfaces is a language-agnostic system that examines an input query log,\nidentifies how the queries structurally change, and generates interactive web\ninterfaces to express these changes. The focus of this paper is on applying\nthis idea towards logs of structured queries. Our experiments show that\nPrecision Interfaces can support multiple query languages (SQL and SPARQL),\nderive Tableau's salient interaction components from OLAP queries, analyze <75k\nqueries in <12 minutes, and generate interaction designs that improve upon\nexisting interfaces and are comparable to human-crafted interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 21:03:24 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Zhang", "Haoci", ""], ["Sellam", "Thibault", ""], ["Wu", "Eugene", ""]]}, {"id": "1712.00498", "submitter": "Karthik Ramachandra", "authors": "Karthik Ramachandra, Kwanghyun Park, K. Venkatesh Emani, Alan\n  Halverson, Cesar Galindo-Legaria, Conor Cunningham", "title": "Optimization of Imperative Programs in a Relational Database", "comments": "Extended version of the paper titled \"FROID: Optimization of\n  Imperative Programs in a Relational Database\" in PVLDB 11(4), 2017. DOI:\n  10.1145/3164135.3164140", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, RDBMSs have supported declarative SQL as well as imperative\nfunctions and procedures as ways for users to express data processing tasks.\nWhile the evaluation of declarative SQL has received a lot of attention\nresulting in highly sophisticated techniques, the evaluation of imperative\nprograms has remained naive and highly inefficient. Imperative programs offer\nseveral benefits over SQL and hence are often preferred and widely used. But\nunfortunately, their abysmal performance discourages, and even prohibits their\nuse in many situations. We address this important problem that has hitherto\nreceived little attention.\n  We present Froid, an extensible framework for optimizing imperative programs\nin relational databases. Froid's novel approach automatically transforms entire\nUser Defined Functions (UDFs) into relational algebraic expressions, and embeds\nthem into the calling SQL query. This form is now amenable to cost-based\noptimization and results in efficient, set-oriented, parallel plans as opposed\nto inefficient, iterative, serial execution of UDFs. Froid's approach\nadditionally brings the benefits of many compiler optimizations to UDFs with no\nadditional implementation effort. We describe the design of Froid and present\nour experimental evaluation that demonstrates performance improvements of up to\nmultiple orders of magnitude on real workloads.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 21:24:25 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 17:52:53 GMT"}, {"version": "v3", "created": "Sat, 17 Aug 2019 09:23:59 GMT"}, {"version": "v4", "created": "Tue, 20 Aug 2019 08:17:54 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Ramachandra", "Karthik", ""], ["Park", "Kwanghyun", ""], ["Emani", "K. Venkatesh", ""], ["Halverson", "Alan", ""], ["Galindo-Legaria", "Cesar", ""], ["Cunningham", "Conor", ""]]}, {"id": "1712.00802", "submitter": "Hayden Jananthan", "authors": "Hayden Jananthan and Ziqi Zhou and Vijay Gadepally and Dylan Hutchison\n  and Suna Kim and Jeremy Kepner", "title": "Polystore Mathematics of Relational Algebra", "comments": "10 pages, 2 figures, accepted to Big Data 2017 2nd International\n  Workshop on Methods to Manage Heterogeneous Big Data and Polystore Databases", "journal-ref": null, "doi": "10.1109/BigData.2017.8258298", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial transactions, internet search, and data analysis are all placing\nincreasing demands on databases. SQL, NoSQL, and NewSQL databases have been\ndeveloped to meet these demands and each offers unique benefits. SQL, NoSQL,\nand NewSQL databases also rely on different underlying mathematical models.\nPolystores seek to provide a mechanism to allow applications to transparently\nachieve the benefits of diverse databases while insulating applications from\nthe details of these databases. Integrating the underlying mathematics of these\ndiverse databases can be an important enabler for polystores as it enables\neffective reasoning across different databases. Associative arrays provide a\ncommon approach for the mathematics of polystores by encompassing the\nmathematics found in different databases: sets (SQL), graphs (NoSQL), and\nmatrices (NewSQL). Prior work presented the SQL relational model in terms of\nassociative arrays and identified key mathematical properties that are\npreserved within SQL. This work provides the rigorous mathematical definitions,\nlemmas, and theorems underlying these properties. Specifically, SQL Relational\nAlgebra deals primarily with relations - multisets of tuples - and operations\non and between these relations. These relations can be modeled as associative\narrays by treating tuples as non-zero rows in an array. Operations in\nrelational algebra are built as compositions of standard operations on\nassociative arrays which mirror their matrix counterparts. These constructions\nprovide insight into how relational algebra can be handled via array\noperations. As an example application, the composition of two projection\noperations is shown to also be a projection, and the projection of a union is\nshown to be equal to the union of the projections.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 17:25:03 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Jananthan", "Hayden", ""], ["Zhou", "Ziqi", ""], ["Gadepally", "Vijay", ""], ["Hutchison", "Dylan", ""], ["Kim", "Suna", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1712.01001", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi", "title": "Specifying and Computing Causes for Query Answers in Databases via\n  Database Repairs and Repair Programs", "comments": "To appear in \"Knowledge and Information Systems\" journal. This is the\n  final version, and a much revised, corrected and extended version of:\n  Bertossi, L. \"Characterizing and Computing Causes for Query Answers in\n  Databases from Database Repairs and Repair Programs\". Proc. FoIKs, 2018,\n  Springer LNCS 10833, pp. 55-76", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A correspondence between database tuples as causes for query answers in\ndatabases and tuple-based repairs of inconsistent databases with respect to\ndenial constraints has already been established. In this work, answer-set\nprograms that specify repairs of databases are used as a basis for solving\ncomputational and reasoning problems about causes. Here, causes are also\nintroduced at the attribute level by appealing to a both null-based and\nattribute-based repair semantics. The corresponding repair programs are\npresented, and they are used as a basis for computation and reasoning about\nattribute-level causes. They are extended to deal with the case of causality\nunder integrity constraints.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 11:00:38 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 00:16:47 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 21:38:17 GMT"}, {"version": "v4", "created": "Fri, 4 Jan 2019 16:12:27 GMT"}, {"version": "v5", "created": "Sat, 2 Mar 2019 22:07:45 GMT"}, {"version": "v6", "created": "Fri, 24 Jul 2020 22:50:14 GMT"}, {"version": "v7", "created": "Mon, 28 Sep 2020 19:26:52 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Bertossi", "Leopoldo", ""]]}, {"id": "1712.01063", "submitter": "Cristian Riveros", "authors": "Alejandro Grez, Cristian Riveros, Martin Ugarte, Stijn Vansummeren", "title": "A Second-Order Approach to Complex Event Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex Event Recognition (CER for short) refers to the activity of detecting\npatterns in streams of continuously arriving data. This field has been\ntraditionally approached from a practical point of view, resulting in\nheterogeneous implementations with fundamentally different capabilities. The\nmain reason behind this is that defining formal semantics for a CER language is\nnot trivial: they usually combine first-order variables for joining and\nfiltering events with regular operators like sequencing and Kleene closure.\nMoreover, their semantics usually focus only on the detection of complex\nevents, leaving the concept of output mostly unattended.\n  In this paper, we propose to unify the semantics and output of complex event\nrecognition languages by using second order objects. Specifically, we introduce\na CER language called Second Order Complex Event Logic (SO-CEL for short), that\nuses second order variables for managing and outputting sequences of events.\nThis makes the definition of the main CER operators simple, allowing us to\ndevelop the first steps in understanding its expressive power. We start by\ncomparing SO-CEL with a version that uses first-order variables called FO-CEL,\nshowing that they are equivalent in expressive power when restricted to unary\npredicates but, surprisingly, incomparable in general. Nevertheless, we show\nthat if we restrict to sets of binary predicates, then SO-CEL is strictly more\nexpressive than FO-CEL. Then, we introduce a natural computational model called\nUnary Complex Event Automata (UCEA) that provides a better understanding of\nSO-CEL. We show that, under unary predicates, SO-CEL captures the subclass of\nUCEA that satisfy the so-called *-property. Finally, we identify the operations\nthat SO-CEL is lacking to capture UCEA and introduce a natural extension of the\nlanguage that captures the complete class of UCEA under unary predicates.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 13:33:37 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Grez", "Alejandro", ""], ["Riveros", "Cristian", ""], ["Ugarte", "Martin", ""], ["Vansummeren", "Stijn", ""]]}, {"id": "1712.01208", "submitter": "Tim Kraska", "authors": "Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, Neoklis Polyzotis", "title": "The Case for Learned Index Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexes are models: a B-Tree-Index can be seen as a model to map a key to the\nposition of a record within a sorted array, a Hash-Index as a model to map a\nkey to a position of a record within an unsorted array, and a BitMap-Index as a\nmodel to indicate if a data record exists or not. In this exploratory research\npaper, we start from this premise and posit that all existing index structures\ncan be replaced with other types of models, including deep-learning models,\nwhich we term learned indexes. The key idea is that a model can learn the sort\norder or structure of lookup keys and use this signal to effectively predict\nthe position or existence of records. We theoretically analyze under which\nconditions learned indexes outperform traditional index structures and describe\nthe main challenges in designing learned index structures. Our initial results\nshow, that by using neural nets we are able to outperform cache-optimized\nB-Trees by up to 70% in speed while saving an order-of-magnitude in memory over\nseveral real-world data sets. More importantly though, we believe that the idea\nof replacing core components of a data management system through learned models\nhas far reaching implications for future systems designs and that this work\njust provides a glimpse of what might be possible.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 17:18:41 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 19:42:46 GMT"}, {"version": "v3", "created": "Mon, 30 Apr 2018 07:54:41 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kraska", "Tim", ""], ["Beutel", "Alex", ""], ["Chi", "Ed H.", ""], ["Dean", "Jeffrey", ""], ["Polyzotis", "Neoklis", ""]]}, {"id": "1712.01550", "submitter": "Hannes Voigt", "authors": "Renzo Angles and Marcelo Arenas and Pablo Barcel\\'o and Peter Boncz\n  and George H. L. Fletcher and Claudio Gutierrez and Tobias Lindaaker and\n  Marcus Paradies and Stefan Plantikow and Juan Sequeda and Oskar van Rest and\n  Hannes Voigt", "title": "G-CORE: A Core for Future Graph Query Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on a community effort between industry and academia to shape the\nfuture of graph query languages. We argue that existing graph database\nmanagement systems should consider supporting a query language with two key\ncharacteristics. First, it should be composable, meaning, that graphs are the\ninput and the output of queries. Second, the graph query language should treat\npaths as first-class citizens. Our result is G-CORE, a powerful graph query\nlanguage design that fulfills these goals, and strikes a careful balance\nbetween path query expressivity and evaluation complexity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 09:59:47 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 08:02:06 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Angles", "Renzo", ""], ["Arenas", "Marcelo", ""], ["Barcel\u00f3", "Pablo", ""], ["Boncz", "Peter", ""], ["Fletcher", "George H. L.", ""], ["Gutierrez", "Claudio", ""], ["Lindaaker", "Tobias", ""], ["Paradies", "Marcus", ""], ["Plantikow", "Stefan", ""], ["Sequeda", "Juan", ""], ["van Rest", "Oskar", ""], ["Voigt", "Hannes", ""]]}, {"id": "1712.01817", "submitter": "Yaron Gonen", "authors": "Yaron Gonen", "title": "Analyzing Large-Scale, Distributed and Uncertain Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth of data in current times and the demand to gain\ninformation and knowledge from the data present new challenges for database\nresearchers. Known database systems and algorithms are no longer capable of\neffectively handling such large data sets. MapReduce is a novel programming\nparadigm for processing distributable problems over large-scale data using a\ncomputer cluster.\n  In this work we explore the MapReduce paradigm from three different angles.\nWe begin by examining a well-known problem in the field of data mining: mining\nclosed frequent itemsets over a large dataset. By harnessing the power of\nMapReduce, we present a novel algorithm for mining closed frequent itemsets\nthat outperforms existing algorithms.\n  Next, we explore one of the fundamental implications of \"Big Data\": The data\nis not known with complete certainty. A probabilistic database is a relational\ndatabase with the addendum that each tuple is associated with a probability of\nits existence. A natural development of MapReduce is of a distributed\nrelational database management system, where relational calculus has been\nreduced to a combination of map and reduce function. We take this development a\nstep further by proposing a query optimizer over distributed, probabilistic\ndatabase.\n  Finally, we analyze the best known implementation of MapReduce called Hadoop,\naiming to overcome one of its major drawbacks: it does not directly support the\nexplicit specification of the data repeatedly processed throughout different\njobs.Many data-mining algorithms, such as clustering and association-rules\nrequire iterative computation: the same data are processed again and again\nuntil the computation converges or a stopping condition is satisfied. We\npropose a modification to Hadoop such that it will support efficient access to\nthe same data in different jobs.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 18:50:07 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Gonen", "Yaron", ""]]}, {"id": "1712.02869", "submitter": "Harold Boley", "authors": "Harold Boley, Gen Zou", "title": "Perspectival Knowledge in PSOA RuleML: Representation, Model Theory, and\n  Translation", "comments": "39 pages, 5 figures, 2 tables; updates for PSOATransRun 1.3.1 to\n  1.4.2; refined terminology and metamodel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Positional-Slotted Object-Applicative (PSOA) RuleML, a predicate\napplication (atom) can have an Object IDentifier (OID) and descriptors that may\nbe positional arguments (tuples) or attribute-value pairs (slots). PSOA RuleML\nexplicitly specifies for each descriptor whether it is to be interpreted under\nthe perspective of the predicate in whose scope it occurs. This\npredicate-dependency dimension refines the space between oidless, positional\natoms (relationships) and oidful, slotted atoms (framepoints): While\nrelationships use only a predicate-scope-sensitive (predicate-dependent) tuple\nand framepoints use only predicate-scope-insensitive (predicate-independent)\nslots, PSOA uses a systematics of orthogonal constructs also permitting atoms\nwith (predicate-)independent tuples and atoms with (predicate-)dependent slots.\nThis supports data and knowledge representation where a slot attribute can have\ndifferent values depending on the predicate. PSOA thus extends object-oriented\nmulti-membership and multiple inheritance. Based on objectification, PSOA laws\nare given: Besides unscoping and centralization, the semantic restriction and\ntransformation of describution permits rescoping of one atom's independent\ndescriptors to another atom with the same OID but a different predicate. For\ninheritance, default descriptors are realized by rules. On top of a metamodel\nand a Grailog visualization, PSOA's atom systematics for facts, queries, and\nrules is explained. The presentation and (XML-)serialization syntaxes of PSOA\nRuleML are introduced. Its model-theoretic semantics is formalized by extending\nthe interpretation functions for dependent descriptors. The open-source\nPSOATransRun system realizes PSOA RuleML by a translator to runtime predicates,\nincluding for dependent tuples (prdtupterm) and slots (prdsloterm). Our tests\nshow efficiency advantages of dependent and tupled modeling.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 21:36:21 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 00:19:21 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 17:58:15 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Boley", "Harold", ""], ["Zou", "Gen", ""]]}, {"id": "1712.02882", "submitter": "Brad Carlile", "authors": "Brad Carlile, Akiko Marti, Guy Delamarter", "title": "Columnar Database Techniques for Creating AI Features", "comments": "7 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances with in-memory columnar database techniques have increased\nthe performance of analytical queries on very large databases and data\nwarehouses. At the same time, advances in artificial intelligence (AI)\nalgorithms have increased the ability to analyze data. We use the term AI to\nencompass both Deep Learning (DL or neural network) and Machine Learning (ML\naka Big Data analytics). Our exploration of the AI full stack has led us to a\ncross-stack columnar database innovation that efficiently creates features for\nAI analytics. The innovation is to create Augmented Dictionary Values (ADVs) to\nadd to existing columnar database dictionaries in order to increase the\nefficiency of featurization by minimizing data movement and data duplication.\nWe show how various forms of featurization (feature selection, feature\nextraction, and feature creation) can be efficiently calculated in a columnar\ndatabase. The full stack AI investigation has also led us to propose an\nintegrated columnar database and AI architecture. This architecture has\ninformation flows and feedback loops to improve the whole analytics cycle\nduring multiple iterations of extracting data from the data sources,\nfeaturization, and analysis.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 22:53:41 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Carlile", "Brad", ""], ["Marti", "Akiko", ""], ["Delamarter", "Guy", ""]]}, {"id": "1712.02912", "submitter": "Fabien Andr\\'e", "authors": "Fabien Andr\\'e", "title": "Exploiting Modern Hardware for High-Dimensional Nearest Neighbor Search", "comments": "PhD Thesis, 123 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.MM cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many multimedia information retrieval or machine learning problems require\nefficient high-dimensional nearest neighbor search techniques. For instance,\nmultimedia objects (images, music or videos) can be represented by\nhigh-dimensional feature vectors. Finding two similar multimedia objects then\ncomes down to finding two objects that have similar feature vectors. In the\ncurrent context of mass use of social networks, large scale multimedia\ndatabases or large scale machine learning applications are more and more\ncommon, calling for efficient nearest neighbor search approaches.\n  This thesis builds on product quantization, an efficient nearest neighbor\nsearch technique that compresses high-dimensional vectors into short codes.\nThis makes it possible to store very large databases entirely in RAM, enabling\nlow response times. We propose several contributions that exploit the\ncapabilities of modern CPUs, especially SIMD and the cache hierarchy, to\nfurther decrease response times offered by product quantization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 02:14:17 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Andr\u00e9", "Fabien", ""]]}, {"id": "1712.03320", "submitter": "Xin Zhang", "authors": "Xin Zhang", "title": "Code Generation Techniques for Raw Data Processing", "comments": "MS thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of the current study was to design an algorithm that can speed\nup the processing of a query. The important feature is generating code\ndynamically for a specific query. We present the technique of code generation\nthat is applied to query processing on a raw file. The idea was to customize a\nquery program with a given query and generate a machine- and query-specific\nsource code. The generated code is compiled by GCC, Clang or any other C/C++\ncompiler, and the compiled file is dynamically linked to the main program for\nfurther processing. Code generation reduces the cost of generalizing query\nprocessing. It also avoids the overhead of the conventional interpretation\nduring achieve high performance. Database Management Systems (DBMSs) perform\nexcellent jobs in many aspects of big data, such as storage, indexing, and\nanalysis. DBMSs typically format entire data and load them into their storage\nlayer. They increase data-to-query time, which is the cost time it takes to\nconvert data into a specific schema and persist them in a disk. Ideally, DBMSs\nshould adapt to the input data and extract one/some of columns, not the entire\ndata, that is/are associated with a given query. Therefore, the query engine on\na raw file can reduce the cost of conventional general operators and avoid some\nunnecessary procedures, such as fully scanning, tokenizing and paring the whole\ndata. In the current study, we introduce our code-generation approach for\nin-situ processing of raw files, which is based on the template approach and\nthe hype approach. The approach minimizes the data-to-query time and achieves a\nhigh performance for query processing. There are some benefits from our work:\nreducing branches and instructions, unrolling loops, eliminating unnecessary\ndata type checks and optimizing the binary code with a compiler on a local\nmachine.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 00:34:47 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Zhang", "Xin", ""]]}, {"id": "1712.03438", "submitter": "Rada Chirkova", "authors": "Rada Chirkova, Jon Doyle, Juan L. Reutter", "title": "Assessing Achievability of Queries and Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing and improving the quality of data in data-intensive systems are\nfundamental challenges that have given rise to numerous applications targeting\ntransformation and cleaning of data. However, while schema design, data\ncleaning, and data migration are nowadays reasonably well understood in\nisolation, not much attention has been given to the interplay between the tools\nthat address issues in these areas. Our focus is on the problem of determining\nwhether there exist sequences of data-transforming procedures that, when\napplied to the (untransformed) input data, would yield data satisfying the\nconditions required for performing the task in question. Our goal is to develop\na framework that would address this problem, starting with the relational\nsetting.\n  In this paper we abstract data-processing tools as black-box procedures. This\nabstraction describes procedures by a specification of which parts of the\ndatabase might be modified by the procedure, as well as by the constraints that\nspecify the required states of the database before and after applying the\nprocedure. We then proceed to study fundamental algorithmic questions arising\nin this context, such as understanding when one can guarantee that sequences of\nprocedures apply to original or transformed data, when they succeed at\nimproving the data, and when knowledge bases can represent the outcomes of\nprocedures. Finally, we turn to the problem of determining whether the\napplication of a sequence of procedures to a database results in the\nsatisfaction of properties specified by either queries or constraints. We show\nthat this problem is decidable for some broad and realistic classes of\nprocedures and properties, even when procedures are allowed to alter the schema\nof instances.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 20:57:51 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chirkova", "Rada", ""], ["Doyle", "Jon", ""], ["Reutter", "Juan L.", ""]]}, {"id": "1712.03529", "submitter": "Behrooz Omidvar-Tehrani", "authors": "Sihem Amer-Yahia, Behrooz Omidvar-Tehrani, Joao Comba, Viviane\n  Moreira, Fabian Colque Zegarra", "title": "Exploration of User Groups in VEXUS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce VEXUS, an interactive visualization framework for exploring user\ndata to fulfill tasks such as finding a set of experts, forming discussion\ngroups and analyzing collective behaviors. User data is characterized by a\ncombination of demographics like age and occupation, and actions such as rating\na movie, writing a paper, following a medical treatment or buying groceries.\nThe ubiquity of user data requires tools that help explorers, be they\nspecialists or novice users, acquire new insights. VEXUS lets explorers\ninteract with user data via visual primitives and builds an exploration profile\nto recommend the next exploration steps. VEXUS combines state-of-the-art\nvisualization techniques with appropriate indexing of user data to provide fast\nand relevant exploration.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 14:01:53 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Amer-Yahia", "Sihem", ""], ["Omidvar-Tehrani", "Behrooz", ""], ["Comba", "Joao", ""], ["Moreira", "Viviane", ""], ["Zegarra", "Fabian Colque", ""]]}, {"id": "1712.03900", "submitter": "Ivens Portugal", "authors": "Ivens Portugal, Paulo Alencar, Donald Cowan", "title": "Developing a Spatial-Temporal Contextual and Semantic Trajectory\n  Clustering Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on ongoing research investigating more expressive\napproaches to spatial-temporal trajectory clustering. Spatial-temporal data is\nincreasingly becoming universal as a result of widespread use of GPS and mobile\ndevices, which makes mining and predictive analyses based on trajectories a\ncritical activity in many domains. Trajectory analysis methods based on\nclustering techniques heavily often rely on a similarity definition to properly\nprovide insights. However, although trajectories are currently described in\nterms of its two dimensions (space and time), their representation is limited\nin that it is not expressive enough to capture, in a combined way, the\nstructure of space and time as well as the contextual and semantic trajectory\nproperties. Moreover, the massive amounts of available trajectory data make\ntrajectory mining and analyses very challenging. In this paper, we briefly\ndiscuss (i) an improved trajectory representation that takes into consideration\nspace-time structures, context and semantic properties of trajectories; (ii)\nnew forms of relations between the dimensions of a pair of trajectories; and\n(iii) big data approaches that can be used to develop a novel spatial-temporal\nclustering framework.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 16:29:48 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Portugal", "Ivens", ""], ["Alencar", "Paulo", ""], ["Cowan", "Donald", ""]]}, {"id": "1712.04108", "submitter": "G\\'abor Sz\\'arnyas", "authors": "G\\'abor Sz\\'arnyas", "title": "Incremental View Maintenance for Property Graph Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the challenges of incremental view maintenance for\nproperty graph queries. We select a subset of property graph queries and\npresent an approach that uses nested relational algebra to allow incremental\nevaluation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 03:02:24 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Sz\u00e1rnyas", "G\u00e1bor", ""]]}, {"id": "1712.04159", "submitter": "Niek Tax", "authors": "Niek Tax, Marlon Dumas", "title": "Mining Non-Redundant Local Process Models From Sequence Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential pattern mining techniques extract patterns corresponding to\nfrequent subsequences from a sequence database. A practical limitation of these\ntechniques is that they overload the user with too many patterns. Local Process\nModel (LPM) mining is an alternative approach coming from the field of process\nmining. While in traditional sequential pattern mining, a pattern describes one\nsubsequence, an LPM captures a set of subsequences. Also, while traditional\nsequential patterns only match subsequences that are observed in the sequence\ndatabase, an LPM may capture subsequences that are not explicitly observed, but\nthat are related to observed subsequences. In other words, LPMs generalize the\nbehavior observed in the sequence database. These properties make it possible\nfor a set of LPMs to cover the behavior of a much larger set of sequential\npatterns. Yet, existing LPM mining techniques still suffer from the pattern\nexplosion problem because they produce sets of redundant LPMs. In this paper,\nwe propose several heuristics to mine a set of non-redundant LPMs either from a\nset of redundant LPMs or from a set of sequential patterns. We empirically\ncompare the proposed heuristics between them and against existing (local)\nprocess mining techniques in terms of coverage, redundancy, and complexity of\nthe produced sets of LPMs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 08:03:50 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 06:51:54 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Tax", "Niek", ""], ["Dumas", "Marlon", ""]]}, {"id": "1712.04202", "submitter": "Adam Agocs", "authors": "Adam Agocs, Dimitrios Dardanis, Jean-Marie Le Goff and Dimitrios\n  Proios", "title": "Interactive graph query language for multidimensional data in\n  Collaboration Spotting visual analytics framework", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human reasoning in visual analytics of data networks relies mainly on the\nquality of visual perception and the capability of interactively exploring the\ndata from different facets. Visual quality strongly depends on networks' size\nand dimensional complexity while network exploration capability on the\nintuitiveness and expressiveness of user frontends. The approach taken in this\npaper aims at addressing the above by decomposing data networks into multiple\nnetworks of smaller dimensions and building an interactive graph query language\nthat supports full navigation across the sub-networks. Within sub-networks of\nreduced dimensionality, structural abstraction and semantic techniques can then\nbe used to enhance visual perception further.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 10:12:43 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Agocs", "Adam", ""], ["Dardanis", "Dimitrios", ""], ["Goff", "Jean-Marie Le", ""], ["Proios", "Dimitrios", ""]]}, {"id": "1712.04301", "submitter": "Mehdi Mohammadi", "authors": "Mehdi Mohammadi, Ala Al-Fuqaha, Sameh Sorour, and Mohsen Guizani", "title": "Deep Learning for IoT Big Data and Streaming Analytics: A Survey", "comments": "40 pages, Accepted at IEEE Communications Surveys and Tutorials\n  Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of the Internet of Things (IoT), an enormous amount of sensing\ndevices collect and/or generate various sensory data over time for a wide range\nof fields and applications. Based on the nature of the application, these\ndevices will result in big or fast/real-time data streams. Applying analytics\nover such data streams to discover new information, predict future insights,\nand make control decisions is a crucial process that makes IoT a worthy\nparadigm for businesses and a quality-of-life improving technology. In this\npaper, we provide a thorough overview on using a class of advanced machine\nlearning techniques, namely Deep Learning (DL), to facilitate the analytics and\nlearning in the IoT domain. We start by articulating IoT data characteristics\nand identifying two major treatments for IoT data from a machine learning\nperspective, namely IoT big data analytics and IoT streaming data analytics. We\nalso discuss why DL is a promising approach to achieve the desired analytics in\nthese types of data and applications. The potential of using emerging DL\ntechniques for IoT data analytics are then discussed, and its promises and\nchallenges are introduced. We present a comprehensive background on different\nDL architectures and algorithms. We also analyze and summarize major reported\nresearch attempts that leveraged DL in the IoT domain. The smart IoT devices\nthat have incorporated DL in their intelligence background are also discussed.\nDL implementation approaches on the fog and cloud centers in support of IoT\napplications are also surveyed. Finally, we shed light on some challenges and\npotential directions for future research. At the end of each section, we\nhighlight the lessons learned based on our experiments and review of the recent\nliterature.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 06:13:43 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 03:30:38 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Mohammadi", "Mehdi", ""], ["Al-Fuqaha", "Ala", ""], ["Sorour", "Sameh", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1712.05193", "submitter": "Nandan Sudarsanam", "authors": "Nandan Sudarsanam, Nishanth Kumar, Abhishek Sharma, and Balaraman\n  Ravindran", "title": "Rate of Change Analysis for Interestingness Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Association Rule Mining techniques in diverse contexts and domains\nhas resulted in the creation of numerous interestingness measures. This, in\nturn, has motivated researchers to come up with various classification schemes\nfor these measures. One popular approach to classify the objective measures is\nto assess the set of mathematical properties they satisfy in order to help\npractitioners select the right measure for a given problem. In this research,\nwe discuss the insufficiency of the existing properties in literature to\ncapture certain behaviors of interestingness measures. This motivates us to\npresent a novel approach to analyze and classify measures. We refer to this as\na rate of change analysis (RCA). In this analysis a measure is described by how\nit varies if there is a unit change in the frequency count\n$(f_{11},f_{10},f_{01},f_{00})$, for different pre-existing states of the\nfrequency counts. More formally, we look at the first partial derivative of the\nmeasure with respect to the various frequency count variables. We then use this\nanalysis to define two new properties, Unit-Null Asymptotic Invariance (UNAI)\nand Unit-Null Zero Rate (UNZR). UNAI looks at the asymptotic effect of adding\nfrequency patterns, while UNZR looks at the initial effect of adding frequency\npatterns when they do not pre-exist in the dataset. We present a comprehensive\nanalysis of 50 interestingness measures and classify them in accordance with\nthe two properties. We also present empirical studies, involving both synthetic\nand real-world datasets, which are used to cluster various measures according\nto the rule ranking patterns of the measures. The study concludes with the\nobservation that classification of measures using the empirical clusters share\nsignificant similarities to the classification of measures done through the\nproperties presented in this research.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 12:13:46 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Sudarsanam", "Nandan", ""], ["Kumar", "Nishanth", ""], ["Sharma", "Abhishek", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1712.06223", "submitter": "Dong Deng", "authors": "Dong Deng", "title": "Error-Tolerant Big Data Processing", "comments": "PhD thesis, Tsinghua University, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data contains various kinds of errors. Before analyzing data, one\nusually needs to process the raw data. However, traditional data processing\nbased on exactly match often misses lots of valid information. To get\nhigh-quality analysis results and fit in the big data era, this thesis studies\nthe error-tolerant big data processing. As most of the data in real world can\nbe represented as a sequence or a set, this thesis utilizes the widely-used\nsequence-based and set-based similar functions to tolerate errors in data\nprocessing and studies the approximate entity extraction, similarity join and\nsimilarity search problems. The main contributions of this thesis include:\n  1. This thesis proposes a unified framework to support approximate entity\nextraction with both sequence-based and set-based similarity functions\nsimultaneously. The experiments show that the unified framework can improve the\nstate-of-the-art methods by 1 to 2 orders of magnitude.\n  2. This thesis designs two methods respectively for the sequence and the set\nsimilarity joins. For the sequence similarity join, this thesis proposes to\nevenly partition the sequences to segments. It is guaranteed that two sequences\nare similar only if one sequence has a subsequence identical to a segment of\nanother sequence. For the set similarity join, this thesis proposes to\npartition all the sets into segments based on the universe. This thesis further\nextends the two partition-based methods to support the large-scale data\nprocessing framework, Map-Reduce and Spark. The partition-based method won the\nstring similarity join competition held by EDBT and beat the second place by 10\ntimes.\n  3. This thesis proposes a pivotal prefix filter technique to solve the\nsequence similarity search problem. This thesis shows that the pivotal prefix\nfilter has stronger pruning power and less filtering cost compared to the\nstate-of-the-art filters.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 01:57:39 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Deng", "Dong", ""]]}, {"id": "1712.06802", "submitter": "Han-mook Yoo", "authors": "Han-mook Yoo, Han-joon Kim, Jonghoon Chun", "title": "Estimation of Individual Micro Data from Aggregated Open Data", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method of estimating individual micro data from\naggregated open data based on semi-supervised learning and conditional\nprobability. Firstly, the proposed method collects aggregated open data and\nsupport data, which are related to the individual micro data to be estimated.\nThen, we perform the locality sensitive hashing (LSH) algorithm to find a\nsubset of the support data that is similar to the aggregated open data and then\nclassify them by using the Ensemble classification model, which is learned by\nsemi-supervised learning. Finally, we use conditional probability to estimate\nthe individual micro data by finding the most suitable record for the\nprobability distribution of the individual micro data among the classification\nresults. To evaluate the performance of the proposed method, we estimated the\nindividual building data where the fire occurred using the aggregated fire open\ndata. According to the experimental results, the micro data estimation\nperformance of the proposed method is 59.41% on average in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 06:41:36 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Yoo", "Han-mook", ""], ["Kim", "Han-joon", ""], ["Chun", "Jonghoon", ""]]}, {"id": "1712.07199", "submitter": "Rajesh Bordawekar", "authors": "Rajesh Bordawekar and Bortik Bandyopadhyay and Oded Shmueli", "title": "Cognitive Database: A Step towards Endowing Relational Databases with\n  Artificial Intelligence Capabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Cognitive Databases, an approach for transparently enabling\nArtificial Intelligence (AI) capabilities in relational databases. A novel\naspect of our design is to first view the structured data source as meaningful\nunstructured text, and then use the text to build an unsupervised neural\nnetwork model using a Natural Language Processing (NLP) technique called word\nembedding. This model captures the hidden inter-/intra-column relationships\nbetween database tokens of different types. For each database token, the model\nincludes a vector that encodes contextual semantic relationships. We seamlessly\nintegrate the word embedding model into existing SQL query infrastructure and\nuse it to enable a new class of SQL-based analytics queries called cognitive\nintelligence (CI) queries. CI queries use the model vectors to enable complex\nqueries such as semantic matching, inductive reasoning queries such as\nanalogies, predictive queries using entities not present in a database, and,\nmore generally, using knowledge from external sources. We demonstrate unique\ncapabilities of Cognitive Databases using an Apache Spark based prototype to\nexecute inductive reasoning CI queries over a multi-modal database containing\ntext and images. We believe our first-of-a-kind system exemplifies using AI\nfunctionality to endow relational databases with capabilities that were\npreviously very hard to realize in practice.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 20:49:26 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Bordawekar", "Rajesh", ""], ["Bandyopadhyay", "Bortik", ""], ["Shmueli", "Oded", ""]]}, {"id": "1712.07445", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Dan Olteanu, Dan Suciu", "title": "Boolean Tensor Decomposition for Conjunctive Queries with Negation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for answering conjunctive queries with negation,\nwhere the negated relations have bounded degree. Its data complexity matches\nthat of the best known algorithms for the positive subquery of the input query\nand is expressed in terms of the fractional hypertree width and the submodular\nwidth. The query complexity depends on the structure of the negated subquery;\nin general it is exponential in the number of join variables occurring in\nnegated relations yet it becomes polynomial for several classes of queries.\n  This algorithm relies on several contributions. We show how to rewrite\nqueries with negation on bounded-degree relations into equivalent conjunctive\nqueries with not-all-equal (NAE) predicates, which are a multi-dimensional\nanalog of disequality (not-equal). We then generalize the known color-coding\ntechnique to conjunctions of NAE predicates and explain it via a Boolean tensor\ndecomposition of conjunctions of NAE predicates. This decomposition can be\nachieved via a probabilistic construction that can be derandomized efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 12:30:59 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 20:58:56 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2019 01:59:24 GMT"}, {"version": "v4", "created": "Thu, 24 Jan 2019 20:00:06 GMT"}, {"version": "v5", "created": "Mon, 28 Jan 2019 03:17:29 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Olteanu", "Dan", ""], ["Suciu", "Dan", ""]]}, {"id": "1712.07705", "submitter": "Ester Livshits", "authors": "Ester Livshits, Benny Kimelfeld, Sudeepa Roy", "title": "Computing Optimal Repairs for Functional Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of computing an optimal repair of an\ninconsistent database, in the case where integrity constraints are Functional\nDependencies (FDs). We focus on two types of repairs: an optimal subset repair\n(optimal S-repair) that is obtained by a minimum number of tuple deletions, and\nan optimal update repair (optimal U-repair) that is obtained by a minimum\nnumber of value (cell) updates. For computing an optimal S-repair, we present a\npolynomial-time algorithm that succeeds on certain sets of FDs and fails on\nothers. We prove the following about the algorithm. When it succeeds, it can\nalso incorporate weighted tuples and duplicate tuples. When it fails, the\nproblem is NP-hard, and in fact, APX-complete (hence, cannot be approximated\nbetter than some constant). Thus, we establish a dichotomy in the complexity of\ncomputing an optimal S-repair. We present general analysis techniques for the\ncomplexity of computing an optimal U-repair, some based on the dichotomy for\nS-repairs. We also draw a connection to a past dichotomy in the complexity of\nfinding a \"most probable database\" that satisfies a set of FDs with a single\nattribute on the left hand side; the case of general FDs was left open, and we\nshow how our dichotomy provides the missing generalization and thereby settles\nthe open problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 20:56:52 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Livshits", "Ester", ""], ["Kimelfeld", "Benny", ""], ["Roy", "Sudeepa", ""]]}, {"id": "1712.07880", "submitter": "Markus Kr\\\"oll", "authors": "Nofar Carmeli and Markus Kr\\\"oll", "title": "Enumeration Complexity of Conjunctive Queries with Functional\n  Dependencies", "comments": "Full version of an article to be published in ICDT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of enumerating the answers of Conjunctive Queries\n(CQs) in the presence of Functional Dependencies (FDs). Our focus is on the\nability to list output tuples with a constant delay in between, following a\nlinear-time preprocessing. A known dichotomy classifies the acyclic\nself-join-free CQs into those that admit such enumeration, and those that do\nnot. However, this classification no longer holds in the common case where the\ndatabase exhibits dependencies among attributes. That is, some queries that are\nclassified as hard are in fact tractable if dependencies are accounted for. We\nestablish a generalization of the dichotomy to accommodate FDs; hence, our\nclassification determines which combination of a CQ and a set of FDs admits\nconstant-delay enumeration with a linear-time preprocessing.\n  In addition, we generalize a hardness result for cyclic CQs to accommodate a\ncommon type of FDs. Further conclusions of our development include a dichotomy\nfor enumeration with linear delay, and a dichotomy for CQs with disequalities.\nFinally, we show that all our results apply to the known class of \"cardinality\ndependencies\" that generalize FDs (e.g., by stating an upper bound on the\nnumber of genres per movies, or friends per person).\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 11:15:55 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Carmeli", "Nofar", ""], ["Kr\u00f6ll", "Markus", ""]]}, {"id": "1712.08198", "submitter": "Liat Peterfreund", "authors": "Liat Peterfreund, Balder ten Cate, Ronald Fagin, Benny Kimelfeld", "title": "Recursive Programs for Document Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A document spanner models a program for Information Extraction (IE) as a\nfunction that takes as input a text document (string over a finite alphabet)\nand produces a relation of spans (intervals in the document) over a predefined\nschema. A well studied language for expressing spanners is that of the regular\nspanners: relational algebra over regex formulas, which are obtained by adding\ncapture variables to regular expressions. Equivalently, the regular spanners\nare the ones expressible in non-recursive Datalog over regex formulas\n(extracting relations that play the role of EDBs from the input document). In\nthis paper, we investigate the expressive power of recursive Datalog over regex\nformulas. Our main result is that such programs capture precisely the document\nspanners computable in polynomial time. Additional results compare recursive\nprograms to known formalisms such as the language of core spanners (that\nextends regular spanners by allowing to test for string equality) and its\nclosure under difference. Finally, we extend our main result to a recently\nproposed framework that generalizes both the relational model and document\nspanners.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 20:22:47 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 07:38:23 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 05:13:34 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Peterfreund", "Liat", ""], ["Cate", "Balder ten", ""], ["Fagin", "Ronald", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "1712.08618", "submitter": "Dimitrios Sisiaridis", "authors": "Dimitrios Sisiaridis and Olivier Markowitch", "title": "Feature Extraction and Feature Selection: Reducing Data Complexity with\n  Apache Spark", "comments": null, "journal-ref": "International Journal of Network Security & Its Applications\n  (IJNSA), Vol.9, No.6, November 2017", "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature extraction and feature selection are the first tasks in\npre-processing of input logs in order to detect cyber security threats and\nattacks while utilizing machine learning. When it comes to the analysis of\nheterogeneous data derived from different sources, these tasks are found to be\ntime-consuming and difficult to be managed efficiently. In this paper, we\npresent an approach for handling feature extraction and feature selection for\nsecurity analytics of heterogeneous data derived from different network\nsensors. The approach is implemented in Apache Spark, using its python API,\nnamed pyspark.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 21:02:21 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Sisiaridis", "Dimitrios", ""], ["Markowitch", "Olivier", ""]]}, {"id": "1712.08707", "submitter": "Niel Chah", "authors": "Niel Chah", "title": "Freebase-triples: A Methodology for Processing the Freebase Data Dumps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Freebase knowledge base was a significant Semantic Web and linked data\ntechnology during its years of operations since 2007. Following its acquisition\nby Google in 2010 and its shutdown in 2016, Freebase data is contained in a\ndata dump of billions of RDF triples. In this research, an exploration of the\nFreebase data dumps will show best practices in understanding and using the\nFreebase data and also present a general methodology for parsing the linked\ndata. The analysis is done with limited computing resources and the use of\nopen-source Unix-like tools. The results showcase the efficiency of the\ntechnique and highlight redundancies in the data, with the possibility of\nrestructuring nearly 60% of the original data. As an archival dataset that has\nnot changed since 2015, Freebase's semantic structured data has applications in\nother prominent fields, such as information retrieval (IR) and knowledge-based\nquestion answering (KBQA). Freebase can also serve as a gateway to other\nstructured datasets, such as DBpedia, Wikidata, and YAGO.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 03:15:50 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Chah", "Niel", ""]]}, {"id": "1712.08809", "submitter": "Miguel Romero", "authors": "Miguel Romero", "title": "The tractability frontier of well-designed SPARQL queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of query evaluation of SPARQL queries. We focus on\nthe fundamental fragment of well-designed SPARQL restricted to the AND,\nOPTIONAL and UNION operators. Our main result is a structural characterisation\nof the classes of well-designed queries that can be evaluated in polynomial\ntime. In particular, we introduce a new notion of width called domination\nwidth, which relies on the well-known notion of treewidth. We show that, under\nsome complexity theoretic assumptions, the classes of well-designed queries\nthat can be evaluated in polynomial time are precisely those of bounded\ndomination width.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 17:23:08 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 15:28:25 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 09:56:52 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Romero", "Miguel", ""]]}, {"id": "1712.08939", "submitter": "Sebastian Skritek", "authors": "Stefan Mengel, Sebastian Skritek", "title": "On tractable query evaluation for SPARQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite much work within the last decade on foundational properties of SPARQL\n- the standard query language for RDF data - rather little is known about the\nexact limits of tractability for this language. In particular, this is the case\nfor SPARQL queries that contain the OPTIONAL-operator, even though it is one of\nthe most intensively studied features of SPARQL. The aim of our work is to\nprovide a more thorough picture of tractable classes of SPARQL queries.\n  In general, SPARQL query evaluation is PSPACE-complete in combined\ncomplexity, and it remains PSPACE-hard already for queries containing only the\nOPTIONAL-operator. To amend this situation, research has focused on\n\"well-designed SPARQL queries\" and their recent generalization \"weakly\nwell-designed SPARQL queries\". For these two fragments the evaluation problem\nis coNP-complete in the absence of projection and SigmaP2-complete otherwise.\nMoreover, they have been shown to contain most SPARQL queries asked in\npractical settings.\n  In this paper, we study tractable classes of weakly well-designed queries in\nparameterized complexity considering the equivalent formulation as pattern\ntrees. We give a complete characterization of the tractable classes in the case\nwithout projection. Moreover, we show a characterization of all tractable\nclasses of simple well-designed pattern trees in the presence of projection.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 15:40:08 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Mengel", "Stefan", ""], ["Skritek", "Sebastian", ""]]}, {"id": "1712.08971", "submitter": "El Kindi Rezig", "authors": "El Kindi Rezig, Mourad Ouzzani, Ahmed K. Elmagarmid, Walid G. Aref", "title": "Human-Centric Data Cleaning [Vision]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Cleaning refers to the process of detecting and fixing errors in the\ndata. Human involvement is instrumental at several stages of this process,\ne.g., to identify and repair errors, to validate computed repairs, etc. There\nis currently a plethora of data cleaning algorithms addressing a wide range of\ndata errors (e.g., detecting duplicates, violations of integrity constraints,\nmissing values, etc.). Many of these algorithms involve a human in the loop,\nhowever, this latter is usually coupled to the underlying cleaning algorithms.\nThere is currently no end-to-end data cleaning framework that systematically\ninvolves humans in the cleaning pipeline regardless of the underlying cleaning\nalgorithms. In this paper, we highlight key challenges that need to be\naddressed to realize such a framework. We present a design vision and discuss\nscenarios that motivate the need for such a framework to judiciously assist\nhumans in the cleaning process. Finally, we present directions to implement\nsuch a framework.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 22:28:18 GMT"}, {"version": "v2", "created": "Sat, 30 Dec 2017 17:13:20 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Rezig", "El Kindi", ""], ["Ouzzani", "Mourad", ""], ["Elmagarmid", "Ahmed K.", ""], ["Aref", "Walid G.", ""]]}, {"id": "1712.09008", "submitter": "Jianbin Huang", "authors": "Yu Zhou, Jianbin Huang, Heli Sun, Yizhou Sun", "title": "Recurrent Meta-Structure for Robust Similarity Measure in Heterogeneous\n  Information Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity measure as a fundamental task in heterogeneous information network\nanalysis has been applied to many areas, e.g., product recommendation,\nclustering and Web search. Most of the existing metrics depend on the meta-path\nor meta-structure specified by users in advance. These metrics are thus\nsensitive to the pre-specified meta-path or meta-structure. In this paper, a\nnovel similarity measure in heterogeneous information networks, called\nRecurrent Meta-Structure-based Similarity (RMSS), is proposed. The recurrent\nmeta-structure as a schematic structure in heterogeneous information networks\nprovides a unified framework to integrate all of the meta-paths and\nmeta-structures. Therefore, RMSS is robust to the meta-paths and\nmeta-structures. We devise an approach to automatically constructing the\nrecurrent meta-structure. In order to formalize the semantics, the recurrent\nmeta-structure is decomposed into several recurrent meta-paths and recurrent\nmeta-trees, and we then define the commuting matrices of the recurrent\nmeta-paths and meta-trees. All of the commuting matrices of the recurrent\nmeta-paths and meta-trees are combined according to different weights. Note\nthat the weights can be determined by two kinds of weighting strategies: local\nweighting strategy and global weighting strategy. As a result, RMSS is defined\nby virtue of the final commuting matrix. Experimental evaluations show that the\nexisting metrics are sensitive to different meta-paths or meta-structures and\nthat the proposed RMSS outperforms the existing metrics in terms of ranking and\nclustering tasks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 05:07:01 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 03:11:06 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Zhou", "Yu", ""], ["Huang", "Jianbin", ""], ["Sun", "Heli", ""], ["Sun", "Yizhou", ""]]}, {"id": "1712.09437", "submitter": "El Kindi Rezig", "authors": "El Kindi Rezig, Mourad Ouzzani, Walid G. Aref, Ahmed K. Elmagarmid,\n  Ahmed R. Mahmood", "title": "Pattern-Driven Data Cleaning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is inherently dirty and there has been a sustained effort to come up\nwith different approaches to clean it. A large class of data repair algorithms\nrely on data-quality rules and integrity constraints to detect and repair the\ndata. A well-studied class of integrity constraints is Functional Dependencies\n(FDs, for short) that specify dependencies among attributes in a relation. In\nthis paper, we address three major challenges in data repairing: (1) Accuracy:\nMost existing techniques strive to produce repairs that minimize changes to the\ndata. However, this process may produce incorrect combinations of attribute\nvalues (or patterns). In this work, we formalize the interaction of FD-induced\npatterns and select repairs that result in preserving frequent patterns found\nin the original data. This has the potential to yield a better repair quality\nboth in terms of precision and recall. (2) Interpretability of repairs: Current\ndata repair algorithms produce repairs in the form of data updates that are not\nnecessarily understandable. This makes it hard to debug repair decisions and\ntrace the chain of steps that produced them. To this end, we define a new\nformalism to declaratively express repairs that are easy for users to reason\nabout. (3) Scalability: We propose a linear-time algorithm to compute repairs\nthat outperforms state-of-the-art FD repairing algorithms by orders of\nmagnitude in repair time. Our experiments using both real-world and synthetic\ndata demonstrate that our new repair approach consistently outperforms existing\ntechniques both in terms of repair quality and scalability.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 22:06:58 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Rezig", "El Kindi", ""], ["Ouzzani", "Mourad", ""], ["Aref", "Walid G.", ""], ["Elmagarmid", "Ahmed K.", ""], ["Mahmood", "Ahmed R.", ""]]}, {"id": "1712.09624", "submitter": "Nicolas Le Scouarnec", "authors": "Nicolas Le Scouarnec", "title": "Cuckoo++ Hash Tables: High-Performance Hash Tables for Networking\n  Applications", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are an essential data-structure for numerous networking\napplications (e.g., connection tracking, firewalls, network address\ntranslators). Among these, cuckoo hash tables provide excellent performance by\nallowing lookups to be processed with very few memory accesses (2 to 3 per\nlookup). Yet, for large tables, cuckoo hash tables remain memory bound and each\nmemory access impacts performance. In this paper, we propose algorithmic\nimprovements to cuckoo hash tables allowing to eliminate some unnecessary\nmemory accesses; these changes are conducted without altering the properties of\nthe original cuckoo hash table so that all existing theoretical analysis remain\napplicable. On a single core, our hash table achieves 37M lookups per second\nfor positive lookups (i.e., when the key looked up is present in the table),\nand 60M lookups per second for negative lookups, a 50% improvement over the\nimplementation included into the DPDK. On a 18-core, with mostly positive\nlookups, our implementation achieves 496M lookups per second, a 45% improvement\nover DPDK.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 16:40:15 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Scouarnec", "Nicolas Le", ""]]}, {"id": "1712.09691", "submitter": "Yuhang Zhang", "authors": "Yuhang Zhang, Kee Siong Ng, Michael Walker, Pauline Chou, Tania\n  Churchill, Peter Christen", "title": "Scalable Entity Resolution Using Probabilistic Signatures on Parallel\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient entity resolution is an open challenge of particular\nrelevance to intelligence organisations that collect large datasets from\ndisparate sources with differing levels of quality and standard. Starting from\na first-principles formulation of entity resolution, this paper presents a\nnovel Entity Resolution algorithm that introduces a data-driven blocking and\nrecord-linkage technique based on the probabilistic identification of entity\nsignatures in data. The scalability and accuracy of the proposed algorithm are\nevaluated using benchmark datasets and shown to achieve state-of-the-art\nresults. The proposed algorithm can be implemented simply on modern parallel\ndatabases, which allows it to be deployed with relative ease in large\nindustrial applications.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 21:36:50 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 08:24:41 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 03:14:00 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Zhang", "Yuhang", ""], ["Ng", "Kee Siong", ""], ["Walker", "Michael", ""], ["Chou", "Pauline", ""], ["Churchill", "Tania", ""], ["Christen", "Peter", ""]]}, {"id": "1712.09752", "submitter": "Abolfazl Asudeh", "authors": "Abolfazl Asudeh, H. V. Jagadish, Julia Stoyanovich, Gautam Das", "title": "Designing Fair Ranking Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Items from a database are often ranked based on a combination of multiple\ncriteria. A user may have the flexibility to accept combinations that weigh\nthese criteria differently, within limits. On the other hand, this choice of\nweights can greatly affect the fairness of the produced ranking. In this paper,\nwe develop a system that helps users choose criterion weights that lead to\ngreater fairness.\n  We consider ranking functions that compute the score of each item as a\nweighted sum of (numeric) attribute values, and then sort items on their score.\nEach ranking function can be expressed as a vector of weights, or as a point in\na multi-dimensional space. For a broad range of fairness criteria, we show how\nto efficiently identify regions in this space that satisfy these criteria.\nUsing this identification method, our system is able to tell users whether\ntheir proposed ranking function satisfies the desired fairness criteria and, if\nit does not, to suggest the smallest modification that does. We develop\nuser-controllable approximation that and indexing techniques that are applied\nduring preprocessing, and support sub-second response times during the online\nphase. Our extensive experiments on real datasets demonstrate that our methods\nare able to find solutions that satisfy fairness criteria effectively and\nefficiently.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 04:56:06 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 16:35:55 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Asudeh", "Abolfazl", ""], ["Jagadish", "H. V.", ""], ["Stoyanovich", "Julia", ""], ["Das", "Gautam", ""]]}, {"id": "1712.10155", "submitter": "Jerome Darmont", "authors": "Varunya Attasena, J\\'er\\^ome Darmont (ERIC), Nouria Harbi (ERIC)", "title": "Secret Sharing for Cloud Data Security", "comments": null, "journal-ref": "The International Journal on Very Large Databases,\n  Springer-Verlag, 2017, 26 (5), pp.657-681", "doi": "10.1007/s00778-017-0470-9", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing helps reduce costs, increase business agility and deploy\nsolutions with a high return on investment for many types of applications.\nHowever, data security is of premium importance to many users and often\nrestrains their adoption of cloud technologies. Various approaches, i.e., data\nencryption, anonymization, replication and verification, help enforce different\nfacets of data security. Secret sharing is a particularly interesting\ncryptographic technique. Its most advanced variants indeed simultaneously\nenforce data privacy, availability and integrity, while allowing computation on\nencrypted data. The aim of this paper is thus to wholly survey secret sharing\nschemes with respect to data security, data access and costs in the\npay-as-you-go paradigm.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 09:10:21 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Attasena", "Varunya", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Harbi", "Nouria", "", "ERIC"]]}, {"id": "1712.10266", "submitter": "Chang Ge", "authors": "Chang Ge, Xi He, Ihab F. Ilyas, Ashwin Machanavajjhala", "title": "APEx: Accuracy-Aware Differentially Private Data Exploration", "comments": "Full version of the ACM SIGMOD 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations are increasingly interested in allowing external data\nscientists to explore their sensitive datasets. Due to the popularity of\ndifferential privacy, data owners want the data exploration to ensure provable\nprivacy guarantees. However, current systems for answering queries with\ndifferential privacy place an inordinate burden on the data analysts to\nunderstand differential privacy, manage their privacy budget, and even\nimplement new algorithms for noisy query answering. Moreover, current systems\ndo not provide any guarantees to the data analyst on the quality they care\nabout, namely accuracy of query answers.\n  We present APEx, a novel system that allows data analysts to pose adaptively\nchosen sequences of queries along with required accuracy bounds. By translating\nqueries and accuracy bounds into differentially private algorithms with the\nleast privacy loss, APEx returns query answers to the data analyst that meet\nthe accuracy bounds, and proves to the data owner that the entire data\nexploration process is differentially private. Our comprehensive experimental\nstudy on real datasets demonstrates that APEx can answer a variety of queries\naccurately with moderate to small privacy loss, and can support data\nexploration for entity resolution with high accuracy under reasonable privacy\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 16:09:37 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 09:59:36 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 00:02:42 GMT"}, {"version": "v4", "created": "Fri, 10 May 2019 20:17:35 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ge", "Chang", ""], ["He", "Xi", ""], ["Ilyas", "Ihab F.", ""], ["Machanavajjhala", "Ashwin", ""]]}]