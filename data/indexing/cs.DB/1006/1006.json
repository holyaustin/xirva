[{"id": "1006.0575", "submitter": "Benjamin Nguyen", "authors": "Bogdan Butnaru (PRISM), Benjamin Nguyen (PRISM), Georges Gardarin\n  (PRISM), Laurent Yeh (PRISM)", "title": "XQ2P: Efficient XQuery P2P Time Series Processing", "comments": null, "journal-ref": "Bases de Donn\\'ees Avanc\\'ees (D\\'emonstration), Namur : Belgium\n  (2009)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this demonstration, we propose a model for the management of XML time\nseries (TS), using the new XQuery 1.1 window operator. We argue that\ncentralized computation is slow, and demonstrate XQ2P, our prototype of\nefficient XQuery P2P TS computation in the context of financial analysis of\nlarge data sets (>1M values).\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2010 07:59:15 GMT"}], "update_date": "2010-07-26", "authors_parsed": [["Butnaru", "Bogdan", "", "PRISM"], ["Nguyen", "Benjamin", "", "PRISM"], ["Gardarin", "Georges", "", "PRISM"], ["Yeh", "Laurent", "", "PRISM"]]}, {"id": "1006.0576", "submitter": "Benjamin Nguyen", "authors": "Georges Gardarin (PRISM), Benjamin Nguyen (PRISM), Laurent Yeh\n  (PRISM), Karine Zeitouni (PRISM), Bogdan Butnaru (PRISM), Iulian Sandu-Popa\n  (PRISM)", "title": "Gestion efficace de s\\'eries temporelles en P2P: Application \\`a\n  l'analyse technique et l'\\'etude des objets mobiles", "comments": null, "journal-ref": "Bases de Donn\\'ees Avanc\\'ees, Namur : Belgium (2009)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple generic model to manage time series. A\ntime series is composed of a calendar with a typed value for each calendar\nentry. Although the model could support any kind of XML typed values, in this\npaper we focus on real numbers, which are the usual application. We define\nbasic vector space operations (plus, minus, scale), and also relational-like\nand application oriented operators to manage time series. We show the interest\nof this generic model on two applications: (i) a stock investment helper; (ii)\nan ecological transport management system. Stock investment requires\nwindow-based operations while trip management requires complex queries. The\nmodel has been implemented and tested in PHP, Java, and XQuery. We show\nbenchmark results illustrating that the computing of 5000 series of over\n100.000 entries in length - common requirements for both applications - is\ndifficult on classical centralized PCs. In order to serve a community of users\nsharing time series, we propose a P2P implementation of time series by dividing\nthem in segments and providing optimized algorithms for operator expression\ncomputation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2010 07:59:40 GMT"}], "update_date": "2010-06-04", "authors_parsed": [["Gardarin", "Georges", "", "PRISM"], ["Nguyen", "Benjamin", "", "PRISM"], ["Yeh", "Laurent", "", "PRISM"], ["Zeitouni", "Karine", "", "PRISM"], ["Butnaru", "Bogdan", "", "PRISM"], ["Sandu-Popa", "Iulian", "", "PRISM"]]}, {"id": "1006.0876", "submitter": "Secretary Aircc Journal", "authors": "Mohamed Salah Gouider and Amine Farhat, (Institut Sup\\'erieur de\n  Gestion, Tunisia)", "title": "Building a Data Warehouse for National Social Security Fund of the\n  Republic of Tunisia", "comments": "13 pages", "journal-ref": "International Journal of Database Management Systems 2.2 (2010)\n  102-114", "doi": "10.5121/ijdms.2010.2207", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The amounts of data available to decision makers are increasingly important,\ngiven the network availability, low cost storage and diversity of applications.\nTo maximize the potential of these data within the National Social Security\nFund (NSSF) in Tunisia, we have built a data warehouse as a multidimensional\ndatabase, cleaned, homogenized, historicized and consolidated. We used Oracle\nWarehouse Builder to extract, transform and load the source data into the Data\nWarehouse, by applying the KDD process. We have implemented the Data Warehouse\nas an Oracle OLAP. The knowledge extraction has been performed using the Oracle\nDiscoverer tool. This allowed users to take maximum advantage of knowledge as a\nregular report or as ad hoc queries. We started by implementing the main topic\nfor this public institution, accounting for the movements of insured persons.\nThe great success that has followed the completion of this work has encouraged\nthe NSSF to complete the achievement of other topics of interest within the\nNSSF. We suggest in the near future to use Multidimensional Data Mining to\nextract hidden knowledge and that are not predictable by the OLAP.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2010 12:03:32 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Gouider", "Mohamed Salah", ""], ["Farhat", "Amine", ""]]}, {"id": "1006.1309", "submitter": "Sugata Sanyal", "authors": "S.M. Joshi, S. Sanyal, S. Banerjee, S. Srikumar", "title": "Using Grid Files for a Relational Database Management System", "comments": "26 Pages, 5 Figures, 2 tables, This Paper was referred to in the\n  seminal Paper by J. Nievergelt, H. Hinterberger,K.C. Sevcik, The Grid File:\n  An Adaptable, Symmetric Multikey File Structure ACM Transactions on Database\n  Systems (TODS), Volume 9, Issue 1, March, 1984. Pages: 38-71, ISSN:\n  0362-5915, as [Reference 12]", "journal-ref": null, "doi": null, "report-no": "Technical Report No. 11 Technical Report No. 11 Technical Report No\n  11, TIFR, INDIA", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our experience with using Grid files as the main storage\norganization for a relational database management system. We primarily focus on\nthe following two aspects. (i) Strategies for implementing grid files\nefficiently. (ii) Methods for efficiency evaluating queries posed to a database\norganized using grid files.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 17:47:30 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Joshi", "S. M.", ""], ["Sanyal", "S.", ""], ["Banerjee", "S.", ""], ["Srikumar", "S.", ""]]}, {"id": "1006.1429", "submitter": "EPTCS", "authors": "James Cheney (University of Edinburgh)", "title": "Causality and the Semantics of Provenance", "comments": null, "journal-ref": "EPTCS 26, 2010, pp. 63-74", "doi": "10.4204/EPTCS.26.6", "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance, or information about the sources, derivation, custody or history\nof data, has been studied recently in a number of contexts, including\ndatabases, scientific workflows and the Semantic Web. Many provenance\nmechanisms have been developed, motivated by informal notions such as\ninfluence, dependence, explanation and causality. However, there has been\nlittle study of whether these mechanisms formally satisfy appropriate policies\nor even how to formalize relevant motivating concepts such as causality. We\ncontend that mathematical models of these concepts are needed to justify and\ncompare provenance techniques. In this paper we review a theory of causality\nbased on structural models that has been developed in artificial intelligence,\nand describe work in progress on using causality to give a semantics to\nprovenance graphs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 01:16:29 GMT"}], "update_date": "2010-06-09", "authors_parsed": [["Cheney", "James", "", "University of Edinburgh"]]}, {"id": "1006.1663", "submitter": "Harco Leslie Hendric Spits Warnars", "authors": "Spits Warnars", "title": "Tata Kelola Database Perguruan Tinggi Yang Optimal Dengan Data Warehouse", "comments": "10 pages, 12 figures and 4 tables", "journal-ref": "TELKOMNIKA Vol. 8, No. 1, April 2010, pp. 25 - 34", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of new higher education institutions has created the\ncompetition in higher education market, and data warehouse can be used as an\neffective technology tools for increasing competitiveness in the higher\neducation market. Data warehouse produce reliable reports for the institution's\nhigh-level management in short time for faster and better decision making, not\nonly on increasing the admission number of students, but also on the\npossibility to find extraordinary, unconventional funds for the institution.\nEfficiency comparison was based on length and amount of processed records,\ntotal processed byte, amount of processed tables, time to run query and\nproduced record on OLTP database and data warehouse. Efficiency percentages was\nmeasured by the formula for percentage increasing and the average efficiency\npercentage of 461.801,04% shows that using data warehouse is more powerful and\nefficient rather than using OLTP database. Data warehouse was modeled based on\nhypercube which is created by limited high demand reports which usually used by\nhigh level management. In every table of fact and dimension fields will be\ninserted which represent the loading constructive merge where the ETL\n(Extraction, Transformation and Loading) process is run based on the old and\nnew files.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 20:57:02 GMT"}], "update_date": "2010-06-10", "authors_parsed": [["Warnars", "Spits", ""]]}, {"id": "1006.1692", "submitter": "Harco Leslie Hendric Spits Warnars", "authors": "Spits Warnars", "title": "Measuring interesting rules in Characteristic rule", "comments": "5 pages, 5 figures, 12 tables", "journal-ref": "2nd International Conference on Soft Computing, Intelligent System\n  and Information Technology (ICSIIT), Bali, Indonesia, 1-2 July 2010", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding interesting rule in the sixth strategy step about threshold control\non generalized relations in attribute oriented induction, there is possibility\nto select candidate attribute for further generalization and merging of\nidentical tuples until the number of tuples is no greater than the threshold\nvalue, as implemented in basic attribute oriented induction algorithm. At this\nstrategy step there is possibility the number of tuples in final generalization\nresult still greater than threshold value. In order to get the final\ngeneralization result which only small number of tuples and can be easy to\ntransfer into simple logical formula, the seventh strategy step about rule\ntransformation is evolved where there will be simplification by unioning or\ngrouping the identical attribute. Our approach to measure interesting rule is\nopposite with heuristic measurement approach by Fudger and Hamilton where the\nmore complex concept hierarchies, more interesting results are likely to be\nfound, but our approach the simpler concept hierarchies, more interesting\nresults are likely to be found and the more complex concept hierarchies, more\ncomplex process generalization in concept tree. The decision to find\ninteresting rule is influenced with wide or length and depth or level of\nconcept tree.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 02:59:26 GMT"}], "update_date": "2010-06-10", "authors_parsed": [["Warnars", "Spits", ""]]}, {"id": "1006.1695", "submitter": "Harco Leslie Hendric Spits Warnars", "authors": "Spits Warnars", "title": "Attribute Oriented Induction with simple select SQL statement", "comments": "17 pages, 20 tables, 4 figures", "journal-ref": "1st International Conference on Computation for Science and\n  Technology (ICCST-I), Chiang Mai, Thailand, 4-6 August 2010", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching learning or rules in relational database for data mining purposes\nwith characteristic or classification/discriminant rule in attribute oriented\ninduction technique can be quicker, easy, and simple with simple SQL statement.\nWith just only one simple SQL statement, characteristic and classification rule\ncan be created simultaneously. Collaboration SQL statement with any other\napplication software will increase the ability for creating t-weight as\nmeasurement the typicality of each record in the characteristic rule and\nd-weight as measurement the discriminating behavior of the learned\nclassification/discriminant rule, particularly for further generalization in\ncharacteristic rule. Handling concept hierarchy into tables based on concept\ntree will influence for the successful simple SQL statement and by knowing the\nright standard knowledge to transform each of concept tree in concept hierarchy\ninto one table as transforming concept hierarchy into table, the simple SQL\nstatement can be run properly.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 03:19:36 GMT"}], "update_date": "2010-06-10", "authors_parsed": [["Warnars", "Spits", ""]]}, {"id": "1006.1699", "submitter": "Harco Leslie Hendric Spits Warnars", "authors": "Spits Warnars", "title": "Multidimensional Datawarehouse with Combination Formula", "comments": "7 pages, 12 figures", "journal-ref": "The 2nd International Conference on Information and Communication\n  Technology and Systems (ICTS), Informatics Department, Faculty of Information\n  Technology, Institute of Technology Sepuluh Nopember (ITS), Surabaya,\n  Indonesia, 29 August 2006", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional in data warehouse is a compulsion and become the most\nimportant for information delivery, without multidimensional Multidimensional\nin data warehouse is a compulsion and become the most important for information\ndelivery, without multidimensional datawarehouse is incomplete.\nMultidimensional give ability to analyze business measurement in many different\nways. Multidimensional is also synonymous with online analytical processing\n(OLAP). By using some concepts in datawarehouse like slice-dice,drill down and\nroll up will increase the ability of multidimensional datawarehouse. The\nresearch question and the discussing for this paper are how much deepest the\nmultidimensional ability from each fact table in datawarehouse. By using the\nstatistic combination formula we try to explore the combination that can be\nyielded from each dimension in hypercubes, the entire of dimensi combination,\nminimum combination and maximum combination.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 03:43:16 GMT"}], "update_date": "2010-06-10", "authors_parsed": [["Warnars", "Spits", ""]]}, {"id": "1006.2077", "submitter": "Harco Leslie Hendric Spits Warnars", "authors": "H.L.H Spits Warnars", "title": "Multidimensi Pada Data Warehouse Dengan Menggunakan Rumus Kombinasi", "comments": "6 pages", "journal-ref": "The 2nd National Seminar Information Technology Application\n  (SNATI) 2006, University of Islam Indonesia, pp. J1-J6, Yogyakarta, 17 June\n  2006", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional in data warehouse is a compulsion and become the most\nimportant for information delivery, without multidimensional data warehouse is\nincomplete. Multidimensional give the able to analyze business measurement in\nmany different ways. Multidimensional is also synonymous with online analytical\nprocessing (OLAP).\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2010 16:16:37 GMT"}], "update_date": "2010-06-11", "authors_parsed": [["Warnars", "H. L. H Spits", ""]]}, {"id": "1006.2088", "submitter": "Harco Leslie Hendric Spits Warnars", "authors": "Spits Warnars H.L.H", "title": "Classification rule with simple select SQL statement", "comments": "6 pages", "journal-ref": "National seminar University of Budi Luhur 2010, University of Budi\n  Luhur, Jakarta, 5 August 2010", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple sql statement can be used to search learning or rule in relational\ndatabase for data mining purposes particularly for classification rule. With\njust only one simple sql statement, characteristic and classification rule can\nbe created simultaneously. Collaboration sql statement with any other\napplication software will increase the ability for creating t-weight as\nmeasurement the typicality of each record in the characteristic rule and\nd-weight as measurement the discriminating behavior of the learned\nclassification/discriminant rule, specifically for further generalization in\ncharacteristic rule. Handling concept hierarchy into tables based on concept\ntree will influence for the successful simple sql statement and by knowing the\nright standard knowledge to transform each of concept tree in concept hierarchy\ninto one table as to transform concept hierarchy into table, the simple sql\nstatement can be run properly.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2010 17:20:14 GMT"}], "update_date": "2010-06-11", "authors_parsed": [["H", "Spits Warnars H. L.", ""]]}, {"id": "1006.2880", "submitter": "Bahman Bahmani", "authors": "Bahman Bahmani, Abdur Chowdhury, Ashish Goel", "title": "Fast Incremental and Personalized PageRank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the efficiency of Monte Carlo methods for\nincremental computation of PageRank, personalized PageRank, and similar random\nwalk based methods (with focus on SALSA), on large-scale dynamically evolving\nsocial networks. We assume that the graph of friendships is stored in\ndistributed shared memory, as is the case for large social networks such as\nTwitter.\n  For global PageRank, we assume that the social network has $n$ nodes, and $m$\nadversarially chosen edges arrive in a random order. We show that with a reset\nprobability of $\\epsilon$, the total work needed to maintain an accurate\nestimate (using the Monte Carlo method) of the PageRank of every node at all\ntimes is $O(\\frac{n\\ln m}{\\epsilon^{2}})$. This is significantly better than\nall known bounds for incremental PageRank. For instance, if we naively\nrecompute the PageRanks as each edge arrives, the simple power iteration method\nneeds $\\Omega(\\frac{m^2}{\\ln(1/(1-\\epsilon))})$ total time and the Monte Carlo\nmethod needs $O(mn/\\epsilon)$ total time; both are prohibitively expensive.\nFurthermore, we also show that we can handle deletions equally efficiently.\n  We then study the computation of the top $k$ personalized PageRanks starting\nfrom a seed node, assuming that personalized PageRanks follow a power-law with\nexponent $\\alpha < 1$. We show that if we store $R>q\\ln n$ random walks\nstarting from every node for large enough constant $q$ (using the approach\noutlined for global PageRank), then the expected number of calls made to the\ndistributed social network database is $O(k/(R^{(1-\\alpha)/\\alpha}))$.\n  We also present experimental results from the social networking site,\nTwitter, verifying our assumptions and analyses. The overall result is that\nthis algorithm is fast enough for real-time queries over a dynamic social\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 03:37:51 GMT"}, {"version": "v2", "created": "Tue, 31 Aug 2010 14:17:30 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Bahmani", "Bahman", ""], ["Chowdhury", "Abdur", ""], ["Goel", "Ashish", ""]]}, {"id": "1006.3180", "submitter": "Graham Kirby", "authors": "Angus Macdonald, Alan Dearle, Graham Kirby", "title": "H2O: An Autonomic, Resource-Aware Distributed Database System", "comments": "Presented at SICSA PhD Conference 2010 (http://www.sicsaconf.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design of an autonomic, resource-aware distributed\ndatabase which enables data to be backed up and shared without complex manual\nadministration. The database, H2O, is designed to make use of unused resources\non workstation machines. Creating and maintaining highly-available, replicated\ndatabase systems can be difficult for untrained users, and costly for IT\ndepartments. H2O reduces the need for manual administration by autonomically\nreplicating data and load-balancing across machines in an enterprise.\nProvisioning hardware to run a database system can be unnecessarily costly as\nmost organizations already possess large quantities of idle resources in\nworkstation machines. H2O is designed to utilize this unused capacity by using\nresource availability information to place data and plan queries over\nworkstation machines that are already being used for other tasks. This paper\ndiscusses the requirements for such a system and presents the design and\nimplementation of H2O.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2010 10:46:04 GMT"}], "update_date": "2010-06-17", "authors_parsed": [["Macdonald", "Angus", ""], ["Dearle", "Alan", ""], ["Kirby", "Graham", ""]]}, {"id": "1006.3424", "submitter": "Marco Aldinucci", "authors": "Marco aldinucci, Salvatore Ruggieri, Massimo Torquati", "title": "Porting Decision Tree Algorithms to Multicore using FastFlow", "comments": "18 pages + cover", "journal-ref": "In J. L. Balc\\'azar, F. Bonchi, A. Gionis, and M. Sebag, editors,\n  Proc. of European Conference in Machine Learning and Knowledge Discovery in\n  Databases (ECML PKDD), volume 6321 of LNCS, pages 7-23, Barcelona, Spain,\n  Sept. 2010. Springer", "doi": "10.1007/978-3-642-15880-3_7", "report-no": "TR-10-11", "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The whole computer hardware industry embraced multicores. For these machines,\nthe extreme optimisation of sequential algorithms is no longer sufficient to\nsqueeze the real machine power, which can be only exploited via thread-level\nparallelism. Decision tree algorithms exhibit natural concurrency that makes\nthem suitable to be parallelised. This paper presents an approach for\neasy-yet-efficient porting of an implementation of the C4.5 algorithm on\nmulticores. The parallel porting requires minimal changes to the original\nsequential code, and it is able to exploit up to 7X speedup on an Intel\ndual-quad core machine.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 10:37:43 GMT"}, {"version": "v2", "created": "Sun, 19 Sep 2010 13:51:56 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["aldinucci", "Marco", ""], ["Ruggieri", "Salvatore", ""], ["Torquati", "Massimo", ""]]}, {"id": "1006.3448", "submitter": "Graham Kirby", "authors": "Alan Dearle, Graham Kirby, Ron Morrison", "title": "Orthogonal Persistence Revisited", "comments": "2nd International Conference on Object Databases (ICOODB 2009),\n  Zurich, Switzerland. pp. 1-22", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social and economic importance of large bodies of programs and data that\nare potentially long-lived has attracted much attention in the commercial and\nresearch communities. Here we concentrate on a set of methodologies and\ntechnologies called persistent programming. In particular we review programming\nlanguage support for the concept of orthogonal persistence, a technique for the\nuniform treatment of objects irrespective of their types or longevity. While\nresearch in persistent programming has become unfashionable, we show how the\nconcept is beginning to appear as a major component of modern systems. We\nrelate these attempts to the original principles of orthogonal persistence and\ngive a few hints about how the concept may be utilised in the future.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 12:35:55 GMT"}], "update_date": "2010-06-18", "authors_parsed": [["Dearle", "Alan", ""], ["Kirby", "Graham", ""], ["Morrison", "Ron", ""]]}, {"id": "1006.3514", "submitter": "Rajendra Shinde", "authors": "Rajendra Shinde, Ashish Goel, Pankaj Gupta, Debojyoti Dutta", "title": "Similarity Search and Locality Sensitive Hashing using TCAMs", "comments": "11 pages, in SIGMOD 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search methods are widely used as kernels in various machine\nlearning applications. Nearest neighbor search (NNS) algorithms are often used\nto retrieve similar entries, given a query. While there exist efficient\ntechniques for exact query lookup using hashing, similarity search using exact\nnearest neighbors is known to be a hard problem and in high dimensions, best\nknown solutions offer little improvement over a linear scan. Fast solutions to\nthe approximate NNS problem include Locality Sensitive Hashing (LSH) based\ntechniques, which need storage polynomial in $n$ with exponent greater than\n$1$, and query time sublinear, but still polynomial in $n$, where $n$ is the\nsize of the database. In this work we present a new technique of solving the\napproximate NNS problem in Euclidean space using a Ternary Content Addressable\nMemory (TCAM), which needs near linear space and has O(1) query time. In fact,\nthis method also works around the best known lower bounds in the cell probe\nmodel for the query time using a data structure near linear in the size of the\ndata base. TCAMs are high performance associative memories widely used in\nnetworking applications such as access control lists. A TCAM can query for a\nbit vector within a database of ternary vectors, where every bit position\nrepresents $0$, $1$ or $*$. The $*$ is a wild card representing either a $0$ or\na $1$. We leverage TCAMs to design a variant of LSH, called Ternary Locality\nSensitive Hashing (TLSH) wherein we hash database entries represented by\nvectors in the Euclidean space into $\\{0,1,*\\}$. By using the added\nfunctionality of a TLSH scheme with respect to the $*$ character, we solve an\ninstance of the approximate nearest neighbor problem with 1 TCAM access and\nstorage nearly linear in the size of the database. We believe that this work\ncan open new avenues in very high speed data mining.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 16:51:51 GMT"}], "update_date": "2010-06-18", "authors_parsed": [["Shinde", "Rajendra", ""], ["Goel", "Ashish", ""], ["Gupta", "Pankaj", ""], ["Dutta", "Debojyoti", ""]]}, {"id": "1006.3726", "submitter": "Daniel Lemire", "authors": "Hazel Webb, Daniel Lemire and Owen Kaser", "title": "Diamond Dicing", "comments": "29 pages", "journal-ref": "Data & Knowledge Engineering, Volume 86, July 2013, Pages 1-18", "doi": "10.1016/j.datak.2013.01.001", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In OLAP, analysts often select an interesting sample of the data. For\nexample, an analyst might focus on products bringing revenues of at least 100\n000 dollars, or on shops having sales greater than 400 000 dollars. However,\ncurrent systems do not allow the application of both of these thresholds\nsimultaneously, selecting products and shops satisfying both thresholds. For\nsuch purposes, we introduce the diamond cube operator, filling a gap among\nexisting data warehouse operations.\n  Because of the interaction between dimensions the computation of diamond\ncubes is challenging. We compare and test various algorithms on large data sets\nof more than 100 million facts. We find that while it is possible to implement\ndiamonds in SQL, it is inefficient. Indeed, our custom implementation can be a\nhundred times faster than popular database engines (including a row-store and a\ncolumn-store).\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 15:38:04 GMT"}, {"version": "v2", "created": "Mon, 30 May 2011 16:48:25 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2013 15:30:06 GMT"}, {"version": "v4", "created": "Wed, 23 Jul 2014 19:18:35 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Webb", "Hazel", ""], ["Lemire", "Daniel", ""], ["Kaser", "Owen", ""]]}, {"id": "1006.4173", "submitter": "Rasmus Pagh", "authors": "Rasmus Resen Amossen and Andrea Campagna and Rasmus Pagh", "title": "Better size estimation for sparse matrix products", "comments": "Corrected a number of mistakes and typos in the first version (also\n  present in the version published at RANDOM 2010). Most importantly, the lower\n  bound on the error epsilon is now a function of z rather than n", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of doing fast and reliable estimation of the number\nof non-zero entries in a sparse boolean matrix product. This problem has\napplications in databases and computer algebra. Let n denote the total number\nof non-zero entries in the input matrices. We show how to compute a 1 +-\nepsilon approximation (with small probability of error) in expected time O(n)\nfor any epsilon > 4/\\sqrt[4]{z}. The previously best estimation algorithm, due\nto Cohen (JCSS 1997), uses time O(n/epsilon^2). We also present a variant using\nO(sort(n)) I/Os in expectation in the cache-oblivious model. In contrast to\nthese results, the currently best algorithms for computing a sparse boolean\nmatrix product use time omega(n^{4/3}) (resp. omega(n^{4/3}/B) I/Os), even if\nthe result matrix has only z=O(n) nonzero entries. Our algorithm combines the\nsize estimation technique of Bar-Yossef et al. (RANDOM 2002) with a particular\nclass of pairwise independent hash functions that allows the sketch of a set of\nthe form A x C to be computed in expected time O(|A|+|C|) and O(sort(|A|+|C|))\nI/Os. We then describe how sampling can be used to maintain (independent)\nsketches of matrices that allow estimation to be performed in time o(n) if z is\nsufficiently large. This gives a simpler alternative to the sketching technique\nof Ganguly et al. (PODS 2005), and matches a space lower bound shown in that\npaper. Finally, we present experiments on real-world data sets that show the\naccuracy of both our methods to be significantly better than the worst-case\nanalysis predicts.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2010 20:47:46 GMT"}, {"version": "v2", "created": "Tue, 22 Feb 2011 19:26:15 GMT"}], "update_date": "2011-02-23", "authors_parsed": [["Amossen", "Rasmus Resen", ""], ["Campagna", "Andrea", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1006.4833", "submitter": "Graham Kirby", "authors": "Graham Kirby, Evangelos Zirintsis, Alan Dearle, Ron Morrison", "title": "A Generic Storage API", "comments": "Submitted to ACSC 2004", "journal-ref": null, "doi": null, "report-no": "University of St Andrews CS/03/2", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic API suitable for provision of highly generic storage\nfacilities that can be tailored to produce various individually customised\nstorage infrastructures. The paper identifies a candidate set of minimal\nstorage system building blocks, which are sufficiently simple to avoid\nencapsulating policy where it cannot be customised by applications, and\ncomposable to build highly flexible storage architectures. Four main generic\ncomponents are defined: the store, the namer, the caster and the interpreter.\nIt is hypothesised that these are sufficiently general that they could act as\nbuilding blocks for any information storage and retrieval system. The essential\ncharacteristics of each are defined by an interface, which may be implemented\nby multiple implementing classes.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2010 16:47:28 GMT"}], "update_date": "2010-06-25", "authors_parsed": [["Kirby", "Graham", ""], ["Zirintsis", "Evangelos", ""], ["Dearle", "Alan", ""], ["Morrison", "Ron", ""]]}, {"id": "1006.5261", "submitter": "Madjid Khalilian", "authors": "Madjid Khalilian, Norwati Mustapha", "title": "Data Stream Clustering: Challenges and Issues", "comments": "IMECS2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large databases are required to store massive amounts of data that are\ncontinuously inserted and queried. Analyzing huge data sets and extracting\nvaluable pattern in many applications are interesting for researchers. We can\nidentify two main groups of techniques for huge data bases mining. One group\nrefers to streaming data and applies mining techniques whereas second group\nattempts to solve this problem directly with efficient algorithms. Recently\nmany researchers have focused on data stream as an efficient strategy against\nhuge data base mining instead of mining on entire data base. The main problem\nin data stream mining means evolving data is more difficult to detect in this\ntechniques therefore unsupervised methods should be applied. However,\nclustering techniques can lead us to discover hidden information. In this\nsurvey, we try to clarify: first, the different problem definitions related to\ndata stream clustering in general; second, the specific difficulties\nencountered in this field of research; third, the varying assumptions,\nheuristics, and intuitions forming the basis of different approaches; and how\nseveral prominent solutions tackle different problems. Index Terms- Data\nStream, Clustering, K-Means, Concept drift\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2010 04:02:17 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Khalilian", "Madjid", ""], ["Mustapha", "Norwati", ""]]}, {"id": "1006.5273", "submitter": "Bum-Soo Kim", "authors": "Myeong-Seon Gil, Yang-Sae Moon, and Bum-Soo Kim", "title": "Linear Detrending Subsequence Matching in Time-Series Databases", "comments": "12 pages", "journal-ref": null, "doi": "10.1587/transinf.E94.D.917", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each time-series has its own linear trend, the directionality of a\ntimeseries, and removing the linear trend is crucial to get the more intuitive\nmatching results. Supporting the linear detrending in subsequence matching is a\nchallenging problem due to a huge number of possible subsequences. In this\npaper we define this problem the linear detrending subsequence matching and\npropose its efficient index-based solution. To this end, we first present a\nnotion of LD-windows (LD means linear detrending), which is obtained as\nfollows: we eliminate the linear trend from a subsequence rather than each\nwindow itself and obtain LD-windows by dividing the subsequence into windows.\nUsing the LD-windows we then present a lower bounding theorem for the\nindex-based matching solution and formally prove its correctness. Based on the\nlower bounding theorem, we next propose the index building and subsequence\nmatching algorithms for linear detrending subsequence matching.We finally show\nthe superiority of our index-based solution through extensive experiments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2010 06:37:32 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Gil", "Myeong-Seon", ""], ["Moon", "Yang-Sae", ""], ["Kim", "Bum-Soo", ""]]}, {"id": "1006.5794", "submitter": "Graham Kirby", "authors": "Evangelos Zirintsis, Graham Kirby, Alan Dearle, Ron Morrison", "title": "Report on the XBase Project", "comments": null, "journal-ref": null, "doi": null, "report-no": "University of St Andrews CS/03/1", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project addressed the conceptual fundamentals of data storage,\ninvestigating techniques for provision of highly generic storage facilities\nthat can be tailored to produce various individually customised storage\ninfrastructures, compliant to the needs of particular applications. This\nrequires the separation of mechanism and policy wherever possible. Aspirations\ninclude: actors, whether users or individual processes, should be able to bind\nto, update and manipulate data and programs transparently with respect to their\nrespective locations; programs should be expressed independently of the storage\nand network technology involved in their execution; storage facilities should\nbe structure-neutral so that actors can impose multiple interpretations over\ninformation, simultaneously and safely; information should not be discarded so\nthat arbitrary historical views are supported; raw stored information should be\nopen to all; where security restrictions on its use are required this should be\nachieved using cryptographic techniques. The key advances of the research were:\n1) the identification of a candidate set of minimal storage system building\nblocks, which are sufficiently simple to avoid encapsulating policy where it\ncannot be customised by applications, and composable to build highly flexible\nstorage architectures 2) insight into the nature of append-only storage\ncomponents, and the issues arising from their application to common storage\nuse-cases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 07:57:10 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Zirintsis", "Evangelos", ""], ["Kirby", "Graham", ""], ["Dearle", "Alan", ""], ["Morrison", "Ron", ""]]}]