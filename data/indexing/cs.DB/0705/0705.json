[{"id": "0705.0281", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (LIMOS), Christophe Fromantin (LIMOS), St\\'ephane\n  R\\'egnier (LIMOS), Le Gruenwald, Michel Schneider (LIMOS)", "title": "Dynamic Clustering in Object-Oriented Databases: An Advocacy for\n  Simplicity", "comments": null, "journal-ref": "LNCS, Vol. 1944 (06/2000) 71-85", "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  We present in this paper three dynamic clustering techniques for\nObject-Oriented Databases (OODBs). The first two, Dynamic, Statistical &\nTunable Clustering (DSTC) and StatClust, exploit both comprehensive usage\nstatistics and the inter-object reference graph. They are quite elaborate.\nHowever, they are also complex to implement and induce a high overhead. The\nthird clustering technique, called Detection & Reclustering of Objects (DRO),\nis based on the same principles, but is much simpler to implement. These three\nclustering algorithm have been implemented in the Texas persistent object store\nand compared in terms of clustering efficiency (i.e., overall performance\nincrease) and overhead using the Object Clustering Benchmark (OCB). The results\nobtained showed that DRO induced a lighter overhead while still achieving\nbetter overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2007 12:50:39 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "LIMOS"], ["Fromantin", "Christophe", "", "LIMOS"], ["R\u00e9gnier", "St\u00e9phane", "", "LIMOS"], ["Gruenwald", "Le", "", "LIMOS"], ["Schneider", "Michel", "", "LIMOS"]]}, {"id": "0705.0450", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (LIMOS), Michel Schneider (LIMOS)", "title": "VOODB: A Generic Discrete-Event Random Simulation Model to Evaluate the\n  Performances of OODBs", "comments": null, "journal-ref": "25th International Conference on Very Large Databases (VLDB 99)\n  (09/1999) 254-265", "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  Performance of object-oriented database systems (OODBs) is still an issue to\nboth designers and users nowadays. The aim of this paper is to propose a\ngeneric discrete-event random simulation model, called VOODB, in order to\nevaluate the performances of OODBs in general, and the performances of\noptimization methods like clustering in particular. Such optimization methods\nundoubtedly improve the performances of OODBs. Yet, they also always induce\nsome kind of overhead for the system. Therefore, it is important to evaluate\ntheir exact impact on the overall performances. VOODB has been designed as a\ngeneric discrete-event random simulation model by putting to use a modelling\napproach, and has been validated by simulating the behavior of the O2 OODB and\nthe Texas persistent object store. Since our final objective is to compare\nobject clustering algorithms, some experiments have also been conducted on the\nDSTC clustering technique, which is implemented in Texas. To validate VOODB,\nperformance results obtained by simulation for a given experiment have been\ncompared to the results obtained by benchmarking the real systems in the same\nconditions. Benchmarking and simulation performance evaluations have been\nobserved to be consistent, so it appears that simulation can be a reliable\napproach to evaluate the performances of OODBs.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2007 12:50:04 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "LIMOS"], ["Schneider", "Michel", "", "LIMOS"]]}, {"id": "0705.0453", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (LIMOS), Bertrand Petit (LIMOS), Michel Schneider\n  (LIMOS)", "title": "OCB: A Generic Benchmark to Evaluate the Performances of Object-Oriented\n  Database Systems", "comments": null, "journal-ref": "LNCS, Vol. 1377 (03/1998) 326-340", "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  We present in this paper a generic object-oriented benchmark (the Object\nClustering Benchmark) that has been designed to evaluate the performances of\nclustering policies in object-oriented databases. OCB is generic because its\nsample database may be customized to fit the databases introduced by the main\nexisting benchmarks (e.g., OO1). OCB's current form is clustering-oriented\nbecause of its clustering-oriented workload, but it can be easily adapted to\nother purposes. Lastly, OCB's code is compact and easily portable. OCB has been\nimplemented in a real system (Texas, running on a Sun workstation), in order to\ntest a specific clustering policy called DSTC. A few results concerning this\ntest are presented.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2007 12:54:30 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "LIMOS"], ["Petit", "Bertrand", "", "LIMOS"], ["Schneider", "Michel", "", "LIMOS"]]}, {"id": "0705.0454", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (LIMOS), Amar Attoui (LIMOS), Michel Gourgand\n  (LIMOS)", "title": "Performance Evaluation for Clustering Algorithms in Object-Oriented\n  Database Systems", "comments": null, "journal-ref": "LNCS, Vol. 978 (09/1995) 187-196", "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  It is widely acknowledged that good object clustering is critical to the\nperformance of object-oriented databases. However, object clustering always\ninvolves some kind of overhead for the system. The aim of this paper is to\npropose a modelling methodology in order to evaluate the performances of\ndifferent clustering policies. This methodology has been used to compare the\nperformances of three clustering algorithms found in the literature (Cactis, CK\nand ORION) that we considered representative of the current research in the\nfield of object clustering. The actual performance evaluation was performed\nusing simulation. Simulation experiments we performed showed that the Cactis\nalgorithm is better than the ORION algorithm and that the CK algorithm totally\noutperforms both other algorithms in terms of response time and clustering\noverhead.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2007 13:02:06 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "LIMOS"], ["Attoui", "Amar", "", "LIMOS"], ["Gourgand", "Michel", "", "LIMOS"]]}, {"id": "0705.1110", "submitter": "Edgar Graaf de", "authors": "Edgar de Graaf Joost Kok Walter Kosters", "title": "Mining Patterns with a Balanced Interval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": null, "abstract": "  In many applications it will be useful to know those patterns that occur with\na balanced interval, e.g., a certain combination of phone numbers are called\nalmost every Friday or a group of products are sold a lot on Tuesday and\nThursday.\n  In previous work we proposed a new measure of support (the number of\noccurrences of a pattern in a dataset), where we count the number of times a\npattern occurs (nearly) in the middle between two other occurrences. If the\nnumber of non-occurrences between two occurrences of a pattern stays almost the\nsame then we call the pattern balanced.\n  It was noticed that some very frequent patterns obviously also occur with a\nbalanced interval, meaning in every transaction. However more interesting\npatterns might occur, e.g., every three transactions. Here we discuss a\nsolution using standard deviation and average. Furthermore we propose a simpler\napproach for pruning patterns with a balanced interval, making estimating the\npruning threshold more intuitive.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2007 15:22:38 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Kosters", "Edgar de Graaf Joost Kok Walter", ""]]}, {"id": "0705.1453", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (ERIC), Fadila Bentayeb (ERIC), Omar Boussa\\\"id\n  (ERIC)", "title": "DWEB: A Data Warehouse Engineering Benchmark", "comments": null, "journal-ref": "LNCS, Vol. 3589 (08/2005) 85-94", "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  Data warehouse architectural choices and optimization techniques are critical\nto decision support query performance. To facilitate these choices, the\nperformance of the designed data warehouse must be assessed. This is usually\ndone with the help of benchmarks, which can either help system users comparing\nthe performances of different systems, or help system engineers testing the\neffect of various design choices. While the TPC standard decision support\nbenchmarks address the first point, they are not tuneable enough to address the\nsecond one and fail to model different data warehouse schemas. By contrast, our\nData Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc\nsynthetic data warehouses and workloads. DWEB is fully parameterized to fulfill\ndata warehouse design needs. However, two levels of parameterization keep it\nrelatively easy to tune. Finally, DWEB is implemented as a Java free software\nthat can be interfaced with most existing relational database management\nsystems. A sample usage of DWEB is also provided in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2007 12:23:35 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Bentayeb", "Fadila", "", "ERIC"], ["Boussa\u00efd", "Omar", "", "ERIC"]]}, {"id": "0705.1454", "submitter": "Jerome Darmont", "authors": "Zhen He, J\\'er\\^ome Darmont (ERIC)", "title": "DOEF: A Dynamic Object Evaluation Framework", "comments": null, "journal-ref": "LNCS, Vol. 2736 (09/2003) 662-671", "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  In object-oriented or object-relational databases such as multimedia\ndatabases or most XML databases, access patterns are not static, i.e.,\napplications do not always access the same objects in the same order\nrepeatedly. However, this has been the way these databases and associated\noptimisation techniques like clustering have been evaluated up to now. This\npaper opens up research regarding this issue by proposing a dynamic object\nevaluation framework (DOEF) that accomplishes access pattern change by defining\nconfigurable styles of change. This preliminary prototype has been designed to\nbe open and fully extensible. To illustrate the capabilities of DOEF, we used\nit to compare the performances of four state of the art dynamic clustering\nalgorithms. The results show that DOEF is indeed effective at determining the\nadaptability of each dynamic clustering algorithm to changes in access pattern.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2007 12:24:27 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["He", "Zhen", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "0705.1455", "submitter": "Jerome Darmont", "authors": "Fadila Bentayeb (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "Decision tree modeling with relational views", "comments": null, "journal-ref": "LNAI, Vol. 2366 (06/2002) 423-431", "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  Data mining is a useful decision support technique that can be used to\ndiscover production rules in warehouses or corporate data. Data mining research\nhas made much effort to apply various mining algorithms efficiently on large\ndatabases. However, a serious problem in their practical application is the\nlong processing time of such algorithms. Nowadays, one of the key challenges is\nto integrate data mining methods within the framework of traditional database\nsystems. Indeed, such implementations can take advantage of the efficiency\nprovided by SQL engines. In this paper, we propose an integrating approach for\ndecision trees within a classical database system. In other words, we try to\ndiscover knowledge from relational databases, in the form of production rules,\nvia a procedure embedding SQL queries. The obtained decision tree is defined by\nsuccessive, related relational views. Each view corresponds to a given\npopulation in the underlying decision tree. We selected the classical Induction\nDecision Tree (ID3) algorithm to build the decision tree. To prove that our\nimplementation of ID3 works properly, we successfully compared the output of\nour procedure with the output of an existing and validated data mining\nsoftware, SIPINA. Furthermore, since our approach is tuneable, it can be\ngeneralized to any other similar decision tree-based method.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2007 12:25:57 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Bentayeb", "Fadila", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "0705.1456", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (ERIC), Omar Boussa\\\"id (ERIC), Fadila Bentayeb\n  (ERIC)", "title": "Warehousing Web Data", "comments": null, "journal-ref": "4th International Conference on Information Integration and\n  Web-based Applications and Services (iiWAS 02) (09/2002) 148-152", "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  In a data warehousing process, mastering the data preparation phase allows\nsubstantial gains in terms of time and performance when performing\nmultidimensional analysis or using data mining algorithms. Furthermore, a data\nwarehouse can require external data. The web is a prevalent data source in this\ncontext. In this paper, we propose a modeling process for integrating diverse\nand heterogeneous (so-called multiform) data into a unified format.\nFurthermore, the very schema definition provides first-rate metadata in our\ndata warehousing context. At the conceptual level, a complex object is\nrepresented in UML. Our logical model is an XML schema that can be described\nwith a DTD or the XML-Schema language. Eventually, we have designed a Java\nprototype that transforms our multiform input data into XML documents\nrepresenting our physical model. Then, the XML documents we obtain are mapped\ninto a relational database we view as an ODS (Operational Data Storage), whose\ncontent will have to be re-modeled in a multidimensional way to allow its\nstorage in a star schema-based warehouse and, later, its analysis.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2007 12:28:52 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Boussa\u00efd", "Omar", "", "ERIC"], ["Bentayeb", "Fadila", "", "ERIC"]]}, {"id": "0705.1457", "submitter": "Jerome Darmont", "authors": "Sami Miniaoui (ERIC), J\\'er\\^ome Darmont (ERIC), Omar Boussa\\\"id\n  (ERIC)", "title": "Web data modeling for integration in data warehouses", "comments": null, "journal-ref": "First International Workshop on Multimedia Data and Document\n  Engineering (MDDE 01) (07/2001) 88-97", "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  In a data warehousing process, the data preparation phase is crucial.\nMastering this phase allows substantial gains in terms of time and performance\nwhen performing a multidimensional analysis or using data mining algorithms.\nFurthermore, a data warehouse can require external data. The web is a prevalent\ndata source in this context, but the data broadcasted on this medium are very\nheterogeneous. We propose in this paper a UML conceptual model for a complex\nobject representing a superclass of any useful data source (databases, plain\ntexts, HTML and XML documents, images, sounds, video clips...). The translation\ninto a logical model is achieved with XML, which helps integrating all these\ndiverse, heterogeneous data into a unified format, and whose schema definition\nprovides first-rate metadata in our data warehousing context. Moreover, we\nbenefit from XML's flexibility, extensibility and from the richness of the\nsemi-structured data model, but we are still able to later map XML documents\ninto a database if more structuring is needed.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2007 12:30:19 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Miniaoui", "Sami", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Boussa\u00efd", "Omar", "", "ERIC"]]}, {"id": "0705.2787", "submitter": "David Martin", "authors": "David J. Martin, Daniel Kifer, Ashwin Machanavajjhala, Johannes\n  Gehrke, Joseph Y. Halpern", "title": "Worst-Case Background Knowledge for Privacy-Preserving Data Publishing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  Recent work has shown the necessity of considering an attacker's background\nknowledge when reasoning about privacy in data publishing. However, in\npractice, the data publisher does not know what background knowledge the\nattacker possesses. Thus, it is important to consider the worst-case. In this\npaper, we initiate a formal study of worst-case background knowledge. We\npropose a language that can express any background knowledge about the data. We\nprovide a polynomial time algorithm to measure the amount of disclosure of\nsensitive information in the worst case, given that the attacker has at most a\nspecified number of pieces of information in this language. We also provide a\nmethod to efficiently sanitize the data so that the amount of disclosure in the\nworst case is less than a specified threshold.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2007 00:12:24 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Martin", "David J.", ""], ["Kifer", "Daniel", ""], ["Machanavajjhala", "Ashwin", ""], ["Gehrke", "Johannes", ""], ["Halpern", "Joseph Y.", ""]]}, {"id": "0705.3949", "submitter": "Yeb Havinga", "authors": "Yeb Havinga", "title": "Translating a first-order modal language to relational algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": null, "abstract": "  This paper is about Kripke structures that are inside a relational database\nand queried with a modal language. At first the modal language that is used is\nintroduced, followed by a definition of the database and relational algebra.\nBased on these definitions two things are presented: a mapping from components\nof the modal structure to a relational database schema and instance, and a\ntranslation from queries in the modal language to relational algebra queries.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2007 12:36:58 GMT"}], "update_date": "2007-05-29", "authors_parsed": [["Havinga", "Yeb", ""]]}, {"id": "0705.4442", "submitter": "Dan Olteanu", "authors": "Dan Olteanu and Christoph Koch and Lyublena Antova", "title": "World-set Decompositions: Expressiveness and Efficient Algorithms", "comments": "34 pages, 13 figures, extended version of ICDT'07 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": null, "abstract": "  Uncertain information is commonplace in real-world data management scenarios.\nThe ability to represent large sets of possible instances (worlds) while\nsupporting efficient storage and processing is an important challenge in this\ncontext. The recent formalism of world-set decompositions (WSDs) provides a\nspace-efficient representation for uncertain data that also supports scalable\nprocessing. WSDs are complete for finite world-sets in that they can represent\nany finite set of possible worlds. For possibly infinite world-sets, we show\nthat a natural generalization of WSDs precisely captures the expressive power\nof c-tables. We then show that several important decision problems are\nefficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we\ngive a polynomial-time algorithm for factorizing WSDs, i.e. an efficient\nalgorithm for minimizing such representations.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2007 17:56:06 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2008 10:58:06 GMT"}], "update_date": "2008-01-09", "authors_parsed": [["Olteanu", "Dan", ""], ["Koch", "Christoph", ""], ["Antova", "Lyublena", ""]]}, {"id": "0705.4676", "submitter": "Daniel Lemire", "authors": "Daniel Lemire and Owen Kaser", "title": "Recursive n-gram hashing is pairwise independent, at best", "comments": "See software at https://github.com/lemire/rollinghashcpp", "journal-ref": "Computer Speech & Language 24(4): 698-710 (2010)", "doi": "10.1016/j.csl.2009.12.001", "report-no": null, "categories": "cs.DB cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many applications use sequences of n consecutive symbols (n-grams). Hashing\nthese n-grams can be a performance bottleneck. For more speed, recursive hash\nfamilies compute hash values by updating previous values. We prove that\nrecursive hash families cannot be more than pairwise independent. While hashing\nby irreducible polynomials is pairwise independent, our implementations either\nrun in time O(n) or use an exponential amount of memory. As a more scalable\nalternative, we make hashing by cyclic polynomials pairwise independent by\nignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials\nis is twice as fast as hashing by irreducible polynomials. We also show that\nrandomized Karp-Rabin hash families are not pairwise independent.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2007 18:41:28 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2008 17:39:33 GMT"}, {"version": "v3", "created": "Fri, 6 Feb 2009 21:37:14 GMT"}, {"version": "v4", "created": "Mon, 23 Feb 2009 16:23:41 GMT"}, {"version": "v5", "created": "Wed, 5 Aug 2009 03:01:20 GMT"}, {"version": "v6", "created": "Wed, 19 Aug 2009 14:39:54 GMT"}, {"version": "v7", "created": "Wed, 4 Jan 2012 20:37:05 GMT"}, {"version": "v8", "created": "Mon, 6 Jun 2016 15:18:03 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Lemire", "Daniel", ""], ["Kaser", "Owen", ""]]}]