[{"id": "1301.0977", "submitter": "Hilmi Yildirim", "authors": "Hilmi Yildirim, Vineet Chaoji and Mohammed J. Zaki", "title": "DAGGER: A Scalable Index for Reachability Queries in Large Dynamic\n  Graphs", "comments": "11 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ubiquity of large-scale graph data in a variety of application\ndomains, querying them effectively is a challenge. In particular, reachability\nqueries are becoming increasingly important, especially for containment,\nsubsumption, and connectivity checks. Whereas many methods have been proposed\nfor static graph reachability, many real-world graphs are constantly evolving,\nwhich calls for dynamic indexing. In this paper, we present a fully dynamic\nreachability index over dynamic graphs. Our method, called DAGGER, is a\nlight-weight index based on interval labeling, that scales to million node\ngraphs and beyond. Our extensive experimental evaluation on real-world and\nsynthetic graphs confirms its effectiveness over baseline methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2013 06:12:42 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Yildirim", "Hilmi", ""], ["Chaoji", "Vineet", ""], ["Zaki", "Mohammed J.", ""]]}, {"id": "1301.1003", "submitter": "Jef Wijsen", "authors": "Jef Wijsen", "title": "Charting the Tractability Frontier of Certain Conjunctive Query\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An uncertain database is defined as a relational database in which primary\nkeys need not be satisfied. A repair (or possible world) of such database is\nobtained by selecting a maximal number of tuples without ever selecting two\ndistinct tuples with the same primary key value. For a Boolean query q, the\ndecision problem CERTAINTY(q) takes as input an uncertain database db and asks\nwhether q is satisfied by every repair of db. Our main focus is on acyclic\nBoolean conjunctive queries without self-join. Previous work has introduced the\nnotion of (directed) attack graph of such queries, and has proved that\nCERTAINTY(q) is first-order expressible if and only if the attack graph of q is\nacyclic. The current paper investigates the boundary between tractability and\nintractability of CERTAINTY(q). We first classify cycles in attack graphs as\neither weak or strong, and then prove among others the following. If the attack\ngraph of a query q contains a strong cycle, then CERTAINTY(q) is coNP-complete.\nIf the attack graph of q contains no strong cycle and every weak cycle of it is\nterminal (i.e., no edge leads from a vertex in the cycle to a vertex outside\nthe cycle), then CERTAINTY(q) is in P. We then partially address the only\nremaining open case, i.e., when the attack graph contains some nonterminal\ncycle and no strong cycle. Finally, we establish a relationship between the\ncomplexities of CERTAINTY(q) and evaluating q on probabilistic databases.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2013 13:08:21 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Wijsen", "Jef", ""]]}, {"id": "1301.1218", "submitter": "Matteo Riondato", "authors": "Matteo Riondato and Fabio Vandin", "title": "Finding the True Frequent Itemsets", "comments": "13 pages, Extended version of work appeared in SIAM International\n  Conference on Data Mining, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent Itemsets (FIs) mining is a fundamental primitive in data mining. It\nrequires to identify all itemsets appearing in at least a fraction $\\theta$ of\na transactional dataset $\\mathcal{D}$. Often though, the ultimate goal of\nmining $\\mathcal{D}$ is not an analysis of the dataset \\emph{per se}, but the\nunderstanding of the underlying process that generated it. Specifically, in\nmany applications $\\mathcal{D}$ is a collection of samples obtained from an\nunknown probability distribution $\\pi$ on transactions, and by extracting the\nFIs in $\\mathcal{D}$ one attempts to infer itemsets that are frequently (i.e.,\nwith probability at least $\\theta$) generated by $\\pi$, which we call the True\nFrequent Itemsets (TFIs). Due to the inherently stochastic nature of the\ngenerative process, the set of FIs is only a rough approximation of the set of\nTFIs, as it often contains a huge number of \\emph{false positives}, i.e.,\nspurious itemsets that are not among the TFIs. In this work we design and\nanalyze an algorithm to identify a threshold $\\hat{\\theta}$ such that the\ncollection of itemsets with frequency at least $\\hat{\\theta}$ in $\\mathcal{D}$\ncontains only TFIs with probability at least $1-\\delta$, for some\nuser-specified $\\delta$. Our method uses results from statistical learning\ntheory involving the (empirical) VC-dimension of the problem at hand. This\nallows us to identify almost all the TFIs without including any false positive.\nWe also experimentally compare our method with the direct mining of\n$\\mathcal{D}$ at frequency $\\theta$ and with techniques based on widely-used\nstandard bounds (i.e., the Chernoff bounds) of the binomial distribution, and\nshow that our algorithm outperforms these methods and achieves even better\nresults than what is guaranteed by the theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2013 15:04:43 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2013 12:54:12 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2014 16:38:44 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Riondato", "Matteo", ""], ["Vandin", "Fabio", ""]]}, {"id": "1301.1332", "submitter": "Daniel Ritter", "authors": "Daniel Ritter", "title": "A Logic Programming Approach to Integration Network Inference", "comments": "15 pages, The 26th Workshop on Logic Programming (WLP), Bonn, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery, representation and reconstruction of (technical) integration\nnetworks from Network Mining (NM) raw data is a difficult problem for\nenterprises. This is due to large and complex IT landscapes within and across\nenterprise boundaries, heterogeneous technology stacks, and fragmented data. To\nremain competitive, visibility into the enterprise and partner IT networks on\ndifferent, interrelated abstraction levels is desirable.\n  We present an approach to represent and reconstruct the integration networks\nfrom NM raw data using logic programming based on first-order logic. The raw\ndata expressed as integration network model is represented as facts, on which\nrules are applied to reconstruct the network. We have built a system that is\nused to apply this approach to real-world enterprise landscapes and we report\non our experience with this system.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2013 20:52:40 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2013 16:24:10 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Ritter", "Daniel", ""]]}, {"id": "1301.1575", "submitter": "Anna Pyayt", "authors": "Anna Pyayt, Michael Gubanov", "title": "BigDB: Automatic Machine Learning Optimizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this short vision paper, we introduce a machine learning optimizer for\ndata management and describe its architecture and main functionality.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2013 04:03:29 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Pyayt", "Anna", ""], ["Gubanov", "Michael", ""]]}, {"id": "1301.1751", "submitter": "Hongyu Liang", "authors": "Hongyu Liang and Hao Yuan", "title": "On the Complexity of $t$-Closeness Anonymization and Related Problems", "comments": "An extended abstract to appear in DASFAA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important issue in releasing individual data is to protect the sensitive\ninformation from being leaked and maliciously utilized. Famous privacy\npreserving principles that aim to ensure both data privacy and data integrity,\nsuch as $k$-anonymity and $l$-diversity, have been extensively studied both\ntheoretically and empirically. Nonetheless, these widely-adopted principles are\nstill insufficient to prevent attribute disclosure if the attacker has partial\nknowledge about the overall sensitive data distribution. The $t$-closeness\nprinciple has been proposed to fix this, which also has the benefit of\nsupporting numerical sensitive attributes. However, in contrast to\n$k$-anonymity and $l$-diversity, the theoretical aspect of $t$-closeness has\nnot been well investigated.\n  We initiate the first systematic theoretical study on the $t$-closeness\nprinciple under the commonly-used attribute suppression model. We prove that\nfor every constant $t$ such that $0\\leq t<1$, it is NP-hard to find an optimal\n$t$-closeness generalization of a given table. The proof consists of several\nreductions each of which works for different values of $t$, which together\ncover the full range. To complement this negative result, we also provide exact\nand fixed-parameter algorithms. Finally, we answer some open questions\nregarding the complexity of $k$-anonymity and $l$-diversity left in the\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2013 04:34:03 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Liang", "Hongyu", ""], ["Yuan", "Hao", ""]]}, {"id": "1301.2236", "submitter": "Rym Khemiri", "authors": "Rym Khemiri and Fadila Bentayeb", "title": "User Profile-Driven Data Warehouse Summary for Adaptive OLAP Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data warehousing is an essential element of decision support systems. It aims\nat enabling the user knowledge to make better and faster daily business\ndecisions. To improve this decision support system and to give more and more\nrelevant information to the user, the need to integrate user's profiles into\nthe data warehouse process becomes crucial. In this paper, we propose to\nexploit users' preferences as a basis for adapting OLAP (On-Line Analytical\nProcessing) queries to the user. For this, we present a user profile-driven\ndata warehouse approach that allows dening user's profile composed by his/her\nidentifier and a set of his/her preferences. Our approach is based on a general\ndata warehouse architecture and an adaptive OLAP analysis system. Our main idea\nconsists in creating a data warehouse materialized view for each user with\nrespect to his/her profile. This task is performed off-line when the user\ndefines his/her profile for the first time. Then, when a user query is\nsubmitted to the data warehouse, the system deals with his/her data warehouse\nmaterialized view instead of the whole data warehouse. In other words, the data\nwarehouse view summaries the data warehouse content for the user by taking into\naccount his/her preferences. Moreover, we are implementing our data warehouse\npersonalization approach under the SQL Server 2005 DBMS (DataBase Management\nSystem).\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 20:09:36 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Khemiri", "Rym", ""], ["Bentayeb", "Fadila", ""]]}, {"id": "1301.2362", "submitter": "Jianxin Li", "authors": "Jianxin Li, Chengfei Liu, Rui Zhou and Jeffrey Xu Yu", "title": "Quasi-SLCA based Keyword Query Processing over Probabilistic XML Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probabilistic threshold query is one of the most common queries in\nuncertain databases, where a result satisfying the query must be also with\nprobability meeting the threshold requirement. In this paper, we investigate\nprobabilistic threshold keyword queries (PrTKQ) over XML data, which is not\nstudied before. We first introduce the notion of quasi-SLCA and use it to\nrepresent results for a PrTKQ with the consideration of possible world\nsemantics. Then we design a probabilistic inverted (PI) index that can be used\nto quickly return the qualified answers and filter out the unqualified ones\nbased on our proposed lower/upper bounds. After that, we propose two efficient\nand comparable algorithms: Baseline Algorithm and PI index-based Algorithm. To\naccelerate the performance of algorithms, we also utilize probability density\nfunction. An empirical study using real and synthetic data sets has verified\nthe effectiveness and the efficiency of our approaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 00:05:46 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Li", "Jianxin", ""], ["Liu", "Chengfei", ""], ["Zhou", "Rui", ""], ["Yu", "Jeffrey Xu", ""]]}, {"id": "1301.2375", "submitter": "Jianxin Li", "authors": "Jianxin Li, Chengfei Liu, Liang Yao and Jeffrey Xu Yu", "title": "Context-based Diversification for Keyword Queries over XML Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While keyword query empowers ordinary users to search vast amount of data,\nthe ambiguity of keyword query makes it difficult to effectively answer keyword\nqueries, especially for short and vague keyword queries. To address this\nchallenging problem, in this paper we propose an approach that automatically\ndiversifies XML keyword search based on its different contexts in the XML data.\nGiven a short and vague keyword query and XML data to be searched, we firstly\nderive keyword search candidates of the query by a classifical feature\nselection model. And then, we design an effective XML keyword search\ndiversification model to measure the quality of each candidate. After that,\nthree efficient algorithms are proposed to evaluate the possible generated\nquery candidates representing the diversified search intentions, from which we\ncan find and return top-$k$ qualified query candidates that are most relevant\nto the given keyword query while they can cover maximal number of distinct\nresults.At last, a comprehensive evaluation on real and synthetic datasets\ndemonstrates the effectiveness of our proposed diversification model and the\nefficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 01:33:50 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Li", "Jianxin", ""], ["Liu", "Chengfei", ""], ["Yao", "Liang", ""], ["Yu", "Jeffrey Xu", ""]]}, {"id": "1301.2378", "submitter": "Jianxin Li", "authors": "Jianxin Li, Chengfei Liu, Liang Yao, Jeffrey Xu Yu and Rui Zhou", "title": "Query-driven Frequent Co-occurring Term Extraction over Relational Data\n  using MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study how to efficiently compute \\textit{frequent\nco-occurring terms} (FCT) in the results of a keyword query in parallel using\nthe popular MapReduce framework. Taking as input a keyword query q and an\ninteger k, an FCT query reports the k terms that are not in q, but appear most\nfrequently in the results of the keyword query q over multiple joined\nrelations. The returned terms of FCT search can be used to do query expansion\nand query refinement for traditional keyword search. Different from the method\nof FCT search in a single platform, our proposed approach can efficiently\nanswer a FCT query using the MapReduce Paradigm without pre-computing the\nresults of the original keyword query, which is run in parallel platform. In\nthis work, we can output the final FCT search results by two MapReduce jobs:\nthe first is to extract the statistical information of the data; and the second\nis to calculate the total frequency of each term based on the output of the\nfirst job. At the two MapReduce jobs, we would guarantee the load balance of\nmappers and the computational balance of reducers as much as possible.\nAnalytical and experimental evaluations demonstrate the efficiency and\nscalability of our proposed approach using TPC-H benchmark datasets with\ndifferent sizes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 01:55:10 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Li", "Jianxin", ""], ["Liu", "Chengfei", ""], ["Yao", "Liang", ""], ["Yu", "Jeffrey Xu", ""], ["Zhou", "Rui", ""]]}, {"id": "1301.3388", "submitter": "Olle Liljenzin", "authors": "Olle Liljenzin", "title": "Confluently Persistent Sets and Maps", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordered sets and maps play important roles as index structures in relational\ndata models. When a shared index in a multi-user system is modified\nconcurrently, the current state of the index will diverge into multiple\nversions containing the local modifications performed in each work flow. The\nconfluent persistence problem arises when versions should be melded in commit\nand refresh operations so that modifications performed by different users\nbecome merged.\n  Confluently Persistent Sets and Maps are functional binary search trees that\nsupport efficient set operations both when operands are disjoint and when they\nare overlapping. Treap properties with hash values as priorities are maintained\nand with hash-consing of nodes a unique representation is provided.\nNon-destructive set merge algorithms that skip inspection of equal subtrees and\na conflict detecting meld algorithm based on set merges are presented. The meld\nalgorithm is used in commit and refresh operations. With m modifications in one\nflow and n items in total, the expected cost of the operations is O(m\nlog(n/m)).\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 12:49:13 GMT"}], "update_date": "2013-01-16", "authors_parsed": [["Liljenzin", "Olle", ""]]}, {"id": "1301.3884", "submitter": "Dmitry Y. Pavlov", "authors": "Dmitry Y. Pavlov, Heikki Mannila, Padhraic Smyth", "title": "Probabilistic Models for Query Approximation with Large Sparse Binary\n  Datasets", "comments": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2000-PG-465-472", "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large sparse sets of binary transaction data with millions of records and\nthousands of attributes occur in various domains: customers purchasing\nproducts, users visiting web pages, and documents containing words are just\nthree typical examples. Real-time query selectivity estimation (the problem of\nestimating the number of rows in the data satisfying a given predicate) is an\nimportant practical problem for such databases.\n  We investigate the application of probabilistic models to this problem. In\nparticular, we study a Markov random field (MRF) approach based on frequent\nsets and maximum entropy, and compare it to the independence model and the\nChow-Liu tree model. We find that the MRF model provides substantially more\naccurate probability estimates than the other methods but is more expensive\nfrom a computational and memory viewpoint. To alleviate the computational\nrequirements we show how one can apply bucket elimination and clique tree\napproaches to take advantage of structure in the models and in the queries. We\nprovide experimental results on two large real-world transaction datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:52:06 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Pavlov", "Dmitry Y.", ""], ["Mannila", "Heikki", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1301.4200", "submitter": "Fabian Hueske", "authors": "Fabian Hueske, Aljoscha Krettek, Kostas Tzoumas", "title": "Enabling Operator Reordering in Data Flow Programs Through Static Code\n  Analysis", "comments": "4 pages, accepted and presented at the First International Workshop\n  on Cross-model Language Design and Implementation (XLDI), affiliated with\n  ICFP 2012, Copenhagen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many massively parallel data management platforms, programs are\nrepresented as small imperative pieces of code connected in a data flow. This\npopular abstraction makes it hard to apply algebraic reordering techniques\nemployed by relational DBMSs and other systems that use an algebraic\nprogramming abstraction. We present a code analysis technique based on reverse\ndata and control flow analysis that discovers a set of properties from user\ncode, which can be used to emulate algebraic optimizations in this setting.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 19:42:55 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Hueske", "Fabian", ""], ["Krettek", "Aljoscha", ""], ["Tzoumas", "Kostas", ""]]}, {"id": "1301.5121", "submitter": "Alex Averbuch", "authors": "Alex Averbuch, Martin Neumann", "title": "Partitioning Graph Databases - A Quantitative Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic data is growing at increasing rates, in both size and\nconnectivity: the increasing presence of, and interest in, relationships\nbetween data. An example is the Twitter social network graph. Due to this\ngrowth demand is increasing for technologies that can process such data.\nCurrently relational databases are the predominant technology, but they are\npoorly suited to processing connected data as they are optimized for\nindex-intensive operations. Conversely, graph databases are optimized for graph\ncomputation. They link records by direct references, avoiding index lookups,\nand enabling retrieval of adjacent elements in constant time, regardless of\ngraph size. However, as data volume increases these databases outgrow the\nresources of one computer and data partitioning becomes necessary. We evaluate\nthe viability of using graph partitioning algorithms to partition graph\ndatabases. A prototype partitioned database was developed. Three partitioning\nalgorithms explored and one implemented. Three graph datasets were used: two\nreal and one synthetically generated. These were partitioned in various ways\nand the impact on database performance measured. We defined one synthetic\naccess pattern per dataset and executed each on the partitioned datasets.\nEvaluation took place in a simulation environment, ensuring repeatability and\nallowing measurement of metrics like network traffic and load balance. Results\nshow that compared to random partitioning the partitioning algorithm reduced\ntraffic by 40-90%. Executing the algorithm intermittently during usage\nmaintained partition quality, while requiring only 1% the computation of\ninitial partitioning. Strong correlations were found between theoretic quality\nmetrics and generated network traffic under non-uniform access patterns.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2013 09:48:34 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Averbuch", "Alex", ""], ["Neumann", "Martin", ""]]}, {"id": "1301.5154", "submitter": "Radhakrishnan Delhibabu", "authors": "Radhakrishnan Delhibabu, Gerhard Lakemeyer", "title": "A Rational and Efficient Algorithm for View Revision in Databases", "comments": null, "journal-ref": "Applied Mathematics & Information Sciences, Volume 7, No. 3\n  (2013), PP:843-856", "doi": "10.12785/amis/070302", "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of belief and knowledge is one of the major components of any\nautonomous system that should be able to incorporate new pieces of information.\nIn this paper, we argue that to apply rationality result of belief dynamics\ntheory to various practical problems, it should be generalized in two respects:\nfirst of all, it should allow a certain part of belief to be declared as\nimmutable; and second, the belief state need not be deductively closed. Such a\ngeneralization of belief dynamics, referred to as base dynamics, is presented,\nalong with the concept of a generalized revision algorithm for Horn knowledge\nbases. We show that Horn knowledge base dynamics has interesting connection\nwith kernel change and abduction. Finally, we also show that both variants are\nrational in the sense that they satisfy certain rationality postulates stemming\nfrom philosophical works on belief dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2013 11:30:37 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Delhibabu", "Radhakrishnan", ""], ["Lakemeyer", "Gerhard", ""]]}, {"id": "1301.5871", "submitter": "Pierre-Francois Marteau", "authors": "Muhammad Marwan Muhammad Fuad (VALORIA), Pierre-Fran\\c{c}ois Marteau\n  (VALORIA)", "title": "Towards a faster symbolic aggregate approximation method", "comments": "ICSOFT 2010 - Fifth International Conference on Software and Data\n  Technologies, Athens : Greece (2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The similarity search problem is one of the main problems in time series data\nmining. Traditionally, this problem was tackled by sequentially comparing the\ngiven query against all the time series in the database, and returning all the\ntime series that are within a predetermined threshold of that query. But the\nlarge size and the high dimensionality of time series databases that are in use\nnowadays make that scenario inefficient. There are many representation\ntechniques that aim at reducing the dimensionality of time series so that the\nsearch can be handled faster at a lower-dimensional space level. The symbolic\naggregate approximation (SAX) is one of the most competitive methods in the\nliterature. In this paper we present a new method that improves the performance\nof SAX by adding to it another exclusion condition that increases the exclusion\npower. This method is based on using two representations of the time series:\none of SAX and the other is based on an optimal approximation of the time\nseries. Pre-computed distances are calculated and stored offline to be used\nonline to exclude a wide range of the search space using two exclusion\nconditions. We conduct experiments which show that the new method is faster\nthan SAX.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2013 19:24:33 GMT"}], "update_date": "2013-01-25", "authors_parsed": [["Fuad", "Muhammad Marwan Muhammad", "", "VALORIA"], ["Marteau", "Pierre-Fran\u00e7ois", "", "VALORIA"]]}, {"id": "1301.6479", "submitter": "Carsten Lutz", "authors": "Meghyn Bienvenu, Balder ten Cate, Carsten Lutz, Frank Wolter", "title": "Ontology-based Data Access: A Study through Disjunctive Datalog, CSP,\n  and MMSNP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology-based data access is concerned with querying incomplete data sources\nin the presence of domain-specific knowledge provided by an ontology. A central\nnotion in this setting is that of an ontology-mediated query, which is a\ndatabase query coupled with an ontology. In this paper, we study several\nclasses of ontology-mediated queries, where the database queries are given as\nsome form of conjunctive query and the ontologies are formulated in description\nlogics or other relevant fragments of first-order logic, such as the guarded\nfragment and the unary-negation fragment. The contributions of the paper are\nthree-fold. First, we characterize the expressive power of ontology-mediated\nqueries in terms of fragments of disjunctive datalog. Second, we establish\nintimate connections between ontology-mediated queries and constraint\nsatisfaction problems (CSPs) and their logical generalization, MMSNP formulas.\nThird, we exploit these connections to obtain new results regarding (i)\nfirst-order rewritability and datalog-rewritability of ontology-mediated\nqueries, (ii) P/NP dichotomies for ontology-mediated queries, and (iii) the\nquery containment problem for ontology-mediated queries.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 09:11:54 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2013 12:52:59 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Bienvenu", "Meghyn", ""], ["Cate", "Balder ten", ""], ["Lutz", "Carsten", ""], ["Wolter", "Frank", ""]]}, {"id": "1301.6626", "submitter": "Xiangnan Kong", "authors": "Xiangnan Kong, Philip S. Yu, Xue Wang, Ann B. Ragin", "title": "Discriminative Feature Selection for Uncertain Graph Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining discriminative features for graph data has attracted much attention in\nrecent years due to its important role in constructing graph classifiers,\ngenerating graph indices, etc. Most measurement of interestingness of\ndiscriminative subgraph features are defined on certain graphs, where the\nstructure of graph objects are certain, and the binary edges within each graph\nrepresent the \"presence\" of linkages among the nodes. In many real-world\napplications, however, the linkage structure of the graphs is inherently\nuncertain. Therefore, existing measurements of interestingness based upon\ncertain graphs are unable to capture the structural uncertainty in these\napplications effectively. In this paper, we study the problem of discriminative\nsubgraph feature selection from uncertain graphs. This problem is challenging\nand different from conventional subgraph mining problems because both the\nstructure of the graph objects and the discrimination score of each subgraph\nfeature are uncertain. To address these challenges, we propose a novel\ndiscriminative subgraph feature selection method, DUG, which can find\ndiscriminative subgraph features in uncertain graphs based upon different\nstatistical measures including expectation, median, mode and phi-probability.\nWe first compute the probability distribution of the discrimination scores for\neach subgraph feature based on dynamic programming. Then a branch-and-bound\nalgorithm is proposed to search for discriminative subgraphs efficiently.\nExtensive experiments on various neuroimaging applications (i.e., Alzheimer's\nDisease, ADHD and HIV) have been performed to analyze the gain in performance\nby taking into account structural uncertainties in identifying discriminative\nsubgraph features for graph classification.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 18:00:33 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Kong", "Xiangnan", ""], ["Yu", "Philip S.", ""], ["Wang", "Xue", ""], ["Ragin", "Ann B.", ""]]}, {"id": "1301.6780", "submitter": "Yuchen Zhao", "authors": "Yuchen Zhao, Philip S. Yu", "title": "On Graph Stream Clustering with Side Information", "comments": "Full version of SIAM SDM 2013 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Graph clustering becomes an important problem due to emerging applications\ninvolving the web, social networks and bio-informatics. Recently, many such\napplications generate data in the form of streams. Clustering massive, dynamic\ngraph streams is significantly challenging because of the complex structures of\ngraphs and computational difficulties of continuous data. Meanwhile, a large\nvolume of side information is associated with graphs, which can be of various\ntypes. The examples include the properties of users in social network\nactivities, the meta attributes associated with web click graph streams and the\nlocation information in mobile communication networks. Such attributes contain\nextremely useful information and has the potential to improve the clustering\nprocess, but are neglected by most recent graph stream mining techniques. In\nthis paper, we define a unified distance measure on both link structures and\nside attributes for clustering. In addition, we propose a novel optimization\nframework DMO, which can dynamically optimize the distance metric and make it\nadapt to the newly received stream data. We further introduce a carefully\ndesigned statistics SGS(C) which consume constant storage spaces with the\nprogression of streams. We demonstrate that the statistics maintained are\nsufficient for the clustering process as well as the distance optimization and\ncan be scalable to massive graphs with side attributes. We will present\nexperiment results to show the advantages of the approach in graph stream\nclustering with both links and side information over the baselines.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 21:28:43 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Zhao", "Yuchen", ""], ["Yu", "Philip S.", ""]]}, {"id": "1301.6905", "submitter": "Robert Kowalski", "authors": "Robert Kowalski, Fariba Sadri", "title": "Towards a Logic-Based Unifying Framework for Computing", "comments": "An improved version of this paper will be published in the journal,\n  New Generation Computing, with the title \"Reactive Computing as Model\n  Generation\". In the meanwhile, a copy of the revised paper can be found on\n  http://www.doc.ic.ac.uk/~rak/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a logic-based, framework inspired by artificial\nintelligence, but scaled down for practical database and programming\napplications. Computation in the framework is viewed as the task of generating\na sequence of state transitions, with the purpose of making an agent's goals\nall true. States are represented by sets of atomic sentences (or facts),\nrepresenting the values of program variables, tuples in a coordination\nlanguage, facts in relational databases, or Herbrand models.\n  In the model-theoretic semantics, the entire sequence of states and events\nare combined into a single model-theoretic structure, by associating timestamps\nwith facts and events. But in the operational semantics, facts are updated\ndestructively, without timestamps. We show that the model generated by\ndestructive updates is identical to the model generated by reasoning with facts\ncontaining timestamps. We also extend the model with intentional predicates and\ncomposite event predicates defined by logic programs containing conditions in\nfirst-order logic, which query the current state.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 12:23:26 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 07:25:29 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Kowalski", "Robert", ""], ["Sadri", "Fariba", ""]]}, {"id": "1301.6952", "submitter": "Yannick Schwartz", "authors": "Yannick Schwartz (INRIA Saclay - Ile de France, LNAO), Alexis Barbot\n  (LNAO), Benjamin Thyreau (LNAO), Vincent Frouin (LNAO), Ga\\\"el Varoquaux\n  (INRIA Saclay - Ile de France, LNAO), Aditya Siram, Daniel Marcus,\n  Jean-Baptiste Poline (LNAO)", "title": "PyXNAT: XNAT in Python", "comments": null, "journal-ref": "Frontiers in Neuroinformatics 6, 12 (2012) 1-14", "doi": "10.3389/fninf.2012.00012", "report-no": null, "categories": "cs.DB cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neuroimaging databases grow in size and complexity, the time researchers\nspend investigating and managing the data increases to the expense of data\nanalysis. As a result, investigators rely more and more heavily on scripting\nusing high-level languages to automate data management and processing tasks.\nFor this, a structured and programmatic access to the data store is necessary.\nWeb services are a first step toward this goal. They however lack in\nfunctionality and ease of use because they provide only low level interfaces to\ndatabases. We introduce here PyXNAT, a Python module that interacts with The\nExtensible Neuroimaging Archive Toolkit (XNAT) through native Python calls\nacross multiple operating systems. The choice of Python enables PyXNAT to\nexpose the XNAT Web Services and unify their features with a higher level and\nmore expressive language. PyXNAT provides XNAT users direct access to all the\nscientific packages in Python. Finally PyXNAT aims to be efficient and easy to\nuse, both as a backend library to build XNAT clients and as an alternative\nfrontend from the command line.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 15:42:18 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Schwartz", "Yannick", "", "INRIA Saclay - Ile de France, LNAO"], ["Barbot", "Alexis", "", "LNAO"], ["Thyreau", "Benjamin", "", "LNAO"], ["Frouin", "Vincent", "", "LNAO"], ["Varoquaux", "Ga\u00ebl", "", "INRIA Saclay - Ile de France, LNAO"], ["Siram", "Aditya", "", "LNAO"], ["Marcus", "Daniel", "", "LNAO"], ["Poline", "Jean-Baptiste", "", "LNAO"]]}, {"id": "1301.7015", "submitter": "Entong Shen", "authors": "Entong Shen, Ting Yu", "title": "Mining Frequent Graph Patterns with Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering frequent graph patterns in a graph database offers valuable\ninformation in a variety of applications. However, if the graph dataset\ncontains sensitive data of individuals such as mobile phone-call graphs and\nweb-click graphs, releasing discovered frequent patterns may present a threat\nto the privacy of individuals. {\\em Differential privacy} has recently emerged\nas the {\\em de facto} standard for private data analysis due to its provable\nprivacy guarantee. In this paper we propose the first differentially private\nalgorithm for mining frequent graph patterns.\n  We first show that previous techniques on differentially private discovery of\nfrequent {\\em itemsets} cannot apply in mining frequent graph patterns due to\nthe inherent complexity of handling structural information in graphs. We then\naddress this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling\nbased algorithm. Unlike previous work on frequent itemset mining, our\ntechniques do not rely on the output of a non-private mining algorithm.\nInstead, we observe that both frequent graph pattern mining and the guarantee\nof differential privacy can be unified into an MCMC sampling framework. In\naddition, we establish the privacy and utility guarantee of our algorithm and\npropose an efficient neighboring pattern counting technique as well.\nExperimental results show that the proposed algorithm is able to output\nfrequent patterns with good precision.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 18:37:35 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2013 21:43:20 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Shen", "Entong", ""], ["Yu", "Ting", ""]]}, {"id": "1301.7669", "submitter": "Nicos Angelopoulos", "authors": "Jan Wielemaker", "title": "Extending the logical update view with transaction support", "comments": "Appeared in CICLOPS 2012. 9 Pages, 0 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the database update view was standardised in the Prolog ISO standard,\nthe so called logical update view is available in all actively maintained\nProlog systems. While this update view provided a well defined update semantics\nand allows for efficient handling of dynamic code, it does not help in\nmaintaining consistency of the dynamic database. With the introduction of\nmultiple threads and deployment of Prolog in continuously running server\napplications, consistency of the dynamic database becomes important.\n  In this article, we propose an extension to the generation-based\nimplementation of the logical update view that supports transactions.\nGeneration-based transactions have been implemented according to this\ndescription in the SWI-Prolog RDF store. The aim of this paper is to motivate\ntransactions, outline an implementation and generate discussion on the\ndesirable semantics and interface prior to implementation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 16:23:43 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Wielemaker", "Jan", ""]]}]