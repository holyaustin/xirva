[{"id": "1807.00035", "submitter": "Vuong M. Ngo", "authors": "Vuong M. Ngo, Nhien-An Le-Khac, M-Tahar Kechadi", "title": "An Efficient Data Warehouse for Crop Yield Prediction", "comments": "12 pages. Keywords. Data warehouse, constellation schema, crop yield\n  prediction, precision agriculture", "journal-ref": "Proceedings of the 14th International Conference on Precision\n  Agriculture. June 24 to June 27, 2018, Montreal, Quebec, Canada", "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, precision agriculture combined with modern information and\ncommunications technologies, is becoming more common in agricultural activities\nsuch as automated irrigation systems, precision planting, variable rate\napplications of nutrients and pesticides, and agricultural decision support\nsystems. In the latter, crop management data analysis, based on machine\nlearning and data mining, focuses mainly on how to efficiently forecast and\nimprove crop yield. In recent years, raw and semi-processed agricultural data\nare usually collected using sensors, robots, satellites, weather stations, farm\nequipment, farmers and agribusinesses while the Internet of Things (IoT) should\ndeliver the promise of wirelessly connecting objects and devices in the\nagricultural ecosystem. Agricultural data typically captures information about\nfarming entities and operations. Every farming entity encapsulates an\nindividual farming concept, such as field, crop, seed, soil, temperature,\nhumidity, pest, and weed. Agricultural datasets are spatial, temporal, complex,\nheterogeneous, non-standardized, and very large. In particular, agricultural\ndata is considered as Big Data in terms of volume, variety, velocity and\nveracity. Designing and developing a data warehouse for precision agriculture\nis a key foundation for establishing a crop intelligence platform, which will\nenable resource efficient agronomy decision making and recommendations. Some of\nthe requirements for such an agricultural data warehouse are privacy, security,\nand real-time access among its stakeholders (e.g., farmers, farm equipment\nmanufacturers, agribusinesses, co-operative societies, customers and possibly\nGovernment agencies). However, currently there are very few reports in the\nliterature that focus on the design of efficient data warehouses with the view\nof enabling Agricultural Big Data analysis and data mining. In this paper ...\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:51:30 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Ngo", "Vuong M.", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1807.00602", "submitter": "Evgeniy Gryaznov", "authors": "Evgeniy Gryaznov", "title": "Semantic Query Language for Temporal Genealogical Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Computers play a crucial role in modern ancestry management, they are used to\ncollect, store, analyze, sort and display genealogical data. However, current\napplications do not take into account the kinship structure of a natural\nlanguage.\n  In this paper we propose a new domain-specific language KISP which is based\non a formalization of English kinship system, for accessing and querying\ntraditional genealogical trees. KISP is a dynamically typed LISP-like\nprogramming language with a rich set of features, such as kinship term\nreduction and temporal information expression.\n  Our solution provides a user with a coherent genealogical framework that\nallows for a natural navigation over any traditional family tree.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 11:27:51 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Gryaznov", "Evgeniy", ""]]}, {"id": "1807.00607", "submitter": "Peter Lindner", "authors": "Martin Grohe and Peter Lindner", "title": "Probabilistic Databases with an Infinite Open-World Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic databases (PDBs) introduce uncertainty into relational\ndatabases by specifying probabilities for several possible instances.\nTraditionally, they are finite probability spaces over database instances. Such\nfinite PDBs inherently make a closed-world assumption: non-occurring facts are\nassumed to be impossible, rather than just unlikely. As convincingly argued by\nCeylan et al. (KR '16), this results in implausibilities and clashes with\nintuition. An open-world assumption, where facts not explicitly listed may have\na small positive probability can yield more reasonable results. The\ncorresponding open-world model of Ceylan et al., however, assumes that all\nentities in the PDB come from a fixed finite universe.\n  In this work, we take one further step and propose a model of \"truly\"\nopen-world PDBs with an infinite universe. This is natural when we consider\nentities from typical domains such as integers, real numbers, or strings. While\nthe probability space might become infinitely large, all instances of a PDB\nremain finite. We provide a sound mathematical framework for infinite PDBs\ngeneralizing the existing theory of finite PDBs. Our main results are concerned\nwith countable, tuple-independent PDBs; we present a generic construction\nshowing that such PDBs exist in the infinite and provide a characterization of\ntheir existence. This construction can be used to give an open-world semantics\nto finite PDBs. The construction can also be extended to so-called\nblock-independent-disjoint probabilistic databases.\n  Algorithmic questions are not the focus of this paper, but we show how query\nevaluation algorithms can be lifted from finite PDBs to perform approximate\nevaluation (with an arbitrarily small additive approximation error) in\ncountably infinite tuple-independent PDBs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 11:41:21 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 08:58:24 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 11:07:59 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Grohe", "Martin", ""], ["Lindner", "Peter", ""]]}, {"id": "1807.00819", "submitter": "Sheikh Rabiul Islam", "authors": "Sheikh Rabiul Islam, William Eberle, Sheikh Khaled Ghafoor", "title": "Mining Bad Credit Card Accounts from OLAP and OLTP", "comments": "Conference proceedings of ICCDA, 2017", "journal-ref": "Islam, S. R., Eberle, W., & Ghafoor, S. K. (2017, May). Mining Bad\n  Credit Card Accounts from OLAP and OLTP. In Proceedings of the International\n  Conference on Compute and Data Analysis (pp. 129-137). ACM", "doi": "10.1145/3093241.3093279", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit card companies classify accounts as a good or bad based on historical\ndata where a bad account may default on payments in the near future. If an\naccount is classified as a bad account, then further action can be taken to\ninvestigate the actual nature of the account and take preventive actions. In\naddition, marking an account as \"good\" when it is actually bad, could lead to\nloss of revenue - and marking an account as \"bad\" when it is actually good,\ncould lead to loss of business. However, detecting bad credit card accounts in\nreal time from Online Transaction Processing (OLTP) data is challenging due to\nthe volume of data needed to be processed to compute the risk factor. We\npropose an approach which precomputes and maintains the risk probability of an\naccount based on historical transactions data from offline data or data from a\ndata warehouse. Furthermore, using the most recent OLTP transactional data,\nrisk probability is calculated for the latest transaction and combined with the\npreviously computed risk probability from the data warehouse. If accumulated\nrisk probability crosses a predefined threshold, then the account is treated as\na bad account and is flagged for manual verification.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 14:24:10 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Islam", "Sheikh Rabiul", ""], ["Eberle", "William", ""], ["Ghafoor", "Sheikh Khaled", ""]]}, {"id": "1807.00878", "submitter": "Qin Zhang", "authors": "David P. Woodruff and Qin Zhang", "title": "Distributed Statistical Estimation of Matrix Products with Applications", "comments": "Appeared in PODS 2018; fixed some typos of the conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical estimations of a matrix product over the integers in\na distributed setting, where we have two parties Alice and Bob; Alice holds a\nmatrix $A$ and Bob holds a matrix $B$, and they want to estimate statistics of\n$A \\cdot B$. We focus on the well-studied $\\ell_p$-norm, distinct elements ($p\n= 0$), $\\ell_0$-sampling, and heavy hitter problems. The goal is to minimize\nboth the communication cost and the number of rounds of communication.\n  This problem is closely related to the fundamental set-intersection join\nproblem in databases: when $p = 0$ the problem corresponds to the size of the\nset-intersection join. When $p = \\infty$ the output is simply the pair of sets\nwith the maximum intersection size. When $p = 1$ the problem corresponds to the\nsize of the corresponding natural join. We also consider the heavy hitters\nproblem which corresponds to finding the pairs of sets with intersection size\nabove a certain threshold, and the problem of sampling an intersecting pair of\nsets uniformly at random.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 20:28:33 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Woodruff", "David P.", ""], ["Zhang", "Qin", ""]]}, {"id": "1807.00971", "submitter": "Eugene Siow", "authors": "Eugene Siow, Thanassis Tiropanis, Wendy Hall", "title": "Analytics for the Internet of Things: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) envisions a world-wide, interconnected network\nof smart physical entities. These physical entities generate a large amount of\ndata in operation and as the IoT gains momentum in terms of deployment, the\ncombined scale of those data seems destined to continue to grow. Increasingly,\napplications for the IoT involve analytics. Data analytics is the process of\nderiving knowledge from data, generating value like actionable insights from\nthem. This article reviews work in the IoT and big data analytics from the\nperspective of their utility in creating efficient, effective and innovative\napplications and services for a wide spectrum of domains. We review the broad\nvision for the IoT as it is shaped in various communities, examine the\napplication of data analytics across IoT domains, provide a categorisation of\nanalytic approaches and propose a layered taxonomy from IoT data to analytics.\nThis taxonomy provides us with insights on the appropriateness of analytical\ntechniques, which in turn shapes a survey of enabling technology and\ninfrastructure for IoT analytics. Finally, we look at some tradeoffs for\nanalytics in the IoT that can shape future research.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 04:15:08 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Siow", "Eugene", ""], ["Tiropanis", "Thanassis", ""], ["Hall", "Wendy", ""]]}, {"id": "1807.01016", "submitter": "JunPing Wang", "authors": "JunPing Wang, WenSheng Zhang, YouKang Shi, ShiHui Duan, Jin Liu", "title": "Industrial Big Data Analytics: Challenges, Methodologies, and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While manufacturers have been generating highly distributed data from various\nsystems, devices and applications, a number of challenges in both data\nmanagement and data analysis require new approaches to support the big data\nera. These challenges for industrial big data analytics is real-time analysis\nand decision-making from massive heterogeneous data sources in manufacturing\nspace. This survey presents new concepts, methodologies, and applications\nscenarios of industrial big data analytics, which can provide dramatic\nimprovements in velocity and veracity problem solving. We focus on five\nimportant methodologies of industrial big data analytics: 1) Highly distributed\nindustrial data ingestion: access and integrate to highly distributed data\nsources from various systems, devices and applications; 2) Industrial big data\nrepository: cope with sampling biases and heterogeneity, and store different\ndata formats and structures; 3) Large-scale industrial data management:\norganizes massive heterogeneous data and share large-scale data; 4) Industrial\ndata analytics: track data provenance, from data generation through data\npreparation; 5) Industrial data governance: ensures data trust, integrity and\nsecurity. For each phase, we introduce to current research in industries and\nacademia, and discusses challenges and potential solutions. We also examine the\ntypical applications of industrial big data, including smart factory\nvisibility, machine fleet, energy management, proactive maintenance, and just\nin time supply chain. These discussions aim to understand the value of\nindustrial big data. Lastly, this survey is concluded with a discussion of open\nproblems and future directions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 08:24:56 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 02:46:35 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Wang", "JunPing", ""], ["Zhang", "WenSheng", ""], ["Shi", "YouKang", ""], ["Duan", "ShiHui", ""], ["Liu", "Jin", ""]]}, {"id": "1807.01367", "submitter": "Phuc Nguyen Tri", "authors": "Phuc Nguyen, Khai Nguyen, Ryutaro Ichise, Hideaki Takeda", "title": "EmbNum: Semantic labeling for numerical values with deep metric learning", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic labeling for numerical values is a task of assigning semantic labels\nto unknown numerical attributes. The semantic labels could be numerical\nproperties in ontologies, instances in knowledge bases, or labeled data that\nare manually annotated by domain experts. In this paper, we refer to semantic\nlabeling as a retrieval setting where the label of an unknown attribute is\nassigned by the label of the most relevant attribute in labeled data. One of\nthe greatest challenges is that an unknown attribute rarely has the same set of\nvalues with the similar one in the labeled data. To overcome the issue,\nstatistical interpretation of value distribution is taken into account.\nHowever, the existing studies assume a specific form of distribution. It is not\nappropriate in particular to apply open data where there is no knowledge of\ndata in advance. To address these problems, we propose a neural numerical\nembedding model (EmbNum) to learn useful representation vectors for numerical\nattributes without prior assumptions on the distribution of data. Then, the\n\"semantic similarities\" between the attributes are measured on these\nrepresentation vectors by the Euclidean distance. Our empirical experiments on\nCity Data and Open Data show that EmbNum significantly outperforms\nstate-of-the-art methods for the task of numerical attribute semantic labeling\nregarding effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 03:51:53 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 06:21:56 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Nguyen", "Phuc", ""], ["Nguyen", "Khai", ""], ["Ichise", "Ryutaro", ""], ["Takeda", "Hideaki", ""]]}, {"id": "1807.01706", "submitter": "Esther Galbrun", "authors": "Esther Galbrun and Peggy Cellier and Nikolaj Tatti and Alexandre\n  Termier and Bruno Cr\\'emilleux", "title": "Mining Periodic Patterns with a MDL Criterion", "comments": "This report extends the conference version (at ECML-PKDD'18) with\n  technical details, numerous examples, and additional experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantity of event logs available is increasing rapidly, be they produced\nby industrial processes, computing systems, or life tracking, for instance. It\nis thus important to design effective ways to uncover the information they\ncontain. Because event logs often record repetitive phenomena, mining periodic\npatterns is especially relevant when considering such data. Indeed, capturing\nsuch regularities is instrumental in providing condensed representations of the\nevent sequences.\n  We present an approach for mining periodic patterns from event logs while\nrelying on a Minimum Description Length (MDL) criterion to evaluate candidate\npatterns. Our goal is to extract a set of patterns that suitably characterises\nthe periodic structure present in the data. We evaluate the interest of our\napproach on several real-world event log datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 13:21:19 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Galbrun", "Esther", ""], ["Cellier", "Peggy", ""], ["Tatti", "Nikolaj", ""], ["Termier", "Alexandre", ""], ["Cr\u00e9milleux", "Bruno", ""]]}, {"id": "1807.01948", "submitter": "Roly Perera", "authors": "Rudi Horn and Roly Perera and James Cheney", "title": "Incremental Relational Lenses", "comments": "To appear, ICFP 2018", "journal-ref": null, "doi": "10.1145/3236769", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lenses are a popular approach to bidirectional transformations, a\ngeneralisation of the view update problem in databases, in which we wish to\nmake changes to source tables to effect a desired change on a view. However,\nperhaps surprisingly, lenses have seldom actually been used to implement\nupdatable views in databases. Bohannon, Pierce and Vaughan proposed an approach\nto updatable views called relational lenses, but to the best of our knowledge\nthis proposal has not been implemented or evaluated to date. We propose\nincremental relational lenses, that equip relational lenses with\nchange-propagating semantics that map small changes to the view to\n(potentially) small changes to the source tables. We also present a\nlanguage-integrated implementation of relational lenses and a detailed\nexperimental evaluation, showing orders of magnitude improvement over the\nnon-incremental approach. Our work shows that relational lenses can be used to\nsupport expressive and efficient view updates at the language level, without\nrelying on updatable view support from the underlying database.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 11:42:04 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 12:16:05 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Horn", "Rudi", ""], ["Perera", "Roly", ""], ["Cheney", "James", ""]]}, {"id": "1807.02258", "submitter": "Aswani Kumar Cherukuri Dr", "authors": "Raghavendra K Chunduri, Aswani Kumar Cherukuri", "title": "Scalable Formal Concept Analysis algorithm for large datasets using\n  Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the process of knowledge discovery and representation in large datasets\nusing formal concept analysis, complexity plays a major role in identifying all\nthe formal concepts and constructing the concept lattice(digraph of the\nconcepts). For identifying the formal concepts and constructing the digraph\nfrom the identified concepts in very large datasets, various distributed\nalgorithms are available in the literature. However, the existing distributed\nalgorithms are not very well suitable for concept generation because it is an\niterative process. The existing algorithms are implemented using distributed\nframeworks like MapReduce and Open MP, these frameworks are not appropriate for\niterative applications. Hence, in this paper we proposed efficient distributed\nalgorithms for both formal concept generation and concept lattice digraph\nconstruction in large formal contexts using Apache Spark. Various performance\nmetrics are considered for the evaluation of the proposed work, the results of\nthe evaluation proves that the proposed algorithms are efficient for concept\ngeneration and lattice graph construction in comparison with the existing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 05:22:31 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Chunduri", "Raghavendra K", ""], ["Cherukuri", "Aswani Kumar", ""]]}, {"id": "1807.02262", "submitter": "Charini Nanayakkara", "authors": "Charini Nanayakkara, Peter Christen and Thilina Ranbaduge", "title": "Temporal graph-based clustering for historical record linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in the social sciences is increasingly based on large and complex\ndata collections, where individual data sets from different domains are linked\nand integrated to allow advanced analytics. A popular type of data used in such\na context are historical censuses, as well as birth, death, and marriage\ncertificates. Individually, such data sets however limit the types of studies\nthat can be conducted. Specifically, it is impossible to track individuals,\nfamilies, or households over time. Once such data sets are linked and family\ntrees spanning several decades are available it is possible to, for example,\ninvestigate how education, health, mobility, employment, and social status\ninfluence each other and the lives of people over two or even more generations.\nA major challenge is however the accurate linkage of historical data sets which\nis due to data quality and commonly also the lack of ground truth data being\navailable. Unsupervised techniques need to be employed, which can be based on\nsimilarity graphs generated by comparing individual records. In this paper we\npresent initial results from clustering birth records from Scotland where we\naim to identify all births of the same mother and group siblings into clusters.\nWe extend an existing clustering technique for record linkage by incorporating\ntemporal constraints that must hold between births by the same mother, and\npropose a novel greedy temporal clustering technique. Experimental results show\nimprovements over non-temporary approaches, however further work is needed to\nobtain links of high quality.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 05:51:48 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Nanayakkara", "Charini", ""], ["Christen", "Peter", ""], ["Ranbaduge", "Thilina", ""]]}, {"id": "1807.02957", "submitter": "Ariyam Das", "authors": "Tyson Condie, Ariyam Das, Matteo Interlandi, Alexander Shkapsky, Mohan\n  Yang, Carlo Zaniolo", "title": "Scaling-Up Reasoning and Advanced Analytics on BigData", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BigDatalog is an extension of Datalog that achieves performance and\nscalability on both Apache Spark and multicore systems to the point that its\ngraph analytics outperform those written in GraphX. Looking back, we see how\nthis realizes the ambitious goal pursued by deductive database researchers\nbeginning forty years ago: this is the goal of combining the rigor and power of\nlogic in expressing queries and reasoning with the performance and scalability\nby which relational databases managed Big Data. This goal led to Datalog which\nis based on Horn Clauses like Prolog but employs implementation techniques,\nsuch as Semi-naive Fixpoint and Magic Sets, that extend the bottom-up\ncomputation model of relational systems, and thus obtain the performance and\nscalability that relational systems had achieved, as far back as the 80s, using\ndata-parallelization on shared-nothing architectures. But this goal proved\ndifficult to achieve because of major issues at (i) the language level and (ii)\nat the system level. The paper describes how (i) was addressed by simple rules\nunder which the fixpoint semantics extends to programs using count, sum and\nextrema in recursion, and (ii) was tamed by parallel compilation techniques\nthat achieve scalability on multicore systems and Apache Spark. This paper is\nunder consideration for acceptance in Theory and Practice of Logic Programming\n(TPLP).\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 06:40:08 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Condie", "Tyson", ""], ["Das", "Ariyam", ""], ["Interlandi", "Matteo", ""], ["Shkapsky", "Alexander", ""], ["Yang", "Mohan", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1807.03100", "submitter": "Oleksandr Polozov", "authors": "Chenglong Wang, Kedar Tatwawadi, Marc Brockschmidt, Po-Sen Huang, Yi\n  Mao, Oleksandr Polozov, Rishabh Singh", "title": "Robust Text-to-SQL Generation with Execution-Guided Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of neural semantic parsing, which translates natural\nlanguage questions into executable SQL queries. We introduce a new mechanism,\nexecution guidance, to leverage the semantics of SQL. It detects and excludes\nfaulty programs during the decoding procedure by conditioning on the execution\nof partially generated program. The mechanism can be used with any\nautoregressive generative model, which we demonstrate on four state-of-the-art\nrecurrent or template-based semantic parsing models. We demonstrate that\nexecution guidance universally improves model performance on various\ntext-to-SQL datasets with different scales and query complexity: WikiSQL, ATIS,\nand GeoQuery. As a result, we achieve new state-of-the-art execution accuracy\nof 83.8% on WikiSQL.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 13:20:28 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2018 21:55:52 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 00:29:17 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Wang", "Chenglong", ""], ["Tatwawadi", "Kedar", ""], ["Brockschmidt", "Marc", ""], ["Huang", "Po-Sen", ""], ["Mao", "Yi", ""], ["Polozov", "Oleksandr", ""], ["Singh", "Rishabh", ""]]}, {"id": "1807.04035", "submitter": "Jerome Darmont", "authors": "Iuri Nogueira (UL2), Maram Romdhane (UL2), J\\'er\\^ome Darmont (ERIC)", "title": "Modeling Data Lake Metadata with a Data Vault", "comments": null, "journal-ref": "22nd International Database Engineering & Applications Symposium\n  (IDEAS 2018), Jun 2018, Villa San Giovanni, Italy. ACM, pp.253-261, 2018,\n  http://confsys.encs.concordia.ca/IDEAS/ideas18/ideas18.php", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of big data, business intelligence had to find solutions for\nmanaging even greater data volumes and variety than in data warehouses, which\nproved ill-adapted. Data lakes answer these needs from a storage point of view,\nbut require managing adequate metadata to guarantee an efficient access to\ndata. Starting from a multidimensional metadata model designed for an\nindustrial heritage data lake presenting a lack of schema evolutivity, we\npropose in this paper to use ensemble modeling, and more precisely a data\nvault, to address this issue. To illustrate the feasibility of this approach,\nwe instantiate our metadata conceptual model into relational and\ndocument-oriented logical and physical models, respectively. We also compare\nthe physical models in terms of metadata storage and query response time.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 09:36:34 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Nogueira", "Iuri", "", "UL2"], ["Romdhane", "Maram", "", "UL2"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1807.04639", "submitter": "Harris Georgiou", "authors": "Harris Georgiou, Sophia Karagiorgou, Yannis Kontoulis, Nikos Pelekis,\n  Petros Petrou, David Scarlatti, Yannis Theodoridis", "title": "Moving Objects Analytics: Survey on Future Location & Trajectory\n  Prediction Methods", "comments": "45 pages, 11 figures, 2 tables, 127 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The tremendous growth of positioning technologies and GPS enabled devices has\nproduced huge volumes of tracking data during the recent years. This source of\ninformation constitutes a rich input for data analytics processes, either\noffline (e.g. cluster analysis, hot motion discovery) or online (e.g.\nshort-term forecasting of forthcoming positions). This paper focuses on\npredictive analytics for moving objects (could be pedestrians, cars, vessels,\nplanes, animals, etc.) and surveys the state-of-the-art in the context of\nfuture location and trajectory prediction. We provide an extensive review of\nover 50 works, also proposing a novel taxonomy of predictive algorithms over\nmoving objects. We also list the properties of several real datasets used in\nthe past for validation purposes of those works and, motivated by this, we\ndiscuss challenges that arise in the transition from conventional to Big Data\napplications.\n  CCS Concepts: Information systems > Spatial-temporal systems; Information\nsystems > Data analytics; Information systems > Data mining; Computing\nmethodologies > Machine learning Additional Key Words and Phrases: mobility\ndata, moving object trajectories, trajectory prediction, future location\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 08:01:38 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Georgiou", "Harris", ""], ["Karagiorgou", "Sophia", ""], ["Kontoulis", "Yannis", ""], ["Pelekis", "Nikos", ""], ["Petrou", "Petros", ""], ["Scarlatti", "David", ""], ["Theodoridis", "Yannis", ""]]}, {"id": "1807.05258", "submitter": "Abolfazl Asudeh", "authors": "Yeshwanth D. Gunasekaran, Abolfazl Asudeh, Sona Hasani, Nan Zhang, Ali\n  Jaoua, Gautam Das", "title": "QR2: A Third-party Query Reranking Service Over Web Databases", "comments": "34th IEEE International Conference on Data Engineering (ICDE Demo),\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ranked retrieval model has rapidly become the de-facto way for search\nquery processing in web databases. Despite the extensive efforts on designing\nbetter ranking mechanisms, in practice, many such databases fail to address the\ndiverse and sometimes contradicting preferences of users. In this paper, we\npresent QR2, a third-party service that uses nothing but the public search\ninterface of a web database and enables the on-the-fly processing of queries\nwith any user-specified ranking functions, no matter if the ranking function is\nsupported by the database or not.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 19:31:08 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Gunasekaran", "Yeshwanth D.", ""], ["Asudeh", "Abolfazl", ""], ["Hasani", "Sona", ""], ["Zhang", "Nan", ""], ["Jaoua", "Ali", ""], ["Das", "Gautam", ""]]}, {"id": "1807.05308", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Ron Brightwell, Alan Edelman, Vijay Gadepally, Hayden\n  Jananthan, Michael Jones, Sam Madden, Peter Michaleas, Hamed Okhravi, Kevin\n  Pedretti, Albert Reuther, Thomas Sterling, Mike Stonebraker", "title": "TabulaROSA: Tabular Operating System Architecture for Massively Parallel\n  Heterogeneous Compute Engines", "comments": "8 pages, 6 figures, accepted at IEEE HPEC 2018", "journal-ref": null, "doi": "10.1109/HPEC.2018.8547577", "report-no": null, "categories": "cs.DC cs.DB cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise in computing hardware choices is driving a reevaluation of operating\nsystems. The traditional role of an operating system controlling the execution\nof its own hardware is evolving toward a model whereby the controlling\nprocessor is distinct from the compute engines that are performing most of the\ncomputations. In this context, an operating system can be viewed as software\nthat brokers and tracks the resources of the compute engines and is akin to a\ndatabase management system. To explore the idea of using a database in an\noperating system role, this work defines key operating system functions in\nterms of rigorous mathematical semantics (associative array algebra) that are\ndirectly translatable into database operations. These operations possess a\nnumber of mathematical properties that are ideal for parallel operating systems\nby guaranteeing correctness over a wide range of parallel operations. The\nresulting operating system equations provide a mathematical specification for a\nTabular Operating System Architecture (TabulaROSA) that can be implemented on\nany platform. Simulations of forking in TabularROSA are performed using an\nassociative array implementation and compared to Linux on a 32,000+ core\nsupercomputer. Using over 262,000 forkers managing over 68,000,000,000\nprocesses, the simulations show that TabulaROSA has the potential to perform\noperating system functions on a massively parallel scale. The TabulaROSA\nsimulations show 20x higher performance as compared to Linux while managing\n2000x more processes in fully searchable tables.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 00:02:55 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kepner", "Jeremy", ""], ["Brightwell", "Ron", ""], ["Edelman", "Alan", ""], ["Gadepally", "Vijay", ""], ["Jananthan", "Hayden", ""], ["Jones", "Michael", ""], ["Madden", "Sam", ""], ["Michaleas", "Peter", ""], ["Okhravi", "Hamed", ""], ["Pedretti", "Kevin", ""], ["Reuther", "Albert", ""], ["Sterling", "Thomas", ""], ["Stonebraker", "Mike", ""]]}, {"id": "1807.05351", "submitter": "Gustavo Publio", "authors": "Gustavo Correa Publio, Diego Esteves, Agnieszka {\\L}awrynowicz,\n  Pan\\v{c}e Panov, Larisa Soldatova, Tommaso Soru, Joaquin Vanschoren, Hamid\n  Zafar", "title": "ML-Schema: Exposing the Semantics of Machine Learning with Schemas and\n  Ontologies", "comments": "Poster, selected for the 2nd Reproducibility in Machine Learning\n  Workshop at ICML 2018, Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ML-Schema, proposed by the W3C Machine Learning Schema Community Group,\nis a top-level ontology that provides a set of classes, properties, and\nrestrictions for representing and interchanging information on machine learning\nalgorithms, datasets, and experiments. It can be easily extended and\nspecialized and it is also mapped to other more domain-specific ontologies\ndeveloped in the area of machine learning and data mining. In this paper we\noverview existing state-of-the-art machine learning interchange formats and\npresent the first release of ML-Schema, a canonical format resulted of more\nthan seven years of experience among different research institutions. We argue\nthat exposing semantics of machine learning algorithms, models, and experiments\nthrough a canonical format may pave the way to better interpretability and to\nrealistically achieve the full interoperability of experiments regardless of\nplatform or adopted workflow solution.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 08:07:31 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Publio", "Gustavo Correa", ""], ["Esteves", "Diego", ""], ["\u0141awrynowicz", "Agnieszka", ""], ["Panov", "Pan\u010de", ""], ["Soldatova", "Larisa", ""], ["Soru", "Tommaso", ""], ["Vanschoren", "Joaquin", ""], ["Zafar", "Hamid", ""]]}, {"id": "1807.05614", "submitter": "Martin Aum\\\"uller", "authors": "Martin Aum\\\"uller, Erik Bernhardsson, Alexander Faithfull", "title": "ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor\n  Algorithms", "comments": "Full version of the SISAP 2017 conference paper. v2: Updated the\n  abstract to avoid arXiv linking to the wrong URL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes ANN-Benchmarks, a tool for evaluating the performance of\nin-memory approximate nearest neighbor algorithms. It provides a standard\ninterface for measuring the performance and quality achieved by nearest\nneighbor algorithms on different standard data sets. It supports several\ndifferent ways of integrating $k$-NN algorithms, and its configuration system\nautomatically tests a range of parameter settings for each algorithm.\nAlgorithms are compared with respect to many different (approximate) quality\nmeasures, and adding more is easy and fast; the included plotting front-ends\ncan visualise these as images, $\\LaTeX$ plots, and websites with interactive\nplots. ANN-Benchmarks aims to provide a constantly updated overview of the\ncurrent state of the art of $k$-NN algorithms. In the short term, this overview\nallows users to choose the correct $k$-NN algorithm and parameters for their\nsimilarity search task; in the longer term, algorithm designers will be able to\nuse this overview to test and refine automatic parameter tuning. The paper\ngives an overview of the system, evaluates the results of the benchmark, and\npoints out directions for future work. Interestingly, very different approaches\nto $k$-NN search yield comparable quality-performance trade-offs. The system is\navailable at http://ann-benchmarks.com .\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 21:25:55 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 20:45:47 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Bernhardsson", "Erik", ""], ["Faithfull", "Alexander", ""]]}, {"id": "1807.05722", "submitter": "Hatem Hajri", "authors": "Hatem Hajri, Emmanuel Doucet, Marc Revilloud, Lynda Halit, Beno\\^it\n  Lusetti, Mohamed-Cherif Rahal", "title": "Automatic generation of ground truth for the evaluation of obstacle\n  detection and tracking techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As automated vehicles are getting closer to becoming a reality, it will\nbecome mandatory to be able to characterise the performance of their obstacle\ndetection systems. This validation process requires large amounts of\nground-truth data, which is currently generated by manually annotation. In this\npaper, we propose a novel methodology to generate ground-truth kinematics\ndatasets for specific objects in real-world scenes. Our procedure requires no\nannotation whatsoever, human intervention being limited to sensors calibration.\nWe present the recording platform which was exploited to acquire the reference\ndata and a detailed and thorough analytical study of the propagation of errors\nin our procedure. This allows us to provide detailed precision metrics for each\nand every data item in our datasets. Finally some visualisations of the\nacquired data are given.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 08:20:40 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Hajri", "Hatem", ""], ["Doucet", "Emmanuel", ""], ["Revilloud", "Marc", ""], ["Halit", "Lynda", ""], ["Lusetti", "Beno\u00eet", ""], ["Rahal", "Mohamed-Cherif", ""]]}, {"id": "1807.06068", "submitter": "Yeounoh Chung", "authors": "Yeounoh Chung, Tim Kraska, Neoklis Polyzotis, Ki Hyun Tae, Steven\n  Euijong Whang", "title": "Automated Data Slicing for Model Validation:A Big data - AI Integration\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning systems become democratized, it becomes increasingly\nimportant to help users easily debug their models. However, current data tools\nare still primitive when it comes to helping users trace model performance\nproblems all the way to the data. We focus on the particular problem of slicing\ndata to identify subsets of the validation data where the model performs\npoorly. This is an important problem in model validation because the overall\nmodel performance can fail to reflect that of the smaller subsets, and slicing\nallows users to analyze the model performance on a more granular-level. Unlike\ngeneral techniques (e.g., clustering) that can find arbitrary slices, our goal\nis to find interpretable slices (which are easier to take action compared to\narbitrary subsets) that are problematic and large. We propose Slice Finder,\nwhich is an interactive framework for identifying such slices using statistical\ntechniques. Applications include diagnosing model fairness and fraud detection,\nwhere identifying slices that are interpretable to humans is crucial. This\nresearch is part of a larger trend of Big data and Artificial Intelligence (AI)\nintegration and opens many opportunities for new research.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 19:21:24 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 15:36:08 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2019 01:01:26 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Chung", "Yeounoh", ""], ["Kraska", "Tim", ""], ["Polyzotis", "Neoklis", ""], ["Tae", "Ki Hyun", ""], ["Whang", "Steven Euijong", ""]]}, {"id": "1807.06070", "submitter": "Sudhakar Singh", "authors": "Sudhakar Singh, Rakhi Garg, P K Mishra", "title": "Performance Optimization of MapReduce-based Apriori Algorithm on Hadoop\n  Cluster", "comments": "24 pages, 5 figures, 12 tables, 5 algorithms", "journal-ref": "Computers & Electrical Engineering 2018; 67: 348-364", "doi": "10.1016/j.compeleceng.2017.10.008", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many techniques have been proposed to implement the Apriori algorithm on\nMapReduce framework but only a few have focused on performance improvement. FPC\n(Fixed Passes Combined-counting) and DPC (Dynamic Passes Combined-counting)\nalgorithms combine multiple passes of Apriori in a single MapReduce phase to\nreduce the execution time. In this paper, we propose improved MapReduce based\nApriori algorithms VFPC (Variable Size based Fixed Passes Combined-counting)\nand ETDPC (Elapsed Time based Dynamic Passes Combined-counting) over FPC and\nDPC. Further, we optimize the multi-pass phases of these algorithms by skipping\npruning step in some passes, and propose Optimized-VFPC and Optimized-ETDPC\nalgorithms. Quantitative analysis reveals that counting cost of additional\nun-pruned candidates produced due to skipped-pruning is less significant than\nreduction in computation cost due to the same. Experimental results show that\nVFPC and ETDPC are more robust and flexible than FPC and DPC whereas their\noptimized versions are more efficient in terms of execution time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 19:30:50 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Singh", "Sudhakar", ""], ["Garg", "Rakhi", ""], ["Mishra", "P K", ""]]}, {"id": "1807.06996", "submitter": "Choiru Za'in", "authors": "Mahardhika Pratama, Choiru Za'in, Eric Pardede", "title": "Evolving Large-Scale Data Stream Analytics based on Scalable PANFIS", "comments": "20 pages, 5 figures", "journal-ref": "Knowledge-based System, 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed machine learning frameworks have recently been built to\nspeed up the large-scale data learning process. However, most distributed\nmachine learning used in these frameworks still uses an offline algorithm model\nwhich cannot cope with the data stream problems. In fact, large-scale data are\nmostly generated by the non-stationary data stream where its pattern evolves\nover time. To address this problem, we propose a novel Evolving Large-scale\nData Stream Analytics framework based on a Scalable Parsimonious Network based\non Fuzzy Inference System (Scalable PANFIS), where the PANFIS evolving\nalgorithm is distributed over the worker nodes in the cloud to learn\nlarge-scale data stream. Scalable PANFIS framework incorporates the active\nlearning (AL) strategy and two model fusion methods. The AL accelerates the\ndistributed learning process to generate an initial evolving large-scale data\nstream model (initial model), whereas the two model fusion methods aggregate an\ninitial model to generate the final model. The final model represents the\nupdate of current large-scale data knowledge which can be used to infer future\ndata. Extensive experiments on this framework are validated by measuring the\naccuracy and running time of four combinations of Scalable PANFIS and other\nSpark-based built in algorithms. The results indicate that Scalable PANFIS with\nAL improves the training time to be almost two times faster than Scalable\nPANFIS without AL. The results also show both rule merging and the voting\nmechanisms yield similar accuracy in general among Scalable PANFIS algorithms\nand they are generally better than Spark-based algorithms. In terms of running\ntime, the Scalable PANFIS training time outperforms all Spark-based algorithms\nwhen classifying numerous benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 15:36:06 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Za'in", "Choiru", ""], ["Pardede", "Eric", ""]]}, {"id": "1807.07346", "submitter": "Esteban Garc\\'ia-Cuesta Dr.", "authors": "Esteban Garc\\'ia-Cuesta (Data Science Laboratory, School of\n  Arquitecture, Engineering and Design, Universidad Europea de Madrid, Spain),\n  Jos\\'e M. G\\'omez-P\\'erez (Expert System, Spain)", "title": "Indexing Execution Patterns in Workflow Provenance Graphs through\n  Generalized Trie Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, scientific workflows have become mature enough to be\nused in a production style. However, despite the increasing maturity, there is\nstill a shortage of tools for searching, adapting, and reusing workflows that\nhinders a more generalized adoption by the scientific communities. Indeed, due\nto the limited availability of machine-readable scientific metadata and the\nheterogeneity of workflow specification formats and representations, new ways\nto leverage alternative sources of information that complement existing\napproaches are needed. In this paper we address such limitations by applying\nstatistically enriched generalized trie structures to exploit workflow\nexecution provenance information in order to assist the analysis, indexing and\nsearch of scientific workflows. Our method bridges the gap between the\ndescription of what a workflow is supposed to do according to its specification\nand related metadata and what it actually does as recorded in its provenance\nexecution trace. In doing so, we also prove that the proposed method\noutperforms SPARQL 1.1 Property Paths for querying provenance graphs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 11:29:40 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Garc\u00eda-Cuesta", "Esteban", "", "Data Science Laboratory, School of\n  Arquitecture, Engineering and Design, Universidad Europea de Madrid, Spain"], ["G\u00f3mez-P\u00e9rez", "Jos\u00e9 M.", "", "Expert System, Spain"]]}, {"id": "1807.07691", "submitter": "Xiaowang Zhang", "authors": "Xiaowang Zhang and Mingyue Zhang and Peng Peng and Jiaming Song and\n  Zhiyong Feng and Lei Zou", "title": "gSMat: A Scalable Sparse Matrix-based Join for SPARQL Query Processing", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource Description Framework (RDF) has been widely used to represent\ninformation on the web, while SPARQL is a standard query language to manipulate\nRDF data. Given a SPARQL query, there often exist many joins which are the\nbottlenecks of efficiency of query processing. Besides, the real RDF datasets\noften reveal strong data sparsity, which indicates that a resource often only\nrelates to a few resources even the number of total resources is large. In this\npaper, we propose a sparse matrix-based (SM-based) SPARQL query processing\napproach over RDF datasets which con- siders both join optimization and data\nsparsity. Firstly, we present a SM-based storage for RDF datasets to lift the\nstorage efficiency, where valid edges are stored only, and then introduce a\npredicate- based hash index on the storage. Secondly, we develop a scalable\nSM-based join algorithm for SPARQL query processing. Finally, we analyze the\noverall cost by accumulating all intermediate results and design a query plan\ngenerated algorithm. Besides, we extend our SM-based join algorithm on GPU for\nparallelizing SPARQL query processing. We have evaluated our approach compared\nwith the state-of-the-art RDF engines over benchmark RDF datasets and the\nexperimental results show that our proposal can significantly improve SPARQL\nquery processing with high scalability.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 01:52:45 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Zhang", "Xiaowang", ""], ["Zhang", "Mingyue", ""], ["Peng", "Peng", ""], ["Song", "Jiaming", ""], ["Feng", "Zhiyong", ""], ["Zou", "Lei", ""]]}, {"id": "1807.07779", "submitter": "Vuong M. Ngo", "authors": "Vuong M. Ngo and Tru H. Cao", "title": "A Generalized Vector Space Model for Ontology-Based Information\n  Retrieval", "comments": "5 pages, in Vietnamese. information retrieval, vector space model,\n  ontology, named entity, keyword. Accepted by Vietnamese Journal on\n  Information Technologies and Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entities (NE) are objects that are referred to by names such as people,\norganizations and locations. Named entities and keywords are important to the\nmeaning of a document. We propose a generalized vector space model that\ncombines named entities and keywords. In the model, we take into account\ndifferent ontological features of named entities, namely, aliases, classes and\nidentifiers. Moreover, we use entity classes to represent the latent\ninformation of interrogative words in Wh-queries, which are ignored in\ntraditional keyword-based searching. We have implemented and tested the\nproposed model on a TREC dataset, as presented and discussed in the paper.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 10:37:31 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Ngo", "Vuong M.", ""], ["Cao", "Tru H.", ""]]}, {"id": "1807.08267", "submitter": "Florin Stoica", "authors": "Florin Stoica, Laura Florentina Stoica", "title": "Generating an ATL Model Checker using an Attribute Grammar", "comments": "18 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use attribute grammars as a formal approach for model\ncheckers development. Our aim is to design an ATL (Alternating-Time Temporal\nLogic) model checker from a context-free grammar which generates the language\nof the ATL formulas. An attribute grammar may be informally defined as a\ncontext-free grammar which is extended with a set of attributes and a\ncollection of semantic rules. We use an ATL attribute grammar for specifying an\noperational semantics of the language of the ATL formulas by defining a\ntranslation into the language which describes the set of states from the ATL\nmodel where the corresponding ATL formulas are satisfied. We provide a formal\ndefinition for an attribute grammar used as input for Another Tool for Language\nRecognition (ANTLR) to generate an ATL model checker. Also, the technique of\nimplementing the semantic actions in ANTLR is presented, which is the concept\nof connection between attribute evaluation in the grammar that generates the\nlanguage of ATL formulas and algebraic compiler implementation that represents\nthe ATL model checker. The original implementation of the model checking\nalgorithm is based on Relational Databases and Web Services. Several database\nsystems and Web Services technologies were used for evaluating the system\nperformance in verification of large ATL models.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 10:18:12 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:54:54 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Stoica", "Florin", ""], ["Stoica", "Laura Florentina", ""]]}, {"id": "1807.08456", "submitter": "Yusuke Kawamoto", "authors": "Yusuke Kawamoto and Takao Murakami", "title": "On the Anonymization of Differentially Private Location Obfuscation", "comments": "ISITA'18 conference paper", "journal-ref": null, "doi": "10.23919/ISITA.2018.8664351", "report-no": null, "categories": "cs.CR cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obfuscation techniques in location-based services (LBSs) have been shown\nuseful to hide the concrete locations of service users, whereas they do not\nnecessarily provide the anonymity. We quantify the anonymity of the location\ndata obfuscated by the planar Laplacian mechanism and that by the optimal\ngeo-indistinguishable mechanism of Bordenabe et al. We empirically show that\nthe latter provides stronger anonymity than the former in the sense that more\nusers in the database satisfy k-anonymity. To formalize and analyze such\napproximate anonymity we introduce the notion of asymptotic anonymity. Then we\nshow that the location data obfuscated by the optimal geo-indistinguishable\nmechanism can be anonymized by removing a smaller number of users from the\ndatabase. Furthermore, we demonstrate that the optimal geo-indistinguishable\nmechanism has better utility both for users and for data analysts.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 07:12:51 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Kawamoto", "Yusuke", ""], ["Murakami", "Takao", ""]]}, {"id": "1807.08461", "submitter": "Wei Emma Zhang", "authors": "Wei Emma Zhang, Quan Z. Sheng, Schahram Dustdar", "title": "A Cache-based Optimizer for Querying Enhanced Knowledge Bases", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent emerging technologies such as the Internet of Things (IoT),\ninformation collection on our physical world and environment can be achieved at\na much higher granularity and such detailed knowledge will play a critical role\nin improving the productivity, operational effectiveness, decision making, and\nin identifying new business models for economic growth. Efficient discovery and\nquerying such knowledge remains a key challenge due to the limited capability\nand high latency of connections to the interfaces of knowledge bases, e.g., the\nSPARQL endpoints. In this article, we present a querying system on SPARQL\nendpoints for knowledge bases that performs queries faster than the\nstate-of-the-art systems. Our system features a cache-based optimization scheme\nto improve querying performance by prefetching and caching the results of\npredicted potential queries. The evaluations on query sets from SPARQL\nendpoints of DBpedia and Linked GeoData showcase the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 07:47:32 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Zhang", "Wei Emma", ""], ["Sheng", "Quan Z.", ""], ["Dustdar", "Schahram", ""]]}, {"id": "1807.08709", "submitter": "Emanuel Sallinger", "authors": "Luigi Bellomarini, Georg Gottlob, Emanuel Sallinger", "title": "The Vadalog System: Datalog-based Reasoning for Knowledge Graphs", "comments": "Extended version of VLDB paper\n  <https://doi.org/10.14778/3213880.3213888>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, there has been a resurgence of Datalog-based systems in\nthe database community as well as in industry. In this context, it has been\nrecognized that to handle the complex knowl\\-edge-based scenarios encountered\ntoday, such as reasoning over large knowledge graphs, Datalog has to be\nextended with features such as existential quantification. Yet, Datalog-based\nreasoning in the presence of existential quantification is in general\nundecidable. Many efforts have been made to define decidable fragments. Warded\nDatalog+/- is a very promising one, as it captures PTIME complexity while\nallowing ontological reasoning. Yet so far, no implementation of Warded\nDatalog+/- was available. In this paper we present the Vadalog system, a\nDatalog-based system for performing complex logic reasoning tasks, such as\nthose required in advanced knowledge graphs. The Vadalog system is Oxford's\ncontribution to the VADA research programme, a joint effort of the universities\nof Oxford, Manchester and Edinburgh and around 20 industrial partners. As the\nmain contribution of this paper, we illustrate the first implementation of\nWarded Datalog+/-, a high-performance Datalog+/- system utilizing an aggressive\ntermination control strategy. We also provide a comprehensive experimental\nevaluation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:38:05 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Bellomarini", "Luigi", ""], ["Gottlob", "Georg", ""], ["Sallinger", "Emanuel", ""]]}, {"id": "1807.08712", "submitter": "Emanuel Sallinger", "authors": "Luigi Bellomarini, Ruslan R. Fayzrakhmanov, Georg Gottlob, Andrey\n  Kravchenko, Eleonora Laurenza, Yavor Nenov, Stephane Reissfelder, Emanuel\n  Sallinger, Evgeny Sherkhonov, Lianlong Wu", "title": "Data Science with Vadalog: Bridging Machine Learning and Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the recent successful examples of large technology companies, many\nmodern enterprises seek to build knowledge graphs to provide a unified view of\ncorporate knowledge and to draw deep insights using machine learning and\nlogical reasoning. There is currently a perceived disconnect between the\ntraditional approaches for data science, typically based on machine learning\nand statistical modelling, and systems for reasoning with domain knowledge. In\nthis paper we present a state-of-the-art Knowledge Graph Management System,\nVadalog, which delivers highly expressive and efficient logical reasoning and\nprovides seamless integration with modern data science toolkits, such as the\nJupyter platform. We demonstrate how to use Vadalog to perform traditional data\nwrangling tasks, as well as complex logical and probabilistic reasoning. We\nargue that this is a significant step forward towards combining machine\nlearning and reasoning in data science.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:40:37 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Bellomarini", "Luigi", ""], ["Fayzrakhmanov", "Ruslan R.", ""], ["Gottlob", "Georg", ""], ["Kravchenko", "Andrey", ""], ["Laurenza", "Eleonora", ""], ["Nenov", "Yavor", ""], ["Reissfelder", "Stephane", ""], ["Sallinger", "Emanuel", ""], ["Sherkhonov", "Evgeny", ""], ["Wu", "Lianlong", ""]]}, {"id": "1807.08804", "submitter": "Erik Cambria", "authors": "Nguyen Ha Tran and Erik Cambria", "title": "GPU-based Commonsense Paradigms Reasoning for Real-Time Query Answering\n  and Multimodal Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize commonsense knowledge bases to address the problem of real- time\nmultimodal analysis. In particular, we focus on the problem of multimodal\nsentiment analysis, which consists in the simultaneous analysis of different\nmodali- ties, e.g., speech and video, for emotion and polarity detection. Our\napproach takes advantages of the massively parallel processing power of modern\nGPUs to enhance the performance of feature extraction from different\nmodalities. In addition, in order to ex- tract important textual features from\nmultimodal sources we generate domain-specific graphs based on commonsense\nknowledge and apply GPU-based graph traversal for fast feature detection. Then,\npowerful ELM classifiers are applied to build the senti- ment analysis model\nbased on the extracted features. We conduct our experiments on the YouTube\ndataset and achieve an accuracy of 78% which outperforms all previous systems.\nIn term of processing speed, our method shows improvements of several orders of\nmagnitude for feature extraction compared to CPU-based counterparts.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 14:46:03 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Tran", "Nguyen Ha", ""], ["Cambria", "Erik", ""]]}, {"id": "1807.08888", "submitter": "Aparna Nitin Joshi Mrs", "authors": "Aparna Joshi, Yu Zhang, Petko Bogdanov and Jeong-Hyon Hwang", "title": "An Efficient System for Subgraph Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph discovery in a single data graph---finding subsets of vertices and\nedges satisfying a user-specified criteria---is an essential and general graph\nanalytics operation with a wide spectrum of applications. Depending on the\ncriteria, subgraphs of interest may correspond to cliques of friends in social\nnetworks, interconnected entities in RDF data, or frequent patterns in protein\ninteraction networks to name a few. Existing systems usually examine a large\nnumber of subgraphs while employing many computers and often produce an\nenormous result set of subgraphs. How can we enable fast discovery of only the\nmost relevant subgraphs while minimizing the computational requirements?\n  We present Nuri, a general subgraph discovery system that allows users to\nsuccinctly specify subgraphs of interest and criteria for ranking them. Given\nsuch specifications, Nuri efficiently finds the k most relevant subgraphs using\nonly a single computer. It prioritizes (i.e., expands earlier than others)\nsubgraphs that are more likely to expand into the desired subgraphs\n(prioritized subgraph expansion) and proactively discards irrelevant subgraphs\nfrom which the desired subgraphs cannot be constructed (pruning). Nuri can also\nefficiently store and retrieve a large number of subgraphs on disk without\nbeing limited by the size of main memory. We demonstrate using both real and\nsynthetic datasets that Nuri on a single core outperforms the closest\nalternative distributed system consuming 40 times more computational resources\nby more than 2 orders of magnitude for clique discovery and 1 order of\nmagnitude for subgraph isomorphism and pattern mining.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 02:58:44 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Joshi", "Aparna", ""], ["Zhang", "Yu", ""], ["Bogdanov", "Petko", ""], ["Hwang", "Jeong-Hyon", ""]]}, {"id": "1807.09009", "submitter": "Jahongir Azimjonov", "authors": "Jahongir Azimjonov and Jumabek Alikhanov", "title": "Rule Based Metadata Extraction Framework from Academic Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Metadata of scientific articles such as title, abstract, keywords or index\nterms, body text, conclusion, reference and others play a decisive role in\ncollecting, managing and storing academic data in scientific databases,\nacademic journals and digital libraries. An accurate extraction of these kinds\nof data from scientific papers is crucial to organize and retrieve important\nscientific information for researchers as well as librarians. Research social\nnetwork systems and academic digital library systems provide academic data\nextracting, organizing and retrieving services. Mostly these types of services\nare not free or open source. They also have some performance problems and\nextracting limitations in the number of PDF (Portable Document Format) files\nthat you can upload to the extraction systems. In this paper, a completely free\nand open source Java based high performance metadata extraction framework is\nproposed. This frameworks extraction speed is 9-10 times faster than existing\nmetadata extraction systems. It is also flexible in that it allows uploading of\nunlimited number of PDF files. In this approach, titles of papers are extracted\nusing layout features, font and size characteristics of text. Other metadata\nfields such as abstracts, body text, keywords, conclusions and references are\nextracted from PDF files using fixed rule sets. Extracted metadata are stored\nin both Oracle database and XML (Extensible Markup Language) file. This\nframework can be used to make scientific collections in digital libraries,\nonline journals, online and offline scientific databases, government research\nagencies and research centers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 10:13:00 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Azimjonov", "Jahongir", ""], ["Alikhanov", "Jumabek", ""]]}, {"id": "1807.09320", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Pierre Bourhis, Stefan Mengel, Matthias Niewerth", "title": "Constant-Delay Enumeration for Nondeterministic Document Spanners", "comments": "25 pages including 17 pages of main material. Integrates all reviewer\n  feedback. T paper is exactly the same as the ICDT'19 paper except that it\n  contains 6 pages of technical appendix, and except that we corrected some\n  additional minor mistakes following reviews of the journal version\n  (arXiv:2003.02576). We recommend reading the journal version instead of this\n  paper", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2019.19", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the information extraction framework known as document spanners,\nand study the problem of efficiently computing the results of the extraction\nfrom an input document, where the extraction task is described as a sequential\nvariable-set automaton (VA). We pose this problem in the setting of enumeration\nalgorithms, where we can first run a preprocessing phase and must then produce\nthe results with a small delay between any two consecutive results. Our goal is\nto have an algorithm which is tractable in combined complexity, i.e., in the\nsizes of the input document and the VA; while ensuring the best possible data\ncomplexity bounds in the input document size, i.e., constant delay in the\ndocument size. Several recent works at PODS'18 proposed such algorithms but\nwith linear delay in the document size or with an exponential dependency in\nsize of the (generally nondeterministic) input VA. In particular, Florenzano et\nal. suggest that our desired runtime guarantees cannot be met for general\nsequential VAs. We refute this and show that, given a nondeterministic\nsequential VA and an input document, we can enumerate the mappings of the VA on\nthe document with the following bounds: the preprocessing is linear in the\ndocument size and polynomial in the size of the VA, and the delay is\nindependent of the document and polynomial in the size of the VA. The resulting\nalgorithm thus achieves tractability in combined complexity and the best\npossible data complexity bounds. Moreover, it is rather easy to describe, in\nparticular for the restricted case of so-called extended VAs.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 19:49:14 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 13:23:00 GMT"}, {"version": "v3", "created": "Sat, 2 Mar 2019 14:53:12 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2020 07:54:45 GMT"}, {"version": "v5", "created": "Mon, 7 Dec 2020 13:43:35 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Mengel", "Stefan", ""], ["Niewerth", "Matthias", ""]]}, {"id": "1807.09887", "submitter": "Mohammad Dashti", "authors": "Mohammad Dashti, Sachin Basil John, Thierry Coppey, Amir Shaikhha,\n  Vojin Jovanovic and Christoph Koch", "title": "Compiling Database Application Programs", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a trend towards increased specialization of data management software\nfor performance reasons. In this paper, we study the automatic specialization\nand optimization of database application programs -- sequences of queries and\nupdates, augmented with control flow constructs as they appear in database\nscripts, UDFs, transactional workloads and triggers in languages such as\nPL/SQL. We show how to build an optimizing compiler for database application\nprograms using generative programming and state-of-the-art compiler technology.\n  We evaluate a hand-optimized low-level implementation of TPC-C, and identify\nthe key optimization techniques that account for its good performance. Our\ncompiler fully automates these optimizations and, applied to this benchmark,\noutperforms the manually optimized baseline by a factor of two. By selectively\ndisabling some of the optimizations in the compiler, we derive a clinical and\nprecise way of obtaining insight into their individual performance\ncontributions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 22:39:52 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Dashti", "Mohammad", ""], ["John", "Sachin Basil", ""], ["Coppey", "Thierry", ""], ["Shaikhha", "Amir", ""], ["Jovanovic", "Vojin", ""], ["Koch", "Christoph", ""]]}, {"id": "1807.09899", "submitter": "Bertram Lud\\\"ascher", "authors": "Shawn Bowers and Timothy McPhillips and Bertram Lud\\\"ascher", "title": "Validation and Inference of Schema-Level Workflow Data-Dependency\n  Annotations", "comments": "To appear in: Provenance and Annotation of Data and Processes - 7th\n  International Provenance and Annotation Workshop, IPAW 2018, King's College\n  London, UK, July 9-10, 2018, Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An advantage of scientific workflow systems is their ability to collect\nruntime provenance information as an execution trace. Traces include the\ncomputation steps invoked as part of the workflow run along with the\ncorresponding data consumed and produced by each workflow step. The information\ncaptured by a trace is used to infer \"lineage\" relationships among data items,\nwhich can help answer provenance queries to find workflow inputs that were\ninvolved in producing specific workflow outputs. Determining lineage\nrelationships, however, requires an understanding of the dependency patterns\nthat exist between each workflow step's inputs and outputs, and this\ninformation is often under-specified or generally assumed by workflow systems.\nFor instance, most approaches assume all outputs depend on all inputs, which\ncan lead to lineage \"false positives\". In prior work, we defined annotations\nfor specifying detailed dependency relationships between inputs and outputs of\ncomputation steps. These annotations are used to define corresponding rules for\ninferring fine-grained data dependencies from a trace. In this paper, we extend\nour previous work by considering the impact of dependency annotations on\nworkflow specifications. In particular, we provide a reasoning framework to\nensure the set of dependency annotations on a workflow specification is\nconsistent. The framework can also infer a complete set of annotations given a\npartially annotated workflow. Finally, we describe an implementation of the\nreasoning framework using answer-set programming.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 23:29:49 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Bowers", "Shawn", ""], ["McPhillips", "Timothy", ""], ["Lud\u00e4scher", "Bertram", ""]]}, {"id": "1807.09920", "submitter": "Jia-Xu Liu", "authors": "Jia-Xu Liu, Ke Xu", "title": "Budget-aware Online Task Assignment in Spatial Crowdsourcing", "comments": "21 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of mobile internet techniques stimulates the emergence of\nvarious spatial crowdsourcing applications. Certain of the applications serve\nfor requesters, budget providers, who submit a batch of tasks and a fixed\nbudget to platform with the desire to search suitable workers to complete the\ntasks in maximum quantity. Platform lays stress on optimizing assignment\nstrategies on seeking less budget-consumed worker-task pairs to meet\nrequesters' demands. Existing research on the task assignment with budget\nconstraint mostly focuses on static offline scenarios, where the spatiotemporal\ninformation of all workers and tasks is known in advance. However, workers\nusually appear dynamically on real spatial crowdsourcing platforms, where\nexisting solutions can hardly handle it. In this paper, we formally define a\nnovel problem Budget-aware Online task Assignment(BOA) in spatial crowdsourcing\napplications. BOA aims to maximize the number of assigned worker- task pairs\nunder a budget constraint where workers appear dynamically on platforms. To\naddress the BOA problem, we first propose an efficient threshold-based greedy\nalgorithm Greedy-RT which utilizes a random generated threshold to prune the\npairs with large travel cost. Greedy-RT performs well in adversary model when\ncompared with simple greedy algorithm, but it is unstable in random model for\nits randomly generated threshold may produce poor quality in matching size. We\nthen propose a revised algorithm Greedy-OT which could learn approximately\noptimal threshold from historical data, and consequently improves matching size\nsignificantly in both models. Finally, we verify the effectiveness and\nefficiency of the proposed methods through extensive experiments on real and\nsynthetic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 02:04:18 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Liu", "Jia-Xu", ""], ["Xu", "Ke", ""]]}, {"id": "1807.10009", "submitter": "Dejan Lavbi\\v{c}", "authors": "Slavko \\v{Z}itnik, Lovro \\v{S}ubelj, Dejan Lavbi\\v{c}, Olegas\n  Vasilecas, Marko Bajec", "title": "General Context-Aware Data Matching and Merging Framework", "comments": "29 pages, 12 figures, 2 tables", "journal-ref": "Informatica 24 (2013) 119-152", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to numerous public information sources and services, many methods to\ncombine heterogeneous data were proposed recently. However, general end-to-end\nsolutions are still rare, especially systems taking into account different\ncontext dimensions. Therefore, the techniques often prove insufficient or are\nlimited to a certain domain. In this paper we briefly review and rigorously\nevaluate a general framework for data matching and merging. The framework\nemploys collective entity resolution and redundancy elimination using three\ndimensions of context types. In order to achieve domain independent results,\ndata is enriched with semantics and trust. However, the main contribution of\nthe paper is evaluation on five public domain-incompatible datasets.\nFurthermore, we introduce additional attribute, relationship, semantic and\ntrust metrics, which allow complete framework management. Besides overall\nresults improvement within the framework, metrics could be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 08:30:25 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["\u017ditnik", "Slavko", ""], ["\u0160ubelj", "Lovro", ""], ["Lavbi\u010d", "Dejan", ""], ["Vasilecas", "Olegas", ""], ["Bajec", "Marko", ""]]}, {"id": "1807.10792", "submitter": "Ioannis Papapanagiotou", "authors": "Ioannis Papapanagiotou and Vinay Chella", "title": "NDBench: Benchmarking Microservices at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software vendors often report performance numbers for the sweet spot or\nrunning on specialized hardware with specific workload parameters and without\nrealistic failures. Accurate benchmarks at the persistence layer are crucial,\nas failures may cause unrecoverable errors such as data loss, inconsistency or\ncorruption. To accurately evaluate data stores and other microservices at\nNetflix, we developed Netflix Data Benchmark (NDBench), a Cloud benchmark tool.\nIt can be deployed in a loosely-coupled fashion with the ability to dynamically\nchange the benchmark parameters at runtime so we can rapidly iterate on\ndifferent tests and failure modes. NDBench offers pluggable patterns and loads,\nsupport for pluggable client APIs, and was designed to run continually. This\ndesign enabled us to test long-running maintenance jobs that may affect the\nperformance, test numerous different systems under adverse conditions, and\nuncover long-term issues like memory leaks or heap pressure.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 18:42:59 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Papapanagiotou", "Ioannis", ""], ["Chella", "Vinay", ""]]}, {"id": "1807.11054", "submitter": "Hongzhi Wang", "authors": "Xuebin Su, Hongzhi Wang, Jianzhong Li, Hong Gao", "title": "MISS: Finding Optimal Sample Sizes for Approximate Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, sampling-based Approximate Query Processing (AQP) is widely\nregarded as a promising way to achieve interactivity in big data analytics. To\nbuild such an AQP system, finding the minimal sample size for a query regarding\ngiven error constraints in general, called Sample Size Optimization (SSO), is\nan essential yet unsolved problem. Ideally, the goal of solving the SSO problem\nis to achieve statistical accuracy, computational efficiency and broad\napplicability all at the same time. Existing approaches either make idealistic\nassumptions on the statistical properties of the query, or completely disregard\nthem. This may result in overemphasizing only one of the three goals while\nneglect the others.\n  To overcome these limitations, we first examine carefully the statistical\nproperties shared by common analytical queries. Then, based on the properties,\nwe propose a linear model describing the relationship between sample sizes and\nthe approximation errors of a query, which is called the error model. Then, we\npropose a Model-guided Iterative Sample Selection (MISS) framework to solve the\nSSO problem generally. Afterwards, based on the MISS framework, we propose a\nconcrete algorithm, called $L^2$Miss, to find optimal sample sizes under the\n$L^2$ norm error metric. Moreover, we extend the $L^2$Miss algorithm to handle\nother error metrics. Finally, we show theoretically and empirically that the\n$L^2$Miss algorithm and its extensions achieve satisfactory accuracy and\nefficiency for a considerably wide range of analytical queries.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 13:36:58 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Su", "Xuebin", ""], ["Wang", "Hongzhi", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}, {"id": "1807.11104", "submitter": "Dimitri Yatsenko", "authors": "Dimitri Yatsenko, Edgar Y. Walker, Andreas S. Tolias", "title": "DataJoint: A Simpler Relational Data Model", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The relational data model offers unrivaled rigor and precision in defining\ndata structure and querying complex data. Yet the use of relational databases\nin scientific data pipelines is limited due to their perceived unwieldiness. We\npropose a simplified and conceptually refined relational data model named\nDataJoint. The model includes a language for schema definition, a language for\ndata queries, and diagramming notation for visualizing entities and\nrelationships among them. The model adheres to the principle of entity\nnormalization, which requires that all data -- both stored and derived -- must\nbe represented by well-formed entity sets. DataJoint's data query language is\nan algebra on entity sets with five operators that provide matching\ncapabilities to those of other relational query languages with greater clarity\ndue to entity normalization. Practical implementations of DataJoint have been\nadopted in neuroscience labs for fluent interaction with scientific data\npipelines.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 19:39:38 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Yatsenko", "Dimitri", ""], ["Walker", "Edgar Y.", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "1807.11149", "submitter": "Feilong Liu", "authors": "Feilong Liu, Niranjan Kamat, Spyros Blanas, Arnab Nandi", "title": "To Ship or Not to (Function) Ship (Extended version)", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling is often used to reduce query latency for interactive big data\nanalytics. The established parallel data processing paradigm relies on function\nshipping, where a coordinator dispatches queries to worker nodes and then\ncollects the results. The commoditization of high-performance networking makes\ndata shipping possible, where the coordinator directly reads data in the\nworkers' memory using RDMA while workers process other queries. In this work,\nwe explore when to use function shipping or data shipping for interactive query\nprocessing with sampling. Whether function shipping or data shipping should be\npreferred depends on the amount of data transferred, the current CPU\nutilization, the sampling method and the number of queries executed over the\ndata set. The results show that data shipping is up to 6.5x faster when\nperforming clustered sampling with heavily-utilized workers.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 02:37:44 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Liu", "Feilong", ""], ["Kamat", "Niranjan", ""], ["Blanas", "Spyros", ""], ["Nandi", "Arnab", ""]]}, {"id": "1807.11317", "submitter": "Takao Murakami", "authors": "Takao Murakami, Yusuke Kawamoto", "title": "Utility-Optimized Local Differential Privacy Mechanisms for Distribution\n  Estimation", "comments": "Accepted to USENIX Security 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LDP (Local Differential Privacy) has been widely studied to estimate\nstatistics of personal data (e.g., distribution underlying the data) while\nprotecting users' privacy. Although LDP does not require a trusted third party,\nit regards all personal data equally sensitive, which causes excessive\nobfuscation hence the loss of utility. In this paper, we introduce the notion\nof ULDP (Utility-optimized LDP), which provides a privacy guarantee equivalent\nto LDP only for sensitive data. We first consider the setting where all users\nuse the same obfuscation mechanism, and propose two mechanisms providing ULDP:\nutility-optimized randomized response and utility-optimized RAPPOR. We then\nconsider the setting where the distinction between sensitive and non-sensitive\ndata can be different from user to user. For this setting, we propose a\npersonalized ULDP mechanism with semantic tags to estimate the distribution of\npersonal data with high utility while keeping secret what is sensitive for each\nuser. We show theoretically and experimentally that our mechanisms provide much\nhigher utility than the existing LDP mechanisms when there are a lot of\nnon-sensitive data. We also show that when most of the data are non-sensitive,\nour mechanisms even provide almost the same utility as non-private mechanisms\nin the low privacy regime.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 12:39:35 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 11:02:44 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 05:57:13 GMT"}, {"version": "v4", "created": "Fri, 14 Dec 2018 01:28:38 GMT"}, {"version": "v5", "created": "Fri, 15 Feb 2019 02:24:43 GMT"}, {"version": "v6", "created": "Tue, 14 May 2019 09:16:27 GMT"}, {"version": "v7", "created": "Mon, 27 May 2019 02:44:56 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Murakami", "Takao", ""], ["Kawamoto", "Yusuke", ""]]}, {"id": "1807.11634", "submitter": "Yuhao Wen", "authors": "Yuhao Wen, Xiaodan Zhu, Sudeepa Roy, Jun Yang", "title": "Interactive Summarization and Exploration of Top Aggregate Query Answers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for summarization and interactive exploration of\nhigh-valued aggregate query answers to make a large set of possible answers\nmore informative to the user. Our system outputs a set of clusters on the\nhigh-valued query answers showing their common properties such that the\nclusters are diverse as much as possible to avoid repeating information, and\ncover a certain number of top original answers as indicated by the user.\nFurther, the system facilitates interactive exploration of the query answers by\nhelping the user (i) choose combinations of parameters for clustering, (ii)\ninspect the clusters as well as the elements they contain, and (iii) visualize\nhow changes in parameters affect clustering. We define optimization problems,\nstudy their complexity, explore properties of the solutions investigating the\nsemi-lattice structure on the clusters, and propose efficient algorithms and\noptimizations to achieve these goals. We evaluate our techniques experimentally\nand discuss our prototype with a graphical user interface that facilitates this\ninteractive exploration. A user study is conducted to evaluate the usability of\nour approach.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 02:31:39 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Wen", "Yuhao", ""], ["Zhu", "Xiaodan", ""], ["Roy", "Sudeepa", ""], ["Yang", "Jun", ""]]}]