[{"id": "1205.0435", "submitter": "Ashwin Machanavajjhala", "authors": "Jianjun Chen and Ashwin Machanavajjhala and George Varghese", "title": "Scalable Social Coordination using Enmeshed Queries", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social coordination allows users to move beyond awareness of their friends to\nefficiently coordinating physical activities with others. While specific forms\nof social coordination can be seen in tools such as Evite, Meetup and Groupon,\nwe introduce a more general model using what we call enmeshed queries. An\nenmeshed query allows users to declaratively specify an intent to coordinate by\nspecifying social attributes such as the desired group size and who/what/when,\nand the database returns matching queries. Enmeshed queries are continuous, but\nnew queries (and not data) answer older queries; the variable group size also\nmakes enmeshed queries different from entangled queries, publish-subscribe\nsystems, and dating services.\n  We show that even offline group coordination using enmeshed queries is\nNP-hard. We then introduce efficient heuristics that use selective indices such\nas location and time to reduce the space of possible matches; we also add\nrefinements such as delayed evaluation and using the relative matchability of\nusers to determine search order. We describe a centralized implementation and\nevaluate its performance against an optimal algorithm. We show that the\ncombination of not stopping prematurely (after finding a match) and delayed\nevaluation results in an algorithm that finds 86% of the matches found by an\noptimal algorithm, and takes an average of 40 usec per query using 1 core of a\n2.5 Ghz server machine. Further, the algorithm has good latency, is reasonably\nfair to large group size requests, and can be scaled to global workloads using\nmultiple cores and multiple servers. We conclude by describing potential\ngeneralizations that add prices, recommendations, and data mining to basic\nenmeshed queries.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 14:10:24 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2012 11:28:42 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Chen", "Jianjun", ""], ["Machanavajjhala", "Ashwin", ""], ["Varghese", "George", ""]]}, {"id": "1205.0439", "submitter": "Aridj Mohamed", "authors": "Aridj Mohamed, Zegour Djamel Eddine", "title": "TH*:Scalable Distributed Trie Hashing", "comments": "IJCSI International Journal of Computer Science Issues, Vol. 7, Issue\n  6, November 2010 ISSN (Online): 1694-0814 http://www.IJCSI.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's world of computers, dealing with huge amounts of data is not\nunusual. The need to distribute this data in order to increase its availability\nand increase the performance of accessing it is more urgent than ever. For\nthese reasons it is necessary to develop scalable distributed data structures.\nIn this paper we propose a TH* distributed variant of the Trie Hashing data\nstructure. First we propose Thsw new version of TH without node Nil in digital\ntree (trie), then this version will be adapted to multicomputer environment.\nThe simulation results reveal that TH* is scalable in the sense that it grows\ngracefully, one bucket at a time, to a large number of servers, also TH* offers\na good storage space utilization and high query efficiency special for ordering\noperations.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 14:17:16 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Mohamed", "Aridj", ""], ["Eddine", "Zegour Djamel", ""]]}, {"id": "1205.0724", "submitter": "Phuc V. Nguyen Mr.", "authors": "Phuc V. Nguyen (Hung Vuong Univesity, Ho Chi Minh City)", "title": "Using Data Warehouse to Support Building Strategy or Forecast Business\n  Tend", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data warehousing is becoming increasingly important in terms of strategic\ndecision making through their capacity to integrate heterogeneous data from\nmultiple information sources in a common storage space, for querying and\nanalysis. So it can evolve into a multi-tier structure where parts of the\norganization take information from the main data warehouse into their own\nsystems. These may include analysis databases or dependent data marts. As the\ndata warehouse evolves and the organization gets better at capturing\ninformation on all interactions with the customer. Data warehouse can track\ncustomer interactions over the whole of the customer's lifetime.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 11:46:13 GMT"}], "update_date": "2012-05-04", "authors_parsed": [["Nguyen", "Phuc V.", "", "Hung Vuong Univesity, Ho Chi Minh City"]]}, {"id": "1205.0837", "submitter": "Sean Chester", "authors": "Sean Chester, Alex Thomo, S. Venkatesh, Sue Whitesides", "title": "Indexing Reverse Top-k Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recently introduced monochromatic reverse top-k queries which\nask for, given a new tuple q and a dataset D, all possible top-k queries on D\nunion {q} for which q is in the result. Towards this problem, we focus on\ndesigning indexes in two dimensions for repeated (or batch) querying, a novel\nbut practical consideration. We present the insight that by representing the\ndataset as an arrangement of lines, a critical k-polygon can be identified and\nused exclusively to respond to reverse top-k queries. We construct an index\nbased on this observation which has guaranteed worst-case query cost that is\nlogarithmic in the size of the k-polygon.\n  We implement our work and compare it to related approaches, demonstrating\nthat our index is fast in practice. Furthermore, we demonstrate through our\nexperiments that a k-polygon is comprised of a small proportion of the original\ndata, so our index structure consumes little disk space.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2012 00:03:18 GMT"}], "update_date": "2012-05-07", "authors_parsed": [["Chester", "Sean", ""], ["Thomo", "Alex", ""], ["Venkatesh", "S.", ""], ["Whitesides", "Sue", ""]]}, {"id": "1205.1117", "submitter": "Soni madhulatha Tagaram", "authors": "T. Soni Madhulatha", "title": "An Overview on Clustering Methods", "comments": "7 pages", "journal-ref": "IOSR Journal of Engineering, Apr. 2012, Vol. 2(4) pp: 719-725,\n  ISSN 2250-3021", "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Clustering is a common technique for statistical data analysis, which is used\nin many fields, including machine learning, data mining, pattern recognition,\nimage analysis and bioinformatics. Clustering is the process of grouping\nsimilar objects into different groups, or more precisely, the partitioning of a\ndata set into subsets, so that the data in each subset according to some\ndefined distance measure. This paper covers about clustering algorithms,\nbenefits and its applications. Paper concludes by discussing some limitations.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2012 09:55:22 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Madhulatha", "T. Soni", ""]]}, {"id": "1205.1125", "submitter": "Khalid Raza", "authors": "Khalid Raza", "title": "Application Of Data Mining In Bioinformatics", "comments": null, "journal-ref": "Indian Journal of Computer Science and Engineering 1(2):114-118\n  2010", "doi": null, "report-no": null, "categories": "cs.CE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article highlights some of the basic concepts of bioinformatics and data\nmining. The major research areas of bioinformatics are highlighted. The\napplication of data mining in the domain of bioinformatics is explained. It\nalso highlights some of the current challenges and opportunities of data mining\nin bioinformatics.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2012 12:19:33 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Raza", "Khalid", ""]]}, {"id": "1205.1126", "submitter": "Khalid Raza", "authors": "Md. Rashid Farooqi and Khalid Raza", "title": "A Comprehensive Study of CRM through Data Mining Techniques", "comments": "Proceedings of the National Conference; NCCIST-2011, September 09,\n  2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's competitive scenario in corporate world, \"Customer Retention\"\nstrategy in Customer Relationship Management (CRM) is an increasingly pressed\nissue. Data mining techniques play a vital role in better CRM. This paper\nattempts to bring a new perspective by focusing the issue of data mining\napplications, opportunities and challenges in CRM. It covers the topic such as\ncustomer retention, customer services, risk assessment, fraud detection and\nsome of the data mining tools which are widely used in CRM.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2012 12:29:35 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Farooqi", "Md. Rashid", ""], ["Raza", "Khalid", ""]]}, {"id": "1205.1609", "submitter": "Jyothi Pillai", "authors": "Jyothi Pillai, O.P.Vyas", "title": "CSHURI - Modified HURI algorithm for Customer Segmentation and\n  Transaction Profitability", "comments": "11 pages, 5 tables, 1 figure, IJCSEIT-2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association rule mining (ARM) is the process of generating rules based on the\ncorrelation between the set of items that the customers purchase.Of late, data\nmining researchers have improved upon the quality of association rule mining\nfor business development by incorporating factors like value (utility),\nquantity of items sold (weight) and profit. The rules mined without considering\nutility values (profit margin) will lead to a probable loss of profitable\nrules. The advantage of wealth of the customers' needs information and rules\naids the retailer in designing his store layout[9]. An algorithm CSHURI,\nCustomer Segmentation using HURI, is proposed, a modified version of HURI [6],\nfinds customers who purchase high profitable rare items and accordingly\nclassify the customers based on some criteria; for example, a retail business\nmay need to identify valuable customers who are major contributors to a\ncompany's overall profit. For a potential customer arriving in the store, which\ncustomer group one should belong to according to customer needs, what are the\npreferred functional features or products that the customer focuses on and what\nkind of offers will satisfy the customer, etc., finds the key in targeting\ncustomers to improve sales [9], which forms the base for customer utility\nmining.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 07:21:44 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Pillai", "Jyothi", ""], ["Vyas", "O. P.", ""]]}, {"id": "1205.1796", "submitter": "Lamia Karim", "authors": "Azedine Boulmakoul, Lamia Karim, Ahmed Lbath", "title": "Moving Object Trajectories Meta-Model And Spatio-Temporal Queries", "comments": "International Journal of Database Management Systems (IJDMS) Vol.4,\n  No.2, April 2012", "journal-ref": null, "doi": "10.5121/ijdms.2012.4203", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a general moving object trajectories framework is put forward\nto allow independent applications processing trajectories data benefit from a\nhigh level of interoperability, information sharing as well as an efficient\nanswer for a wide range of complex trajectory queries. Our proposed meta-model\nis based on ontology and event approach, incorporates existing presentations of\ntrajectory and integrates new patterns like space-time path to describe\nactivities in geographical space-time. We introduce recursive Region of\nInterest concepts and deal mobile objects trajectories with diverse\nspatio-temporal sampling protocols and different sensors available that\ntraditional data model alone are incapable for this purpose.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 19:58:28 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Boulmakoul", "Azedine", ""], ["Karim", "Lamia", ""], ["Lbath", "Ahmed", ""]]}, {"id": "1205.1853", "submitter": "Priya Iyer  K B", "authors": "K. B. Priya Iyer, V. Shanthi", "title": "Goal Directed Relative Skyline Queries in Time Dependent Road Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Wireless GIS technology is progressing rapidly in the area of mobile\ncommunications. Location-based spatial queries are becoming an integral part of\nmany new mobile applications. The Skyline queries are latest apps under\nLocation-based services. In this paper we introduce Goal Directed Relative\nSkyline queries on Time dependent (GD-RST) road networks. The algorithm uses\ntravel time as a metric in finding the data object by considering multiple\nquery points (multi-source skyline) relative to user location and in the user\ndirection of travelling. We design an efficient algorithm based on Filter\nphase, Heap phase and Refine Skyline phases. At the end, we propose a dynamic\nskyline caching (DSC) mechanism which helps to reduce the computation cost for\nfuture skyline queries. The experimental evaluation reflects the performance of\nGD-RST algorithm over the traditional branch and bound algorithm for skyline\nqueries in real road networks.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 01:16:34 GMT"}], "update_date": "2012-05-10", "authors_parsed": [["Iyer", "K. B. Priya", ""], ["Shanthi", "V.", ""]]}, {"id": "1205.1923", "submitter": "Shweta Kharya", "authors": "Shweta Kharya", "title": "Using data mining techniques for diagnosis and prognosis of cancer\n  disease", "comments": "12 pages, 4 figures, 4 tables, IJCSEIT 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is one of the leading cancers for women in developed countries\nincluding India. It is the second most common cause of cancer death in women.\nThe high incidence of breast cancer in women has increased significantly in the\nlast years. In this paper we have discussed various data mining approaches that\nhave been utilized for breast cancer diagnosis and prognosis. Breast Cancer\nDiagnosis is distinguishing of benign from malignant breast lumps and Breast\nCancer Prognosis predicts when Breast Cancer is to recur in patients that have\nhad their cancers excised. This study paper summarizes various review and\ntechnical articles on breast cancer diagnosis and prognosis also we focus on\ncurrent research being carried out using the data mining techniques to enhance\nthe breast cancer diagnosis and prognosis.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 09:37:52 GMT"}], "update_date": "2012-05-10", "authors_parsed": [["Kharya", "Shweta", ""]]}, {"id": "1205.2292", "submitter": "George Papastefanatos Dr.", "authors": "Yannis Stavrakas, George Papastefanatos, Theodore Dalamagas, Vassilis\n  Christophides", "title": "Diachronic Linked Data: Towards Long-Term Preservation of Structured\n  Interrelated Information", "comments": "Presented at the First International Workshop On Open Data, WOD-2012\n  (http://arxiv.org/abs/1204.3726)", "journal-ref": null, "doi": null, "report-no": "WOD/2012/NANTES/10", "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Linked Data Paradigm is one of the most promising technologies for\npublishing, sharing, and connecting data on the Web, and offers a new way for\ndata integration and interoperability. However, the proliferation of\ndistributed, inter-connected sources of information and services on the Web\nposes significant new challenges for managing consistently a huge number of\nlarge datasets and their interdependencies. In this paper we focus on the key\nproblem of preserving evolving structured interlinked data. We argue that a\nnumber of issues that hinder applications and users are related to the temporal\naspect that is intrinsic in linked data. We present a number of real use cases\nto motivate our approach, we discuss the problems that occur, and propose a\ndirection for a solution.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2012 15:28:30 GMT"}], "update_date": "2012-05-11", "authors_parsed": [["Stavrakas", "Yannis", ""], ["Papastefanatos", "George", ""], ["Dalamagas", "Theodore", ""], ["Christophides", "Vassilis", ""]]}, {"id": "1205.2320", "submitter": "Theodore Dalamagas", "authors": "Theodore Dalamagas, Nikos Bikakis, George Papastefanatos, Yannis\n  Stavrakas, Artemis G. Hatzigeorgiou", "title": "Publishing Life Science Data as Linked Open Data: the Case Study of\n  miRBase", "comments": "Presented at the First International Workshop On Open Data, WOD-2012\n  (arXiv:1204.3726)", "journal-ref": null, "doi": null, "report-no": "WOD/2012/NANTES/7", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our Linked Open Data (LOD) infrastructures for genomic\nand experimental data related to microRNA biomolecules. Legacy data from two\nwell-known microRNA databases with experimental data and observations, as well\nas change and version information about microRNA entities, are fused and\nexported as LOD. Our LOD server assists biologists to explore biological\nentities and their evolution, and provides a SPARQL endpoint for applications\nand services to query historical miRNA data and track changes, their causes and\neffects.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2012 17:29:05 GMT"}], "update_date": "2012-05-11", "authors_parsed": [["Dalamagas", "Theodore", ""], ["Bikakis", "Nikos", ""], ["Papastefanatos", "George", ""], ["Stavrakas", "Yannis", ""], ["Hatzigeorgiou", "Artemis G.", ""]]}, {"id": "1205.2465", "submitter": "Julian Eberius", "authors": "Julian Eberius, Katrin Braunschweig, Maik Thiele and Wolfgang Lehner", "title": "Identifying And Weighting Integration Hypotheses On Open Data Platforms", "comments": "Presented at the First International Workshop On Open Data, WOD-2012\n  (http://arxiv.org/abs/1204.3726)", "journal-ref": null, "doi": null, "report-no": "WOD/2012/NANTES/11", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open data platforms such as data.gov or opendata.socrata. com provide a huge\namount of valuable information. Their free-for-all nature, the lack of\npublishing standards and the multitude of domains and authors represented on\nthese platforms lead to new integration and standardization problems. At the\nsame time, crowd-based data integration techniques are emerging as new way of\ndealing with these problems. However, these methods still require input in form\nof specific questions or tasks that can be passed to the crowd. This paper\ndiscusses integration problems on Open Data Platforms, and proposes a method\nfor identifying and ranking integration hypotheses in this context. We will\nevaluate our findings by conducting a comprehensive evaluation using on one of\nthe largest Open Data platforms.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2012 09:31:27 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Eberius", "Julian", ""], ["Braunschweig", "Katrin", ""], ["Thiele", "Maik", ""], ["Lehner", "Wolfgang", ""]]}, {"id": "1205.2691", "submitter": "Corentin Follenfant", "authors": "Ahmad Assaf, Eldad Louw, Aline Senart, Corentin Follenfant, Rapha\\\"el\n  Troncy, David Trastour", "title": "Improving Schema Matching with Linked Data", "comments": "Presented at the First International Workshop On Open Data, WOD-2012\n  (http://arxiv.org/abs/1204.3726)", "journal-ref": null, "doi": null, "report-no": "WOD/2012/NANTES/4", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With today's public data sets containing billions of data items, more and\nmore companies are looking to integrate external data with their traditional\nenterprise data to improve business intelligence analysis. These distributed\ndata sources however exhibit heterogeneous data formats and terminologies and\nmay contain noisy data. In this paper, we present a novel framework that\nenables business users to semi-automatically perform data integration on\npotentially noisy tabular data. This framework offers an extension to Google\nRefine with novel schema matching algorithms leveraging Freebase rich types.\nFirst experiments show that using Linked Data to map cell values with instances\nand column headers with types improves significantly the quality of the\nmatching results and therefore should lead to more informed decisions.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2012 17:20:02 GMT"}, {"version": "v2", "created": "Tue, 15 May 2012 09:30:14 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Assaf", "Ahmad", ""], ["Louw", "Eldad", ""], ["Senart", "Aline", ""], ["Follenfant", "Corentin", ""], ["Troncy", "Rapha\u00ebl", ""], ["Trastour", "David", ""]]}, {"id": "1205.2726", "submitter": "David Leoni", "authors": "David Leoni", "title": "Non-Interactive Differential Privacy: a Survey", "comments": "Presented at the First International Workshop On Open Data, WOD-2012\n  (http://arxiv.org/abs/1204.3726)", "journal-ref": null, "doi": null, "report-no": "WOD/2012/NANTES/12", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenData movement around the globe is demanding more access to information\nwhich lies locked in public or private servers. As recently reported by a\nMcKinsey publication, this data has significant economic value, yet its release\nhas potential to blatantly conflict with people privacy. Recent UK government\ninquires have shown concern from various parties about publication of\nanonymized databases, as there is concrete possibility of user identification\nby means of linkage attacks. Differential privacy stands out as a model that\nprovides strong formal guarantees about the anonymity of the participants in a\nsanitized database. Only recent results demonstrated its applicability on\nreal-life datasets, though. This paper covers such breakthrough discoveries, by\nreviewing applications of differential privacy for non-interactive publication\nof anonymized real-life datasets. Theory, utility and a data-aware comparison\nare discussed on a variety of principles and concrete applications.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2012 21:38:16 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Leoni", "David", ""]]}, {"id": "1205.2880", "submitter": "Gao Cong", "authors": "Gao Cong, Hua Lu, Beng Chin Ooi, Dongxiang Zhang, Meihui Zhang", "title": "Efficient Spatial Keyword Search in Trajectory Databases", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of trajectory data is being annotated with text\ndescriptions to better capture the semantics associated with locations. The\nfusion of spatial locations and text descriptions in trajectories engenders a\nnew type of top-$k$ queries that take into account both aspects. Each\ntrajectory in consideration consists of a sequence of geo-spatial locations\nassociated with text descriptions. Given a user location $\\lambda$ and a\nkeyword set $\\psi$, a top-$k$ query returns $k$ trajectories whose text\ndescriptions cover the keywords $\\psi$ and that have the shortest match\ndistance. To the best of our knowledge, previous research on querying\ntrajectory databases has focused on trajectory data without any text\ndescription, and no existing work has studied such kind of top-$k$ queries on\ntrajectories. This paper proposes one novel method for efficiently computing\ntop-$k$ trajectories. The method is developed based on a new hybrid index,\ncell-keyword conscious B$^+$-tree, denoted by \\cellbtree, which enables us to\nexploit both text relevance and location proximity to facilitate efficient and\neffective query processing. The results of our extensive empirical studies with\nan implementation of the proposed algorithms on BerkeleyDB demonstrate that our\nproposed methods are capable of achieving excellent performance and good\nscalability.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2012 16:35:40 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Cong", "Gao", ""], ["Lu", "Hua", ""], ["Ooi", "Beng Chin", ""], ["Zhang", "Dongxiang", ""], ["Zhang", "Meihui", ""]]}, {"id": "1205.2889", "submitter": "Youssef Bassil", "authors": "Youssef Bassil", "title": "A Comparative Study on the Performance of the Top DBMS Systems", "comments": "LACSC - Lebanese Association for Computational Sciences,\n  http://www.lacsc.org", "journal-ref": "Journal of Computer Science & Research (JCSCR), Vol.1, No.1,\n  pp.20-31, 2012", "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database management systems are today's most reliable mean to organize data\ninto collections that can be searched and updated. However, many DBMS systems\nare available on the market each having their pros and cons in terms of\nreliability, usability, security, and performance. This paper presents a\ncomparative study on the performance of the top DBMS systems. They are mainly\nMS SQL Server 2008, Oracle 11g, IBM DB2, MySQL 5.5, and MS Access 2010. The\ntesting is aimed at executing different SQL queries with different level of\ncomplexities over the different five DBMSs under test. This would pave the way\nto build a head-to-head comparative evaluation that shows the average execution\ntime, memory usage, and CPU utilization of each DBMS after completion of the\ntest.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2012 17:54:51 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Bassil", "Youssef", ""]]}, {"id": "1205.2958", "submitter": "Ping Li", "authors": "Ping Li and Anshumali Shrivastava and Arnd Christian Konig", "title": "b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning\n  and Using GPUs for Fast Preprocessing with Simple Hash Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study several critical issues which must be tackled before\none can apply b-bit minwise hashing to the volumes of data often used\nindustrial applications, especially in the context of search.\n  1. (b-bit) Minwise hashing requires an expensive preprocessing step that\ncomputes k (e.g., 500) minimal values after applying the corresponding\npermutations for each data vector. We developed a parallelization scheme using\nGPUs and observed that the preprocessing time can be reduced by a factor of\n20-80 and becomes substantially smaller than the data loading time.\n  2. One major advantage of b-bit minwise hashing is that it can substantially\nreduce the amount of memory required for batch learning. However, as online\nalgorithms become increasingly popular for large-scale learning in the context\nof search, it is not clear if b-bit minwise yields significant improvements for\nthem. This paper demonstrates that $b$-bit minwise hashing provides an\neffective data size/dimension reduction scheme and hence it can dramatically\nreduce the data loading time for each epoch of the online training process.\nThis is significant because online learning often requires many (e.g., 10 to\n100) epochs to reach a sufficient accuracy.\n  3. Another critical issue is that for very large data sets it becomes\nimpossible to store a (fully) random permutation matrix, due to its space\nrequirements. Our paper is the first study to demonstrate that $b$-bit minwise\nhashing implemented using simple hash functions, e.g., the 2-universal (2U) and\n4-universal (4U) hash families, can produce very similar learning results as\nusing fully random permutations. Experiments on datasets of up to 200GB are\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 08:28:10 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Li", "Ping", ""], ["Shrivastava", "Anshumali", ""], ["Konig", "Arnd Christian", ""]]}, {"id": "1205.3231", "submitter": "Sabu Thampi m", "authors": "Rekha Sunny T and Sabu M. Thampi", "title": "Survey on Distributed Data Mining in P2P Networks", "comments": "23 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential increase of availability of digital data and the necessity to\nprocess it in business and scientific fields has literally forced upon us the\nneed to analyze and mine useful knowledge from it. Traditionally data mining\nhas used a data warehousing model of gathering all data into a central site,\nand then running an algorithm upon that data. Such a centralized approach is\nfundamentally inappropriate due to many reasons like huge amount of data,\ninfeasibility to centralize data stored at multiple sites, bandwidth limitation\nand privacy concerns. To solve these problems, Distributed Data Mining (DDM)\nhas emerged as a hot research area. Careful attention in the usage of\ndistributed resources of data, computing, communication, and human factors in a\nnear optimal fashion are paid by distributed data mining. DDM is gaining\nattention in peer-to-peer (P2P) systems which are emerging as a choice of\nsolution for applications such as file sharing, collaborative movie and song\nscoring, electronic commerce, and surveillance using sensor networks. The main\nintension of this draft paper is to provide an overview of DDM and P2P Data\nMining. The paper discusses the need for DDM, taxonomy of DDM architectures,\nvarious DDM approaches, DDM related works in P2P systems and issues and\nchallenges in P2P data mining.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2012 01:00:07 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["T", "Rekha Sunny", ""], ["Thampi", "Sabu M.", ""]]}, {"id": "1205.3321", "submitter": "Francesco Scarcello", "authors": "Gianluigi Greco and Francesco Scarcello", "title": "Tree Projections and Structural Decomposition Methods: The Power of\n  Local Consistency and Larger Islands of Tractability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating conjunctive queries and solving constraint satisfaction problems\nare fundamental problems in database theory and artificial intelligence,\nrespectively. These problems are NP-hard, so that several research efforts have\nbeen made in the literature for identifying tractable classes, known as islands\nof tractability, as well as for devising clever heuristics for solving\nefficiently real-world instances. Many heuristic approaches are based on\nenforcing on the given instance a property called local consistency, where (in\ndatabase terms) each tuple in every query atom matches at least one tuple in\nevery other query atom. Interestingly, it turns out that, for many well-known\nclasses of queries, such as for the acyclic queries, enforcing local\nconsistency is even sufficient to solve the given instance correctly. However,\nthe precise power of such a procedure was unclear, but for some very restricted\ncases. The paper provides full answers to the long-standing questions about the\nprecise power of algorithms based on enforcing local consistency. The classes\nof instances where enforcing local consistency turns out to be a correct\nquery-answering procedure are however not efficiently recognizable. In fact,\nthe paper finally focuses on certain subclasses defined in terms of the novel\nnotion of greedy tree projections. These latter classes are shown to be\nefficiently recognizable and strictly larger than most islands of tractability\nknown so far, both in the general case of tree projections and for specific\nstructural decomposition methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2012 11:00:48 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2012 18:01:40 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Greco", "Gianluigi", ""], ["Scarcello", "Francesco", ""]]}, {"id": "1205.4378", "submitter": "Yu Zheng", "authors": "Yin Zhu, Yu Zheng, Liuhang Zhang, Darshan Santani, Xing Xie, Qiang\n  Yang", "title": "Inferring Taxi Status Using GPS Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we infer the statuses of a taxi, consisting of occupied,\nnon-occupied and parked, in terms of its GPS trajectory. The status information\ncan enable urban computing for improving a city's transportation systems and\nland use planning. In our solution, we first identify and extract a set of\neffective features incorporating the knowledge of a single trajectory,\nhistorical trajectories and geographic data like road network. Second, a\nparking status detection algorithm is devised to find parking places (from a\ngiven trajectory), dividing a trajectory into segments (i.e.,\nsub-trajectories). Third, we propose a two-phase inference model to learn the\nstatus (occupied or non-occupied) of each point from a taxi segment. This model\nfirst uses the identified features to train a local probabilistic classifier\nand then carries out a Hidden Semi-Markov Model (HSMM) for globally considering\nlong term travel patterns. We evaluated our method with a large-scale\nreal-world trajectory dataset generated by 600 taxis, showing the advantages of\nour method over baselines.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2012 03:24:25 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2012 08:15:27 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Zhu", "Yin", ""], ["Zheng", "Yu", ""], ["Zhang", "Liuhang", ""], ["Santani", "Darshan", ""], ["Xie", "Xing", ""], ["Yang", "Qiang", ""]]}, {"id": "1205.4477", "submitter": "Srivatsan Laxman", "authors": "Debprakash Patnaik and Naren Ramakrishnan and Srivatsan Laxman and\n  Badrish Chandramouli", "title": "Streaming Algorithms for Pattern Discovery over Dynamically Changing\n  Event Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering frequent episodes over event sequences is an important data\nmining task. In many applications, events constituting the data sequence arrive\nas a stream, at furious rates, and recent trends (or frequent episodes) can\nchange and drift due to the dynamical nature of the underlying event generation\nprocess. The ability to detect and track such the changing sets of frequent\nepisodes can be valuable in many application scenarios. Current methods for\nfrequent episode discovery are typically multipass algorithms, making them\nunsuitable in the streaming context. In this paper, we propose a new streaming\nalgorithm for discovering frequent episodes over a window of recent events in\nthe stream. Our algorithm processes events as they arrive, one batch at a time,\nwhile discovering the top frequent episodes over a window consisting of several\nbatches in the immediate past. We derive approximation guarantees for our\nalgorithm under the condition that frequent episodes are approximately\nwell-separated from infrequent ones in every batch of the window. We present\nextensive experimental evaluations of our algorithm on both real and synthetic\ndata. We also present comparisons with baselines and adaptations of streaming\nalgorithms from itemset mining literature.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 01:46:57 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Patnaik", "Debprakash", ""], ["Ramakrishnan", "Naren", ""], ["Laxman", "Srivatsan", ""], ["Chandramouli", "Badrish", ""]]}, {"id": "1205.4655", "submitter": "Miroslaw Truszczynski", "authors": "Luciano Caroprese, Irina Trubitsyna, Miroslaw Truszczynski, Ester\n  Zumpano", "title": "The View-Update Problem for Indefinite Databases", "comments": "24 pages (includes proofs)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and studies a declarative framework for updating views\nover indefinite databases. An indefinite database is a database with null\nvalues that are represented, following the standard database approach, by a\nsingle null constant. The paper formalizes views over such databases as\nindefinite deductive databases, and defines for them several classes of\ndatabase repairs that realize view-update requests. Most notable is the class\nof constrained repairs. Constrained repairs change the database \"minimally\" and\navoid making arbitrary commitments. They narrow down the space of alternative\nways to fulfill the view-update request to those that are grounded, in a\ncertain strong sense, in the database, the view and the view-update request.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 16:42:06 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Caroprese", "Luciano", ""], ["Trubitsyna", "Irina", ""], ["Truszczynski", "Miroslaw", ""], ["Zumpano", "Ester", ""]]}, {"id": "1205.4813", "submitter": "Praveen Sivadasan", "authors": "Praveen Sivadasan, P. Sojan Lal", "title": "Securing SQLJ Source Codes from Business Logic Disclosure by Data Hiding\n  Obfuscation", "comments": "4 pages,3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information security is protecting information from unauthorized access, use,\ndisclosure, disruption, modification, perusal and destruction. CAIN model\nsuggest maintaining the Confidentiality, Authenticity, Integrity and\nNon-repudiation (CAIN) of information. Oracle 8i, 9i and 11g Databases support\nSQLJ framework allowing embedding of SQL statements in Java Programs and\nproviding programmer friendly means to access the Oracle database. As cloud\ncomputing technology is becoming popular, SQLJ is considered as a flexible and\nuser friendly language for developing distributed applications in grid\narchitectures. SQLJ source codes are translated to java byte codes and\ndecompilation is generation of source codes from intermediate byte codes. The\nintermediate SQLJ application byte codes are open to decompilation, allowing a\nmalicious reader to forcefully decompile it for understanding confidential\nbusiness logic or data from the codes. To the best of our knowledge, strong and\ncost effective techniques exist for Oracle Database security, but still data\nsecurity techniques are lacking for client side applications, giving\npossibility for revelation of confidential business data. Data obfuscation is\nhiding the data in codes and we suggest enhancing the data security in SQLJ\nsource codes by data hiding, to mitigate disclosure of confidential business\ndata, especially integers in distributed applications.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2012 06:18:32 GMT"}], "update_date": "2012-05-23", "authors_parsed": [["Sivadasan", "Praveen", ""], ["Lal", "P. Sojan", ""]]}, {"id": "1205.5353", "submitter": "Ravindra Jain", "authors": "Ravindra Jain", "title": "A hybrid clustering algorithm for data mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is a process of arranging similar data into groups. A\nclustering algorithm partitions a data set into several groups such that the\nsimilarity within a group is better than among groups. In this paper a hybrid\nclustering algorithm based on K-mean and K-harmonic mean (KHM) is described.\nThe proposed algorithm is tested on five different datasets. The research is\nfocused on fast and accurate clustering. Its performance is compared with the\ntraditional K-means & KHM algorithm. The result obtained from proposed hybrid\nalgorithm is much better than the traditional K-mean & KHM algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2012 07:37:28 GMT"}], "update_date": "2012-05-25", "authors_parsed": [["Jain", "Ravindra", ""]]}, {"id": "1205.5745", "submitter": "Hubie Chen", "authors": "Simone Bova and Hubie Chen and Matthew Valeriote", "title": "Generic Expression Hardness Results for Primitive Positive Formula\n  Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the expression complexity of two basic problems involving the\ncomparison of primitive positive formulas: equivalence and containment. In\nparticular, we study the complexity of these problems relative to finite\nrelational structures. We present two generic hardness results for the studied\nproblems, and discuss evidence that they are optimal and yield, for each of the\nproblems, a complexity trichotomy.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2012 16:18:29 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Bova", "Simone", ""], ["Chen", "Hubie", ""], ["Valeriote", "Matthew", ""]]}, {"id": "1205.5921", "submitter": "Noreddine Gherabi", "authors": "Noreddine GHERABI, Mohamed BAHAJ", "title": "Robust representation for conversion UML class into XML Document using\n  DOM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Framework for converting a class diagram into an XML\nstructure and shows how to use Web files for the design of data warehouses\nbased on the classification UML. Extensible Markup Language (XML) has become a\nstandard for representing data over the Internet. We use XSD schema for define\nthe structure of XML documents and validate XML documents.\n  A prototype has been developed, which migrates successfully UML Class into\nXML document based on the formulation mathematics model. The experimental\nresults were very encouraging, demonstrating that the proposed approach is\nfeasible efficient and correct.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2012 22:37:10 GMT"}], "update_date": "2012-05-29", "authors_parsed": [["GHERABI", "Noreddine", ""], ["BAHAJ", "Mohamed", ""]]}, {"id": "1205.5922", "submitter": "Noreddine Gherabi", "authors": "Noreddine Gherabi, Khaoula Addakiri, Mohamed Bahaj", "title": "Mapping relational database into OWL Structure with data semantic\n  preservation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a solution for migrating an RDB into Web semantic. The\nsolution takes an existing RDB as input, and extracts its metadata\nrepresentation (MTRDB). Based on the MTRDB, a Canonical Data Model (CDM) is\ngenerated. Finally, the structure of the classification scheme in the CDM model\nis converted into OWL ontology and the recordsets of database are stored in owl\ndocument. A prototype has been implemented, which migrates a RDB into OWL\nstructure, for demonstrate the practical applicability of our approach by\nshowing how the results of reasoning of this technique can help improve the Web\nsystems.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2012 22:41:44 GMT"}], "update_date": "2012-05-29", "authors_parsed": [["Gherabi", "Noreddine", ""], ["Addakiri", "Khaoula", ""], ["Bahaj", "Mohamed", ""]]}, {"id": "1205.6691", "submitter": "Hongzhi Wang", "authors": "Zhao Sun, Hongzhi Wang, Haixun Wang, Bin Shao, Jianzhong Li", "title": "Efficient Subgraph Matching on Billion Node Graphs", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.\n  788-799 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to handle large scale graph data is crucial to an increasing\nnumber of applications. Much work has been dedicated to supporting basic graph\noperations such as subgraph matching, reachability, regular expression\nmatching, etc. In many cases, graph indices are employed to speed up query\nprocessing. Typically, most indices require either super-linear indexing time\nor super-linear indexing space. Unfortunately, for very large graphs,\nsuper-linear approaches are almost always infeasible. In this paper, we study\nthe problem of subgraph matching on billion-node graphs. We present a novel\nalgorithm that supports efficient subgraph matching for graphs deployed on a\ndistributed memory store. Instead of relying on super-linear indices, we use\nefficient graph exploration and massive parallel computing for query\nprocessing. Our experimental results demonstrate the feasibility of performing\nsubgraph matching on web-scale graph data.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 14:32:16 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Sun", "Zhao", ""], ["Wang", "Hongzhi", ""], ["Wang", "Haixun", ""], ["Shao", "Bin", ""], ["Li", "Jianzhong", ""]]}, {"id": "1205.6692", "submitter": "Ye Yuan", "authors": "Ye Yuan, Guoren Wang, Lei Chen, Haixun Wang", "title": "Efficient Subgraph Similarity Search on Large Probabilistic Graph\n  Databases", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.\n  800-811 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have been conducted on seeking the efficient solution for\nsubgraph similarity search over certain (deterministic) graphs due to its wide\napplication in many fields, including bioinformatics, social network analysis,\nand Resource Description Framework (RDF) data management. All these works\nassume that the underlying data are certain. However, in reality, graphs are\noften noisy and uncertain due to various factors, such as errors in data\nextraction, inconsistencies in data integration, and privacy preserving\npurposes. Therefore, in this paper, we study subgraph similarity search on\nlarge probabilistic graph databases. Different from previous works assuming\nthat edges in an uncertain graph are independent of each other, we study the\nuncertain graphs where edges' occurrences are correlated. We formally prove\nthat subgraph similarity search over probabilistic graphs is #P-complete, thus,\nwe employ a filter-and-verify framework to speed up the search. In the\nfiltering phase,we develop tight lower and upper bounds of subgraph similarity\nprobability based on a probabilistic matrix index, PMI. PMI is composed of\ndiscriminative subgraph features associated with tight lower and upper bounds\nof subgraph isomorphism probability. Based on PMI, we can sort out a large\nnumber of probabilistic graphs and maximize the pruning capability. During the\nverification phase, we develop an efficient sampling algorithm to validate the\nremaining candidates. The efficiency of our proposed solutions has been\nverified through extensive experiments.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 14:32:39 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Yuan", "Ye", ""], ["Wang", "Guoren", ""], ["Chen", "Lei", ""], ["Wang", "Haixun", ""]]}, {"id": "1205.6693", "submitter": "Jia Wang", "authors": "Jia Wang, James Cheng", "title": "Truss Decomposition in Massive Networks", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.\n  812-823 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-truss is a type of cohesive subgraphs proposed recently for the study\nof networks. While the problem of computing most cohesive subgraphs is NP-hard,\nthere exists a polynomial time algorithm for computing k-truss. Compared with\nk-core which is also efficient to compute, k-truss represents the \"core\" of a\nk-core that keeps the key information of, while filtering out less important\ninformation from, the k-core. However, existing algorithms for computing\nk-truss are inefficient for handling today's massive networks. We first improve\nthe existing in-memory algorithm for computing k-truss in networks of moderate\nsize. Then, we propose two I/O-efficient algorithms to handle massive networks\nthat cannot fit in main memory. Our experiments on real datasets verify the\nefficiency of our algorithms and the value of k-truss.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 14:32:46 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Wang", "Jia", ""], ["Cheng", "James", ""]]}, {"id": "1205.6694", "submitter": "Ju Fan", "authors": "Ju Fan, Guoliang Li, Lizhu Zhou, Shanshan Chen, Jun Hu", "title": "SEAL: Spatio-Textual Similarity Search", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.\n  824-835 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location-based services (LBS) have become more and more ubiquitous recently.\nExisting methods focus on finding relevant points-of-interest (POIs) based on\nusers' locations and query keywords. Nowadays, modern LBS applications generate\na new kind of spatio-textual data, regions-of-interest (ROIs), containing\nregion-based spatial information and textual description, e.g., mobile user\nprofiles with active regions and interest tags. To satisfy search requirements\non ROIs, we study a new research problem, called spatio-textual similarity\nsearch: Given a set of ROIs and a query ROI, we find the similar ROIs by\nconsidering spatial overlap and textual similarity. Spatio-textual similarity\nsearch has many important applications, e.g., social marketing in\nlocation-aware social networks. It calls for an efficient search method to\nsupport large scales of spatio-textual data in LBS systems. To this end, we\nintroduce a filter-and-verification framework to compute the answers. In the\nfilter step, we generate signatures for the ROIs and the query, and utilize the\nsignatures to generate candidates whose signatures are similar to that of the\nquery. In the verification step, we verify the candidates and identify the\nfinal answers. To achieve high performance, we generate effective high-quality\nsignatures, and devise efficient filtering algorithms as well as pruning\ntechniques. Experimental results on real and synthetic datasets show that our\nmethod achieves high performance.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 14:32:51 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Fan", "Ju", ""], ["Li", "Guoliang", ""], ["Zhou", "Lizhu", ""], ["Chen", "Shanshan", ""], ["Hu", "Jun", ""]]}, {"id": "1205.6695", "submitter": "Theodoros Lappas", "authors": "Theodoros Lappas, Marcos R. Vieira, Dimitrios Gunopulos, Vassilis J.\n  Tsotras", "title": "On The Spatiotemporal Burstiness of Terms", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.\n  836-847 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thousands of documents are made available to the users via the web on a daily\nbasis. One of the most extensively studied problems in the context of such\ndocument streams is burst identification. Given a term t, a burst is generally\nexhibited when an unusually high frequency is observed for t. While spatial and\ntemporal burstiness have been studied individually in the past, our work is the\nfirst to simultaneously track and measure spatiotemporal term burstiness. In\naddition, we use the mined burstiness information toward an efficient\ndocument-search engine: given a user's query of terms, our engine returns a\nranked list of documents discussing influential events with a strong\nspatiotemporal impact. We demonstrate the efficiency of our methods with an\nextensive experimental evaluation on real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 14:32:56 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Lappas", "Theodoros", ""], ["Vieira", "Marcos R.", ""], ["Gunopulos", "Dimitrios", ""], ["Tsotras", "Vassilis J.", ""]]}, {"id": "1205.6696", "submitter": "Houtan Shirani-Mehr", "authors": "Houtan Shirani-Mehr, Farnoush Banaei Kashani, Cyrus Shahabi", "title": "Efficient Reachability Query Evaluation in Large Spatiotemporal Contact\n  Datasets", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.\n  848-859 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of reliable positioning technologies and prevalence of\nlocation-based services, it is now feasible to accurately study the propagation\nof items such as infectious viruses, sensitive information pieces, and malwares\nthrough a population of moving objects, e.g., individuals, mobile devices, and\nvehicles. In such application scenarios, an item passes between two objects\nwhen the objects are sufficiently close (i.e., when they are, so-called, in\ncontact), and hence once an item is initiated, it can penetrate the object\npopulation through the evolving network of contacts among objects, termed\ncontact network. In this paper, for the first time we define and study\nreachability queries in large (i.e., disk-resident) contact datasets which\nrecord the movement of a (potentially large) set of objects moving in a spatial\nenvironment over an extended time period. A reachability query verifies whether\ntwo objects are \"reachable\" through the evolving contact network represented by\nsuch contact datasets. We propose two contact-dataset indexes that enable\nefficient evaluation of such queries despite the potentially humongous size of\nthe contact datasets. With the first index, termed ReachGrid, at the query time\nonly a small necessary portion of the contact network which is required for\nreachability evaluation is constructed and traversed. With the second approach,\ntermed ReachGraph, we precompute reachability at different scales and leverage\nthese precalculations at the query time for efficient query processing. We\noptimize the placement of both indexes on disk to enable efficient index\ntraversal during query processing. We study the pros and cons of our proposed\napproaches by performing extensive experiments with both real and synthetic\ndata. Based on our experimental results, our proposed approaches outperform\nexisting reachability query processing techniques in contact n...[truncated].\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 14:33:01 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Shirani-Mehr", "Houtan", ""], ["Kashani", "Farnoush Banaei", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1205.6697", "submitter": "Thi Nguyen", "authors": "Thi Nguyen, Zhen He, Rui Zhang, Phillip Ward", "title": "Boosting Moving Object Indexing through Velocity Partitioning", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.\n  860-871 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been intense research interests in moving object indexing in the\npast decade. However, existing work did not exploit the important property of\nskewed velocity distributions. In many real world scenarios, objects travel\npredominantly along only a few directions. Examples include vehicles on road\nnetworks, flights, people walking on the streets, etc. The search space for a\nquery is heavily dependent on the velocity distribution of the objects grouped\nin the nodes of an index tree. Motivated by this observation, we propose the\nvelocity partitioning (VP) technique, which exploits the skew in velocity\ndistribution to speed up query processing using moving object indexes. The VP\ntechnique first identifies the \"dominant velocity axes (DVAs)\" using a\ncombination of principal components analysis (PCA) and k-means clustering.\nThen, a moving object index (e.g., a TPR-tree) is created based on each DVA,\nusing the DVA as an axis of the underlying coordinate system. An object is\nmaintained in the index whose DVA is closest to the object's current moving\ndirection. Thus, all the objects in an index are moving in a near 1-dimensional\nspace instead of a 2-dimensional space. As a result, the expansion of the\nsearch space with time is greatly reduced, from a quadratic function of the\nmaximum speed (of the objects in the search range) to a near linear function of\nthe maximum speed. The VP technique can be applied to a wide range of moving\nobject index structures. We have implemented the VP technique on two\nrepresentative ones, the TPR*-tree and the Bx-tree. Extensive experiments\nvalidate that the VP technique consistently improves the performance of those\nindex structures.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 14:33:06 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Nguyen", "Thi", ""], ["He", "Zhen", ""], ["Zhang", "Rui", ""], ["Ward", "Phillip", ""]]}, {"id": "1205.6698", "submitter": "Federico Ulliana", "authors": "Nicole Bidoit-Tollu, Dario Colazzo, Federico Ulliana", "title": "Type-Based Detection of XML Query-Update Independence", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.\n  872-883 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel static analysis technique to detect XML\nquery-update independence, in the presence of a schema. Rather than types, our\nsystem infers chains of types. Each chain represents a path that can be\ntraversed on a valid document during query/update evaluation. The resulting\nindependence analysis is precise, although it raises a challenging issue:\nrecursive schemas may lead to infer infinitely many chains. A sound and\ncomplete approximation technique ensuring a finite analysis in any case is\npresented, together with an efficient implementation performing the chain-based\nanalysis in polynomial space and time.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 14:33:10 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Bidoit-Tollu", "Nicole", ""], ["Colazzo", "Dario", ""], ["Ulliana", "Federico", ""]]}, {"id": "1205.6699", "submitter": "Benjamin Sowell", "authors": "Benjamin Sowell, Wojciech Golab, Mehul A. Shah", "title": "Minuet: A Scalable Distributed Multiversion B-Tree", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.\n  884-895 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data management systems have traditionally been designed to support either\nlong-running analytics queries or short-lived transactions, but an increasing\nnumber of applications need both. For example, online games, socio-mobile apps,\nand e-commerce sites need to not only maintain operational state, but also\nanalyze that data quickly to make predictions and recommendations that improve\nuser experience. In this paper, we present Minuet, a distributed, main-memory\nB-tree that supports both transactions and copy-on-write snapshots for in-situ\nanalytics. Minuet uses main-memory storage to enable low-latency transactional\noperations as well as analytics queries without compromising transaction\nperformance. In addition to supporting read-only analytics queries on\nsnapshots, Minuet supports writable clones, so that users can create branching\nversions of the data. This feature can be quite useful, e.g. to support complex\n\"what-if\" analysis or to facilitate wide-area replication. Our experiments show\nthat Minuet outperforms a commercial main-memory database in many ways. It\nscales to hundreds of cores and TBs of memory, and can process hundreds of\nthousands of B-tree operations per second while executing long-running scans.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 14:33:38 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Sowell", "Benjamin", ""], ["Golab", "Wojciech", ""], ["Shah", "Mehul A.", ""]]}, {"id": "1205.6700", "submitter": "Hongzhi Yin", "authors": "Hongzhi Yin, Bin Cui, Jing Li, Junjie Yao, Chen Chen", "title": "Challenging the Long Tail Recommendation", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.\n  896-907 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of \"infinite-inventory\" retailers such as Amazon.com and Netflix\nhas been largely attributed to a \"long tail\" phenomenon. Although the majority\nof their inventory is not in high demand, these niche products, unavailable at\nlimited-inventory competitors, generate a significant fraction of total revenue\nin aggregate. In addition, tail product availability can boost head sales by\noffering consumers the convenience of \"one-stop shopping\" for both their\nmainstream and niche tastes. However, most of existing recommender systems,\nespecially collaborative filter based methods, can not recommend tail products\ndue to the data sparsity issue. It has been widely acknowledged that to\nrecommend popular products is easier yet more trivial while to recommend long\ntail products adds more novelty yet it is also a more challenging task. In this\npaper, we propose a novel suite of graph-based algorithms for the long tail\nrecommendation. We first represent user-item information with undirected\nedge-weighted graph and investigate the theoretical foundation of applying\nHitting Time algorithm for long tail item recommendation. To improve\nrecommendation diversity and accuracy, we extend Hitting Time and propose\nefficient Absorbing Time algorithm to help users find their favorite long tail\nitems. Finally, we refine the Absorbing Time algorithm and propose two\nentropy-biased Absorbing Cost algorithms to distinguish the variation on\ndifferent user-item rating pairs, which further enhances the effectiveness of\nlong tail recommendation. Empirical experiments on two real life datasets show\nthat our proposed algorithms are effective to recommend long tail items and\noutperform state-of-the-art recommendation techniques.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 14:33:56 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Yin", "Hongzhi", ""], ["Cui", "Bin", ""], ["Li", "Jing", ""], ["Yao", "Junjie", ""], ["Chen", "Chen", ""]]}]