[{"id": "2011.00096", "submitter": "Peter Lindner", "authors": "Martin Grohe, Peter Lindner", "title": "Independence in Infinite Probabilistic Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic databases (PDBs) model uncertainty in data. The current\nstandard is to view PDBs as finite probability spaces over relational database\ninstances. Since many attributes in typical databases have infinite domains,\nsuch as integers, strings, or real numbers, it is often more natural to view\nPDBs as infinite probability spaces over database instances. In this paper, we\nlay the mathematical foundations of infinite probabilistic databases.\n  Our focus then is on independence assumptions. Tuple-independent PDBs play a\ncentral role in theory and practice of PDBs. Here, we study infinite\ntuple-independent PDBs as well as related models such as infinite\nblock-independent disjoint PDBs. While the standard model of PDBs focuses on a\nset-based semantics, we also study tuple-independent PDBs with a bag semantics\nand propose Poisson-PDBs as a suitable model. It turns out that for uncountable\nPDBs, Poisson-PDBs form a natural model of tuple-independence even for a set\nsemantics, and they nicely lock-in with the mathematical theory of Poisson\nprocesses.\n  We also propose a new approach to PDBs with an open-world assumption,\naddressing issues raised by Ceylan et al. (Proc. KR 2016) and generalizing\ntheir work, which is still rooted in finite tuple-independent PDBs.\n  Moreover, for countable PDBs we propose an approximate query answering\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 20:34:39 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Grohe", "Martin", ""], ["Lindner", "Peter", ""]]}, {"id": "2011.00235", "submitter": "Bilal Abu-Salih", "authors": "Bilal Abu-Salih", "title": "Domain-specific Knowledge Graphs: A survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs (KGs) have made a qualitative leap and effected a real\nrevolution in knowledge representation. This is leveraged by the underlying\nstructure of the KG which underpins a better comprehension, reasoning and\ninterpretation of knowledge for both human and machine. Therefore, KGs continue\nto be used as the main means of tackling a plethora of real-life problems in\nvarious domains. However, there is no consensus in regard to a plausible and\ninclusive definition of a domain-specific KG. Further, in conjunction with\nseveral limitations and deficiencies, various domain-specific KG construction\napproaches are far from perfect. This survey is the first to offer a\ncomprehensive definition of a domain-specific KG. Also, the paper presents a\nthorough review of the state-of-the-art approaches drawn from academic works\nrelevant to seven domains of knowledge. An examination of current approaches\nreveals a range of limitations and deficiencies. At the same time, uncharted\nterritories on the research map are highlighted to tackle extant issues in the\nliterature and point to directions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 10:39:53 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 16:02:24 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 13:25:56 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Abu-Salih", "Bilal", ""]]}, {"id": "2011.00867", "submitter": "Benjamin Murray Mr", "authors": "Benjamin Murray, Eric Kerfoot, Mark S. Graham, Carole H. Sudre, Erika\n  Molteni, Liane S. Canas, Michela Antonelli, Kerstin Klaser, Alessia Visconti,\n  Andrew T. Chan, Paul W. Franks, Richard Davies, Jonathan Wolf, Tim Spector,\n  Claire J. Steves, Marc Modat, Sebastien Ourselin", "title": "Accessible Data Curation and Analytics for International-Scale Citizen\n  Science Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covid Symptom Study, a smartphone-based surveillance study on COVID-19\nsymptoms in the population, is an exemplar of big data citizen science. Over\n4.7 million participants and 189 million unique assessments have been logged\nsince its introduction in March 2020. The success of the Covid Symptom Study\ncreates technical challenges around effective data curation for two reasons.\nFirstly, the scale of the dataset means that it can no longer be easily\nprocessed using standard software on commodity hardware. Secondly, the size of\nthe research group means that replicability and consistency of key analytics\nused across multiple publications becomes an issue. We present ExeTera, an open\nsource data curation software designed to address scalability challenges and to\nenable reproducible research across an international research group for\ndatasets such as the Covid Symptom Study dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:17:32 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 14:58:58 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Murray", "Benjamin", ""], ["Kerfoot", "Eric", ""], ["Graham", "Mark S.", ""], ["Sudre", "Carole H.", ""], ["Molteni", "Erika", ""], ["Canas", "Liane S.", ""], ["Antonelli", "Michela", ""], ["Klaser", "Kerstin", ""], ["Visconti", "Alessia", ""], ["Chan", "Andrew T.", ""], ["Franks", "Paul W.", ""], ["Davies", "Richard", ""], ["Wolf", "Jonathan", ""], ["Spector", "Tim", ""], ["Steves", "Claire J.", ""], ["Modat", "Marc", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "2011.01024", "submitter": "Chen Chen", "authors": "Chen Chen, Wenshao Zhong, Xingbo Wu", "title": "Efficient Data Management with Flexible File Address Space", "comments": "14 pages incl. references; 13 figures; 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data management applications store their data using structured files in which\ndata are usually sorted to serve indexing and queries. In order to insert or\nremove a record in a sorted file, the positions of existing data need to be\nshifted. To this end, the existing data after the insertion or removal point\nmust be rewritten to admit the change in place, which can be unaffordable for\napplications that make frequent updates. As a result, applications often employ\nextra layers of indirections to admit changes out-of-place. However, it causes\nincreased access costs and excessive complexity.\n  This paper presents a novel file abstraction, FlexFile, that provides a\nflexible file address space where in-place updates of arbitrary-sized data,\nsuch as insertions and removals, can be performed efficiently. With FlexFile,\napplications can manage their data in a linear file address space with minimal\ncomplexity. Extensive evaluation results show that a simple key-value store\nbuilt on top of this abstraction can achieve high performance for both reads\nand writes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 15:48:20 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 21:07:55 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chen", "Chen", ""], ["Zhong", "Wenshao", ""], ["Wu", "Xingbo", ""]]}, {"id": "2011.01192", "submitter": "Yikai Wu", "authors": "David Pujol, Yikai Wu, Brandon Fain, Ashwin Machanavajjhala", "title": "Budget Sharing for Multi-Analyst Differential Privacy", "comments": "13 pages, 5 figures. Proceedings of the VLDB Endowment (PVLDB) Vol.\n  14 No. 10. To be presented at the International Conference on Very Large Data\n  Bases (VLDB) 2021", "journal-ref": "PVLDB, 14(10): 1805-1817, 2021", "doi": "10.14778/3467861.3467870", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Large organizations that collect data about populations (like the US Census\nBureau) release summary statistics that are used by multiple stakeholders for\nresource allocation and policy making problems. These organizations are also\nlegally required to protect the privacy of individuals from whom they collect\ndata. Differential Privacy (DP) provides a solution to release useful summary\ndata while preserving privacy. Most DP mechanisms are designed to answer a\nsingle set of queries. In reality, there are often multiple stakeholders that\nuse a given data release and have overlapping but not-identical queries. This\nintroduces a novel joint optimization problem in DP where the privacy budget\nmust be shared among different analysts.\n  We initiate study into the problem of DP query answering across multiple\nanalysts. To capture the competing goals and priorities of multiple analysts,\nwe formulate three desiderata that any mechanism should satisfy in this setting\n-- The Sharing Incentive, Non-Interference, and Adaptivity -- while still\noptimizing for overall error. We demonstrate how existing DP query answering\nmechanisms in the multi-analyst settings fail to satisfy at least one of the\ndesiderata. We present novel DP algorithms that provably satisfy all our\ndesiderata and empirically show that they incur low error on realistic tasks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 18:33:15 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 22:11:37 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 19:19:44 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 19:26:06 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Pujol", "David", ""], ["Wu", "Yikai", ""], ["Fain", "Brandon", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "2011.01773", "submitter": "Sandra Obermeier", "authors": "Sandra Obermeier, Max Berrendorf, Peer Kr\\\"oger", "title": "Memory-Efficient RkNN Retrieval by Nonlinear k-Distance Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reverse k-nearest neighbor (RkNN) query is an established query type with\nvarious applications reaching from identifying highly influential objects over\nincrementally updating kNN graphs to optimizing sensor communication and\noutlier detection. State-of-the-art solutions exploit that the k-distances in\nreal-world datasets often follow the power-law distribution, and bound them\nwith linear lines in log-log space. In this work, we investigate this\nassumption and uncover that it is violated in regions of changing density,\nwhich we show are typical for real-life datasets. Towards a generic solution,\nwe pose the estimation of k-distances as a regression problem. Thereby, we\nenable harnessing the power of the abundance of available Machine Learning\nmodels and profiting from their advancement. We propose a flexible approach\nwhich allows steering the performance-memory consumption trade-off, and in\nparticular to find good solutions with a fixed memory budget crucial in the\ncontext of edge computing. Moreover, we show how to obtain and improve\nguaranteed bounds essential to exact query processing. In experiments on\nreal-world datasets, we demonstrate how this framework can significantly reduce\nthe index memory consumption, and strongly reduce the candidate set size. We\npublish our code at https://github.com/sobermeier/nonlinear-kdist.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:13:05 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Obermeier", "Sandra", ""], ["Berrendorf", "Max", ""], ["Kr\u00f6ger", "Peer", ""]]}, {"id": "2011.01934", "submitter": "Chihiro Noguchi", "authors": "Chihiro Noguchi and Tatsuro Kawamoto", "title": "Palette diagram: A Python package for visualization of collective\n  categorical data", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.GR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical data, wherein a numerical quantity is assigned to each category\n(nominal variable), are ubiquitous in data science. A palette diagram is a\nvisualization tool for a large number of categorical datasets, each comprising\nseveral categories.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 10:50:05 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Noguchi", "Chihiro", ""], ["Kawamoto", "Tatsuro", ""]]}, {"id": "2011.02556", "submitter": "Saeed Kargar", "authors": "Saeed Kargar, Heiner Litz, Faisal Nawab", "title": "Predict and Write: Using K-Means Clustering to Extend the Lifetime of\n  NVM Storage", "comments": "ICDE2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM) technologies suffer from limited write endurance.\nTo address this challenge, we propose Predict and Write (PNW), a K/V-store that\nuses a clustering-based machine learning approach to extend the lifetime of\nNVMs. PNW decreases the number of bit flips for PUT/UPDATE operations by\ndetermining the best memory location an updated value should be written to. PNW\nleverages the indirection level of K/V-stores to freely choose the target\nmemory location for any given write based on its value. PNW organizes NVM\naddresses in a dynamic address pool clustered by the similarity of the data\nvalues they refer to. We show that, by choosing the right target memory\nlocation for a given PUT/UPDATE operation, the number of total bit flips and\ncache lines can be reduced by up to 85% and 56% over the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 22:03:09 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Kargar", "Saeed", ""], ["Litz", "Heiner", ""], ["Nawab", "Faisal", ""]]}, {"id": "2011.02615", "submitter": "Neal E. Young", "authors": "Claire Mathieu, Rajmohan Rajaraman, Neal E. Young, Arman Yousefi", "title": "Competitive Data-Structure Dynamization", "comments": "SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-structure dynamization is a general approach for making static data\nstructures dynamic. It is used extensively in geometric settings and in the\nguise of so-called merge (or compaction) policies in big-data databases such as\nGoogle Bigtable and LevelDB (our focus). Previous theoretical work is based on\nworst-case analyses for uniform inputs -- insertions of one item at a time and\nconstant read rate. In practice, merge policies must not only handle batch\ninsertions and varying read/write ratios, they can take advantage of such\nnon-uniformity to reduce cost on a per-input basis.\n  To model this, we initiate the study of data-structure dynamization through\nthe lens of competitive analysis, via two new online set-cover problems. For\neach, the input is a sequence of disjoint sets of weighted items. The sets are\nrevealed one at a time. The algorithm must respond to each with a set cover\nthat covers all items revealed so far. It obtains the cover incrementally from\nthe previous cover by adding one or more sets and optionally removing existing\nsets. For each new set the algorithm incurs build cost equal to the weight of\nthe items in the set. In the first problem the objective is to minimize total\nbuild cost plus total query cost, where the algorithm incurs a query cost at\neach time $t$ equal to the current cover size. In the second problem, the\nobjective is to minimize the build cost while keeping the query cost from\nexceeding $k$ (a given parameter) at any time. We give deterministic online\nalgorithms for both variants, with competitive ratios of $\\Theta(\\log^* n)$ and\n$k$, respectively. The latter ratio is optimal for the second variant.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 02:09:04 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 02:26:52 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Mathieu", "Claire", ""], ["Rajaraman", "Rajmohan", ""], ["Young", "Neal E.", ""], ["Yousefi", "Arman", ""]]}, {"id": "2011.03331", "submitter": "Tobias Skovgaard Jepsen", "authors": "Florian Barth and Stefan Funke and Tobias Skovgaard Jepsen and\n  Claudius Proissl", "title": "Scalable Unsupervised Multi-Criteria Trajectory Segmentation and Driving\n  Preference Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present analysis techniques for large trajectory data sets that aim to\nprovide a semantic understanding of trajectories reaching beyond them being\npoint sequences in time and space. The presented techniques use a driving\npreference model w.r.t. road segment traversal costs, e.g., travel time and\ndistance, to analyze and explain trajectories.\n  In particular, we present trajectory mining techniques that can (a) find\ninteresting points within a trajectory indicating, e.g., a via-point, and (b)\nrecover the driving preferences of a driver based on their chosen trajectory.\nWe evaluate our techniques on the tasks of via-point identification and\npersonalized routing using a data set of more than 1 million vehicle\ntrajectories collected throughout Denmark during a 3-year period. Our\ntechniques can be implemented efficiently and are highly parallelizable,\nallowing them to scale to millions or billions of trajectories.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:32:26 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Barth", "Florian", ""], ["Funke", "Stefan", ""], ["Jepsen", "Tobias Skovgaard", ""], ["Proissl", "Claudius", ""]]}, {"id": "2011.04378", "submitter": "Marta Arias", "authors": "Christian Lezcano, Marta Arias", "title": "Characterizing Transactional Databases for Frequent Itemset Mining", "comments": "Workshop on Evaluation and Experimental Design in Data Mining and\n  Machine Learning (EDML@SDM 2019), May 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study of the characteristics of transactional databases\nused in frequent itemset mining. Such characterizations have typically been\nused to benchmark and understand the data mining algorithms working on these\ndatabases. The aim of our study is to give a picture of how diverse and\nrepresentative these benchmarking databases are, both in general but also in\nthe context of particular empirical studies found in the literature. Our\nproposed list of metrics contains many of the existing metrics found in the\nliterature, as well as new ones. Our study shows that our list of metrics is\nable to capture much of the datasets' inner complexity and thus provides a good\nbasis for the characterization of transactional datasets. Finally, we provide a\nset of representative datasets based on our characterization that may be used\nas a benchmark safely.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 12:26:14 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lezcano", "Christian", ""], ["Arias", "Marta", ""]]}, {"id": "2011.04730", "submitter": "Paulo Henrique Oliveira", "authors": "Paulo H. Oliveira, Daniel S. Kaster, Caetano Traina-Jr., Ihab F. Ilyas", "title": "Batchwise Probabilistic Incremental Data Cleaning", "comments": "29 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lack of data and data quality issues are among the main bottlenecks that\nprevent further artificial intelligence adoption within many organizations,\npushing data scientists to spend most of their time cleaning data before being\nable to answer analytical questions. Hence, there is a need for more effective\nand efficient data cleaning solutions, which, not surprisingly, is rife with\ntheoretical and engineering problems. This report addresses the problem of\nperforming holistic data cleaning incrementally, given a fixed rule set and an\nevolving categorical relational dataset acquired in sequential batches. To the\nbest of our knowledge, our contributions compose the first incremental\nframework that cleans data (i) independently of user interventions, (ii)\nwithout requiring knowledge about the incoming dataset, such as the number of\nclasses per attribute, and (iii) holistically, enabling multiple error types to\nbe repaired simultaneously, and thus avoiding conflicting repairs. Extensive\nexperiments show that our approach outperforms the competitors with respect to\nrepair quality, execution time, and memory consumption.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 20:15:02 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Oliveira", "Paulo H.", ""], ["Kaster", "Daniel S.", ""], ["Traina-Jr.", "Caetano", ""], ["Ilyas", "Ihab F.", ""]]}, {"id": "2011.04838", "submitter": "Zahid Abul-Basher", "authors": "Zahid Abul-Basher, Nikolay Yakovets, Parke Godfrey, Stanley Clark and\n  Mark Chignell", "title": "Answer Graph: Factorization Matters in Large Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our answer-graph method to evaluate SPARQL conjunctive queries (CQs) finds a\nfactorized answer set first, an answer graph, and then finds the embedding\ntuples from this. This approach can reduce greatly the cost to evaluate CQs.\nThis affords a second advantage: we can construct a cost-based planner. We\npresent the answer-graph approach, and overview our prototype system,\nWireframe. We then offer proof of concept via a micro-benchmark over the YAGO2s\ndataset with two prevalent shapes of queries, snowflake and diamond. We compare\nWireframe's performance over these against PostgreSQL, Virtuoso, MonetDB, and\nNeo4J to illustrate the performance advantages of our answer-graph approach.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 00:10:43 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Abul-Basher", "Zahid", ""], ["Yakovets", "Nikolay", ""], ["Godfrey", "Parke", ""], ["Clark", "Stanley", ""], ["Chignell", "Mark", ""]]}, {"id": "2011.05549", "submitter": "Shaleen Deep", "authors": "Shaleen Deep, Anja Gruenheid, Paraschos Koutris, Jeffrey Naughton,\n  Stratis Viglas", "title": "Comprehensive and Efficient Workload Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work studies the problem of constructing a representative workload from\na given input analytical query workload where the former serves as an\napproximation with guarantees of the latter. We discuss our work in the context\nof workload analysis and monitoring. As an example, evolving system usage\npatterns in a database system can cause load imbalance and performance\nregressions which can be controlled by monitoring system usage patterns,\ni.e.,~a representative workload, over time. To construct such a workload in a\nprincipled manner, we formalize the notions of workload {\\em representativity}\nand {\\em coverage}. These metrics capture the intuition that the distribution\nof features in a compressed workload should match a target distribution,\nincreasing representativity, and include common queries as well as outliers,\nincreasing coverage. We show that solving this problem optimally is NP-hard and\npresent a novel greedy algorithm that provides approximation guarantees. We\ncompare our techniques to established algorithms in this problem space such as\nsampling and clustering, and demonstrate advantages and key trade-offs of our\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 05:16:30 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 05:15:03 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Deep", "Shaleen", ""], ["Gruenheid", "Anja", ""], ["Koutris", "Paraschos", ""], ["Naughton", "Jeffrey", ""], ["Viglas", "Stratis", ""]]}, {"id": "2011.06330", "submitter": "Mika\\\"el Monet", "authors": "Marcelo Arenas, Pablo Barcel\\'o, Mika\\\"el Monet", "title": "The Complexity of Counting Problems over Incomplete Databases", "comments": "51 pages, including 43 pages of main text. Extended version of\n  arXiv:1912.11064. Up to the stylesheet, page/environment numbering, minor\n  formatting, and publisher-induced changes, this is the exact content of the\n  paper in ACM Transactions on Computational Logic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the complexity of various fundamental counting problems that arise\nin the context of incomplete databases, i.e., relational databases that can\ncontain unknown values in the form of labeled nulls. Specifically, we assume\nthat the domains of these unknown values are finite and, for a Boolean query\n$q$, we consider the following two problems: given as input an incomplete\ndatabase $D$, (a) return the number of completions of $D$ that satisfy $q$; or\n(b) return the number of valuations of the nulls of $D$ yielding a completion\nthat satisfies $q$. We obtain dichotomies between \\#P-hardness and\npolynomial-time computability for these problems when $q$ is a self-join-free\nconjunctive query, and study the impact on the complexity of the following two\nrestrictions: (1) every null occurs at most once in $D$ (what is called Codd\ntables); and (2) the domain of each null is the same. Roughly speaking, we show\nthat counting completions is much harder than counting valuations: for\ninstance, while the latter is always in \\#P, we prove that the former is not in\n\\#P under some widely believed theoretical complexity assumption. Moreover, we\nfind that both (1) and (2) can reduce the complexity of our problems. We also\nstudy the approximability of these problems and show that, while counting\nvaluations always has a fully polynomial-time randomized approximation scheme\n(FPRAS), in most cases counting completions does not. Finally, we consider more\nexpressive query languages and situate our problems with respect to known\ncomplexity classes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 12:05:17 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 07:35:17 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Arenas", "Marcelo", ""], ["Barcel\u00f3", "Pablo", ""], ["Monet", "Mika\u00ebl", ""]]}, {"id": "2011.06381", "submitter": "Milos Nikolic", "authors": "Jaclyn Smith, Michael Benedikt, Milos Nikolic, Amir Shaikhha", "title": "Scalable Querying of Nested Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While large-scale distributed data processing platforms have become an\nattractive target for query processing, these systems are problematic for\napplications that deal with nested collections. Programmers are forced either\nto perform non-trivial translations of collection programs or to employ\nautomated flattening procedures, both of which lead to performance problems.\nThese challenges only worsen for nested collections with skewed cardinalities,\nwhere both handcrafted rewriting and automated flattening are unable to enforce\nload balancing across partitions.\n  In this work, we propose a framework that translates a program manipulating\nnested collections into a set of semantically equivalent shredded queries that\ncan be efficiently evaluated. The framework employs a combination of query\ncompilation techniques, an efficient data representation for nested\ncollections, and automated skew-handling. We provide an extensive experimental\nevaluation, demonstrating significant improvements provided by the framework in\ndiverse scenarios for nested collection programs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 13:45:08 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Smith", "Jaclyn", ""], ["Benedikt", "Michael", ""], ["Nikolic", "Milos", ""], ["Shaikhha", "Amir", ""]]}, {"id": "2011.06423", "submitter": "Mario Scrocca", "authors": "Mario Scrocca, Marco Comerio, Alessio Carenini and Irene Celino", "title": "Turning Transport Data to Comply with EU Standards while Enabling a\n  Multimodal Transport Knowledge Graph", "comments": "International Semantic Web Conference (ISWC 2020) - In Use Track", "journal-ref": null, "doi": "10.1007/978-3-030-62466-8_26", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complying with the EU Regulation on multimodal transportation services\nrequires sharing data on the National Access Points in one of the standards\n(e.g., NeTEx and SIRI) indicated by the European Commission. These standards\nare complex and of limited practical adoption. This means that datasets are\nnatively expressed in other formats and require a data translation process for\nfull compliance.\n  This paper describes the solution to turn the authoritative data of three\ndifferent transport stakeholders from Italy and Spain into a format compliant\nwith EU standards by means of Semantic Web technologies. Our solution addresses\nthe challenge and also contributes to build a multi-modal transport Knowledge\nGraph of interlinked and interoperable information that enables intelligent\nquerying and exploration, as well as facilitates the design of added-value\nservices.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 14:56:15 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Scrocca", "Mario", ""], ["Comerio", "Marco", ""], ["Carenini", "Alessio", ""], ["Celino", "Irene", ""]]}, {"id": "2011.07376", "submitter": "George Obaido", "authors": "George Obaido, Abejide Ade-Ibijola, Hima Vadapalli", "title": "Synthesis of SQL Queries from South African Local Language Narrations", "comments": "8 pages, 3 figures, Advances in Science, Technology and Engineering\n  Systems Journal", "journal-ref": "5(5), 1189-1195 (2020)", "doi": "10.25046/aj0505144", "report-no": null, "categories": "cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  English remains the language of choice for database courses and widely used\nfor instruction in nearly all South African universities, and also in many\ncountries. Novice programmers of native origins are mostly taught Structured\nQuery Language (SQL) through English as the medium of instruction.\nConsequently, this creates a myriad of problems in understanding the syntax of\nSQL as most native learners are not too proficient in English. This could\naffect a learner's ability in comprehending SQL syntaxes. To resolve this\nproblem, this work proposes a tool called local language narrations\n(Local-Nar-SQL) to SQL that uses a type of Finite Machine, such as a Jumping\nFinite Automaton to translate local language narratives into SQL queries.\nFurther, the generated query extracts information from a sample database and\npresents output to the learner. This paper is an extension of work originally\npresented in a previous study in this field. A survey involving 145\nparticipants concluded that the majority found Local-Nar-SQL to be helpful in\nunderstanding SQL queries from local languages. If the proposed tool is used as\na learning aid, native learners will find it easier to work with SQL, which\nwill eliminate many of the barriers faced with English proficiencies in\nprogramming pedagogies.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 19:38:08 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Obaido", "George", ""], ["Ade-Ibijola", "Abejide", ""], ["Vadapalli", "Hima", ""]]}, {"id": "2011.07423", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi", "title": "Declarative Approaches to Counterfactual Explanations for Classification", "comments": "Revised and considerably extended version of journal submission after\n  reviews, by invitation. Based on RuleML-RR'20 paper [arXiv:2004.13237]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose answer-set programs that specify and compute counterfactual\ninterventions on entities that are input on a classification model. In relation\nto the outcome of the model, the resulting counterfactual entities serve as a\nbasis for the definition and computation of causality-based explanation scores\nfor the feature values in the entity under classification, namely\n\"responsibility scores\". The approach and the programs can be applied with\nblack-box models, and also with models that can be specified as logic programs,\nsuch as rule-based classifiers. The main focus of this work is on the\nspecification and computation of \"best\" counterfactual entities, i.e. those\nthat lead to maximum responsibility scores. From them one can read off the\nexplanations as maximum responsibility feature values in the original entity.\nWe also extend the programs to bring into the picture semantic or domain\nknowledge. We show how the approach could be extended by means of probabilistic\nmethods, and how the underlying probability distributions could be modified\nthrough the use of constraints.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 00:44:33 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 01:33:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bertossi", "Leopoldo", ""]]}, {"id": "2011.07921", "submitter": "Nikolas Ioannou", "authors": "Thomas Schmied, Diego Didona, Andreas D\\\"oring, Thomas Parnell, and\n  Nikolas Ioannou", "title": "Towards a General Framework for ML-based Self-tuning Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) methods have recently emerged as an effective way to\nperform automated parameter tuning of databases. State-of-the-art approaches\ninclude Bayesian optimization (BO) and reinforcement learning (RL). In this\nwork, we describe our experience when applying these methods to a database not\nyet studied in this context: FoundationDB. Firstly, we describe the challenges\nwe faced, such as unknown valid ranges of configuration parameters and\ncombinations of parameter values that result in invalid runs, and how we\nmitigated them. While these issues are typically overlooked, we argue that they\nare a crucial barrier to the adoption of ML self-tuning techniques in\ndatabases, and thus deserve more attention from the research community.\nSecondly, we present experimental results obtained when tuning FoundationDB\nusing ML methods. Unlike prior work in this domain, we also compare with the\nsimplest of baselines: random search. Our results show that, while BO and RL\nmethods can improve the throughput of FoundationDB by up to 38%, random search\nis a highly competitive baseline, finding a configuration that is only 4% worse\nthan the, vastly more complex, ML methods. We conclude that future work in this\narea may want to focus more on randomized, model-free optimization algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:13:10 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 15:57:04 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Schmied", "Thomas", ""], ["Didona", "Diego", ""], ["D\u00f6ring", "Andreas", ""], ["Parnell", "Thomas", ""], ["Ioannou", "Nikolas", ""]]}, {"id": "2011.07957", "submitter": "Markus Schr\\\"oder", "authors": "Markus Schr\\\"oder, Michael Schulze, Christian Jilek, Andreas Dengel", "title": "Bridging the Technology Gap Between Industry and Semantic Web:\n  Generating Databases and Server Code From RDF", "comments": "9 pages, accepted at ICAART 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite great advances in the area of Semantic Web, industry rather seldom\nadopts Semantic Web technologies and their storage and query concepts. Instead,\nrelational databases (RDB) are often deployed to store business-critical data,\nwhich are accessed via REST interfaces. Yet, some enterprises would greatly\nbenefit from Semantic Web related datasets which are usually represented with\nthe Resource Description Framework (RDF). To bridge this technology gap, we\npropose a fully automatic approach that generates suitable RDB models with REST\nAPIs to access them. In our evaluation, generated databases from different RDF\ndatasets are examined and compared. Our findings show that the databases\nsufficiently reflect their counterparts while the API is able to reproduce\nrather simple SPARQL queries. Potentials for improvements are identified, for\nexample, the reduction of data redundancies in generated databases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:50:43 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Schr\u00f6der", "Markus", ""], ["Schulze", "Michael", ""], ["Jilek", "Christian", ""], ["Dengel", "Andreas", ""]]}, {"id": "2011.08077", "submitter": "Jan Martin Keil", "authors": "Jan Martin Keil", "title": "Why Not to Use Binary Floating Point Datatypes in RDF", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The XSD binary floating point datatypes are regularly used for precise\nnumeric values in RDF. However, the use of these datatypes for knowledge\nrepresentation can systematically impair the quality of data and, compared to\nthe XSD decimal datatype, increases the probability of data processing\nproducing false results. We argue why in most cases the XSD decimal datatype is\nbetter suited to represent numeric values in RDF.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 14:27:50 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Keil", "Jan Martin", ""]]}, {"id": "2011.08253", "submitter": "Isaac Sheff", "authors": "Isaac Sheff, Xinwen Wang, Robbert van Renesse, Andrew C. Myers", "title": "Heterogeneous Paxos: Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In distributed systems, a group of $\\textit{learners}$ achieve\n$\\textit{consensus}$ when, by observing the output of some\n$\\textit{acceptors}$, they all arrive at the same value. Consensus is crucial\nfor ordering transactions in failure-tolerant systems. Traditional consensus\nalgorithms are homogeneous in three ways:\n  - all learners are treated equally,\n  - all acceptors are treated equally, and\n  - all failures are treated equally.\n  These assumptions, however, are unsuitable for cross-domain applications,\nincluding blockchains, where not all acceptors are equally trustworthy, and not\nall learners have the same assumptions and priorities. We present the first\nconsensus algorithm to be heterogeneous in all three respects. Learners set\ntheir own mixed failure tolerances over differently trusted sets of acceptors.\nWe express these assumptions in a novel $\\textit{Learner Graph}$, and\ndemonstrate sufficient conditions for consensus. We present\n$\\textit{Heterogeneous Paxos}$: an extension of Byzantine Paxos. Heterogeneous\nPaxos achieves consensus for any viable Learner Graph in best-case three\nmessage sends, which is optimal. We present a proof-of-concept implementation,\nand demonstrate how tailoring for heterogeneous scenarios can save resources\nand latency.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 20:16:34 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 22:40:14 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Sheff", "Isaac", ""], ["Wang", "Xinwen", ""], ["van Renesse", "Robbert", ""], ["Myers", "Andrew C.", ""]]}, {"id": "2011.08399", "submitter": "Kai Wang", "authors": "Kai Wang, Wenjie Zhang, Xuemin Lin, Ying Zhang, Lu Qin, Yuting Zhang", "title": "Efficient and Effective Community Search on Large-scale Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite graphs are widely used to model relationships between two types of\nentities. Community search retrieves densely connected subgraphs containing a\nquery vertex, which has been extensively studied on unipartite graphs. However,\ncommunity search on bipartite graphs remains largely unexplored. Moreover, all\nexisting cohesive subgraph models on bipartite graphs can only be applied to\nmeasure the structure cohesiveness between two sets of vertices while\noverlooking the edge weight in forming the community. In this paper, we study\nthe significant (alpha, beta)-community search problem on weighted bipartite\ngraphs. Given a query vertex q, we aim to find the significant (alpha,\nbeta)-community R of q which adopts (alpha, beta)-core to characterize the\nengagement level of vertices, and maximizes the minimum edge weight\n(significance) within R.\n  To support fast retrieval of R, we first retrieve the maximal connected\nsubgraph of (alpha, beta)-core containing the query vertex (the (alpha,\nbeta)-community), and the search space is limited to this subgraph with a much\nsmaller size than the original graph. A novel index structure is presented\nwhich can be built in O(delta * m) time and takes O(delta * m) space where m is\nthe number of edges in G, delta is bounded by the square root of m and is much\nsmaller in practice. Utilizing the index, the (alpha, beta)-community can be\nretrieved in optimal time. To further obtain R, we develop peeling and\nexpansion algorithms to conduct searches by shrinking from the (alpha,\nbeta)-community and expanding from the query vertex, respectively. The\nexperimental results on real graphs not only demonstrate the effectiveness of\nthe significant (alpha, beta)-community model but also validate the efficiency\nof our query processing and indexing techniques.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 03:26:13 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Kai", ""], ["Zhang", "Wenjie", ""], ["Lin", "Xuemin", ""], ["Zhang", "Ying", ""], ["Qin", "Lu", ""], ["Zhang", "Yuting", ""]]}, {"id": "2011.08663", "submitter": "Birgitta Dresp-Langley", "authors": "Birgitta Dresp-Langley, Ole Kristian Ekseth, Jan Fesl, Seiichi Gohshi,\n  Marc Kurz, Hans-Werner Sehring", "title": "Occams Razor for Big Data? On Detecting Quality in Large Unstructured\n  Datasets", "comments": null, "journal-ref": "Appl. Sci. 2019, 9, 3065", "doi": "10.3390/app9153065", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting quality in large unstructured datasets requires capacities far\nbeyond the limits of human perception and communicability and, as a result,\nthere is an emerging trend towards increasingly complex analytic solutions in\ndata science to cope with this problem. This new trend towards analytic\ncomplexity represents a severe challenge for the principle of parsimony or\nOccams Razor in science. This review article combines insight from various\ndomains such as physics, computational science, data engineering, and cognitive\nscience to review the specific properties of big data. Problems for detecting\ndata quality without losing the principle of parsimony are then highlighted on\nthe basis of specific examples. Computational building block approaches for\ndata clustering can help to deal with large unstructured datasets in minimized\ncomputation time, and meaning can be extracted rapidly from large sets of\nunstructured image or video data parsimoniously through relatively simple\nunsupervised machine learning algorithms. Why we still massively lack in\nexpertise for exploiting big data wisely to extract relevant information for\nspecific tasks, recognize patterns, generate new information, or store and\nfurther process large amounts of sensor data is then reviewed; examples\nillustrating why we need subjective views and pragmatic methods to analyze big\ndata contents are brought forward. The review concludes on how cultural\ndifferences between East and West are likely to affect the course of big data\nanalytics, and the development of increasingly autonomous artificial\nintelligence aimed at coping with the big data deluge in the near future.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 16:06:01 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Dresp-Langley", "Birgitta", ""], ["Ekseth", "Ole Kristian", ""], ["Fesl", "Jan", ""], ["Gohshi", "Seiichi", ""], ["Kurz", "Marc", ""], ["Sehring", "Hans-Werner", ""]]}, {"id": "2011.08724", "submitter": "Yu Yan", "authors": "Yu Yan, Nan Jiang, Hongzhi Wang, Yutong Wang, Chang Liu, Yuzhuo Wang", "title": "Multi-SQL: An extensible multi-model data query language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Big data management aims to establish data hubs that support data in multiple\nmodels and types in an all-around way. Thus, the multi-model database system is\na promising architecture for building such a multi-model data store. For an\nintegrated data hub, a unified and flexible query language is incredibly\nnecessary. In this paper, an extensible and practical query language--Multi-SQL\nis proposed to realize the unified management of multi-model data considering\nthe co-processing of multi-model data. To the best of our knowledge, Multi-SQL\nis the first query language based on various data models. Multi-SQL can also be\nexpanded to suit more complicated scenarios as it is flexible to support other\ndata models. Moreover, we provide a formal semantic definition of the core\nfeatures of Multi-SQL, including the multi-model definition, multi-model\nfilters, multi-model joins, etc. Furthermore, we propose a two-level query\nimplementation method to totally exploit the existing query optimization\ncapabilities of the underlying engines which could largely improve the query\nexcution efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:54:18 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 07:36:43 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yan", "Yu", ""], ["Jiang", "Nan", ""], ["Wang", "Hongzhi", ""], ["Wang", "Yutong", ""], ["Liu", "Chang", ""], ["Wang", "Yuzhuo", ""]]}, {"id": "2011.09022", "submitter": "Rong Zhu", "authors": "Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping\n  Qian, Jingren Zhou, Bin Cui", "title": "FLAT: Fast, Lightweight and Accurate Method for Cardinality Estimation", "comments": "Technical Report of the FLAT Paper in VLDB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Query optimizers rely on accurate cardinality estimation (CardEst) to produce\ngood execution plans. The core problem of CardEst is how to model the rich\njoint distribution of attributes in an accurate and compact manner. Despite\ndecades of research, existing methods either over simplify the models only\nusing independent factorization which leads to inaccurate estimates, or over\ncomplicate them by lossless conditional factorization without any independent\nassumption which results in slow probability computation. In this paper, we\npropose FLAT, a CardEst method that is simultaneously fast in probability\ncomputation, lightweight in model size and accurate in estimation quality. The\nkey idea of FLAT is a novel unsupervised graphical model, called FSPN. It\nutilizes both independent and conditional factorization to adaptively model\ndifferent levels of attributes correlations, and thus dovetails their\nadvantages. FLAT supports efficient online probability computation in near\nliner time on the underlying FSPN model, provides effective offline model\nconstruction and enables incremental model updates. It can estimate cardinality\nfor both single table queries and multi table join queries. Extensive\nexperimental study demonstrates the superiority of FLAT over existing CardEst\nmethods on well known IMDB benchmarks: FLAT achieves 1 to 5 orders of magnitude\nbetter accuracy, 1 to 3 orders of magnitude faster probability computation\nspeed and 1 to 2 orders of magnitude lower storage cost. We also integrate FLAT\ninto Postgres to perform an end to end test. It improves the query execution\ntime by 12.9% on the benchmark workload, which is very close to the optimal\nresult 14.2% using the true cardinality.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 01:14:45 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 07:26:58 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 07:52:43 GMT"}, {"version": "v4", "created": "Fri, 5 Mar 2021 08:37:57 GMT"}, {"version": "v5", "created": "Wed, 19 May 2021 07:37:36 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhu", "Rong", ""], ["Wu", "Ziniu", ""], ["Han", "Yuxing", ""], ["Zeng", "Kai", ""], ["Pfadler", "Andreas", ""], ["Qian", "Zhengping", ""], ["Zhou", "Jingren", ""], ["Cui", "Bin", ""]]}, {"id": "2011.09176", "submitter": "Leif Sabellek", "authors": "Carsten Lutz, Johannes Marti, Leif Sabellek", "title": "Query Expressibility and Verification in Ontology-Based Data Access", "comments": null, "journal-ref": "Principles of Knowledge Representation and Reasoning: Proceedings\n  of the Sixteenth International Conference, KR 2018, Tempe, Arizona, 30\n  October - 2 November 2018, pages 389--398, AAAI Press, 2018", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ontology-based data access, multiple data sources are integrated using an\nontology and mappings. In practice, this is often achieved by a bootstrapping\nprocess, that is, the ontology and mappings are first designed to support only\nthe most important queries over the sources and then gradually extended to\nenable additional queries. In this paper, we study two reasoning problems that\nsupport such an approach. The expressibility problem asks whether a given\nsource query $q_s$ is expressible as a target query (that is, over the\nontology's vocabulary) and the verification problem asks, additionally given a\ncandidate target query $q_t$, whether $q_t$ expresses $q_s$. We consider (U)CQs\nas source and target queries and GAV mappings, showing that both problems are\n$\\Pi^p_2$-complete in DL-Lite, coNExpTime-complete between EL and ELHI when\nsource queries are rooted, and 2ExpTime-complete for unrestricted source\nqueries.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 09:50:51 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Lutz", "Carsten", ""], ["Marti", "Johannes", ""], ["Sabellek", "Leif", ""]]}, {"id": "2011.09314", "submitter": "Carsten Lutz", "authors": "Pablo Barcelo, Gerald Berger, Carsten Lutz, Andreas Pieris", "title": "First-Order Rewritability of Frontier-Guarded Ontology-Mediated Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We focus on ontology-mediated queries (OMQs) based on (frontier-)guarded\nexistential rules and (unions of) conjunctive queries, and we investigate the\nproblem of FO-rewritability, i.e., whether an OMQ can be rewritten as a\nfirst-order query. We adopt two different approaches. The first approach\nemploys standard two-way alternating parity tree automata. Although it does not\nlead to a tight complexity bound, it provides a transparent solution based on\nwidely known tools. The second approach relies on a sophisticated automata\nmodel, known as cost automata. This allows us to show that our problem is\n2ExpTime-complete. In both approaches, we provide semantic characterizations of\nFO-rewritability that are of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 14:31:17 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Barcelo", "Pablo", ""], ["Berger", "Gerald", ""], ["Lutz", "Carsten", ""], ["Pieris", "Andreas", ""]]}, {"id": "2011.09748", "submitter": "Farah Karim Ms", "authors": "Farah Karim, Maria-Esther Vidal, S\\\"oren Auer", "title": "Compact Representations for Efficient Storage of Semantic Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, there is a rapid increase in the number of sensor data generated by\na wide variety of sensors and devices. Data semantics facilitate information\nexchange, adaptability, and interoperability among several sensors and devices.\nSensor data and their meaning can be described using ontologies, e.g., the\nSemantic Sensor Network (SSN) Ontology. Notwithstanding, semantically enriched,\nthe size of semantic sensor data is substantially larger than raw sensor data.\nMoreover, some measurement values can be observed by sensors several times, and\na huge number of repeated facts about sensor data can be produced. We propose a\ncompact or factorized representation of semantic sensor data, where repeated\nmeasurement values are described only once. Furthermore, these compact\nrepresentations are able to enhance the storage and processing of semantic\nsensor data. To scale up to large datasets, factorization based, tabular\nrepresentations are exploited to store and manage factorized semantic sensor\ndata using Big Data technologies. We empirically study the effectiveness of a\nsemantic sensor's proposed compact representations and their impact on query\nprocessing. Additionally, we evaluate the effects of storing the proposed\nrepresentations on diverse RDF implementations. Results suggest that the\nproposed compact representations empower the storage and query processing of\nsensor data over diverse RDF implementations, and up to two orders of magnitude\ncan reduce query execution time.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 09:57:28 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Karim", "Farah", ""], ["Vidal", "Maria-Esther", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2011.09753", "submitter": "Rachid Zennou", "authors": "Rachid Zennou, Ranadeep Biswas, Ahmed Bouajjani, Constantin Enea,\n  Mohammed Erradi", "title": "Checking Causal Consistency of Distributed Databases", "comments": "Extended version of the paper <Checking Causal Consistency of\n  Distributed Databases>. It has been published in the Special issue of\n  NETYS2019 by Computing Journal\n  (https://link.springer.com/article/10.1007/s00607-021-00911-3). It is\n  extended with more than 30% of novel contribution,\n  CFP:https://www.springer.com/journal/607/updates/17646200 . Computing is\n  abstracted and indexed by SCOPUS and DBLP", "journal-ref": null, "doi": "10.1007/s00607-021-00911-3", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The CAP Theorem shows that (strong) Consistency, Availability, and Partition\ntolerance are impossible to be ensured together. Causal consistency is one of\nthe weak consistency models that can be implemented to ensure availability and\npartition tolerance in distributed systems. In this work, we propose a tool to\ncheck automatically the conformance of distributed/concurrent systems\nexecutions to causal consistency models. Our approach consists in reducing the\nproblem of checking if an execution is causally consistent to solving Datalog\nqueries. The reduction is based on complete characterizations of the executions\nviolating causal consistency in terms of the existence of cycles in suitably\ndefined relations between the operations occurring in these executions. We have\nimplemented the reduction in a testing tool for distributed databases, and\ncarried out several experiments on real case studies, showing the efficiency of\nthe suggested approach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 10:10:21 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 16:45:56 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 23:40:44 GMT"}, {"version": "v4", "created": "Fri, 29 Jan 2021 09:02:57 GMT"}, {"version": "v5", "created": "Tue, 16 Feb 2021 08:35:45 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zennou", "Rachid", ""], ["Biswas", "Ranadeep", ""], ["Bouajjani", "Ahmed", ""], ["Enea", "Constantin", ""], ["Erradi", "Mohammed", ""]]}, {"id": "2011.09836", "submitter": "Carsten Lutz", "authors": "Meghyn Bienvenu, Peter Hansen, Carsten Lutz, Frank Wolter", "title": "First Order-Rewritability and Containment of Conjunctive Queries in Horn\n  Description Logics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study FO-rewritability of conjunctive queries in the presence of\nontologies formulated in a description logic between EL and Horn-SHIF, along\nwith related query containment problems. Apart from providing\ncharacterizations, we establish complexity results ranging from ExpTime via\nNExpTime to 2ExpTime, pointing out several interesting effects. In particular,\nFO-rewriting is more complex for conjunctive queries than for atomic queries\nwhen inverse roles are present, but not otherwise.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:24:02 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Bienvenu", "Meghyn", ""], ["Hansen", "Peter", ""], ["Lutz", "Carsten", ""], ["Wolter", "Frank", ""]]}, {"id": "2011.10180", "submitter": "Chaochao Chen", "authors": "Chaochao Chen, Jamie Cui, Guanfeng Liu, Jia Wu, Li Wang", "title": "Survey and Open Problems in Privacy Preserving Knowledge Graph: Merging,\n  Query, Representation, Completion and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph (KG) has attracted more and more companies' attention for its\nability to connect different types of data in meaningful ways and support rich\ndata services. However, the data isolation problem limits the performance of KG\nand prevents its further development. That is, multiple parties have their own\nKGs but they cannot share with each other due to regulation or competition\nreasons. Therefore, how to conduct privacy preserving KG becomes an important\nresearch question to answer. That is, multiple parties conduct KG related tasks\ncollaboratively on the basis of protecting the privacy of multiple KGs. To\ndate, there is few work on solving the above KG isolation problem. In this\npaper, to fill this gap, we summarize the open problems for privacy preserving\nKG in data isolation setting and propose possible solutions for them.\nSpecifically, we summarize the open problems in privacy preserving KG from four\naspects, i.e., merging, query, representation, and completion. We present these\nproblems in details and propose possible technical solutions for them.\nMoreover, we present three privacy preserving KG-aware applications and simply\ndescribe how can our proposed techniques be applied into these applications.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 02:35:47 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Chen", "Chaochao", ""], ["Cui", "Jamie", ""], ["Liu", "Guanfeng", ""], ["Wu", "Jia", ""], ["Wang", "Li", ""]]}, {"id": "2011.10406", "submitter": "Alex Bogatu", "authors": "Alex Bogatu, Norman W. Paton, Mark Douthwaite, Stuart Davie, Andre\n  Freitas", "title": "Cost-effective Variational Active Entity Resolution", "comments": null, "journal-ref": "2021 IEEE 37th International Conference on Data Engineering (ICDE)", "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurately identifying different representations of the same real-world\nentity is an integral part of data cleaning and many methods have been proposed\nto accomplish it. The challenges of this entity resolution task that demand so\nmuch research attention are often rooted in the task-specificity and\nuser-dependence of the process. Adopting deep learning techniques has the\npotential to lessen these challenges. In this paper, we set out to devise an\nentity resolution method that builds on the robustness conferred by deep\nautoencoders to reduce human-involvement costs. Specifically, we reduce the\ncost of training deep entity resolution models by performing unsupervised\nrepresentation learning. This unveils a transferability property of the\nresulting model that can further reduce the cost of applying the approach to\nnew datasets by means of transfer learning. Finally, we reduce the cost of\nlabelling training data through an active learning approach that builds on the\nproperties conferred by the use of deep autoencoders. Empirical evaluation\nconfirms the accomplishment of our cost-reduction desideratum while achieving\ncomparable effectiveness with state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 13:47:11 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 19:29:35 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 11:13:27 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bogatu", "Alex", ""], ["Paton", "Norman W.", ""], ["Douthwaite", "Mark", ""], ["Davie", "Stuart", ""], ["Freitas", "Andre", ""]]}, {"id": "2011.10427", "submitter": "Alex Bogatu", "authors": "Alex Bogatu, Alvaro A.A. Fernandes, Norman W. Paton, Nikolaos\n  Konstantinou", "title": "Dataset Discovery in Data Lakes", "comments": null, "journal-ref": "2020 IEEE 36th International Conference on Data Engineering (ICDE)", "doi": "10.1109/ICDE48307.2020.00067", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data analytics stands to benefit from the increasing availability of datasets\nthat are held without their conceptual relationships being explicitly known.\nWhen collected, these datasets form a data lake from which, by processes like\ndata wrangling, specific target datasets can be constructed that enable\nvalue-adding analytics. Given the potential vastness of such data lakes, the\nissue arises of how to pull out of the lake those datasets that might\ncontribute to wrangling out a given target. We refer to this as the problem of\ndataset discovery in data lakes and this paper contributes an effective and\nefficient solution to it. Our approach uses features of the values in a dataset\nto construct hash-based indexes that map those features into a uniform distance\nspace. This makes it possible to define similarity distances between features\nand to take those distances as measurements of relatedness w.r.t. a target\ntable. Given the latter (and exemplar tuples), our approach returns the most\nrelated tables in the lake. We provide a detailed description of the approach\nand report on empirical results for two forms of relatedness (unionability and\njoinability) comparing them with prior work, where pertinent, and showing\nsignificant improvements in all of precision, recall, target coverage, indexing\nand discovery times.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:48:09 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Bogatu", "Alex", ""], ["Fernandes", "Alvaro A. A.", ""], ["Paton", "Norman W.", ""], ["Konstantinou", "Nikolaos", ""]]}, {"id": "2011.11176", "submitter": "Yu Sun", "authors": "Yu Sun, Jian Zhang", "title": "Distance-based Data Cleaning: A Survey (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid development of the internet technology, dirty data are\ncommonly observed in various real scenarios, e.g., owing to unreliable sensor\nreading, transmission and collection from heterogeneous sources. To deal with\ntheir negative effects on downstream applications, data cleaning approaches are\ndesigned to preprocess the dirty data before conducting applications. The idea\nof most data cleaning methods is to identify or correct dirty data, referring\nto the values of their neighbors which share the same information.\nUnfortunately, owing to data sparsity and heterogeneity, the number of\nneighbors based on equality relationship is rather limited, especially in the\npresence of data values with variances. To tackle this problem, distance-based\ndata cleaning approaches propose to consider similarity neighbors based on\nvalue distance. By tolerance of small variants, the enriched similarity\nneighbors can be identified and used for data cleaning tasks. At the same time,\ndistance relationship between tuples is also helpful to guide the data\ncleaning, which contains more information and includes the equality\nrelationship. Therefore, distance-based technology plays an important role in\nthe data cleaning area, and we also have reason to believe that distance-based\ndata cleaning technology will attract more attention in data preprocessing\nresearch in the future. Hence this survey provides a classification of four\nmain data cleaning tasks, i.e., rule profiling, error detection, data repair\nand data imputation, and comprehensively reviews the state of the art for each\nclass.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:26:25 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sun", "Yu", ""], ["Zhang", "Jian", ""]]}, {"id": "2011.11442", "submitter": "Quoc Hung Ngo", "authors": "Quoc Hung Ngo, Tahar Kechadi, and Nhien-An Le-Khac", "title": "OAK: Ontology-Based Knowledge Map Model for Digital Agriculture", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-63924-2_14", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, a huge amount of knowledge has been amassed in digital agriculture.\nThis knowledge and know-how information are collected from various sources,\nhence the question is how to organise this knowledge so that it can be\nefficiently exploited. Although this knowledge about agriculture practices can\nbe represented using ontology, rule-based expert systems, or knowledge model\nbuilt from data mining processes, the scalability still remains an open issue.\nIn this study, we propose a knowledge representation model, called an\nontology-based knowledge map, which can collect knowledge from different\nsources, store it, and exploit either directly by stakeholders or as an input\nto the knowledge discovery process (Data Mining). The proposed model consists\nof two stages, 1) build an ontology as a knowledge base for a specific domain\nand data mining concepts, and 2) build the ontology-based knowledge map model\nfor representing and storing the knowledge mined on the crop datasets. A\nframework of the proposed model has been implemented in agriculture domain. It\nis an efficient and scalable model, and it can be used as knowledge repository\na digital agriculture.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:16:12 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ngo", "Quoc Hung", ""], ["Kechadi", "Tahar", ""], ["Le-Khac", "Nhien-An", ""]]}, {"id": "2011.11487", "submitter": "Masoud Nosrati", "authors": "Masoud Nosrati and Ying Cai", "title": "Verifying the Correctness of Analytic Query Results", "comments": "IEEE Transactions on Knowledge and Data Engineering (12 Pages)", "journal-ref": null, "doi": "10.1109/TKDE.2020.3037313", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data outsourcing is a cost-effective solution for data owners to tackle\nissues such as large volumes of data, huge number of users, and intensive\ncomputation needed for data analysis. They can simply upload their databases to\na cloud and let it perform all management works, including query processing.\nOne problem with this service model is how query issuers can verify the query\nresults they receive are indeed correct. This concern is legitimate because, as\na third party, clouds may not be fully trustworthy, and as a large data center,\nclouds are ideal targets for hackers. There has been significant work on query\nresult verification, but most consider only simple queries where query results\ncan be attained by checking the raw data against the query conditions directly.\nIn this paper, we consider the problem of enabling users to verify the\ncorrectness of the results of analytic queries. Unlike simple queries, analytic\nqueries involve ranking functions to score a database, which makes it difficult\nto build data structures for verification purposes. We propose two approaches,\nnamely one-signature and multi-signature, and show that they work well on three\nrepresentative types of analytic queries, including top-k, range, and KNN\nqueries, through both analysis and experiments.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:09:12 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Nosrati", "Masoud", ""], ["Cai", "Ying", ""]]}, {"id": "2011.11545", "submitter": "Xuhong Wang", "authors": "Xuhong Wang, Ding Lyu, Mengjian Li, Yang Xia, Qi Yang, Xinwen Wang,\n  Xinguang Wang, Ping Cui, Yupu Yang, Bowen Sun, Zhenyu Guo", "title": "APAN: Asynchronous Propagation Attention Network for Real-time Temporal\n  Graph Embedding", "comments": "In Proceedings of the 2021 International Conference on Management of\n  Data (SIGMOD/PODS '21)", "journal-ref": null, "doi": "10.1145/3448016.3457564", "report-no": null, "categories": "cs.AI cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Limited by the time complexity of querying k-hop neighbors in a graph\ndatabase, most graph algorithms cannot be deployed online and execute\nmillisecond-level inference. This problem dramatically limits the potential of\napplying graph algorithms in certain areas, such as financial fraud detection.\nTherefore, we propose Asynchronous Propagation Attention Network, an\nasynchronous continuous time dynamic graph algorithm for real-time temporal\ngraph embedding. Traditional graph models usually execute two serial\noperations: first graph computation and then model inference. We decouple model\ninference and graph computation step so that the heavy graph query operations\nwill not damage the speed of model inference. Extensive experiments demonstrate\nthat the proposed method can achieve competitive performance and 8.7 times\ninference speed improvement in the meantime.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:58:50 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 03:12:47 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 02:35:09 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 05:42:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Xuhong", ""], ["Lyu", "Ding", ""], ["Li", "Mengjian", ""], ["Xia", "Yang", ""], ["Yang", "Qi", ""], ["Wang", "Xinwen", ""], ["Wang", "Xinguang", ""], ["Cui", "Ping", ""], ["Yang", "Yupu", ""], ["Sun", "Bowen", ""], ["Guo", "Zhenyu", ""]]}, {"id": "2011.11682", "submitter": "Xiaohui Yu", "authors": "Zhaoyue Chen, Nick Koudas, Zhe Zhang, Xiaohui Yu", "title": "Efficient Construction of Nonlinear Models over Normalized Data", "comments": "Accepted at IEEE International Conference on Data Engineering (ICDE\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) applications are proliferating in the enterprise.\nRelational data which are prevalent in enterprise applications are typically\nnormalized; as a result, data has to be denormalized via primary/foreign-key\njoins to be provided as input to ML algorithms. In this paper, we study the\nimplementation of popular nonlinear ML models, Gaussian Mixture Models (GMM)\nand Neural Networks (NN), over normalized data addressing both cases of binary\nand multi-way joins over normalized relations.\n  For the case of GMM, we show how it is possible to decompose computation in a\nsystematic way both for binary joins and for multi-way joins to construct\nmixture models. We demonstrate that by factoring the computation, one can\nconduct the training of the models much faster compared to other applicable\napproaches, without any loss in accuracy.\n  For the case of NN, we propose algorithms to train the network taking\nnormalized data as the input. Similarly, we present algorithms that can conduct\nthe training of the network in a factorized way and offer performance\nadvantages. The redundancy introduced by denormalization can be exploited for\ncertain types of activation functions. However, we demonstrate that attempting\nto explore this redundancy is helpful up to a certain point; exploring\nredundancy at higher layers of the network will always result in increased\ncosts and is not recommended.\n  We present the results of a thorough experimental evaluation, varying several\nparameters of the input relations involved and demonstrate that our proposals\nfor the training of GMM and NN yield drastic performance improvements typically\nstarting at 100%, which become increasingly higher as parameters of the\nunderlying data vary, without any loss in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 19:20:03 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 14:55:06 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Chen", "Zhaoyue", ""], ["Koudas", "Nick", ""], ["Zhang", "Zhe", ""], ["Yu", "Xiaohui", ""]]}, {"id": "2011.11907", "submitter": "Huan Hu", "authors": "Huan Hu and Jianzhong Li", "title": "Efficient Approximate Nearest Neighbor Search for Multiple Weighted\n  $l_{p\\leq2}$ Distance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor search is fundamental to a wide range of applications. Since\nthe exact nearest neighbor search suffers from the \"curse of dimensionality\",\napproximate approaches, such as Locality-Sensitive Hashing (LSH), are widely\nused to trade a little query accuracy for a much higher query efficiency. In\nmany scenarios, it is necessary to perform nearest neighbor search under\nmultiple weighted distance functions in high-dimensional spaces. This paper\nconsiders the important problem of supporting efficient approximate nearest\nneighbor search for multiple weighted distance functions in high-dimensional\nspaces. To the best of our knowledge, prior work can only solve the problem for\nthe $l_2$ distance. However, numerous studies have shown that the $l_p$\ndistance with $p\\in(0,2)$ could be more effective than the $l_2$ distance in\nhigh-dimensional spaces. We propose a novel method, WLSH, to address the\nproblem for the $l_p$ distance for $p\\in(0,2]$. WLSH takes the LSH approach and\ncan theoretically guarantee both the efficiency of processing queries and the\naccuracy of query results while minimizing the required total number of hash\ntables. We conduct extensive experiments on synthetic and real data sets, and\nthe results show that WLSH achieves high performance in terms of query\nefficiency, query accuracy and space consumption.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 05:56:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hu", "Huan", ""], ["Li", "Jianzhong", ""]]}, {"id": "2011.12052", "submitter": "Projjal Gupta", "authors": "Projjal Gupta", "title": "A decentralized approach towards secure firmware updates and testing\n  over commercial IoT Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet technologies have made a paradigm shift in the fields of computing\nand data science and one such paradigm defining change is the Internet of\nThings or IoT. Nowadays, thousands of household appliances use integrated smart\ndevices which allow remote monitoring and control and also allow intensive\ncomputational work such as high end AI-integrated smart security systems with\nsustained alerts for the user. The update process of these IoT devices usually\nlack the ability of checking the security of centralized servers, which may be\ncompromised and host malicious firmware files as it is presumed that the\nservers are secure during deployment. The solution for this problem can be\nsolved using a decentralized database to hold the hashes and the firmware. This\npaper discusses the possible implications of insecure servers used to host the\nfirmwares of commercial IoT products, and aims to provide a blockchain based\ndecentralized solution to host firmware files with the property of\nimmutability, and controlled access to the firmware upload functions so as to\nstop unauthorized use. The paper sheds light over possible hardware\nimplementations and the use of cryptographically secure components in such\nsecure architecture models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:59:06 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Gupta", "Projjal", ""]]}, {"id": "2011.12445", "submitter": "Jing Yang", "authors": "Jing Yang, Chun Ouyang, Wil M.P. van der Aalst, Arthur H.M. ter\n  Hofstede, Yang Yu", "title": "OrgMining 2.0: A Novel Framework for Organizational Model Mining from\n  Event Logs", "comments": "Manuscript initially submitted for review on 13/5/2020 with 38 pages,\n  10 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing appropriate structures around human resources can streamline\noperations and thus facilitate the competitiveness of an organization. To\nachieve this goal, modern organizations need to acquire an accurate and timely\nunderstanding of human resource grouping while faced with an ever-changing\nenvironment. The use of process mining offers a promising way to help address\nthe need through utilizing event log data stored in information systems. By\nextracting knowledge about the actual behavior of resources participating in\nbusiness processes from event logs, organizational models can be constructed,\nwhich facilitate the analysis of the de facto grouping of human resources\nrelevant to process execution. Nevertheless, open research gaps remain to be\naddressed when applying the state-of-the-art process mining to analyze resource\ngrouping. For one, the discovery of organizational models has only limited\nconnections with the context of process execution. For another, a rigorous\nsolution that evaluates organizational models against event log data is yet to\nbe proposed. In this paper, we aim to tackle these research challenges by\ndeveloping a novel framework built upon a richer definition of organizational\nmodels coupling resource grouping with process execution knowledge. By\nintroducing notions of conformance checking for organizational models, the\nframework allows effective evaluation of organizational models, and therefore\nprovides a foundation for analyzing and improving resource grouping based on\nevent logs. We demonstrate the feasibility of this framework by proposing an\napproach underpinned by the framework for organizational model discovery, and\nalso conduct experiments on real-life event logs to discover and evaluate\norganizational models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 23:46:47 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 02:15:39 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Yang", "Jing", ""], ["Ouyang", "Chun", ""], ["van der Aalst", "Wil M. P.", ""], ["ter Hofstede", "Arthur H. M.", ""], ["Yu", "Yang", ""]]}, {"id": "2011.12707", "submitter": "Girmaw Abebe Tadesse", "authors": "Girmaw Abebe Tadesse, Celia Cintas, Skyler Speakman, Komminist\n  Weldemariam", "title": "Prediction of neonatal mortality in Sub-Saharan African countries using\n  data-level linkage of multiple surveys", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing datasets available to address crucial problems, such as child\nmortality and family planning discontinuation in developing countries, are not\nample for data-driven approaches. This is partly due to disjoint data\ncollection efforts employed across locations, times, and variations of\nmodalities. On the other hand, state-of-the-art methods for small data problem\nare confined to image modalities. In this work, we proposed a data-level\nlinkage of disjoint surveys across Sub-Saharan African countries to improve\nprediction performance of neonatal death and provide cross-domain\nexplainability.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 13:18:28 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Tadesse", "Girmaw Abebe", ""], ["Cintas", "Celia", ""], ["Speakman", "Skyler", ""], ["Weldemariam", "Komminist", ""]]}, {"id": "2011.13454", "submitter": "Wensheng Gan", "authors": "Chunkai Zhang, Zilin Du, Wensheng Gan and Philip S. Yu", "title": "TKUS: Mining Top-K High-Utility Sequential Patterns", "comments": "Preprint, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-utility sequential pattern mining (HUSPM) has recently emerged as a\nfocus of intense research interest. The main task of HUSPM is to find all\nsubsequences, within a quantitative sequential database, that have high utility\nwith respect to a user-defined minimum utility threshold. However, it is\ndifficult to specify the minimum utility threshold, especially when database\nfeatures, which are invisible in most cases, are not understood. To handle this\nproblem, top-k HUSPM was proposed. Up to now, only very preliminary work has\nbeen conducted to capture top-k HUSPs, and existing strategies require\nimprovement in terms of running time, memory consumption, unpromising candidate\nfiltering, and scalability. Moreover, no systematic problem statement has been\ndefined. In this paper, we formulate the problem of top-k HUSPM and propose a\nnovel algorithm called TKUS. To improve efficiency, TKUS adopts a projection\nand local search mechanism and employs several schemes, including the Sequence\nUtility Raising, Terminate Descendants Early, and Eliminate Unpromising Items\nstrategies, which allow it to greatly reduce the search space. Finally,\nexperimental results demonstrate that TKUS can achieve sufficiently good top-k\nHUSPM performance compared to state-of-the-art algorithm TKHUS-Span.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 19:36:20 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zhang", "Chunkai", ""], ["Du", "Zilin", ""], ["Gan", "Wensheng", ""], ["Yu", "Philip S.", ""]]}, {"id": "2011.13455", "submitter": "Wensheng Gan", "authors": "Chunkai Zhang, Zilin Du, Yuting Yang, Wensheng Gan and Philip S. Yu", "title": "On-shelf Utility Mining of Sequence Data", "comments": "Preprint under review, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utility mining has emerged as an important and interesting topic owing to its\nwide application and considerable popularity. However, conventional utility\nmining methods have a bias toward items that have longer on-shelf time as they\nhave a greater chance to generate a high utility. To eliminate the bias, the\nproblem of on-shelf utility mining (OSUM) is introduced. In this paper, we\nfocus on the task of OSUM of sequence data, where the sequential database is\ndivided into several partitions according to time periods and items are\nassociated with utilities and several on-shelf time periods. To address the\nproblem, we propose two methods, OSUM of sequence data (OSUMS) and OSUMS+, to\nextract on-shelf high-utility sequential patterns. For further efficiency, we\nalso designed several strategies to reduce the search space and avoid redundant\ncalculation with two upper bounds time prefix extension utility (TPEU) and time\nreduced sequence utility (TRSU). In addition, two novel data structures were\ndeveloped for facilitating the calculation of upper bounds and utilities.\nSubstantial experimental results on certain real and synthetic datasets show\nthat the two methods outperform the state-of-the-art algorithm. In conclusion,\nOSUMS may consume a large amount of memory and is unsuitable for cases with\nlimited memory, while OSUMS+ has wider real-life applications owing to its high\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 19:38:23 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zhang", "Chunkai", ""], ["Du", "Zilin", ""], ["Yang", "Yuting", ""], ["Gan", "Wensheng", ""], ["Yu", "Philip S.", ""]]}, {"id": "2011.13583", "submitter": "Apoorv Khandelwal", "authors": "Margot Hanley, Apoorv Khandelwal, Hadar Averbuch-Elor, Noah Snavely\n  and Helen Nissenbaum", "title": "An Ethical Highlighter for People-Centric Dataset Creation", "comments": "Part of the Navigating the Broader Impacts of AI Research Workshop at\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important ethical concerns arising from computer vision datasets of people\nhave been receiving significant attention, and a number of datasets have been\nwithdrawn as a result. To meet the academic need for people-centric datasets,\nwe propose an analytical framework to guide ethical evaluation of existing\ndatasets and to serve future dataset creators in avoiding missteps. Our work is\ninformed by a review and analysis of prior works and highlights where such\nethical challenges arise.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 07:18:44 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hanley", "Margot", ""], ["Khandelwal", "Apoorv", ""], ["Averbuch-Elor", "Hadar", ""], ["Snavely", "Noah", ""], ["Nissenbaum", "Helen", ""]]}, {"id": "2011.14024", "submitter": "Chun Ouyang", "authors": "Bemali Wickramanayake, Dakshi Kapugama Geeganage, Chun Ouyang, Yue Xu", "title": "A Survey of Online Card Payment Fraud Detection using Data Mining-based\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Card payment fraud is a serious problem, and a roadblock for an optimally\nfunctioning digital economy, with cards (Debits and Credit) being the most\npopular digital payment method across the globe. Despite the occurrence of\nfraud could be relatively rare, the impact of fraud could be significant,\nespecially on the cardholder. In the research, there have been many attempts to\ndevelop methods of detecting potentially fraudulent transactions based on data\nmining techniques, predominantly exploiting the developments in the space of\nmachine learning over the last decade. This survey proposes a taxonomy based on\na review of existing research attempts and experiments, which mainly elaborates\nthe approaches taken by researchers to incorporate the (i) business impact of\nfraud (and fraud detection) into their work , (ii) the feature engineering\ntechniques that focus on cardholder behavioural profiling to separate\nfraudulent activities happening with the same card, and (iii) the adaptive\nefforts taken to address the changing nature of fraud. Further, there will be a\ncomparative performance evaluation of classification algorithms used and\nefforts of addressing class imbalance problem. Forty-five peer-reviewed papers\npublished in the domain of card fraud detection between 2009 and 2020 were\nintensively reviewed to develop this paper.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 23:07:38 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wickramanayake", "Bemali", ""], ["Geeganage", "Dakshi Kapugama", ""], ["Ouyang", "Chun", ""], ["Xu", "Yue", ""]]}, {"id": "2011.14195", "submitter": "Abdelkader Mokkadem", "authors": "Abdelkader Mokkadem, Mariane Pelletier, Louis Raimbault", "title": "Recursive Association Rule Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mining frequent itemsets and association rules is an essential task within\ndata mining and data analysis. In this paper, we introduce PrefRec, a recursive\nalgorithm for finding frequent itemsets and association rules. Its main\nadvantage is its recursiveness with respect to the items. It is particularly\nefficient for updating the mining process when new items are added to the\ndatabase or when some are excluded. We present in a complete way the logic of\nthe algorithm as well as its various applications. Finally we present\nexperiments carried out in the R language comparing PrefRec with Apriori and\nEclat the two most powerful algorithms in this language. To achieve this we\nbuilt an R package to run our algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:53:03 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 16:54:45 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Mokkadem", "Abdelkader", ""], ["Pelletier", "Mariane", ""], ["Raimbault", "Louis", ""]]}, {"id": "2011.14482", "submitter": "Yufei Tao", "authors": "Bas Ketsman, Dan Suciu, and Yufei Tao", "title": "A Near-Optimal Parallel Algorithm for Joining Binary Relations", "comments": "Short versions of this article appeared in PODS'17 and ICDT'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a constant-round algorithm in the massively parallel computation\n(MPC) model for evaluating a natural join where every input relation has two\nattributes. Our algorithm achieves a load of $\\tilde{O}(m/p^{1/\\rho})$ where\n$m$ is the total size of the input relations, $p$ is the number of machines,\n$\\rho$ is the join's fractional edge covering number, and $\\tilde{O}(.)$ hides\na polylogarithmic factor. The load matches a known lower bound up to a\npolylogarithmic factor. At the core of the proposed algorithm is a new theorem\n(which we name {\\em the isolated cartesian product theorem}) that provides\nfresh insight into the problem's mathematical structure. Our result implies\nthat the {\\em subgraph enumeration problem}, where the goal is to report all\nthe occurrences of a constant-sized subgraph pattern, can be settled optimally\n(up to a polylogarithmic factor) in the MPC model.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 00:48:35 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ketsman", "Bas", ""], ["Suciu", "Dan", ""], ["Tao", "Yufei", ""]]}, {"id": "2011.14843", "submitter": "Tatiana Makhalova", "authors": "Tatiana Makhalova, Sergei O. Kuznetsov, Amedeo Napoli", "title": "Mint: MDL-based approach for Mining INTeresting Numerical Pattern Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern mining is well established in data mining research, especially for\nmining binary datasets. Surprisingly, there is much less work about numerical\npattern mining and this research area remains under-explored. In this paper, we\npropose Mint, an efficient MDL-based algorithm for mining numerical datasets.\nThe MDL principle is a robust and reliable framework widely used in pattern\nmining, and as well in subgroup discovery. In Mint we reuse MDL for discovering\nuseful patterns and returning a set of non-redundant overlapping patterns with\nwell-defined boundaries and covering meaningful groups of objects. Mint is not\nalone in the category of numerical pattern miners based on MDL. In the\nexperiments presented in the paper we show that Mint outperforms competitors\namong which Slim and RealKrimp.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:36:29 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Makhalova", "Tatiana", ""], ["Kuznetsov", "Sergei O.", ""], ["Napoli", "Amedeo", ""]]}, {"id": "2011.14860", "submitter": "Peter Lindner", "authors": "Martin Grohe and Peter Lindner", "title": "Infinite Probabilistic Databases", "comments": "This is the full version of the paper \"Infinite Probabilistic\n  Databases\" presented at ICDT 2020 (arXiv:1904.06766)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic databases (PDBs) model uncertainty in data in a quantitative\nway. In the established formal framework, probabilistic (relational) databases\nare finite probability spaces over relational database instances. This\nfiniteness can clash with intuitive query behavior (Ceylan et al., KR 2016),\nand with application scenarios that are better modeled by continuous\nprobability distributions (Dalvi et al., CACM 2009).\n  We formally introduced infinite PDBs in (Grohe and Lindner, PODS 2019) with a\nprimary focus on countably infinite spaces. However, an extension beyond\ncountable probability spaces raises nontrivial foundational issues concerned\nwith the measurability of events and queries and ultimately with the question\nwhether queries have a well-defined semantics.\n  We argue that finite point processes are an appropriate model from\nprobability theory for dealing with general probabilistic databases. This\nallows us to construct suitable (uncountable) probability spaces of database\ninstances in a systematic way. Our main technical results are measurability\nstatements for relational algebra queries as well as aggregate queries and\nDatalog queries.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:57:38 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 09:05:38 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 17:49:19 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Grohe", "Martin", ""], ["Lindner", "Peter", ""]]}, {"id": "2011.14906", "submitter": "Julio Cezar Silveira Jacques Junior", "authors": "Julio C. S. Jacques Junior, Agata Lapedriza, Cristina Palmero, Xavier\n  Bar\\'o and Sergio Escalera", "title": "Person Perception Biases Exposed: Revisiting the First Impressions\n  Dataset", "comments": "accepted on 11th International Workshop on Human Behavior\n  Understanding (HBU), organized as part of WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work revisits the ChaLearn First Impressions database, annotated for\npersonality perception using pairwise comparisons via crowdsourcing. We analyse\nfor the first time the original pairwise annotations, and reveal existing\nperson perception biases associated to perceived attributes like gender,\nethnicity, age and face attractiveness. We show how person perception bias can\ninfluence data labelling of a subjective task, which has received little\nattention from the computer vision and machine learning communities by now. We\nfurther show that the mechanism used to convert pairwise annotations to\ncontinuous values may magnify the biases if no special treatment is considered.\nThe findings of this study are relevant for the computer vision community that\nis still creating new datasets on subjective tasks, and using them for\npractical applications, ignoring these perceptual biases.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:41:27 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Junior", "Julio C. S. Jacques", ""], ["Lapedriza", "Agata", ""], ["Palmero", "Cristina", ""], ["Bar\u00f3", "Xavier", ""], ["Escalera", "Sergio", ""]]}, {"id": "2011.15028", "submitter": "G\\'abor Sz\\'arnyas", "authors": "Alexandru Iosup, Ahmed Musaafir, Alexandru Uta, Arnau Prat P\\'erez,\n  G\\'abor Sz\\'arnyas, Hassan Chafi, Ilie Gabriel T\\u{a}nase, Lifeng Nai,\n  Michael Anderson, Mihai Capot\\u{a}, Narayanan Sundaram, Peter Boncz,\n  Siegfried Depner, Stijn Heldens, Thomas Manhardt, Tim Hegeman, Wing Lung\n  Ngai, Yinglong Xia", "title": "The LDBC Graphalytics Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document, we describe LDBC Graphalytics, an industrial-grade\nbenchmark for graph analysis platforms. The main goal of Graphalytics is to\nenable the fair and objective comparison of graph analysis platforms. Due to\nthe diversity of bottlenecks and performance issues such platforms need to\naddress, Graphalytics consists of a set of selected deterministic algorithms\nfor full-graph analysis, standard graph datasets, synthetic dataset generators,\nand reference output for validation purposes. Its test harness produces deep\nmetrics that quantify multiple kinds of systems scalability, weak and strong,\nand robustness, such as failures and performance variability. The benchmark\nalso balances comprehensiveness with runtime necessary to obtain the deep\nmetrics. The benchmark comes with open-source software for generating\nperformance data, for validating algorithm results, for monitoring and sharing\nperformance data, and for obtaining the final benchmark result as a standard\nperformance report.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:34:37 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 13:59:29 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 18:37:57 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Iosup", "Alexandru", ""], ["Musaafir", "Ahmed", ""], ["Uta", "Alexandru", ""], ["P\u00e9rez", "Arnau Prat", ""], ["Sz\u00e1rnyas", "G\u00e1bor", ""], ["Chafi", "Hassan", ""], ["T\u0103nase", "Ilie Gabriel", ""], ["Nai", "Lifeng", ""], ["Anderson", "Michael", ""], ["Capot\u0103", "Mihai", ""], ["Sundaram", "Narayanan", ""], ["Boncz", "Peter", ""], ["Depner", "Siegfried", ""], ["Heldens", "Stijn", ""], ["Manhardt", "Thomas", ""], ["Hegeman", "Tim", ""], ["Ngai", "Wing Lung", ""], ["Xia", "Yinglong", ""]]}]