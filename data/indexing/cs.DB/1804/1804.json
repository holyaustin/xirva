[{"id": "1804.00224", "submitter": "Ashish Tapdiya", "authors": "Ashish Tapdiya and Daniel Fabbri", "title": "A comparative analysis of state-of-the-art SQL-on-Hadoop systems for\n  interactive analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hadoop is emerging as the primary data hub in enterprises, and SQL represents\nthe de facto language for data analysis. This combination has led to the\ndevelopment of a variety of SQL-on-Hadoop systems in use today. While the\nvarious SQL-on-Hadoop systems target the same class of analytical workloads,\ntheir different architectures, design decisions and implementations impact\nquery performance. In this work, we perform a comparative analysis of four\nstate-of-the-art SQL-on-Hadoop systems (Impala, Drill, Spark SQL and Phoenix)\nusing the Web Data Analytics micro benchmark and the TPC-H benchmark on the\nAmazon EC2 cloud platform. The TPC-H experiment results show that, although\nImpala outperforms other systems (4.41x - 6.65x) in the text format, trade-offs\nexists in the parquet format, with each system performing best on subsets of\nqueries. A comprehensive analysis of execution profiles expands upon the\nperformance results to provide insights into performance variations,\nperformance bottlenecks and query execution characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 23:16:01 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Tapdiya", "Ashish", ""], ["Fabbri", "Daniel", ""]]}, {"id": "1804.00370", "submitter": "Yu-Hsuan Kuo", "authors": "Yu-Hsuan Kuo, Cho-Chun Chiu, Daniel Kifer, Michael Hay, Ashwin\n  Machanavajjhala", "title": "Differentially Private Hierarchical Count-of-Counts Histograms", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of privately releasing a class of queries that we\ncall hierarchical count-of-counts histograms. Count-of-counts histograms\npartition the rows of an input table into groups (e.g., group of people in the\nsame household), and for every integer j report the number of groups of size j.\nHierarchical count-of-counts queries report count-of-counts histograms at\ndifferent granularities as per hierarchy defined on an attribute in the input\ndata (e.g., geographical location of a household at the national, state and\ncounty levels). In this paper, we introduce this problem, along with\nappropriate error metrics and propose a differentially private solution that\ngenerates count-of-counts histograms that are consistent across all levels of\nthe hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 01:51:10 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 20:44:30 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Kuo", "Yu-Hsuan", ""], ["Chiu", "Cho-Chun", ""], ["Kifer", "Daniel", ""], ["Hay", "Michael", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "1804.00399", "submitter": "Hung Dang", "authors": "Hung Dang, Tien Tuan Anh Dinh, Dumitrel Loghin, Ee-Chien Chang, Qian\n  Lin, Beng Chin Ooi", "title": "Towards Scaling Blockchain Systems via Sharding", "comments": "This is an updated version of the Chain of Trust: Can Trusted\n  Hardware Help Scaling Blockchains? paper. This version is to be appeared in\n  SIGMOD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing blockchain systems scale poorly because of their distributed\nconsensus protocols. Current attempts at improving blockchain scalability are\nlimited to cryptocurrency. Scaling blockchain systems under general workloads\n(i.e., non-cryptocurrency applications) remains an open question. In this work,\nwe take a principled approach to apply sharding, which is a well-studied and\nproven technique to scale out databases, to blockchain systems in order to\nimprove their transaction throughput at scale. This is challenging, however,\ndue to the fundamental difference in failure models between databases and\nblockchain. To achieve our goal, we first enhance the performance of Byzantine\nconsensus protocols, by doing so we improve individual shards' throughput.\nNext, we design an efficient shard formation protocol that leverages a trusted\nrandom beacon to securely assign nodes into shards. We rely on trusted\nhardware, namely Intel SGX, to achieve high performance for both consensus and\nshard formation protocol. Third, we design a general distributed transaction\nprotocol that ensures safety and liveness even when transaction coordinators\nare malicious. Finally, we conduct an extensive evaluation of our design both\non a local cluster and on Google Cloud Platform. The results show that our\nconsensus and shard formation protocols outperform state-of-the-art solutions\nat scale. More importantly, our sharded blockchain reaches a high throughput\nthat can handle Visa-level workloads, and is the largest ever reported in a\nrealistic environment.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 05:33:18 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 03:47:45 GMT"}, {"version": "v3", "created": "Sat, 11 Aug 2018 02:44:04 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2019 05:03:53 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Dang", "Hung", ""], ["Dinh", "Tien Tuan Anh", ""], ["Loghin", "Dumitrel", ""], ["Chang", "Ee-Chien", ""], ["Lin", "Qian", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1804.00401", "submitter": "Carsten Binnig", "authors": "Prasetya Utama, Nathaniel Weir, Fuat Basik, Carsten Binnig, Ugur\n  Cetintemel, Benjamin H\\\"attasch, Amir Ilkhechi, Shekar Ramaswamy, Arif Usta", "title": "An End-to-end Neural Natural Language Interface for Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to extract insights from new data sets is critical for decision\nmaking. Visual interactive tools play an important role in data exploration\nsince they provide non-technical users with an effective way to visually\ncompose queries and comprehend the results. Natural language has recently\ngained traction as an alternative query interface to databases with the\npotential to enable non-expert users to formulate complex questions and\ninformation needs efficiently and effectively. However, understanding natural\nlanguage questions and translating them accurately to SQL is a challenging\ntask, and thus Natural Language Interfaces for Databases (NLIDBs) have not yet\nmade their way into practical tools and commercial products.\n  In this paper, we present DBPal, a novel data exploration tool with a natural\nlanguage interface. DBPal leverages recent advances in deep models to make\nquery understanding more robust in the following ways: First, DBPal uses a deep\nmodel to translate natural language statements to SQL, making the translation\nprocess more robust to paraphrasing and other linguistic variations. Second, to\nsupport the users in phrasing questions without knowing the database schema and\nthe query features, DBPal provides a learned auto-completion model that\nsuggests partial query extensions to users during query formulation and thus\nhelps to write complex queries.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 05:36:38 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Utama", "Prasetya", ""], ["Weir", "Nathaniel", ""], ["Basik", "Fuat", ""], ["Binnig", "Carsten", ""], ["Cetintemel", "Ugur", ""], ["H\u00e4ttasch", "Benjamin", ""], ["Ilkhechi", "Amir", ""], ["Ramaswamy", "Shekar", ""], ["Usta", "Arif", ""]]}, {"id": "1804.00443", "submitter": "Egor V. Kostylev", "authors": "Egor V. Kostylev and Dan Suciu", "title": "A Note on the Hardness of the Critical Tuple Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of critical tuple was introduced by Miklau and Suciu (Gerome\nMiklau and Dan Suciu. A formal analysis of information disclosure in data\nexchange. J. Comput. Syst. Sci., 73(3):507-534, 2007), who also claimed that\nthe problem of checking whether a tuple is non-critical is complete for the\nsecond level of the polynomial hierarchy. Kostylev identified an error in the\n12-page-long hardness proof. It turns out that the issue is rather fundamental:\nthe proof can be adapted to show hardness of a relative variant of\ntuple-non-criticality, but we have neither been able to prove the original\nclaim nor found an algorithm for it of lower complexity. In this note we state\nformally the relative variant and present an alternative, simplified proof of\nits hardness; we also give an NP-hardness proof for the original problem, the\nbest lower bound we have been able to show. Hence, the precise complexity of\nthe original critical tuple problem remains open.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 10:22:51 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kostylev", "Egor V.", ""], ["Suciu", "Dan", ""]]}, {"id": "1804.00465", "submitter": "Zheng Xi", "authors": "Xi Zheng", "title": "Database as a Service - Current Issues and Its Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of applications in cloud, Database as a Service (DBaaS)\nbecomes a promising method to provide cloud applications with reliable and\nflexible data storage services. It provides a number of interesting features to\ncloud developers, however, it suffers a few drawbacks: long learning curve and\ndevelopment cycle, lacking of in-depth support for NoSQL, lacking of flexible\nconfiguration for security and privacy, and high cost models. In this paper, we\ninvestigate these issues among current DBaaS providers and propose a novel\nTrinity Model that can significantly reduce the learning curves, improve the\nsecurity and privacy, and accelerate database design and development. We\nfurther elaborate our ongoing and future work on developing large real-world\nSaaS projects using this new DBaaS model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 12:08:01 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Zheng", "Xi", ""]]}, {"id": "1804.00599", "submitter": "Shadman Saqib Eusuf", "authors": "Mohammed Eunus Ali, Kaysar Abdullah, Shadman Saqib Eusuf, Farhana M.\n  Choudhury, J. Shane Culpepper, Timos Sellis", "title": "The Maximum Trajectory Coverage Query in Spatial Databases", "comments": null, "journal-ref": null, "doi": "10.14778/3291264.3291266", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of GPS-enabled mobile devices, an unprecedented\namount of trajectory data is becoming available from various sources such as\nBikely, GPS-wayPoints, and Uber. The rise of innovative transportation services\nand recent break-throughs in autonomous vehicles will lead to the continued\ngrowth of trajectory data and related applications. Supporting these services\nin emerging platforms will require more efficient query processing in\ntrajectory databases. In this paper, we propose two new coverage queries for\ntrajectory databases: (i) k Maximizing Reverse Range Search on Trajectories\n(kMaxRRST); and (ii) a Maximum k Coverage Range Search on Trajectories\n(MaxkCovRST). We propose a novel index structure, the Trajectory Quadtree\n(TQ-tree) that utilizes a quadtree to hierarchically organize trajectories into\ndifferent quadtree nodes, and then applies a z-ordering to further organize the\ntrajectories by spatial locality inside each node. This structure is highly\neffective in pruning the trajectory search space, which is of independent\ninterest. By exploiting the TQ-tree data structure, we develop a\ndivide-and-conquer approach to compute the trajectory \"service value\", and a\nbest-first strategy to explore the trajectories using the appropriate upper\nbound on the service value to efficiently process a kMaxRRST query. Moreover,\nto solve the MaxkCovRST, which is a non-submodular NP-hard problem, we propose\na greedy approximation which also exploits the TQ-tree. We evaluate our\nalgorithms through an extensive experimental study on several real datasets,\nand demonstrate that our TQ-tree based algorithms outperform common baselines\nby two to three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 15:51:22 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ali", "Mohammed Eunus", ""], ["Abdullah", "Kaysar", ""], ["Eusuf", "Shadman Saqib", ""], ["Choudhury", "Farhana M.", ""], ["Culpepper", "J. Shane", ""], ["Sellis", "Timos", ""]]}, {"id": "1804.00742", "submitter": "Jing Zhong", "authors": "Jing Zhong, Roy D. Yates and Emina Soljanin", "title": "Minimizing Content Staleness in Dynamo-Style Replicated Storage Systems", "comments": "INFOCOM AoI Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency in data storage systems requires any read operation to return the\nmost recent written version of the content. In replicated storage systems,\nconsistency comes at the price of delay due to large-scale write and read\noperations. Many applications with low latency requirements tolerate data\nstaleness in order to provide high availability and low operation latency.\nUsing age of information as the staleness metric, we examine a data updating\nsystem in which real-time content updates are replicated and stored in a\nDynamo-style quorum- based distributed system. A source sends updates to all\nthe nodes in the system and waits for acknowledgements from the earliest subset\nof nodes, known as a write quorum. An interested client fetches the update from\nanother set of nodes, defined as a read quorum. We analyze the staleness-delay\ntradeoff in replicated storage by varying the write quorum size. With a larger\nwrite quorum, an instantaneous read is more likely to get the latest update\nwritten by the source. However, the age of the content written to the system is\nmore likely to become stale as the write quorum size increases. For shifted\nexponential distributed write delay, we derive the age optimized write quorum\nsize that balances the likelihood of reading the latest update and the\nfreshness of the latest update written by the source.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 21:45:43 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Zhong", "Jing", ""], ["Yates", "Roy D.", ""], ["Soljanin", "Emina", ""]]}, {"id": "1804.00770", "submitter": "Yongjoo Park", "authors": "Yongjoo Park, Barzan Mozafari, Joseph Sorenson, Junhao Wang", "title": "VerdictDB: Universalizing Approximate Query Processing", "comments": "Extended technical report of the paper that appeared in Proceedings\n  of the 2018 International Conference on Management of Data, pp. 1461-1476.\n  ACM, 2018", "journal-ref": null, "doi": "10.1145/3183713.3196905", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite 25 years of research in academia, approximate query processing (AQP)\nhas had little industrial adoption. One of the major causes of this slow\nadoption is the reluctance of traditional vendors to make radical changes to\ntheir legacy codebases, and the preoccupation of newer vendors (e.g.,\nSQL-on-Hadoop products) with implementing standard features. Additionally, the\nfew AQP engines that are available are each tied to a specific platform and\nrequire users to completely abandon their existing databases---an unrealistic\nexpectation given the infancy of the AQP technology. Therefore, we argue that a\nuniversal solution is needed: a database-agnostic approximation engine that\nwill widen the reach of this emerging technology across various platforms.\n  Our proposal, called VerdictDB, uses a middleware architecture that requires\nno changes to the backend database, and thus, can work with all off-the-shelf\nengines. Operating at the driver-level, VerdictDB intercepts analytical queries\nissued to the database and rewrites them into another query that, if executed\nby any standard relational engine, will yield sufficient information for\ncomputing an approximate answer. VerdictDB uses the returned result set to\ncompute an approximate answer and error estimates, which are then passed on to\nthe user or application. However, lack of access to the query execution layer\nintroduces significant challenges in terms of generality, correctness, and\nefficiency. This paper shows how VerdictDB overcomes these challenges and\ndelivers up to 171$\\times$ speedup (18.45$\\times$ on average) for a variety of\nexisting engines, such as Impala, Spark SQL, and Amazon Redshift, while\nincurring less than 2.6% relative error. VerdictDB is open-sourced under Apache\nLicense.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 00:33:34 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 02:16:09 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 15:13:02 GMT"}, {"version": "v4", "created": "Sat, 9 Jun 2018 16:19:34 GMT"}, {"version": "v5", "created": "Fri, 15 Jun 2018 20:48:21 GMT"}, {"version": "v6", "created": "Thu, 8 Nov 2018 18:20:06 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Park", "Yongjoo", ""], ["Mozafari", "Barzan", ""], ["Sorenson", "Joseph", ""], ["Wang", "Junhao", ""]]}, {"id": "1804.00914", "submitter": "Marc Shapiro", "authors": "Marc Shapiro (DELYS, Inria, LIP6), Pierre Sutra", "title": "Database Consistency Models", "comments": null, "journal-ref": "Sherif Sakr; Albert Zomaya. Encyclopedia of Big Data Technologies,\n  Springer, 2017, 978-3-319-63962-8", "doi": "10.1007/978-3-319-63962-8\\_203-1", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data store allows application processes to put and get data from a shared\nmemory. In general, a data store cannot be modelled as a strictly sequential\nprocess. Applications observe non-sequential behaviours, called anomalies. The\nset of pos- sible behaviours, and conversely of possible anomalies, constitutes\nthe consistency model of the data store.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 11:33:18 GMT"}], "update_date": "2018-06-24", "authors_parsed": [["Shapiro", "Marc", "", "DELYS, Inria, LIP6"], ["Sutra", "Pierre", ""]]}, {"id": "1804.01256", "submitter": "Thomas Guyet", "authors": "Thomas Guyet (LACODAM), Ren\\'e Quiniou (LACODAM)", "title": "NegPSpan: efficient extraction of negative sequential patterns with\n  embedding constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent sequential patterns consists in extracting recurrent\nbehaviors, modeled as patterns, in a big sequence dataset. Such patterns inform\nabout which events are frequently observed in sequences, i.e. what does really\nhappen. Sometimes, knowing that some specific event does not happen is more\ninformative than extracting a lot of observed events. Negative sequential\npatterns (NSP) formulate recurrent behaviors by patterns containing both\nobserved events and absent events. Few approaches have been proposed to mine\nsuch NSPs. In addition, the syntax and semantics of NSPs differ in the\ndifferent methods which makes it difficult to compare them. This article\nprovides a unified framework for the formulation of the syntax and the\nsemantics of NSPs. Then, we introduce a new algorithm, NegPSpan, that extracts\nNSPs using a PrefixSpan depth-first scheme and enabling maxgap constraints that\nother approaches do not take into account. The formal framework allows for\nhighlighting the differences between the proposed approach wrt to the methods\nfrom the literature, especially wrt the state of the art approach eNSP.\nIntensive experiments on synthetic and real datasets show that NegPSpan can\nextract meaningful NSPs and that it can process bigger datasets than eNSP\nthanks to significantly lower memory requirements and better computation times.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 06:47:32 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 13:42:47 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Guyet", "Thomas", "", "LACODAM"], ["Quiniou", "Ren\u00e9", "", "LACODAM"]]}, {"id": "1804.01379", "submitter": "Iqbal H. Sarker", "authors": "Iqbal H. Sarker, Flora D. Salim", "title": "Mining User Behavioral Rules from Smartphone Data through Association\n  Analysis", "comments": "12 pages, Springer, The 22nd Pacific-Asia Conference on Knowledge\n  Discovery and Data Mining (PAKDD 2018), Melbourne, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of smart mobile phones and their powerful sensing\ncapabilities have enabled the collection of rich contextual information and\nmobile phone usage records through the device logs. This paper formulates the\nproblem of mining behavioral association rules of individual mobile phone users\nutilizing their smartphone data. Association rule learning is the most popular\ntechnique to discover rules utilizing large datasets. However, it is well-known\nthat a large proportion of association rules generated are redundant. This\nredundant production makes not only the rule-set unnecessarily large but also\nmakes the decision making process more complex and ineffective. In this paper,\nwe propose an approach that effectively identifies the redundancy in\nassociations and extracts a concise set of behavioral association rules that\nare non-redundant. The effectiveness of the proposed approach is examined by\nconsidering the real mobile phone datasets of individual users.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 04:37:33 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Sarker", "Iqbal H.", ""], ["Salim", "Flora D.", ""]]}, {"id": "1804.01405", "submitter": "Manuel Namici", "authors": "Manuel Namici", "title": "R2RML Mappings in OBDA Systems: Enabling Comparison among OBDA Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's large enterprises there is a significant increasing trend in the\namount of data that has to be stored and processed. To complicate this scenario\nthe complexity of organizing and managing a large collection of data,\nstructured according to a single, unified schema, makes so that there is almost\nnever a single place where to look to satisfy an information need.\n  The Ontology-Based Data Access (OBDA) paradigm aims at mitigating this\nphenomenon by providing to the users of the system a unified and shared\nconceptual view of the domain of interest (ontology), while still enabling the\ndata to be stored in different data sources, which are managed by a relational\ndatabase. In an OBDA system the link between the data stored at the sources and\nthe ontology is provided through a declarative specification given in terms of\na set of mappings.\n  In this work we focus on comparing two of the available systems for OBDA,\nnamely, Mastro and Ontop, by adopting OBDA specifications based on W3C\nrecommendations. We first show how support for R2RML mappings has been\nintegrated in Mastro, which was the last feature missing in order to enable the\nsystem to use specifications based solely on W3C recommendations relevant to\nOBDA. We then proceed in performing a comparison between these systems over two\nOBDA specifications, the NPD Benchmark and the ACI specification.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 13:43:21 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Namici", "Manuel", ""]]}, {"id": "1804.01614", "submitter": "Jianbin Qin", "authors": "Jianbin Qin and Chuan Xiao", "title": "Pigeonring: A Principle for Faster Thresholded Similarity Search", "comments": "17 pages, 38 figures. Accepted and published in VLDB 2019. Please\n  cite the VLDB paper: @article{DBLP:journals/pvldb/QinX18, author = {Jianbin\n  Qin and Chuan Xiao}, title = {Pigeonring: {A} Principle for Faster\n  Thresholded Similarity Search}, journal = {{PVLDB}}, year = {2018}, }", "journal-ref": "Proceedings of the VLDB Endowment, vol12, 2018, 1, 28-42", "doi": "10.14778/3275536.3275539", "report-no": null, "categories": "cs.DB cs.DS cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pigeonhole principle states that if $n$ items are contained in $m$ boxes,\nthen at least one box has no more than $n / m$ items. It is utilized to solve\nmany data management problems, especially for thresholded similarity searches.\nDespite many pigeonhole principle-based solutions proposed in the last few\ndecades, the condition stated by the principle is weak. It only constrains the\nnumber of items in a single box. By organizing the boxes in a ring, we propose\na new principle, called the pigeonring principle, which constrains the number\nof items in multiple boxes and yields stronger conditions. To utilize the new\nprinciple, we focus on problems defined in the form of identifying data objects\nwhose similarities or distances to the query is constrained by a threshold.\nMany solutions to these problems utilize the pigeonhole principle to find\ncandidates that satisfy a filtering condition. By the new principle, stronger\nfiltering conditions can be established. We show that the pigeonhole principle\nis a special case of the new principle. This suggests that all the pigeonhole\nprinciple-based solutions are possible to be accelerated by the new principle.\nA universal filtering framework is introduced to encompass the solutions to\nthese problems based on the new principle. Besides, we discuss how to quickly\nfind candidates specified by the new principle. The implementation requires\nonly minor modifications on top of existing pigeonhole principle-based\nalgorithms. Experimental results on real datasets demonstrate the applicability\nof the new principle as well as the superior performance of the algorithms\nbased on the new principle.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 22:01:43 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 21:42:29 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 03:10:24 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Qin", "Jianbin", ""], ["Xiao", "Chuan", ""]]}, {"id": "1804.01640", "submitter": "Sai Vikneshwar Mani Jayaraman", "authors": "Aarthy Shivram Arun, Sai Vikneshwar Mani Jayaraman, Christopher R\\'e\n  and Atri Rudra", "title": "Hypertree Decompositions Revisited for PGMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the classical problem of exact inference on probabilistic\ngraphical models (PGMs). Our algorithm is based on recent worst-case optimal\ndatabase join algorithms, which can be asymptotically faster than traditional\ndata processing methods. We present the first empirical evaluation of these new\nalgorithms via JoinInfer, a new exact inference engine. We empirically explore\nthe properties of the data for which our engine can be expected to outperform\ntraditional inference engines refining current theoretical notions. Further,\nJoinInfer outperforms existing state-of-the-art inference engines (ACE, IJGP\nand libDAI) on some standard benchmark datasets by up to a factor of 630x.\nFinally, we propose a promising data-driven heuristic that extends JoinInfer to\nautomatically tailor its parameters and/or switch to the traditional inference\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 01:05:34 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Arun", "Aarthy Shivram", ""], ["Jayaraman", "Sai Vikneshwar Mani", ""], ["R\u00e9", "Christopher", ""], ["Rudra", "Atri", ""]]}, {"id": "1804.01942", "submitter": "Habib Saissi", "authors": "Habib Saissi, Marco Serafini and Neeraj Suri", "title": "Scaling Out Acid Applications with Operation Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OLTP applications with high workloads that cannot be served by a single\nserver need to scale out to multiple servers. Typically, scaling out entails\nassigning a different partition of the application state to each server. But\ndata partitioning is at odds with preserving the strong consistency guarantees\nof ACID transactions, a fundamental building block of many OLTP applications.\nThe more we scale out and spread data across multiple servers, the more\nfrequent distributed transactions accessing data at different servers will be.\nWith a large number of servers, the high cost of distributed transactions makes\nscaling out ineffective or even detrimental.\n  In this paper we propose Operation Partitioning, a novel paradigm to scale\nout OLTP applications that require ACID guarantees. Operation Partitioning\nindirectly partitions data across servers by partitioning the application's\noperations through static analysis. This partitioning of operations yields to a\nlock-free Conveyor Belt protocol for distributed coordination, which can scale\nout unmodified applications running on top of unmodified database management\nsystems. We implement the protocol in a system called Elia and use it to scale\nout two applications, TPC-W and RUBiS. Our experiments show that Elia can\nincrease maximum throughput by up to 4.2x and reduce latency by up to 58.6x\ncompared to MySQL Cluster while at the same time providing a stronger isolation\nguarantee (serializability instead of read committed).\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 16:31:58 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Saissi", "Habib", ""], ["Serafini", "Marco", ""], ["Suri", "Neeraj", ""]]}, {"id": "1804.02593", "submitter": "Philipp Eichmann", "authors": "Philipp Eichmann, Carsten Binnig, Tim Kraska, Emanuel Zgraggen", "title": "IDEBench: A Benchmark for Interactive Data Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing benchmarks for analytical database systems such as TPC-DS and TPC-H\nare designed for static reporting scenarios. The main metric of these\nbenchmarks is the performance of running individual SQL queries over a\nsynthetic database. In this paper, we argue that such benchmarks are not\nsuitable for evaluating database workloads originating from interactive data\nexploration (IDE) systems where most queries are ad-hoc, not based on\npredefined reports, and built incrementally. As a main contribution, we present\na novel benchmark called IDEBench that can be used to evaluate the performance\nof database systems for IDE workloads. As opposed to traditional benchmarks for\nanalytical database systems, our goal is to provide more meaningful workloads\nand datasets that can be used to benchmark IDE query engines, with a particular\nfocus on metrics that capture the trade-off between query performance and\nquality of the result. As a second contribution, this paper evaluates and\ndiscusses the performance results of selected IDE query engines using our\nbenchmark. The study includes two commercial systems, as well as two research\nprototypes (IDEA, approXimateDB/XDB), and one traditional analytical database\nsystem (MonetDB).\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 21:23:16 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Eichmann", "Philipp", ""], ["Binnig", "Carsten", ""], ["Kraska", "Tim", ""], ["Zgraggen", "Emanuel", ""]]}, {"id": "1804.02780", "submitter": "Ahmet Kara", "authors": "Ahmet Kara and Hung Q. Ngo and Milos Nikolic and Dan Olteanu and\n  Haozhe Zhang", "title": "Counting Triangles under Updates in Worst-Case Optimal Time", "comments": "simplified notation; incremental maintenance of full triangle query,\n  4-path count query, count queries with three relations added; improved the\n  space complexity of the dynamic algorithm maintaining the triangle count\n  query", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of incrementally maintaining the triangle count query\nunder single-tuple updates to the input relations. We introduce an approach\nthat exhibits a space-time tradeoff such that the space-time product is\nquadratic in the size of the input database and the update time can be as low\nas the square root of this size. This lowest update time is worst-case optimal\nconditioned on the Online Matrix-Vector Multiplication conjecture. The\nclassical and factorized incremental view maintenance approaches are recovered\nas special cases of our approach within the space-time tradeoff. In particular,\nthey require linear-time update maintenance, which is suboptimal. Our approach\nalso recovers the worst-case optimal time complexity for computing the triangle\ncount in the non-incremental setting.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 00:51:11 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 07:40:37 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Kara", "Ahmet", ""], ["Ngo", "Hung Q.", ""], ["Nikolic", "Milos", ""], ["Olteanu", "Dan", ""], ["Zhang", "Haozhe", ""]]}, {"id": "1804.03048", "submitter": "Cagatay Demiralp", "authors": "Marco Cavallo and \\c{C}a\\u{g}atay Demiralp", "title": "Clustrophile 2: Guided Visual Clustering Analysis", "comments": "IEEE VIS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is a common unsupervised learning method frequently used in\nexploratory data analysis. However, identifying relevant structures in\nunlabeled, high-dimensional data is nontrivial, requiring iterative\nexperimentation with clustering parameters as well as data features and\ninstances. The number of possible clusterings for a typical dataset is vast,\nand navigating in this vast space is also challenging. The absence of\nground-truth labels makes it impossible to define an optimal solution, thus\nrequiring user judgment to establish what can be considered a satisfiable\nclustering result. Data scientists need adequate interactive tools to\neffectively explore and navigate the large clustering space so as to improve\nthe effectiveness of exploratory clustering analysis. We introduce\n\\textit{Clustrophile~2}, a new interactive tool for guided clustering analysis.\n\\textit{Clustrophile~2} guides users in clustering-based exploratory analysis,\nadapts user feedback to improve user guidance, facilitates the interpretation\nof clusters, and helps quickly reason about differences between clusterings. To\nthis end, \\textit{Clustrophile~2} contributes a novel feature, the Clustering\nTour, to help users choose clustering parameters and assess the quality of\ndifferent clustering results in relation to current analysis goals and user\nexpectations. We evaluate \\textit{Clustrophile~2} through a user study with 12\ndata scientists, who used our tool to explore and interpret sub-cohorts in a\ndataset of Parkinson's disease patients. Results suggest that\n\\textit{Clustrophile~2} improves the speed and effectiveness of exploratory\nclustering analysis for both experts and non-experts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 15:05:56 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 04:25:02 GMT"}, {"version": "v3", "created": "Sat, 8 Sep 2018 02:57:14 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Cavallo", "Marco", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1804.04260", "submitter": "Houari Mahfoud", "authors": "Houari Mahfoud", "title": "Graph Pattern Matching Preserving Label-Repetition Constraints", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pattern matching is a routine process for a wide variety of\napplications such as social network analysis. It is typically defined in terms\nof subgraph isomorphism which is NP-Complete. To lower its complexity, many\nextensions of graph simulation have been proposed which focus on some\ntopological constraints of pattern graphs that can be preserved in\npolynomial-time over data graphs. We discuss in this paper the satisfaction of\na new topological constraint, called Label-Repetition constraint. To the best\nof our knowledge, existing polynomial approaches fail to preserve this\nconstraint, and moreover, one can adopt only subgraph isomorphism for this end\nwhich is cost-prohibitive. We present first a necessary and sufficient\ncondition that a data subgraph must satisfy to preserve the Label-Repetition\nconstraints of the pattern graph. Furthermore, we define matching based on a\nnotion of triple simulation, an extension of graph simulation by considering\nthe new topological constraint. We show that with this extension, graph pattern\nmatching can be performed in polynomial-time, by providing such an algorithm.\nOur algorithm is sub-quadratic in the size of data graphs only, and quartic in\ngeneral. We show that our results can be combined with orthogonal approaches\nfor more expressive graph pattern matching.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 00:04:05 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Mahfoud", "Houari", ""]]}, {"id": "1804.04343", "submitter": "Amit Saha", "authors": "Ramdoot Pydipaty and Amit Saha", "title": "On Using Non-Volatile Memory in Apache Lucene", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Lucene is a widely popular information retrieval library used to\nprovide search functionality in an extremely wide variety of applications.\nNaturally, it has to efficiently index and search large number of documents.\nWith non-volatile memory in DIMM form factor (NVDIMM), software now has access\nto durable, byte-addressable memory with write latency within an order of\nmagnitude of DRAM write latency.\n  In this preliminary article, we present the first reported work on the impact\nof using NVDIMM on the performance of committing, searching, and near-real time\nsearching in Apache Lucene. We show modest improvements by using NVM but, our\nempirical study suggests that bigger impact requires redesigning Lucene to\naccess NVM as byte-addressable memory using loads and stores, instead of\naccessing NVM via the file system.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 06:39:28 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Pydipaty", "Ramdoot", ""], ["Saha", "Amit", ""]]}, {"id": "1804.04367", "submitter": "Olivier Cur\\'e", "authors": "Xiangnan Ren and Olivier Cur\\'e and Hubert Naacke and Guohui Xiao", "title": "BigSR: an empirical study of real-time expressive RDF stream reasoning\n  on modern Big Data platforms", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trade-off between language expressiveness and system scalability (E&S) is\na well-known problem in RDF stream reasoning. Higher expressiveness supports\nmore complex reasoning logic, however, it may also hinder system scalability.\nCurrent research mainly focuses on logical frameworks suitable for stream\nreasoning as well as the implementation and the evaluation of prototype\nsystems. These systems are normally developed in a centralized setting which\nsuffer from inherent limited scalability, while an in-depth study of applying\ndistributed solutions to cover E&S is still missing. In this paper, we aim to\nexplore the feasibility of applying modern distributed computing frameworks to\nmeet E&S all together. To do so, we first propose BigSR, a technical\ndemonstrator that supports a positive fragment of the LARS framework. For the\nsake of generality and to cover a wide variety of use cases, BigSR relies on\nthe two main execution models adopted by major distributed execution\nframeworks: Bulk Synchronous Processing (BSP) and Record-at-A-Time (RAT).\nAccordingly, we implement BigSR on top of Apache Spark Streaming (BSP model)\nand Apache Flink (RAT model). In order to conclude on the impacts of BSP and\nRAT on E&S, we analyze the ability of the two models to support distributed\nstream reasoning and identify several types of use cases characterized by their\nlevels of support. This classification allows for quantifying the E&S trade-off\nby assessing the scalability of each type of use case \\wrt its level of\nexpressiveness. Then, we conduct a series of experiments with 15 queries from 4\ndifferent datasets. Our experiments show that BigSR over both BSP and RAT\ngenerally scales up to high throughput beyond million-triples per second (with\nor without recursion), and RAT attains sub-millisecond delay for stateless\nquery operators.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 08:15:17 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ren", "Xiangnan", ""], ["Cur\u00e9", "Olivier", ""], ["Naacke", "Hubert", ""], ["Xiao", "Guohui", ""]]}, {"id": "1804.04526", "submitter": "Simon Gottschalk", "authors": "Simon Gottschalk, Elena Demidova", "title": "EventKG: A Multilingual Event-Centric Temporal Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key requirements to facilitate semantic analytics of information\nregarding contemporary and historical events on the Web, in the news and in\nsocial media is the availability of reference knowledge repositories containing\ncomprehensive representations of events and temporal relations. Existing\nknowledge graphs, with popular examples including DBpedia, YAGO and Wikidata,\nfocus mostly on entity-centric information and are insufficient in terms of\ntheir coverage and completeness with respect to events and temporal relations.\nEventKG presented in this paper is a multilingual event-centric temporal\nknowledge graph that addresses this gap. EventKG incorporates over 690 thousand\ncontemporary and historical events and over 2.3 million temporal relations\nextracted from several large-scale knowledge graphs and semi-structured sources\nand makes them available through a canonical representation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 14:12:48 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Gottschalk", "Simon", ""], ["Demidova", "Elena", ""]]}, {"id": "1804.05639", "submitter": "Jos\\'e M. Gim\\'enez-Garc\\'ia", "authors": "Jos\\'e M. Gim\\'enez-Garc\\'ia, Ma\\'isa Duarte, Antoine Zimmermann,\n  Christophe Gravier, Estevam R. Hruschke Jr., Pierre Maret", "title": "NELL2RDF: Reading the Web, and Publishing it as Linked Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NELL is a system that continuously reads the Web to extract knowledge in form\nof entities and relations between them. It has been running since January 2010\nand extracted over 50,000,000 candidate statements. NELL's generated data\ncomprises all the candidate statements together with detailed information about\nhow it was generated. This information includes how each component of the\nsystem contributed to the extraction of the statement, as well as when that\nhappened and how confident the system is in the veracity of the statement.\nHowever, the data is only available in an ad hoc CSV format that makes it\ndifficult to exploit out of the context of NELL. In order to make it more\nusable for other communities, we adopt Linked Data principles to publish a more\nstandardized, self-describing dataset with rich provenance metadata.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 12:37:35 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Gim\u00e9nez-Garc\u00eda", "Jos\u00e9 M.", ""], ["Duarte", "Ma\u00edsa", ""], ["Zimmermann", "Antoine", ""], ["Gravier", "Christophe", ""], ["Hruschke", "Estevam R.", "Jr."], ["Maret", "Pierre", ""]]}, {"id": "1804.05892", "submitter": "Doris Xin", "authors": "Doris Xin, Litian Ma, Jialin Liu, Stephen Macke, Shuchen Song, Aditya\n  Parameswaran", "title": "Accelerating Human-in-the-loop Machine Learning: Challenges and\n  Opportunities", "comments": "to be published in SIGMOD '18 DEEM Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of machine learning (ML) workflows is a tedious process of\niterative experimentation: developers repeatedly make changes to workflows\nuntil the desired accuracy is attained. We describe our vision for a\n\"human-in-the-loop\" ML system that accelerates this process: by intelligently\ntracking changes and intermediate results over time, such a system can enable\nrapid iteration, quick responsive feedback, introspection and debugging, and\nbackground execution and automation. We finally describe Helix, our preliminary\nattempt at such a system that has already led to speedups of up to 10x on\ntypical iterative workflows against competing systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 18:54:11 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Xin", "Doris", ""], ["Ma", "Litian", ""], ["Liu", "Jialin", ""], ["Macke", "Stephen", ""], ["Song", "Shuchen", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1804.06087", "submitter": "Wei Wang", "authors": "Wei Wang and Sheng Wang and Jinyang Gao and Meihui Zhang and Gang Chen\n  and Teck Khim Ng and Beng Chin Ooi", "title": "Rafiki: Machine Learning as an Analytics Service System", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analytics is gaining massive momentum in the last few years.\nApplying machine learning models to big data has become an implicit requirement\nor an expectation for most analysis tasks, especially on high-stakes\napplications.Typical applications include sentiment analysis against reviews\nfor analyzing on-line products, image classification in food logging\napplications for monitoring user's daily intake and stock movement prediction.\nExtending traditional database systems to support the above analysis is\nintriguing but challenging. First, it is almost impossible to implement all\nmachine learning models in the database engines. Second, expertise knowledge is\nrequired to optimize the training and inference procedures in terms of\nefficiency and effectiveness, which imposes heavy burden on the system users.\nIn this paper, we develop and present a system, called Rafiki, to provide the\ntraining and inference service of machine learning models, and facilitate\ncomplex analytics on top of cloud platforms. Rafiki provides distributed\nhyper-parameter tuning for the training service, and online ensemble modeling\nfor the inference service which trades off between latency and accuracy.\nExperimental results confirm the efficiency, effectiveness, scalability and\nusability of Rafiki.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 07:54:55 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wang", "Wei", ""], ["Wang", "Sheng", ""], ["Gao", "Jinyang", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Ng", "Teck Khim", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1804.06653", "submitter": "Andrea Tagarelli", "authors": "Domenico Mandaglio, Alessia Amelio, Andrea Tagarelli", "title": "Consensus Community Detection in Multilayer Networks using\n  Parameter-free Graph Pruning", "comments": "Accepted as regular paper at The 22nd Pacific-Asia Conference on\n  Knowledge Discovery and Data Mining (PAKDD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering ensemble paradigm has emerged as an effective tool for\ncommunity detection in multilayer networks, which allows for producing\nconsensus solutions that are designed to be more robust to the algorithmic\nselection and configuration bias. However, one limitation is related to the\ndependency on a co-association threshold that controls the degree of consensus\nin the community structure solution. The goal of this work is to overcome this\nlimitation with a new framework of ensemble-based multilayer community\ndetection, which features parameter-free identification of consensus\ncommunities based on generative models of graph pruning that are able to filter\nout noisy co-associations. We also present an enhanced version of the\nmodularity-driven ensemble-based multilayer community detection method, in\nwhich community memberships of nodes are reconsidered to optimize the\nmultilayer modularity of the consensus solution. Experimental evidence on\nreal-world networks confirms the beneficial effect of using model-based\nfiltering methods and also shows the superiority of the proposed method on\nstate-of-the-art multilayer community detection.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 11:19:01 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Mandaglio", "Domenico", ""], ["Amelio", "Alessia", ""], ["Tagarelli", "Andrea", ""]]}, {"id": "1804.06764", "submitter": "Ioannis Christou Ph.D.", "authors": "Ioannis T. Christou, Emmanouil Amolochitis, Zheng-Hua Tan", "title": "A Parallel/Distributed Algorithmic Framework for Mining All Quantitative\n  Association Rules", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present QARMA, an efficient novel parallel algorithm for mining all\nQuantitative Association Rules in large multidimensional datasets where items\nare required to have at least a single common attribute to be specified in the\nrules single consequent item. Given a minimum support level and a set of\nthreshold criteria of interestingness measures such as confidence, conviction\netc. our algorithm guarantees the generation of all non-dominated Quantitative\nAssociation Rules that meet the minimum support and interestingness\nrequirements. Such rules can be of great importance to marketing departments\nseeking to optimize targeted campaigns, or general market segmentation. They\ncan also be of value in medical applications, financial as well as predictive\nmaintenance domains. We provide computational results showing the scalability\nof our algorithm, and its capability to produce all rules to be found in large\nscale synthetic and real world datasets such as Movie Lens, within a few\nseconds or minutes of computational time on commodity hardware.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 14:34:08 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Christou", "Ioannis T.", ""], ["Amolochitis", "Emmanouil", ""], ["Tan", "Zheng-Hua", ""]]}, {"id": "1804.06829", "submitter": "Arnab Bhattacharya", "authors": "Akhil Arora and Sakshi Sinha and Piyush Kumar and Arnab Bhattacharya", "title": "HD-Index: Pushing the Scalability-Accuracy Boundary for Approximate kNN\n  Search in High-Dimensional Spaces", "comments": "PVLDB 11(8):906-919, 2018", "journal-ref": null, "doi": "10.14778/3204028.3204034", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor searching of large databases in high-dimensional spaces is\ninherently difficult due to the curse of dimensionality. A flavor of\napproximation is, therefore, necessary to practically solve the problem of\nnearest neighbor search. In this paper, we propose a novel yet simple indexing\nscheme, HD-Index, to solve the problem of approximate k-nearest neighbor\nqueries in massive high-dimensional databases. HD-Index consists of a set of\nnovel hierarchical structures called RDB-trees built on Hilbert keys of\ndatabase objects. The leaves of the RDB-trees store distances of database\nobjects to reference objects, thereby allowing efficient pruning using distance\nfilters. In addition to triangular inequality, we also use Ptolemaic inequality\nto produce better lower bounds. Experiments on massive (up to billion scale)\nhigh-dimensional (up to 1000+) datasets show that HD-Index is effective,\nefficient, and scalable.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 17:28:48 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 13:13:00 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Arora", "Akhil", ""], ["Sinha", "Sakshi", ""], ["Kumar", "Piyush", ""], ["Bhattacharya", "Arnab", ""]]}, {"id": "1804.06894", "submitter": "Carsten Lutz", "authors": "Andre Hernich, Carsten Lutz, Fabio Papacchini, Frank Wolter", "title": "Dichotomies in Ontology-Mediated Querying with the Guarded Fragment", "comments": null, "journal-ref": null, "doi": "10.1145/3034786.3056108", "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of ontology-mediated querying when ontologies are\nformulated in the guarded fragment of first-order logic (GF). Our general aim\nis to classify the data complexity on the level of ontologies where query\nevaluation w.r.t. an ontology O is considered to be in PTime if all (unions of\nconjunctive) queries can be evaluated in PTime w.r.t. O and coNP-hard if at\nleast one query is coNP-hard w.r.t. O. We identify several large and relevant\nfragments of GF that enjoy a dichotomy between PTime and coNP, some of them\nadditionally admitting a form of counting. In fact, almost all ontologies in\nthe BioPortal repository fall into these fragments or can easily be rewritten\nto do so. We then establish a variation of Ladner's Theorem on the existence of\nNP-intermediate problems and use this result to show that for other fragments,\nthere is provably no such dichotomy. Again for other fragments (such as full\nGF), establishing a dichotomy implies the Feder-Vardi conjecture on the\ncomplexity of constraint satisfaction problems. We also link these results to\nDatalog-rewritability and study the decidability of whether a given ontology\nenjoys PTime query evaluation, presenting both positive and negative results.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 19:49:15 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Hernich", "Andre", ""], ["Lutz", "Carsten", ""], ["Papacchini", "Fabio", ""], ["Wolter", "Frank", ""]]}, {"id": "1804.07099", "submitter": "Vernon Asuncion Va", "authors": "Vernon Asuncion, Yan Zhang, Heng Zhang, Yun Bai and Weisheng Si", "title": "Loop Restricted Existential Rules and First-order Rewritability for\n  Query Answering", "comments": "Full paper version of extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ontology-based data access (OBDA), the classical database is enhanced with\nan ontology in the form of logical assertions generating new intensional\nknowledge. A powerful form of such logical assertions is the tuple-generating\ndependencies (TGDs), also called existential rules, where Horn rules are\nextended by allowing existential quantifiers to appear in the rule heads. In\nthis paper we introduce a new language called loop restricted (LR) TGDs\n(existential rules), which are TGDs with certain restrictions on the loops\nembedded in the underlying rule set. We study the complexity of this new\nlanguage. We show that the conjunctive query answering (CQA) under the LR TGDs\nis decid- able. In particular, we prove that this language satisfies the\nso-called bounded derivation-depth prop- erty (BDDP), which implies that the\nCQA is first-order rewritable, and its data complexity is in AC0 . We also\nprove that the combined complexity of the CQA is EXPTIME complete, while the\nlanguage membership is PSPACE complete. Then we extend the LR TGDs language to\nthe generalised loop restricted (GLR) TGDs language, and prove that this class\nof TGDs still remains to be first-order rewritable and properly contains most\nof other first-order rewritable TGDs classes discovered in the literature so\nfar.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 11:45:14 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 01:48:26 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Asuncion", "Vernon", ""], ["Zhang", "Yan", ""], ["Zhang", "Heng", ""], ["Bai", "Yun", ""], ["Si", "Weisheng", ""]]}, {"id": "1804.07156", "submitter": "Xing Niu", "authors": "Xing Niu, Raghav Kapoor, Boris Glavic, Dieter Gawlick, Zhen Hua Liu,\n  Vasudha Krishnaswamy and Venkatesh Radhakrishnan", "title": "Heuristic and Cost-based Optimization for Diverse Provenance Tasks", "comments": "IEEE Transactions on Knowledge and Data Engineering (TKDE), 2018,\n  long version, 31 pages. arXiv admin note: substantial text overlap with\n  arXiv:1701.05513", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering (TKDE), 2018", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-established technique for capturing database provenance as annotations\non data is to instrument queries to propagate such annotations. However, even\nsophisticated query optimizers often fail to produce efficient execution plans\nfor instrumented queries. We develop provenance-aware optimization techniques\nto address this problem. Specifically, we study algebraic equivalences targeted\nat instrumented queries and alternative ways of instrumenting queries for\nprovenance capture. Furthermore, we present an extensible heuristic and\ncost-based optimization framework utilizing these optimizations. Our\nexperiments confirm that these optimizations are highly effective, improving\nperformance by several orders of magnitude for diverse provenance tasks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 22:35:47 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Niu", "Xing", ""], ["Kapoor", "Raghav", ""], ["Glavic", "Boris", ""], ["Gawlick", "Dieter", ""], ["Liu", "Zhen Hua", ""], ["Krishnaswamy", "Vasudha", ""], ["Radhakrishnan", "Venkatesh", ""]]}, {"id": "1804.07274", "submitter": "Cristina Feier", "authors": "David Carral, Cristina Feier, Pascal Hitzler", "title": "A Practical Acyclicity Notion for Query Answering over Horn-SRIQ\n  Ontologies", "comments": null, "journal-ref": "The Semantic Web - ISWC 2016 - 15th International Semantic Web\n  Conference, Kobe, Japan. Proceedings, Part I, volume 9981 of LNCS, 70-85,\n  October 2016. Springer", "doi": "10.1007/978-3-319-46523-4_5", "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjunctive query answering over expressive Horn Description Logic ontologies\nis a relevant and challenging problem which, in some cases, can be addressed by\napplication of the chase algorithm. In this paper, we define a novel acyclicity\nnotion which provides a sufficient condition for termination of the restricted\nchase over Horn-SRIQ TBoxes. We show that this notion generalizes most of the\nexisting acyclicity conditions (both theoretically and empirically).\nFurthermore, this new acyclicity notion gives rise to a very efficient\nreasoning procedure. We provide evidence for this by providing a\nmaterialization based reasoner for acyclic ontologies which outperforms other\nstate-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 16:57:25 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Carral", "David", ""], ["Feier", "Cristina", ""], ["Hitzler", "Pascal", ""]]}, {"id": "1804.07525", "submitter": "Jerome Darmont", "authors": "Ciprian-Octavian Truica (UPB), J\\'er\\^ome Darmont (ERIC), Alexandru\n  Boicea (UPB), Florin Radulescu (UPB)", "title": "Benchmarking Top-K Keyword and Top-K Document Processing with\n  T${}^2$K${}^2$ and T${}^2$K${}^2$D${}^2$", "comments": null, "journal-ref": "Future Generation Computer Systems, Elsevier, 2018, 85, pp.60-75.\n  https://www.sciencedirect.com/science/article/pii/S0167739X17323580", "doi": "10.1016/j.future.2018.02.037", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k keyword and top-k document extraction are very popular text analysis\ntechniques. Top-k keywords and documents are often computed on-the-fly, but\nthey exploit weighted vocabularies that are costly to build. To compare\ncompeting weighting schemes and database implementations, benchmarking is\ncustomary. To the best of our knowledge, no benchmark currently addresses these\nproblems. Hence, in this paper, we present T${}^2$K${}^2$, a top-k keywords and\ndocuments benchmark, and its decision support-oriented evolution\nT${}^2$K${}^2$D${}^2$. Both benchmarks feature a real tweet dataset and queries\nwith various complexities and selectivities. They help evaluate weighting\nschemes and database implementations in terms of computing performance. To\nillustrate our bench-marks' relevance and genericity, we successfully ran\nperformance tests on the TF-IDF and Okapi BM25 weighting schemes, on one hand,\nand on different relational (Oracle, PostgreSQL) and document-oriented\n(MongoDB) database implementations, on the other hand.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 10:01:43 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Truica", "Ciprian-Octavian", "", "UPB"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Boicea", "Alexandru", "", "UPB"], ["Radulescu", "Florin", "", "UPB"]]}, {"id": "1804.07550", "submitter": "Tianshu Song", "authors": "Tianshu Song, Feng Zhu, Ke Xu", "title": "Specialty-Aware Task Assignment in Spatial Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of Mobile Internet, spatial crowdsourcing is\ngaining more and more attention from both academia and industry.\n  In spatial crowdsourcing, spatial tasks are sent to workers based on their\nlocations.\n  A wide kind of tasks in spatial crowdsourcing are specialty-aware, which are\ncomplex and need to be completed by workers with different skills\ncollaboratively.\n  Existing studies on specialty-aware spatial crowdsourcing assume that each\nworker has a united charge when performing different tasks, no matter how many\nskills of her/him are used to complete the task, which is not fair and\npractical.\n  In this paper, we study the problem of specialty-aware task assignment in\nspatial crowdsourcing, where each worker has fine-grained charge for each of\ntheir skills, and the goal is to maximize the total number of completed tasks\nbased on tasks' budget and requirements on particular skills.\n  The problem is proven to be NP-hard. Thus, we propose two efficient\nheuristics to solve the problem.\n  Experiments on both synthetic and real datasets demonstrate the effectiveness\nand efficiency of our solutions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 11:11:36 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Song", "Tianshu", ""], ["Zhu", "Feng", ""], ["Xu", "Ke", ""]]}, {"id": "1804.07686", "submitter": "Saehan Jo", "authors": "Saehan Jo, Immanuel Trummer, Weicheng Yu, Daniel Liu, Xuezhi Wang,\n  Cong Yu, Niyati Mehta", "title": "Verifying Text Summaries of Relational Data Sets", "comments": "18 pages, 13 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel natural language query interface, the AggChecker, aimed at\ntext summaries of relational data sets. The tool focuses on natural language\nclaims that translate into an SQL query and a claimed query result. Similar in\nspirit to a spell checker, the AggChecker marks up text passages that seem to\nbe inconsistent with the actual data. At the heart of the system is a\nprobabilistic model that reasons about the input document in a holistic\nfashion. Based on claim keywords and the document structure, it maps each text\nclaim to a probability distribution over associated query translations. By\nefficiently executing tens to hundreds of thousands of candidate translations\nfor a typical input document, the system maps text claims to correctness\nprobabilities. This process becomes practical via a specialized processing\nbackend, avoiding redundant work via query merging and result caching.\nVerification is an interactive process in which users are shown tentative\nresults, enabling them to take corrective actions if necessary.\n  Our system was tested on a set of 53 public articles containing 392 claims.\nOur test cases include articles from major newspapers, summaries of survey\nresults, and Wikipedia articles. Our tool revealed erroneous claims in roughly\na third of test cases. A detailed user study shows that users using our tool\nare in average six times faster at checking text summaries, compared to generic\nSQL interfaces. In fully automated verification, our tool achieves\nsignificantly higher recall and precision than baselines from the areas of\nnatural language query interfaces and fact-checking.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 15:46:05 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 18:27:55 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Jo", "Saehan", ""], ["Trummer", "Immanuel", ""], ["Yu", "Weicheng", ""], ["Liu", "Daniel", ""], ["Wang", "Xuezhi", ""], ["Yu", "Cong", ""], ["Mehta", "Niyati", ""]]}, {"id": "1804.07829", "submitter": "Bin Yang", "authors": "Georgi Andonov, Bin Yang", "title": "A New Formulation of The Shortest Path Problem with On-Time Arrival\n  Reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study stochastic routing in the PAth-CEntric (PACE) uncertain road network\nmodel. In the PACE model, uncertain travel times are associated with not only\nedges but also some paths. The uncertain travel times associated with paths are\nable to well capture the travel time dependency among different edges. This\nsignificantly improves the accuracy of travel time distribution estimations for\narbitrary paths, which is a fundamental functionality in stochastic routing,\ncompared to classic uncertain road network models where uncertain travel times\nare associated with only edges.\n  We investigate a new formulation of the shortest path with on-time arrival\nreliability (SPOTAR) problem under the PACE model. Given a source, a\ndestination, and a travel time budget, the SPOTAR problem aims at finding a\npath that maximizes the on-time arrival probability. We develop a generic\nalgorithm with different speed-up strategies to solve the SPOTAR problem under\nthe PACE model. Empirical studies with substantial GPS trajectory data offer\ninsight into the design properties of the proposed algorithm and confirm that\nthe algorithm is effective.\n  This report extends the paper \"Stochastic Shortest Path Finding in\nPath-Centric Uncertain Road Networks\", to appear in IEEE MDM 2018, by providing\na concrete running example of the studied SPOTAR problem in the PACE model and\nadditional statistics of the used GPS trajectories in the experiments.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 21:10:46 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 18:54:35 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Andonov", "Georgi", ""], ["Yang", "Bin", ""]]}, {"id": "1804.07890", "submitter": "Bill Howe", "authors": "Ke Yang, Julia Stoyanovich, Abolfazl Asudeh, Bill Howe, HV Jagadish,\n  Gerome Miklau", "title": "A Nutritional Label for Rankings", "comments": "4 pages, SIGMOD demo, 3 figuress, ACM SIGMOD 2018", "journal-ref": null, "doi": "10.1145/3183713.3193568", "report-no": null, "categories": "cs.CY cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Algorithmic decisions often result in scoring and ranking individuals to\ndetermine credit worthiness, qualifications for college admissions and\nemployment, and compatibility as dating partners. While automatic and seemingly\nobjective, ranking algorithms can discriminate against individuals and\nprotected groups, and exhibit low diversity. Furthermore, ranked results are\noften unstable --- small changes in the input data or in the ranking\nmethodology may lead to drastic changes in the output, making the result\nuninformative and easy to manipulate. Similar concerns apply in cases where\nitems other than individuals are ranked, including colleges, academic\ndepartments, or products.\n  In this demonstration we present Ranking Facts, a Web-based application that\ngenerates a \"nutritional label\" for rankings. Ranking Facts is made up of a\ncollection of visual widgets that implement our latest research results on\nfairness, stability, and transparency for rankings, and that communicate\ndetails of the ranking methodology, or of the output, to the end user. We will\nshowcase Ranking Facts on real datasets from different domains, including\ncollege rankings, criminal risk assessment, and financial services.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 05:00:58 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Yang", "Ke", ""], ["Stoyanovich", "Julia", ""], ["Asudeh", "Abolfazl", ""], ["Howe", "Bill", ""], ["Jagadish", "HV", ""], ["Miklau", "Gerome", ""]]}, {"id": "1804.08822", "submitter": "Jimmy Lin", "authors": "Kareem El Gebaly and Jimmy Lin", "title": "In-Browser Split-Execution Support for Interactive Analytics in the\n  Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The canonical analytics architecture today consists of a browser connected to\na backend in the cloud. In all deployments that we are aware of, the browser is\nsimply a dumb rendering endpoint. As an alternative, this paper explores\nsplit-execution architectures that push analytics capabilities into the\nbrowser. We show that, by taking advantage of typed arrays and asm.js, it is\npossible to build an analytical RDBMS in JavaScript that runs in a browser,\nachieving performance rivaling native databases. To support interactive data\nexploration, our Afterburner prototype automatically generates local\nmaterialized views from a backend database that are then shipped to the browser\nto facilitate subsequent interactions seamlessly and efficiently. We compare\nthis architecture to several alternative deployments, experimentally\ndemonstrating performance parity, while at the same time providing additional\nadvantages in terms of administrative and operational simplicity.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 03:02:54 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Gebaly", "Kareem El", ""], ["Lin", "Jimmy", ""]]}, {"id": "1804.08834", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi", "title": "Measuring and Computing Database Inconsistency via Repairs", "comments": "Submission as short paper; to appear in Proc. Scalable Uncertainty\n  Management, SUM 2018. Abstract and keywords added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic numerical measure of inconsistency of a database with\nrespect to a set of integrity constraints. It is based on an abstract repair\nsemantics. A particular inconsistency measure associated to cardinality-repairs\nis investigated; and we show that it can be computed via answer-set programs.\n  Keywords: Integrity constraints in databases, inconsistent databases,\ndatabase repairs, inconsistency measure.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 04:04:27 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 01:50:28 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2018 20:42:04 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Bertossi", "Leopoldo", ""]]}, {"id": "1804.08985", "submitter": "Pradeeban Kathiravelu", "authors": "Pradeeban Kathiravelu, Ashish Sharma, Helena Galhardas, Peter Van Roy,\n  Lu{\\i}s Veiga", "title": "On-Demand Big Data Integration: A Hybrid ETL Approach for Reproducible\n  Scientific Research", "comments": "Pre-print Submitted to the DMAH Special Issue of the Springer DAPD\n  Journal", "journal-ref": null, "doi": null, "report-no": "INESC-ID Lisboa Tech. Rep. 3/2018", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific research requires access, analysis, and sharing of data that is\ndistributed across various heterogeneous data sources at the scale of the\nInternet. An eager ETL process constructs an integrated data repository as its\nfirst step, integrating and loading data in its entirety from the data sources.\nThe bootstrapping of this process is not efficient for scientific research that\nrequires access to data from very large and typically numerous distributed data\nsources. a lazy ETL process loads only the metadata, but still eagerly. Lazy\nETL is faster in bootstrapping. However, queries on the integrated data\nrepository of eager ETL perform faster, due to the availability of the entire\ndata beforehand.\n  In this paper, we propose a novel ETL approach for scientific data\nintegration, as a hybrid of eager and lazy ETL approaches, and applied both to\ndata as well as metadata. This way, Hybrid ETL supports incremental integration\nand loading of metadata and data from the data sources. We incorporate a\nhuman-in-the-loop approach, to enhance the hybrid ETL, with selective data\nintegration driven by the user queries and sharing of integrated data between\nusers. We implement our hybrid ETL approach in a prototype platform, Obidos,\nand evaluate it in the context of data sharing for medical research. Obidos\noutperforms both the eager ETL and lazy ETL approaches, for scientific research\ndata integration and sharing, through its selective loading of data and\nmetadata, while storing the integrated data in a scalable integrated data\nrepository.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 12:27:06 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Kathiravelu", "Pradeeban", ""], ["Sharma", "Ashish", ""], ["Galhardas", "Helena", ""], ["Van Roy", "Peter", ""], ["Veiga", "Lu\u0131s", ""]]}, {"id": "1804.09324", "submitter": "Abhirup  Chakraborty", "authors": "Abhirup Chakraborty", "title": "Processing Database Joins over a Shared-Nothing System of Multicore\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To process a large volume of data, modern data management systems use a\ncollection of machines connected through a network. This paper looks into the\nfeasibility of scaling up such a shared-nothing system while processing a\ncompute- and communication-intensive workload---processing distributed joins.\nBy exploiting multiple processing cores within the individual machines, we\nimplement a system to process database joins that parallelizes computation\nwithin each node, pipelines the computation with communication, parallelizes\nthe communication by allowing multiple simultaneous data transfers\n(send/receive), and removes synchronization barriers (a scalability bottleneck\nin a distributed data processing system). Our experimental results show that\nusing only four threads per node the framework achieves a 3.5x gains in\nintra-node performance while compared with a single-threaded counterpart.\nMoreover, with the join processing workload the cluster-wide performance (and\nspeedup) is observed to be dictated by the intra-node computational loads; this\nproperty brings a near-linear speedup with increasing nodes in the system, a\nfeature much desired in modern large-scale data processing system.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 02:30:43 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Chakraborty", "Abhirup", ""]]}, {"id": "1804.09996", "submitter": "Matthijs Douze", "authors": "Matthijs Douze and Alexandre Sablayrolles and Herv\\'e J\\'egou", "title": "Link and code: Fast indexing with graphs and compact regression codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search approaches based on graph walks have recently attained\noutstanding speed-accuracy trade-offs, taking aside the memory requirements. In\nthis paper, we revisit these approaches by considering, additionally, the\nmemory constraint required to index billions of images on a single server. This\nleads us to propose a method based both on graph traversal and compact\nrepresentations. We encode the indexed vectors using quantization and exploit\nthe graph structure to refine the similarity estimation.\n  In essence, our method takes the best of these two worlds: the search\nstrategy is based on nested graphs, thereby providing high precision with a\nrelatively small set of comparisons. At the same time it offers a significant\nmemory compression. As a result, our approach outperforms the state of the art\non operating points considering 64-128 bytes per vector, as demonstrated by our\nresults on two billion-scale public benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 11:24:42 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 10:01:51 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Douze", "Matthijs", ""], ["Sablayrolles", "Alexandre", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1804.09997", "submitter": "Jinyang  Gao", "authors": "Jinyang Gao, Wei Wang, Meihui Zhang, Gang Chen, H.V. Jagadish,\n  Guoliang Li, Teck Khim Ng, Beng Chin Ooi, Sheng Wang, Jingren Zhou", "title": "PANDA: Facilitating Usable AI Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in artificial intelligence (AI) and machine learning have\ncreated a general perception that AI could be used to solve complex problems,\nand in some situations over-hyped as a tool that can be so easily used.\nUnfortunately, the barrier to realization of mass adoption of AI on various\nbusiness domains is too high because most domain experts have no background in\nAI. Developing AI applications involves multiple phases, namely data\npreparation, application modeling, and product deployment. The effort of AI\nresearch has been spent mostly on new AI models (in the model training stage)\nto improve the performance of benchmark tasks such as image recognition. Many\nother factors such as usability, efficiency and security of AI have not been\nwell addressed, and therefore form a barrier to democratizing AI. Further, for\nmany real world applications such as healthcare and autonomous driving,\nlearning via huge amounts of possibility exploration is not feasible since\nhumans are involved. In many complex applications such as healthcare, subject\nmatter experts (e.g. Clinicians) are the ones who appreciate the importance of\nfeatures that affect health, and their knowledge together with existing\nknowledge bases are critical to the end results. In this paper, we take a new\nperspective on developing AI solutions, and present a solution for making AI\nusable. We hope that this resolution will enable all subject matter experts\n(eg. Clinicians) to exploit AI like data scientists.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 11:37:03 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Gao", "Jinyang", ""], ["Wang", "Wei", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Jagadish", "H. V.", ""], ["Li", "Guoliang", ""], ["Ng", "Teck Khim", ""], ["Ooi", "Beng Chin", ""], ["Wang", "Sheng", ""], ["Zhou", "Jingren", ""]]}, {"id": "1804.09999", "submitter": "Elias Alevizos", "authors": "Elias Alevizos, Alexander Artikis, Georgios Paliouras", "title": "Symbolic Automata with Memory: a Computational Model for Complex Event\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automaton model which is a combination of symbolic and register\nautomata, i.e., we enrich symbolic automata with memory. We call such automata\nRegister Match Automata (RMA). RMA extend the expressive power of symbolic\nautomata, by allowing formulas to be applied not only to the last element read\nfrom the input string, but to multiple elements, stored in their registers. RMA\nalso extend register automata, by allowing arbitrary formulas, besides equality\npredicates. We study the closure properties of RMA under union, concatenation,\nKleene+, complement and determinization and show that RMA, contrary to symbolic\nautomata, are not determinizable when viewed as recognizers, without taking the\noutput of transitions into account. However, when a window operator, a\nquintessential feature in Complex Event Processing, is used, RMA are indeed\ndeterminizable even when viewed as recognizers. We present detailed algorithms\nfor constructing deterministic RMA from regular expressions extended with\n$n$-ary constraints. We show how RMA can be used in Complex Event Processing in\norder to detect patterns upon streams of events, using a framework that\nprovides denotational and compositional semantics, and that allows for a\nsystematic treatment of such automata.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 11:38:45 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 12:21:40 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Alevizos", "Elias", ""], ["Artikis", "Alexander", ""], ["Paliouras", "Georgios", ""]]}, {"id": "1804.10388", "submitter": "Elias Alevizos", "authors": "Elias Alevizos, Alexander Artikis, Georgios Paliouras", "title": "Event Forecasting with Pattern Markov Chains", "comments": null, "journal-ref": null, "doi": "10.1145/3093742.3093920", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for online probabilistic event forecasting. We assume\nthat a user is interested in detecting and forecasting event patterns, given in\nthe form of regular expressions. Our system can consume streams of events and\nforecast when the pattern is expected to be fully matched. As more events are\nconsumed, the system revises its forecasts to reflect possible changes in the\nstate of the pattern. The framework of Pattern Markov Chains is used in order\nto learn a probabilistic model for the pattern, with which forecasts with\nguaranteed precision may be produced, in the form of intervals within which a\nfull match is expected. Experimental results from real-world datasets are shown\nand the quality of the produced forecasts is explored, using both precision\nscores and two other metrics: spread, which refers to the \"focusing resolution\"\nof a forecast (interval length), and distance, which captures how early a\nforecast is reported.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 08:33:12 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Alevizos", "Elias", ""], ["Artikis", "Alexander", ""], ["Paliouras", "Georgios", ""]]}, {"id": "1804.10565", "submitter": "Stefania Dumbrava", "authors": "Angela Bonifati, Stefania Dumbrava, and Emilio Jesus Gallego Arias", "title": "Certified Graph View Maintenance with Regular Datalog", "comments": "Paper presented at the 34nd International Conference on Logic\n  Programming (ICLP 2018), Oxford, UK, July 14 to July 17, 2018. 18 pages,\n  LaTeX, (arXiv:YYMM.NNNNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ the Coq proof assistant to develop a mechanically-certified\nframework for evaluating graph queries and incrementally maintaining\nmaterialized graph instances, also called views. The language we use for\ndefining queries and views is Regular Datalog (RD) -- a notable fragment of\nnon-recursive Datalog that can express complex navigational queries, with\ntransitive closure as native operator. We first design and encode the theory of\nRD and then mechanize a RD-specific evaluation algorithm capable of\nfine-grained, incremental graph view computation, which we prove sound with\nrespect to the declarative RD semantics. By using the Coq extraction mechanism,\nwe test an Ocaml version of the verified engine on a set of preliminary\nbenchmarks. Our development is particularly focused on leveraging existing\nverification and notational techniques to: a) define mechanized properties that\ncan be easily understood by logicians and database researchers and b) attain\nformal verification with limited effort. Our work is the first step towards a\nunified, machine-verified, formal framework for dynamic graph query languages\nand their evaluation engines. This paper is under consideration for acceptance\nin TPLP.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 15:42:36 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Bonifati", "Angela", ""], ["Dumbrava", "Stefania", ""], ["Arias", "Emilio Jesus Gallego", ""]]}, {"id": "1804.10711", "submitter": "Pritish Yuvraj", "authors": "Pritish Yuvraj, Suneetha K. R", "title": "Modified Apriori Graph Algorithm for Frequent Pattern Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web Usage Mining is an application of Data Mining Techniques to discover\ninteresting usage patterns from web data in order to understand and better\nserve the needs of web-based applications. The paper proposes an algorithm for\nfinding these usage patterns using a modified version of Apriori Algorithm\ncalled Apriori-Graph. These rules will help service providers to predict, which\nweb pages, the user is likely to visit next. This will optimize the website in\nterms of efficiency, bandwidth and will have positive economic benefits for\nthem. The proposed Apriori Graph Algorithm O((V)(E)) works faster compared to\nthe existing Apriori Algorithm and is well suitable for real-time application.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 23:15:20 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Yuvraj", "Pritish", ""], ["R", "Suneetha K.", ""]]}, {"id": "1804.10726", "submitter": "Zang Xinshi", "authors": "Xinshi Zang, Peiwen Hao, Xiaofeng Gao, Bin Yao, Guihai Chen", "title": "QDR-Tree: An Efcient Index Scheme for Complex Spatial Keyword Query", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of mobile devices and the development of geo-positioning\ntechnology, location-based services (LBS) attract much attention and top-k\nspatial keyword queries become increasingly complex. It is common to see that\nclients issue a query to find a restaurant serving pizza and steak, low in\nprice and noise level particularly. However, most of prior works focused only\non the spatial keyword while ignoring these independent numerical attributes.\nIn this paper we demonstrate, for the first time, the Attributes-Aware Spatial\nKeyword Query (ASKQ), and devise a two-layer hybrid index structure called\nQuad-cluster Dual-filtering R-Tree (QDR-Tree). In the keyword cluster layer, a\nQuad-Cluster Tree (QC-Tree) is built based on the hierarchical clustering\nalgorithm using kernel k-means to classify keywords. In the spatial layer, for\neach leaf node of the QC-Tree, we attach a Dual-Filtering R-Tree (DR-Tree) with\ntwo filtering algorithms, namely, keyword bitmap-based and attributes\nskyline-based filtering. Accordingly, efficient query processing algorithms are\nproposed. Through theoretical analysis, we have verified the optimization both\nin processing time and space consumption. Finally, massive experiments with\nreal-data demonstrate the efficiency and effectiveness of QDR-Tree.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 02:58:00 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Zang", "Xinshi", ""], ["Hao", "Peiwen", ""], ["Gao", "Xiaofeng", ""], ["Yao", "Bin", ""], ["Chen", "Guihai", ""]]}, {"id": "1804.10949", "submitter": "Giulio Ermanno Pibiri", "authors": "Giulio Ermanno Pibiri and Rossano Venturini", "title": "On Optimally Partitioning Variable-Byte Codes", "comments": "Published in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE), 15 April 2019", "journal-ref": null, "doi": "10.1109/TKDE.2019.2911288", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquitous Variable-Byte encoding is one of the fastest compressed\nrepresentation for integer sequences. However, its compression ratio is usually\nnot competitive with other more sophisticated encoders, especially when the\nintegers to be compressed are small that is the typical case for inverted\nindexes. This paper shows that the compression ratio of Variable-Byte can be\nimproved by 2x by adopting a partitioned representation of the inverted lists.\nThis makes Variable-Byte surprisingly competitive in space with the best\nbit-aligned encoders, hence disproving the folklore belief that Variable-Byte\nis space-inefficient for inverted index compression. Despite the significant\nspace savings, we show that our optimization almost comes for free, given that:\nwe introduce an optimal partitioning algorithm that does not affect indexing\ntime because of its linear-time complexity; we show that the query processing\nspeed of Variable-Byte is preserved, with an extensive experimental analysis\nand comparison with several other state-of-the-art encoders.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 14:57:53 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 09:09:28 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Pibiri", "Giulio Ermanno", ""], ["Venturini", "Rossano", ""]]}, {"id": "1804.10990", "submitter": "Abolfazl Asudeh", "authors": "Abolfazl Asudeh and H. V. Jagadish and Gerome Miklau and Julia\n  Stoyanovich", "title": "On Obtaining Stable Rankings", "comments": null, "journal-ref": "Abolfazl Asudeh, H. V. Jagadish, Gerome Miklau, Julia Stoyanovich.\n  On Obtaining Stable Rankings. PVLDB , 12(3): 237-250, 2018", "doi": "10.14778/3291264.3291269", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making is challenging when there is more than one criterion to\nconsider. In such cases, it is common to assign a goodness score to each item\nas a weighted sum of its attribute values and rank them accordingly. Clearly,\nthe ranking obtained depends on the weights used for this summation. Ideally,\none would want the ranked order not to change if the weights are changed\nslightly. We call this property {\\em stability} of the ranking. A consumer of a\nranked list may trust the ranking more if it has high stability. A producer of\na ranked list prefers to choose weights that result in a stable ranking, both\nto earn the trust of potential consumers and because a stable ranking is\nintrinsically likely to be more meaningful. In this paper, we develop a\nframework that can be used to assess the stability of a provided ranking and to\nobtain a stable ranking within an \"acceptable\" range of weight values (called\n\"the region of interest\"). We address the case where the user cares about the\nrank order of the entire set of items, and also the case where the user cares\nonly about the top-$k$ items. Using a geometric interpretation, we propose\nalgorithms that produce stable rankings. In addition to theoretical analyses,\nwe conduct extensive experiments on real datasets that validate our proposal.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 20:59:09 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 00:36:38 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 23:39:04 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Asudeh", "Abolfazl", ""], ["Jagadish", "H. V.", ""], ["Miklau", "Gerome", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "1804.11052", "submitter": "Iovka Boneva", "authors": "Iovka Boneva and Jose Lozano and S{\\l}awek Staworko", "title": "Relational to RDF Data Exchange in Presence of a Shape Expression Schema", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the relational to RDF data exchange problem, where the tar- get\nconstraints are specified using Shape Expression schema (ShEx). We investi-\ngate two fundamental problems: 1) consistency which is checking for a given\ndata exchange setting whether there always exists a solution for any source\ninstance, and 2) constructing a universal solution which is a solution that\nrepresents the space of all solutions. We propose to use typed IRI constructors\nin source-to- target tuple generating dependencies to create the IRIs of the\nRDF graph from the values in the relational instance, and we translate ShEx\ninto a set of target dependencies. We also identify data exchange settings that\nare key covered, a property that is decidable and guarantees consistency.\nFurthermore, we show that this property is a sufficient and necessary condition\nfor the existence of universal solutions for a practical subclass of\nweakly-recursive ShEx.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 05:45:39 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Boneva", "Iovka", ""], ["Lozano", "Jose", ""], ["Staworko", "S\u0142awek", ""]]}]