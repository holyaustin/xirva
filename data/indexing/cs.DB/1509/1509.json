[{"id": "1509.00100", "submitter": "Babak Bagheri Hariri", "authors": "Babak Bagheri Hariri, Val Tannen", "title": "Decidability of Equivalence of Aggregate Count-Distinct Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of equivalence of count-distinct aggregate queries,\nprove that the problem is decidable, and can be decided in the third level of\nPolynomial hierarchy. We introduce the notion of core for conjunctive queries\nwith comparisons as an extension of the classical notion for relational\nqueries, and prove that the existence of isomorphism among cores of queries is\na sufficient and necessary condition for equivalence of conjunctive queries\nwith comparisons similar to the classical relational setting. However, it is\nnot a necessary condition for equivalence of count-distinct queries. We\nintroduce a relaxation of this condition based on a new notion, which is a\npotentially new query equivalent to the initial query, introduced to capture\nthe behavior of count-distinct operator.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 00:15:42 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Hariri", "Babak Bagheri", ""], ["Tannen", "Val", ""]]}, {"id": "1509.00104", "submitter": "Wenqiang Liu", "authors": "Wenqiang Liu", "title": "Truth Discovery to Resolve Object Conflicts in Linked Data", "comments": "Have many crucial faults in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the community of Linked Data, anyone can publish their data as Linked Data\non the web because of the openness of the Semantic Web. As such, RDF (Resource\nDescription Framework) triples described the same real-world entity can be\nobtained from multiple sources; it inevitably results in conflicting objects\nfor a certain predicate of a real-world entity. The objective of this study is\nto identify one truth from multiple conflicting objects for a certain predicate\nof a real-world entity. An intuitive principle based on common sense is that an\nobject from a reliable source is trustworthy; thus, a source that provide\ntrustworthy object is reliable. Many truth discovery methods based on this\nprinciple have been proposed to estimate source reliability and identify the\ntruth. However, the effectiveness of existing truth discovery methods is\nsignificantly affected by the number of objects provided by each source.\nTherefore, these methods cannot be trivially extended to resolve conflicts in\nLinked Data with a scale-free property, i.e., most of the sources provide few\nconflicting objects, whereas only a few sources have many conflicting objects.\nTo address this challenge, we propose a novel approach called TruthDiscover to\nidentify the truth in Linked Data with a scale-free property. Two strategies\nare adopted in TruthDiscover to reduce the effect of the scale-free property on\ntruth discovery. First, this approach leverages the topological properties of\nthe Source Belief Graph to estimate the priori beliefs of sources, which are\nutilized to smooth the trustworthiness of sources. Second, this approach\nutilizes the Hidden Markov Random Field to model the interdependencies between\nobjects to estimate the trust values of objects accurately. Experiments are\nconducted in the six datasets to evaluate TruthDiscover.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 00:58:16 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 00:40:30 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2015 00:56:14 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2015 12:00:26 GMT"}, {"version": "v5", "created": "Sat, 28 Nov 2015 09:38:52 GMT"}, {"version": "v6", "created": "Tue, 8 Mar 2016 02:10:02 GMT"}, {"version": "v7", "created": "Wed, 22 Feb 2017 21:34:06 GMT"}, {"version": "v8", "created": "Fri, 21 Apr 2017 22:46:34 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Liu", "Wenqiang", ""]]}, {"id": "1509.00690", "submitter": "Zahid Ansari", "authors": "Zahid Ansari, M.F.Azeem, A. Vinaya Babu and Waseem Ahmed", "title": "A Fuzzy Approach for Feature Evaluation and Dimensionality Reduction to\n  Improve the Quality of Web Usage Mining Results", "comments": null, "journal-ref": "International Journal on Advanced Science Engineering and\n  Information Technology, pp. 67-73 Vol. 2 No. 6, 2012. (ISSN: 2088-5334,\n  INSIGHT Publishers, Indonesia)", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web Usage Mining is the application of data mining techniques to web usage\nlog repositories in order to discover the usage patterns that can be used to\nanalyze the users navigational behavior. During the preprocessing stage, raw\nweb log data is transformed into a set of user profiles. Each user profile\ncaptures a set of URLs representing a user session. Clustering can be applied\nto this sessionized data in order to capture similar interests and trends among\nusers navigational patterns. Since the sessionized data may contain thousands\nof user sessions and each user session may consist of hundreds of URL accesses,\ndimensionality reduction is achieved by eliminating the low support URLs. Very\nsmall sessions are also removed in order to filter out the noise from the data.\nBut direct elimination of low support URLs and small sized sessions may results\nin loss of a significant amount of information especially when the count of low\nsupport URLs and small sessions is large. We propose a fuzzy solution to deal\nwith this problem by assigning weights to URLs and user sessions based on a\nfuzzy membership function. After assigning the weights we apply a Fuzzy c-Mean\nClustering algorithm to discover the clusters of user profiles. In this paper,\nwe describe our fuzzy set theoretic approach to perform feature selection (or\ndimensionality reduction) and session weight assignment. Finally we compare our\nsoft computing based approach of dimensionality reduction with the traditional\napproach of direct elimination of small sessions and low support count URLs.\nOur results show that fuzzy feature evaluation and dimensionality reduction\nresults in better performance and validity indices for the discovered clusters.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:56:02 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Ansari", "Zahid", ""], ["Azeem", "M. F.", ""], ["Babu", "A. Vinaya", ""], ["Ahmed", "Waseem", ""]]}, {"id": "1509.00692", "submitter": "Zahid Ansari", "authors": "Zahid Ansari, Waseem Ahmed, M.F. Azeem and A.Vinaya Babu", "title": "Discovery of Web Usage Profiles Using Various Clustering Techniques", "comments": "arXiv admin note: substantial text overlap with arXiv:1507.03340", "journal-ref": "International Journal of Computer Information Systems, pp. 18-27\n  Vol. 1, No. 3, July 2011. (ISSN 2229-5208, Silicon Valley Publishers, United\n  Kingdom)", "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth of World Wide Web (WWW) has necessitated the development\nof Web personalization systems in order to understand the user preferences to\ndynamically serve customized content to individual users. To reveal information\nabout user preferences from Web usage data, Web Usage Mining (WUM) techniques\nare extensively being applied to the Web log data. Clustering techniques are\nwidely used in WUM to capture similar interests and trends among users\naccessing a Web site. Clustering aims to divide a data set into groups or\nclusters where inter-cluster similarities are minimized while the intra cluster\nsimilarities are maximized. This paper reviews four of the popularly used\nclustering techniques: k-Means, k-Medoids, Leader and DBSCAN. These techniques\nare implemented and tested against the Web user navigational data. Performance\nand validity results of each technique are presented and compared.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:31:37 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Ansari", "Zahid", ""], ["Ahmed", "Waseem", ""], ["Azeem", "M. F.", ""], ["Babu", "A. Vinaya", ""]]}, {"id": "1509.00693", "submitter": "Zahid Ansari", "authors": "Zahid Ansari, Mohammad Fazle Azeem, A. Vinaya Babu and Waseem Ahmed", "title": "A Fuzzy Clustering Based Approach for Mining Usage Profiles from Web Log\n  Data", "comments": null, "journal-ref": "International Journal of Computer Science and Information\n  Security, pp. 70-79 Vol. 9, No. 6, June 2011. (ISSN 1947-5500, IJCSIS\n  Publications, United State)", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web continues to grow at an amazing rate in both the size and\ncomplexity of Web sites and is well on its way to being the main reservoir of\ninformation and data. Due to this increase in growth and complexity of WWW, web\nsite publishers are facing increasing difficulty in attracting and retaining\nusers. To design popular and attractive websites publishers must understand\ntheir users needs. Therefore analyzing users behaviour is an important part of\nweb page design. Web Usage Mining (WUM) is the application of data mining\ntechniques to web usage log repositories in order to discover the usage\npatterns that can be used to analyze the users navigational behavior. WUM\ncontains three main steps: preprocessing, knowledge extraction and results\nanalysis. The goal of the preprocessing stage in Web usage mining is to\ntransform the raw web log data into a set of user profiles. Each such profile\ncaptures a sequence or a set of URLs representing a user session.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:13:23 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Ansari", "Zahid", ""], ["Azeem", "Mohammad Fazle", ""], ["Babu", "A. Vinaya", ""], ["Ahmed", "Waseem", ""]]}, {"id": "1509.00910", "submitter": "Ablimit Aji", "authors": "Ablimit Aji, Vo Hoang, Fusheng Wang", "title": "Effective Spatial Data Partitioning for Scalable Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, MapReduce based spatial query systems have emerged as a cost\neffective and scalable solution to large scale spatial data processing and\nanalytics. MapReduce based systems achieve massive scalability by partitioning\nthe data and running query tasks on those partitions in parallel. Therefore,\neffective data partitioning is critical for task parallelization, load\nbalancing, and directly affects system performance. However, several pitfalls\nof spatial data partitioning make this task particularly challenging. First,\ndata skew is very common in spatial applications. To achieve best query\nperformance, data skew need to be reduced. Second, spatial partitioning\napproaches generate boundary objects that cross multiple partitions, and add\nextra query processing overhead. Consequently, boundary objects need to be\nminimized. Third, the high computational complexity of spatial partitioning\nalgorithms combined with massive amounts of data require an efficient approach\nfor partitioning to achieve overall fast query response. In this paper, we\nprovide a systematic evaluation of multiple spatial partitioning methods with a\nset of different partitioning strategies, and study their implications on the\nperformance of MapReduce based spatial queries. We also study sampling based\npartitioning methods and their impact on queries, and propose several MapReduce\nbased high performance spatial partitioning methods. The main objective of our\nwork is to provide a comprehensive guidance for optimal spatial data\npartitioning to support scalable and fast spatial data processing in massively\nparallel data processing frameworks such as MapReduce. The algorithms developed\nin this work are open source and can be easily integrated into different high\nperformance spatial data processing systems.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 01:23:44 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Aji", "Ablimit", ""], ["Hoang", "Vo", ""], ["Wang", "Fusheng", ""]]}, {"id": "1509.01183", "submitter": "Miao Fan", "authors": "Miao Fan, Qiang Zhou, Thomas Fang Zheng and Ralph Grishman", "title": "Parallel Knowledge Embedding with MapReduce on a Multi-core Processor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article firstly attempts to explore parallel algorithms of learning\ndistributed representations for both entities and relations in large-scale\nknowledge repositories with {\\it MapReduce} programming model on a multi-core\nprocessor. We accelerate the training progress of a canonical knowledge\nembedding method, i.e. {\\it translating embedding} ({\\bf TransE}) model, by\ndividing a whole knowledge repository into several balanced subsets, and\nfeeding each subset into an individual core where local embeddings can\nconcurrently run updating during the {\\it Map} phase. However, it usually\nsuffers from inconsistent low-dimensional vector representations of the same\nkey, which are collected from different {\\it Map} workers, and further leads to\nconflicts when conducting {\\it Reduce} to merge the various vectors associated\nwith the same key. Therefore, we try several strategies to acquire the merged\nembeddings which may not only retain the performance of {\\it entity inference},\n{\\it relation prediction}, and even {\\it triplet classification} evaluated by\nthe single-thread {\\bf TransE} on several well-known knowledge bases such as\nFreebase and NELL, but also scale up the learning speed along with the number\nof cores within a processor. So far, the empirical studies show that we could\nachieve comparable results as the single-thread {\\bf TransE} performs by the\n{\\it stochastic gradient descend} (SGD) algorithm, as well as increase the\ntraining speed multiple times via adapting the {\\it batch gradient descend}\n(BGD) algorithm for {\\it MapReduce} paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 17:53:24 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Fan", "Miao", ""], ["Zhou", "Qiang", ""], ["Zheng", "Thomas Fang", ""], ["Grishman", "Ralph", ""]]}, {"id": "1509.01881", "submitter": "Camila Ferreira Costa", "authors": "Camila F. Costa, Mario A. Nascimento, Jose A. F. Macedo, Yannis\n  Theodoridis, Nikos Pelekis and Javam Machado", "title": "Optimal Time-dependent Sequenced Route Queries in Road Networks", "comments": "10 pages, 12 figures To be published as a short paper in the 23rd ACM\n  SIGSPATIAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an algorithm for optimal processing of\ntime-dependent sequenced route queries in road networks, i.e., given a road\nnetwork where the travel time over an edge is time-dependent and a given\nordered list of categories of interest, we find the fastest route between an\norigin and destination that passes through a sequence of points of interest\nbelonging to each of the specified categories of interest. For instance,\nconsidering a city road network at a given departure time, one can find the\nfastest route between one's work and his/her home, passing through a bank, a\nsupermarket and a restaurant, in this order. The main contribution of our work\nis the consideration of the time dependency of the network, a realistic\ncharacteristic of urban road networks, which has not been considered previously\nwhen addressing the optimal sequenced route query. Our approach uses the A*\nsearch paradigm that is equipped with an admissible heuristic function, thus\nguaranteed to yield the optimal solution, along with a pruning scheme for\nfurther reducing the search space. In order to compare our proposal we extended\na previously proposed solution aimed at non-time dependent sequenced route\nqueries, enabling it to deal with the time-dependency. Our experiments using\nreal and synthetic data sets have shown our proposed solution to be up to two\norders of magnitude faster than the temporally extended previous solution.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 01:32:59 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Costa", "Camila F.", ""], ["Nascimento", "Mario A.", ""], ["Macedo", "Jose A. F.", ""], ["Theodoridis", "Yannis", ""], ["Pelekis", "Nikos", ""], ["Machado", "Javam", ""]]}, {"id": "1509.02822", "submitter": "Vinh Nguyen", "authors": "Gang Fu, Evan Bolton, N\\'uria Queralt Rosinach, Laura I. Furlong, Vinh\n  Nguyen, Amit Sheth, Olivier Bodenreider, Michel Dumontier", "title": "Exposing Provenance Metadata Using Different RDF Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard model for exposing structured provenance metadata of scientific\nassertions on the Semantic Web would increase interoperability,\ndiscoverability, reliability, as well as reproducibility for scientific\ndiscourse and evidence-based knowledge discovery. Several Resource Description\nFramework (RDF) models have been proposed to track provenance. However,\nprovenance metadata may not only be verbose, but also significantly redundant.\nTherefore, an appropriate RDF provenance model should be efficient for\npublishing, querying, and reasoning over Linked Data. In the present work, we\nhave collected millions of pairwise relations between chemicals, genes, and\ndiseases from multiple data sources, and demonstrated the extent of redundancy\nof provenance information in the life science domain. We also evaluated the\nsuitability of several RDF provenance models for this crowdsourced data set,\nincluding the N-ary model, the Singleton Property model, and the\nNanopublication model. We examined query performance against three commonly\nused large RDF stores, including Virtuoso, Stardog, and Blazegraph. Our\nexperiments demonstrate that query performance depends on both RDF store as\nwell as the RDF provenance model.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 15:59:03 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Fu", "Gang", ""], ["Bolton", "Evan", ""], ["Rosinach", "N\u00faria Queralt", ""], ["Furlong", "Laura I.", ""], ["Nguyen", "Vinh", ""], ["Sheth", "Amit", ""], ["Bodenreider", "Olivier", ""], ["Dumontier", "Michel", ""]]}, {"id": "1509.03016", "submitter": "EPTCS", "authors": "Dilian Gurov (KTH Royal Institute of Technology, Stockholm, Sweden),\n  Minko Markov (St. Kliment Ohridski University of Sofia, Sofia, Bulgaria)", "title": "Self-Correlation and Maximum Independence in Finite Relations", "comments": "In Proceedings FICS 2015, arXiv:1509.02826", "journal-ref": "EPTCS 191, 2015, pp. 60-74", "doi": "10.4204/EPTCS.191.7", "report-no": null, "categories": "cs.DM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider relations with no order on their attributes as in Database\nTheory. An independent partition of the set of attributes S of a finite\nrelation R is any partition X of S such that the join of the projections of R\nover the elements of X yields R. Identifying independent partitions has many\napplications and corresponds conceptually to revealing orthogonality between\nsets of dimensions in multidimensional point spaces. A subset of S is termed\nself-correlated if there is a value of each of its attributes such that no\ntuple of R contains all those values. This paper uncovers a connection between\nindependence and self-correlation, showing that the maximum independent\npartition is the least fixed point of a certain inflationary transformer alpha\nthat operates on the finite lattice of partitions of S. alpha is defined via\nthe minimal self-correlated subsets of S. We use some additional properties of\nalpha to show the said fixed point is still the limit of the standard\napproximation sequence, just as in Kleene's well-known fixed point theorem for\ncontinuous functions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 05:31:46 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Gurov", "Dilian", "", "KTH Royal Institute of Technology, Stockholm, Sweden"], ["Markov", "Minko", "", "St. Kliment Ohridski University of Sofia, Sofia, Bulgaria"]]}, {"id": "1509.03045", "submitter": "Bj{\\o}rn Magnus Mathisen", "authors": "Bj{\\o}rn Magnus Mathisen and Leendert Wienhofen and Dumitru Roman", "title": "Empirical Big Data Research: A Systematic Literature Mapping", "comments": "Submitted to Springer journal Data Science and Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: Big Data is a relatively new field of research and technology,\nand literature reports a wide variety of concepts labeled with Big Data. The\nmaturity of a research field can be measured in the number of publications\ncontaining empirical results. In this paper we present the current status of\nempirical research in Big Data. Method: We employed a systematic mapping method\nwith which we mapped the collected research according to the labels Variety,\nVolume and Velocity. In addition, we addressed the application areas of Big\nData. Results: We found that 151 of the assessed 1778 contributions contain a\nform of empirical result and can be mapped to one or more of the 3 V's and 59\naddress an application area. Conclusions: The share of publications containing\nempirical results is well below the average compared to computer science\nresearch as a whole. In order to mature the research on Big Data, we recommend\napplying empirical methods to strengthen the confidence in the reported\nresults. Based on our trend analysis we consider Volume and Variety to be the\nmost promising uncharted area in Big Data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 07:45:52 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 07:13:47 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Mathisen", "Bj\u00f8rn Magnus", ""], ["Wienhofen", "Leendert", ""], ["Roman", "Dumitru", ""]]}, {"id": "1509.03302", "submitter": "Matt Barnes", "authors": "Matt Barnes, Kyle Miller, Artur Dubrawski", "title": "Performance Bounds for Pairwise Entity Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One significant challenge to scaling entity resolution algorithms to massive\ndatasets is understanding how performance changes after moving beyond the realm\nof small, manually labeled reference datasets. Unlike traditional machine\nlearning tasks, when an entity resolution algorithm performs well on small\nhold-out datasets, there is no guarantee this performance holds on larger\nhold-out datasets. We prove simple bounding properties between the performance\nof a match function on a small validation set and the performance of a pairwise\nentity resolution algorithm on arbitrarily sized datasets. Thus, our approach\nenables optimization of pairwise entity resolution algorithms for large\ndatasets, using a small set of labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 19:58:44 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Barnes", "Matt", ""], ["Miller", "Kyle", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1509.04238", "submitter": "Matt Barnes", "authors": "Matt Barnes", "title": "A Practioner's Guide to Evaluating Entity Resolution Results", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is the task of identifying records belonging to the\nsame entity (e.g. individual, group) across one or multiple databases.\nIronically, it has multiple names: deduplication and record linkage, among\nothers. In this paper we survey metrics used to evaluate ER results in order to\niteratively improve performance and guarantee sufficient quality prior to\ndeployment. Some of these metrics are borrowed from multi-class classification\nand clustering domains, though some key differences exist differentiating\nentity resolution from general clustering. Menestrina et al. empirically showed\nrankings from these metrics often conflict with each other, thus our primary\nmotivation for studying them. This paper provides practitioners the basic\nknowledge to begin evaluating their entity resolution results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 18:57:02 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Barnes", "Matt", ""]]}, {"id": "1509.04349", "submitter": "Niranjan Kamat", "authors": "Niranjan Kamat, Arnab Nandi", "title": "A Closer Look at Variance Implementations in Modern Database Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance is a popular and often necessary component of sampled aggregation\nqueries. It is typically used as a secondary measure to ascertain statistical\nproperties of the result such as its error. Yet, it is more expensive to\ncompute than simple, primary measures such as \\texttt{SUM}, \\texttt{MEAN}, and\n\\texttt{COUNT}.\n  There exist numerous techniques to compute variance. While the definition of\nvariance is considered to require multiple passes on the data, other\nmathematical representations can compute the value in a single pass. Some\nsingle-pass representations, however, can suffer from severe precision loss,\nespecially for large number of data points.\n  In this paper, we study variance implementations in various real-world\nsystems and find that major database systems such as PostgreSQL 9.4 and most\nlikely System X, a major commercially used closed-source database, use a\nrepresentation that is efficient, but suffers from floating point precision\nloss resulting from catastrophic cancellation. We note deficiencies in another\npopular representation, used by databases such as MySQL and Impala, that\nsuffers from not being distributive and therefore cannot take advantage of\nmodern parallel computational resources. We review literature over the past\nfive decades on variance calculation in both the statistics and database\ncommunities, and summarize recommendations on implementing variance functions\nin various settings, such as approximate query processing and large-scale\ndistributed aggregation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 23:00:28 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 22:32:53 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Kamat", "Niranjan", ""], ["Nandi", "Arnab", ""]]}, {"id": "1509.04513", "submitter": "Vinh Nguyen", "authors": "Vinh Nguyen, Olivier Bodenreider, Krishnaprasad Thirunarayan, Gang Fu,\n  Evan Bolton, N\\'uria Queralt Rosinach, Laura I. Furlong, Michel Dumontier,\n  Amit Sheth", "title": "On Reasoning with RDF Statements about Statements using Singleton\n  Property Triples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Singleton Property (SP) approach has been proposed for representing and\nquerying metadata about RDF triples such as provenance, time, location, and\nevidence. In this approach, one singleton property is created to uniquely\nrepresent a relationship in a particular context, and in general, generates a\nlarge property hierarchy in the schema. It has become the subject of important\nquestions from Semantic Web practitioners. Can an existing reasoner recognize\nthe singleton property triples? And how? If the singleton property triples\ndescribe a data triple, then how can a reasoner infer this data triple from the\nsingleton property triples? Or would the large property hierarchy affect the\nreasoners in some way? We address these questions in this paper and present our\nstudy about the reasoning aspects of the singleton properties. We propose a\nsimple mechanism to enable existing reasoners to recognize the singleton\nproperty triples, as well as to infer the data triples described by the\nsingleton property triples. We evaluate the effect of the singleton property\ntriples in the reasoning processes by comparing the performance on RDF datasets\nwith and without singleton properties. Our evaluation uses as benchmark the\nLUBM datasets and the LUBM-SP datasets derived from LUBM with temporal\ninformation added through singleton properties.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 12:10:37 GMT"}], "update_date": "2016-08-07", "authors_parsed": [["Nguyen", "Vinh", ""], ["Bodenreider", "Olivier", ""], ["Thirunarayan", "Krishnaprasad", ""], ["Fu", "Gang", ""], ["Bolton", "Evan", ""], ["Rosinach", "N\u00faria Queralt", ""], ["Furlong", "Laura I.", ""], ["Dumontier", "Michel", ""], ["Sheth", "Amit", ""]]}, {"id": "1509.05066", "submitter": "Priyank Gupta", "authors": "Priyank Gupta, Nick Koudas, Europa Shang, Ryan Johnson and Calisto\n  Zuzarte", "title": "Processing Analytical Workloads Incrementally", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of large data collections using popular machine learning and\nstatistical algorithms has been a topic of increasing research interest. A\ntypical analysis workload consists of applying an algorithm to build a model on\na data collection and subsequently refining it based on the results.\n  In this paper we introduce model materialization and incremental model reuse\nas first class citizens in the execution of analysis workloads. We materialize\nbuilt models instead of discarding them in a way that can be reused in\nsubsequent computations. At the same time we consider manipulating an existing\nmodel (adding or deleting data from it) in order to build a new one. We discuss\nour approach in the context of popular machine learning models. We specify the\ndetails of how to incrementally maintain models as well as outline the suitable\noptimizations required to optimally use models and their incremental\nadjustments to build new ones. We detail our techniques for linear regression,\nnaive bayes and logistic regression and present the suitable algorithms and\noptimizations to handle these models in our framework.\n  We present the results of a detailed performance evaluation, using real and\nsynthetic data sets. Our experiments analyze the various trade offs inherent in\nour approach and demonstrate vast performance benefits.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 21:10:12 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Gupta", "Priyank", ""], ["Koudas", "Nick", ""], ["Shang", "Europa", ""], ["Johnson", "Ryan", ""], ["Zuzarte", "Calisto", ""]]}, {"id": "1509.05376", "submitter": "EPTCS", "authors": "Makoto Hamana (Department of Computer Science, Gunma University)", "title": "Iteration Algebras for UnQL Graphs and Completeness for Bisimulation", "comments": "In Proceedings FICS 2015, arXiv:1509.02826", "journal-ref": "EPTCS 191, 2015, pp. 75-89", "doi": "10.4204/EPTCS.191.8", "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows an application of Bloom and Esik's iteration algebras to\nmodel graph data in a graph database query language. About twenty years ago,\nBuneman et al. developed a graph database query language UnQL on the top of a\nfunctional meta-language UnCAL for describing and manipulating graphs.\nRecently, the functional programming community has shown renewed interest in\nUnCAL, because it provides an efficient graph transformation language which is\nuseful for various applications, such as bidirectional computation. However, no\nmathematical semantics of UnQL/UnCAL graphs has been developed. In this paper,\nwe give an equational axiomatisation and algebraic semantics of UnCAL graphs.\nThe main result of this paper is to prove that completeness of our equational\naxioms for UnCAL for the original bisimulation of UnCAL graphs via iteration\nalgebras. Another benefit of algebraic semantics is a clean characterisation of\nstructural recursion on graphs using free iteration algebra.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 19:06:38 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Hamana", "Makoto", "", "Department of Computer Science, Gunma University"]]}, {"id": "1509.05393", "submitter": "Martin Kleppmann", "authors": "Martin Kleppmann", "title": "A Critique of the CAP Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CAP Theorem is a frequently cited impossibility result in distributed\nsystems, especially among NoSQL distributed databases. In this paper we survey\nsome of the confusion about the meaning of CAP, including inconsistencies and\nambiguities in its definitions, and we highlight some problems in its\nformalization. CAP is often interpreted as proof that eventually consistent\ndatabases have better availability properties than strongly consistent\ndatabases; although there is some truth in this, we show that more careful\nreasoning is required. These problems cast doubt on the utility of CAP as a\ntool for reasoning about trade-offs in practical systems. As alternative to\nCAP, we propose a \"delay-sensitivity\" framework, which analyzes the sensitivity\nof operation latency to network delay, and which may help practitioners reason\nabout the trade-offs between consistency guarantees and tolerance of network\nfaults.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 19:50:15 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 17:38:02 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Kleppmann", "Martin", ""]]}, {"id": "1509.05437", "submitter": "Thabet Slimani", "authors": "Thabet Slimani", "title": "Class Association Rules Mining based Rough Set Method", "comments": "10 pages, 2 figures", "journal-ref": "International Journal of Engineering and Technology (IJET), Vol 6\n  No 6 Dec 2014-Jan 2015, ISSN : 0975-4024, PP. 2786-2794", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper investigates the mining of class association rules with rough set\napproach. In data mining, an association occurs between two set of elements\nwhen one element set happen together with another. A class association rule set\n(CARs) is a subset of association rules with classes specified as their\nconsequences. We present an efficient algorithm for mining the finest class\nrule set inspired form Apriori algorithm, where the support and confidence are\ncomputed based on the elementary set of lower approximation included in the\nproperty of rough set theory. Our proposed approach has been shown very\neffective, where the rough set approach for class association discovery is much\nsimpler than the classic association method.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 20:42:19 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Slimani", "Thabet", ""]]}, {"id": "1509.05969", "submitter": "Daniel Haas", "authors": "Daniel Haas, Jiannan Wang, Eugene Wu, Michael J. Franklin", "title": "CLAMShell: Speeding up Crowds for Low-latency Data Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data labeling is a necessary but often slow process that impedes the\ndevelopment of interactive systems for modern data analysis. Despite rising\ndemand for manual data labeling, there is a surprising lack of work addressing\nits high and unpredictable latency. In this paper, we introduce CLAMShell, a\nsystem that speeds up crowds in order to achieve consistently low-latency data\nlabeling. We offer a taxonomy of the sources of labeling latency and study\nseveral large crowd-sourced labeling deployments to understand their empirical\nlatency profiles. Driven by these insights, we comprehensively tackle each\nsource of latency, both by developing novel techniques such as straggler\nmitigation and pool maintenance and by optimizing existing methods such as\ncrowd retainer pools and active learning. We evaluate CLAMShell in simulation\nand on live workers on Amazon's Mechanical Turk, demonstrating that our\ntechniques can provide an order of magnitude speedup and variance reduction\nover existing crowdsourced labeling strategies.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 05:47:32 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Haas", "Daniel", ""], ["Wang", "Jiannan", ""], ["Wu", "Eugene", ""], ["Franklin", "Michael J.", ""]]}, {"id": "1509.06390", "submitter": "Richard Halpert", "authors": "Balder ten Cate (1 and 2), Richard L. Halpert (1), Phokion G. Kolaitis\n  (1 and 3) ((1) University of California Santa Cruz, (2) LogicBlox Inc, (3)\n  IBM Research - Almaden)", "title": "Exchange-Repairs: Managing Inconsistency in Data Exchange", "comments": "29 pages, 13 figures, submitted to the Journal on Data Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a data exchange setting with target constraints, it is often the case that\na given source instance has no solutions. In such cases, the semantics of\ntarget queries trivialize. The aim of this paper is to introduce and explore a\nnew framework that gives meaningful semantics in such cases by using the notion\nof exchange-repairs. Informally, an exchange-repair of a source instance is\nanother source instance that differs minimally from the first, but has a\nsolution. Exchange-repairs give rise to a natural notion of exchange-repair\ncertain answers (XR-certain answers) for target queries. We show that for\nschema mappings specified by source-to-target GAV dependencies and target\nequality-generating dependencies (egds), the XR-certain answers of a target\nconjunctive query can be rewritten as the consistent answers (in the sense of\nstandard database repairs) of a union of conjunctive queries over the source\nschema with respect to a set of egds over the source schema, making it possible\nto use a consistent query-answering system to compute XR-certain answers in\ndata exchange. We then examine the general case of schema mappings specified by\nsource-to-target GLAV constraints, a weakly acyclic set of target tgds and a\nset of target egds. The main result asserts that, for such settings, the\nXR-certain answers of conjunctive queries can be rewritten as the certain\nanswers of a union of conjunctive queries with respect to the stable models of\na disjunctive logic program over a suitable expansion of the source schema.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 20:25:18 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Cate", "Balder ten", "", "1 and 2"], ["Halpert", "Richard L.", "", "1 and 3"], ["Kolaitis", "Phokion G.", "", "1 and 3"]]}, {"id": "1509.07266", "submitter": "Smita Roy", "authors": "Smita Roy, Samrat Mondal and Asif Ekbal", "title": "CRDT: Correlation Ratio Based Decision Tree Model for Healthcare Data\n  Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenal growth in the healthcare data has inspired us in investigating\nrobust and scalable models for data mining. For classification problems\nInformation Gain(IG) based Decision Tree is one of the popular choices.\nHowever, depending upon the nature of the dataset, IG based Decision Tree may\nnot always perform well as it prefers the attribute with more number of\ndistinct values as the splitting attribute. Healthcare datasets generally have\nmany attributes and each attribute generally has many distinct values. In this\npaper, we have tried to focus on this characteristics of the datasets while\nanalysing the performance of our proposed approach which is a variant of\nDecision Tree model and uses the concept of Correlation Ratio(CR). Unlike IG\nbased approach, this CR based approach has no biasness towards the attribute\nwith more number of distinct values. We have applied our model on some\nbenchmark healthcare datasets to show the effectiveness of the proposed\ntechnique.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 07:57:27 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Roy", "Smita", ""], ["Mondal", "Samrat", ""], ["Ekbal", "Asif", ""]]}, {"id": "1509.07445", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Multi-Objective Weighted Sampling", "comments": "14 pages; full version of a HotWeb 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {\\em Multi-objective samples} are powerful and versatile summaries of large\ndata sets. For a set of keys $x\\in X$ and associated values $f_x \\geq 0$, a\nweighted sample taken with respect to $f$ allows us to approximate {\\em\nsegment-sum statistics} $\\text{Sum}(f;H) = \\text{sum}_{x\\in H} f_x$, for any\nsubset $H$ of the keys, with statistically-guaranteed quality that depends on\nsample size and the relative weight of $H$. When estimating $\\text{Sum}(g;H)$\nfor $g\\not=f$, however, quality guarantees are lost. A multi-objective sample\nwith respect to a set of functions $F$ provides for each $f\\in F$ the same\nstatistical guarantees as a dedicated weighted sample while minimizing the\nsummary size.\n  We analyze properties of multi-objective samples and present sampling schemes\nand meta-algortithms for estimation and optimization while showcasing two\nimportant application domains. The first are key-value data sets, where\ndifferent functions $f\\in F$ applied to the values correspond to different\nstatistics such as moments, thresholds, capping, and sum. A multi-objective\nsample allows us to approximate all statistics in $F$. The second is metric\nspaces, where keys are points, and each $f\\in F$ is defined by a set of points\n$C$ with $f_x$ being the service cost of $x$ by $C$, and $\\text{Sum}(f;X)$\nmodels centrality or clustering cost of $C$. A multi-objective sample allows us\nto estimate costs for each $f\\in F$. In these domains, multi-objective samples\nare often of small size, are efficiently to construct, and enable scalable\nestimation and optimization. We aim here to facilitate further applications of\nthis powerful technique.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 17:27:13 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 15:04:46 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2016 16:35:48 GMT"}, {"version": "v4", "created": "Fri, 23 Dec 2016 09:42:08 GMT"}, {"version": "v5", "created": "Sat, 29 Apr 2017 19:18:49 GMT"}, {"version": "v6", "created": "Tue, 13 Jun 2017 09:00:05 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1509.07454", "submitter": "Sanjay Krishnan", "authors": "Sanjay Krishnan, Jiannan Wang, Michael J. Franklin, Ken Goldberg, Tim\n  Kraska", "title": "Stale View Cleaning: Getting Fresh Answers from Stale Materialized Views", "comments": null, "journal-ref": "Proceedings of the VLDB Endowment - Proceedings of the 41st\n  International Conference on Very Large Data Bases, Kohala Coast, Hawaii\n  Volume 8 Issue 12, August 2015 Pages 1370-1381", "doi": "10.14778/2824032.2824037", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Materialized views (MVs), stored pre-computed results, are widely used to\nfacilitate fast queries on large datasets. When new records arrive at a high\nrate, it is infeasible to continuously update (maintain) MVs and a common\nsolution is to defer maintenance by batching updates together. Between batches\nthe MVs become increasingly stale with incorrect, missing, and superfluous rows\nleading to increasingly inaccurate query results. We propose Stale View\nCleaning (SVC) which addresses this problem from a data cleaning perspective.\nIn SVC, we efficiently clean a sample of rows from a stale MV, and use the\nclean sample to estimate aggregate query results. While approximate, the\nestimated query results reflect the most recent data. As sampling can be\nsensitive to long-tailed distributions, we further explore an outlier indexing\ntechnique to give increased accuracy when the data distributions are skewed.\nSVC complements existing deferred maintenance approaches by giving accurate and\nbounded query answers between maintenance. We evaluate our method on a\ngenerated dataset from the TPC-D benchmark and a real video distribution\napplication. Experiments confirm our theoretical results: (1) cleaning an MV\nsample is more efficient than full view maintenance, (2) the estimated results\nare more accurate than using the stale MV, and (3) SVC is applicable for a wide\nvariety of MVs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 18:01:33 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Krishnan", "Sanjay", ""], ["Wang", "Jiannan", ""], ["Franklin", "Michael J.", ""], ["Goldberg", "Ken", ""], ["Kraska", "Tim", ""]]}, {"id": "1509.08035", "submitter": "Sugam Sharma", "authors": "Sugam Sharma", "title": "An Extended Classification and Comparison of NoSQL Big Data Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In last few years, the volume of the data has grown manyfold. The data\nstorages have been inundated by various disparate potential data outlets,\nleading by social media such as Facebook, Twitter, etc. The existing data\nmodels are largely unable to illuminate the full potential of Big Data; the\ninformation that may serve as the key solution to several complex problems is\nleft unexplored. The existing computation capacity falls short for the\nincreasingly expanded storage capacity. The fast-paced volume expansion of the\nunorganized data entails a complete paradigm shift in new age data computation\nand witnesses the evolution of new capable data engineering techniques such as\ncapture, curation, visualization, analyses, etc. In this paper, we provide the\nfirst level classification for modern Big Data models. Some of the leading\nrepresentatives of each classification that claim to best process the Big Data\nin reliable and efficient way are also discussed. Also, the classification is\nfurther strengthened by the intra-class and inter-class comparisons and\ndiscussions of the undertaken Big Data models.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 22:42:52 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 20:33:47 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Sharma", "Sugam", ""]]}, {"id": "1509.08443", "submitter": "Ayush Dubey", "authors": "Ayush Dubey, Greg D. Hill, Robert Escriva, Emin G\\\"un Sirer", "title": "Weaver: A High-Performance, Transactional Graph Database Based on\n  Refinable Timestamps", "comments": null, "journal-ref": null, "doi": "10.14778/2983200.2983202", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph databases have become an increasingly common infrastructure component.\nYet existing systems either operate on offline snapshots, provide weak\nconsistency guarantees, or use expensive concurrency control techniques that\nlimit performance. In this paper, we introduce a new distributed graph\ndatabase, called Weaver, which enables efficient, transactional graph analyses\nas well as strictly serializable ACID transactions on dynamic graphs. The key\ninsight that allows Weaver to combine strict serializability with horizontal\nscalability and high performance is a novel request ordering mechanism called\nrefinable timestamps. This technique couples coarse-grained vector timestamps\nwith a fine-grained timeline oracle to pay the overhead of strong consistency\nonly when needed. Experiments show that Weaver enables a Bitcoin blockchain\nexplorer that is 8x faster than Blockchain.info, and achieves 12x higher\nthroughput than the Titan graph database on social network workloads and 4x\nlower latency than GraphLab on offline graph traversal workloads.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 19:30:30 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 03:41:20 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Dubey", "Ayush", ""], ["Hill", "Greg D.", ""], ["Escriva", "Robert", ""], ["Sirer", "Emin G\u00fcn", ""]]}, {"id": "1509.08608", "submitter": "Sudip Biswas", "authors": "Sharma V. Thankachan, Manish Patil, Rahul Shah, and Sudip Biswas", "title": "Probabilistic Threshold Indexing for Uncertain Strings", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strings form a fundamental data type in computer systems. String searching\nhas been extensively studied since the inception of computer science.\nIncreasingly many applications have to deal with imprecise strings or strings\nwith fuzzy information in them. String matching becomes a probabilistic event\nwhen a string contains uncertainty, i.e. each position of the string can have\ndifferent probable characters with associated probability of occurrence for\neach character. Such uncertain strings are prevalent in various applications\nsuch as biological sequence data, event monitoring and automatic ECG\nannotations. We explore the problem of indexing uncertain strings to support\nefficient string searching. In this paper we consider two basic problems of\nstring searching, namely substring searching and string listing. In substring\nsearching, the task is to find the occurrences of a deterministic string in an\nuncertain string. We formulate the string listing problem for uncertain\nstrings, where the objective is to output all the strings from a collection of\nstrings, that contain probable occurrence of a deterministic query string.\nIndexing solution for both these problems are significantly more challenging\nfor uncertain strings than for deterministic strings. Given a construction time\nprobability value $\\tau$, our indexes can be constructed in linear space and\nsupports queries in near optimal time for arbitrary values of probability\nthreshold parameter greater than $\\tau$. To the best of our knowledge, this is\nthe first indexing solution for searching in uncertain strings that achieves\nstrong theoretical bound and supports arbitrary values of probability threshold\nparameter. We also propose an approximate substring search index that can\nanswer substring search queries with an additive error in optimal time. We\nconduct experiments to evaluate the performance of our indexes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 06:49:32 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Thankachan", "Sharma V.", ""], ["Patil", "Manish", ""], ["Shah", "Rahul", ""], ["Biswas", "Sudip", ""]]}, {"id": "1509.08855", "submitter": "Shantanu Sharma", "authors": "Foto Afrati, Shantanu Sharma, Jeffrey D. Ullman, Jonathan R. Ullman", "title": "Computing Marginals Using MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the data-cube marginals of a fixed order\n$k$ (i.e., all marginals that aggregate over $k$ dimensions), using a single\nround of MapReduce. The focus is on the relationship between the reducer size\n(number of inputs allowed at a single reducer) and the replication rate (number\nof reducers to which an input is sent). We show that the replication rate is\nminimized when the reducers receive all the inputs necessary to compute one\nmarginal of higher order. That observation lets us view the problem as one of\ncovering sets of $k$ dimensions with sets of a larger size $m$, a problem that\nhas been studied under the name \"covering numbers.\" We offer a number of\nconstructions that, for different values of $k$ and $m$ meet or come close to\nyielding the minimum possible replication rate for a given reducer size.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 17:11:30 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Afrati", "Foto", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""], ["Ullman", "Jonathan R.", ""]]}, {"id": "1509.08937", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, Karim Benouaret, Dimitris Sacharidis", "title": "Finding Desirable Objects under Group Categorical Preferences", "comments": "To appear in Knowledge and Information Systems Journal (KAIS),\n  Springer 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering a group of users, each specifying individual preferences over\ncategorical attributes, the problem of determining a set of objects that are\nobjectively preferable by all users is challenging on two levels. First, we\nneed to determine the preferable objects based on the categorical preferences\nfor each user, and second we need to reconcile possible conflicts among users'\npreferences. A naive solution would first assign degrees of match between each\nuser and each object, by taking into account all categorical attributes, and\nthen for each object combine these matching degrees across users to compute the\ntotal score of an object. Such an approach, however, performs two series of\naggregation, among categorical attributes and then across users, which\ncompletely obscure and blur individual preferences. Our solution, instead of\ncombining individual matching degrees, is to directly operate on categorical\nattributes, and define an objective Pareto-based aggregation for group\npreferences. Building on our interpretation, we tackle two distinct but\nrelevant problems: finding the Pareto-optimal objects, and objectively ranking\nobjects with respect to the group preferences. To increase the efficiency when\ndealing with categorical attributes, we introduce an elegant transformation of\ncategorical attribute values into numerical values, which exhibits certain nice\nproperties and allows us to use well-known index structures to accelerate the\nsolutions to the two problems. In fact, experiments on real and synthetic data\nshow that our index-based techniques are an order of magnitude faster than\nbaseline approaches, scaling up to millions of objects and thousands of users.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 20:15:46 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Bikakis", "Nikos", ""], ["Benouaret", "Karim", ""], ["Sacharidis", "Dimitris", ""]]}, {"id": "1509.08960", "submitter": "Udayan Khurana", "authors": "Udayan Khurana and Amol Deshpande", "title": "Storing and Analyzing Historical Graph Data at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work on large-scale graph analytics to date has largely focused on the\nstudy of static properties of graph snapshots. However, a static view of\ninteractions between entities is often an oversimplification of several complex\nphenomena like the spread of epidemics, information diffusion, formation of\nonline communities}, and so on. Being able to find temporal interaction\npatterns, visualize the evolution of graph properties, or even simply compare\nthem across time, adds significant value in reasoning over graphs. However,\nbecause of lack of underlying data management support, an analyst today has to\nmanually navigate the added temporal complexity of dealing with large evolving\ngraphs. In this paper, we present a system, called Historical Graph Store, that\nenables users to store large volumes of historical graph data and to express\nand run complex temporal graph analytical tasks against that data. It consists\nof two key components: a Temporal Graph Index (TGI), that compactly stores\nlarge volumes of historical graph evolution data in a partitioned and\ndistributed fashion; it provides support for retrieving snapshots of the graph\nas of any timepoint in the past or evolution histories of individual nodes or\nneighborhoods; and a Spark-based Temporal Graph Analysis Framework (TAF), for\nexpressing complex temporal analytical tasks and for executing them in an\nefficient and scalable manner. Our experiments demonstrate our system's\nefficient storage, retrieval and analytics across a wide variety of queries on\nlarge volumes of historical graph data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 21:49:58 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Khurana", "Udayan", ""], ["Deshpande", "Amol", ""]]}, {"id": "1509.08979", "submitter": "Diego Calvanese", "authors": "Diego Calvanese, Giuseppe De Giacomo, Maurizio Lenzerini, Moshe Y.\n  Vardi", "title": "Fixpoint Node Selection Query Languages for Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of node selection query languages for (finite) trees has been a\nmajor topic in the recent research on query languages for Web documents. On one\nhand, there has been an extensive study of XPath and its various extensions. On\nthe other hand, query languages based on classical logics, such as first-order\nlogic (FO) or Monadic Second-Order Logic (MSO), have been considered. Results\nin this area typically relate an XPath-based language to a classical logic.\nWhat has yet to emerge is an XPath-related language that is as expressive as\nMSO, and at the same time enjoys the computational properties of XPath, which\nare linear time query evaluation and exponential time query-containment test.\nIn this paper we propose muXPath, which is the alternation-free fragment of\nXPath extended with fixpoint operators. Using two-way alternating automata, we\nshow that this language does combine desired expressiveness and computational\nproperties, placing it as an attractive candidate for the definite\nnode-selection query language for trees.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 00:12:55 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 17:48:36 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2018 05:31:19 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Calvanese", "Diego", ""], ["De Giacomo", "Giuseppe", ""], ["Lenzerini", "Maurizio", ""], ["Vardi", "Moshe Y.", ""]]}]