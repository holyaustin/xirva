[{"id": "2006.00088", "submitter": "Filip Ilievski", "authors": "Filip Ilievski and Daniel Garijo and Hans Chalupsky and Naren Teja\n  Divvala and Yixiang Yao and Craig Rogers and Rongpeng Li and Jun Liu and\n  Amandeep Singh and Daniel Schwabe and Pedro Szekely", "title": "KGTK: A Toolkit for Large Knowledge Graph Manipulation and Analysis", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graphs (KGs) have become the preferred technology for representing,\nsharing and adding knowledge to modern AI applications. While KGs have become a\nmainstream technology, the RDF/SPARQL-centric toolset for operating with them\nat scale is heterogeneous, difficult to integrate and only covers a subset of\nthe operations that are commonly needed in data science applications. In this\npaper we present KGTK, a data science-centric toolkit designed to represent,\ncreate, transform, enhance and analyze KGs. KGTK represents graphs in tables\nand leverages popular libraries developed for data science applications,\nenabling a wide audience of developers to easily construct knowledge graph\npipelines for their applications. We illustrate the framework with real-world\nscenarios where we have used KGTK to integrate and manipulate large KGs, such\nas Wikidata, DBpedia and ConceptNet.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 21:29:14 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 15:14:45 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 15:22:48 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Ilievski", "Filip", ""], ["Garijo", "Daniel", ""], ["Chalupsky", "Hans", ""], ["Divvala", "Naren Teja", ""], ["Yao", "Yixiang", ""], ["Rogers", "Craig", ""], ["Li", "Rongpeng", ""], ["Liu", "Jun", ""], ["Singh", "Amandeep", ""], ["Schwabe", "Daniel", ""], ["Szekely", "Pedro", ""]]}, {"id": "2006.00227", "submitter": "Yang Song", "authors": "Yang Song, Yu Gu, Rui Zhang and Ge Yu", "title": "BrePartition: Optimized High-Dimensional kNN Search with Bregman\n  Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bregman distances (also known as Bregman divergences) are widely used in\nmachine learning, speech recognition and signal processing, and kNN searches\nwith Bregman distances have become increasingly important with the rapid\nadvances of multimedia applications. Data in multimedia applications such as\nimages and videos are commonly transformed into space of hundreds of\ndimensions. Such high-dimensional space has posed significant challenges for\nexisting kNN search algorithms with Bregman distances, which could only handle\ndata of medium dimensionality (typically less than 100). This paper addresses\nthe urgent problem of high-dimensional kNN search with Bregman distances. We\npropose a novel partition-filter-refinement framework. Specifically, we propose\nan optimized dimensionality partitioning scheme to solve several non-trivial\nissues. First, an effective bound from each partitioned subspace to obtain\nexact kNN results is derived. Second, we conduct an in-depth analysis of the\noptimized number of partitions and devise an effective strategy for\npartitioning. Third, we design an efficient integrated index structure for all\nthe subspaces together to accelerate the search processing. Moreover, we extend\nour exact solution to an approximate version by a trade-off between the\naccuracy and efficiency. Experimental results on four real-world datasets and\ntwo synthetic datasets show the clear advantage of our method in comparison to\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 09:32:17 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Song", "Yang", ""], ["Gu", "Yu", ""], ["Zhang", "Rui", ""], ["Yu", "Ge", ""]]}, {"id": "2006.00694", "submitter": "Haozhe Zhang", "authors": "Milos Nikolic, Haozhe Zhang, Ahmet Kara, Dan Olteanu", "title": "F-IVM: Learning over Fast-Evolving Relational Data", "comments": "SIGMOD DEMO 2020, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  F-IVM is a system for real-time analytics such as machine learning\napplications over training datasets defined by queries over fast-evolving\nrelational databases. We will demonstrate F-IVM for three such applications:\nmodel selection, Chow-Liu trees, and ridge linear regression.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 03:36:27 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Nikolic", "Milos", ""], ["Zhang", "Haozhe", ""], ["Kara", "Ahmet", ""], ["Olteanu", "Dan", ""]]}, {"id": "2006.00888", "submitter": "Ursin Brunner", "authors": "Ursin Brunner and Kurt Stockinger", "title": "ValueNet: A Natural Language-to-SQL System that Learns from Database\n  Information", "comments": null, "journal-ref": "37th IEEE International Conference on Data Engineering (ICDE 2021)", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Building natural language (NL) interfaces for databases has been a\nlong-standing challenge for several decades. The major advantage of these\nso-called NL-to-SQL systems is that end-users can query complex databases\nwithout the need to know SQL or the underlying database schema. Due to\nsignificant advancements in machine learning, the recent focus of research has\nbeen on neural networks to tackle this challenge on complex datasets like\nSpider. Several recent NL-to-SQL systems achieve promising results on this\ndataset. However, none of the published systems, that provide either the source\ncode or executable binaries, extract and incorporate values from the user\nquestions for generating SQL statements. Thus, the practical use of these\nsystems in a real-world scenario has not been sufficiently demonstrated yet.\n  In this paper we propose ValueNet light and ValueNet -- two end-to-end\nNL-to-SQL systems that incorporate values using the challenging Spider dataset.\nThe main idea of our approach is to use not only metadata information from the\nunderlying database but also information on the base data as input for our\nneural network architecture. In particular, we propose a novel architecture\nsketch to extract values from a user question and come up with possible value\ncandidates which are not explicitly mentioned in the question. We then use a\nneural model based on an encoder-decoder architecture to synthesize the SQL\nquery. Finally, we evaluate our model on the Spider challenge using the\nExecution Accuracy metric, a more difficult metric than used by most\nparticipants of the challenge. Our experimental evaluation demonstrates that\nValueNet light and ValueNet reach state-of-the-art results of 67% and 62%\naccuracy, respectively, for translating from NL to SQL whilst incorporating\nvalues.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 15:43:39 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 09:31:01 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Brunner", "Ursin", ""], ["Stockinger", "Kurt", ""]]}, {"id": "2006.01060", "submitter": "Kyuhan Lee", "authors": "Kyuhan Lee, Hyeonsoo Jo, Jihoon Ko, Sungsu Lim, Kijung Shin", "title": "SSumM: Sparse Summarization of Massive Graphs", "comments": "to be published in the 26th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining (KDD '20)", "journal-ref": null, "doi": "10.1145/3394486.3403057", "report-no": null, "categories": "cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given a graph G and the desired size k in bits, how can we summarize G within\nk bits, while minimizing the information loss?\n  Large-scale graphs have become omnipresent, posing considerable computational\nchallenges. Analyzing such large graphs can be fast and easy if they are\ncompressed sufficiently to fit in main memory or even cache. Graph\nsummarization, which yields a coarse-grained summary graph with merged nodes,\nstands out with several advantages among graph compression techniques. Thus, a\nnumber of algorithms have been developed for obtaining a concise summary graph\nwith little information loss or equivalently small reconstruction error.\nHowever, the existing methods focus solely on reducing the number of nodes, and\nthey often yield dense summary graphs, failing to achieve better compression\nrates. Moreover, due to their limited scalability, they can be applied only to\nmoderate-size graphs.\n  In this work, we propose SSumM, a scalable and effective graph-summarization\nalgorithm that yields a sparse summary graph. SSumM not only merges nodes\ntogether but also sparsifies the summary graph, and the two strategies are\ncarefully balanced based on the minimum description length principle. Compared\nwith state-of-the-art competitors, SSumM is (a) Concise: yields up to 11.2X\nsmaller summary graphs with similar reconstruction error, (b) Accurate:\nachieves up to 4.2X smaller reconstruction error with similarly concise\noutputs, and (c) Scalable: summarizes 26X larger graphs while exhibiting linear\nscalability. We validate these advantages through extensive experiments on 10\nreal-world graphs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 16:38:19 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 00:49:18 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 04:20:44 GMT"}, {"version": "v4", "created": "Sun, 21 Feb 2021 16:07:19 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lee", "Kyuhan", ""], ["Jo", "Hyeonsoo", ""], ["Ko", "Jihoon", ""], ["Lim", "Sungsu", ""], ["Shin", "Kijung", ""]]}, {"id": "2006.01279", "submitter": "Fubao Wu", "authors": "Fubao Wu, Lixin Gao", "title": "Scalable Top-k Query on Information Networks with Hierarchical\n  Inheritance Relations", "comments": "18 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph query, pattern mining and knowledge discovery become challenging on\nlarge-scale heterogeneous information networks (HINs). State-of-the-art\ntechniques involving path propagation mainly focus on the inference on nodes\nlabels and neighborhood structures. However, entity links in the real world\nalso contain rich hierarchical inheritance relations. For example, the\nvulnerability of a product version is likely to be inherited from its older\nversions. Taking advantage of the hierarchical inheritances can potentially\nimprove the quality of query results. Motivated by this, we explore\nhierarchical inheritance relations between entities and formulate the problem\nof graph query on HINs with hierarchical inheritance relations. We propose a\ngraph query search algorithm by decomposing the original query graph into\nmultiple star queries and apply a star query algorithm to each star query.\nFurther candidates from each star query result are then constructed for final\ntop-k query answers to the original query. To efficiently obtain the graph\nquery result from a large-scale HIN, we design a bound-based pruning technique\nby using uniform cost search to prune search spaces. We implement our algorithm\nin GraphX to test the effectiveness and efficiency on synthetic and real-world\ndatasets. Compared with two common graph query algorithms, our algorithm can\neffectively obtain more accurate results and competitive performances.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 21:35:36 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Wu", "Fubao", ""], ["Gao", "Lixin", ""]]}, {"id": "2006.01294", "submitter": "Fubao Wu", "authors": "Fubao Wu, Han Hee Song, Jiangtao Yin, Lixin Gao, Mario Baldi, Narendra\n  Anand", "title": "NEMA: Automatic Integration of Large Network Management Databases", "comments": "14 pages, 13 Figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network management, whether for malfunction analysis, failure prediction,\nperformance monitoring and improvement, generally involves large amounts of\ndata from different sources. To effectively integrate and manage these sources,\nautomatically finding semantic matches among their schemas or ontologies is\ncrucial. Existing approaches on database matching mainly fall into two\ncategories. One focuses on the schema-level matching based on schema properties\nsuch as field names, data types, constraints and schema structures. Network\nmanagement databases contain massive tables (e.g., network products, incidents,\nsecurity alert and logs) from different departments and groups with nonuniform\nfield names and schema characteristics. It is not reliable to match them by\nthose schema properties. The other category is based on the instance-level\nmatching using general string similarity techniques, which are not applicable\nfor the matching of large network management databases. In this paper, we\ndevelop a matching technique for large NEtwork MAnagement databases (NEMA)\ndeploying instance-level matching for effective data integration and\nconnection. We design matching metrics and scores for both numerical and\nnon-numerical fields and propose algorithms for matching these fields. The\neffectiveness and efficiency of NEMA are evaluated by conducting experiments\nbased on ground truth field pairs in large network management databases. Our\nmeasurement on large databases with 1,458 fields, each of which contains over\n10 million records, reveals that the accuracies of NEMA are up to 95%. It\nachieves 2%-10% higher accuracy and 5x-14x speedup over baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 22:21:40 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Wu", "Fubao", ""], ["Song", "Han Hee", ""], ["Yin", "Jiangtao", ""], ["Gao", "Lixin", ""], ["Baldi", "Mario", ""], ["Anand", "Narendra", ""]]}, {"id": "2006.01994", "submitter": "Chase Smith", "authors": "Chase Smith, Alex Rusnak", "title": "Dynamic Merkle B-tree with Efficient Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and define a recursive Merkle structure with q-mercurial\ncommitments, in order to create a concise B-Merkle tree. This Merkle B-Tree\nbuilds on previous work of q-ary Merkle trees which use concise, constant size,\nq-mercurial commitments for intranode proofs. Although these q-ary trees reduce\nthe branching factor and height, they still have their heights based on the key\nlength, and are forced to fixed heights. Instead of basing nodes on q-ary\nprefix trees, the B Merkle Tree incorporates concise intranode commitments\nwithin a self-balancing tree. The tree is based on the ordering of elements,\nwhich requires extra information to determine element placement, but it enables\nsignificantly smaller proof sizes. This allows for much lower tree heights\n(directed by the order of elements, not the size of the key), and therefore\ncreates smaller and more efficient proofs and operations. Additionally, the B\nMerkle Tree is defined with subset queries that feature similar communication\ncosts to non-membership proofs. Our scheme has the potential to benefit\noutsourced database models, like blockchain, which use authenticated data\nstructures and database indices to ensure immutability and integrity of data.\nWe present potential applications in key-value stores, relational databases,\nand, in part, Merkle forests.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 00:40:37 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 00:08:18 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 01:59:05 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Smith", "Chase", ""], ["Rusnak", "Alex", ""]]}, {"id": "2006.02155", "submitter": "Brian Kroth", "authors": "Carlo Curino, Neha Godwal, Brian Kroth, Sergiy Kuryata, Greg Lapinski,\n  Siqi Liu, Slava Oks, Olga Poppe, Adam Smiechowski, Ed Thayer, Markus Weimer,\n  Yiwen Zhu", "title": "MLOS: An Infrastructure for Automated Software Performance Engineering", "comments": "4 pages, DEEM 2020", "journal-ref": null, "doi": "10.1145/3399579.3399927", "report-no": null, "categories": "cs.DC cs.DB cs.LG cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing modern systems software is a complex task that combines business\nlogic programming and Software Performance Engineering (SPE). The later is an\nexperimental and labor-intensive activity focused on optimizing the system for\na given hardware, software, and workload (hw/sw/wl) context.\n  Today's SPE is performed during build/release phases by specialized teams,\nand cursed by: 1) lack of standardized and automated tools, 2) significant\nrepeated work as hw/sw/wl context changes, 3) fragility induced by a\n\"one-size-fit-all\" tuning (where improvements on one workload or component may\nimpact others). The net result: despite costly investments, system software is\noften outside its optimal operating point - anecdotally leaving 30% to 40% of\nperformance on the table.\n  The recent developments in Data Science (DS) hints at an opportunity:\ncombining DS tooling and methodologies with a new developer experience to\ntransform the practice of SPE. In this paper we present: MLOS, an ML-powered\ninfrastructure and methodology to democratize and automate Software Performance\nEngineering. MLOS enables continuous, instance-level, robust, and trackable\nsystems optimization. MLOS is being developed and employed within Microsoft to\noptimize SQL Server performance. Early results indicated that component-level\noptimizations can lead to 20%-90% improvements when custom-tuning for a\nspecific hw/sw/wl, hinting at a significant opportunity. However, several\nresearch challenges remain that will require community involvement. To this\nend, we are in the process of open-sourcing the MLOS core infrastructure, and\nwe are engaging with academic institutions to create an educational program\naround Software 2.0 and MLOS ideas.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 22:38:30 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 11:10:53 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Curino", "Carlo", ""], ["Godwal", "Neha", ""], ["Kroth", "Brian", ""], ["Kuryata", "Sergiy", ""], ["Lapinski", "Greg", ""], ["Liu", "Siqi", ""], ["Oks", "Slava", ""], ["Poppe", "Olga", ""], ["Smiechowski", "Adam", ""], ["Thayer", "Ed", ""], ["Weimer", "Markus", ""], ["Zhu", "Yiwen", ""]]}, {"id": "2006.02643", "submitter": "Lele Wang", "authors": "Alankrita Bhatt, Ziao Wang, Chi Wang, Lele Wang", "title": "Universal Graph Compression: Stochastic Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the prevalent data science applications of processing and mining\nlarge-scale graph data such as social networks, web graphs, and biological\nnetworks, as well as the high I/O and communication costs of storing and\ntransmitting such data, this paper investigates lossless compression of data\nappearing in the form of a labeled graph. A universal graph compression scheme\nis proposed, which does not depend on the underlying statistics/distribution of\nthe graph model. For graphs generated by a stochastic block model, which is a\nwidely used random graph model capturing the clustering effects in social\nnetworks, the proposed scheme achieves the optimal theoretical limit of\nlossless compression without the need to know edge probabilities, community\nlabels, or the number of communities.\n  The key ideas in establishing universality for stochastic block models\ninclude: 1) block decomposition of the adjacency matrix of the graph; 2)\ngeneralization of the Krichevsky-Trofimov probability assignment, which was\ninitially designed for i.i.d. random processes. In four benchmark graph\ndatasets (protein-to-protein interaction, LiveJournal friendship, Flickr, and\nYouTube), the compressed files from competing algorithms (including CSR,\nLigra+, PNG image compressor, and Lempel-Ziv compressor for two-dimensional\ndata) take 2.4 to 27 times the space needed by the proposed scheme.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 04:51:26 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 02:12:38 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bhatt", "Alankrita", ""], ["Wang", "Ziao", ""], ["Wang", "Chi", ""], ["Wang", "Lele", ""]]}, {"id": "2006.02862", "submitter": "Emna Jabri EmnaJ", "authors": "Emna Jabri", "title": "A Novel Approach for Generating SPARQL Queries from RDF Graphs", "comments": "in French. arXiv admin note: substantial text overlap with\n  arXiv:1810.02869 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is done as part of a research master's thesis project. The goal is\nto generate SPARQL queries based on user-supplied keywords to query RDF graphs.\nTo do this, we first transformed the input ontology into an RDF graph that\nreflects the semantics represented in the ontology. Subsequently, we stored\nthis RDF graph in the Neo4j graphical database to ensure efficient and\npersistent management of RDF data. At the time of the interrogation, we studied\nthe different possible and desired interpretations of the request originally\nmade by the user. We have also proposed to carry out a sort of transformation\nbetween the two query languages SPARQL and Cypher, which is specific to Neo4j.\nThis allows us to implement the architecture of our system over a wide variety\nof BD-RDFs providing their query languages, without changing any of the other\ncomponents of the system. Finally, we tested and evaluated our tool using\ndifferent test bases, and it turned out that our tool is comprehensive,\neffective, and powerful enough.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 18:28:49 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Jabri", "Emna", ""]]}, {"id": "2006.02958", "submitter": "Maureen Daum", "authors": "Maureen Daum, Brandon Haynes, Dong He, Amrita Mazumdar, Magdalena\n  Balazinska", "title": "TASM: A Tile-Based Storage Manager for Video Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern video data management systems store videos as a single encoded file,\nwhich significantly limits possible storage level optimizations. We design,\nimplement, and evaluate TASM, a new tile-based storage manager for video data.\nTASM uses a feature in modern video codecs called \"tiles\" that enables spatial\nrandom access into encoded videos. TASM physically tunes stored videos by\noptimizing their tile layouts given the video content and a query workload.\nAdditionally, TASM dynamically tunes that layout in response to changes in the\nquery workload or if the query workload and video contents are incrementally\ndiscovered. Finally, TASM also produces efficient initial tile layouts for\nnewly ingested videos. We demonstrate that TASM can speed up subframe selection\nqueries by an average of over 50% and up to 94%. TASM can also improve the\nthroughput of the full scan phase of object detection queries by up to 2X.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:40:42 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 21:27:57 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 21:06:41 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Daum", "Maureen", ""], ["Haynes", "Brandon", ""], ["He", "Dong", ""], ["Mazumdar", "Amrita", ""], ["Balazinska", "Magdalena", ""]]}, {"id": "2006.02963", "submitter": "Jacob Shermeyer", "authors": "Jacob Shermeyer, Thomas Hossler, Adam Van Etten, Daniel Hogan, Ryan\n  Lewis, Daeil Kim", "title": "RarePlanes: Synthetic Data Takes Flight", "comments": "To appear in WACV 2021 - 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RarePlanes is a unique open-source machine learning dataset that incorporates\nboth real and synthetically generated satellite imagery. The RarePlanes dataset\nspecifically focuses on the value of synthetic data to aid computer vision\nalgorithms in their ability to automatically detect aircraft and their\nattributes in satellite imagery. Although other synthetic/real combination\ndatasets exist, RarePlanes is the largest openly-available very-high resolution\ndataset built to test the value of synthetic data from an overhead perspective.\nPrevious research has shown that synthetic data can reduce the amount of real\ntraining data needed and potentially improve performance for many tasks in the\ncomputer vision domain. The real portion of the dataset consists of 253 Maxar\nWorldView-3 satellite scenes spanning 112 locations and 2,142 km^2 with 14,700\nhand-annotated aircraft. The accompanying synthetic dataset is generated via\nAI.Reverie's simulation platform and features 50,000 synthetic satellite images\nsimulating a total area of 9331.2 km^2 with ~630,000 aircraft annotations. Both\nthe real and synthetically generated aircraft feature 10 fine grain attributes\nincluding: aircraft length, wingspan, wing-shape, wing-position, wingspan\nclass, propulsion, number of engines, number of vertical-stabilizers, presence\nof canards, and aircraft role. Finally, we conduct extensive experiments to\nevaluate the real and synthetic datasets and compare performances. By doing so,\nwe show the value of synthetic data for the task of detecting and classifying\naircraft from an overhead perspective.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:46:43 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 17:17:01 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Shermeyer", "Jacob", ""], ["Hossler", "Thomas", ""], ["Van Etten", "Adam", ""], ["Hogan", "Daniel", ""], ["Lewis", "Ryan", ""], ["Kim", "Daeil", ""]]}, {"id": "2006.03048", "submitter": "Mohamed Attia", "authors": "Islam Samy, Mohamed A. Attia, Ravi Tandon, Loukas Lazos", "title": "Asymmetric Leaky Private Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-theoretic formulations of the private information retrieval (PIR)\nproblem have been investigated under a variety of scenarios. Symmetric private\ninformation retrieval (SPIR) is a variant where a user is able to privately\nretrieve one out of $K$ messages from $N$ non-colluding replicated databases\nwithout learning anything about the remaining $K-1$ messages. However, the goal\nof perfect privacy can be too taxing for certain applications. In this paper,\nwe investigate if the information-theoretic capacity of SPIR (equivalently, the\ninverse of the minimum download cost) can be increased by relaxing both user\nand DB privacy definitions. Such relaxation is relevant in applications where\nprivacy can be traded for communication efficiency. We introduce and\ninvestigate the Asymmetric Leaky PIR (AL-PIR) model with different privacy\nleakage budgets in each direction. For user privacy leakage, we bound the\nprobability ratios between all possible realizations of DB queries by a\nfunction of a non-negative constant $\\epsilon$. For DB privacy, we bound the\nmutual information between the undesired messages, the queries, and the\nanswers, by a function of a non-negative constant $\\delta$. We propose a\ngeneral AL-PIR scheme that achieves an upper bound on the optimal download cost\nfor arbitrary $\\epsilon$ and $\\delta$. We show that the optimal download cost\nof AL-PIR is upper-bounded as $D^{*}(\\epsilon,\\delta)\\leq\n1+\\frac{1}{N-1}-\\frac{\\delta e^{\\epsilon}}{N^{K-1}-1}$. Second, we obtain an\ninformation-theoretic lower bound on the download cost as\n$D^{*}(\\epsilon,\\delta)\\geq\n1+\\frac{1}{Ne^{\\epsilon}-1}-\\frac{\\delta}{(Ne^{\\epsilon})^{K-1}-1}$. The gap\nanalysis between the two bounds shows that our AL-PIR scheme is optimal when\n$\\epsilon =0$, i.e., under perfect user privacy and it is optimal within a\nmaximum multiplicative gap of $\\frac{N-e^{-\\epsilon}}{N-1}$ for any\n$(\\epsilon,\\delta)$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:58:46 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Samy", "Islam", ""], ["Attia", "Mohamed A.", ""], ["Tandon", "Ravi", ""], ["Lazos", "Loukas", ""]]}, {"id": "2006.03156", "submitter": "Simone Parisotto Dr", "authors": "Simone Parisotto and Alessandro Launaro and Ninetta Leone and\n  Carola-Bibiane Sch\\\"onlieb", "title": "Unsupervised clustering of Roman pottery profiles from their SSAE\n  representation", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the ROman COmmonware POTtery (ROCOPOT) database,\nwhich comprises of more than 2000 black and white imaging profiles of pottery\nshapes extracted from 11 Roman catalogues and related to different excavation\nsites. The partiality and the handcrafted variance of the shape fragments\nwithin this new database make their unsupervised clustering a very challenging\nproblem: profile similarities are thus explored via the hierarchical clustering\nof non-linear features learned in the latent representation space of a stacked\nsparse autoencoder (SSAE) network, unveiling new profile matches. Results are\ncommented both from a mathematical and archaeological perspective so as to\nunlock new research directions in the respective communities.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 22:19:22 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Parisotto", "Simone", ""], ["Launaro", "Alessandro", ""], ["Leone", "Ninetta", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2006.03176", "submitter": "Kapil Vaidya", "authors": "Kapil Vaidya, Eric Knorr, Tim Kraska, Michael Mitzenmacher", "title": "Partitioned Learned Bloom Filter", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bloom filters are space-efficient probabilistic data structures that are used\nto test whether an element is a member of a set, and may return false\npositives. Recently, variations referred to as learned Bloom filters were\ndeveloped that can provide improved performance in terms of the rate of false\npositives, by using a learned model for the represented set. However, previous\nmethods for learned Bloom filters do not take full advantage of the learned\nmodel. Here we show how to frame the problem of optimal model utilization as an\noptimization problem, and using our framework derive algorithms that can\nachieve near-optimal performance in many cases. Experimental results from both\nsimulated and real-world datasets show significant performance improvements\nfrom our optimization approach over both the original learned Bloom filter\nconstructions and previously proposed heuristic improvements.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 00:05:32 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 15:15:17 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Vaidya", "Kapil", ""], ["Knorr", "Eric", ""], ["Kraska", "Tim", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "2006.03198", "submitter": "Xiaolong Wan", "authors": "Xiaolong Wan, Hongzhi Wang", "title": "Efficient Semi-External Depth-First Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing Depth-First Search (DFS) results, i.e. depth-first order or\nDFS-Tree, on the semi-external environment becomes a hot topic, because the\nscales of the graphs grow rapidly which can hardly be hold in the main memory,\nin the big data era. Existing semi-external DFS algorithms assume the main\nmemory could, at least, hold a spanning tree T of a graph G, and gradually\nrestructure T into a DFS-Tree, which is non-trivial. In this paper, we present\na comprehensive study of semi-external DFS problem, including the first\ntheoretical analysis of the main challenge of this problem, as far as we know.\nBesides, we introduce a new semi-external DFS algorithm with an efficient edge\npruning principle, named EP-DFS. Unlike the traditional algorithms, we not only\nfocus on addressing such complex problem efficiently with less I/Os, but also\nfocus on that with simpler CPU calculation (Implementation-friendly) and less\nrandom I/O access (key-to-efficiency). The former is based on our efficient\npruning principle; the latter is addressed by a lightweight index N+-index,\nwhich is a compressed storage for a subset of the edges for G. The extensive\nexperimental evaluation on both synthetic and real graphs confirms that our\nEP-DFS algorithm outperforms the existing techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 01:55:22 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 01:12:06 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wan", "Xiaolong", ""], ["Wang", "Hongzhi", ""]]}, {"id": "2006.03206", "submitter": "Chinmay Kulkarni", "authors": "Chinmay Kulkarni, Badrish Chandramouli, Ryan Stutsman", "title": "Achieving High Throughput and Elasticity in a Larger-than-Memory Store", "comments": null, "journal-ref": "PVLDB, 14(8): 1427 - 1440, 2021", "doi": "10.14778/3457390.3457406", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of sensors, mobile applications and machines now generate billions\nof events. Specialized many-core key-value stores (KVSs) can ingest and index\nthese events at high rates (over 100 Mops/s on one machine) if events are\ngenerated on the same machine; however, to be practical and cost-effective they\nmust ingest events over the network and scale across cloud resources\nelastically.\n  We present Shadowfax, a new distributed KVS based on FASTER, that\ntransparently spans DRAM, SSDs, and cloud blob storage while serving 130\nMops/s/VM over commodity Azure VMs using conventional Linux TCP. Beyond high\nsingle-VM performance, Shadowfax uses a unique approach to distributed\nreconfiguration that avoids any server-side key ownership checks or cross-core\ncoordination both during normal operation and migration. Hence, Shadowfax can\nshift load in 17 s to improve system throughput by 10 Mops/s with little\ndisruption. Compared to the state-of-the-art, it has 8x better throughput (than\nSeastar+memcached) and avoids costly I/O to move cold data during migration. On\n12 machines, Shadowfax retains its high throughput to perform 930 Mops/s,\nwhich, to the best of our knowledge, is the highest reported throughput for a\ndistributed KVS used for large-scale data ingestion and indexing.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 02:32:06 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 22:21:59 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Kulkarni", "Chinmay", ""], ["Chandramouli", "Badrish", ""], ["Stutsman", "Ryan", ""]]}, {"id": "2006.04277", "submitter": "Jan Hidders", "authors": "Jan Hidders and Jan Paredaens and Jan Van den Bussche", "title": "J-Logic: a Logic for Querying JSON", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a logical framework, based on Datalog, to study the foundations of\nquerying JSON data. The main feature of our approach, which we call J-Logic, is\nthe emphasis on paths. Paths are sequences of keys and are used to access the\ntree structure of nested JSON objects. J-Logic also features ``packing'' as a\nmeans to generate a new key from a path or subpath. J-Logic with recursion is\ncomputationally complete, but many queries can be expressed without recursion,\nsuch as deep equality. We give a necessary condition for queries to be\nexpressible without recursion. Most of our results focus on the deterministic\nnature of JSON objects as partial functions from keys to values. Predicates\ndefined by J-Logic programs may not properly describe objects, however.\nNevertheless we show that every object-to-object transformation in J-Logic can\nbe defined using only objects in intermediate results. Moreover we show that it\nis decidable whether a positive, nonrecursive J-Logic program always returns an\nobject when given objects as inputs. Regarding packing, we show that packing is\nunnecessary if the output does not require new keys. Finally, we show the\ndecidability of query containment for positive, nonrecursive J-Logic programs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 21:56:29 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Hidders", "Jan", ""], ["Paredaens", "Jan", ""], ["Bussche", "Jan Van den", ""]]}, {"id": "2006.04509", "submitter": "Siddhant Arora", "authors": "Siddhant Arora, Srikanta Bedathur, Maya Ramanath, Deepak Sharma", "title": "IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge", "comments": "16 pages, 7 figures, AKBC 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs (KGs) extracted from text sources are often noisy and lead\nto poor performance in downstream application tasks such as KG-based question\nanswering.While much of the recent activity is focused on addressing the\nsparsity of KGs by using embeddings for inferring new facts, the issue of\ncleaning up of noise in KGs through KG refinement task is not as actively\nstudied. Most successful techniques for KG refinement make use of inference\nrules and reasoning over ontologies. Barring a few exceptions, embeddings do\nnot make use of ontological information, and their performance in KG refinement\ntask is not well understood. In this paper, we present a KG refinement\nframework called IterefinE which iteratively combines the two techniques - one\nwhich uses ontological information and inferences rules, PSL-KGI, and the KG\nembeddings such as ComplEx and ConvE which do not. As a result, IterefinE is\nable to exploit not only the ontological information to improve the quality of\npredictions, but also the power of KG embeddings which (implicitly) perform\nlonger chains of reasoning. The IterefinE framework, operates in a co-training\nmode and results in explicit type-supervised embedding of the refined KG from\nPSL-KGI which we call as TypeE-X. Our experiments over a range of KG benchmarks\nshow that the embeddings that we produce are able to reject noisy facts from KG\nand at the same time infer higher quality new facts resulting in up to 9%\nimprovement of overall weighted F1 score\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 14:05:54 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Arora", "Siddhant", ""], ["Bedathur", "Srikanta", ""], ["Ramanath", "Maya", ""], ["Sharma", "Deepak", ""]]}, {"id": "2006.04556", "submitter": "Ariam Rivas", "authors": "Ariam Rivas, Irl\\'an Grangel-Gonz\\'alez, Diego Collarana, Jens\n  Lehmann, Maria-Esther Vidal", "title": "Unveiling Relations in the Industry 4.0 Standards Landscape based on\n  Knowledge Graph Embeddings", "comments": "15 pages, 7 figures, DEXA2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry~4.0 (I4.0) standards and standardization frameworks have been\nproposed with the goal of \\emph{empowering interoperability} in smart\nfactories. These standards enable the description and interaction of the main\ncomponents, systems, and processes inside of a smart factory. Due to the\ngrowing number of frameworks and standards, there is an increasing need for\napproaches that automatically analyze the landscape of I4.0 standards.\nStandardization frameworks classify standards according to their functions into\nlayers and dimensions. However, similar standards can be classified differently\nacross the frameworks, producing, thus, interoperability conflicts among them.\nSemantic-based approaches that rely on ontologies and knowledge graphs, have\nbeen proposed to represent standards, known relations among them, as well as\ntheir classification according to existing frameworks. Albeit informative, the\nstructured modeling of the I4.0 landscape only provides the foundations for\ndetecting interoperability issues. Thus, graph-based analytical methods able to\nexploit knowledge encoded by these approaches, are required to uncover\nalignments among standards. We study the relatedness among standards and\nframeworks based on community analysis to discover knowledge that helps to cope\nwith interoperability conflicts between standards. We use knowledge graph\nembeddings to automatically create these communities exploiting the meaning of\nthe existing relationships. In particular, we focus on the identification of\nsimilar standards, i.e., communities of standards, and analyze their properties\nto detect unknown relations. We empirically evaluate our approach on a\nknowledge graph of I4.0 standards using the Trans$^*$ family of embedding\nmodels for knowledge graph entities. Our results are promising and suggest that\nrelations among standards can be detected accurately.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 17:37:08 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Rivas", "Ariam", ""], ["Grangel-Gonz\u00e1lez", "Irl\u00e1n", ""], ["Collarana", "Diego", ""], ["Lehmann", "Jens", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "2006.04658", "submitter": "Diego Didona Dr", "authors": "Diego Didona, Nikolas Ioannou, Radu Stoica, Kornilios Kourtis", "title": "Toward a Better Understanding and Evaluation of Tree Structures on Flash\n  SSDs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solid-state drives (SSDs) are extensively used to deploy persistent data\nstores, as they provide low latency random access, high write throughput, high\ndata density, and low cost. Tree-based data structures are widely used to build\npersistent data stores, and indeed they lie at the backbone of many of the data\nmanagement systems used in production and research today. In this paper, we\nshow that benchmarking a persistent tree-based data structure on an SSD is a\ncomplex process, which may easily incur subtle pitfalls that can lead to an\ninaccurate performance assessment. At a high-level, these pitfalls stem from\nthe interaction of complex software running on complex hardware. On one hand,\ntree structures implement internal operations that have nontrivial effects on\nperformance. On the other hand, SSDs employ firmware logic to deal with the\nidiosyncrasies of the underlying flash memory, which are well known to lead to\ncomplex performance dynamics. We identify seven benchmarking pitfalls using\nRocksDB and WiredTiger, two widespread implementations of an LSM-Tree and a\nB+Tree, respectively. We show that such pitfalls can lead to incorrect\nmeasurements of key performance indicators, hinder the reproducibility and the\nrepresentativeness of the results, and lead to suboptimal deployments in\nproduction environments. We also provide guidelines on how to avoid these\npitfalls to obtain more reliable performance measurements, and to perform more\nthorough and fair comparison among different design points.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:03:09 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Didona", "Diego", ""], ["Ioannou", "Nikolas", ""], ["Stoica", "Radu", ""], ["Kourtis", "Kornilios", ""]]}, {"id": "2006.04693", "submitter": "Yang Zhao", "authors": "Leong Mei Han, Yang Zhao, Jun Zhao", "title": "Blockchain-Based Differential Privacy Cost Management System", "comments": "This paper appears in ACM ASIA Conference on Computer and\n  Communications Security (ACMASIACCS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy preservation is a big concern for various sectors. To protect\nindividual user data, one emerging technology is differential privacy. However,\nit still has limitations for datasets with frequent queries, such as the fast\naccumulation of privacy cost. To tackle this limitation, this paper explores\nthe integration of a secured decentralised ledger, blockchain. Blockchain will\nbe able to keep track of all noisy responses generated with differential\nprivacy algorithm and allow for certain queries to reuse old responses. In this\npaper, a demo of a proposed blockchain-based privacy management system is\ndesigned as an interactive decentralised web application (DApp). The demo\ncreated illustrates that leveraging on blockchain will allow the total privacy\ncost accumulated to decrease significantly.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:47:14 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Han", "Leong Mei", ""], ["Zhao", "Yang", ""], ["Zhao", "Jun", ""]]}, {"id": "2006.04777", "submitter": "Subhadeep Sarkar", "authors": "Subhadeep Sarkar, Tarikul Islam Papon, Dimitris Staratzis, Manos\n  Athanassoulis", "title": "Lethe: A Tunable Delete-Aware LSM Engine (Updated Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-intensive applications fueled the evolution of log structured merge\n(LSM) based key-value engines that employ the out-of-place paradigm to support\nhigh ingestion rates with low read/write interference. These benefits, however,\ncome at the cost of treating deletes as a second-class citizen. A delete\ninserts a tombstone that invalidates older instances of the deleted key.\nState-of-the-art LSM engines do not provide guarantees as to how fast a\ntombstone will propagate to persist the deletion. Further, LSM engines only\nsupport deletion on the sort key. To delete on another attribute (e.g.,\ntimestamp), the entire tree is read and re-written. We highlight that fast\npersistent deletion without affecting read performance is key to support: (i)\nstreaming systems operating on a window of data, (ii) privacy with latency\nguarantees on the right-to-be-forgotten, and (iii) en masse cloud deployment of\ndata systems that makes storage a precious resource.\n  To address these challenges, in this paper, we build a new key-value storage\nengine, Lethe, that uses a very small amount of additional metadata, a set of\nnew delete-aware compaction policies, and a new physical data layout that\nweaves the sort and the delete key order. We show that Lethe supports any\nuser-defined threshold for the delete persistence latency offering higher read\nthroughput ($1.17-1.4\\times$) and lower space amplification ($2.1-9.8\\times$),\nwith a modest increase in write amplification (between $4\\%$ and $25\\%$). In\naddition, Lethe supports efficient range deletes on a secondary delete key by\ndropping entire data pages without sacrificing read performance nor employing a\ncostly full tree merge.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 17:52:03 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 01:06:57 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 01:23:44 GMT"}, {"version": "v4", "created": "Sat, 13 Jun 2020 00:01:32 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Sarkar", "Subhadeep", ""], ["Papon", "Tarikul Islam", ""], ["Staratzis", "Dimitris", ""], ["Athanassoulis", "Manos", ""]]}, {"id": "2006.05134", "submitter": "Kevin Wellenzohn", "authors": "Kevin Wellenzohn, Michael H. B\\\"ohlen, Sven Helmer", "title": "Dynamic Interleaving of Content and Structure for Robust Indexing of\n  Semi-Structured Hierarchical Data (Extended Version)", "comments": null, "journal-ref": null, "doi": "10.14778/3401960.3401963", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust index for semi-structured hierarchical data that supports\ncontent-and-structure (CAS) queries specified by path and value predicates. At\nthe heart of our approach is a novel dynamic interleaving scheme that merges\nthe path and value dimensions of composite keys in a balanced way. We store\nthese keys in our trie-based Robust Content-And-Structure index, which\nefficiently supports a wide range of CAS queries, including queries with\nwildcards and descendant axes. Additionally, we show important properties of\nour scheme, such as robustness against varying selectivities, and demonstrate\nimprovements of up to two orders of magnitude over existing approaches in our\nexperimental evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 09:21:18 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wellenzohn", "Kevin", ""], ["B\u00f6hlen", "Michael H.", ""], ["Helmer", "Sven", ""]]}, {"id": "2006.05564", "submitter": "Satoshi Koide", "authors": "Satoshi Koide, Chuan Xiao, Yoshiharu Ishikawa", "title": "Fast Subtrajectory Similarity Search in Road Networks under Weighted\n  Edit Distance Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address a similarity search problem for spatial\ntrajectories in road networks. In particular, we focus on the subtrajectory\nsimilarity search problem, which involves finding in a database the\nsubtrajectories similar to a query trajectory. A key feature of our approach is\nthat we do not focus on a specific similarity function; instead, we consider\nweighted edit distance (WED), a class of similarity functions which allows\nuser-defined cost functions and hence includes several important similarity\nfunctions such as EDR and ERP. We model trajectories as strings, and propose a\ngeneric solution which is able to deal with any similarity function belonging\nto the class of WED. By employing the filter-and-verify strategy, we introduce\nsubsequence filtering to efficiently prunes trajectories and find candidates.\nIn order to choose a proper subsequence to optimize the candidate number, we\nmodel the choice as a discrete optimization problem (NP-hard) and compute it\nusing a 2-approximation algorithm. To verify candidates, we design\nbidirectional tries, with which the verification starts from promising\npositions and leverage the shared segments of trajectories and the sparsity of\nroad networks for speed-up. Experiments are conducted on large datasets to\ndemonstrate the effectiveness of WED and the efficiency of our method for\nvarious similarity functions under WED.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 23:58:17 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 00:09:54 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Koide", "Satoshi", ""], ["Xiao", "Chuan", ""], ["Ishikawa", "Yoshiharu", ""]]}, {"id": "2006.06053", "submitter": "Sainyam Galhotra Mr", "authors": "Sainyam Galhotra, Karthikeyan Shanmugam, Prasanna Sattigeri and Kush\n  R. Varshney", "title": "Fair Data Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of machine learning (ML) in high-stakes societal decisions has\nencouraged the consideration of fairness throughout the ML lifecycle. Although\ndata integration is one of the primary steps to generate high quality training\ndata, most of the fairness literature ignores this stage. In this work, we\nconsider fairness in the integration component of data management, aiming to\nidentify features that improve prediction without adding any bias to the\ndataset. We work under the causal interventional fairness paradigm. Without\nrequiring the underlying structural causal model a priori, we propose an\napproach to identify a sub-collection of features that ensure the fairness of\nthe dataset by performing conditional independence tests between different\nsubsets of features. We use group testing to improve the complexity of the\napproach. We theoretically prove the correctness of the proposed algorithm to\nidentify features that ensure interventional fairness and show that sub-linear\nconditional independence tests are sufficient to identify these variables. A\ndetailed empirical evaluation is performed on real-world datasets to\ndemonstrate the efficacy and efficiency of our technique.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 20:20:10 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Shanmugam", "Karthikeyan", ""], ["Sattigeri", "Prasanna", ""], ["Varshney", "Kush R.", ""]]}, {"id": "2006.06434", "submitter": "Xuefeng Yang", "authors": "Ningyuan Sun, Xuefeng Yang, Yunfeng Liu", "title": "TableQA: a Large-Scale Chinese Text-to-SQL Dataset for Table-Aware SQL\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Parsing natural language to corresponding SQL (NL2SQL) with data driven\napproaches like deep neural networks attracts much attention in recent years.\nExisting NL2SQL datasets assume that condition values should appear exactly in\nnatural language questions and the queries are answerable given the table.\nHowever, these assumptions may fail in practical scenarios, because user may\nuse different expressions for the same content in the table, and query\ninformation outside the table without the full picture of contents in table.\nTherefore we present TableQA, a large-scale cross-domain Natural Language to\nSQL dataset in Chinese language consisting 64,891 questions and 20,311 unique\nSQL queries on over 6,000 tables. Different from exisiting NL2SQL datasets,\nTableQA requires to generalize well not only to SQL skeletons of different\nquestions and table schemas, but also to the various expressions for condition\nvalues. Experiment results show that the state-of-the-art model with 95.1%\ncondition value accuracy on WikiSQL only gets 46.8% condition value accuracy\nand 43.0% logic form accuracy on TableQA, indicating the proposed dataset is\nchallenging and necessary to handle. Two table-aware approaches are proposed to\nalleviate the problem, the end-to-end approaches obtains 51.3% and 47.4%\naccuracy on the condition value and logic form tasks, with improvement of 4.7%\nand 3.4% respectively.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 03:49:08 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Sun", "Ningyuan", ""], ["Yang", "Xuefeng", ""], ["Liu", "Yunfeng", ""]]}, {"id": "2006.06436", "submitter": "Meng Jiang", "authors": "Yang Zhou, Tong Zhao, Meng Jiang", "title": "A Probabilistic Model with Commonsense Constraints for Pattern-based\n  Temporal Fact Extraction", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual patterns (e.g., Country's president Person) are specified and/or\ngenerated for extracting factual information from unstructured data.\nPattern-based information extraction methods have been recognized for their\nefficiency and transferability. However, not every pattern is reliable: A major\nchallenge is to derive the most complete and accurate facts from diverse and\nsometimes conflicting extractions. In this work, we propose a probabilistic\ngraphical model which formulates fact extraction in a generative process. It\nautomatically infers true facts and pattern reliability without any\nsupervision. It has two novel designs specially for temporal facts: (1) it\nmodels pattern reliability on two types of time signals, including temporal tag\nin text and text generation time; (2) it models commonsense constraints as\nobservable variables. Experimental results demonstrate that our model\nsignificantly outperforms existing methods on extracting true temporal facts\nfrom news data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 13:48:04 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Zhou", "Yang", ""], ["Zhao", "Tong", ""], ["Jiang", "Meng", ""]]}, {"id": "2006.06890", "submitter": "Seung Won Min", "authors": "Seung Won Min, Vikram Sharma Mailthody, Zaid Qureshi, Jinjun Xiong,\n  Eiman Ebrahimi, Wen-mei Hwu", "title": "EMOGI: Efficient Memory-access for Out-of-memory Graph-traversal In GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern analytics and recommendation systems are increasingly based on graph\ndata that capture the relations between entities being analyzed. Practical\ngraphs come in huge sizes, offer massive parallelism, and are stored in\nsparse-matrix formats such as CSR. To exploit the massive parallelism,\ndevelopers are increasingly interested in using GPUs for graph traversal.\nHowever, due to their sizes, graphs often do not fit into the GPU memory. Prior\nworks have either used input data pre-processing/partitioning or UVM to migrate\nchunks of data from the host memory to the GPU memory. However, the large,\nmulti-dimensional, and sparse nature of graph data presents a major challenge\nto these schemes and results in significant amplification of data movement and\nreduced effective data throughput. In this work, we propose EMOGI, an\nalternative approach to traverse graphs that do not fit in GPU memory using\ndirect cacheline-sized access to data stored in host memory. This paper\naddresses the open question of whether a sufficiently large number of\noverlapping cacheline-sized accesses can be sustained to 1) tolerate the long\nlatency to host memory, 2) fully utilize the available bandwidth, and 3)\nachieve favorable execution performance. We analyze the data access patterns of\nseveral graph traversal applications in GPU over PCIe using an FPGA to\nunderstand the cause of poor external bandwidth utilization. By carefully\ncoalescing and aligning external memory requests, we show that we can minimize\nthe number of PCIe transactions and nearly fully utilize the PCIe bandwidth\neven with direct cache-line accesses to the host memory. EMOGI achieves\n2.92$\\times$ speedup on average compared to the optimized UVM implementations\nin various graph traversal applications. We also show that EMOGI scales better\nthan a UVM-based solution when the system uses higher bandwidth interconnects\nsuch as PCIe 4.0.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 00:37:51 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 23:04:37 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Min", "Seung Won", ""], ["Mailthody", "Vikram Sharma", ""], ["Qureshi", "Zaid", ""], ["Xiong", "Jinjun", ""], ["Ebrahimi", "Eiman", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2006.06894", "submitter": "Natasha Noy", "authors": "Omar Benjelloun and Shiyu Chen and Natasha Noy", "title": "Google Dataset Search by the Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists, governments, and companies increasingly publish datasets on the\nWeb. Google's Dataset Search extracts dataset metadata -- expressed using\nschema.org and similar vocabularies -- from Web pages in order to make datasets\ndiscoverable. Since we started the work on Dataset Search in 2016, the number\nof datasets described in schema.org has grown from about 500K to almost 30M.\nThus, this corpus has become a valuable snapshot of data on the Web. To the\nbest of our knowledge, this corpus is the largest and most diverse of its kind.\nWe analyze this corpus and discuss where the datasets originate from, what\ntopics they cover, which form they take, and what people searching for datasets\nare interested in. Based on this analysis, we identify gaps and possible future\nwork to help make data more discoverable.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 00:54:15 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Benjelloun", "Omar", ""], ["Chen", "Shiyu", ""], ["Noy", "Natasha", ""]]}, {"id": "2006.07064", "submitter": "Till Blume", "authors": "Till Blume and Ansgar Scherp", "title": "Indexing Data on the Web: A Comparison of Schema-level Indices for Data\n  Search -- Extended Technical Report", "comments": "Extended technical report of an accepted paper in Database and Expert\n  Systems Applications (DEXA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing the Web of Data offers many opportunities, in particular, to find\nand explore data sources. One major design decision when indexing the Web of\nData is to find a suitable index model, i.e., how to index and summarize data.\nVarious efforts have been conducted to develop specific index models for a\ngiven task. With each index model designed, implemented, and evaluated\nindependently, it remains difficult to judge whether an approach generalizes\nwell to another task, set of queries, or dataset. In this work, we empirically\nevaluate six representative index models with unique feature combinations.\nAmong them is a new index model incorporating inferencing over RDFS and\nowl:sameAs. We implement all index models for the first time into a single,\nstream-based framework. We evaluate variations of the index models considering\nsub-graphs of size 0, 1, and 2 hops on two large, real-world datasets. We\nevaluate the quality of the indices regarding the compression ratio,\nsummarization ratio, and F1-score denoting the approximation quality of the\nstream-based index computation. The experiments reveal huge variations in\ncompression ratio, summarization ratio, and approximation quality for different\nindex models, queries, and datasets. However, we observe meaningful\ncorrelations in the results that help to determine the right index model for a\ngiven task, type of query, and dataset.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 10:39:24 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Blume", "Till", ""], ["Scherp", "Ansgar", ""]]}, {"id": "2006.07180", "submitter": "Rudra Pratap Deb Nath", "authors": "Rudra Pratap Deb Nath, Oscar Romero, Torben Bach Pedersen, and Katja\n  Hose", "title": "High-Level ETL for Semantic Data Warehouses---Full Version", "comments": "44 pages including reference, 13 figures and 4 tables. This paper is\n  submitted to Semantic Web Journal and now it is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of the Semantic Web (SW) encourages organizations to organize\nand publish semantic data using the RDF model. This growth poses new\nrequirements to Business Intelligence (BI) technologies to enable On-Line\nAnalytical Processing (OLAP)-like analysis over semantic data. The\nincorporation of semantic data into a Data Warehouse (DW) is not supported by\nthe traditional Extract-Transform-Load (ETL) tools because they do not consider\nsemantic issues in the integration process. In this paper, we propose a\nlayer-based integration process and a set of high-level RDF-based ETL\nconstructs required to define, map, extract, process, transform, integrate,\nupdate, and load (multidimensional) semantic data. Different to other ETL\ntools, we automate the ETL data flows by creating metadata at the schema level.\nTherefore, it relieves ETL developers from the burden of manual mapping at the\nETL operation level. We create a prototype, named Semantic ETL Construct\n(SETLCONSTRUCT), based on the innovative ETL constructs proposed here. To\nevaluate SETLCONSTRUCT, we create a multidimensional semantic DW by integrating\na Danish Business dataset and an EU Subsidy dataset using it and compare it\nwith the previous programmable framework SETLPROG in terms of productivity,\ndevelopment time and performance. The evaluation shows that 1) SETLCONSTRUCT\nuses 92% fewer Number of Typed Characters (NOTC) than SETLPROG, and SETLAUTO\n(the extension of SETLCONSTRUCT for generating ETL execution flow\nautomatically) further reduces the Number of Used Concepts (NOUC) by another\n25%; 2) using SETLCONSTRUCT, the development time is almost cut in half\ncompared to SETLPROG, and is cut by another 27% using SETLAUTO; 3)\nSETLCONSTRUCT is scalable and has similar performance compared to SETLPROG.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:35:24 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Nath", "Rudra Pratap Deb", ""], ["Romero", "Oscar", ""], ["Pedersen", "Torben Bach", ""], ["Hose", "Katja", ""]]}, {"id": "2006.07357", "submitter": "Rolando Garcia", "authors": "Rolando Garcia, Eric Liu, Vikram Sreekanti, Bobby Yan, Anusha\n  Dandamudi, Joseph E. Gonzalez, Joseph M. Hellerstein, Koushik Sen", "title": "Hindsight Logging for Model Training", "comments": null, "journal-ref": null, "doi": "10.14778/3436905.3436925", "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In modern Machine Learning, model training is an iterative, experimental\nprocess that can consume enormous computation resources and developer time. To\naid in that process, experienced model developers log and visualize program\nvariables during training runs. Exhaustive logging of all variables is\ninfeasible. Optimistic logging can be accompanied by program checkpoints; this\nallows developers to add log statements post-hoc, and \"replay\" desired log\nstatements from checkpoint -- a process we refer to as hindsight logging.\nUnfortunately, hindsight logging raises tricky problems in data management and\nsoftware engineering. Done poorly, hindsight logging can waste resources and\ngenerate technical debt embodied in multiple variants of training code.\n  In this paper, we present methodologies for efficient and effective logging\npractices for model training, with a focus on techniques for hindsight logging.\nOur goal is for experienced model developers to learn and adopt these\npractices. To make this easier, we provide an open-source suite of tools for\nFast Low-Overhead Recovery (flor) that embodies our design across three tasks:\n(i) efficient background logging in Python, (ii) adaptable periodic\ncheckpointing, and (iii) an instrumentation library that codifies hindsight\nlogging for efficient and automatic record-replay of model-training. Model\ndevelopers can use each flor tool separately as they see fit, or they can use\nflor in hands-free mode, entrusting it to instrument their code end-to-end for\nefficient record-replay. Our solutions leverage techniques from physiological\ntransaction logs and recovery in database systems. Evaluations on modern ML\nbenchmarks demonstrate that flor can produce fast checkpointing with small\nuser-specifiable overheads (e.g. 7%), and still provide hindsight log replay\ntimes orders of magnitude faster than restarting training from scratch.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:47:32 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 05:14:53 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Garcia", "Rolando", ""], ["Liu", "Eric", ""], ["Sreekanti", "Vikram", ""], ["Yan", "Bobby", ""], ["Dandamudi", "Anusha", ""], ["Gonzalez", "Joseph E.", ""], ["Hellerstein", "Joseph M.", ""], ["Sen", "Koushik", ""]]}, {"id": "2006.07712", "submitter": "Patrick Lambrix", "authors": "Huanyu Li and Rickard Armiento and Patrick Lambrix", "title": "An Ontology for the Materials Design Domain", "comments": "16 pages", "journal-ref": null, "doi": "10.1007/978-3-030-62466-8_14", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the materials design domain, much of the data from materials calculations\nare stored in different heterogeneous databases. Materials databases usually\nhave different data models. Therefore, the users have to face the challenges to\nfind the data from adequate sources and integrate data from multiple sources.\nOntologies and ontology-based techniques can address such problems as the\nformal representation of domain knowledge can make data more available and\ninteroperable among different systems. In this paper, we introduce the\nMaterials Design Ontology (MDO), which defines concepts and relations to cover\nknowledge in the field of materials design. MDO is designed using domain\nknowledge in materials science (especially in solid-state physics), and is\nguided by the data from several databases in the materials design field. We\nshow the application of the MDO to materials data retrieved from well-known\nmaterials databases.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 20:32:07 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 14:46:09 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Li", "Huanyu", ""], ["Armiento", "Rickard", ""], ["Lambrix", "Patrick", ""]]}, {"id": "2006.07916", "submitter": "James Cheney", "authors": "James Cheney, Xavier Gombau, Ghita Berrada and Sidahmed\n  Benabderrahmane", "title": "Categorical anomaly detection in heterogeneous data using minimum\n  description length clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and effective unsupervised anomaly detection algorithms have been\nproposed for categorical data based on the minimum description length (MDL)\nprinciple. However, they can be ineffective when detecting anomalies in\nheterogeneous datasets representing a mixture of different sources, such as\nsecurity scenarios in which system and user processes have distinct behavior\npatterns. We propose a meta-algorithm for enhancing any MDL-based anomaly\ndetection model to deal with heterogeneous data by fitting a mixture model to\nthe data, via a variant of k-means clustering. Our experimental results show\nthat using a discrete mixture model provides competitive performance relative\nto two previous anomaly detection algorithms, while mixtures of more\nsophisticated models yield further gains, on both synthetic datasets and\nrealistic datasets from a security scenario.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 14:48:37 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Cheney", "James", ""], ["Gombau", "Xavier", ""], ["Berrada", "Ghita", ""], ["Benabderrahmane", "Sidahmed", ""]]}, {"id": "2006.07931", "submitter": "Juan F. Montesinos", "authors": "Juan F. Montesinos, Olga Slizovskaia, Gloria Haro", "title": "Solos: A Dataset for Audio-Visual Music Analysis", "comments": "Rephrased some sentenced. Explanation about OpenPose. Minor\n  grammatical errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.DB cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a new dataset of music performance videos which can\nbe used for training machine learning methods for multiple tasks such as\naudio-visual blind source separation and localization, cross-modal\ncorrespondences, cross-modal generation and, in general, any audio-visual\nself-supervised task. These videos, gathered from YouTube, consist of solo\nmusical performances of 13 different instruments. Compared to previously\nproposed audio-visual datasets, Solos is cleaner since a big amount of its\nrecordings are auditions and manually checked recordings, ensuring there is no\nbackground noise nor effects added in the video post-processing. Besides, it\nis, up to the best of our knowledge, the only dataset that contains the whole\nset of instruments present in the URMP\\cite{URPM} dataset, a high-quality\ndataset of 44 audio-visual recordings of multi-instrument classical music\npieces with individual audio tracks. URMP was intented to be used for source\nseparation, thus, we evaluate the performance on the URMP dataset of two\ndifferent source-separation models trained on Solos. The dataset is publicly\navailable at https://juanfmontesinos.github.io/Solos/\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 15:30:44 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 23:37:27 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Montesinos", "Juan F.", ""], ["Slizovskaia", "Olga", ""], ["Haro", "Gloria", ""]]}, {"id": "2006.08067", "submitter": "Victor Zakhary", "authors": "Victor Zakhary, Lawrence Lim, Divyakant Agrawal, Amr El Abbadi", "title": "CoT: Decentralized Elastic Caches for Cloud Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed caches are widely deployed to serve social networks and web\napplications at billion-user scales. This paper presents Cache-on-Track (CoT),\na decentralized, elastic, and predictive caching framework for cloud\nenvironments. CoT proposes a new cache replacement policy specifically tailored\nfor small front-end caches that serve skewed workloads. Front-end servers use a\nheavy hitter tracking algorithm to continuously track the top-k hot keys. CoT\ndynamically caches the hottest C keys out of the tracked keys. Our experiments\nshow that CoT's replacement policy consistently outperforms the hit-rates of\nLRU, LFU, and ARC for the same cache size on different skewed workloads. Also,\n\\algoname slightly outperforms the hit-rate of LRU-2 when both policies are\nconfigured with the same tracking (history) size. CoT achieves server size\nload-balance with 50\\% to 93.75\\% less front-end cache in comparison to other\nreplacement policies.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 01:03:47 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 04:22:21 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zakhary", "Victor", ""], ["Lim", "Lawrence", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "2006.08109", "submitter": "Zongheng Yang", "authors": "Zongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi\n  Chen, and Ion Stoica", "title": "NeuroCard: One Cardinality Estimator for All Tables", "comments": "VLDB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query optimizers rely on accurate cardinality estimates to produce good\nexecution plans. Despite decades of research, existing cardinality estimators\nare inaccurate for complex queries, due to making lossy modeling assumptions\nand not capturing inter-table correlations. In this work, we show that it is\npossible to learn the correlations across all tables in a database without any\nindependence assumptions. We present NeuroCard, a join cardinality estimator\nthat builds a single neural density estimator over an entire database.\nLeveraging join sampling and modern deep autoregressive models, NeuroCard makes\nno inter-table or inter-column independence assumptions in its probabilistic\nmodeling. NeuroCard achieves orders of magnitude higher accuracy than the best\nprior methods (a new state-of-the-art result of 8.5$\\times$ maximum error on\nJOB-light), scales to dozens of tables, while being compact in space (several\nMBs) and efficient to construct or update (seconds to minutes).\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 03:21:46 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 19:15:11 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yang", "Zongheng", ""], ["Kamsetty", "Amog", ""], ["Luan", "Sifei", ""], ["Liang", "Eric", ""], ["Duan", "Yan", ""], ["Chen", "Xi", ""], ["Stoica", "Ion", ""]]}, {"id": "2006.08224", "submitter": "Medha Atre", "authors": "Medha Atre, Anand Deshpande, Reshma Godse, Pooja Deokar, Sandip\n  Moharir, Dhruva Ray, Akshay Chitlangia, Trupti Phadnis, Yugansh Goyal", "title": "Needles in the 'Sheet'stack: Augmented Analytics to get Insights from\n  Spreadsheets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business intelligence (BI) tools for database analytics have come a long way\nand nowadays also provide ready insights or visual query explorations, e.g.\nQuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In\nthis demo, we focus on providing insights by examining periodic spreadsheets of\ndifferent reports (aka views), without prior knowledge of the schema of the\ndatabase or reports, or data information. Such a solution is targeted at users\nwithout the familiarity with the database schema or resources to conduct\nanalytics in the contemporary way.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 08:54:22 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Atre", "Medha", ""], ["Deshpande", "Anand", ""], ["Godse", "Reshma", ""], ["Deokar", "Pooja", ""], ["Moharir", "Sandip", ""], ["Ray", "Dhruva", ""], ["Chitlangia", "Akshay", ""], ["Phadnis", "Trupti", ""], ["Goyal", "Yugansh", ""]]}, {"id": "2006.08467", "submitter": "Marie-Laure Mugnier", "authors": "Pierre Bourhis and Michel Lecl\\`ere and Marie-Laure Mugnier and Sophie\n  Tison and Federico Ulliana and Lily Galois", "title": "Oblivious and Semi-Oblivious Boundedness for Existential Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the notion of boundedness in the context of positive existential\nrules, that is, whether there exists an upper bound to the depth of the chase\nprocedure, that is independent from the initial instance. By focussing our\nattention on the oblivious and the semi-oblivious chase variants, we give a\ncharacterization of boundedness in terms of FO-rewritability and chase\ntermination. We show that it is decidable to recognize if a set of rules is\nbounded for several classes and outline the complexity of the problem.\n  This report contains the paper published at IJCAI 2019 and an appendix with\nfull proofs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:18:57 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Bourhis", "Pierre", ""], ["Lecl\u00e8re", "Michel", ""], ["Mugnier", "Marie-Laure", ""], ["Tison", "Sophie", ""], ["Ulliana", "Federico", ""], ["Galois", "Lily", ""]]}, {"id": "2006.08475", "submitter": "Muhammad Cheema", "authors": "Lingxiao Li, Muhammad Aamir Cheema, Hua Lu, Mohammed Eunus Ali, Adel\n  N. Toosi", "title": "Comparing Alternative Route Planning Techniques: A Comparative User\n  Study on Melbourne, Dhaka and Copenhagen Road Networks", "comments": "Extended the user study to also include the road networks of Dhaka\n  and Copenhagen (the previous version only had Melbourne road network)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern navigation systems and map-based services do not only provide the\nfastest route from a source location s to a target location t but also provide\na few alternative routes to the users as more options to choose from.\nConsequently, computing alternative paths has received significant research\nattention. However, it is unclear which of the existing approaches generates\nalternative routes of better quality because the quality of these alternatives\nis mostly subjective. Motivated by this, in this paper, we present a user study\nconducted on the road networks of Melbourne, Dhaka and Copenhagen that compares\nthe quality (as perceived by the users) of the alternative routes generated by\nfour of the most popular existing approaches including the routes provided by\nGoogle Maps. We also present a web-based demo system that can be accessed using\nany internet-enabled device and allows users to see the alternative routes\ngenerated by the four approaches for any pair of selected source and target. We\nreport the average ratings received by the four approaches and our statistical\nanalysis shows that there is no credible evidence that the four approaches\nreceive different ratings on average. We also discuss the limitations of this\nuser study and recommend the readers to interpret these results with caution\nbecause certain factors may have affected the participants' ratings.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:25:48 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 08:52:53 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Li", "Lingxiao", ""], ["Cheema", "Muhammad Aamir", ""], ["Lu", "Hua", ""], ["Ali", "Mohammed Eunus", ""], ["Toosi", "Adel N.", ""]]}, {"id": "2006.08842", "submitter": "Hongzhi Wang", "authors": "Shun Yao, Hongzhi Wang and Yu Yan", "title": "Index Selection for NoSQL Database with Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach of NoSQL database index selection. For different\nworkloads, we select different indexes and their different parameters to\noptimize the database performance. The approach builds a deep reinforcement\nlearning model to select an optimal index for a given fixed workload and adapts\nto a changing workload. Experimental results show that, Deep Reinforcement\nLearning Index Selection Approach (DRLISA) has improved performance to varying\ndegrees according to traditional single index structures.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 00:40:50 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Yao", "Shun", ""], ["Wang", "Hongzhi", ""], ["Yan", "Yu", ""]]}, {"id": "2006.09085", "submitter": "Leonardo Pellegrina", "authors": "Leonardo Pellegrina, Cyrus Cousins, Fabio Vandin, Matteo Riondato", "title": "MCRapper: Monte-Carlo Rademacher Averages for Poset Families and\n  Approximate Pattern Mining", "comments": null, "journal-ref": null, "doi": "10.1145/3394486.3403267", "report-no": null, "categories": "cs.LG cs.DB cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MCRapper, an algorithm for efficient computation of Monte-Carlo\nEmpirical Rademacher Averages (MCERA) for families of functions exhibiting\nposet (e.g., lattice) structure, such as those that arise in many pattern\nmining tasks. The MCERA allows us to compute upper bounds to the maximum\ndeviation of sample means from their expectations, thus it can be used to find\nboth statistically-significant functions (i.e., patterns) when the available\ndata is seen as a sample from an unknown distribution, and approximations of\ncollections of high-expectation functions (e.g., frequent patterns) when the\navailable data is a small sample from a large dataset. This feature is a strong\nimprovement over previously proposed solutions that could only achieve one of\nthe two. MCRapper uses upper bounds to the discrepancy of the functions to\nefficiently explore and prune the search space, a technique borrowed from\npattern mining itself. To show the practical use of MCRapper, we employ it to\ndevelop an algorithm TFP-R for the task of True Frequent Pattern (TFP) mining.\nTFP-R gives guarantees on the probability of including any false positives\n(precision) and exhibits higher statistical power (recall) than existing\nmethods offering the same guarantees. We evaluate MCRapper and TFP-R and show\nthat they outperform the state-of-the-art for their respective tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 11:42:56 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Pellegrina", "Leonardo", ""], ["Cousins", "Cyrus", ""], ["Vandin", "Fabio", ""], ["Riondato", "Matteo", ""]]}, {"id": "2006.09935", "submitter": "Jihoon Ko", "authors": "Jihoon Ko, Yunbum Kook, Kijung Shin", "title": "Incremental Lossless Graph Summarization", "comments": "to appear at the 26th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining (KDD '20)", "journal-ref": null, "doi": "10.1145/3394486.3403074", "report-no": null, "categories": "cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given a fully dynamic graph, represented as a stream of edge insertions and\ndeletions, how can we obtain and incrementally update a lossless summary of its\ncurrent snapshot? As large-scale graphs are prevalent, concisely representing\nthem is inevitable for efficient storage and analysis. Lossless graph\nsummarization is an effective graph-compression technique with many desirable\nproperties. It aims to compactly represent the input graph as (a) a summary\ngraph consisting of supernodes (i.e., sets of nodes) and superedges (i.e.,\nedges between supernodes), which provide a rough description, and (b) edge\ncorrections which fix errors induced by the rough description. While a number\nof batch algorithms, suited for static graphs, have been developed for rapid\nand compact graph summarization, they are highly inefficient in terms of time\nand space for dynamic graphs, which are common in practice. In this work, we\npropose MoSSo, the first incremental algorithm for lossless summarization of\nfully dynamic graphs. In response to each change in the input graph, MoSSo\nupdates the output representation by repeatedly moving nodes among supernodes.\nMoSSo decides nodes to be moved and their destinations carefully but rapidly\nbased on several novel ideas. Through extensive experiments on 10 real graphs,\nwe show MoSSo is (a) Fast and 'any time': processing each change in\nnear-constant time (less than 0.1 millisecond), up to 7 orders of magnitude\nfaster than running state-of-the-art batch methods, (b) Scalable: summarizing\ngraphs with hundreds of millions of edges, requiring sub-linear memory during\nthe process, and (c) Effective: achieving comparable compression ratios even to\nstate-of-the-art batch methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 15:33:30 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Ko", "Jihoon", ""], ["Kook", "Yunbum", ""], ["Shin", "Kijung", ""]]}, {"id": "2006.10188", "submitter": "Jen Rexford", "authors": "Rachit Agarwal and Jen Rexford (workshop co-chairs) with contributions\n  from numerous workshop attendees", "title": "Wide-Area Data Analytics", "comments": "A Computing Community Consortium (CCC) workshop report, 16 pages", "journal-ref": null, "doi": null, "report-no": "ccc2020report_2", "categories": "cs.CY cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We increasingly live in a data-driven world, with diverse kinds of data\ndistributed across many locations. In some cases, the datasets are collected\nfrom multiple locations, such as sensors (e.g., mobile phones and street\ncameras) spread throughout a geographic region. The data may need to be\nanalyzed close to where they are produced, particularly when the applications\nrequire low latency, high, low cost, user privacy, and regulatory constraints.\nIn other cases, large datasets are distributed across public clouds, private\nclouds, or edge-cloud computing sites with more plentiful computation, storage,\nbandwidth, and energy resources. Often, some portion of the analysis may take\nplace on the end-host or edge cloud (to respect user privacy and reduce the\nvolume of data) while relying on remote clouds to complete the analysis (to\nleverage greater computation and storage resources).\n  Wide-area data analytics is any analysis of data that is generated by, or\nstored at, geographically dispersed entities. Over the past few years, several\nparts of the computer science research community have started to explore\neffective ways to analyze data spread over multiple locations. In particular,\nseveral areas of \"systems\" research - including databases, distributed systems,\ncomputer networking, and security and privacy - have delved into these topics.\nThese research subcommunities often focus on different aspects of the problem,\nconsider different motivating applications and use cases, and design and\nevaluate their solutions differently. To address these challenges the Computing\nCommunity Consortium (CCC) convened a 1.5-day workshop focused on wide-area\ndata analytics in October 2019. This report summarizes the challenges discussed\nand the conclusions generated at the workshop.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 22:44:33 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Agarwal", "Rachit", "", "workshop co-chairs"], ["Rexford", "Jen", "", "workshop co-chairs"], ["attendees", "with contributions from numerous workshop", ""]]}, {"id": "2006.10208", "submitter": "Alireza Heidari", "authors": "Alireza Heidari, George Michalopoulos, Shrinu Kushagra, Ihab F. Ilyas,\n  Theodoros Rekatsinas", "title": "Record fusion: A learning approach", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record fusion is the task of aggregating multiple records that correspond to\nthe same real-world entity in a database. We can view record fusion as a\nmachine learning problem where the goal is to predict the \"correct\" value for\neach attribute for each entity. Given a database, we use a combination of\nattribute-level, recordlevel, and database-level signals to construct a feature\nvector for each cell (or (row, col)) of that database. We use this feature\nvector alongwith the ground-truth information to learn a classifier for each of\nthe attributes of the database.\n  Our learning algorithm uses a novel stagewise additive model. At each stage,\nwe construct a new feature vector by combining a part of the original feature\nvector with features computed by the predictions from the previous stage. We\nthen learn a softmax classifier over the new feature space. This greedy\nstagewise approach can be viewed as a deep model where at each stage, we are\nadding more complicated non-linear transformations of the original feature\nvector. We show that our approach fuses records with an average precision of\n~98% when source information of records is available, and ~94% without source\ninformation across a diverse array of real-world datasets. We compare our\napproach to a comprehensive collection of data fusion and entity consolidation\nmethods considered in the literature. We show that our approach can achieve an\naverage precision improvement of ~20%/~45% with/without source information\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 00:04:37 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Heidari", "Alireza", ""], ["Michalopoulos", "George", ""], ["Kushagra", "Shrinu", ""], ["Ilyas", "Ihab F.", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "2006.10949", "submitter": "Jiping Zheng", "authors": "Jiping Zheng and Chen Chen", "title": "Sorting-based Interactive Regret Minimization", "comments": "15 pages, accepted for publication of the 4th APWeb-WAIM joint\n  international conference on Web and Big Data (APWebWAIM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important tool for multi-criteria decision making in database systems,\nthe regret minimization query is shown to have the merits of top-k and skyline\nqueries: it controls the output size while does not need users to provide any\npreferences. Existing researches verify that the regret ratio can be much\ndecreased when interaction is available. In this paper, we study how to enhance\ncurrent interactive regret minimization query by sorting mechanism. Instead of\nselecting the most favorite point from the displayed points for each\ninteraction round, users sort the displayed data points and send the results to\nthe system. By introducing sorting mechanism, for each round of interaction the\nutility space explored will be shrunk to some extent. Further the candidate\npoints selection for following rounds of interaction will be narrowed to\nsmaller data spaces thus the number of interaction rounds will be reduced. We\npropose two effective sorting-based algorithms namely Sorting-Simplex and\nSorting-Random to find the maximum utility point based on Simplex method and\nrandomly selection strategy respectively. Experiments on synthetic and real\ndatasets verify our Sorting-Simplex and Sorting-Random algorithms outperform\ncurrent state-of-art ones.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 04:08:08 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Zheng", "Jiping", ""], ["Chen", "Chen", ""]]}, {"id": "2006.11284", "submitter": "Omid Jafari", "authors": "Omid Jafari, Parth Nagarkar, Jonathan Monta\\~no", "title": "Improving Locality Sensitive Hashing by Efficiently Finding Projected\n  Nearest Neighbors", "comments": "arXiv admin note: text overlap with arXiv:2003.06415", "journal-ref": "SISAP 2020. Lecture Notes in Computer Science, vol 12440.\n  Springer, Cham", "doi": "10.1007/978-3-030-60936-8_25", "report-no": null, "categories": "cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search in high-dimensional spaces is an important task for many\nmultimedia applications. Due to the notorious curse of dimensionality,\napproximate nearest neighbor techniques are preferred over exact searching\ntechniques since they can return good enough results at a much better speed.\nLocality Sensitive Hashing (LSH) is a very popular random hashing technique for\nfinding approximate nearest neighbors. Existing state-of-the-art Locality\nSensitive Hashing techniques that focus on improving performance of the overall\nprocess, mainly focus on minimizing the total number of IOs while sacrificing\nthe overall processing time. The main time-consuming process in LSH techniques\nis the process of finding neighboring points in projected spaces. We present a\nnovel index structure called radius-optimized Locality Sensitive Hashing\n(roLSH). With the help of sampling techniques and Neural Networks, we present\ntwo techniques to find neighboring points in projected spaces efficiently,\nwithout sacrificing the accuracy of the results. Our extensive experimental\nanalysis on real datasets shows the performance benefit of roLSH over existing\nstate-of-the-art LSH techniques.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:46:30 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Jafari", "Omid", ""], ["Nagarkar", "Parth", ""], ["Monta\u00f1o", "Jonathan", ""]]}, {"id": "2006.11285", "submitter": "Omid Jafari", "authors": "Omid Jafari, Parth Nagarkar", "title": "Experimental Analysis of Locality Sensitive Hashing Techniques for\n  High-Dimensional Approximate Nearest Neighbor Searches", "comments": "arXiv admin note: text overlap with arXiv:2003.06415", "journal-ref": "ADC 2021. Lecture Notes in Computer Science, vol. 12610. Springer,\n  Cham, pp. 62-73", "doi": "10.1007/978-3-030-69377-0_6", "report-no": null, "categories": "cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding nearest neighbors in high-dimensional spaces is a fundamental\noperation in many multimedia retrieval applications. Exact tree-based indexing\napproaches are known to suffer from the notorious curse of dimensionality for\nhigh-dimensional data. Approximate searching techniques sacrifice some accuracy\nwhile returning good enough results for faster performance. Locality Sensitive\nHashing (LSH) is a very popular technique for finding approximate nearest\nneighbors in high-dimensional spaces. Apart from providing theoretical\nguarantees on the query results, one of the main benefits of LSH techniques is\ntheir good scalability to large datasets because they are external memory\nbased. The most dominant costs for existing LSH techniques are the algorithm\ntime and the index I/Os required to find candidate points. Existing works do\nnot compare both of these dominant costs in their evaluation. In this\nexperimental survey paper, we show the impact of both these costs on the\noverall performance of the LSH technique. We compare three state-of-the-art\ntechniques on four real-world datasets, and show that, in contrast to recent\nworks, C2LSH is still the state-of-the-art algorithm in terms of performance\nwhile achieving similar accuracy as its recent competitors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:57:41 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jafari", "Omid", ""], ["Nagarkar", "Parth", ""]]}, {"id": "2006.11454", "submitter": "Kostas Zoumpatianos", "authors": "Karima Echihabi, Kostas Zoumpatianos, Themis Palpanas, Houda Benbrahim", "title": "The Lernaean Hydra of Data Series Similarity Search: An Experimental\n  Evaluation of the State of the Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly large data series collections are becoming commonplace across\nmany different domains and applications. A key operation in the analysis of\ndata series collections is similarity search, which has attracted lots of\nattention and effort over the past two decades. Even though several relevant\napproaches have been proposed in the literature, none of the existing studies\nprovides a detailed evaluation against the available alternatives. The lack of\ncomparative results is further exacerbated by the non-standard use of\nterminology, which has led to confusion and misconceptions. In this paper, we\nprovide definitions for the different flavors of similarity search that have\nbeen studied in the past, and present the first systematic experimental\nevaluation of the efficiency of data series similarity search techniques. Based\non the experimental results, we describe the strengths and weaknesses of each\napproach and give recommendations for the best approach to use under typical\nuse cases. Finally, by identifying the shortcomings of each method, our\nfindings lay the ground for solid further developments in the field.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 01:04:27 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Echihabi", "Karima", ""], ["Zoumpatianos", "Kostas", ""], ["Palpanas", "Themis", ""], ["Benbrahim", "Houda", ""]]}, {"id": "2006.11459", "submitter": "Kostas Zoumpatianos", "authors": "Karima Echihabi, Kostas Zoumpatianos, Themis Palpanas, Houda Benbrahim", "title": "Return of the Lernaean Hydra: Experimental Evaluation of Data Series\n  Approximate Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data series are a special type of multidimensional data present in numerous\ndomains, where similarity search is a key operation that has been extensively\nstudied in the data series literature. In parallel, the multidimensional\ncommunity has studied approximate similarity search techniques. We propose a\ntaxonomy of similarity search techniques that reconciles the terminology used\nin these two domains, we describe modifications to data series indexing\ntechniques enabling them to answer approximate similarity queries with quality\nguarantees, and we conduct a thorough experimental evaluation to compare\napproximate similarity search techniques under a unified framework, on\nsynthetic and real datasets in memory and on disk. Although data series differ\nfrom generic multidimensional vectors (series usually exhibit correlation\nbetween neighboring values), our results show that data series techniques\nanswer approximate %similarity queries with strong guarantees and an excellent\nempirical performance, on data series and vectors alike. These techniques\noutperform the state-of-the-art approximate techniques for vectors when\noperating on disk, and remain competitive in memory.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 01:27:49 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Echihabi", "Karima", ""], ["Zoumpatianos", "Kostas", ""], ["Palpanas", "Themis", ""], ["Benbrahim", "Houda", ""]]}, {"id": "2006.11474", "submitter": "Kostas Zoumpatianos", "authors": "Haridimos Kondylakis, Niv Dayan, Kostas Zoumpatianos, Themis Palpanas", "title": "Coconut: sortable summarizations for scalable indexes over static and\n  streaming data series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern applications produce massive streams of data series that need to\nbe analyzed, requiring efficient similarity search operations. However, the\nstate-of-the-art data series indexes that are used for this purpose do not\nscale well for massive datasets in terms of performance, or storage costs. We\npinpoint the problem to the fact that existing summarizations of data series\nused for indexing cannot be sorted while keeping similar data series close to\neach other in the sorted order. To address this problem, we present Coconut,\nthe first data series index based on sortable summarizations and the first\nefficient solution for indexing and querying streaming series. The first\ninnovation in Coconut is an inverted, sortable data series summarization that\norganizes data series based on a z-order curve, keeping similar series close to\neach other in the sorted order. As a result, Coconut is able to use bulk\nloading and updating techniques that rely on sorting to quickly build and\nmaintain a contiguous index using large sequential disk I/Os. We then explore\nprefix-based and median-based splitting policies for bottom-up bulk loading,\nshowing that median-based splitting outperforms the state of the art, ensuring\nthat all nodes are densely populated. Finally, we explore the impact of\nsortable summarizations on variable-sized window queries, showing that they can\nbe supported in the presence of updates through efficient merging of temporal\npartitions. Overall, we show analytically and empirically that Coconut\ndominates the state-of-the-art data series indexes in terms of construction\nspeed, query speed, and storage costs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 02:18:51 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 07:33:22 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kondylakis", "Haridimos", ""], ["Dayan", "Niv", ""], ["Zoumpatianos", "Kostas", ""], ["Palpanas", "Themis", ""]]}, {"id": "2006.11494", "submitter": "Michael Yu", "authors": "Michael Yu, Lu Qin, Ying Zhang, Wenjie Zhang, Xuemin Lin", "title": "AOT: Pushing the Efficiency Boundary of Main-memory Triangle Listing", "comments": "Submitted to and accepted by DASFFA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangle listing is an important topic significant in many practical\napplications. Efficient algorithms exist for the task of triangle listing.\nRecent algorithms leverage an orientation framework, which can be thought of as\nmapping an undirected graph to a directed acylic graph, namely oriented graph,\nwith respect to any global vertex order. In this paper, we propose an adaptive\norientation technique that satisfies the orientation technique but refines it\nby traversing carefully based on the out-degree of the vertices in the oriented\ngraph during the computation of triangles. Based on this adaptive orientation\ntechnique, we design a new algorithm, namely aot, to enhance the edge-iterator\nlisting paradigm. We also make improvements to the performance of aot by\nexploiting the local order within the adjacent list of the vertices.\n  We show that aot is the first work which can achieve best performance in\nterms of both practical performance and theoretical time complexity. Our\ncomprehensive experiments over $16$ real-life large graphs show a superior\nperformance of our \\aot algorithm when compared against the state-of-the-art,\nespecially for massive graphs with billions of edges. Theoretically, we show\nthat our proposed algorithm has a time complexity of $\\Theta(\\sum_{ \\langle u,v\n\\rangle \\in \\vec{E} } \\min\\{ deg^{+}(u),deg^{+}(v)\\}))$, where $\\vec{E}$ and\n$deg^{+}(x)$ denote the set of directed edges in an oriented graph and the\nout-degree of vertex $x$ respectively. As to our best knowledge, this is the\nbest time complexity among in-memory triangle listing algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 04:53:44 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 03:01:22 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Yu", "Michael", ""], ["Qin", "Lu", ""], ["Zhang", "Ying", ""], ["Zhang", "Wenjie", ""], ["Lin", "Xuemin", ""]]}, {"id": "2006.12018", "submitter": "Pratiksha Thaker", "authors": "Pratiksha Thaker and Mihai Budiu and Parikshit Gopalan and Udi Wieder\n  and Matei Zaharia", "title": "Overlook: Differentially Private Exploratory Visualization for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data exploration systems that provide differential privacy must manage a\nprivacy budget that measures the amount of privacy lost across multiple\nqueries. One effective strategy to manage the privacy budget is to compute a\none-time private synopsis of the data, to which users can make an unlimited\nnumber of queries. However, existing systems using synopses are built for\noffline use cases, where a set of queries is known ahead of time and the system\ncarefully optimizes a synopsis for it. The synopses that these systems build\nare costly to compute and may also be costly to store.\n  We introduce Overlook, a system that enables private data exploration at\ninteractive latencies for both data analysts and data curators. The key idea in\nOverlook is a virtual synopsis that can be evaluated incrementally, without\nextra space storage or expensive precomputation. Overlook simply executes\nqueries using an existing engine, such as a SQL DBMS, and adds noise to their\nresults. Because Overlook's synopses do not require costly precomputation or\nstorage, data curators can also use Overlook to explore the impact of privacy\nparameters interactively. Overlook offers a rich visual query interface based\non the open source Hillview system. Overlook achieves accuracy comparable to\nexisting synopsis-based systems, while offering better performance and removing\nthe need for extra storage.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 05:56:16 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Thaker", "Pratiksha", ""], ["Budiu", "Mihai", ""], ["Gopalan", "Parikshit", ""], ["Wieder", "Udi", ""], ["Zaharia", "Matei", ""]]}, {"id": "2006.12101", "submitter": "Tsubasa Takahashi", "authors": "Shun Takagi, Tsubasa Takahashi, Yang Cao, Masatoshi Yoshikawa", "title": "P3GM: Private High-Dimensional Data Release via Privacy Preserving\n  Phased Generative Model", "comments": "Accepted at ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we release a massive volume of sensitive data while mitigating\nprivacy risks? Privacy-preserving data synthesis enables the data holder to\noutsource analytical tasks to an untrusted third party. The state-of-the-art\napproach for this problem is to build a generative model under differential\nprivacy, which offers a rigorous privacy guarantee. However, the existing\nmethod cannot adequately handle high dimensional data. In particular, when the\ninput dataset contains a large number of features, the existing techniques\nrequire injecting a prohibitive amount of noise to satisfy differential\nprivacy, which results in the outsourced data analysis meaningless. To address\nthe above issue, this paper proposes privacy-preserving phased generative model\n(P3GM), which is a differentially private generative model for releasing such\nsensitive data. P3GM employs the two-phase learning process to make it robust\nagainst the noise, and to increase learning efficiency (e.g., easy to\nconverge). We give theoretical analyses about the learning complexity and\nprivacy loss in P3GM. We further experimentally evaluate our proposed method\nand demonstrate that P3GM significantly outperforms existing solutions.\nCompared with the state-of-the-art methods, our generated samples look fewer\nnoises and closer to the original data in terms of data diversity. Besides, in\nseveral data mining tasks with synthesized data, our model outperforms the\ncompetitors in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 09:47:54 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 01:51:02 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 07:52:58 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Takagi", "Shun", ""], ["Takahashi", "Tsubasa", ""], ["Cao", "Yang", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "2006.12737", "submitter": "Ziaur Rahman", "authors": "T.M. Amir-Ul-Haque Bhuiyan, Mehedi Hasan Talukdar, Ziaur Rahman, Dr.\n  Mohammad Motiur Rahman", "title": "Database Optimization to Recommend Software Developers using Canonical\n  Order Tree", "comments": "4 Pages 8 Figures 7 Tables", "journal-ref": "2015 International Conference on Advances in Electrical\n  Engineering (ICAEE), Dhaka, 2015, pp. 157-160", "doi": "10.1109/ICAEE.2015.7506820", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently frequent and sequential pattern mining algorithms have been widely\nused in the field of software engineering to mine various source code or\nspecification patterns. In practice software evolves from one version to\nanother is needed for providing extra facilities to user. This kind of task is\nchallenging in this domain since the database is usually updated in all kinds\nof manners such as insertion, various modifications as well as removal of\nsequences. If database is optimized then this optimized information will help\ndeveloper in their development process and save their valuable time as well as\ndevelopment expenses. Some existing algorithms which are used to optimize\ndatabase but it does not work faster when database is incrementally updated. To\novercome this challenges an efficient algorithm is recently introduce, called\nthe Canonical Order Tree that captures the content of the transactions of the\ndatabase and orders. In this paper we have proposed a technique based on the\nCanonical Order Tree that can find out frequent patterns from the incremental\ndatabase with speedy and efficient way. Thus the database will be optimized as\nwell as it gives useful information to recommend software developer.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 05:02:29 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Bhuiyan", "T. M. Amir-Ul-Haque", ""], ["Talukdar", "Mehedi Hasan", ""], ["Rahman", "Ziaur", ""], ["Rahman", "Dr. Mohammad Motiur", ""]]}, {"id": "2006.12804", "submitter": "Ryan Marcus", "authors": "Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit\n  Misra, Alfons Kemper, Thomas Neumann, Tim Kraska", "title": "Benchmarking Learned Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in learned index structures propose replacing existing\nindex structures, like B-Trees, with approximate learned models. In this work,\nwe present a unified benchmark that compares well-tuned implementations of\nthree learned index structures against several state-of-the-art \"traditional\"\nbaselines. Using four real-world datasets, we demonstrate that learned index\nstructures can indeed outperform non-learned indexes in read-only in-memory\nworkloads over a dense array. We also investigate the impact of caching,\npipelining, dataset size, and key size. We study the performance profile of\nlearned index structures, and build an explanation for why learned models\nachieve such good performance. Finally, we investigate other important\nproperties of learned index structures, such as their performance in\nmulti-threaded systems and their build times.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 07:37:59 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 15:51:05 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Marcus", "Ryan", ""], ["Kipf", "Andreas", ""], ["van Renen", "Alexander", ""], ["Stoian", "Mihail", ""], ["Misra", "Sanchit", ""], ["Kemper", "Alfons", ""], ["Neumann", "Thomas", ""], ["Kraska", "Tim", ""]]}, {"id": "2006.12812", "submitter": "Shadman Saqib Eusuf", "authors": "Mohammed Eunus Ali, Shadman Saqib Eusuf, Kazi Ashik Islam", "title": "An Efficient Index for Contact Tracing Query in a Large Spatio-Temporal\n  Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a novel contact tracing query (CTQ) that finds users\nwho have been in $direct$ $contact$ with the query user or $in$ $contact$\n$with$ $the$ $already$ $contacted$ $users$ in subsequent timestamps from a\nlarge spatio-temporal database. The CTQ is of paramount importance in the era\nof new COVID-19 pandemic world for finding possible list of potential COVID-19\nexposed patients. A straightforward way to answer the CTQ is using traditional\nspatio-temporal indexes. However, these indexes cannot serve the purpose as\neach user covers a large area within the time-span of potential disease\nspreading and thus they can hardly use efficient pruning techniques. We propose\na multi-level index, namely QR-tree, that consider both space coverage and the\nco-visiting patterns to group users so that users who are likely to meet the\nquery user are grouped together. More specifically, we use a quadtree to\npartition user movement traces w.r.t. space and time, and then exploit these\nspace-time mapping of user traces to group users using an R-tree. The QR-tree\nfacilitates efficient pruning and enables accessing only potential sets of user\nwho can be the candidate answers for the CTQ. Experiments with real datasets\nshow the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 08:10:17 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 11:28:36 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 17:46:49 GMT"}, {"version": "v4", "created": "Sun, 9 Aug 2020 05:24:16 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ali", "Mohammed Eunus", ""], ["Eusuf", "Shadman Saqib", ""], ["Islam", "Kazi Ashik", ""]]}, {"id": "2006.12819", "submitter": "Zhaokang Wang", "authors": "Zhaokang Wang, Weiwei Hu, Chunfeng Yuan, Rong Gu, Yihua Huang", "title": "Distributed Subgraph Enumeration via Backtracking-based Framework", "comments": "Modify some terms;Fix typos; Unify line styles in Fig. 13 and Fig. 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding or monitoring subgraph instances that are isomorphic to a given\npattern graph in a data graph is a fundamental query operation in many graph\nanalytic applications, such as network motif mining and fraud detection. The\nstate-of-the-art distributed methods are inefficient in communication. They\nhave to shuffle partial matching results during the distributed multiway join.\nThe partial matching results may be much larger than the data graph itself. To\novercome the drawback, we develop the Batch-BENU framework (B-BENU) for\ndistributed subgraph enumeration. B-BENU executes a group of local search tasks\nin parallel. Each task enumerates subgraphs around a vertex in the data graph,\nguided by a backtracking-based execution plan. B-BENU does not shuffle any\npartial matching result. Instead, it stores the data graph in a distributed\ndatabase. Each task queries adjacency sets of the data graph on demand. To\nsupport dynamic data graphs, we propose the concept of incremental pattern\ngraphs and turn continuous subgraph enumeration into enumerating incremental\npattern graphs at each time step. We develop the Streaming-BENU framework\n(S-BENU) to enumerate their matches efficiently. We implement B-BENU and S-BENU\nwith the local database cache and the task splitting techniques. The extensive\nexperiments show that B-BENU and S-BENU can scale to big data graphs and\ncomplex pattern graphs. They outperform the state-of-the-art methods by up to\none and two orders of magnitude, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 08:23:59 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 07:18:47 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 03:23:51 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Wang", "Zhaokang", ""], ["Hu", "Weiwei", ""], ["Yuan", "Chunfeng", ""], ["Gu", "Rong", ""], ["Huang", "Yihua", ""]]}, {"id": "2006.12856", "submitter": "Stephan Fahrenkrog-Petersen", "authors": "Stephan A. Fahrenkrog-Petersen, Han van der Aa, Matthias Weidlich", "title": "PRIPEL: Privacy-Preserving Event Log Publishing Including Contextual\n  Information", "comments": null, "journal-ref": "International Conference on Business Process Management 2020", "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event logs capture the execution of business processes in terms of executed\nactivities and their execution context. Since logs contain potentially\nsensitive information about the individuals involved in the process, they\nshould be pre-processed before being published to preserve the individuals'\nprivacy. However, existing techniques for such pre-processing are limited to a\nprocess' control-flow and neglect contextual information, such as attribute\nvalues and durations. This thus precludes any form of process analysis that\ninvolves contextual factors. To bridge this gap, we introduce PRIPEL, a\nframework for privacy-aware event log publishing. Compared to existing work,\nPRIPEL takes a fundamentally different angle and ensures privacy on the level\nof individual cases instead of the complete log. This way, contextual\ninformation as well as the long tail process behaviour are preserved, which\nenables the application of a rich set of process analysis techniques. We\ndemonstrate the feasibility of our framework in a case study with a real-world\nevent log.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 09:28:40 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Fahrenkrog-Petersen", "Stephan A.", ""], ["van der Aa", "Han", ""], ["Weidlich", "Matthias", ""]]}, {"id": "2006.13079", "submitter": "Kostas Zoumpatianos", "authors": "Haridimos Kondylakis, Niv Dayan, Kostas Zoumpatianos, Themis Palpanas", "title": "Coconut Palm: Static and Streaming Data Series Exploration Now in your\n  Palm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern applications produce massive streams of data series and maintain\nthem in indexes to be able to explore them through nearest neighbor search.\nExisting data series indexes, however, are expensive to operate as they issue\nmany random I/Os to storage. To address this problem, we recently proposed\nCoconut, a new infrastructure that organizes data series based on a new\nsortable format. In this way, Coconut is able to leverage state-of-the-art\nindexing techniques that rely on sorting for the first time to build, maintain\nand query data series indexes using fast sequential I/Os. In this\ndemonstration, we present Coconut Palm, a new exploration tool that allows to\ninteractively combine different indexing techniques from within the Coconut\ninfrastructure and to thereby seamlessly explore data series from across\nvarious scientific domains. We highlight the rich indexing design choices that\nCoconut opens up, and we present a new recommender tool that allows users to\nintelligently navigate them for both static and streaming data exploration\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 02:25:34 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kondylakis", "Haridimos", ""], ["Dayan", "Niv", ""], ["Zoumpatianos", "Kostas", ""], ["Palpanas", "Themis", ""]]}, {"id": "2006.13282", "submitter": "Jialin Ding", "authors": "Jialin Ding and Vikram Nathan and Mohammad Alizadeh and Tim Kraska", "title": "Tsunami: A Learned Multi-dimensional Index for Correlated Data and\n  Skewed Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering data based on predicates is one of the most fundamental operations\nfor any modern data warehouse. Techniques to accelerate the execution of filter\nexpressions include clustered indexes, specialized sort orders (e.g., Z-order),\nmulti-dimensional indexes, and, for high selectivity queries, secondary\nindexes. However, these schemes are hard to tune and their performance is\ninconsistent. Recent work on learned multi-dimensional indexes has introduced\nthe idea of automatically optimizing an index for a particular dataset and\nworkload. However, the performance of that work suffers in the presence of\ncorrelated data and skewed query workloads, both of which are common in real\napplications. In this paper, we introduce Tsunami, which addresses these\nlimitations to achieve up to 6X faster query performance and up to 8X smaller\nindex size than existing learned multi-dimensional indexes, in addition to up\nto 11X faster query performance and 170X smaller index size than\noptimally-tuned traditional indexes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 19:25:51 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ding", "Jialin", ""], ["Nathan", "Vikram", ""], ["Alizadeh", "Mohammad", ""], ["Kraska", "Tim", ""]]}, {"id": "2006.13713", "submitter": "Kostas Zoumpatianos", "authors": "Haridimos Kondylakis, Niv Dayan, Kostas Zoumpatianos, Themis Palpanas", "title": "Coconut: a scalable bottom-up approach for building data series indexes", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.11474", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern applications produce massive amounts of data series that need to\nbe analyzed, requiring efficient similarity search operations. However, the\nstate-of-the-art data series indexes that are used for this purpose do not\nscale well for massive datasets in terms of performance, or storage costs. We\npinpoint the problem to the fact that existing summarizations of data series\nused for indexing cannot be sorted while keeping similar data series close to\neach other in the sorted order. This leads to two design problems. First,\ntraditional bulk-loading algorithms based on sorting cannot be used. Instead,\nindex construction takes place through slow top-down insertions, which create a\nnon-contiguous index that results in many random I/Os. Second, data series\ncannot be sorted and split across nodes evenly based on their median value;\nthus, most leaf nodes are in practice nearly empty. This further slows down\nquery speed and amplifies storage costs. To address these problems, we present\nCoconut. The first innovation in Coconut is an inverted, sortable data series\nsummarization that organizes data series based on a z-order curve, keeping\nsimilar series close to each other in the sorted order. As a result, Coconut is\nable to use bulk-loading techniques that rely on sorting to quickly build a\ncontiguous index using large sequential disk I/Os. We then explore prefix-based\nand median-based splitting policies for bottom-up bulk-loading, showing that\nmedian-based splitting outperforms the state of the art, ensuring that all\nnodes are densely populated. Overall, we show analytically and empirically that\nCoconut dominates the state-of-the-art data series indexes in terms of\nconstruction speed, query speed, and storage costs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 01:52:13 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Kondylakis", "Haridimos", ""], ["Dayan", "Niv", ""], ["Zoumpatianos", "Kostas", ""], ["Palpanas", "Themis", ""]]}, {"id": "2006.14416", "submitter": "Pranav Addepalli", "authors": "Pranav Addepalli, Eric Wu, Douglas Bossart, Christina Lin, Allistar\n  Smith", "title": "SPIDER: Selective Plotting of Interconnected Data and Entity Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligence analysts have long struggled with an abundance of data that must\nbe investigated on a daily basis. In the U.S. Army, this activity involves\nreconciling information from various sources, a process that has been automated\nto a certain extent, but which remains highly manual. To promote automation, a\nsemantic analysis prototype was designed to aid in the intelligence analysis\nprocess. This tool, called Selective Plotting of Interconnected Data and Entity\nRelations (SPIDER), extracts entities and their relationships from text in\norder to streamline investigations. SPIDER is a web application that can be\nremotely-accessed via a web browser, and has three major components: (1) a Java\nAPI that reads documents, extracts entities and relationships using Stanford\nCoreNLP, (2) a Neo4j graph database that stores entities, relationships, and\nproperties; (3) a JavaScript-based SigmaJS visualization tool for displaying\nthe graph on the browser. SPIDER can scale document analysis to thousands of\nfiles for quick visualization, making the intelligence analysis process more\nefficient, and allowing military leadership quicker insights into a vast array\nof potentially-hidden knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 13:59:26 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Addepalli", "Pranav", ""], ["Wu", "Eric", ""], ["Bossart", "Douglas", ""], ["Lin", "Christina", ""], ["Smith", "Allistar", ""]]}, {"id": "2006.15673", "submitter": "Ferdinando Fioretto", "authors": "Ferdinando Fioretto, Pascal Van Hentenryck, Keyu Zhu", "title": "Differential Privacy of Hierarchical Census Data: An Optimization\n  Approach", "comments": "Corrected a claim in the Introduction and a typo in Model 1", "journal-ref": "Artificial Intelligence 296 (2021): 103475", "doi": "10.1016/j.artint.2021.103475", "report-no": null, "categories": "cs.DB cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated by applications of a Census Bureau interested in\nreleasing aggregate socio-economic data about a large population without\nrevealing sensitive information about any individual. The released information\ncan be the number of individuals living alone, the number of cars they own, or\ntheir salary brackets. Recent events have identified some of the privacy\nchallenges faced by these organizations. To address them, this paper presents a\nnovel differential-privacy mechanism for releasing hierarchical counts of\nindividuals. The counts are reported at multiple granularities (e.g., the\nnational, state, and county levels) and must be consistent across all levels.\nThe core of the mechanism is an optimization model that redistributes the noise\nintroduced to achieve differential privacy in order to meet the consistency\nconstraints between the hierarchical levels. The key technical contribution of\nthe paper shows that this optimization problem can be solved in polynomial time\nby exploiting the structure of its cost functions. Experimental results on very\nlarge, real datasets show that the proposed mechanism provides improvements of\nup to two orders of magnitude in terms of computational efficiency and accuracy\nwith respect to other state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 18:19:55 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 20:02:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Fioretto", "Ferdinando", ""], ["Van Hentenryck", "Pascal", ""], ["Zhu", "Keyu", ""]]}, {"id": "2006.15980", "submitter": "Yuanhang Yu", "authors": "Yuanhang Yu, Dong Wen, Ying Zhang, Xiaoyang Wang, Wenjie Zhang and\n  Xuemin Lin", "title": "Efficient Matrix Factorization on Heterogeneous CPU-GPU Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix Factorization (MF) has been widely applied in machine learning and\ndata mining. A large number of algorithms have been studied to factorize\nmatrices. Among them, stochastic gradient descent (SGD) is a commonly used\nmethod. Heterogeneous systems with multi-core CPUs and GPUs have become more\nand more promising recently due to the prevalence of GPUs in general-purpose\ndata-parallel applications. Due to the large computational cost of MF, we aim\nto improve the efficiency of SGD-based MF computation by utilizing the massive\nparallel processing power of heterogeneous multiprocessors. The main challenge\nin parallel SGD algorithms on heterogeneous CPU-GPU systems lies in the\ngranularity of the matrix division and the strategy to assign tasks. We design\na novel strategy to divide the matrix into a set of blocks by considering two\naspects. First, we observe that the matrix should be divided nonuniformly, and\nrelatively large blocks should be assigned to GPUs to saturate the computing\npower of GPUs. In addition to exploiting the characteristics of hardware, the\nworkloads assigned to two types of hardware should be balanced. Aiming at the\nfinal division strategy, we design a cost model tailored for our problem to\naccurately estimate the performance of hardware on different data sizes. A\ndynamic scheduling policy is also used to further balance workloads in\npractice. Extensive experiments show that our proposed algorithm achieves high\nefficiency with a high quality of training quality.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 14:33:39 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Yu", "Yuanhang", ""], ["Wen", "Dong", ""], ["Zhang", "Ying", ""], ["Wang", "Xiaoyang", ""], ["Zhang", "Wenjie", ""], ["Lin", "Xuemin", ""]]}, {"id": "2006.16339", "submitter": "Yongli Zhu", "authors": "Yongli Zhu, Renchang Dai, Guangyi Liu", "title": "Parallel Betweenness Computation in Graph Database for Contingency\n  Selection", "comments": "This paper has been accepted by the 2020 IEEE PES General Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel betweenness computation algorithms are proposed and implemented in a\ngraph database for power system contingency selection. Principles of the graph\ndatabase and graph computing are investigated for both node and edge\nbetweenness computation. Experiments on the 118-bus system and a real power\nsystem show that speed-up can be achieved for both node and edge betweenness\ncomputation while the speeding effect on the latter is more remarkable due to\nthe data retrieving advantages of the graph database on the power network data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 19:45:51 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Zhu", "Yongli", ""], ["Dai", "Renchang", ""], ["Liu", "Guangyi", ""]]}, {"id": "2006.16385", "submitter": "Wenxin Ding", "authors": "Wenxin Ding, Nihar B. Shah, Weina Wang", "title": "On the Privacy-Utility Tradeoff in Peer-Review Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major impediment to research on improving peer review is the unavailability\nof peer-review data, since any release of such data must grapple with the\nsensitivity of the peer review data in terms of protecting identities of\nreviewers from authors. We posit the need to develop techniques to release\npeer-review data in a privacy-preserving manner. Identifying this problem, in\nthis paper we propose a framework for privacy-preserving release of certain\nconference peer-review data -- distributions of ratings, miscalibration, and\nsubjectivity -- with an emphasis on the accuracy (or utility) of the released\ndata. The crux of the framework lies in recognizing that a part of the data\npertaining to the reviews is already available in public, and we use this\ninformation to post-process the data released by any privacy mechanism in a\nmanner that improves the accuracy (utility) of the data while retaining the\nprivacy guarantees. Our framework works with any privacy-preserving mechanism\nthat operates via releasing perturbed data. We present several positive and\nnegative theoretical results, including a polynomial-time algorithm for\nimproving on the privacy-utility tradeoff.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 21:08:21 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Ding", "Wenxin", ""], ["Shah", "Nihar B.", ""], ["Wang", "Weina", ""]]}, {"id": "2006.16393", "submitter": "Ali Hadian", "authors": "Ali Hadian, Behzad Ghaffari, Taiyi Wang, Thomas Heinis", "title": "COAX: Correlation-Aware Indexing on Multidimensional Data with Soft\n  Functional Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work proposed learned index structures, which learn the distribution\nof the underlying dataset to improve performance. The initial work on learned\nindexes has shown that by learning the cumulative distribution function of the\ndata, index structures such as the B-Tree can improve their performance by one\norder of magnitude while having a smaller memory footprint.\n  In this paper, we present COAX, a learned index for multidimensional data\nthat, instead of learning the distribution of keys, learns the correlations\nbetween attributes of the dataset. Our approach is driven by the observation\nthat in many datasets, values of two (or multiple) attributes are correlated.\nCOAX exploits these correlations to reduce the dimensionality of the datasets.\n  More precisely, we learn how to infer one (or multiple) attribute $C_d$ from\nthe remaining attributes and hence no longer need to index attribute $C_d$.\nThis reduces the dimensionality and hence makes the index smaller and more\nefficient.\n  We theoretically investigate the effectiveness of the proposed technique\nbased on the predictability of the FD attributes. We further show\nexperimentally that by predicting correlated attributes in the data, we can\nimprove the query execution time and reduce the memory overhead of the index.\nIn our experiments, we reduce the execution time by 25% while reducing the\nmemory footprint of the index by four orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 21:22:15 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 20:47:00 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 15:43:57 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hadian", "Ali", ""], ["Ghaffari", "Behzad", ""], ["Wang", "Taiyi", ""], ["Heinis", "Thomas", ""]]}, {"id": "2006.16411", "submitter": "Ali Hadian", "authors": "Ali Hadian, Ankit Kumar, Thomas Heinis", "title": "Hands-off Model Integration in Spatial Index Structures", "comments": "Proceedings of the 2nd International Workshop on Applied AI for\n  Database Systems and Applications (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial indexes are crucial for the analysis of the increasing amounts of\nspatial data, for example generated through IoT applications. The plethora of\nindexes that has been developed in recent decades has primarily been optimised\nfor disk. With increasing amounts of memory even on commodity machines,\nhowever, moving them to main memory is an option. Doing so opens up the\nopportunity to use additional optimizations that are only amenable to main\nmemory. In this paper we thus explore the opportunity to use light-weight\nmachine learning models to accelerate queries on spatial indexes. We do so by\nexploring the potential of using interpolation and similar techniques on the\nR-tree, arguably the most broadly used spatial index. As we show in our\nexperimental analysis, the query execution time can be reduced by up to 60%\nwhile simultaneously shrinking the index's memory footprint by over 90%\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 22:05:28 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 19:43:38 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Hadian", "Ali", ""], ["Kumar", "Ankit", ""], ["Heinis", "Thomas", ""]]}, {"id": "2006.16529", "submitter": "Jia Zou", "authors": "Jia Zou, Amitabh Das, Pratik Barhate, Arun Iyengar, Binhang Yuan,\n  Dimitrije Jankov, Chris Jermaine", "title": "Lachesis: Automatic Partitioning for UDF-Centric Analytics", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent partitioning is effective in avoiding expensive shuffling\noperations. However it remains a significant challenge to automate this process\nfor Big Data analytics workloads that extensively use user defined functions\n(UDFs), where sub-computations are hard to be reused for partitionings compared\nto relational applications. In addition, functional dependency that is widely\nutilized for partitioning selection is often unavailable in the unstructured\ndata that is ubiquitous in UDF-centric analytics. We propose the Lachesis\nsystem, which represents UDF-centric workloads as workflows of analyzable and\nreusable sub-computations. Lachesis further adopts a deep reinforcement\nlearning model to infer which sub-computations should be used to partition the\nunderlying data. This analysis is then applied to automatically optimize the\nstorage of the data across applications to improve the performance and users'\nproductivity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 04:49:44 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 02:15:27 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 00:25:31 GMT"}, {"version": "v4", "created": "Sat, 10 Oct 2020 08:43:46 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2021 08:21:08 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zou", "Jia", ""], ["Das", "Amitabh", ""], ["Barhate", "Pratik", ""], ["Iyengar", "Arun", ""], ["Yuan", "Binhang", ""], ["Jankov", "Dimitrije", ""], ["Jermaine", "Chris", ""]]}, {"id": "2006.16551", "submitter": "Xiao Yan", "authors": "Haibo Xiu, Xiao Yan, Xiaoqiang Wang, James Cheng, Lei Cao", "title": "Hierarchical Graph Matching Network for Graph Similarity Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph edit distance / similarity is widely used in many tasks, such as graph\nsimilarity search, binary function analysis, and graph clustering. However,\ncomputing the exact graph edit distance (GED) or maximum common subgraph (MCS)\nbetween two graphs is known to be NP-hard. In this paper, we propose the\nhierarchical graph matching network (HGMN), which learns to compute graph\nsimilarity from data. HGMN is motivated by the observation that two similar\ngraphs should also be similar when they are compressed into more compact\ngraphs. HGMN utilizes multiple stages of hierarchical clustering to organize a\ngraph into successively more compact graphs. At each stage, the earth mover\ndistance (EMD) is adopted to obtain a one-to-one mapping between the nodes in\ntwo graphs (on which graph similarity is to be computed), and a correlation\nmatrix is also derived from the embeddings of the nodes in the two graphs. The\ncorrelation matrices from all stages are used as input for a convolutional\nneural network (CNN), which is trained to predict graph similarity by\nminimizing the mean squared error (MSE). Experimental evaluation on 4 datasets\nin different domains and 4 performance metrics shows that HGMN consistently\noutperforms existing baselines in the accuracy of graph similarity\napproximation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 06:16:19 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Xiu", "Haibo", ""], ["Yan", "Xiao", ""], ["Wang", "Xiaoqiang", ""], ["Cheng", "James", ""], ["Cao", "Lei", ""]]}, {"id": "2006.16723", "submitter": "Hongyuan Mei", "authors": "Hongyuan Mei and Guanghui Qin and Minjie Xu and Jason Eisner", "title": "Neural Datalog Through Time: Informed Temporal Modeling via Logical\n  Specification", "comments": "ICML 2020 camera-ready (new Appendix A.3, rewritten Appendix F)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to predict future events from patterns of past events is\ndifficult when the set of possible event types is large. Training an\nunrestricted neural model might overfit to spurious patterns. To exploit\ndomain-specific knowledge of how past events might affect an event's present\nprobability, we propose using a temporal deductive database to track structured\nfacts over time. Rules serve to prove facts from other facts and from past\nevents. Each fact has a time-varying state---a vector computed by a neural net\nwhose topology is determined by the fact's provenance, including its experience\nof past events. The possible event types at any time are given by special\nfacts, whose probabilities are neurally modeled alongside their states. In both\nsynthetic and real-world domains, we show that neural probabilistic models\nderived from concise Datalog programs improve prediction by encoding\nappropriate domain knowledge in their architecture.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 12:26:04 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 02:58:01 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Mei", "Hongyuan", ""], ["Qin", "Guanghui", ""], ["Xu", "Minjie", ""], ["Eisner", "Jason", ""]]}, {"id": "2006.16869", "submitter": "V\\'ictor Guti\\'errez-Basulto", "authors": "Tomasz Gogacz, V\\'ictor Guti\\'errez-Basulto, Albert Gutowski, Yazm\\'in\n  Ib\\'a\\~nez-Garc\\'ia, Filip Murlak", "title": "On Finite Entailment of Non-Local Queries in Description Logics", "comments": "25 pages, 4 Figures, Accepted at KR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finite entailment of ontology-mediated queries. Going\nbeyond local queries, we allow transitive closure over roles. We focus on\nontologies formulated in the description logics ALCOI and ALCOQ, extended with\ntransitive closure. For both logics, we show 2EXPTIME upper bounds for finite\nentailment of unions of conjunctive queries with transitive closure. We also\nprovide a matching lower bound by showing that finite entailment of conjunctive\nqueries with transitive closure in ALC is 2EXPTIME-hard.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 14:57:12 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Gogacz", "Tomasz", ""], ["Guti\u00e9rrez-Basulto", "V\u00edctor", ""], ["Gutowski", "Albert", ""], ["Ib\u00e1\u00f1ez-Garc\u00eda", "Yazm\u00edn", ""], ["Murlak", "Filip", ""]]}, {"id": "2006.16984", "submitter": "Guillaume Baudart", "authors": "Guillaume Baudart, Peter D. Kirchner, Martin Hirzel, Kiran Kate", "title": "Mining Documentation to Extract Hyperparameter Schemas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI automation tools need machine-readable hyperparameter schemas to define\ntheir search spaces. At the same time, AI libraries often come with good\nhuman-readable documentation. While such documentation contains most of the\nnecessary information, it is unfortunately not ready to consume by tools. This\npaper describes how to automatically mine Python docstrings in AI libraries to\nextract JSON Schemas for their hyperparameters. We evaluate our approach on 119\ntransformers and estimators from three different libraries and find that it is\neffective at extracting machine-readable schemas. Our vision is to reduce the\nburden to manually create and maintain such schemas for AI automation tools and\nbroaden the reach of automation to larger libraries and richer schemas.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 17:32:47 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 21:04:16 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Baudart", "Guillaume", ""], ["Kirchner", "Peter D.", ""], ["Hirzel", "Martin", ""], ["Kate", "Kiran", ""]]}]