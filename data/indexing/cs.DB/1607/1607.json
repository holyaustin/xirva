[{"id": "1607.00542", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang and Senzhang Wang and Qianyi Zhan and Philip S. Yu", "title": "Intertwined Viral Marketing through Online Social Networks", "comments": "11 pages, 5 figures, Accepted by ASONAM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional viral marketing problems aim at selecting a subset of seed users\nfor one single product to maximize its awareness in social networks. However,\nin real scenarios, multiple products can be promoted in social networks at the\nsame time. At the product level, the relationships among these products can be\nquite intertwined, e.g., competing, complementary and independent. In this\npaper, we will study the \"interTwined Influence Maximization\" (i.e., TIM)\nproblem for one product that we target on in online social networks, where\nmultiple other competing/complementary/independent products are being promoted\nsimultaneously. The TIM problem is very challenging to solve due to (1) few\nexisting models can handle the intertwined diffusion procedure of multiple\nproducts concurrently, and (2) optimal seed user selection for the target\nproduct may depend on other products' marketing strategies a lot. To address\nthe TIM problem, a unified greedy framework TIER (interTwined Influence\nEstimatoR) is proposed in this paper. Extensive experiments conducted on four\ndifferent types of real-world social networks demonstrate that TIER can\noutperform all the comparison methods with significant advantages in solving\nthe TIM problem.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 18:00:33 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Zhang", "Jiawei", ""], ["Wang", "Senzhang", ""], ["Zhan", "Qianyi", ""], ["Yu", "Philip S.", ""]]}, {"id": "1607.00655", "submitter": "Erfan Zamanian", "authors": "Erfan Zamanian, Carsten Binnig, Tim Kraska, Tim Harris", "title": "The End of a Myth: Distributed Transactions Can Scale", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common wisdom is that distributed transactions do not scale. But what if\ndistributed transactions could be made scalable using the next generation of\nnetworks and a redesign of distributed databases? There would be no need for\ndevelopers anymore to worry about co-partitioning schemes to achieve decent\nperformance. Application development would become easier as data placement\nwould no longer determine how scalable an application is. Hardware provisioning\nwould be simplified as the system administrator can expect a linear scale-out\nwhen adding more machines rather than some complex sub-linear function, which\nis highly application specific.\n  In this paper, we present the design of our novel scalable database system\nNAM-DB and show that distributed transactions with the very common Snapshot\nIsolation guarantee can indeed scale using the next generation of RDMA-enabled\nnetwork technology without any inherent bottlenecks. Our experiments with the\nTPC-C benchmark show that our system scales linearly to over 6.5 million\nnew-order (14.5 million total) distributed transactions per second on 56\nmachines.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 16:44:48 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 06:22:10 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Zamanian", "Erfan", ""], ["Binnig", "Carsten", ""], ["Kraska", "Tim", ""], ["Harris", "Tim", ""]]}, {"id": "1607.00771", "submitter": "Andrea Detti PhD", "authors": "Andrea Detti, Nicola Blefari Melazzi, Michele Orru, Riccardo Paolillo,\n  Giulio Rossi", "title": "OpenGeoBase: Information Centric Networking meets Spatial Database\n  applications - Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores methodologies, advantages and challenges related to the\nuse of Information Centric Networking (ICN) for realizing distributed spatial\ndatabases. Our findings show that the ICN functionality perfectly fits database\nrequirements: routing-by-name can be used to dispatch queries and insertions,\nin-network caching to accelerate queries, and data-centric security to\nimplement secure multi-tenancy. We present an ICN-based distributed spatial\ndatabase, named OpenGeoBase, and describe its design choices. Thanks to ICN,\nOpenGeoBase can quickly and efficiently provide information to database users;\neasily operate in a distributed way, deploying and using many database engines\nin parallel; secure every piece of content; naturally slice resources, so that\nseveral tenants and users can concurrently and independently use the database.\nWe also show how OpenGeoBase can support a real world Intelligent Transport\nSystem application, by enabling discovery of geo-referenced public\ntransportation information.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 08:36:07 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 10:03:50 GMT"}, {"version": "v3", "created": "Sat, 19 Nov 2016 18:54:08 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Detti", "Andrea", ""], ["Melazzi", "Nicola Blefari", ""], ["Orru", "Michele", ""], ["Paolillo", "Riccardo", ""], ["Rossi", "Giulio", ""]]}, {"id": "1607.00813", "submitter": "Michael Vanden Boom", "authors": "Antoine Amarilli, Michael Benedikt, Pierre Bourhis, Michael Vanden\n  Boom", "title": "Query Answering with Transitive and Linear-Ordered Data", "comments": "36 pages. To appear in IJCAI 2016. Extended version with proofs", "journal-ref": "A journal version of this conference article was published in JAIR\n  (Volume 63, 2018): https://www.jair.org/index.php/jair/article/view/11240", "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider entailment problems involving powerful constraint languages such\nas guarded existential rules, in which additional semantic restrictions are put\non a set of distinguished relations. We consider restricting a relation to be\ntransitive, restricting a relation to be the transitive closure of another\nrelation, and restricting a relation to be a linear order. We give some natural\ngeneralizations of guardedness that allow inference to be decidable in each\ncase, and isolate the complexity of the corresponding decision problems.\nFinally we show that slight changes in our conditions lead to undecidability.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 10:39:58 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Amarilli", "Antoine", ""], ["Benedikt", "Michael", ""], ["Bourhis", "Pierre", ""], ["Boom", "Michael Vanden", ""]]}, {"id": "1607.01046", "submitter": "Olaf Hartig", "authors": "Olaf Hartig and M. Tamer \\\"Ozsu", "title": "Walking without a Map: Optimizing Response Times of Traversal-Based\n  Linked Data Queries (Extended Version)", "comments": "This document is an extended version of a paper published in ISWC\n  2016. In addition to a more detailed discussion of the experimental results\n  presented in the conference version, this extended version provides an\n  in-depth description of our approach to implement traversal-based query\n  execution, and we present a number of additional experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Linked Data on the WWW has spawned research interest in an\nonline execution of declarative queries over this data. A particularly\ninteresting approach is traversal-based query execution which fetches data by\ntraversing data links and, thus, is able to make use of up-to-date data from\ninitially unknown data sources. The downside of this approach is the delay\nbefore the query engine completes a query execution. In this paper, we address\nthis problem by proposing an approach to return as many elements of the result\nset as soon as possible. The basis of this approach is a traversal strategy\nthat aims to fetch result-relevant data as early as possible. The challenge for\nsuch a strategy is that the query engine does not know a priori which of the\ndata sources that will be discovered during the query execution contain\nresult-relevant data. We introduce 16 different traversal approaches and\nexperimentally study their impact on response times. Our experiments show that\nsome of the approaches can achieve significant improvements over the baseline\nof looking up URIs on a first-come, first-served basis. Additionally, we verify\nthe importance of these approaches by showing that typical query optimization\ntechniques that focus solely on the process of constructing the query result\ncannot have any significant impact on the response times of traversal-based\nquery executions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 21:05:01 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Hartig", "Olaf", ""], ["\u00d6zsu", "M. Tamer", ""]]}, {"id": "1607.02018", "submitter": "Daniel P. Lupp", "authors": "Daniel P. Lupp and Evgenij Thorstensen", "title": "Mapping Data to Ontologies with Exceptions Using Answer Set Programming", "comments": "8 pages, ONTOLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ontology-based data access, databases are connected to an ontology via\nmappings from queries over the database to queries over the ontology. In this\npaper, we consider mappings from relational databases to first-order\nontologies, and define an ASP-based framework for GLAV mappings with queries\nover the ontology in the mapping rule bodies. We show that this type of\nmappings can be used to express constraints and exceptions, as well as being a\npowerful mechanism for succinctly representing OBDA mappings. We give an\nalgorithm for brave reasoning in this setting, and show that this problem has\neither the same data complexity as ASP (NP- complete), or it is at least as\nhard as the complexity of checking entailment for the ontology queries.\nFurthermore, we show that for ontologies with UCQ-rewritable queries there\nexists a natural reduction from mapping programs to \\exists-ASP, an extension\nof ASP with existential variables that itself admits a natural reduction to\nASP.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 14:00:06 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Lupp", "Daniel P.", ""], ["Thorstensen", "Evgenij", ""]]}, {"id": "1607.02399", "submitter": "Alexander Hinneburg", "authors": "Frank Rosner, Alexander Hinneburg", "title": "Translating Bayesian Networks into Entity Relationship Models, Extended\n  Version", "comments": "This is an extended version of a short paper published in the\n  Proceedings of the 35th International Conference on Conceptual Modeling, ER\n  2016. In addition to a more detailed discussion of the method, this extended\n  version describes a case study that applies the method as well as first ideas\n  of a conceptual framework for developing big data analytics applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analytics applications drive the convergence of data management and\nmachine learning. But there is no conceptual language available that is spoken\nin both worlds. The main contribution of the paper is a method to translate\nBayesian networks, a main conceptual language for probabilistic graphical\nmodels, into usable entity relationship models. The transformed representation\nof a Bayesian network leaves out mathematical details about probabilistic\nrelationships but unfolds all information relevant for data management tasks.\nAs a real world example, we present the TopicExplorer system that uses Bayesian\ntopic models as a core component in an interactive, database-supported web\napplication. Last, we sketch a conceptual framework that eases machine learning\nspecific development tasks while building big data analytics applications.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 15:06:46 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Rosner", "Frank", ""], ["Hinneburg", "Alexander", ""]]}, {"id": "1607.02561", "submitter": "Cong Yan", "authors": "Cong Yan, Alvin Cheung, Shan Lu", "title": "Database-Backed Web Applications in the Wild: How Well Do They Work?", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern database-backed web applications are built upon Object Relational\nMapping (ORM) frameworks. While ORM frameworks ease application development by\nabstracting persistent data as objects, such convenience often comes with a\nperformance cost. In this paper, we present CADO, a tool that analyzes the\napplication logic and its interaction with databases using the Ruby on Rails\nORM framework. CADO includes a static program analyzer, a profiler and a\nsynthetic data generator to extract and understand application's performance\ncharacteristics. We used CADO to analyze the performance problems of 27\nreal-world open-source Rails applications, covering domains such as online\nforums, e-commerce, project management, blogs, etc. Based on the results, we\nuncovered a number of issues that lead to sub-optimal application performance,\nranging from issuing queries, how result sets are used, and physical design. We\nsuggest possible remedies for each issue, and highlight new research\nopportunities that arise from them.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 02:40:37 GMT"}, {"version": "v2", "created": "Sat, 16 Jul 2016 01:01:43 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 21:15:15 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Yan", "Cong", ""], ["Cheung", "Alvin", ""], ["Lu", "Shan", ""]]}, {"id": "1607.02669", "submitter": "Hasan Jamil", "authors": "Carlos R. Rivero and Hasan M. Jamil", "title": "A Novel Model for Distributed Big Data Service Composition using\n  Stratified Functional Graph Matching", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A significant number of current industrial applications rely on web services.\nA cornerstone task in these applications is discovering a suitable service that\nmeets the threshold of some user needs. Then, those services can be composed to\nperform specific functionalities. We argue that the prevailing approach to\ncompose services based on the \"all or nothing\" paradigm is limiting and leads\nto exceedingly high rejection of potentially suitable services. Furthermore,\ncontemporary models do not allow \"mix and match\" composition from atomic\nservices of different composite services when binary matching is not possible\nor desired. In this paper, we propose a new model for service composition based\non \"stratified graph summarization\" and \"service stitching\". We discuss the\nlimitations of existing approaches with a motivating example, present our\napproach to overcome these limitations, and outline a possible architecture for\nservice composition from atomic services. Our thesis is that, with the advent\nof Big Data, our approach will reduce latency in service discovery, and will\nimprove efficiency and accuracy of matchmaking and composition of services.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 22:04:39 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Rivero", "Carlos R.", ""], ["Jamil", "Hasan M.", ""]]}, {"id": "1607.02682", "submitter": "Mostafa Milani", "authors": "Mostafa Milani and Leopoldo Bertossi", "title": "Extending Weakly-Sticky Datalog+/-: Query-Answering Tractability and\n  Optimizations", "comments": "Extended version of RR'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-sticky (WS) Datalog+/- is an expressive member of the family of\nDatalog+/- programs that is based on the syntactic notions of stickiness and\nweak-acyclicity. Query answering over the WS programs has been investigated,\nbut there is still much work to do on the design and implementation of\npractical query answering (QA) algorithms and their optimizations. Here, we\nstudy sticky and WS programs from the point of view of the behavior of the\nchase procedure, extending the stickiness property of the chase to that of\ngeneralized stickiness of the chase (gsch-property). With this property we\nspecify the semantic class of GSCh programs, which includes sticky and WS\nprograms, and other syntactic subclasses that we identify. In particular, we\nintroduce joint-weakly-sticky (JWS) programs, that include WS programs. We also\npropose a bottom-up QA algorithm for a range of subclasses of GSCh. The\nalgorithm runs in polynomial time (in data) for JWS programs. Unlike the WS\nclass, JWS is closed under a general magic-sets rewriting procedure for the\noptimization of programs with existential rules. We apply the magic-sets\nrewriting in combination with the proposed QA algorithm for the optimization of\nQA over JWS programs.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 02:56:33 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Milani", "Mostafa", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1607.02988", "submitter": "Luigi Santocanale", "authors": "Luigi Santocanale (LIF)", "title": "The quasiequational theory of relational lattices, in the pure lattice\n  signature (embeddability into relational lattices is undecidable)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB math.CT math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural join and the inner union operations combine relations of a\ndatabase. Tropashko and Spight realized that these two operations are themeet\nand join operations in a class of lattices, known by now as the relational\nlattices. They proposed then lattice theory as an algebraic approach to\nthetheory of databases alternative to the relational algebra. Litak et al.\nproposed an axiomatization of relational lattices over the signature that\nextends thepure lattice signature with a constant and argued that the\nquasiequational theory of relational lattices over this extended signature is\nundecidable.We prove in this paper that embeddability is undecidable for\nrelational lattices. More precisely, it is undecidable whether a finite\nsubdirectly-irreduciblelattice can be embedded into a relational lattice. Our\nproof is a reduction from the coverability problem of a multimodal frame by a\nuniversal product frameand, indirectly, from the representability problem for\nrelation algebras. As corollaries we obtain the following results: the\nquasiequational theoryof relational lattices over the pure lattice signature is\nundecidable and has no finite base; there is a quasiequation over the pure\nlattice signature which holds in all the finite relational lattices but fails\nin an infinite relational lattice.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 14:59:42 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 13:40:54 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 09:34:21 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Santocanale", "Luigi", "", "LIF"]]}, {"id": "1607.03306", "submitter": "Enmei Tu", "authors": "Shangbo Mao, Enmei Tu, Guanghao Zhang, Lily Rachmawati, Eshan\n  Rajabally, Guang-Bin Huang", "title": "An Automatic Identification System (AIS) Database for Maritime\n  Trajectory Prediction and Data Mining", "comments": "Accepted in ELM2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, maritime safety and efficiency become more and more\nimportant across the world. Automatic Identification System (AIS) tracks vessel\nmovement by onboard transceiver and terrestrial and/or satellite base station.\nThe data collected by AIS contains broadcast kinematic information and static\ninformation. Both of them are useful for anomaly detection and route prediction\nwhich are key techniques in intelligent maritime research area. This paper is\ndevoted to construct a standard AIS database for maritime trajectory learning,\nprediction and data mining. A path prediction algorithm is tested on this AIS\ndatabase and the testing results show this database can be used as a\nstandardized training resource for different trajectory prediction algorithms\nand other AIS data mining algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 10:49:59 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 05:25:31 GMT"}, {"version": "v3", "created": "Sun, 24 Jul 2016 07:27:31 GMT"}, {"version": "v4", "created": "Thu, 13 Oct 2016 06:56:55 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Mao", "Shangbo", ""], ["Tu", "Enmei", ""], ["Zhang", "Guanghao", ""], ["Rachmawati", "Lily", ""], ["Rajabally", "Eshan", ""], ["Huang", "Guang-Bin", ""]]}, {"id": "1607.04104", "submitter": "James Cheney", "authors": "Stefan Fehrenbach and James Cheney", "title": "Language-integrated provenance", "comments": "Accepted to Science of Computer Programming special issue on PPDP\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance, or information about the origin or derivation of data, is\nimportant for assessing the trustworthiness of data and identifying and\ncorrecting mistakes. Most prior implementations of data provenance have\ninvolved heavyweight modifications to database systems and little attention has\nbeen paid to how the provenance data can be used outside such a system. We\npresent extensions to the Links programming language that build on its support\nfor language-integrated query to support provenance queries by rewriting and\nnormalizing monadic comprehensions and extending the type system to distinguish\nprovenance metadata from normal data. The main contribution of this article is\nto show that the two most common forms of provenance can be implemented\nefficiently and used safely as a programming language feature with no changes\nto the database system.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 12:20:11 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 19:42:19 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 15:57:54 GMT"}, {"version": "v4", "created": "Tue, 22 Aug 2017 09:42:04 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Fehrenbach", "Stefan", ""], ["Cheney", "James", ""]]}, {"id": "1607.04822", "submitter": "Shumo Chu", "authors": "Shumo Chu, Konstantin Weitz, Alvin Cheung, Dan Suciu", "title": "HoTTSQL: Proving Query Rewrites with Univalent SQL Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every database system contains a query optimizer that performs query\nrewrites. Unfortunately, developing query optimizers remains a highly\nchallenging task. Part of the challenges comes from the intricacies and rich\nfeatures of query languages, which makes reasoning about rewrite rules\ndifficult. In this paper, we propose a machine-checkable denotational semantics\nfor SQL, the de facto language for relational database, for rigorously\nvalidating rewrite rules. Unlike previously proposed semantics that are either\nnon-mechanized or only cover a small amount of SQL language features, our\nsemantics covers all major features of SQL, including bags, correlated\nsubqueries, aggregation, and indexes. Our mechanized semantics, called HoTTSQL,\nis based on K-Relations and homotopy type theory, where we denote relations as\nmathematical functions from tuples to univalent types. We have implemented\nHoTTSQL in Coq, which takes only fewer than 300 lines of code and have proved a\nwide range of SQL rewrite rules, including those from database research\nliterature (e.g., magic set rewrites) and real-world query optimizers (e.g.,\nsubquery elimination). Several of these rewrite rules have never been\npreviously proven correct. In addition, while query equivalence is generally\nundecidable, we have implemented an automated decision procedure using HoTTSQL\nfor conjunctive queries: a well-studied decidable fragment of SQL that\nencompasses many real-world queries.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 03:15:20 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 20:44:47 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Chu", "Shumo", ""], ["Weitz", "Konstantin", ""], ["Cheung", "Alvin", ""], ["Suciu", "Dan", ""]]}, {"id": "1607.05351", "submitter": "Dmitriy Zheleznyakov", "authors": "Evgeny Kharlamov, Yannis Kotidis, Theofilos Mailis, Christian\n  Neuenstadt, Charalampos Nikolaou, \\\"Ozg\\\"ur \\\"Ozcep, Christoforos Svingos,\n  Dmitriy Zheleznyakov, Sebastian Brandt, Ian Horrocks, Yannis Ioannidis,\n  Steffen Lamparter, Ralf M\\\"oller", "title": "Towards Analytics Aware Ontology Based Access to Static and Streaming\n  Data (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time analytics that requires integration and aggregation of\nheterogeneous and distributed streaming and static data is a typical task in\nmany industrial scenarios such as diagnostics of turbines in Siemens. OBDA\napproach has a great potential to facilitate such tasks; however, it has a\nnumber of limitations in dealing with analytics that restrict its use in\nimportant industrial applications. Based on our experience with Siemens, we\nargue that in order to overcome those limitations OBDA should be extended and\nbecome analytics, source, and cost aware. In this work we propose such an\nextension. In particular, we propose an ontology, mapping, and query language\nfor OBDA, where aggregate and other analytical functions are first class\ncitizens. Moreover, we develop query optimisation techniques that allow to\nefficiently process analytical tasks over static and streaming data. We\nimplement our approach in a system and evaluate our system with Siemens turbine\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 23:23:21 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 12:06:28 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Kharlamov", "Evgeny", ""], ["Kotidis", "Yannis", ""], ["Mailis", "Theofilos", ""], ["Neuenstadt", "Christian", ""], ["Nikolaou", "Charalampos", ""], ["\u00d6zcep", "\u00d6zg\u00fcr", ""], ["Svingos", "Christoforos", ""], ["Zheleznyakov", "Dmitriy", ""], ["Brandt", "Sebastian", ""], ["Horrocks", "Ian", ""], ["Ioannidis", "Yannis", ""], ["Lamparter", "Steffen", ""], ["M\u00f6ller", "Ralf", ""]]}, {"id": "1607.05412", "submitter": "Lu\\'is Cruz-Filipe", "authors": "Lu\\'is Cruz-Filipe", "title": "Grounded Fixpoints and Active Integrity Constraints", "comments": null, "journal-ref": null, "doi": "10.4230/OASIcs.ICLP.2016.11", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The formalism of active integrity constraints was introduced as a way to\nspecify particular classes of integrity constraints over relational databases\ntogether with preferences on how to repair existing inconsistencies. The\nrule-based syntax of such integrity constraints also provides algorithms for\nfinding such repairs that achieve the best asymptotic complexity. However, the\ndifferent semantics that have been proposed for these integrity constraints all\nexhibit some counter-intuitive examples. In this work, we look at active\nintegrity constraints using ideas from algebraic fixpoint theory. We show how\ndatabase repairs can be modeled as fixpoints of particular operators on\ndatabases, and study how the notion of grounded fixpoint induces a\ncorresponding notion of grounded database repair that captures several natural\nintuitions, and in particular avoids the problems of previous alternative\nsemantics. In order to study grounded repairs in their full generality, we need\nto generalize the notion of grounded fixpoint to non-deterministic operators.\nWe propose such a definition and illustrate its plausibility in the database\ncontext.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 05:57:05 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""]]}, {"id": "1607.05538", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Silviu Maniu, Mika\\\"el Monet", "title": "Challenges for Efficient Query Evaluation on Structured Probabilistic\n  Data", "comments": "9 pages, 1 figure, 23 references. Accepted for publication at SUM\n  2016", "journal-ref": null, "doi": "10.1007/978-3-319-45856-4_22", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query answering over probabilistic data is an important task but is generally\nintractable. However, a new approach for this problem has recently been\nproposed, based on structural decompositions of input databases, following,\ne.g., tree decompositions. This paper presents a vision for a database\nmanagement system for probabilistic data built following this structural\napproach. We review our existing and ongoing work on this topic and highlight\nmany theoretical and practical challenges that remain to be addressed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 12:15:37 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Maniu", "Silviu", ""], ["Monet", "Mika\u00ebl", ""]]}, {"id": "1607.05702", "submitter": "Fereidoon Sadri", "authors": "Fereidoon Sadri and Gayatri Tallur", "title": "Integration of Probabilistic Uncertain Information", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of data integration from sources that contain\nprobabilistic uncertain information. Data is modeled by possible-worlds with\nprobability distribution, compactly represented in the probabilistic relation\nmodel. Integration is achieved efficiently using the extended probabilistic\nrelation model. We study the problem of determining the probability\ndistribution of the integration result. It has been shown that, in general,\nonly probability ranges can be determined for the result of integration. In\nthis paper we concentrate on a subclass of extended probabilistic relations,\nthose that are obtainable through integration. We show that under intuitive and\nreasonable assumptions we can determine the exact probability distribution of\nthe result of integration.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 19:18:08 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Sadri", "Fereidoon", ""], ["Tallur", "Gayatri", ""]]}, {"id": "1607.06063", "submitter": "Mohammad Reza Abbasifard", "authors": "Mohammad Reza Abbasifard and Omid Isfahani Alamdari", "title": "Fragment Allocation Configuration in Distributed Database Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": "MODB-201607DDB", "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed database (DDB) management systems, fragment allocation is one\nof the most important components that can directly affect the performance of\nDDB. In this research work, we will show that declarative programming\nlanguages, e.g. logic programming languages, can be used to represent different\ndata fragment allocation techniques. Results indicate that, using declarative\nprogramming language significantly simplifies the representation of fragment\nallocation algorithm, thus opens door for any further developments and\noptimizations. The under consideration case study also show that our approach\ncan be extended to be used in different areas of distributed systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 19:10:48 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Abbasifard", "Mohammad Reza", ""], ["Alamdari", "Omid Isfahani", ""]]}, {"id": "1607.06179", "submitter": "Ninh Pham", "authors": "Ninh Pham", "title": "Hybrid LSH: Faster Near Neighbors Reporting in High-dimensional Space", "comments": "Accepted as a short paper in EDBT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the $r$-near neighbors reporting problem ($r$-NN), i.e., reporting\n\\emph{all} points in a high-dimensional point set $S$ that lie within a radius\n$r$ of a given query point $q$. Our approach builds upon on the\nlocality-sensitive hashing (LSH) framework due to its appealing asymptotic\nsublinear query time for near neighbor search problems in high-dimensional\nspace. A bottleneck of the traditional LSH scheme for solving $r$-NN is that\nits performance is sensitive to data and query-dependent parameters. On\ndatasets whose data distributions have diverse local density patterns, LSH with\ninappropriate tuning parameters can sometimes be outperformed by a simple\nlinear search.\n  In this paper, we introduce a hybrid search strategy between LSH-based search\nand linear search for $r$-NN in high-dimensional space. By integrating an\nauxiliary data structure into LSH hash tables, we can efficiently estimate the\ncomputational cost of LSH-based search for a given query regardless of the data\ndistribution. This means that we are able to choose the appropriate search\nstrategy between LSH-based search and linear search to achieve better\nperformance. Moreover, the integrated data structure is time efficient and fits\nwell with many recent state-of-the-art LSH-based approaches. Our experiments on\nreal-world datasets show that the hybrid search approach outperforms (or is\ncomparable to) both LSH-based search and linear search for a wide range of\nsearch radii and data distributions in high-dimensional space.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 03:24:06 GMT"}, {"version": "v2", "created": "Sat, 7 Jan 2017 02:28:08 GMT"}, {"version": "v3", "created": "Tue, 28 Mar 2017 12:30:35 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Pham", "Ninh", ""]]}, {"id": "1607.06343", "submitter": "Davide Lanti", "authors": "Davide Lanti, Guohui Xiao, Diego Calvanese", "title": "Data Scaling in OBDA Benchmarks: The VIG Approach", "comments": "Typo 1: ShallowWellbore -> DevelopmentWellbore Typo 2: f(fname) [page\n  4] -> f(fid)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe VIG, a data scaler for benchmarks in the context of\nontology-based data access (OBDA). Data scaling is a relatively recent\napproach, proposed in the database community, that allows for quickly scaling\nup an input data instance to s times its size, while preserving certain\napplication-specific characteristics. The advantage of the approach is that the\nuser is not required to manually input the characteristics of the data to be\nproduced, making it particularly suitable for OBDA benchmarks, where the\ncomplexity of database schemas might pose a challenge for manual input (e.g.,\nthe NPD benchmark contains 70 tables with some containing more than 60\ncolumns). As opposed to a traditional data scaler, VIG includes domain\ninformation provided by the OBDA mappings and the ontology in order to produce\ndata. VIG is currently used in the NPD benchmark, but it is not NPD-specific\nand can be seeded with any data instance. The distinguishing features of VIG\nare (1) its simple and clear generation strategy; (2) its efficiency, as each\nvalue is generated in constant time, without accesses to the disk or to RAM to\nretrieve previously generated values; (3) and its generality, as the data is\nexported in CSV files that can be easily imported by any RDBMS system. VIG is a\njava implementation licensed under Apache 2.0, and its source code is available\non GitHub (https://github.com/ontop/vig) in the form of a Maven project. The\ncode is being maintained since two years by the -ontop- team at the Free\nUniversity of Bozen-Bolzano.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 14:37:31 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 13:43:42 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Lanti", "Davide", ""], ["Xiao", "Guohui", ""], ["Calvanese", "Diego", ""]]}, {"id": "1607.07249", "submitter": "Joern Hees", "authors": "J\\\"orn Hees, Rouven Bauer, Joachim Folz, Damian Borth and Andreas\n  Dengel", "title": "An Evolutionary Algorithm to Learn SPARQL Queries for\n  Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia", "comments": "15 pages, 2 figures, as of 2016-09-13\n  6a19d5d7020770dc0711081ce2c1e52f71bf4b86", "journal-ref": null, "doi": "10.1007/978-3-319-49004-5_22", "report-no": null, "categories": "cs.AI cs.DB cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient usage of the knowledge provided by the Linked Data community is\noften hindered by the need for domain experts to formulate the right SPARQL\nqueries to answer questions. For new questions they have to decide which\ndatasets are suitable and in which terminology and modelling style to phrase\nthe SPARQL query.\n  In this work we present an evolutionary algorithm to help with this\nchallenging task. Given a training list of source-target node-pair examples our\nalgorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The\nlearned patterns can be visualised to form the basis for further investigation,\nor they can be used to predict target nodes for new source nodes.\n  Amongst others, we apply our algorithm to a dataset of several hundred human\nassociations (such as \"circle - square\") to find patterns for them in DBpedia.\nWe show the scalability of the algorithm by running it against a SPARQL\nendpoint loaded with > 7.9 billion triples. Further, we use the resulting\nSPARQL queries to mimic human associations with a Mean Average Precision (MAP)\nof 39.9 % and a Recall@10 of 63.9 %.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 12:47:38 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 12:13:14 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 10:27:06 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Hees", "J\u00f6rn", ""], ["Bauer", "Rouven", ""], ["Folz", "Joachim", ""], ["Borth", "Damian", ""], ["Dengel", "Andreas", ""]]}, {"id": "1607.07967", "submitter": "Xiaowang Zhang", "authors": "Xiaowang Zhang and Zhenyu Song and Zhiyong Feng and Xin Wang", "title": "PIWD: A Plugin-based Framework for Well-Designed SPARQL", "comments": "16 pages", "journal-ref": "In Proceedings of the 2016 Joint International Semantic Technology\n  Conference (JIST 2016), 2 - 4 Nov, 2016, Singapore", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world datasets (e.g.,DBpedia query log), queries built on\nwell-designed patterns containing only AND and OPT operators (for short,\nWDAO-patterns) account for a large proportion among all SPARQL queries. In this\npaper, we present a plugin-based framework for all SELECT queries built on\nWDAO-patterns, named PIWD. The framework is based on a parse tree called\n\\emph{well-designed AND-OPT tree} (for short, WDAO-tree) whose leaves are basic\ngraph patterns (BGP) and inner nodes are the OPT operators. We prove that for\nany WDAO-pattern, its parse tree can be equivalently transformed into a\nWDAO-tree. Based on the proposed framework, we can employ any query engine to\nevaluate BGP for evaluating queries built on WDAO-patterns in a convenient way.\nTheoretically, we can reduce the query evaluation of WDAO-patterns to subgraph\nhomomorphism as well as BGP since the query evaluation of BGP is equivalent to\nsubgraph homomorphism. Finally, our preliminary experiments on gStore and\nRDF-3X show that PIWD can answer all queries built on WDAO-patterns effectively\nand efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 05:47:34 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 04:40:42 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Zhang", "Xiaowang", ""], ["Song", "Zhenyu", ""], ["Feng", "Zhiyong", ""], ["Wang", "Xin", ""]]}, {"id": "1607.08098", "submitter": "Carlos Leandro", "authors": "Carlos Leandro and Helder Pita and Lu\\'is Monteiro", "title": "The Actias system: supervised multi-strategy learning paradigm using\n  categorical logic", "comments": "9 pages, 6 figures, conference ICKEDS'04", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most difficult problems in the development of intelligent systems\nis the construction of the underlying knowledge base. As a consequence, the\nrate of progress in the development of this type of system is directly related\nto the speed with which knowledge bases can be assembled, and on its quality.\nWe attempt to solve the knowledge acquisition problem, for a Business\nInformation System, developing a supervised multistrategy learning paradigm.\nThis paradigm is centred on a collaborative data mining strategy, where groups\nof experts collaborate using data-mining process on the supervised acquisition\nof new knowledge extracted from heterogeneous machine learning data models.\n  The Actias system is our approach to this paradigm. It is the result of\napplying the graphic logic based language of sketches to knowledge integration.\nThe system is a data mining collaborative workplace, where the Information\nSystem knowledge base is an algebraic structure. It results from the\nintegration of background knowledge with new insights extracted from data\nmodels, generated for specific data modelling tasks, and represented as rules\nusing the sketches language.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 20:04:40 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Leandro", "Carlos", ""], ["Pita", "Helder", ""], ["Monteiro", "Lu\u00eds", ""]]}, {"id": "1607.08188", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff", "title": "Online Trajectory Segmentation and Summary With Applications to\n  Visualization and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory segmentation is the process of subdividing a trajectory into parts\neither by grouping points similar with respect to some measure of interest, or\nby minimizing a global objective function. Here we present a novel online\nalgorithm for segmentation and summary, based on point density along the\ntrajectory, and based on the nature of the naturally occurring structure of\nintermittent bouts of locomotive and local activity. We show an application to\nvisualization of trajectory datasets, and discuss the use of the summary as an\nindex allowing efficient queries which are otherwise impossible or\ncomputationally expensive, over very large datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 14:50:45 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Resheff", "Yehezkel S.", ""]]}, {"id": "1607.08325", "submitter": "Gianmarco De Francisci Morales", "authors": "Nicolas Kourtellis and Gianmarco De Francisci Morales and Albert Bifet\n  and Arinto Murdopo", "title": "VHT: Vertical Hoeffding Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT Big Data requires new machine learning methods able to scale to large\nsize of data arriving at high speed. Decision trees are popular machine\nlearning models since they are very effective, yet easy to interpret and\nvisualize. In the literature, we can find distributed algorithms for learning\ndecision trees, and also streaming algorithms, but not algorithms that combine\nboth features. In this paper we present the Vertical Hoeffding Tree (VHT), the\nfirst distributed streaming algorithm for learning decision trees. It features\na novel way of distributing decision trees via vertical parallelism. The\nalgorithm is implemented on top of Apache SAMOA, a platform for mining\ndistributed data streams, and thus able to run on real-world clusters. We run\nseveral experiments to study the accuracy and throughput performance of our new\nVHT algorithm, as well as its ability to scale while keeping its superior\nperformance with respect to non-distributed decision trees.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 06:15:24 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Kourtellis", "Nicolas", ""], ["Morales", "Gianmarco De Francisci", ""], ["Bifet", "Albert", ""], ["Murdopo", "Arinto", ""]]}, {"id": "1607.08329", "submitter": "Jiongqian Liang", "authors": "Jiongqian Liang and Srinivasan Parthasarathy", "title": "Robust Contextual Outlier Detection: Where Context Meets Sparsity", "comments": "11 pages. Extended version of CIKM'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is a fundamental data science task with applications\nranging from data cleaning to network security. Given the fundamental nature of\nthe task, this has been the subject of much research. Recently, a new class of\noutlier detection algorithms has emerged, called {\\it contextual outlier\ndetection}, and has shown improved performance when studying anomalous behavior\nin a specific context. However, as we point out in this article, such\napproaches have limited applicability in situations where the context is sparse\n(i.e. lacking a suitable frame of reference). Moreover, approaches developed to\ndate do not scale to large datasets. To address these problems, here we propose\na novel and robust approach alternative to the state-of-the-art called RObust\nContextual Outlier Detection (ROCOD). We utilize a local and global behavioral\nmodel based on the relevant contexts, which is then integrated in a natural and\nrobust fashion. We also present several optimizations to improve the\nscalability of the approach. We run ROCOD on both synthetic and real-world\ndatasets and demonstrate that it outperforms other competitive baselines on the\naxes of efficacy and efficiency (40X speedup compared to modern contextual\noutlier detection methods). We also drill down and perform a fine-grained\nanalysis to shed light on the rationale for the performance gains of ROCOD and\nreveal its effectiveness when handling objects with sparse contexts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 06:40:30 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 03:47:51 GMT"}, {"version": "v3", "created": "Thu, 22 Dec 2016 21:52:12 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Liang", "Jiongqian", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1607.08681", "submitter": "Dingming Wu", "authors": "Dingming Wu, Christian S. Jensen", "title": "A Density-Based Approach to the Retrieval of Top-K Spatial Textual\n  Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword-based web queries with local intent retrieve web content that is\nrelevant to supplied keywords and that represent points of interest that are\nnear the query location. Two broad categories of such queries exist. The first\nencompasses queries that retrieve single spatial web objects that each satisfy\nthe query arguments. Most proposals belong to this category. The second\ncategory, to which this paper's proposal belongs, encompasses queries that\nsupport exploratory user behavior and retrieve sets of objects that represent\nregions of space that may be of interest to the user. Specifically, the paper\nproposes a new type of query, namely the top-k spatial textual clusters (k-STC)\nquery that returns the top-k clusters that (i) are located the closest to a\ngiven query location, (ii) contain the most relevant objects with regard to\ngiven query keywords, and (iii) have an object density that exceeds a given\nthreshold. To compute this query, we propose a basic algorithm that relies on\non-line density-based clustering and exploits an early stop condition. To\nimprove the response time, we design an advanced approach that includes three\ntechniques: (i) an object skipping rule, (ii) spatially gridded posting lists,\nand (iii) a fast range query algorithm. An empirical study on real data\ndemonstrates that the paper's proposals offer scalability and are capable of\nexcellent performance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 03:15:58 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Wu", "Dingming", ""], ["Jensen", "Christian S.", ""]]}]