[{"id": "1611.00423", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Qin Zhang", "title": "Computing Skylines on Distributed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study skyline queries in the distributed computational\nmodel, where we have $s$ remote sites and a central coordinator (the query\nnode); each site holds a piece of data, and the coordinator wants to compute\nthe skyline of the union of the $s$ datasets. The computation is in terms of\nrounds, and the goal is to minimize both the total communication cost and the\nround cost.\n  Viewing data objects as points in the Euclidean space, we consider both the\nhorizontal data partition case where each site holds a subset of points, and\nthe vertical data partition case where each site holds one coordinate of all\nthe points. We give a set of algorithms that have provable theoretical\nguarantees, and complement them with information theoretical lower bounds. We\nalso demonstrate the superiority of our algorithms over existing heuristics by\nan extensive set of experiments on both synthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 23:41:03 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Zhang", "Haoyu", ""], ["Zhang", "Qin", ""]]}, {"id": "1611.00484", "submitter": "Yangli-Ao Geng", "authors": "Yangli-ao Geng, Qingyong Li, Rong Zheng, Fuzhen Zhuang, Ruisi He,\n  Naixue Xiong", "title": "RECOME: a New Density-Based Clustering Algorithm Using Relative KNN\n  Kernel Density", "comments": "The manuscript has been updated to the latest version. The official\n  version please see\n  https://www.sciencedirect.com/science/article/pii/S0020025518300215 The code\n  has been released in\n  https://github.com/gyla1993/RECOME-A-new-density-based-clustering-algorithm-using-relative-KNN-kernel-density", "journal-ref": "Information Sciences 436 (2018): 13-30", "doi": "10.1016/j.ins.2018.01.013", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discovering clusters from a dataset with different shapes, densities, and\nscales is a known challenging problem in data clustering. In this paper, we\npropose the RElative COre MErge (RECOME) clustering algorithm. The core of\nRECOME is a novel density measure, i.e., Relative $K$ nearest Neighbor Kernel\nDensity (RNKD). RECOME identifies core objects with unit RNKD, and {partitions}\nnon-core objects into atom clusters by successively following higher-density\nneighbor relations toward core objects. Core objects and their corresponding\natom clusters are then merged through $\\alpha$-reachable paths on a KNN graph.\nWe discover that the number of clusters computed by RECOME is a step function\nof the $\\alpha$ parameter with jump discontinuity on a small collection of\nvalues. A fast jump discontinuity discovery (FJDD) method is proposed based on\ngraph theory. RECOME is evaluated on both synthetic datasets and real datasets.\nExperimental results indicate that RECOME is able to discover clusters with\ndifferent shapes, densities, and scales. It outperforms six baseline methods on\nboth synthetic datasets and real datasets. Moreover, FJDD is shown to be\neffective to extract the jump discontinuity set of parameter $\\alpha$ for all\ntested datasets, which can ease the task of data exploration and parameter\ntuning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 07:10:40 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 02:58:41 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 05:13:17 GMT"}, {"version": "v4", "created": "Wed, 16 Sep 2020 19:47:43 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Geng", "Yangli-ao", ""], ["Li", "Qingyong", ""], ["Zheng", "Rong", ""], ["Zhuang", "Fuzhen", ""], ["He", "Ruisi", ""], ["Xiong", "Naixue", ""]]}, {"id": "1611.00547", "submitter": "Dario Garcia-Gasulla", "authors": "Dario Garcia-Gasulla, Eduard Ayguad\\'e, Jes\\'us Labarta, Ulises\n  Cort\\'es", "title": "Limitations and Alternatives for the Evaluation of Large-scale Link\n  Prediction", "comments": "Submitted to New Generation Computing, 15 pages, 4 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction, the problem of identifying missing links among a set of\ninter-related data entities, is a popular field of research due to its\napplication to graph-like domains. Producing consistent evaluations of the\nperformance of the many link prediction algorithms being proposed can be\nchallenging due to variable graph properties, such as size and density. In this\npaper we first discuss traditional data mining solutions which are applicable\nto link prediction evaluation, arguing about their capacity for producing\nfaithful and useful evaluations. We also introduce an innovative modification\nto a traditional evaluation methodology with the goal of adapting it to the\nproblem of evaluating link prediction algorithms when applied to large graphs,\nby tackling the problem of class imbalance. We empirically evaluate the\nproposed methodology and, building on these findings, make a case for its\nimportance on the evaluation of large-scale graph processing.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 11:07:51 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 08:52:02 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Garcia-Gasulla", "Dario", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jes\u00fas", ""], ["Cort\u00e9s", "Ulises", ""]]}, {"id": "1611.00676", "submitter": "Qunzhi Zhou", "authors": "Qunzhi Zhou, Yogesh Simmhan and Viktor Prasanna", "title": "Knowledge-infused and Consistent Complex Event Processing over Real-time\n  and Persistent Streams", "comments": "34 pages, 16 figures, accepted in Future Generation Computer Systems,\n  October 27, 2016", "journal-ref": "Future Generation Computer Systems, Volume 76, November 2017,\n  Pages 391-406", "doi": "10.1016/j.future.2016.10.030", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging applications in Internet of Things (IoT) and Cyber-Physical Systems\n(CPS) present novel challenges to Big Data platforms for performing online\nanalytics. Ubiquitous sensors from IoT deployments are able to generate data\nstreams at high velocity, that include information from a variety of domains,\nand accumulate to large volumes on disk. Complex Event Processing (CEP) is\nrecognized as an important real-time computing paradigm for analyzing\ncontinuous data streams. However, existing work on CEP is largely limited to\nrelational query processing, exposing two distinctive gaps for query\nspecification and execution: (1) infusing the relational query model with\nhigher level knowledge semantics, and (2) seamless query evaluation across\ntemporal spaces that span past, present and future events. These allow\naccessible analytics over data streams having properties from different\ndisciplines, and help span the velocity (real-time) and volume (persistent)\ndimensions. In this article, we introduce a Knowledge-infused CEP (X-CEP)\nframework that provides domain-aware knowledge query constructs along with\ntemporal operators that allow end-to-end queries to span across real-time and\npersistent streams. We translate this query model to efficient query execution\nover online and offline data streams, proposing several optimizations to\nmitigate the overheads introduced by evaluating semantic predicates and in\naccessing high-volume historic data streams. The proposed X-CEP query model and\nexecution approaches are implemented in our prototype semantic CEP engine,\nSCEPter. We validate our query model using domain-aware CEP queries from a\nreal-world Smart Power Grid application, and experimentally analyze the\nbenefits of our optimizations for executing these queries, using event streams\nfrom a campus-microgrid IoT deployment.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 16:39:18 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Zhou", "Qunzhi", ""], ["Simmhan", "Yogesh", ""], ["Prasanna", "Viktor", ""]]}, {"id": "1611.01090", "submitter": "Reinhard Pichler", "authors": "Wolfgang Fischl, Georg Gottlob and Reinhard Pichler", "title": "General and Fractional Hypertree Decompositions: Hard and Easy Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypertree decompositions, as well as the more powerful generalized hypertree\ndecompositions (GHDs), and the yet more general fractional hypertree\ndecompositions (FHD) are hypergraph decomposition methods successfully used for\nanswering conjunctive queries and for the solution of constraint satisfaction\nproblems. Every hypergraph H has a width relative to each of these\ndecomposition methods: its hypertree width hw(H), its generalized hypertree\nwidth ghw(H), and its fractional hypertree width fhw(H), respectively.\n  It is known that hw(H) <= k can be checked in polynomial time for fixed k,\nwhile checking ghw(H) <= k is NP-complete for any k greater than or equal to 3.\nThe complexity of checking fhw(H) <= k for a fixed k has been open for more\nthan a decade.\n  We settle this open problem by showing that checking fhw(H) <= k is\nNP-complete, even for k=2. The same construction allows us to prove also the\nNP-completeness of checking ghw(H) <= k for k=2. After proving these hardness\nresults, we identify meaningful restrictions, for which checking for bounded\nghw or fhw becomes tractable.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 16:54:37 GMT"}, {"version": "v10", "created": "Tue, 21 May 2019 08:22:25 GMT"}, {"version": "v11", "created": "Thu, 18 Jul 2019 11:38:17 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 17:48:37 GMT"}, {"version": "v3", "created": "Tue, 29 Nov 2016 14:49:35 GMT"}, {"version": "v4", "created": "Thu, 22 Dec 2016 11:26:00 GMT"}, {"version": "v5", "created": "Fri, 23 Jun 2017 08:55:29 GMT"}, {"version": "v6", "created": "Mon, 28 Aug 2017 09:29:25 GMT"}, {"version": "v7", "created": "Tue, 29 Aug 2017 11:13:24 GMT"}, {"version": "v8", "created": "Wed, 25 Jul 2018 14:20:36 GMT"}, {"version": "v9", "created": "Tue, 20 Nov 2018 10:50:34 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Fischl", "Wolfgang", ""], ["Gottlob", "Georg", ""], ["Pichler", "Reinhard", ""]]}, {"id": "1611.01137", "submitter": "Elias Stehle", "authors": "Elias Stehle, Hans-Arno Jacobsen", "title": "A Memory Bandwidth-Efficient Hybrid Radix Sort on GPUs", "comments": "16 pages, accepted at SIGMOD 2017", "journal-ref": "SIGMOD (2017) 417-432", "doi": "10.1145/3035918.3064043", "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is at the core of many database operations, such as index creation,\nsort-merge joins, and user-requested output sorting. As GPUs are emerging as a\npromising platform to accelerate various operations, sorting on GPUs becomes a\nviable endeavour. Over the past few years, several improvements have been\nproposed for sorting on GPUs, leading to the first radix sort implementations\nthat achieve a sorting rate of over one billion 32-bit keys per second. Yet,\nstate-of-the-art approaches are heavily memory bandwidth-bound, as they require\nsubstantially more memory transfers than their CPU-based counterparts.\n  Our work proposes a novel approach that almost halves the amount of memory\ntransfers and, therefore, considerably lifts the memory bandwidth limitation.\nBeing able to sort two gigabytes of eight-byte records in as little as 50\nmilliseconds, our approach achieves a 2.32-fold improvement over the\nstate-of-the-art GPU-based radix sort for uniform distributions, sustaining a\nminimum speed-up of no less than a factor of 1.66 for skewed distributions.\n  To address inputs that either do not reside on the GPU or exceed the\navailable device memory, we build on our efficient GPU sorting approach with a\npipelined heterogeneous sorting algorithm that mitigates the overhead\nassociated with PCIe data transfers. Comparing the end-to-end sorting\nperformance to the state-of-the-art CPU-based radix sort running 16 threads,\nour heterogeneous approach achieves a 2.06-fold and a 1.53-fold improvement for\nsorting 64 GB key-value pairs with a skewed and a uniform distribution,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 19:33:06 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 12:22:16 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Stehle", "Elias", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "1611.01546", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Sanjukta Bhowmick and Sharma Chakravarthy", "title": "Scalable Holistic Analysis of Multi-Source, Data-Intensive Problems\n  Using Multilayered Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holistic analysis of many real-world problems are based on data collected\nfrom multiple sources contributing to some aspect of that problem. The word\nfusion has also been used in the literature for such problems involving\ndisparate data types. Holistically understanding traffic patterns, causes of\naccidents, bombings, terrorist planning and many natural phenomenon such as\nstorms, earthquakes fall into this category. Some may have real-time\nrequirements and some may need to be analyzed after the fact (post-mortem or\nforensic analysis.) What is common for all these problems is that the amount\nand types of data associated with the event. Data may also be incomplete and\ntrustworthiness of sources may also vary. Currently, manual and ad-hoc\napproaches are used in aggregating data in different ways for analyzing and\nunderstanding these problems.\n  In this paper, we approach this problem in a novel way using multilayered\nnetworks. We identify features of a central event and propose a network layer\nfor each feature. This approach allows us to study the effect of each feature\nindependently and its impact on the event. We also establish that the proposed\napproach allows us to compose these features in arbitrary ways (without loss of\ninformation) to analyze their combined effect. Additionally, formulation of\nrelationships (e.g., distance measure for a single feature instead of several\nat the same time) is simpler. Further, computations can be done once on each\nlayer in this approach and reused for mixing and matching the features for\naggregate impacts and \"what if\" scenarios to understand the problem\nholistically. This has been demonstrated by recreating the communities for the\nAND-Composed network by using the communities of the individual layers.\n  We believe that techniques proposed here make an important contribution to\nthe nascent yet fast growing area of data fusion.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 21:32:18 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Santra", "Abhishek", ""], ["Bhowmick", "Sanjukta", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "1611.01560", "submitter": "R\\'obert Beck", "authors": "R\\'obert Beck, L\\'aszl\\'o Dobos, Tam\\'as Budav\\'ari, Alexander S.\n  Szalay, Istv\\'an Csabai", "title": "Photo-z-SQL: integrated, flexible photometric redshift computation in a\n  database", "comments": "14 pages, 5 figures. Minor revision accepted by Astronomy & Computing\n  on 2017 March 11", "journal-ref": null, "doi": "10.1016/j.ascom.2017.03.002", "report-no": null, "categories": "astro-ph.GA astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a flexible template-based photometric redshift estimation\nframework, implemented in C#, that can be seamlessly integrated into a SQL\ndatabase (or DB) server and executed on-demand in SQL. The DB integration\neliminates the need to move large photometric datasets outside a database for\nredshift estimation, and utilizes the computational capabilities of DB\nhardware. The code is able to perform both maximum likelihood and Bayesian\nestimation, and can handle inputs of variable photometric filter sets and\ncorresponding broad-band magnitudes. It is possible to take into account the\nfull covariance matrix between filters, and filter zero points can be\nempirically calibrated using measurements with given redshifts. The list of\nspectral templates and the prior can be specified flexibly, and the expensive\nsynthetic magnitude computations are done via lazy evaluation, coupled with a\ncaching of results. Parallel execution is fully supported. For large upcoming\nphotometric surveys such as the LSST, the ability to perform in-place photo-z\ncalculation would be a significant advantage. Also, the efficient handling of\nvariable filter sets is a necessity for heterogeneous databases, for example\nthe Hubble Source Catalog, and for cross-match services such as SkyQuery. We\nillustrate the performance of our code on two reference photo-z estimation\ntesting datasets, and provide an analysis of execution time and scalability\nwith respect to different configurations. The code is available for download at\nhttps://github.com/beckrob/Photo-z-SQL.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 22:48:06 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 11:46:57 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Beck", "R\u00f3bert", ""], ["Dobos", "L\u00e1szl\u00f3", ""], ["Budav\u00e1ri", "Tam\u00e1s", ""], ["Szalay", "Alexander S.", ""], ["Csabai", "Istv\u00e1n", ""]]}, {"id": "1611.01711", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Babak Salimi", "title": "Causes for Query Answers from Databases: Datalog Abduction,\n  View-Updates, and Integrity Constraints", "comments": "To appear in International Journal of Approximate Reasoning. Extended\n  version of \"Flairs'16\" and \"UAI'15 WS on Causality\" papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality has been recently introduced in databases, to model, characterize,\nand possibly compute causes for query answers. Connections between QA-causality\nand consistency-based diagnosis and database repairs (wrt. integrity constraint\nviolations) have already been established. In this work we establish precise\nconnections between QA-causality and both abductive diagnosis and the\nview-update problem in databases, allowing us to obtain new algorithmic and\ncomplexity results for QA-causality. We also obtain new results on the\ncomplexity of view-conditioned causality, and investigate the notion of\nQA-causality in the presence of integrity constraints, obtaining complexity\nresults from a connection with view-conditioned causality. The abduction\nconnection under integrity constraints allows us to obtain algorithmic tools\nfor QA-causality.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 00:35:09 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 23:02:08 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 17:58:01 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Salimi", "Babak", ""]]}, {"id": "1611.01853", "submitter": "Aviv Yehezkel", "authors": "Reuven Cohen, Liran Katzir and Aviv Yehezkel", "title": "MTS Sketch for Accurate Estimation of Set-Expression Cardinalities from\n  Small Samples", "comments": "arXiv admin note: text overlap with arXiv:1508.06216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based streaming algorithms allow efficient processing of big data.\nThese algorithms use small fixed-size storage to store a summary (\"sketch\") of\nthe input data, and use probabilistic algorithms to estimate the desired\nquantity. However, in many real-world applications it is impractical to collect\nand process the entire data stream, the common practice is thus to sample and\nprocess only a small part of it. While sampling is crucial for handling massive\ndata sets, it may reduce accuracy. In this paper we present a new framework\nthat can accurately estimate the cardinality of any set expression between any\nnumber of streams using only a small sample of each stream. The proposed\nframework consists of a new sketch, called Maximal-Term with Subsample (MTS),\nand a family of algorithms that use this sketch. An example of a possible query\nthat can be efficiently answered using the proposed sketch is, How many\ndistinct tuples appear in tables $T_1$ and $T_2$, but not in $T_3$? The\nalgorithms presented in this paper answer such queries accurately, processing\nonly a small sample of the tuples in each table and using a constant amount of\nmemory. Such estimations are useful for the optimization of queries over very\nlarge database systems. We show that all our algorithms are unbiased, and we\nanalyze their asymptotic variance.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 22:22:40 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Cohen", "Reuven", ""], ["Katzir", "Liran", ""], ["Yehezkel", "Aviv", ""]]}, {"id": "1611.01868", "submitter": "Luyang Li", "authors": "Luyang Li, Bing Qin, Wenjing Ren, Ting Liu", "title": "Truth Discovery with Memory Network", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truth discovery is to resolve conflicts and find the truth from\nmultiple-source statements. Conventional methods mostly research based on the\nmutual effect between the reliability of sources and the credibility of\nstatements, however, pay no attention to the mutual effect among the\ncredibility of statements about the same object. We propose memory network\nbased models to incorporate these two ideas to do the truth discovery. We use\nfeedforward memory network and feedback memory network to learn the\nrepresentation of the credibility of statements which are about the same\nobject. Specially, we adopt memory mechanism to learn source reliability and\nuse it through truth prediction. During learning models, we use multiple types\nof data (categorical data and continuous data) by assigning different weights\nautomatically in the loss function based on their own effect on truth discovery\nprediction. The experiment results show that the memory network based models\nmuch outperform the state-of-the-art method and other baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 01:08:11 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Li", "Luyang", ""], ["Qin", "Bing", ""], ["Ren", "Wenjing", ""], ["Liu", "Ting", ""]]}, {"id": "1611.01919", "submitter": "Sam Fletcher", "authors": "Sam Fletcher, Md Zahidul Islam", "title": "Decision Tree Classification with Differential Privacy: A Survey", "comments": "Pre-print of paper accepted in ACM Computing Surveys, 35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining information about people is becoming increasingly important in\nthe data-driven society of the 21st century. Unfortunately, sometimes there are\nreal-world considerations that conflict with the goals of data mining;\nsometimes the privacy of the people being data mined needs to be considered.\nThis necessitates that the output of data mining algorithms be modified to\npreserve privacy while simultaneously not ruining the predictive power of the\noutputted model. Differential privacy is a strong, enforceable definition of\nprivacy that can be used in data mining algorithms, guaranteeing that nothing\nwill be learned about the people in the data that could not already be\ndiscovered without their participation. In this survey, we focus on one\nparticular data mining algorithm -- decision trees -- and how differential\nprivacy interacts with each of the components that constitute decision tree\nalgorithms. We analyze both greedy and random decision trees, and the conflicts\nthat arise when trying to balance privacy requirements with the accuracy of the\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 07:13:27 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 19:32:43 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Fletcher", "Sam", ""], ["Islam", "Md Zahidul", ""]]}, {"id": "1611.02737", "submitter": "Jaroslaw Szlichta", "authors": "Sridevi Baskaran, Alexander Keller, Fei Chiang, Golab Lukasz, Jaroslaw\n  Szlichta", "title": "Efficient Discovery of Ontology Functional Dependencies", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poor data quality has become a pervasive issue due to the increasing\ncomplexity and size of modern datasets. Constraint based data cleaning\ntechniques rely on integrity constraints as a benchmark to identify and correct\nerrors. Data values that do not satisfy the given set of constraints are\nflagged as dirty, and data updates are made to re-align the data and the\nconstraints. However, many errors often require user input to resolve due to\ndomain expertise defining specific terminology and relationships. For example,\nin pharmaceuticals, 'Advil' \\emph{is-a} brand name for 'ibuprofen' that can be\ncaptured in a pharmaceutical ontology. While functional dependencies (FDs) have\ntraditionally been used in existing data cleaning solutions to model syntactic\nequivalence, they are not able to model broader relationships (e.g., is-a)\ndefined by an ontology. In this paper, we take a first step towards extending\nthe set of data quality constraints used in data cleaning by defining and\ndiscovering \\emph{Ontology Functional Dependencies} (OFDs). We lay out\ntheoretical and practical foundations for OFDs, including a set of sound and\ncomplete axioms, and a linear inference procedure. We then develop effective\nalgorithms for discovering OFDs, and a set of optimizations that efficiently\nprune the search space. Our experimental evaluation using real data show the\nscalability and accuracy of our algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 22:03:35 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 05:13:36 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 01:44:45 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Baskaran", "Sridevi", ""], ["Keller", "Alexander", ""], ["Chiang", "Fei", ""], ["Lukasz", "Golab", ""], ["Szlichta", "Jaroslaw", ""]]}, {"id": "1611.02816", "submitter": "Jean Vincent Fonou Dombeu", "authors": "Kgotatso Desmond Mogotlane and Jean Vincent Fonou-Dombeu", "title": "Automatic Conversion of Relational Databases into Ontologies: A\n  Comparative Analysis of Prot\\'eg\\'e Plug-ins Performances", "comments": "7(3/4):21-40", "journal-ref": "International Journal of Web and Semantic Technology (IJWesT),\n  2016", "doi": "10.5121/ijwest.2016.7403", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing ontologies from relational databases is an active research topic\nin the Semantic Web domain. While conceptual mapping rules/principles of\nrelational databases and ontology structures are being proposed, several\nsoftware modules or plug-ins are being developed to enable the automatic\nconversion of relational databases into ontologies. However, the correlation\nbetween the resulting ontologies built automatically with plug-ins from\nrelational databases and the database-to-ontology mapping principles has been\ngiven little attention. This study reviews and applies two Prot\\'eg\\'e\nplug-ins, namely, DataMaster and OntoBase to automatically construct ontologies\nfrom a relational database. The resulting ontologies are further analysed to\nmatch their structures against the database-to-ontology mapping principles. A\ncomparative analysis of the matching results reveals that OntoBase outperforms\nDataMaster in applying the database-to-ontology mapping principles for\nautomatically converting relational databases into ontologies.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 09:00:53 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Mogotlane", "Kgotatso Desmond", ""], ["Fonou-Dombeu", "Jean Vincent", ""]]}, {"id": "1611.03204", "submitter": "Ying Zhang Dr.", "authors": "Xiang Wang, Ying Zhang, Wenjie Zhang, Xuemin Lin, Zengfeng Huang", "title": "Top-k Spatial-keyword Publish/Subscribe Over Sliding Window", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of social media and GPS-enabled devices, a massive amount\nof geo-textual data has been generated in a stream fashion, leading to a\nvariety of applications such as location-based recommendation and information\ndissemination. In this paper, we investigate a novel real-time top-k monitoring\nproblem over sliding window of streaming data; that is, we continuously\nmaintain the top-k most relevant geo-textual messages (e.g., geo-tagged tweets)\nfor a large number of spatial-keyword subscriptions (e.g., registered users\ninterested in local events) simultaneously. To provide the most recent\ninformation under controllable memory cost, sliding window model is employed on\nthe streaming geo-textual data. To the best of our knowledge, this is the first\nwork to study top-k spatial-keyword publish/subscribe over sliding window. A\nnovel centralized system, called Skype (Topk Spatial-keyword\nPublish/Subscribe), is proposed in this paper. In Skype, to continuously\nmaintain top-k results for massive subscriptions, we devise a novel indexing\nstructure upon subscriptions such that each incoming message can be immediately\ndelivered on its arrival. To reduce the expensive top-k re-evaluation cost\ntriggered by message expiration, we develop a novel cost-based k-skyband\ntechnique to reduce the number of re-evaluations in a cost-effective way.\nExtensive experiments verify the great efficiency and effectiveness of our\nproposed techniques. Furthermore, to support better scalability and higher\nthroughput, we propose a distributed version of Skype, namely, DSkype, on top\nof Storm, which is a popular distributed stream processing system. With the\nhelp of fine-tuned subscription/message distribution mechanisms, DSkype can\nachieve orders of magnitude speed-up than its centralized version.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 07:29:07 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Wang", "Xiang", ""], ["Zhang", "Ying", ""], ["Zhang", "Wenjie", ""], ["Lin", "Xuemin", ""], ["Huang", "Zengfeng", ""]]}, {"id": "1611.03254", "submitter": "Ying Zhang Dr.", "authors": "Fan Zhang, Ying Zhang, Lu Qin, Wenjie Zhang, Xuemin Lin", "title": "When Engagement Meets Similarity: Efficient (k,r)-Core Computation on\n  Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of (k,r)-core which intends to find\ncohesive subgraphs on social networks considering both user engagement and\nsimilarity perspectives. In particular, we adopt the popular concept of k-core\nto guarantee the engagement of the users (vertices) in a group (subgraph) where\neach vertex in a (k,r)-core connects to at least k other vertices. Meanwhile,\nwe also consider the pairwise similarity between users based on their profiles.\nFor a given similarity metric and a similarity threshold r, the similarity\nbetween any two vertices in a (k,r)-core is ensured not less than r. Efficient\nalgorithms are proposed to enumerate all maximal (k,r)-cores and find the\nmaximum (k,r)-core, where both problems are shown to be NP-hard. Effective\npruning techniques significantly reduce the search space of two algorithms and\na novel (k,k')-core based (k,r)-core size upper bound enhances performance of\nthe maximum (k,r)-core computation. We also devise effective search orders to\naccommodate the different nature of two mining algorithms. Comprehensive\nexperiments on real-life data demonstrate that the maximal/maximum (k,r)-cores\nenable us to find interesting cohesive subgraphs, and performance of two mining\nalgorithms is significantly improved by proposed techniques.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 11:02:31 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Zhang", "Fan", ""], ["Zhang", "Ying", ""], ["Qin", "Lu", ""], ["Zhang", "Wenjie", ""], ["Lin", "Xuemin", ""]]}, {"id": "1611.03380", "submitter": "Vijay Gadepally", "authors": "Sang-Woo Jun, Huy T. Nguyen, Vijay N. Gadepally, Arvind", "title": "In-Storage Embedded Accelerator for Sparse Pattern Processing", "comments": "Accepted to IEEE HPEC 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761588", "report-no": null, "categories": "cs.AR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel architecture for sparse pattern processing, using flash\nstorage with embedded accelerators. Sparse pattern processing on large data\nsets is the essence of applications such as document search, natural language\nprocessing, bioinformatics, subgraph matching, machine learning, and graph\nprocessing. One slice of our prototype accelerator is capable of handling up to\n1TB of data, and experiments show that it can outperform C/C++ software\nsolutions on a 16-core system at a fraction of the power and cost; an optimized\nversion of the accelerator can match the performance of a 48-core server.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 16:21:51 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Jun", "Sang-Woo", ""], ["Nguyen", "Huy T.", ""], ["Gadepally", "Vijay N.", ""], ["Arvind", "", ""]]}, {"id": "1611.03543", "submitter": "Carl Simon Adorf", "authors": "Carl S. Adorf, Paul M. Dodd, Vyas Ramasubramani, Sharon C. Glotzer", "title": "Simple Data and Workflow Management with the signac Framework", "comments": "12 pages, 5 figures", "journal-ref": "C. S. Adorf, P. M. Dodd, V. Ramasubramani, and S. C. Glotzer,\n  \"Simple data and workflow management with the signac framework,\" Comput.\n  Mater. Sci., vol. 146, pp. 220-229, 2018", "doi": "10.1016/j.commatsci.2018.01.035", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in the field of materials science, chemistry, and computational\nphysics are regularly posed with the challenge of managing large and\nheterogeneous data spaces. The amount of data increases in lockstep with\ncomputational efficiency multiplied by the amount of available computational\nresources, which shifts the bottleneck in the scientific process from data\nacquisition to data processing and analysis. We present a framework designed to\naid in the integration of various specialized data formats, tools and\nworkflows. The signac framework provides all basic components required to\ncreate a well-defined and thus collectively accessible and searchable data\nspace, simplifying data access and modification through a homogeneous data\ninterface that is largely agnostic to the data source, i.e., computation or\nexperiment. The framework's data model is designed to not require absolute\ncommitment to the presented implementation, simplifying adaption into existing\ndata sets and workflows. This approach not only increases the efficiency with\nwhich scientific results can be produced, but also significantly lowers\nbarriers for collaborations requiring shared data access.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 23:34:13 GMT"}, {"version": "v2", "created": "Sat, 28 Oct 2017 21:42:15 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 23:30:00 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Adorf", "Carl S.", ""], ["Dodd", "Paul M.", ""], ["Ramasubramani", "Vyas", ""], ["Glotzer", "Sharon C.", ""]]}, {"id": "1611.03652", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves", "title": "Show me the material evidence: Initial experiments on evaluating\n  hypotheses from user-generated multimedia data", "comments": "6 pages, 6 figures, 3 tables in Proc. of the 1st Workshop on\n  Multimedia Support for Decision-Making Processes, at IEEE Intl. Symposium on\n  Multimedia (ISM'16), San Jose, CA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective questions such as `does neymar dive', or `is clinton lying', or\n`is trump a fascist', are popular queries to web search engines, as can be seen\nby autocompletion suggestions on Google, Yahoo and Bing. In the era of\ncognitive computing, beyond search, they could be handled as hypotheses issued\nfor evaluation. Our vision is to leverage on unstructured data and metadata of\nthe rich user-generated multimedia that is often shared as material evidence in\nfavor or against hypotheses in social media platforms. In this paper we present\ntwo preliminary experiments along those lines and discuss challenges for a\ncognitive computing system that collects material evidence from user-generated\nmultimedia towards aggregating it into some form of collective decision on the\nhypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 10:46:58 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""]]}, {"id": "1611.03680", "submitter": "Marco Montali", "authors": "Marco Montali and Andrey Rivkin", "title": "DB-Nets: on The Marriage of Colored Petri Nets and Relational Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integrated management of business processes and mas- ter data is being\nincreasingly considered as a fundamental problem, by both the academia and the\nindustry. In this position paper, we focus on the foundations of the problem,\narguing that contemporary approaches struggle to find a suitable equilibrium\nbetween data- and process-related aspects. We then propose db-nets, a new\nformal model that balances such two pillars through the marriage of colored\nPetri nets and relational databases. We invite the research community to build\non this model, discussing its potential in modeling, formal verification, and\nsimulation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 12:39:41 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Montali", "Marco", ""], ["Rivkin", "Andrey", ""]]}, {"id": "1611.03736", "submitter": "Maria Carmen Calvo", "authors": "Mar\\'ia Carmen Calvo Yanguas (DAE), Carmen Elvira Don\\'azar (DAE),\n  Raquel Trillo Lado (DIIS)", "title": "A Formal Definition for Configuration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB math.CO math.GR math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a wide set of techniques to perform keyword-based search over\nrelational databases but all of them match the keywords in the users' queries\nto elements of the databases to be queried as first step. The matching process\nis a time-consuming and complex task. So, improving the performance of this\ntask is a key issue to improve the keyword based search on relational data\nsources.In this work, we show how to model the matching process on\nkeyword-based search on relational databases by means of the symmetric group.\nBesides, how this approach reduces the search space is explained in detail.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 13:04:49 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Yanguas", "Mar\u00eda Carmen Calvo", "", "DAE"], ["Don\u00e1zar", "Carmen Elvira", "", "DAE"], ["Lado", "Raquel Trillo", "", "DIIS"]]}, {"id": "1611.03751", "submitter": "Pengfei Xu", "authors": "Pengfei Xu, Jiaheng Lu", "title": "Top-k String Auto-Completion with Synonyms", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-completion is one of the most prominent features of modern information\nsystems. The existing solutions of auto-completion provide the suggestions\nbased on the beginning of the currently input character sequence (i.e. prefix).\nHowever, in many real applications, one entity often has synonyms or\nabbreviations. For example, \"DBMS\" is an abbreviation of \"Database Management\nSystems\". In this paper, we study a novel type of auto-completion by using\nsynonyms and abbreviations. We propose three trie-based algorithms to solve the\ntop-k auto-completion with synonyms; each one with different space and time\ncomplexity trade-offs. Experiments on large-scale datasets show that it is\npossible to support effective and efficient synonym-based retrieval of\ncompletions of a million strings with thousands of synonyms rules at about a\nmicrosecond per-completion, while taking small space overhead (i.e. 160-200\nbytes per string). The source code of our experiments can be download at:\nhttp://udbms.cs.helsinki.fi/?projects/autocompletion/download .\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 15:40:06 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 20:12:56 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 22:29:33 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Xu", "Pengfei", ""], ["Lu", "Jiaheng", ""]]}, {"id": "1611.03959", "submitter": "Arijit Khan", "authors": "Arijit Khan, Gustavo Segovia, Donald Kossmann", "title": "On Smart Query Routing: For Distributed Graph Querying with Decoupled\n  Storage", "comments": "12 pages, 16 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online graph queries that retrieve nearby nodes of a query node from\na large network. To answer such queries with high throughput and low latency,\nwe partition the graph and process the data in parallel across a cluster of\nservers. State-of-the-art distributed graph querying systems place each graph\npartition on a separate server, where query answering over that partition takes\nplace. This design has two major disadvantages. First, the router needs to\nmaintain a fixed routing table. Hence, these systems are less flexible with\nrespect to query routing, fault tolerance, and graph updates. Second, the graph\ndata must be partitioned such that the workload across the servers is balanced,\nand the inter-machine communication is minimized. In addition, it is required\nto update the existing partitions based on workload changes over graph nodes.\nHowever, graph partitioning, online monitoring of workloads, and dynamically\nupdating the graph partitions are expensive. In this work, we mitigate both\nthese problems by decoupling graph storage from query processors, and by\ndeveloping smart routing strategies that improve the cache locality in query\nprocessors. Since a query processor is no longer assigned any fixed part of the\ngraph, it is equally capable of handling any request, thus facilitating load\nbalancing and fault tolerance. On the other hand, due to our smart routing\nstrategies, query processors can effectively leverage their cache contents,\nreducing the overall impact of how the graph is partitioned across storage\nservers. A detailed experimental evaluation with several real-world, large\ngraph datasets demonstrates that our proposed framework, gRouting - even with\nsimple hash partitioning of the data - achieves up to an order of magnitude\nbetter query throughput compared to existing graph querying systems that employ\nexpensive graph partitioning and re-partitioning strategies.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 06:22:28 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 02:10:16 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Khan", "Arijit", ""], ["Segovia", "Gustavo", ""], ["Kossmann", "Donald", ""]]}, {"id": "1611.04288", "submitter": "Ruoxi Shi", "authors": "Yiwen Tang, Hongzhi Wang, Shiwei Zhang, Huijun Zhang, and Ruoxi Shi", "title": "Efficient Web-based Data Imputation with Graph Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge for data imputation is the lack of knowledge. In this paper, we\nattempt to address this challenge by involving extra knowledge from web. To\nachieve high-performance web-based imputation, we use the dependency, i.e.FDs\nand CFDs, to impute as many as possible values automatically and fill in the\nother missing values with the minimal access of web, whose cost is relatively\nlarge. To make sufficient use of dependencies, We model the dependency set on\nthe data as a graph and perform automatical imputation and keywords generation\nfor web-based imputation based on such graph model. With the generated\nkeywords, we design two algorithms to extract values for imputation from the\nsearch results. Extensive experimental results based on real-world data\ncollections show that the proposed approach could impute missing values\nefficiently and effectively compared to existing approach.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 09:08:19 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tang", "Yiwen", ""], ["Wang", "Hongzhi", ""], ["Zhang", "Shiwei", ""], ["Zhang", "Huijun", ""], ["Shi", "Ruoxi", ""]]}, {"id": "1611.04308", "submitter": "Panos Parchas Mr", "authors": "Panos Parchas, Nikolaos Papailiou, Dimitris Papadias, Francesco Bonchi", "title": "Uncertain Graph Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertain graphs are prevalent in several applications including\ncommunications systems, biological databases and social networks. The ever\nincreasing size of the underlying data renders both graph storage and query\nprocessing extremely expensive. Sparsification has often been used to reduce\nthe size of deterministic graphs by maintaining only the important edges.\nHowever, adaptation of deterministic sparsification methods fails in the\nuncertain setting. To overcome this problem, we introduce the first\nsparsification techniques aimed explicitly at uncertain graphs. The proposed\nmethods reduce the number of edges and redistribute their probabilities in\norder to decrease the graph size, while preserving its underlying structure.\nThe resulting graph can be used to efficiently and accurately approximate any\nquery and mining tasks on the original graph. An extensive experimental\nevaluation with real and synthetic datasets illustrates the effectiveness of\nour techniques on several common graph tasks, including clustering coefficient,\npage rank, reliability and shortest path distance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 09:58:11 GMT"}, {"version": "v2", "created": "Sun, 29 Jan 2017 11:39:21 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 11:29:06 GMT"}, {"version": "v4", "created": "Wed, 24 May 2017 05:50:38 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Parchas", "Panos", ""], ["Papailiou", "Nikolaos", ""], ["Papadias", "Dimitris", ""], ["Bonchi", "Francesco", ""]]}, {"id": "1611.04689", "submitter": "Ruoxi Shi", "authors": "Ruoxi Shi, Hongzhi Wang, Tao Wang, Yutai Hou and Yiwen Tang", "title": "Similarity Search Combining Query Relaxation and Diversification", "comments": "Conference: DASFAA 2017", "journal-ref": null, "doi": null, "report-no": "287", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the similarity search problem which aims to find the similar query\nresults according to a set of given data and a query string. To balance the\nresult number and result quality, we combine query result diversity with query\nrelaxation. Relaxation guarantees the number of the query results, returning\nmore relevant elements to the query if the results are too few, while the\ndiversity tries to reduce the similarity among the returned results. By making\na trade-off of similarity and diversity, we improve the user experience. To\nachieve this goal, we define a novel goal function combining similarity and\ndiversity. Aiming at this goal, we propose three algorithms. Among them,\nalgorithms genGreedy and genCluster perform relaxation first and select part of\nthe candidates to diversify. The third algorithm CB2S splits the dataset into\nsmaller pieces using the clustering algorithm of k-means and processes queries\nin several small sets to retrieve more diverse results. The balance of\nsimilarity and diversity is determined through setting a threshold, which has a\ndefault value and can be adjusted according to users' preference. The\nperformance and efficiency of our system are demonstrated through extensive\nexperiments based on various datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 03:26:19 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 07:04:17 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Shi", "Ruoxi", ""], ["Wang", "Hongzhi", ""], ["Wang", "Tao", ""], ["Hou", "Yutai", ""], ["Tang", "Yiwen", ""]]}, {"id": "1611.04705", "submitter": "Albert Kim", "authors": "Albert Kim, Liqi Xu, Tarique Siddiqui, Silu Huang, Samuel Madden,\n  Aditya Parameswaran", "title": "Optimally Leveraging Density and Locality to Support LIMIT Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing database systems are not optimized for queries with a LIMIT\nclause---operating instead in an all-or-nothing manner. In this paper, we\npropose a fast LIMIT query evaluation engine, called NeedleTail, aimed at\nletting analysts browse a small sample of the query results on large datasets\nas quickly as possible, independent of the overall size of the result set.\nNeedleTail introduces density maps, a lightweight in-memory indexing structure,\nand a set of efficient algorithms (with desirable theoretical guarantees) to\nquickly locate promising blocks, trading off locality and density. In settings\nwhere the samples are used to compute aggregates, we extend techniques from\nsurvey sampling to mitigate the bias in our samples. Our experimental results\ndemonstrate that NeedleTail returns results 4x faster on HDDs and 9x faster on\nSSDs on average, while occupying up to 23x less memory than existing\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 05:12:10 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 21:04:27 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 02:58:30 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Kim", "Albert", ""], ["Xu", "Liqi", ""], ["Siddiqui", "Tarique", ""], ["Huang", "Silu", ""], ["Madden", "Samuel", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1611.04878", "submitter": "Yeounoh Chung", "authors": "Yeounoh Chung, Sanjay Krishnan, Tim Kraska", "title": "A Data Quality Metric (DQM): How to Estimate The Number of Undetected\n  Errors in Data Sets", "comments": "To appear in VLDB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data cleaning, whether manual or algorithmic, is rarely perfect leaving a\ndataset with an unknown number of false positives and false negatives after\ncleaning. In many scenarios, quantifying the number of remaining errors is\nchallenging because our data integrity rules themselves may be incomplete, or\nthe available gold-standard datasets may be too small to extrapolate. As the\nuse of inherently fallible crowds becomes more prevalent in data cleaning\nproblems, it is important to have estimators to quantify the extent of such\nerrors. We propose novel species estimators to estimate the number of distinct\nremaining errors in a dataset after it has been cleaned by a set of crowd\nworkers -- essentially, quantifying the utility of hiring additional workers to\nclean the dataset. This problem requires new estimators that are robust to\nfalse positives and false negatives, and we empirically show on three\nreal-world datasets that existing species estimators are unstable for this\nproblem, while our proposed techniques quickly converge.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 15:00:53 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 04:46:38 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 18:24:26 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Chung", "Yeounoh", ""], ["Krishnan", "Sanjay", ""], ["Kraska", "Tim", ""]]}, {"id": "1611.04977", "submitter": "Thang Van Doan", "authors": "Doan Van Thang, Doan Van Ban", "title": "Query Data With Fuzzy Information In Object-Oriented Databases An\n  Approach Interval Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose methods of handling attributive values of object\nclasses in object oriented database with fuzzy information and uncertainty\nbased on quantitatively semantics based hedge algebraic. In this approach we\nconsider to attributive values (as well as methods) object class is interval\nvalues and the interval values are converted into sub interval in [0, 1]\nrespectively. That its the fuzziness of the elements in the hedge algebra is\nalso sub interval in [0,1]. So, we present an algorithm allows the comparison\nof two sub interval [0,1] helping the requirements of the query data\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 12:29:38 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Van Thang", "Doan", ""], ["Van Ban", "Doan", ""]]}, {"id": "1611.05428", "submitter": "Daniel Lemire", "authors": "Daniel Lemire and Christoph Rupp", "title": "Upscaledb: Efficient Integer-Key Compression in a Key-Value Store using\n  SIMD Instructions", "comments": null, "journal-ref": "Information Systems Volume 66, June 2017, Pages 13-23", "doi": "10.1016/j.is.2017.01.002", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compression can sometimes improve performance by making more of the data\navailable to the processors faster. We consider the compression of integer keys\nin a B+-tree index. For this purpose, systems such as IBM DB2 use variable-byte\ncompression over differentially coded keys. We revisit this problem with\nvarious compression alternatives such as Google's VarIntGB, Binary Packing and\nFrame-of-Reference. In all cases, we describe algorithms that can operate\ndirectly on compressed data. Many of our alternatives exploit the\nsingle-instruction-multiple-data (SIMD) instructions supported by modern CPUs.\nWe evaluate our techniques in a database environment provided by Upscaledb, a\nproduction-quality key-value database. Our best techniques are SIMD\naccelerated: they simultaneously reduce memory usage while improving\nsingle-threaded speeds. In particular, a differentially coded SIMD\nbinary-packing techniques (BP128) can offer a superior query speed (e.g., 40%\nbetter than an uncompressed database) while providing the best compression\n(e.g., by a factor of ten). For analytic workloads, our fast compression\ntechniques offer compelling benefits. Our software is available as open source.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 20:17:07 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 15:40:05 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Lemire", "Daniel", ""], ["Rupp", "Christoph", ""]]}, {"id": "1611.05557", "submitter": "Weidong Xiong", "authors": "Weidong Xiong, Feng Yu, Mohammed Hamdi and Wen-Chi Hou", "title": "A Prudent-Precedence Concurrency Control Protocol for High Data\n  Contention Database Enviornments", "comments": "14 pages, 16 figures, 1 table", "journal-ref": "International Journal of Database Management Systems (IJDMS),\n  8(5), 1-14 (2016)", "doi": "10.5121/ijdms.2016.8501", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a concurrency control protocol, called the\nPrudent-Precedence Concurrency Control (PPCC) protocol, for high data\ncontention database environments. PPCC is prudently more aggressive in\npermitting more serializable schedules than two-phase locking. It maintains a\nrestricted precedence among conflicting transactions and commits the\ntransactions according to the serialization order established in the\nexecutions. A detailed simulation model has been constructed and extensive\nexperiments have been conducted to evaluate the performance of the proposed\napproach. The results demonstrate that the proposed algorithm outperforms the\ntwo-phase locking and optimistic concurrency control in all ranges of system\nworkload.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 04:02:54 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Xiong", "Weidong", ""], ["Yu", "Feng", ""], ["Hamdi", "Mohammed", ""], ["Hou", "Wen-Chi", ""]]}, {"id": "1611.05788", "submitter": "Jacob Abernethy", "authors": "Jacob Abernethy (University of Michigan), Cyrus Anderson (University\n  of Michigan), Alex Chojnacki (University of Michigan), Chengyu Dai\n  (University of Michigan), John Dryden (University of Michigan), Eric Schwartz\n  (University of Michigan), Wenbo Shen (University of Michigan), Jonathan\n  Stroud (University of Michigan), Laura Wendlandt (University of Michigan),\n  Sheng Yang (University of Michigan), Daniel Zhang (University of Michigan)", "title": "Data Science in Service of Performing Arts: Applying Machine Learning to\n  Predicting Audience Preferences", "comments": "Presented at the Data For Good Exchange 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing arts organizations aim to enrich their communities through the\narts. To do this, they strive to match their performance offerings to the taste\nof those communities. Success relies on understanding audience preference and\npredicting their behavior. Similar to most e-commerce or digital entertainment\nfirms, arts presenters need to recommend the right performance to the right\ncustomer at the right time. As part of the Michigan Data Science Team (MDST),\nwe partnered with the University Musical Society (UMS), a non-profit performing\narts presenter housed in the University of Michigan, Ann Arbor. We are\nproviding UMS with analysis and business intelligence, utilizing historical\nindividual-level sales data. We built a recommendation system based on\ncollaborative filtering, gaining insights into the artistic preferences of\ncustomers, along with the similarities between performances. To better\nunderstand audience behavior, we used statistical methods from customer-base\nanalysis. We characterized customer heterogeneity via segmentation, and we\nmodeled customer cohorts to understand and predict ticket purchasing patterns.\nFinally, we combined statistical modeling with natural language processing\n(NLP) to explore the impact of wording in program descriptions. These ongoing\nefforts provide a platform to launch targeted marketing campaigns, helping UMS\ncarry out its mission by allocating its resources more efficiently. Celebrating\nits 138th season, UMS is a 2014 recipient of the National Medal of Arts, and it\ncontinues to enrich communities by connecting world-renowned artists with\ndiverse audiences, especially students in their formative years. We aim to\ncontribute to that mission through data science and customer analytics.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 03:49:16 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Abernethy", "Jacob", "", "University of Michigan"], ["Anderson", "Cyrus", "", "University\n  of Michigan"], ["Chojnacki", "Alex", "", "University of Michigan"], ["Dai", "Chengyu", "", "University of Michigan"], ["Dryden", "John", "", "University of Michigan"], ["Schwartz", "Eric", "", "University of Michigan"], ["Shen", "Wenbo", "", "University of Michigan"], ["Stroud", "Jonathan", "", "University of Michigan"], ["Wendlandt", "Laura", "", "University of Michigan"], ["Yang", "Sheng", "", "University of Michigan"], ["Zhang", "Daniel", "", "University of Michigan"]]}, {"id": "1611.06128", "submitter": "Mohamed Sherif", "authors": "Mohamed Ahmed Sherif, Kevin Dre{\\ss}ler, Panayiotis Smeros and\n  Axel-Cyrille Ngonga Ngomo", "title": "Annex: Radon - Rapid Discovery of Topological Relations", "comments": "19 pages, 3 figures, i algorithm and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets containing geo-spatial resources are increasingly being represented\naccording to the Linked Data principles. Several time-efficient approaches for\ndiscovering links between RDF resources have been developed over the last\nyears. However, the time-efficient discovery of topological relations between\ngeospatial resources has been paid little attention to. We address this\nresearch gap by presenting Radon, a novel approach for the rapid computation of\ntopological relations between geo-spatial resources. Our approach uses a sparse\ntiling index in combination with minimum bounding boxes to reduce the\ncomputation time of topological relations. Our evaluation of Radon's runtime on\n45 datasets and in more than 800 experiments shows that it outperforms the\nstate of the art by up to 3 orders of magnitude while maintaining an F-measure\nof 100%. Moreover, our experiments suggest that Radon scales up well when\nimplemented in parallel.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 15:42:22 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Sherif", "Mohamed Ahmed", ""], ["Dre\u00dfler", "Kevin", ""], ["Smeros", "Panayiotis", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1611.06224", "submitter": "Hui Miao", "authors": "Hui Miao, Ang Li, Larry S. Davis, Amol Deshpande", "title": "ModelHub: Towards Unified Data and Lifecycle Management for Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has improved state-of-the-art results in many important fields,\nand has been the subject of much research in recent years, leading to the\ndevelopment of several systems for facilitating deep learning. Current systems,\nhowever, mainly focus on model building and training phases, while the issues\nof data management, model sharing, and lifecycle management are largely\nignored. Deep learning modeling lifecycle generates a rich set of data\nartifacts, such as learned parameters and training logs, and comprises of\nseveral frequently conducted tasks, e.g., to understand the model behaviors and\nto try out new models. Dealing with such artifacts and tasks is cumbersome and\nlargely left to the users. This paper describes our vision and implementation\nof a data and lifecycle management system for deep learning. First, we\ngeneralize model exploration and model enumeration queries from commonly\nconducted tasks by deep learning modelers, and propose a high-level domain\nspecific language (DSL), inspired by SQL, to raise the abstraction level and\naccelerate the modeling process. To manage the data artifacts, especially the\nlarge amount of checkpointed float parameters, we design a novel model\nversioning system (dlv), and a read-optimized parameter archival storage system\n(PAS) that minimizes storage footprint and accelerates query workloads without\nlosing accuracy. PAS archives versioned models using deltas in a\nmulti-resolution fashion by separately storing the less significant bits, and\nfeatures a novel progressive query (inference) evaluation algorithm. Third, we\nshow that archiving versioned models using deltas poses a new dataset\nversioning problem and we develop efficient algorithms for solving it. We\nconduct extensive experiments over several real datasets from computer vision\ndomain to show the efficiency of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 20:59:25 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Miao", "Hui", ""], ["Li", "Ang", ""], ["Davis", "Larry S.", ""], ["Deshpande", "Amol", ""]]}, {"id": "1611.06417", "submitter": "Saad Bin Suhaim", "authors": "Saad Bin Suhaim, Weimo Liu, Nan Zhang", "title": "Discover Aggregates Exceptions over Hidden Web Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, many web databases \"hidden\" behind their restrictive search\ninterfaces (e.g., Amazon, eBay) contain rich and valuable information that is\nof significant interests to various third parties. Recent studies have\ndemonstrated the possibility of estimating/tracking certain aggregate queries\nover dynamic hidden web databases. Nonetheless, tracking all possible aggregate\nquery answers to report interesting findings (i.e., exceptions), while still\nadhering to the stringent query-count limitations enforced by many hidden web\ndatabases providers, is very challenging. In this paper, we develop a novel\ntechnique for tracking and discovering exceptions (in terms of sudden changes\nof aggregates) over dynamic hidden web databases. Extensive real-world\nexperiments demonstrate the superiority of our proposed algorithms over\nbaseline solutions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 19:37:41 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Suhaim", "Saad Bin", ""], ["Liu", "Weimo", ""], ["Zhang", "Nan", ""]]}, {"id": "1611.06951", "submitter": "Leopoldo Bertossi", "authors": "Zeinab Bahmani and Leopoldo Bertossi", "title": "Enforcing Relational Matching Dependencies with Datalog for Entity\n  Resolution", "comments": "New revisions applied. To appear in Proc. FLAIRS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is about identifying and merging records in a database\nthat represent the same real-world entity. Matching dependencies (MDs) have\nbeen introduced and investigated as declarative rules that specify ER policies.\nAn ER process induced by MDs over a dirty instance leads to multiple clean\ninstances, in general. General \"answer sets programs\" have been proposed to\nspecify the MD-based cleaning task and its results. In this work, we extend MDs\nto \"relational MDs\", which capture more application semantics, and identify\nclasses of relational MDs for which the general ASP can be automatically\nrewritten into a stratified Datalog program, with the single clean instance as\nits standard model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:02:19 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 23:23:27 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Bahmani", "Zeinab", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1611.07623", "submitter": "EPTCS", "authors": "Maaz Bin Safeer Ahmad, Alvin Cheung", "title": "Leveraging Parallel Data Processing Frameworks with Verified Lifting", "comments": "In Proceedings SYNT 2016, arXiv:1611.07178", "journal-ref": "EPTCS 229, 2016, pp. 67-83", "doi": "10.4204/EPTCS.229.7", "report-no": null, "categories": "cs.PL cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many parallel data frameworks have been proposed in recent years that let\nsequential programs access parallel processing. To capitalize on the benefits\nof such frameworks, existing code must often be rewritten to the\ndomain-specific languages that each framework supports. This rewriting-tedious\nand error-prone-also requires developers to choose the framework that best\noptimizes performance given a specific workload.\n  This paper describes Casper, a novel compiler that automatically retargets\nsequential Java code for execution on Hadoop, a parallel data processing\nframework that implements the MapReduce paradigm. Given a sequential code\nfragment, Casper uses verified lifting to infer a high-level summary expressed\nin our program specification language that is then compiled for execution on\nHadoop. We demonstrate that Casper automatically translates Java benchmarks\ninto Hadoop. The translated results execute on average 3.3x faster than the\nsequential implementations and scale better, as well, to larger datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 03:16:38 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Ahmad", "Maaz Bin Safeer", ""], ["Cheung", "Alvin", ""]]}, {"id": "1611.08269", "submitter": "Olivier Cur\\'e", "authors": "Xiangnan Ren and Houda Khrouf and Zakia Kazi-Aoul and Yousra Chabchoub\n  and Olivier Cur\\'e", "title": "On measuring performances of C-SPARQL and CQELS", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the massive growth of semantic data streams, several RDF Stream\nProcessing (RSP) engines have been implemented. The efficiency of their\nthroughput, latency and memory consumption can be evaluated using available\nbenchmarks such as LSBench and City- Bench. Nevertheless, these benchmarks lack\nan in-depth performance evaluation as some measurement metrics have not been\nconsidered. The main goal of this paper is to analyze the performance of two\npopular RSP engines, namely C-SPARQL and CQELS, when varying a set of\nperformance metrics. More precisely, we evaluate the impact of stream rate,\nnumber of streams and window size on execution time as well as on memory\nconsumption.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 17:43:48 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Ren", "Xiangnan", ""], ["Khrouf", "Houda", ""], ["Kazi-Aoul", "Zakia", ""], ["Chabchoub", "Yousra", ""], ["Cur\u00e9", "Olivier", ""]]}, {"id": "1611.09170", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (LIMOS)", "title": "DESP-C++: A Discrete-Event Simulation Package for C++", "comments": null, "journal-ref": "Software: Practice and Experience, Wiley, 2000, 30 (1), pp.37-60", "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DESP-C++ is a C++ discrete-event random simulation engine that has been\ndesigned to be fast, very easy to use and expand, and valid. DESP-C++ is based\non the resource view. Its complete architecture is presented in detail, as well\nas a short \" user manual \". The validity of DESP-C++ is demonstrated by the\nsimulation of three significant models. In each case, the simulation results\nobtained with DESP-C++ match those obtained with a validated simulation\nsoftware: QNAP2. The versatility of DESP-C++ is also illustrated this way,\nsince the modelled systems are very different from each other: a simple\nproduction system, the dining philosopher classical deadlock problem, and a\ncomplex object-oriented database management system.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:16:42 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "LIMOS"]]}, {"id": "1611.09172", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (LIMOS), Michel Schneider (LIMOS)", "title": "Benchmarking OODBs with a Generic Tool", "comments": null, "journal-ref": "Journal of Database Management, IGI Global, 2000, 11 (3), pp.16-27", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a generic object-oriented benchmark (OCB: the Object\nClustering Benchmark) that has been designed to evaluate the performances of\nObject-Oriented Data-bases (OODBs), and more specifically the performances of\nclustering policies within OODBs. OCB is generic because its sample database\nmay be customized to fit any of the databases in-troduced by the main existing\nbenchmarks, e.g., OO1 (Object Operation 1) or OO7. The first version of OCB was\npurposely clustering-oriented due to a clustering-oriented workload, but OCB\nhas been thoroughly extended to be able to suit other purposes. Eventually,\nOCB's code is compact and easily portable. OCB has been validated through two\nimplementations: one within the O2 OODB and another one within the Texas\npersistent object store. The perfor-mances of a specific clustering policy\ncalled DSTC (Dynamic, Statistical, Tunable Clustering) have also been evaluated\nwith OCB.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:17:18 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "LIMOS"], ["Schneider", "Michel", "", "LIMOS"]]}, {"id": "1611.09176", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (LIMOS), Amar Attoui (LIMOS), Michel Gourgand\n  (LIMOS)", "title": "Simulation of clustering algorithms in OODBs in order to evaluate their\n  performances", "comments": "arXiv admin note: substantial text overlap with arXiv:0705.0454,\n  arXiv:1611.09177", "journal-ref": "Simulation Practice and Theory, Elsevier, 1997, 5 (3), pp.269-287", "doi": "10.1016/S0928-4869(96)00013-4", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good object clustering is critical to the performance of object-oriented\ndatabases. However, it always involves some kind of overhead for the system.\nThe aim of this paper is to propose a modelling methodology in order to\nevaluate the performances of different clustering policies. This methodology\nhas been used to compare the performances of three clustering algorithms found\nin the literature (Cactis, CK and ORION) that we considered representative of\nthe current research in the field of object clustering. The actual performance\nevaluation was performed using simulation. Simulation experiments showed that\nthe Cactis algorithm is better than the ORION algorithm and that the CK\nalgorithm totally outperforms both other algorithms in terms of response time\nand clustering overhead.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:19:14 GMT"}], "update_date": "2017-01-01", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "LIMOS"], ["Attoui", "Amar", "", "LIMOS"], ["Gourgand", "Michel", "", "LIMOS"]]}, {"id": "1611.09177", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (LIMOS), Le Gruenwald", "title": "A comparison study of object-oriented database clustering techniques", "comments": "arXiv admin note: text overlap with arXiv:1611.09176", "journal-ref": "Information Sciences, Elsevier, 1996, 94 (1-4), pp.55-86", "doi": "10.1016/0020-0255(96)00119-3", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely acknowledged that a good object clustering is critical to the\nperformance of OODBs. Clustering means storing related objects close together\non secondary storage so that when one object is accessed from disk, all its\nrelated objects are also brought into memory. Then access to these related\nobjects is a main memory access that is much faster than a disk access. The aim\nof this paper is to compare the performance of three clustering algorithms:\nCactis, CK and ORION. Simulation experiments we performed showed that the\nCactis algorithm is better than the ORION algorithm and that the CK algorithm\ntotally out-performs both other algorithms in terms of response time and\nclustering overhead.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:19:52 GMT"}], "update_date": "2017-01-01", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "LIMOS"], ["Gruenwald", "Le", ""]]}, {"id": "1611.09691", "submitter": "Shichao Zhang", "authors": "Shichao Zhang", "title": "Data Partitioning View of Mining Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two main approximations of mining big data in memory. One is to\npartition a big dataset to several subsets, so as to mine each subset in\nmemory. By this way, global patterns can be obtained by synthesizing all local\npatterns discovered from these subsets. Another is the statistical sampling\nmethod. This indicates that data partitioning should be an important strategy\nfor mining big data. This paper recalls our work on mining big data with a data\npartitioning and shows some interesting findings among the local patterns\ndiscovered from subsets of a dataset.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 16:05:56 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Zhang", "Shichao", ""]]}]