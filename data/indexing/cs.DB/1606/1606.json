[{"id": "1606.00046", "submitter": "Oliver Kennedy", "authors": "Juliana Freire, Boris Glavic, Oliver Kennedy, Heiko Mueller", "title": "The Exception that Improves the Rule", "comments": "Authors in alphabetical order; Preprint for HILDA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The database community has developed numerous tools and techniques for data\ncuration and exploration, from declarative languages, to specialized techniques\nfor data repair, and more. Yet, there is currently no consensus on how to best\nexpose these powerful tools to an analyst in a simple, intuitive, and above\nall, flexible way. Thus, analysts continue to rely on tools such as\nspreadsheets, imperative languages, and notebook style programming environments\nlike Jupyter for data curation. In this work, we explore the integration of\nspreadsheets, notebooks, and relational databases. We focus on a key advantage\nthat both spreadsheets and imperative notebook environments have over classical\nrelational databases: ease of exception. By relying on set-at-a-time\noperations, relational databases sacrifice the ability to easily define\nsingleton operations, exceptions to a normal data processing workflow that\naffect query processing for a fixed set of explicitly targeted records. In\ncomparison, a spreadsheet user can easily change the formula for just one cell,\nwhile a notebook user can add an imperative operation to her notebook that\nalters an output 'view'. We believe that enabling such idiosyncratic manual\ntransformations in a classical relational database is critical for curation, as\ncuration operations that are easy to declare for individual values can often be\nextremely challenging to generalize. We explore the challenges of enabling\nsingletons in relational databases, propose a hybrid spreadsheet/relational\nnotebook environment for data curation, and present our vision of Vizier, a\nsystem that exposes data curation through such an interface.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 21:04:34 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Freire", "Juliana", ""], ["Glavic", "Boris", ""], ["Kennedy", "Oliver", ""], ["Mueller", "Heiko", ""]]}, {"id": "1606.00200", "submitter": "Yikai Zhang", "authors": "Yikai Zhang, Jeffrey Xu Yu, Ying Zhang, Lu Qin", "title": "A Fast Order-Based Approach for Core Maintenance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs have been widely used in many applications such as social networks,\ncollaboration networks, and biological networks. One important graph analytics\nis to explore cohesive subgraphs in a large graph. Among several cohesive\nsubgraphs studied, k-core is one that can be computed in linear time for a\nstatic graph. Since graphs are evolving in real applications, in this paper, we\nstudy core maintenance which is to reduce the computational cost to compute\nk-cores for a graph when graphs are updated from time to time dynamically. We\nidentify drawbacks of the existing efficient algorithm, which needs a large\nsearch space to find the vertices that need to be updated, and has high\noverhead to maintain the index built, when a graph is updated. We propose a new\norder-based approach to maintain an order, called k-order, among vertices,\nwhile a graph is updated. Our new algorithm can significantly outperform the\nstate-of-the-art algorithm up to 3 orders of magnitude for the 11 large real\ngraphs tested. We report our findings in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 09:59:17 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 17:53:39 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Zhang", "Yikai", ""], ["Yu", "Jeffrey Xu", ""], ["Zhang", "Ying", ""], ["Qin", "Lu", ""]]}, {"id": "1606.00295", "submitter": "Jimi Sanchez", "authors": "Jimi Sanchez", "title": "A Review of Star Schema Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the Star Schema Benchmark, an alternative to the flawed\nTPC-H decision support system and presents reasons why this benchmark should be\nadopted over the industry standard for decision support systems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 14:07:56 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Sanchez", "Jimi", ""]]}, {"id": "1606.00480", "submitter": "Vinh Nguyen", "authors": "Vinh Nguyen, Jyoti Leeka, Olivier Bodenreider, Amit Sheth", "title": "A Formal Graph Model for RDF and Its Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formalizing an RDF abstract graph model to be compatible with the RDF formal\nsemantics has remained one of the foundational problems in the Semantic Web. In\nthis paper, we propose a new formal graph model for RDF datasets. This model\nallows us to express the current model-theoretic semantics in the form of a\ngraph. We also propose the concepts of resource path and triple path as well as\nan algorithm for traversing the new graph. We demonstrate the feasibility of\nthis graph model through two implementations: one is a new graph engine called\nGraphKE, and the other is extended from RDF-3X to show that existing systems\ncan also benefit from this model. In order to evaluate the empirical aspect of\nour graph model, we choose the shortest path algorithm and implement it in the\nGraphKE and the RDF-3X. Our experiments on both engines for finding the\nshortest paths in the YAGO2S-SP dataset give decent performance in terms of\nexecution time. The empirical results show that our graph model with\nwell-defined semantics can be effectively implemented.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 21:51:38 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Nguyen", "Vinh", ""], ["Leeka", "Jyoti", ""], ["Bodenreider", "Olivier", ""], ["Sheth", "Amit", ""]]}, {"id": "1606.00740", "submitter": "Kenneth Baclawski", "authors": "Kenneth Baclawski", "title": "The Meaning of Null in Databases and Programming Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The meaning of null in relational databases is a major source of confusion\nnot only among database users but also among database textbook writers. The\npurpose of this article is to examine what database nulls could mean and to\nmake some modest suggestions about how to reduce the confusion.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 16:07:15 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Baclawski", "Kenneth", ""]]}, {"id": "1606.00996", "submitter": "Aviv Yehezkel", "authors": "Reuven Cohen, Liran Katzir and Aviv Yehezkel", "title": "A Minimal Variance Estimator for the Cardinality of Big Data Set\n  Intersection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been a growing interest in developing \"streaming\nalgorithms\" for efficient processing and querying of continuous data streams.\nThese algorithms seek to provide accurate results while minimizing the required\nstorage and the processing time, at the price of a small inaccuracy in their\noutput. A fundamental query of interest is the intersection size of two big\ndata streams. This problem arises in many different application areas, such as\nnetwork monitoring, database systems, data integration and information\nretrieval. In this paper we develop a new algorithm for this problem, based on\nthe Maximum Likelihood (ML) method. We show that this algorithm outperforms all\nknown schemes and that it asymptotically achieves the optimal variance.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 07:51:56 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Cohen", "Reuven", ""], ["Katzir", "Liran", ""], ["Yehezkel", "Aviv", ""]]}, {"id": "1606.01081", "submitter": "Rod Moten", "authors": "Rod Moten, Kemafor Anyanwu-Ogan, Sahibi Miranshah", "title": "Implementing graph grammars for intelligence analysis in OCaml", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on implementing graph grammars for intelligence analysis in OCaml.\nGraph grammars are represented as elements of an algebraic data type in OCaml.\nIn addition to algebraic data types, we use other concepts from functional\nprogramming languages to implement features of graph grammars. We use type\nchecking to perform graph pattern matching. Graph transformations are defined\nas implicit coercions derived from structural subtyping proofs, subset types,\nlambda abstractions, and analytics. An analytic is a general-purpose OCaml\nfunction whose output is required to match a graph pattern described by an\nelement of an algebraic data type. By using a strongly-typed language for\nrepresenting graphs, we can ensure graphs produced from a graph transformation\nwill match a specific schema. This is a high priority requirement for\nintelligence analysis.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 13:28:37 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Moten", "Rod", ""], ["Anyanwu-Ogan", "Kemafor", ""], ["Miranshah", "Sahibi", ""]]}, {"id": "1606.01206", "submitter": "Pablo Barcelo", "authors": "Pablo Barcelo and Miguel Romero", "title": "The complexity of reverse engineering problems for conjunctive queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reverse engineering problems for conjunctive queries (CQs), such as query by\nexample (QBE) or definability, take a set of user examples and convert them\ninto an explanatory CQ. Despite their importance, the complexity of these\nproblems is prohibitively high (coNEXPTIME-complete). We isolate their two main\nsources of complexity and propose relaxations of them that reduce the\ncomplexity while having meaningful theoretical interpretations. The first\nrelaxation is based on the idea of using existential pebble games for\napproximating homomorphism tests. We show that this characterizes\nQBE/definability for CQs up to treewidth $k$, while reducing the complexity to\nEXPTIME. As a side result, we obtain that the complexity of the\nQBE/definability problems for CQs of treewidth $k$ is EXPTIME-complete for each\n$k \\geq 1$. The second relaxation is based on the idea of \"desynchronizing\"\ndirect products, which characterizes QBE/definability for unions of CQs and\nreduces the complexity to coNP. The combination of these two relaxations yields\ntractability for QBE and characterizes it in terms of unions of CQs of\ntreewidth at most $k$. We also study the complexity of these problems for\nconjunctive regular path queries over graph databases, showing them to be no\nmore difficult than for CQs.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 18:14:35 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 19:28:17 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Barcelo", "Pablo", ""], ["Romero", "Miguel", ""]]}, {"id": "1606.01340", "submitter": "Ruifeng Liu", "authors": "Ruifeng Liu, Ada WaiChee Fu, Zitong Chen, Silu Huang, Yubao Liu", "title": "Finding Multiple New Optimal Locations in a Road Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of optimal location querying for location based services\nin road networks, which aims to find locations for new servers or facilities.\nThe existing optimal solutions on this problem consider only the cases with one\nnew server. When two or more new servers are to be set up, the problem with\nminmax cost criteria, MinMax, becomes NP-hard. In this work we identify some\nuseful properties about the potential locations for the new servers, from which\nwe derive a novel algorithm for MinMax, and show that it is efficient when the\nnumber of new servers is small. When the number of new servers is large, we\npropose an efficient 3-approximate algorithm. We verify with experiments on\nreal road networks that our solutions are effective and attains significantly\nbetter result quality compared to the existing greedy algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 07:42:16 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 02:56:15 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Liu", "Ruifeng", ""], ["Fu", "Ada WaiChee", ""], ["Chen", "Zitong", ""], ["Huang", "Silu", ""], ["Liu", "Yubao", ""]]}, {"id": "1606.01441", "submitter": "Daniel Hern\\'andez", "authors": "Daniel Hern\\'andez and Claudio Gutierrez and Renzo Angles", "title": "Correlation and Substitution in SPARQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the current SPARQL specification the notion of correlation and\nsubstitution are not well defined. This problem triggers several ambiguities in\nthe semantics. In fact, implementations as Fuseki and Virtuoso assume different\nsemantics.\n  In this technical report, we provide a semantics of correlation and\nsubstitution following the classic philosophy of substitution and correlation\nin logic, programming languages and SQL. We think this proposal not only fix\nthe current ambiguities and problems, but helps to set a safe formal base to\nfurther extensions of the language.\n  This work is part of an ongoing work of Daniel Hernandez. These anomalies in\nthe W3C Specification of SPARQL 1.1 were detected early and reported no later\nthan 2014, when two erratas were registered (cf.\nhttps://www.w3.org/2013/sparql-errata#errata-query-8 and\nhttps://www.w3.org/2013/sparql-errata#errata-query-10).\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 00:43:44 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 03:11:13 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Hern\u00e1ndez", "Daniel", ""], ["Gutierrez", "Claudio", ""], ["Angles", "Renzo", ""]]}, {"id": "1606.01742", "submitter": "Johann Eder", "authors": "Claus Dabringer and Johann Eder", "title": "Adaptive Distributed Top-k Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ADiT is an adaptive approach for processing distributed top-$k$ queries over\npeer-to-peer networks optimizing both system load and query response time. This\napproach considers the size of the peer to peer network, the amount $k$ of\nsearched objects, the network capabilities of a connected peer, i.e. the\ntransmission rate, the amount of objects stored on each peer, and the speed of\na peer in processing a local top-$k$ query. In extensive experiments with a\nvariety of scenarios we could show that ADiT outperforms state of the art\ndistributed query processing techniques.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 13:48:34 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Dabringer", "Claus", ""], ["Eder", "Johann", ""]]}, {"id": "1606.01924", "submitter": "Robert Rovetto", "authors": "Robert John Rovetto, T.S. Kelso", "title": "Preliminaries of a Space Situational Awareness Ontology", "comments": "Paper presented at 26th AIAA/AAS Space Flight Mechanics meeting,\n  Napa, CA. USA, February 14-18, 2016. Forthcoming in Advances in the\n  Astronautics, Univelt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space situational awareness (SSA) is vital for international safety and\nsecurity, and the future of space travel. By improving SSA data-sharing we\nimprove global SSA. Computational ontology may provide one means toward that\ngoal. This paper develops the ontology of the SSA domain and takes steps in the\ncreation of the space situational awareness ontology. Ontology objectives,\nrequirements and desiderata are outlined; and both the SSA domain and the\ndiscipline of ontology are described. The purposes of the ontology include:\nexploring the potential for ontology development and engineering to (i)\nrepresent SSA data, general domain knowledge, objects and relationships (ii)\nannotate and express the meaning of that data, and (iii) foster SSA\ndata-exchange and integration among SSA actors, orbital debris databases, space\nobject catalogs and other SSA data repositories. By improving SSA via data- and\nknowledge-sharing, we can (iv) expand our scientific knowledge of the space\nenvironment, (v) advance our capacity for planetary defense from near-Earth\nobjects, and (vi) ensure the future of safe space flight for generations to\ncome.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 19:37:14 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 00:38:21 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Rovetto", "Robert John", ""], ["Kelso", "T. S.", ""]]}, {"id": "1606.01930", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Loreto Bravo", "title": "Consistency and Trust in Peer Data Exchange Systems", "comments": "To appear in Theory and Practice of Logic Programming (TPLP). It\n  includes appendix that will be published only in electronic format", "journal-ref": null, "doi": "10.1017/S147106841600017X", "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and investigate a semantics for \"peer data exchange systems\" where\ndifferent peers are related by data exchange constraints and trust\nrelationships. These two elements plus the data at the peers' sites and their\nlocal integrity constraints are made compatible via a semantics that\ncharacterizes sets of \"solution instances\" for the peers. They are the intended\n-possibly virtual- instances for a peer that are obtained through a data repair\nsemantics that we introduce and investigate. The semantically correct answers\nfrom a peer to a query, the so-called \"peer consistent answers\", are defined as\nthose answers that are invariant under all its different solution instances. We\nshow that solution instances can be specified as the models of logic programs\nwith a stable model semantics. The repair semantics is based on null values as\nused in SQL databases, and is also of independent interest for repairs of\nsingle databases with respect to integrity constraints.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 20:26:42 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Bravo", "Loreto", ""]]}, {"id": "1606.01957", "submitter": "Hasan Jamil", "authors": "Hasan M. Jamil and Fereidoon Sadri", "title": "Reliable Querying of Very Large, Fast Moving and Noisy Predicted\n  Interaction Data using Hierarchical Crowd Curation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of predicted and mined but uncertain biological data show huge\nneeds for massive, efficient and scalable curation efforts. The human expertise\nwarranted by any successful curation enterprize is often economically\nprohibitive especially for speculative end user queries that may not ultimately\nbear fruit. So the challenge remains in devising a low cost engine capable of\ndelivering fast but tentative annotation and curation of a set of data items\nthat can be authoritatively validated by experts later demanding significantly\nsmall investment. The aim thus is to make a large volume of predicted data\navailable for use as early as possible with an acceptable degree of confidence\nin their accuracy while the curation continues. In this paper, we present a\nnovel approach to annotation and curation of biological database contents using\ncrowd computing. The technical contribution is in the identification and\nmanagement of trust of mechanical turks, and support for ad hoc declarative\nqueries, both of which are leveraged to support reliable analytics using noisy\npredicted interactions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 22:06:26 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Jamil", "Hasan M.", ""], ["Sadri", "Fereidoon", ""]]}, {"id": "1606.02015", "submitter": "Lei Guo", "authors": "Lei Guo, Dejun Teng, Rubao Lee, Feng Chen, Siyuan Ma, Xiaodong Zhang", "title": "Re-enabling high-speed caching for LSM-trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSM-tree has been widely used in cloud computing systems by Google, Facebook,\nand Amazon, to achieve high performance for write-intensive workloads. However,\nin LSM-tree, random key-value queries can experience long latency and low\nthroughput due to the interference from the compaction, a basic operation in\nthe algorithm, to caching. LSM-tree relies on frequent compaction operations to\nmerge data into a sorted structure. After a compaction, the original data are\nreorganized and written to other locations on the disk. As a result, the cached\ndata are invalidated since their referencing addresses are changed, causing\nserious performance degradations.\n  We propose dLSM in order to re-enable high-speed caching during intensive\nwrites. dLSM is an LSM-tree with a compaction buffer on the disk, working as a\ncushion to minimize the cache invalidation caused by compactions. The\ncompaction buffer maintains a series of snapshots of the frequently compacted\ndata, which represent a consistent view of the corresponding data in the\nunderlying LSM-tree. Being updated in a much lower rate than that of\ncompactions, data in the compaction buffer are almost stationary. In dLSM, an\nobject is referenced by the disk address of the corresponding block either in\nthe compaction buffer for frequently compacted data, or in the underlying\nLSM-tree for infrequently compacted data. Thus, hot objects can be effectively\nkept in the cache without harmful invalidations. With the help of a small\non-disk compaction buffer, dLSM achieves a high query performance by enabling\neffective caching, while retaining all merits of LSM-tree for write-intensive\ndata processing. We have implemented dLSM based on LevelDB. Our evaluations\nshow that with a standard DRAM cache, dLSM can achieve 5--8x performance\nimprovement over LSM with the same cache on HDD storage.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 03:53:45 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Guo", "Lei", ""], ["Teng", "Dejun", ""], ["Lee", "Rubao", ""], ["Chen", "Feng", ""], ["Ma", "Siyuan", ""], ["Zhang", "Xiaodong", ""]]}, {"id": "1606.02208", "submitter": "Kalyani Natu", "authors": "Kalyani Natu", "title": "Initialization Errors in Quantum Data Base Recall", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the relationship between initialization error and recall\nof a specific memory in the Grover algorithm for quantum database search. It is\nshown that the correct memory is obtained with high probability even when the\ninitial state is far removed from the correct one. The analysis is done by\nrelating the variance of error in the initial state to the recovery of the\ncorrect memory and the surprising result is obtained that the relationship\nbetween the two is essentially linear.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 16:46:53 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Natu", "Kalyani", ""]]}, {"id": "1606.02237", "submitter": "Alexandr Savinov", "authors": "Alexandr Savinov", "title": "Concept-Oriented Model: the Functional View", "comments": "32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plethora of existing data models and specific data modeling techniques is\nnot only confusing but leads to complex, eclectic and inefficient designs of\nsystems for data management and analytics. The main goal of this paper is to\ndescribe a unified approach to data modeling, called the concept-oriented model\n(COM), by using functions as a basis for its formalization. COM tries to answer\nthe question what is data and to rethink basic assumptions underlying this and\nrelated notions. Its main goal is to unify major existing views on data\n(generality), using only a few main notions (simplicity) which are very close\nto how data is used in real life (naturalness).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 18:05:50 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Savinov", "Alexandr", ""]]}, {"id": "1606.02250", "submitter": "Oliver Kennedy", "authors": "Poonam Kumari, Said Achmiz, Oliver Kennedy", "title": "Communicating Data Quality in On-Demand Curation", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-demand curation (ODC) tools like Paygo, KATARA, and Mimir allow users to\ndefer expensive curation effort until it is necessary. In contrast to classical\ndatabases that do not respond to queries over potentially erroneous data, ODC\nsystems instead answer with guesses or approximations. The quality and scope of\nthese guesses may vary and it is critical that an ODC system be able to\ncommunicate this information to an end-user. The central contribution of this\npaper is a preliminary user study evaluating the cognitive burden and\nexpressiveness of four representations of \"attribute-level\" uncertainty. The\nstudy shows (1) insignificant differences in time taken for users to interpret\nthe four types of uncertainty tested, and (2) that different presentations of\nuncertainty change the way people interpret and react to data. Ultimately, we\nshow that a set of UI design guidelines and best practices for conveying\nuncertainty will be necessary for ODC tools to be effective. This paper\nrepresents the first step towards establishing such guidelines.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 18:31:54 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Kumari", "Poonam", ""], ["Achmiz", "Said", ""], ["Kennedy", "Oliver", ""]]}, {"id": "1606.02314", "submitter": "Sutanay Choudhury", "authors": "Sutanay Choudhury and Khushbu Agarwal and Sumit Purohit and Baichuan\n  Zhang and Meg Pirrung and Will Smith and Mathew Thomas", "title": "NOUS: Construction and Querying of Dynamic Knowledge Graphs", "comments": "Codebase: https://github.com/streaming-graphs/NOUS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to construct domain specific knowledge graphs (KG) and perform\nquestion-answering or hypothesis generation is a transformative capability.\nDespite their value, automated construction of knowledge graphs remains an\nexpensive technical challenge that is beyond the reach for most enterprises and\nacademic institutions. We propose an end-to-end framework for developing custom\nknowledge graph driven analytics for arbitrary application domains. The\nuniqueness of our system lies A) in its combination of curated KGs along with\nknowledge extracted from unstructured text, B) support for advanced trending\nand explanatory questions on a dynamic KG, and C) the ability to answer queries\nwhere the answer is embedded across multiple data sources.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 20:02:04 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Choudhury", "Sutanay", ""], ["Agarwal", "Khushbu", ""], ["Purohit", "Sumit", ""], ["Zhang", "Baichuan", ""], ["Pirrung", "Meg", ""], ["Smith", "Will", ""], ["Thomas", "Mathew", ""]]}, {"id": "1606.02573", "submitter": "Edgars Rencis", "authors": "Janis Barzdins, Mikus Grasmanis, Edgars Rencis, Agris Sostaks, Juris\n  Barzdins", "title": "Self-service Ad-hoc Querying Using Controlled Natural Language", "comments": "13 pages, 4 figures, 2 tables, accepted for publication in the 12th\n  International Baltic Conference on Databases and Information Systems 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ad-hoc querying process is slow and error prone due to inability of\nbusiness experts of accessing data directly without involving IT experts. The\nproblem lies in complexity of means used to query data. We propose a new\nnatural language- and semistar ontology-based ad-hoc querying approach which\nlowers the steep learning curve required to be able to query data. The proposed\napproach would significantly shorten the time needed to master the ad-hoc\nquerying and to gain the direct access to data by business experts, thus\nfacilitating the decision making process in enterprises, government\ninstitutions and other organizations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 14:24:07 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Barzdins", "Janis", ""], ["Grasmanis", "Mikus", ""], ["Rencis", "Edgars", ""], ["Sostaks", "Agris", ""], ["Barzdins", "Juris", ""]]}, {"id": "1606.03935", "submitter": "Matej Mihel\\v{c}i\\'c", "authors": "Matej Mihel\\v{c}i\\'c, Sa\\v{s}o D\\v{z}eroski, Nada Lavra\\v{c}, Tomislav\n  \\v{S}muc", "title": "A framework for redescription set construction", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2016.10.012", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redescription mining is a field of knowledge discovery that aims at finding\ndifferent descriptions of similar subsets of instances in the data. These\ndescriptions are represented as rules inferred from one or more disjoint sets\nof attributes, called views. As such, they support knowledge discovery process\nand help domain experts in formulating new hypotheses or constructing new\nknowledge bases and decision support systems. In contrast to previous\napproaches that typically create one smaller set of redescriptions satisfying a\npre-defined set of constraints, we introduce a framework that creates large and\nheterogeneous redescription set from which user/expert can extract compact sets\nof differing properties, according to its own preferences. Construction of\nlarge and heterogeneous redescription set relies on CLUS-RM algorithm and a\nnovel, conjunctive refinement procedure that facilitates generation of larger\nand more accurate redescription sets. The work also introduces the variability\nof redescription accuracy when missing values are present in the data, which\nsignificantly extends applicability of the method. Crucial part of the\nframework is the redescription set extraction based on heuristic\nmulti-objective optimization procedure that allows user to define importance\nlevels towards one or more redescription quality criteria. We provide both\ntheoretical and empirical comparison of the novel framework against current\nstate of the art redescription mining algorithms and show that it represents\nmore efficient and versatile approach for mining redescriptions from data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 13:15:41 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 16:41:06 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Mihel\u010di\u0107", "Matej", ""], ["D\u017eeroski", "Sa\u0161o", ""], ["Lavra\u010d", "Nada", ""], ["\u0160muc", "Tomislav", ""]]}, {"id": "1606.03957", "submitter": "Erika Varga-Vereb\\'elyi", "authors": "L\\'aszl\\'o Dobos, Erika Varga-Vereb\\'elyi, Eva Verdugo, David\n  Teyssier, Katrina Exter, Ivan Valtchanov, Tam\\'as Budav\\'ari and Csaba Kiss", "title": "The Footprint Database and Web Services of the Herschel Space\n  Observatory", "comments": "Accepted for publication in Experimental Astronomy", "journal-ref": null, "doi": "10.1007/s10686-016-9502-5", "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data from the Herschel Space Observatory is freely available to the public\nbut no uniformly processed catalogue of the observations has been published so\nfar. To date, the Herschel Science Archive does not contain the exact sky\ncoverage (footprint) of individual observations and supports search for\nmeasurements based on bounding circles only. Drawing on previous experience in\nimplementing footprint databases, we built the Herschel Footprint Database and\nWeb Services for the Herschel Space Observatory to provide efficient search\ncapabilities for typical astronomical queries. The database was designed with\nthe following main goals in mind: (a) provide a unified data model for\nmeta-data of all instruments and observational modes, (b) quickly find\nobservations covering a selected object and its neighbourhood, (c) quickly find\nevery observation in a larger area of the sky, (d) allow for finding solar\nsystem objects crossing observation fields. As a first step, we developed a\nunified data model of observations of all three Herschel instruments for all\npointing and instrument modes. Then, using telescope pointing information and\nobservational meta-data, we compiled a database of footprints. As opposed to\nmethods using pixellation of the sphere, we represent sky coverage in an exact\ngeometric form allowing for precise area calculations. For easier handling of\nHerschel observation footprints with rather complex shapes, two algorithms were\nimplemented to reduce the outline. Furthermore, a new visualisation tool to\nplot footprints with various spherical projections was developed. Indexing of\nthe footprints using Hierarchical Triangular Mesh makes it possible to quickly\nfind observations based on sky coverage, time and meta-data. The database is\naccessible via a web site (http://herschel.vo.elte.hu) and also as a set of\nREST web service functions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 14:05:52 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Dobos", "L\u00e1szl\u00f3", ""], ["Varga-Vereb\u00e9lyi", "Erika", ""], ["Verdugo", "Eva", ""], ["Teyssier", "David", ""], ["Exter", "Katrina", ""], ["Valtchanov", "Ivan", ""], ["Budav\u00e1ri", "Tam\u00e1s", ""], ["Kiss", "Csaba", ""]]}, {"id": "1606.04722", "submitter": "Xi Wu", "authors": "Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, Jeffrey\n  F. Naughton", "title": "Bolt-on Differential Privacy for Scalable Stochastic Gradient\n  Descent-based Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While significant progress has been made separately on analytics systems for\nscalable stochastic gradient descent (SGD) and private SGD, none of the major\nscalable analytics frameworks have incorporated differentially private SGD.\nThere are two inter-related issues for this disconnect between research and\npractice: (1) low model accuracy due to added noise to guarantee privacy, and\n(2) high development and runtime overhead of the private algorithms. This paper\ntakes a first step to remedy this disconnect and proposes a private SGD\nalgorithm to address \\emph{both} issues in an integrated manner. In contrast to\nthe white-box approach adopted by previous work, we revisit and use the\nclassical technique of {\\em output perturbation} to devise a novel \"bolt-on\"\napproach to private SGD. While our approach trivially addresses (2), it makes\n(1) even more challenging. We address this challenge by providing a novel\nanalysis of the $L_2$-sensitivity of SGD, which allows, under the same privacy\nguarantees, better convergence of SGD when only a constant number of passes can\nbe made over the data. We integrate our algorithm, as well as other\nstate-of-the-art differentially private SGD, into Bismarck, a popular scalable\nSGD-based analytics system on top of an RDBMS. Extensive experiments show that\nour algorithm can be easily integrated, incurs virtually no overhead, scales\nwell, and most importantly, yields substantially better (up to 4X) test\naccuracy than the state-of-the-art algorithms on many real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 11:14:29 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 16:26:59 GMT"}, {"version": "v3", "created": "Thu, 23 Mar 2017 17:35:09 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Wu", "Xi", ""], ["Li", "Fengan", ""], ["Kumar", "Arun", ""], ["Chaudhuri", "Kamalika", ""], ["Jha", "Somesh", ""], ["Naughton", "Jeffrey F.", ""]]}, {"id": "1606.05053", "submitter": "T\\^an Th\\^ong Nguy\\^en", "authors": "Th\\^ong T. Nguy\\^en, Xiaokui Xiao, Yin Yang, Siu Cheung Hui, Hyejin\n  Shin, Junbum Shin", "title": "Collecting and Analyzing Data from Smart Device Users with Local\n  Differential Privacy", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations with a large user base, such as Samsung and Google, can\npotentially benefit from collecting and mining users' data. However, doing so\nraises privacy concerns, and risks accidental privacy breaches with serious\nconsequences. Local differential privacy (LDP) techniques address this problem\nby only collecting randomized answers from each user, with guarantees of\nplausible deniability; meanwhile, the aggregator can still build accurate\nmodels and predictors by analyzing large amounts of such randomized data. So\nfar, existing LDP solutions either have severely restricted functionality, or\nfocus mainly on theoretical aspects such as asymptotical bounds rather than\npractical usability and performance. Motivated by this, we propose Harmony, a\npractical, accurate and efficient system for collecting and analyzing data from\nsmart device users, while satisfying LDP. Harmony applies to multi-dimensional\ndata containing both numerical and categorical attributes, and supports both\nbasic statistics (e.g., mean and frequency estimates), and complex machine\nlearning tasks (e.g., linear regression, logistic regression and SVM\nclassification). Experiments using real data confirm Harmony's effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 05:25:58 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Nguy\u00ean", "Th\u00f4ng T.", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Yin", ""], ["Hui", "Siu Cheung", ""], ["Shin", "Hyejin", ""], ["Shin", "Junbum", ""]]}, {"id": "1606.05633", "submitter": "Burak Y{\\i}ld{\\i}z", "authors": "Burak Y{\\i}ld{\\i}z, Tolga B\\\"uy\\\"uktan{\\i}r and Fatih Emekci", "title": "Equi-depth Histogram Construction for Big Data with Quality Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of data generated and stored in cloud systems has been increasing\nexponentially. The examples of data include user generated data, machine\ngenerated data as well as data crawled from the Internet. There have been\nseveral frameworks with proven efficiency to store and process the petabyte\nscale data such as Apache Hadoop, HDFS and several NoSQL frameworks. These\nsystems have been widely used in industry and thus are subject to several\nresearch. The proposed data processing techniques should be compatible with the\nabove frameworks in order to be practical. One of the key data operations is\nderiving equi-depth histograms as they are crucial in understanding the\nstatistical properties of the underlying data with many applications including\nquery optimization. In this paper, we focus on approximate equi-depth histogram\nconstruction for big data and propose a novel merge based histogram\nconstruction method with a histogram processing framework which constructs an\nequi-depth histogram for a given time interval. The proposed method constructs\napproximate equi-depth histograms by merging exact equi-depth histograms of\npartitioned data by guaranteeing a maximum error bound on the number of items\nin a bucket (bucket size) as well as any range on the histogram.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 19:45:03 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Y\u0131ld\u0131z", "Burak", ""], ["B\u00fcy\u00fcktan\u0131r", "Tolga", ""], ["Emekci", "Fatih", ""]]}, {"id": "1606.05708", "submitter": "Kristi Morton", "authors": "Kristi Morton, Hannaneh Hajishirzi, Magdalena Balazinska, Dan Grossman", "title": "View-Driven Deduplication with Active Learning", "comments": "13 pgs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analytics systems such as Tableau are increasingly popular for\ninteractive data exploration. These tools, however, do not currently assist\nusers with detecting or resolving potential data quality problems including the\nwell-known deduplication problem. Recent approaches for deduplication focus on\ncleaning entire datasets and commonly require hundreds to thousands of user\nlabels. In this paper, we address the problem of deduplication in the context\nof visual data analytics. We present a new approach for record deduplication\nthat strives to produce the cleanest view possible with a limited budget for\ndata labeling. The key idea behind our approach is to consider the impact that\nindividual tuples have on a visualization and to monitor how the view changes\nduring cleaning. With experiments on nine different visualizations for two\nreal-world datasets, we show that our approach produces significantly cleaner\nviews for small labeling budgets than state-of-the-art alternatives and that it\nalso stops the cleaning process after requesting fewer labels.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 23:38:51 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Morton", "Kristi", ""], ["Hajishirzi", "Hannaneh", ""], ["Balazinska", "Magdalena", ""], ["Grossman", "Dan", ""]]}, {"id": "1606.05781", "submitter": "Xiufeng Liu", "authors": "Xiufeng Liu, Per Sieverts Nielsen", "title": "Regression-based Online Anomaly Detection for Smart Grid Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widely used smart meters in the energy sector, anomaly detection\nbecomes a crucial mean to study the unusual consumption behaviors of customers,\nand to discover unexpected events of using energy promptly. Detecting\nconsumption anomalies is, essentially, a real-time big data analytics problem,\nwhich does data mining on a large amount of parallel data streams from smart\nmeters. In this paper, we propose a supervised learning and statistical-based\nanomaly detection method, and implement a Lambda system using the in-memory\ndistributed computing framework, Spark and its extension Spark Streaming. The\nsystem supports not only iterative detection model refreshment from scalable\ndata sets, but also real-time detection on scalable live data streams. This\npaper empirically evaluates the system and the detection algorithm, and the\nresults show the effectiveness and the scalability of the proposed lambda\ndetection system.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 17:03:18 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Liu", "Xiufeng", ""], ["Nielsen", "Per Sieverts", ""]]}, {"id": "1606.05787", "submitter": "Xiufeng Liu", "authors": "Xiufeng Liu, Per Sieverts Nielsen", "title": "A Hybrid ICT-Solution for Smart Meter Data Analytics", "comments": "Energy, 2016", "journal-ref": null, "doi": "10.1016/j.energy.2016.05.068", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart meters are increasingly used worldwide. Smart meters are the advanced\nmeters capable of measuring energy consumption at a fine-grained time interval,\ne.g., every 15 minutes. Smart meter data are typically bundled with social\neconomic data in analytics, such as meter geographic locations, weather\nconditions and user information, which makes the data sets very sizable and the\nanalytics complex. Data mining and emerging cloud computing technologies make\ncollecting, processing, and analyzing the so-called big data possible. This\npaper proposes an innovative ICT-solution to streamline smart meter data\nanalytics. The proposed solution offers an information integration pipeline for\ningesting data from smart meters, a scalable platform for processing and mining\nbig data sets, and a web portal for visualizing analytics results. The\nimplemented system has a hybrid architecture of using Spark or Hive for big\ndata processing, and using the machine learning toolkit, MADlib, for doing\nin-database data analytics in PostgreSQL database. This paper evaluates the key\ntechnologies of the proposed ICT-solution, and the results show the\neffectiveness and efficiency of using the system for both batch and online\nanalytics.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 18:07:37 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Liu", "Xiufeng", ""], ["Nielsen", "Per Sieverts", ""]]}, {"id": "1606.05797", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Dylan Hutchison, Hayden Jananthan,\n  Timothy Mattson, Siddharth Samsi, Albert Reuther", "title": "Associative Array Model of SQL, NoSQL, and NewSQL Databases", "comments": "9 pages; 6 figures; accepted to IEEE High Performance Extreme\n  Computing (HPEC) conference 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761647", "report-no": null, "categories": "cs.DB cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of SQL, NoSQL, and NewSQL databases is a reflection of their\nability to provide significant functionality and performance benefits for\nspecific domains, such as financial transactions, internet search, and data\nanalysis. The BigDAWG polystore seeks to provide a mechanism to allow\napplications to transparently achieve the benefits of diverse databases while\ninsulating applications from the details of these databases. Associative arrays\nprovide a common approach to the mathematics found in different databases: sets\n(SQL), graphs (NoSQL), and matrices (NewSQL). This work presents the SQL\nrelational model in terms of associative arrays and identifies the key\nmathematical properties that are preserved within SQL. These properties include\nassociativity, commutativity, distributivity, identities, annihilators, and\ninverses. Performance measurements on distributivity and associativity show the\nimpact these properties can have on associative array operations. These results\ndemonstrate that associative arrays could provide a mathematical model for\npolystores to optimize the exchange of data and execution queries.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 19:29:17 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Hutchison", "Dylan", ""], ["Jananthan", "Hayden", ""], ["Mattson", "Timothy", ""], ["Samsi", "Siddharth", ""], ["Reuther", "Albert", ""]]}, {"id": "1606.06066", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Reinder Haakma, Wil M. P. van der Aalst", "title": "Mining Local Process Models", "comments": "Published in Elsevier's Journal of Innovation in Digital Ecosystems,\n  Special Issue on Data Mining", "journal-ref": "Journal of Innovation in Digital Ecosystems volume 3 issue 2\n  (2016), pages 183-196", "doi": "10.1016/j.jides.2016.11.001", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a method to discover frequent behavioral patterns\nin event logs. We express these patterns as \\emph{local process models}. Local\nprocess model mining can be positioned in-between process discovery and episode\n/ sequential pattern mining. The technique presented in this paper is able to\nlearn behavioral patterns involving sequential composition, concurrency, choice\nand loop, like in process mining. However, we do not look at start-to-end\nmodels, which distinguishes our approach from process discovery and creates a\nlink to episode / sequential pattern mining. We propose an incremental\nprocedure for building local process models capturing frequent patterns based\non so-called process trees. We propose five quality dimensions and\ncorresponding metrics for local process models, given an event log. We show\nmonotonicity properties for some quality dimensions, enabling a speedup of\nlocal process model discovery through pruning. We demonstrate through a real\nlife case study that mining local patterns allows us to get insights in\nprocesses where regular start-to-end process discovery techniques are only able\nto learn unstructured, flower-like, models.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 11:28:26 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 17:23:10 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["Haakma", "Reinder", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1606.06269", "submitter": "Yanhong Annie Liu", "authors": "Yanhong A. Liu and Scott D. Stoller", "title": "Founded Semantics and Constraint Semantics of Logic Rules", "comments": null, "journal-ref": "Journal of Logic and Computation, Volume 30, Issue 8, December\n  2020, Pages 1609-1668", "doi": "10.1093/logcom/exaa056", "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic rules and inference are fundamental in computer science and have been\nstudied extensively. However, prior semantics of logic languages can have\nsubtle implications and can disagree significantly, on even very simple\nprograms, including in attempting to solve the well-known Russell's paradox.\nThese semantics are often non-intuitive and hard-to-understand when\nunrestricted negation is used in recursion.\n  This paper describes a simple new semantics for logic rules, founded\nsemantics, and its straightforward extension to another simple new semantics,\nconstraint semantics, that unify the core of different prior semantics. The new\nsemantics support unrestricted negation, as well as unrestricted existential\nand universal quantifications. They are uniquely expressive and intuitive by\nallowing assumptions about the predicates, rules, and reasoning to be specified\nexplicitly, as simple and precise binary choices. They are completely\ndeclarative and relate cleanly to prior semantics. In addition, founded\nsemantics can be computed in linear time in the size of the ground program.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 19:48:20 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 18:28:29 GMT"}, {"version": "v3", "created": "Sat, 15 Apr 2017 00:24:14 GMT"}, {"version": "v4", "created": "Thu, 26 Mar 2020 15:34:32 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Liu", "Yanhong A.", ""], ["Stoller", "Scott D.", ""]]}, {"id": "1606.06808", "submitter": "Johes Bater", "authors": "Johes Bater, Gregory Elliott, Craig Eggen, Satyender Goel, Abel Kho,\n  Jennie Rogers", "title": "SMCQL: Secure Querying for Federated Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People and machines are collecting data at an unprecedented rate. Despite\nthis newfound abundance of data, progress has been slow in sharing it for open\nscience, business, and other data-intensive endeavors. Many such efforts are\nstymied by privacy concerns and regulatory compliance issues. For example, many\nhospitals are interested in pooling their medical records for research, but\nnone may disclose arbitrary patient records to researchers or other healthcare\nproviders. In this context we propose the Private Data Network (PDN), a\nfederated database for querying over the collective data of mutually\ndistrustful parties. In a PDN, each member database does not reveal its tuples\nto its peers nor to the query writer. Instead, the user submits a query to an\nhonest broker that plans and coordinates its execution over multiple private\ndatabases using secure multiparty computation (SMC). Here, each database's\nquery execution is oblivious, and its program counters and memory traces are\nagnostic to the inputs of others. We introduce a framework for executing PDN\nqueries named SMCQL. This system translates SQL statements into SMC primitives\nto compute query results over the union of its source databases without\nrevealing sensitive information about individual tuples to peer data providers\nor the honest broker. Only the honest broker and the querier receive the\nresults of a PDN query. For fast, secure query evaluation, we explore a\nheuristics-driven optimizer that minimizes the PDN's use of secure computation\nand partitions its query evaluation into scalable slices.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 02:45:39 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 20:15:00 GMT"}, {"version": "v3", "created": "Thu, 30 Jun 2016 18:44:51 GMT"}, {"version": "v4", "created": "Thu, 17 Nov 2016 07:37:41 GMT"}, {"version": "v5", "created": "Mon, 6 Mar 2017 21:42:17 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Bater", "Johes", ""], ["Elliott", "Gregory", ""], ["Eggen", "Craig", ""], ["Goel", "Satyender", ""], ["Kho", "Abel", ""], ["Rogers", "Jennie", ""]]}, {"id": "1606.07085", "submitter": "Dylan Hutchison", "authors": "Dylan Hutchison, Jeremy Kepner, Vijay Gadepally, Bill Howe", "title": "From NoSQL Accumulo to NewSQL Graphulo: Design and Utility of Graph\n  Algorithms inside a BigTable Database", "comments": "9 pages, to appear in 2016 IEEE High Performance Extreme Computing\n  Conference (HPEC)", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761577", "report-no": null, "categories": "cs.DB cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google BigTable's scale-out design for distributed key-value storage inspired\na generation of NoSQL databases. Recently the NewSQL paradigm emerged in\nresponse to analytic workloads that demand distributed computation local to\ndata storage. Many such analytics take the form of graph algorithms, a trend\nthat motivated the GraphBLAS initiative to standardize a set of matrix math\nkernels for building graph algorithms. In this article we show how it is\npossible to implement the GraphBLAS kernels in a BigTable database by\npresenting the design of Graphulo, a library for executing graph algorithms\ninside the Apache Accumulo database. We detail the Graphulo implementation of\ntwo graph algorithms and conduct experiments comparing their performance to two\nmain-memory matrix math systems. Our results shed insight into the conditions\nthat determine when executing a graph algorithm is faster inside a database\nversus an external system---in short, that memory requirements and relative I/O\nare critical factors.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 20:08:47 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 04:09:48 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Hutchison", "Dylan", ""], ["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Howe", "Bill", ""]]}, {"id": "1606.07259", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Reinder Haakma, Wil M. P. van der Aalst", "title": "Log-based Evaluation of Label Splits for Process Models", "comments": "Paper accepted at the 20th International Conference on\n  Knowledge-Based and Intelligent Information & Engineering Systems, to appear\n  in Procedia Computer Science", "journal-ref": "Procedia Computer Science, 96 (2016) 63-72", "doi": "10.1016/j.procs.2016.08.096", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining techniques aim to extract insights in processes from event\nlogs. One of the challenges in process mining is identifying interesting and\nmeaningful event labels that contribute to a better understanding of the\nprocess. Our application area is mining data from smart homes for elderly,\nwhere the ultimate goal is to signal deviations from usual behavior and provide\ntimely recommendations in order to extend the period of independent living.\nExtracting individual process models showing user behavior is an important\ninstrument in achieving this goal. However, the interpretation of sensor data\nat an appropriate abstraction level is not straightforward. For example, a\nmotion sensor in a bedroom can be triggered by tossing and turning in bed or by\ngetting up. We try to derive the actual activity depending on the context\n(time, previous events, etc.). In this paper we introduce the notion of label\nrefinements, which links more abstract event descriptions with their more\nrefined counterparts. We present a statistical evaluation method to determine\nthe usefulness of a label refinement for a given event log from a process\nperspective. Based on data from smart homes, we show how our statistical\nevaluation method for label refinements can be used in practice. Our method was\nable to select two label refinements out of a set of candidate label\nrefinements that both had a positive effect on model precision.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 10:29:48 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["Haakma", "Reinder", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1606.07550", "submitter": "Rok Sosic", "authors": "Jure Leskovec and Rok Sosic", "title": "SNAP: A General Purpose Network Analysis and Graph Mining Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large networks are becoming a widely used abstraction for studying complex\nsystems in a broad set of disciplines, ranging from social network analysis to\nmolecular biology and neuroscience. Despite an increasing need to analyze and\nmanipulate large networks, only a limited number of tools are available for\nthis task.\n  Here, we describe Stanford Network Analysis Platform (SNAP), a\ngeneral-purpose, high-performance system that provides easy to use, high-level\noperations for analysis and manipulation of large networks. We present SNAP\nfunctionality, describe its implementational details, and give performance\nbenchmarks. SNAP has been developed for single big-memory machines and it\nbalances the trade-off between maximum performance, compact in-memory graph\nrepresentation, and the ability to handle dynamic graphs where nodes and edges\nare being added or removed over time. SNAP can process massive networks with\nhundreds of millions of nodes and billions of edges. SNAP offers over 140\ndifferent graph algorithms that can efficiently manipulate large graphs,\ncalculate structural properties, generate regular and random graphs, and handle\nattributes and meta-data on nodes and edges. Besides being able to handle large\ngraphs, an additional strength of SNAP is that networks and their attributes\nare fully dynamic, they can be modified during the computation at low cost.\nSNAP is provided as an open source library in C++ as well as a module in\nPython.\n  We also describe the Stanford Large Network Dataset, a set of social and\ninformation real-world networks and datasets, which we make publicly available.\nThe collection is a complementary resource to our SNAP software and is widely\nused for development and benchmarking of graph analytics algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 03:17:12 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Leskovec", "Jure", ""], ["Sosic", "Rok", ""]]}, {"id": "1606.07572", "submitter": "S Subhashree", "authors": "Subhashree S and P Sreenivasa Kumar", "title": "Enriching Linked Datasets with New Object Properties", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although several RDF knowledge bases are available through the LOD\ninitiative, the ontology schema of such linked datasets is not very rich. In\nparticular, they lack object properties. The problem of finding new object\nproperties (and their instances) between any two given classes has not been\ninvestigated in detail in the context of Linked Data. In this paper, we present\nDART (Detecting Arbitrary Relations for enriching T-Boxes of Linked Data) - an\nunsupervised solution to enrich the LOD cloud with new object properties\nbetween two given classes. DART exploits contextual similarity to identify text\npatterns from the web corpus that can potentially represent relations between\nindividuals. These text patterns are then clustered by means of paraphrase\ndetection to capture the object properties between the two given LOD classes.\nDART also performs fully automated mapping of the discovered relations to the\nproperties in the linked dataset. This serves many purposes such as\nidentification of completely new relations, elimination of irrelevant\nrelations, and generation of prospective property axioms. We have empirically\nevaluated our approach on several pairs of classes and found that the system\ncan indeed be used for enriching the linked datasets with new object properties\nand their instances. We compared DART with newOntExt system which is an\noffshoot of the NELL (Never-Ending Language Learning) effort. Our experiments\nreveal that DART gives better results than newOntExt with respect to both the\ncorrectness, as well as the number of relations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 06:00:42 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 04:49:49 GMT"}, {"version": "v3", "created": "Mon, 4 Sep 2017 11:12:42 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["S", "Subhashree", ""], ["Kumar", "P Sreenivasa", ""]]}, {"id": "1606.07784", "submitter": "Sidahmed Benabderrahmane", "authors": "Dalila Attaf and Djamila Hamdadou and Sidahmed Benabderrahmane and\n  Aicha Lafrid", "title": "Satellite Images Analysis with Symbolic Time Series: A Case Study of the\n  Algerian Zone", "comments": "7 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1301.5871 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Satellite Image Time Series (SITS) are an important source of information for\nstudying land occupation and its evolution. Indeed, the very large volumes of\ndigital data stored, usually are not ready to a direct analysis. In order to\nboth reduce the dimensionality and information extraction, time series data\nmining generally gives rise to change of time series representation. In an\nobjective of information intelligibility extracted from the representation\nchange, we may use symbolic representations of time series. Many high level\nrepresentations of time series have been proposed for data mining, including\nFourier transforms, wavelets, piecewise polynomial models, etc. Many\nresearchers have also considered symbolic representations of time series,\nnoting that such representations would potentiality allow researchers to avail\nof the wealth of data structures and algorithms from the text processing and\nbioinformatics communities. We present in this work, one of the main symbolic\nrepresentation methods \"SAX\"(Symbolic Aggregate Approximation) and we\nexperience this method to symbolize and reduce the dimensionality of a\nSatellite Image Times Series acquired over a period of 5 years by\ncharacterizing the evolution of a vegetation index (NDVI).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 18:47:48 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Attaf", "Dalila", ""], ["Hamdadou", "Djamila", ""], ["Benabderrahmane", "Sidahmed", ""], ["Lafrid", "Aicha", ""]]}, {"id": "1606.08657", "submitter": "S{\\l}awomir Staworko", "authors": "Peter Buneman, S{\\l}awek Staworko", "title": "RDF Graph Alignment with Bisimulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of aligning two RDF databases, an essential\nproblem in understanding the evolution of ontologies. Our approaches address\nthree fundamental challenges: 1) the use of \"blank\" (null) names, 2) ontology\nchanges in which different names are used to identify the same entity, and 3)\nsmall changes in the data values as well as small changes in the graph\nstructure of the RDF database. We propose approaches inspired by the classical\nnotion of graph bisimulation and extend them to capture the natural metrics of\nedit distance on the data values and the graph structure. We evaluate our\nmethods on three evolving curated data sets. Overall, our results show that the\nproposed methods perform well and are scalable.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 11:33:55 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Buneman", "Peter", ""], ["Staworko", "S\u0142awek", ""]]}, {"id": "1606.08884", "submitter": "Ashwin Kayyoor", "authors": "Ashwin Narayan, Vuk Markovic, Natalia Postawa, Anna King, Alejandro\n  Morales, K. Ashwin Kumar, Petros Efstathopoulos", "title": "Efficient Routing for Cost Effective Scale-out Data Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient retrieval of information is of key importance when using Big Data\nsystems. In large scale-out data architectures, data are distributed and\nreplicated across several machines. Queries/tasks to such data architectures,\nare sent to a router which determines the machines containing the requested\ndata. Ideally, to reduce the overall cost of analytics, the smallest set of\nmachines required to satisfy the query should be returned by the router.\nMathematically, this can be modeled as the set cover problem, which is NP-hard,\nthus making the routing process a balance between optimality and performance.\nEven though an efficient greedy approximation algorithm for routing a single\nquery exists, there is currently no better method for processing multiple\nqueries than running the greedy set cover algorithm repeatedly for each query.\nThis method is impractical for Big Data systems and the state-of-the-art\ntechniques route a query to all machines and choose as a cover the machines\nthat respond fastest. In this paper, we propose an efficient technique to\nspeedup the routing of a large number of real-time queries while minimizing the\nnumber of machines that each query touches (query span). We demonstrate that by\nanalyzing the correlation between known queries and performing query\nclustering, we can reduce the set cover computation time, thereby significantly\nspeeding up routing of unknown queries. Experiments show that our incremental\nset cover-based routing is 2.5 times faster and can return on average 50% fewer\nmachines per query when compared to repeated greedy set cover and baseline\nrouting techniques.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 20:53:33 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Narayan", "Ashwin", ""], ["Markovic", "Vuk", ""], ["Postawa", "Natalia", ""], ["King", "Anna", ""], ["Morales", "Alejandro", ""], ["Kumar", "K. Ashwin", ""], ["Efstathopoulos", "Petros", ""]]}, {"id": "1606.09315", "submitter": "Chunbin Lin", "authors": "Chunbin Lin, Jianguo Wang, Yannis Papakonstantinou", "title": "Data Compression for Analytics over Large-scale In-memory Column\n  Databases", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data compression schemes have exhibited their importance in column databases\nby contributing to the high-performance OLAP (Online Analytical Processing)\nquery processing. Existing works mainly concentrate on evaluating compression\nschemes for disk-resident databases as data is mostly stored on disks. With the\ncontinuously decreasing of the price/capacity ratio of main memory, it is the\ntendencies of the times to reside data in main memory. But the discussion of\ndata compression on in-memory databases is very vague in the literature. In\nthis work, we present an updated discussion about whether it is valuable to use\ndata compression techniques in memory databases. If yes, how should memory\ndatabases apply data compression schemes to maximize performance?\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 00:44:05 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 18:11:25 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Lin", "Chunbin", ""], ["Wang", "Jianguo", ""], ["Papakonstantinou", "Yannis", ""]]}, {"id": "1606.09376", "submitter": "Shaleen Deep", "authors": "Shaleen Deep, Paraschos Koutris", "title": "The Design of Arbitrage-Free Data Pricing Schemes", "comments": "full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a growing market that involves buying and selling data over the\nweb, we study pricing schemes that assign value to queries issued over a\ndatabase. Previous work studied pricing mechanisms that compute the price of a\nquery by extending a data seller's explicit prices on certain queries, or\ninvestigated the properties that a pricing function should exhibit without\ndetailing a generic construction. In this work, we present a formal framework\nfor pricing queries over data that allows the construction of general families\nof pricing functions, with the main goal of avoiding arbitrage. We consider two\ntypes of pricing schemes: instance-independent schemes, where the price depends\nonly on the structure of the query, and answer-dependent schemes, where the\nprice also depends on the query output. Our main result is a complete\ncharacterization of the structure of pricing functions in both settings, by\nrelating it to properties of a function over a lattice. We use our\ncharacterization, together with information-theoretic methods, to construct a\nvariety of arbitrage-free pricing functions. Finally, we discuss various\ntradeoffs in the design space and present techniques for efficient computation\nof the proposed pricing functions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 07:44:18 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Deep", "Shaleen", ""], ["Koutris", "Paraschos", ""]]}]