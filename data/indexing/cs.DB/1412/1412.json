[{"id": "1412.0223", "submitter": "Peng Cheng", "authors": "Peng Cheng, Xiang Lian, Zhao Chen, Rui Fu, Lei Chen, Jinsong Han,\n  Jizhong Zhao", "title": "Reliable Diversity-Based Spatial Crowdsourcing by Moving Workers", "comments": "16 pages", "journal-ref": null, "doi": "10.14778/2794367.2794372", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of mobile devices and the crowdsourcig platforms,\nthe spatial crowdsourcing has attracted much attention from the database\ncommunity, specifically, spatial crowdsourcing refers to sending a\nlocation-based request to workers according to their positions. In this paper,\nwe consider an important spatial crowdsourcing problem, namely reliable\ndiversity-based spatial crowdsourcing (RDB-SC), in which spatial tasks (such as\ntaking videos/photos of a landmark or firework shows, and checking whether or\nnot parking spaces are available) are time-constrained, and workers are moving\ntowards some directions. Our RDB-SC problem is to assign workers to spatial\ntasks such that the completion reliability and the spatial/temporal diversities\nof spatial tasks are maximized. We prove that the RDB-SC problem is NP-hard and\nintractable. Thus, we propose three effective approximation approaches,\nincluding greedy, sampling, and divide-and-conquer algorithms. In order to\nimprove the efficiency, we also design an effective cost-model-based index,\nwhich can dynamically maintain moving workers and spatial tasks with low cost,\nand efficiently facilitate the retrieval of RDB-SC answers. Through extensive\nexperiments, we demonstrate the efficiency and effectiveness of our proposed\napproaches over both real and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 15:06:53 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2015 08:26:38 GMT"}, {"version": "v3", "created": "Sat, 9 May 2015 02:18:23 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2015 01:23:23 GMT"}, {"version": "v5", "created": "Tue, 10 Nov 2015 14:56:18 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Cheng", "Peng", ""], ["Lian", "Xiang", ""], ["Chen", "Zhao", ""], ["Fu", "Rui", ""], ["Chen", "Lei", ""], ["Han", "Jinsong", ""], ["Zhao", "Jizhong", ""]]}, {"id": "1412.0321", "submitter": "Jiajun Liu", "authors": "Jiajun Liu, Kun Zhao, Philipp Sommer, Shuo Shang, Brano Kusy, Raja\n  Jurdak", "title": "Bounded Quadrant System: Error-bounded Trajectory Compression on the Go", "comments": "International Conference on Data Engineering (ICDE) 2015, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term location tracking, where trajectory compression is commonly used,\nhas gained high interest for many applications in transport, ecology, and\nwearable computing. However, state-of-the-art compression methods involve high\nspace-time complexity or achieve unsatisfactory compression rate, leading to\nrapid exhaustion of memory, computation, storage and energy resources. We\npropose a novel online algorithm for error-bounded trajectory compression\ncalled the Bounded Quadrant System (BQS), which compresses trajectories with\nextremely small costs in space and time using convex-hulls. In this algorithm,\nwe build a virtual coordinate system centered at a start point, and establish a\nrectangular bounding box as well as two bounding lines in each of its\nquadrants. In each quadrant, the points to be assessed are bounded by the\nconvex-hull formed by the box and lines. Various compression error-bounds are\ntherefore derived to quickly draw compression decisions without expensive error\ncomputations. In addition, we also propose a light version of the BQS version\nthat achieves $\\mathcal{O}(1)$ complexity in both time and space for processing\neach point to suit the most constrained computation environments. Furthermore,\nwe briefly demonstrate how this algorithm can be naturally extended to the 3-D\ncase.\n  Using empirical GPS traces from flying foxes, cars and simulation, we\ndemonstrate the effectiveness of our algorithm in significantly reducing the\ntime and space complexity of trajectory compression, while greatly improving\nthe compression rates of the state-of-the-art algorithms (up to 47%). We then\nshow that with this algorithm, the operational time of the target\nresource-constrained hardware platform can be prolonged by up to 41%.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 01:14:42 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 14:49:37 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Liu", "Jiajun", ""], ["Zhao", "Kun", ""], ["Sommer", "Philipp", ""], ["Shang", "Shuo", ""], ["Kusy", "Brano", ""], ["Jurdak", "Raja", ""]]}, {"id": "1412.0364", "submitter": "Manas Joglekar", "authors": "Manas Joglekar, Hector Garcia-Molina, Aditya Parameswaran", "title": "Interactive Data Exploration with Smart Drill-Down", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present {\\em smart drill-down}, an operator for interactively exploring a\nrelational table to discover and summarize \"interesting\" groups of tuples. Each\ngroup of tuples is described by a {\\em rule}. For instance, the rule $(a, b,\n\\star, 1000)$ tells us that there are a thousand tuples with value $a$ in the\nfirst column and $b$ in the second column (and any value in the third column).\nSmart drill-down presents an analyst with a list of rules that together\ndescribe interesting aspects of the table. The analyst can tailor the\ndefinition of interesting, and can interactively apply smart drill-down on an\nexisting rule to explore that part of the table. We demonstrate that the\nunderlying optimization problems are {\\sc NP-Hard}, and describe an algorithm\nfor finding the approximately optimal list of rules to display when the user\nuses a smart drill-down, and a dynamic sampling scheme for efficiently\ninteracting with large tables. Finally, we perform experiments on real datasets\non our experimental prototype to demonstrate the usefulness of smart drill-down\nand study the performance of our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 07:09:14 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 01:05:03 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 06:31:52 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Joglekar", "Manas", ""], ["Garcia-Molina", "Hector", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1412.1069", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Dan Suciu", "title": "Approximate Lifted Inference with Probabilistic Databases", "comments": "12 pages, 5 figures, pre-print for a paper appearing in VLDB 2015.\n  arXiv admin note: text overlap with arXiv:1310.6257", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach for approximate evaluation of #P-hard\nqueries with probabilistic databases. In our approach, every query is evaluated\nentirely in the database engine by evaluating a fixed number of query plans,\neach providing an upper bound on the true probability, then taking their\nminimum. We provide an algorithm that takes into account important schema\ninformation to enumerate only the minimal necessary plans among all possible\nplans. Importantly, this algorithm is a strict generalization of all known\nresults of PTIME self-join-free conjunctive queries: A query is safe if and\nonly if our algorithm returns one single plan. We also apply three relational\nquery optimization techniques to evaluate all minimal safe plans very fast. We\ngive a detailed experimental evaluation of our approach and, in the process,\nprovide a new way of thinking about the value of probabilistic methods over\nnon-probabilistic methods for ranking query answers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 20:58:20 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Suciu", "Dan", ""]]}, {"id": "1412.1470", "submitter": "Mostafa Haghir Chehreghani", "authors": "Mostafa Haghir Chehreghani and Maurice Bruynooghe", "title": "Mining Rooted Ordered Trees under Subtree Homeomorphism", "comments": "This paper is accepted in the Data Mining and Knowledge Discovery\n  journal\n  (http://www.springer.com/computer/database+management+%26+information+retrieval/journal/10618)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent tree patterns has many applications in different areas such\nas XML data, bioinformatics and World Wide Web. The crucial step in frequent\npattern mining is frequency counting, which involves a matching operator to\nfind occurrences (instances) of a tree pattern in a given collection of trees.\nA widely used matching operator for tree-structured data is subtree\nhomeomorphism, where an edge in the tree pattern is mapped onto an\nancestor-descendant relationship in the given tree. Tree patterns that are\nfrequent under subtree homeomorphism are usually called embedded patterns. In\nthis paper, we present an efficient algorithm for subtree homeomorphism with\napplication to frequent pattern mining. We propose a compact data-structure,\ncalled occ, which stores only information about the rightmost paths of\noccurrences and hence can encode and represent several occurrences of a tree\npattern. We then define efficient join operations on the occ data-structure,\nwhich help us count occurrences of tree patterns according to occurrences of\ntheir proper subtrees. Based on the proposed subtree homeomorphism method, we\ndevelop an effective pattern mining algorithm, called TPMiner. We evaluate the\nefficiency of TPMiner on several real-world and synthetic datasets. Our\nextensive experiments confirm that TPMiner always outperforms well-known\nexisting algorithms, and in several cases the improvement with respect to\nexisting algorithms is significant.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 15:57:00 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 17:43:23 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 20:36:21 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Chehreghani", "Mostafa Haghir", ""], ["Bruynooghe", "Maurice", ""]]}, {"id": "1412.1505", "submitter": "Guy Van den Broeck", "authors": "Paul Beame, Guy Van den Broeck, Eric Gribkoff, Dan Suciu", "title": "Symmetric Weighted First-Order Model Counting", "comments": "To appear at PODS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FO Model Counting problem (FOMC) is the following: given a sentence\n$\\Phi$ in FO and a number $n$, compute the number of models of $\\Phi$ over a\ndomain of size $n$; the Weighted variant (WFOMC) generalizes the problem by\nassociating a weight to each tuple and defining the weight of a model to be the\nproduct of weights of its tuples. In this paper we study the complexity of the\nsymmetric WFOMC, where all tuples of a given relation have the same weight. Our\nmotivation comes from an important application, inference in Knowledge Bases\nwith soft constraints, like Markov Logic Networks, but the problem is also of\nindependent theoretical interest. We study both the data complexity, and the\ncombined complexity of FOMC and WFOMC. For the data complexity we prove the\nexistence of an FO$^{3}$ formula for which FOMC is #P$_1$-complete, and the\nexistence of a Conjunctive Query for which WFOMC is #P$_1$-complete. We also\nprove that all $\\gamma$-acyclic queries have polynomial time data complexity.\nFor the combined complexity, we prove that, for every fragment FO$^{k}$, $k\\geq\n2$, the combined complexity of FOMC (or WFOMC) is #P-complete.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 22:03:52 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 13:29:54 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2015 14:58:14 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Beame", "Paul", ""], ["Broeck", "Guy Van den", ""], ["Gribkoff", "Eric", ""], ["Suciu", "Dan", ""]]}, {"id": "1412.1671", "submitter": "Camilo Thorne", "authors": "Camilo Thorne", "title": "Chases and Bag-Set Certain Answers", "comments": "4pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that the chase technique is powerful enough to capture\nthe bag-set semantics of conjunctive queries over IDBs and IDs and TGDs. In\naddition, we argue that in such cases it provides efficient (LogSpace) query\nevaluation algorithms and that, moreover, it can serve as a basis for\nevaluating some restricted classes of aggregate queries under incomplete\ninformation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 14:12:23 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 10:20:23 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Thorne", "Camilo", ""]]}, {"id": "1412.2221", "submitter": "Dan Olteanu", "authors": "Vince Barany and Balder ten Cate and Benny Kimelfeld and Dan Olteanu\n  and Zografoula Vagena", "title": "Declarative Statistical Modeling with Datalog", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formalisms for specifying statistical models, such as\nprobabilistic-programming languages, typically consist of two components: a\nspecification of a stochastic process (the prior), and a specification of\nobservations that restrict the probability space to a conditional subspace (the\nposterior). Use cases of such formalisms include the development of algorithms\nin machine learning and artificial intelligence. We propose and investigate a\ndeclarative framework for specifying statistical models on top of a database,\nthrough an appropriate extension of Datalog. By virtue of extending Datalog,\nour framework offers a natural integration with the database, and has a robust\ndeclarative semantics. Our Datalog extension provides convenient mechanisms to\ninclude numerical probability functions; in particular, conclusions of rules\nmay contain values drawn from such functions. The semantics of a program is a\nprobability distribution over the possible outcomes of the input database with\nrespect to the program; these outcomes are minimal solutions with respect to a\nrelated program with existentially quantified variables in conclusions.\nObservations are naturally incorporated by means of integrity constraints over\nthe extensional and intensional relations. We focus on programs that use\ndiscrete numerical distributions, but even then the space of possible outcomes\nmay be uncountable (as a solution can be infinite). We define a probability\nmeasure over possible outcomes by applying the known concept of cylinder sets\nto a probabilistic chase procedure. We show that the resulting semantics is\nrobust under different chases. We also identify conditions guaranteeing that\nall possible outcomes are finite (and then the probability space is discrete).\nWe argue that the framework we propose retains the purely declarative nature of\nDatalog, and allows for natural specifications of statistical models.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 11:04:14 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 19:49:24 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Barany", "Vince", ""], ["Cate", "Balder ten", ""], ["Kimelfeld", "Benny", ""], ["Olteanu", "Dan", ""], ["Vagena", "Zografoula", ""]]}, {"id": "1412.2324", "submitter": "Jose Faleiro", "authors": "Jose M. Faleiro and Daniel J. Abadi", "title": "Rethinking serializable multiversion concurrency control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-versioned database systems have the potential to significantly increase\nthe amount of concurrency in transaction processing because they can avoid\nread-write conflicts. Unfortunately, the increase in concurrency usually comes\nat the cost of transaction serializability. If a database user requests full\nserializability, modern multi-versioned systems significantly constrain\nread-write concurrency among conflicting transactions and employ expensive\nsynchronization patterns in their design. In main-memory multi-core settings,\nthese additional constraints are so burdensome that multi-versioned systems are\noften significantly outperformed by single-version systems.\n  We propose Bohm, a new concurrency control protocol for main-memory\nmulti-versioned database systems. Bohm guarantees serializable execution while\nensuring that reads never block writes. In addition, Bohm does not require\nreads to perform any book-keeping whatsoever, thereby avoiding the overhead of\ntracking reads via contended writes to shared memory. This leads to excellent\nscalability and performance in multi-core settings. Bohm has all the above\ncharacteristics without performing validation based concurrency control.\nInstead, it is pessimistic, and is therefore not prone to excessive aborts in\nthe presence of contention. An experimental evaluation shows that Bohm performs\nwell in both high contention and low contention settings, and is able to\ndramatically outperform state-of-the-art multi-versioned systems despite\nmaintaining the full set of serializability guarantees.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 07:09:02 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 23:06:00 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 03:16:28 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Faleiro", "Jose M.", ""], ["Abadi", "Daniel J.", ""]]}, {"id": "1412.2332", "submitter": "Balder ten Cate", "authors": "Balder ten Cate and Cristina Civili and Evgeny Sherkhonov and\n  Wang-Chiew Tan", "title": "High-Level Why-Not Explanations using Ontologies", "comments": "in PODS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel foundational framework for why-not explanations, that is,\nexplanations for why a tuple is missing from a query result. Our why-not\nexplanations leverage concepts from an ontology to provide high-level and\nmeaningful reasons for why a tuple is missing from the result of a query. A key\nalgorithmic problem in our framework is that of computing a most-general\nexplanation for a why-not question, relative to an ontology, which can either\nbe provided by the user, or it may be automatically derived from the data\nand/or schema. We study the complexity of this problem and associated problems,\nand present concrete algorithms for computing why-not explanations. In the case\nwhere an external ontology is provided, we first show that the problem of\ndeciding the existence of an explanation to a why-not question is NP-complete\nin general. However, the problem is solvable in polynomial time for queries of\nbounded arity, provided that the ontology is specified in a suitable language,\nsuch as a member of the DL-Lite family of description logics, which allows for\nefficient concept subsumption checking. Furthermore, we show that a\nmost-general explanation can be computed in polynomial time in this case. In\naddition, we propose a method for deriving a suitable (virtual) ontology from a\ndatabase and/or a data workspace schema, and we present an algorithm for\ncomputing a most-general explanation to a why-not question, relative to such\nontologies. This algorithm runs in polynomial-time in the case when concepts\nare defined in a selection-free language, or if the underlying schema is fixed.\nFinally, we also study the problem of computing short most-general\nexplanations, and we briefly discuss alternative definitions of what it means\nto be an explanation, and to be most general.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 08:39:28 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 16:47:18 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Cate", "Balder ten", ""], ["Civili", "Cristina", ""], ["Sherkhonov", "Evgeny", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "1412.3040", "submitter": "Albert Kim", "authors": "Albert Kim, Eric Blais, Aditya Parameswaran, Piotr Indyk, Sam Madden,\n  Ronitt Rubinfeld", "title": "Rapid Sampling for Visualizations with Ordering Guarantees", "comments": "Tech Report. 17 pages. Condensed version to appear in VLDB Vol. 8 No.\n  5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Visualizations are frequently used as a means to understand trends and gather\ninsights from datasets, but often take a long time to generate. In this paper,\nwe focus on the problem of rapidly generating approximate visualizations while\npreserving crucial visual proper- ties of interest to analysts. Our primary\nfocus will be on sampling algorithms that preserve the visual property of\nordering; our techniques will also apply to some other visual properties. For\ninstance, our algorithms can be used to generate an approximate visualization\nof a bar chart very rapidly, where the comparisons between any two bars are\ncorrect. We formally show that our sampling algorithms are generally applicable\nand provably optimal in theory, in that they do not take more samples than\nnecessary to generate the visualizations with ordering guarantees. They also\nwork well in practice, correctly ordering output groups while taking orders of\nmagnitude fewer samples and much less time than conventional sampling schemes.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 18:08:26 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Kim", "Albert", ""], ["Blais", "Eric", ""], ["Parameswaran", "Aditya", ""], ["Indyk", "Piotr", ""], ["Madden", "Sam", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1412.3100", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer", "title": "Semi-Supervised Learning with Heterophily", "comments": "17 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a family of linear inference algorithms that generalize existing\ngraph-based label propagation algorithms by allowing them to propagate\ngeneralized assumptions about \"attraction\" or \"compatibility\" between classes\nof neighboring nodes (in particular those that involve heterophily between\nnodes where \"opposites attract\"). We thus call this formulation Semi-Supervised\nLearning with Heterophily (SSLH) and show how it generalizes and improves upon\na recently proposed approach called Linearized Belief Propagation (LinBP).\nImportantly, our framework allows us to reduce the problem of estimating the\nrelative compatibility between nodes from partially labeled graph to a simple\noptimization problem. The result is a very fast algorithm that -- despite its\nsimplicity -- is surprisingly effective: we can classify unlabeled nodes within\nthe same graph in the same time as LinBP but with a superior accuracy and\ndespite our algorithm not knowing the compatibilities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 20:58:45 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 02:27:12 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Gatterbauer", "Wolfgang", ""]]}, {"id": "1412.3136", "submitter": "Isaac Sheff", "authors": "Isaac C. Sheff, Robbert van Renesse, Andrew C. Myers", "title": "Distributed Protocols and Heterogeneous Trust: Technical Report", "comments": "This is the technical report of a submission for EuroSys 2015. 26\n  Pages 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustness of distributed systems is usually phrased in terms of the\nnumber of failures of certain types that they can withstand. However, these\nfailure models are too crude to describe the different kinds of trust and\nexpectations of participants in the modern world of complex, integrated systems\nextending across different owners, networks, and administrative domains. Modern\nsystems often exist in an environment of heterogeneous trust, in which\ndifferent participants may have different opinions about the trustworthiness of\nother nodes, and a single participant may consider other nodes to differ in\ntheir trustworthiness. We explore how to construct distributed protocols that\nmeet the requirements of all participants, even in heterogeneous trust\nenvironments. The key to our approach is using lattice-based information flow\nto analyse and prove protocol properties. To demonstrate this approach, we show\nhow two earlier distributed algorithms can be generalized to work in the\npresence of heterogeneous trust: first, Heterogeneous Fast Consensus, an\nadaptation of the earlier Bosco Fast Consensus protocol; and second, Nysiad, an\nalgorithm for converting crash-tolerant protocols to be Byzantine-tolerant.\nThrough simulations, we show that customizing a protocol to a heterogeneous\ntrust configuration yields performance improvements over the conventional\nprotocol designed for homogeneous trust.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 22:09:48 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Sheff", "Isaac C.", ""], ["van Renesse", "Robbert", ""], ["Myers", "Andrew C.", ""]]}, {"id": "1412.3328", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Teddy Furon, Vincent Gripon, Michael Rabbat and Herv\\'e\n  J\\'egou", "title": "Memory vectors for similarity search in high-dimensional spaces", "comments": "Accepted to IEEE Transactions on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an indexing architecture to store and search in a database of\nhigh-dimensional vectors from the perspective of statistical signal processing\nand decision theory. This architecture is composed of several memory units,\neach of which summarizes a fraction of the database by a single representative\nvector. The potential similarity of the query to one of the vectors stored in\nthe memory unit is gauged by a simple correlation with the memory unit's\nrepresentative vector. This representative optimizes the test of the following\nhypothesis: the query is independent from any vector in the memory unit vs. the\nquery is a simple perturbation of one of the stored vectors.\n  Compared to exhaustive search, our approach finds the most similar database\nvectors significantly faster without a noticeable reduction in search quality.\nInterestingly, the reduction of complexity is provably better in\nhigh-dimensional spaces. We empirically demonstrate its practical interest in a\nlarge-scale image search scenario with off-the-shelf state-of-the-art\ndescriptors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 14:56:41 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 17:11:21 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 14:47:33 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2015 20:53:08 GMT"}, {"version": "v5", "created": "Tue, 29 Dec 2015 15:16:22 GMT"}, {"version": "v6", "created": "Tue, 3 Jan 2017 13:48:57 GMT"}, {"version": "v7", "created": "Wed, 1 Mar 2017 21:14:52 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Iscen", "Ahmet", ""], ["Furon", "Teddy", ""], ["Gripon", "Vincent", ""], ["Rabbat", "Michael", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1412.3538", "submitter": "Jerome Darmont", "authors": "Varunya Attasena (ERIC), Nouria Harbi (ERIC), J\\'er\\^ome Darmont\n  (ERIC)", "title": "fVSS: A New Secure and Cost-Efficient Scheme for Cloud Data Warehouses", "comments": null, "journal-ref": "ACM. 17th International Workshop on Data Warehousing and OLAP\n  (DOLAP 2014), Nov 2014, Shangai, China. pp.81-90, Proceedings of the 17th\n  International Workshop on Data Warehousing and OLAP", "doi": "10.1145/2666158.2666173", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud business intelligence is an increasingly popular choice to deliver\ndecision support capabilities via elastic, pay-per-use resources. However, data\nsecurity issues are one of the top concerns when dealing with sensitive data.\nIn this pa-per, we propose a novel approach for securing cloud data warehouses\nby flexible verifiable secret sharing, fVSS. Secret sharing encrypts and\ndistributes data over several cloud ser-vice providers, thus enforcing data\nprivacy and availability. fVSS addresses four shortcomings in existing secret\nsharing-based approaches. First, it allows refreshing the data ware-house when\nsome service providers fail. Second, it allows on-line analysis processing.\nThird, it enforces data integrity with the help of both inner and outer\nsignatures. Fourth, it helps users control the cost of cloud warehousing by\nbalanc-ing the load among service providers with respect to their pricing\npolicies. To illustrate fVSS' efficiency, we thoroughly compare it with\nexisting secret sharing-based approaches with respect to security features,\nquerying power and data storage and computing costs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 04:41:30 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Attasena", "Varunya", "", "ERIC"], ["Harbi", "Nouria", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1412.3633", "submitter": "Jan Triska", "authors": "Jan Triska and Vilem Vychodil", "title": "Logic of temporal attribute implications", "comments": null, "journal-ref": null, "doi": "10.1007/s10472-016-9526-6", "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study logic for reasoning with if-then formulas describing dependencies\nbetween attributes of objects which are observed in consecutive points in time.\nWe introduce semantic entailment of the formulas, show its fixed-point\ncharacterization, investigate closure properties of model classes, present an\naxiomatization and prove its completeness, and investigate alternative\naxiomatizations and normalized proofs. We investigate decidability and\ncomplexity issues of the logic and prove that the entailment problem is NP-hard\nand belongs to EXPSPACE. We show that by restricting to predictive formulas,\nthe entailment problem is decidable in pseudo-linear time.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 12:41:51 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 08:01:28 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Triska", "Jan", ""], ["Vychodil", "Vilem", ""]]}, {"id": "1412.3750", "submitter": "Jeremy Debattista", "authors": "Jeremy Debattista, Christoph Lange, S\\\"oren Auer", "title": "Luzzu - A Framework for Linked Data Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing adoption and growth of the Linked Open Data cloud [9],\nwith RDFa, Microformats and other ways of embedding data into ordinary Web\npages, and with initiatives such as schema.org, the Web is currently being\ncomplemented with a Web of Data. Thus, the Web of Data shares many\ncharacteristics with the original Web of Documents, which also varies in\nquality. This heterogeneity makes it challenging to determine the quality of\nthe data published on the Web and to subsequently make this information\nexplicit to data consumers. The main contribution of this article is LUZZU, a\nquality assessment framework for Linked Open Data. Apart from providing quality\nmetadata and quality problem reports that can be used for data cleaning, LUZZU\nis extensible: third party metrics can be easily plugged-in the framework. The\nframework does not rely on SPARQL endpoints, and is thus free of all the\nproblems that come with them, such as query timeouts. Another advantage over\nSPARQL based qual- ity assessment frameworks is that metrics implemented in\nLUZZU can have more complex functionality than triple matching. Using the\nframework, we performed a quality assessment of a number of statistical linked\ndatasets that are available on the LOD cloud. For this evaluation, 25 metrics\nfrom ten different dimensions were implemented.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 18:28:47 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 15:01:16 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 17:19:41 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Debattista", "Jeremy", ""], ["Lange", "Christoph", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1412.3869", "submitter": "Paraschos Koutris", "authors": "Paraschos Koutris, Tova Milo, Sudeepa Roy and Dan Suciu", "title": "Answering Conjunctive Queries with Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the complexity of answering conjunctive queries (CQ)\nwith inequalities). In particular, we are interested in comparing the\ncomplexity of the query with and without inequalities. The main contribution of\nour work is a novel combinatorial technique that enables us to use any\nSelect-Project-Join query plan for a given CQ without inequalities in answering\nthe CQ with inequalities, with an additional factor in running time that only\ndepends on the query. The key idea is to define a new projection operator,\nwhich keeps a small representation (independent of the size of the database) of\nthe set of input tuples that map to each tuple in the output of the projection;\nthis representation is used to evaluate all the inequalities in the query.\nSecond, we generalize a result by Papadimitriou-Yannakakis [17] and give an\nalternative algorithm based on the color-coding technique [4] to evaluate a CQ\nwith inequalities by using an algorithm for the CQ without inequalities. Third,\nwe investigate the structure of the query graph, inequality graph, and the\naugmented query graph with inequalities, and show that even if the query and\nthe inequality graphs have bounded treewidth, the augmented graph not only can\nhave an unbounded treewidth but can also be NP-hard to evaluate. Further, we\nillustrate classes of queries and inequalities where the augmented graphs have\nunbounded treewidth, but the CQ with inequalities can be evaluated in\npoly-time. Finally, we give necessary properties and sufficient properties that\nallow a class of CQs to have poly-time combined complexity with respect to any\ninequality pattern. We also illustrate classes of queries where our\nquery-plan-based technique outperforms the alternative approaches discussed in\nthe paper.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 01:41:17 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Koutris", "Paraschos", ""], ["Milo", "Tova", ""], ["Roy", "Sudeepa", ""], ["Suciu", "Dan", ""]]}, {"id": "1412.4030", "submitter": "Thomas Schwentick", "authors": "Tom J. Ameloot, Gaetano Geck, Bas Ketsman, Frank Neven, Thomas\n  Schwentick", "title": "Parallel-Correctness and Transferability for Conjunctive Queries", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dominant cost for query evaluation in modern massively distributed systems\nis the number of communication rounds. For this reason, there is a growing\ninterest in single-round multiway join algorithms where data is first\nreshuffled over many servers and then evaluated in a parallel but\ncommunication-free way. The reshuffling itself is specified as a distribution\npolicy. We introduce a correctness condition, called parallel-correctness, for\nthe evaluation of queries w.r.t. a distribution policy. We study the complexity\nof parallel-correctness for conjunctive queries as well as transferability of\nparallel-correctness between queries. We also investigate the complexity of\ntransferability for certain families of distribution policies, including, for\ninstance, the Hypercube distribution.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 16:06:08 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 08:53:47 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Ameloot", "Tom J.", ""], ["Geck", "Gaetano", ""], ["Ketsman", "Bas", ""], ["Neven", "Frank", ""], ["Schwentick", "Thomas", ""]]}, {"id": "1412.4303", "submitter": "Mingjie Tang", "authors": "Mingjie Tang, Ruby Y. Tahboub, Walid G. Aref, Qutaibah M. Malluhi,\n  Mourad Ouzzani", "title": "On Order-independent Semantics of the Similarity Group-By Relational\n  Database Operator", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity group-by (SGB, for short) has been proposed as a relational\ndatabase operator to match the needs of emerging database applications. Many\nSGB operators that extend SQL have been proposed in the literature, e.g.,\nsimilarity operators in the one-dimensional space. These operators have various\nsemantics. Depending on how these operators are implemented, some of the\nimplementations may lead to different groupings of the data. Hence, if SQL code\nis ported from one database system to another, it is not guaranteed that the\ncode will produce the same results. In this paper, we investigate the various\nsemantics for the relational similarity group-by operators in the\nmulti-dimensional space. We define the class of order-independent SGB operators\nthat produce the same results regardless of the order in which the input data\nis presented to them. Using the notion of interval graphs borrowed from graph\ntheory, we prove that, for certain SGB operators, there exist order-independent\nimplementations. For each of these operators, we provide a sample algorithm\nthat is order-independent. Also, we prove that for other SGB operators, there\ndoes not exist an order-independent implementation for them, and hence these\nSGB operators are ill-defined and should not be adopted in extensions to SQL to\nrealize similarity group-by. In this paper, we introduce an SGB operator,\nnamely SGB-All, for grouping multi-dimensional data using similarity. SGB-All\nforms groups such that a data item, say O, belongs to a group, say G, if and\nonly if O is within a user-defined threshold from all other data items in G. In\nother words, each group in SGB-All forms a clique of nearby data items in the\nmulti-dimensional space. We prove that SGB-All are order-independent, i.e.,\nthere is at least one algorithm for each option that is independent of the\npresentation order of the input data.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 01:55:06 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Tang", "Mingjie", ""], ["Tahboub", "Ruby Y.", ""], ["Aref", "Walid G.", ""], ["Malluhi", "Qutaibah M.", ""], ["Ouzzani", "Mourad", ""]]}, {"id": "1412.4311", "submitter": "Leopoldo Bertossi", "authors": "Babak Salimi and Leopoldo Bertossi", "title": "From Causes for Database Queries to Repairs and Model-Based Diagnosis\n  and Back", "comments": "Extended version of paper to appear in Proceedings of ICDT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we establish and investigate connections between causality for\nquery answers in databases, database repairs wrt. denial constraints, and\nconsistency-based diagnosis. The first two are relatively new problems in\ndatabases, and the third one is an established subject in knowledge\nrepresentation. We show how to obtain database repairs from causes and the\nother way around. Causality problems are formulated as diagnosis problems, and\nthe diagnoses provide causes and their responsibilities. The vast body of\nresearch on database repairs can be applied to the newer problem of determining\nactual causes for query answers and their responsibilities. These connections,\nwhich are interesting per se, allow us, after a transition -inspired by\nconsistency-based diagnosis- to computational problems on hitting sets and\nvertex covers in hypergraphs, to obtain several new algorithmic and complexity\nresults for causality in databases.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 04:27:14 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Salimi", "Babak", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1412.4320", "submitter": "Daniel Lupei", "authors": "Christoph Koch, Daniel Lupei, Val Tannen", "title": "Incremental View Maintenance For Collection Programming", "comments": "24 pages (12 pages plus appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of incremental view maintenance (IVM), delta query derivation\nis an essential technique for speeding up the processing of large, dynamic\ndatasets. The goal is to generate delta queries that, given a small change in\nthe input, can update the materialized view more efficiently than via\nrecomputation. In this work we propose the first solution for the efficient\nincrementalization of positive nested relational calculus (NRC+) on bags (with\ninteger multiplicities). More precisely, we model the cost of NRC+ operators\nand classify queries as efficiently incrementalizable if their delta has a\nstrictly lower cost than full re-evaluation. Then, we identify IncNRC+; a large\nfragment of NRC+ that is efficiently incrementalizable and we provide a\nsemantics-preserving translation that takes any NRC+ query to a collection of\nIncNRC+ queries. Furthermore, we prove that incremental maintenance for NRC+ is\nwithin the complexity class NC0 and we showcase how recursive IVM, a technique\nthat has provided significant speedups over traditional IVM in the case of flat\nqueries [25], can also be applied to IncNRC+.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 06:12:32 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 05:07:14 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Koch", "Christoph", ""], ["Lupei", "Daniel", ""], ["Tannen", "Val", ""]]}, {"id": "1412.4378", "submitter": "Bharath Kumar Samanthula", "authors": "Bharath K. Samanthula, Fang-Yu Rao, Elisa Bertino, Xun Yi, Dongxi Liu", "title": "Privacy-Preserving and Outsourced Multi-User k-Means Clustering", "comments": "16 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many techniques for privacy-preserving data mining (PPDM) have been\ninvestigated over the past decade. Often, the entities involved in the data\nmining process are end-users or organizations with limited computing and\nstorage resources. As a result, such entities may want to refrain from\nparticipating in the PPDM process. To overcome this issue and to take many\nother benefits of cloud computing, outsourcing PPDM tasks to the cloud\nenvironment has recently gained special attention. We consider the scenario\nwhere n entities outsource their databases (in encrypted format) to the cloud\nand ask the cloud to perform the clustering task on their combined data in a\nprivacy-preserving manner. We term such a process as privacy-preserving and\noutsourced distributed clustering (PPODC). In this paper, we propose a novel\nand efficient solution to the PPODC problem based on k-means clustering\nalgorithm. The main novelty of our solution lies in avoiding the secure\ndivision operations required in computing cluster centers altogether through an\nefficient transformation technique. Our solution builds the clusters securely\nin an iterative fashion and returns the final cluster centers to all entities\nwhen a pre-determined termination condition holds. The proposed solution\nprotects data confidentiality of all the participating entities under the\nstandard semi-honest model. To the best of our knowledge, ours is the first\nwork to discuss and propose a comprehensive solution to the PPODC problem that\nincurs negligible cost on the participating entities. We theoretically estimate\nboth the computation and communication costs of the proposed protocol and also\ndemonstrate its practical value through experiments on a real dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 16:54:26 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Samanthula", "Bharath K.", ""], ["Rao", "Fang-Yu", ""], ["Bertino", "Elisa", ""], ["Yi", "Xun", ""], ["Liu", "Dongxi", ""]]}, {"id": "1412.4463", "submitter": "Manjunatha Praveen", "authors": "M. Praveen and B. Srivathsan", "title": "Defining relations on graphs: how hard is it in the presence of node\n  partitions?", "comments": "Small corrections based on reviews from PODS 2015, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing query languages for graph structured data is an active field of\nresearch. Evaluating a query on a graph results in a relation on the set of its\nnodes. In other words, a query is a mechanism for defining relations on a\ngraph. Some relations may not be definable by any query in a given language.\nThis leads to the following question: given a graph, a query language and a\nrelation on the graph, does there exist a query in the language that defines\nthe relation? This is called the definability problem. When the given query\nlanguage is standard regular expressions, the definability problem is known to\nbe PSPACE-complete.\n  The model of graphs can be extended by labeling nodes with values from an\ninfinite domain. These labels induce a partition on the set of nodes: two nodes\nare equivalent if they are labeled by the same value. Query languages can also\nbe extended to make use of this equivalence. Two such extensions are Regular\nExpressions with Memory (REM) and Regular Expressions with Equality (REE).\n  In this paper, we study the complexity of the definability problem in this\nextended model when the query language is either REM or REE. We show that the\ndefinability problem is EXPSPACE-complete when the query language is REM, and\nit is PSPACE-complete when the query language is REE. In addition, when the\nquery language is a union of conjunctive queries based on REM or REE, we show\ncoNP-completeness.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 05:02:37 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 07:02:16 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Praveen", "M.", ""], ["Srivathsan", "B.", ""]]}, {"id": "1412.4485", "submitter": "Micha\\\"el Thomazo", "authors": "Sebastian Rudolph, Micha\\\"el Thomazo, Jean-Fran\\c{c}ois Baget,\n  Marie-Laure Mugnier", "title": "Worst-case Optimal Query Answering for Greedy Sets of Existential Rules\n  and Their Subclasses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for an ontological layer on top of data, associated with advanced\nreasoning mechanisms able to exploit the semantics encoded in ontologies, has\nbeen acknowledged both in the database and knowledge representation\ncommunities. We focus in this paper on the ontological query answering problem,\nwhich consists of querying data while taking ontological knowledge into\naccount. More specifically, we establish complexities of the conjunctive query\nentailment problem for classes of existential rules (also called\ntuple-generating dependencies, Datalog+/- rules, or forall-exists-rules. Our\ncontribution is twofold. First, we introduce the class of greedy\nbounded-treewidth sets (gbts) of rules, which covers guarded rules, and their\nmost well-known generalizations. We provide a generic algorithm for query\nentailment under gbts, which is worst-case optimal for combined complexity with\nor without bounded predicate arity, as well as for data complexity and query\ncomplexity. Secondly, we classify several gbts classes, whose complexity was\nunknown, with respect to combined complexity (with both unbounded and bounded\npredicate arity) and data complexity to obtain a comprehensive picture of the\ncomplexity of existential rule fragments that are based on diverse guardedness\nnotions. Upper bounds are provided by showing that the proposed algorithm is\noptimal for all of them.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 07:46:19 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Rudolph", "Sebastian", ""], ["Thomazo", "Micha\u00ebl", ""], ["Baget", "Jean-Fran\u00e7ois", ""], ["Mugnier", "Marie-Laure", ""]]}, {"id": "1412.4842", "submitter": "Mingjie Tang", "authors": "Mingjie Tang, Ruby Y.Tahboub, Walid G.Are, Mikhail J. Atallah,\n  Qutaibah M. Malluhi, Mourad Ouzzani, and Yasin N. Silva", "title": "Similarity Group-by Operators for Multi-dimensional Relational Data", "comments": "submit to TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SQL group-by operator plays an important role in summarizing and\naggregating large datasets in a data analytic stack.While the standard group-by\noperator, which is based on equality, is useful in several applications,\nallowing similarity aware grouping provides a more realistic view on real-world\ndata that could lead to better insights. The Similarity SQL-based Group-By\noperator (SGB, for short) extends the semantics of the standard SQL Group-by by\ngrouping data with similar but not necessarily equal values. While existing\nsimilarity-based grouping operators efficiently materialize this approximate\nsemantics, they primarily focus on one-dimensional attributes and treat\nmultidimensional attributes independently. However, correlated attributes, such\nas in spatial data, are processed independently, and hence, groups in the\nmultidimensional space are not detected properly. To address this problem, we\nintroduce two new SGB operators for multidimensional data. The first operator\nis the clique (or distance-to-all) SGB, where all the tuples in a group are\nwithin some distance from each other. The second operator is the\ndistance-to-any SGB, where a tuple belongs to a group if the tuple is within\nsome distance from any other tuple in the group. We implement and test the new\nSGB operators and their algorithms inside PostgreSQL. The overhead introduced\nby these operators proves to be minimal and the execution times are comparable\nto those of the standard Group-by. The experimental study, based on TPC-H and a\nsocial check-in data, demonstrates that the proposed algorithms can achieve up\nto three orders of magnitude enhancement in performance over baseline methods\ndeveloped to solve the same problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 00:27:52 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Tang", "Mingjie", ""], ["Tahboub", "Ruby Y.", ""], ["Are", "Walid G.", ""], ["Atallah", "Mikhail J.", ""], ["Malluhi", "Qutaibah M.", ""], ["Ouzzani", "Mourad", ""], ["Silva", "Yasin N.", ""]]}, {"id": "1412.5143", "submitter": "Matthew Hague", "authors": "Matthew Hague, Anthony Widjaja Lin, Luke Ong", "title": "Detecting Redundant CSS Rules in HTML5 Applications: A Tree-Rewriting\n  Approach", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HTML5 applications normally have a large set of CSS (Cascading Style Sheets)\nrules for data display. Each CSS rule consists of a node selector (given in an\nXPath-like query language) and a declaration block (assigning values to\nselected nodes' display attributes). As web applications evolve, maintaining\nCSS files can easily become problematic. Some CSS rules will be replaced by new\nones, but these obsolete (hence redundant) CSS rules often remain in the\napplications. Not only does this \"bloat\" the applications, but it also\nsignificantly increases web browsers' processing time. Most works on detecting\nredundant CSS rules in HTML5 applications do not consider the dynamic behaviors\nof HTML5 (specified in JavaScript); in fact, the only proposed method that\ntakes these into account is dynamic analysis (a.k.a. testing), which cannot\nsoundly prove redundancy of CSS rules. In this paper, we introduce an\nabstraction of HTML5 applications based on monotonic tree-rewriting and study\nits \"redundancy problem\". We establish the precise complexity of the problem\nand various subproblems of practical importance (ranging from P to EXP). In\nparticular, our algorithm relies on an efficient reduction to an analysis of\nsymbolic pushdown systems (for which highly optimised solvers are available),\nwhich yields a fast method for checking redundancy in practice. We implemented\nour algorithm and demonstrated its efficacy in detecting redundant CSS rules in\nHTML5 applications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 14:22:03 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 02:30:30 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2015 12:24:10 GMT"}, {"version": "v4", "created": "Tue, 18 Aug 2015 13:25:58 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Hague", "Matthew", ""], ["Lin", "Anthony Widjaja", ""], ["Ong", "Luke", ""]]}, {"id": "1412.5263", "submitter": "Alekh Jindal", "authors": "Alekh Jindal, Samuel Madden, Malu Castellanos, Meichun Hsu", "title": "Graph Analytics using the Vertica Relational Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph analytics is becoming increasingly popular, with a deluge of new\nsystems for graph analytics having been proposed in the past few years. These\nsystems often start from the assumption that a new storage or query processing\nsystem is needed, in spite of graph data being often collected and stored in a\nrelational database in the first place. In this paper, we study Vertica\nrelational database as a platform for graph analytics. We show that\nvertex-centric graph analysis can be translated to SQL queries, typically\ninvolving table scans and joins, and that modern column-oriented databases are\nvery well suited to running such queries. Specifically, we present an\nexperimental evaluation of the Vertica relational database system on a variety\nof graph analytics, including iterative analysis, a combination of graph and\nrelational analyses, and more complex 1- hop neighborhood graph analytics,\nshowing that it is competitive to two popular vertex-centric graph analytics\nsystems, namely Giraph and GraphLab.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 05:56:35 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Jindal", "Alekh", ""], ["Madden", "Samuel", ""], ["Castellanos", "Malu", ""], ["Hsu", "Meichun", ""]]}, {"id": "1412.5910", "submitter": "Martin Schuster", "authors": "Martin Schuster and Thomas Schwentick", "title": "Games for Active XML Revisited", "comments": "To be published in ICDT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper studies the rewriting mechanisms for intensional documents in the\nActive XML framework, abstracted in the form of active context-free games. The\nsafe rewriting problem studied in this paper is to decide whether the first\nplayer, Juliet, has a winning strategy for a given game and (nested) word; this\ncorresponds to a successful rewriting strategy for a given intensional\ndocument. The paper examines several extensions to active context-free games.\n  The primary extension allows more expressive schemas (namely XML schemas and\nregular nested word languages) for both target and replacement languages and\nhas the effect that games are played on nested words instead of (flat) words as\nin previous studies. Other extensions consider validation of input parameters\nof web services, and an alternative semantics based on insertion of service\ncall results.\n  In general, the complexity of the safe rewriting problem is highly\nintractable (doubly exponential time), but the paper identifies interesting\ntractable cases.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 15:54:45 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Schuster", "Martin", ""], ["Schwentick", "Thomas", ""]]}, {"id": "1412.6170", "submitter": "Francesco Lettich", "authors": "Francesco Lettich, Salvatore Orlando and Claudio Silvestri", "title": "Manycore processing of repeated k-NN queries over massive moving objects\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to timely process significant amounts of continuously updated\nspatial data is mandatory for an increasing number of applications. In this\npaper we focus on a specific data-intensive problem concerning the repeated\nprocessing of huge amounts of k nearest neighbours (k-NN) queries over massive\nsets of moving objects, where the spatial extents of queries and the position\nof objects are continuously modified over time. In particular, we propose a\nnovel hybrid CPU/GPU pipeline that significantly accelerate query processing\nthanks to a combination of ad-hoc data structures and non-trivial memory access\npatterns. To the best of our knowledge this is the first work that exploits\nGPUs to efficiently solve repeated k-NN queries over massive sets of\ncontinuously moving objects, even characterized by highly skewed spatial\ndistributions. In comparison with state-of-the-art sequential CPU-based\nimplementations, our method highlights significant speedups in the order of\n10x-20x, depending on the datasets, even when considering cheap GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 22:43:28 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Lettich", "Francesco", ""], ["Orlando", "Salvatore", ""], ["Silvestri", "Claudio", ""]]}, {"id": "1412.6477", "submitter": "Marcus Paradies", "authors": "Marcus Paradies, Wolfgang Lehner, Christof Bornhoevd", "title": "GRAPHITE: An Extensible Graph Traversal Framework for Relational\n  Database Management Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph traversals are a basic but fundamental ingredient for a variety of\ngraph algorithms and graph-oriented queries. To achieve the best possible query\nperformance, they need to be implemented at the core of a database management\nsystem that aims at storing, manipulating, and querying graph data.\nIncreasingly, modern business applications demand native graph query and\nprocessing capabilities for enterprise-critical operations on data stored in\nrelational database management systems. In this paper we propose an extensible\ngraph traversal framework (GRAPHITE) as a central graph processing component on\na common storage engine inside a relational database management system.\n  We study the influence of the graph topology on the execution time of graph\ntraversals and derive two traversal algorithm implementations specialized for\ndifferent graph topologies and traversal queries. We conduct extensive\nexperiments on GRAPHITE for a large variety of real-world graph data sets and\ninput configurations. Our experiments show that the proposed traversal\nalgorithms differ by up to two orders of magnitude for different input\nconfigurations and therefore demonstrate the need for a versatile framework to\nefficiently process graph traversals on a wide range of different graph\ntopologies and types of queries. Finally, we highlight that the query\nperformance of our traversal implementations is competitive with those of two\nnative graph database management systems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 18:43:20 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Paradies", "Marcus", ""], ["Lehner", "Wolfgang", ""], ["Bornhoevd", "Christof", ""]]}, {"id": "1412.6545", "submitter": "Pablo Fillottrani", "authors": "Pablo R. Fillottrani, C. Maria Keet", "title": "KF metamodel formalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The KF metamodel is a comprehensive unifying metamodel covering the static\nstructural entities and constraints of UML Class Diagrams (v2.4.1), ER, EER,\nORM, and ORM2, and intended to boost interoperability of common conceptual data\nmodelling languages. It was originally designed in UML with textual\nconstraints, and in this report we present its formalisations in FOL and OWL,\nwhich accompanies the paper that describes, discusses, and analyses the KF\nmetamodel in detail. These new formalizations contribute to give a precise\nmeaning to the metamodel, to understand its complexity properties and to\nprovide a basis for future implementations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 21:56:59 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Fillottrani", "Pablo R.", ""], ["Keet", "C. Maria", ""]]}, {"id": "1412.7282", "submitter": "Jundong Li", "authors": "Jundong Li, Aibek Adilmagambetovm, Mohomed Shazan Mohomed Jabbar,\n  Osmar R. Zaiane, Alvaro Osornio-Vargas, Osnat Wine", "title": "On Discovering Co-Location Patterns in Datasets: A Case Study of\n  Pollutants and Child Cancers", "comments": "In GeoInformatica, 2016", "journal-ref": "GeoInformatica 2016", "doi": "10.1007/s10707-016-0254-1", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We intend to identify relationships between cancer cases and pollutant\nemissions and attempt to understand whether cancer in children is typically\nlocated together with some specific chemical combinations or is independent.\nCo-location pattern analysis seems to be the appropriate investigation to\nperform. Co-location mining is one of the tasks of spatial data mining which\nfocuses on the detection of co-location patterns, the sets of spatial features\nfrequently located in close proximity of each other. Most previous works are\nbased on transaction-free apriori-like algorithms which are dependent on\nuser-defined thresholds and are designed for boolean data points. Due to the\nabsence of a clear notion of transactions, it is nontrivial to use association\nrule mining techniques to tackle the co-location mining problem. The approach\nwe propose is based on a grid \"transactionization\" of the geographic space and\nis designed to mine datasets with extended spatial objects. Uncertainty of the\nfeature presence in transactions is taken into account in our model. The\nstatistical test is used instead of global thresholds to detect significant\nco-location patterns and rules. We evaluate our approach on synthetic and real\ndatasets. This approach can be used by researchers looking for spatial\nassociations between environmental and health factors. In addition, we explain\nthe data modelling framework which is used on real datasets of pollutants\n(PRTR/NPRI) and childhood cancer cases.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 07:59:09 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 08:36:17 GMT"}, {"version": "v3", "created": "Thu, 31 Mar 2016 18:56:07 GMT"}, {"version": "v4", "created": "Fri, 1 Apr 2016 20:34:34 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Li", "Jundong", ""], ["Adilmagambetovm", "Aibek", ""], ["Jabbar", "Mohomed Shazan Mohomed", ""], ["Zaiane", "Osmar R.", ""], ["Osornio-Vargas", "Alvaro", ""], ["Wine", "Osnat", ""]]}, {"id": "1412.7584", "submitter": "Zhanglong Ji", "authors": "Zhanglong Ji, Zachary C. Lipton, Charles Elkan", "title": "Differential Privacy and Machine Learning: a Survey and Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of machine learning is to extract useful information from data,\nwhile privacy is preserved by concealing information. Thus it seems hard to\nreconcile these competing interests. However, they frequently must be balanced\nwhen mining sensitive data. For example, medical research represents an\nimportant application where it is necessary both to extract useful information\nand protect patient privacy. One way to resolve the conflict is to extract\ngeneral characteristics of whole populations without disclosing the private\ninformation of individuals.\n  In this paper, we consider differential privacy, one of the most popular and\npowerful definitions of privacy. We explore the interplay between machine\nlearning and differential privacy, namely privacy-preserving machine learning\nalgorithms and learning-based data release mechanisms. We also describe some\ntheoretical results that address what can be learned differentially privately\nand upper bounds of loss functions for differentially private algorithms.\n  Finally, we present some open questions, including how to incorporate public\ndata, how to deal with missing data in private datasets, and whether, as the\nnumber of observed samples grows arbitrarily large, differentially private\nmachine learning algorithms can be achieved at no cost to utility as compared\nto corresponding non-differentially private algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 01:51:06 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Ji", "Zhanglong", ""], ["Lipton", "Zachary C.", ""], ["Elkan", "Charles", ""]]}, {"id": "1412.8339", "submitter": "Charith Perera", "authors": "Charith Perera, Rajiv Ranjan, Lizhe Wang, Samee U. Khan, and Albert Y.\n  Zomaya", "title": "Big Data Privacy in the Internet of Things Era", "comments": "Accepted to be published in IEEE IT Professional Magazine: Special\n  Issue Internet of Anything 2015", "journal-ref": null, "doi": "10.1109/MITP.2015.34", "report-no": null, "categories": "cs.CY cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, we have seen a plethora of Internet of Things (IoT)\nsolutions, products and services, making their way into the industry's\nmarket-place. All such solution will capture a large amount of data pertaining\nto the environment, as well as their users. The objective of the IoT is to\nlearn more and to serve better the system users. Some of these solutions may\nstore the data locally on the devices ('things'), and others may store in the\nCloud. The real value of collecting data comes through data processing and\naggregation in large-scale where new knowledge can be extracted. However, such\nprocedures can also lead to user privacy issues. This article discusses some of\nthe main challenges of privacy in IoT, and opportunities for research and\ninnovation. We also introduce some of the ongoing research efforts that address\nIoT privacy issues.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 13:45:13 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 09:18:53 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Perera", "Charith", ""], ["Ranjan", "Rajiv", ""], ["Wang", "Lizhe", ""], ["Khan", "Samee U.", ""], ["Zomaya", "Albert Y.", ""]]}]