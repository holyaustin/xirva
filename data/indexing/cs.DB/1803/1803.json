[{"id": "1803.00055", "submitter": "Ryan Marcus", "authors": "Ryan Marcus, Olga Papaemmanouil", "title": "Deep Reinforcement Learning for Join Order Enumeration", "comments": null, "journal-ref": "aiDM@SIGMOD 2018 Proceedings of the First International Workshop\n  on Exploiting Artificial Intelligence Techniques for Data Management", "doi": "10.1145/3211954.3211957", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Join order selection plays a significant role in query performance. However,\nmodern query optimizers typically employ static join enumeration algorithms\nthat do not receive any feedback about the quality of the resulting plan.\nHence, optimizers often repeatedly choose the same bad plan, as they do not\nhave a mechanism for \"learning from their mistakes\". In this paper, we argue\nthat existing deep reinforcement learning techniques can be applied to address\nthis challenge. These techniques, powered by artificial neural networks, can\nautomatically improve decision making by incorporating feedback from their\nsuccesses and failures. Towards this goal, we present ReJOIN, a\nproof-of-concept join enumerator, and present preliminary results indicating\nthat ReJOIN can match or outperform the PostgreSQL optimizer in terms of plan\nquality and join enumeration efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 20:00:33 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 21:09:45 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Marcus", "Ryan", ""], ["Papaemmanouil", "Olga", ""]]}, {"id": "1803.00497", "submitter": "Ismail Toroslu", "authors": "Ugur Turan, Ismail H. Toroslu, Murat Kantarcioglu", "title": "Graph Based Proactive Secure Decomposition Algorithm for Context\n  Dependent Attribute Based Inference Control Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational DBMSs continue to dominate the database market, and inference\nproblem on external schema of relational DBMS's is still an important issue in\nterms of data privacy.Especially for the last 10 years, external schema\nconstruction for application-specific database usage has increased its\nindependency from the conceptual schema, as the definitions and implementations\nof views and procedures have been optimized. This paper offers an optimized\ndecomposition strategy for the external schema, which concentrates on the\nprivacy policy and required associations of attributes for the intended user\nroles. The method proposed in this article performs a proactive decomposition\nof the external schema, in order to satisfy both the forbidden and required\nassociations of attributes.Functional dependency constraints of a database\nschema can be represented as a graph, in which vertices are attribute sets and\nedges are functional dependencies. In this representation, inference problem\ncan be defined as a process of searching a subtree in the dependency graph\ncontaining the attributes that need to be related. The optimized decomposition\nprocess aims to generate an external schema, which guarantees the prevention of\nthe inference of the forbidden attribute sets while guaranteeing the\nassociation of the required attribute sets with a minimal loss of possible\nassociation among other attributes, if the inhibited and required attribute\nsets are consistent with each other. Our technique is purely proactive, and can\nbe viewed as a normalization process. Due to the usage independency of external\nschema construction tools, it can be easily applied to any existing systems\nwithout rewriting data access layer of applications. Our extensive experimental\nanalysis shows the effectiveness of this optimized proactive strategy for a\nwide variety of logical schema volumes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 16:53:49 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Turan", "Ugur", ""], ["Toroslu", "Ismail H.", ""], ["Kantarcioglu", "Murat", ""]]}, {"id": "1803.00701", "submitter": "Zhongjun Jin", "authors": "Zhongjun Jin, Michael Cafarella, H. V. Jagadish, Sean Kandel, Michael\n  Minar, Joseph M. Hellerstein", "title": "CLX: Towards verifiable PBE data transformation", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective data analytics on data collected from the real world usually begins\nwith a notoriously expensive pre-processing step of data transformation and\nwrangling. Programming By Example (PBE) systems have been proposed to\nautomatically infer transformations using simple examples that users provide as\nhints. However, an important usability issue - verification - limits the\neffective use of such PBE data transformation systems, since the verification\nprocess is often effort-consuming and unreliable. We propose a data\ntransformation paradigm design CLX (pronounced \"clicks\") with a focus on\nfacilitating verification for end users in a PBE-like data transformation. CLX\nperforms pattern clustering in both input and output data, which allows the\nuser to verify at the pattern level, rather than the data instance level,\nwithout having to write any regular expressions, thereby significantly reducing\nuser verification effort. Thereafter, CLX automatically generates\ntransformation programs as regular-expression replace operations that are easy\nfor average users to verify. We experimentally compared the CLX prototype with\nboth FlashFill, a state-of-the-art PBE data transformation tool, and Trifacta,\nan influential system supporting interactive data transformation. The results\nshow improvements over the state of the art tools in saving user verification\neffort, without loss of efficiency or expressive power. In a user effort study\non data sets of various sizes, when the data size grew by a factor of 30, the\nuser verification time required by the CLX prototype grew by 1.3x whereas that\nrequired by FlashFill grew by 11.4x. In another test assessing the users'\nunderstanding of the transformation logic - a key ingredient in effective\nverification - CLX users achieved a success rate about twice that of FlashFill\nusers.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 03:47:39 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 21:31:45 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2018 20:40:48 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 01:39:24 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Jin", "Zhongjun", ""], ["Cafarella", "Michael", ""], ["Jagadish", "H. V.", ""], ["Kandel", "Sean", ""], ["Minar", "Michael", ""], ["Hellerstein", "Joseph M.", ""]]}, {"id": "1803.01024", "submitter": "Besim Bilalli", "authors": "Besim Bilalli and Alberto Abell\\'o and Tom\\`as Aluja-Banet and Robert\n  Wrembel", "title": "PRESISTANT: Learning based assistant for data pre-processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data pre-processing is one of the most time consuming and relevant steps in a\ndata analysis process (e.g., classification task). A given data pre-processing\noperator (e.g., transformation) can have positive, negative or zero impact on\nthe final result of the analysis. Expert users have the required knowledge to\nfind the right pre-processing operators. However, when it comes to non-experts,\nthey are overwhelmed by the amount of pre-processing operators and it is\nchallenging for them to find operators that would positively impact their\nanalysis (e.g., increase the predictive accuracy of a classifier). Existing\nsolutions either assume that users have expert knowledge, or they recommend\npre-processing operators that are only \"syntactically\" applicable to a dataset,\nwithout taking into account their impact on the final analysis. In this work,\nwe aim at providing assistance to non-expert users by recommending data\npre-processing operators that are ranked according to their impact on the final\nanalysis. We developed a tool PRESISTANT, that uses Random Forests to learn the\nimpact of pre-processing operators on the performance (e.g., predictive\naccuracy) of 5 different classification algorithms, such as J48, Naive Bayes,\nPART, Logistic Regression, and Nearest Neighbor. Extensive evaluations on the\nrecommendations provided by our tool, show that PRESISTANT can effectively help\nnon-experts in order to achieve improved results in their analytical tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 19:50:30 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Bilalli", "Besim", ""], ["Abell\u00f3", "Alberto", ""], ["Aluja-Banet", "Tom\u00e0s", ""], ["Wrembel", "Robert", ""]]}, {"id": "1803.01135", "submitter": "Giorgos Santipantakis", "authors": "Georgios Santipantakis and Christos Doulkeridis and George A. Vouros\n  and Akrivi Vlachou", "title": "MaskLink: Efficient Link Discovery for Spatial Relations via Masking\n  Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of spatial link discovery (LD), focusing\nprimarily on topological and proximity relations between spatial objects. The\nproblem is timely due to the large number of sources that generate spatial\ndata, including streaming sources (e.g., surveillance of moving objects) but\nalso archival sources (such as static areas of interest). To address the\nproblem of integrating data from such diverse sources, link discovery\ntechniques are required to identify various spatial relations efficiently.\nExisting approaches typically adopt the filter and refine methodology by\nexploiting a blocking technique for effective filtering.\n  In this paper, we present a new spatial LD technique, called MaskLink, that\nimproves the effectiveness of the filtering step. We show that MaskLink\noutperforms the state-of-the-art algorithm for link discovery of topological\nrelations, while also addressing some of its limitations, such as applicability\nfor streaming data, low memory requirements, and parallelization. Furthermore,\nwe show that MaskLink can be extended and generalized to the case of\nproximity-based link discovery, which has not been studied before for spatial\ndata.\n  Our empirical study demonstrates the superiority of MaskLink against the\nstate-of-the-art in the case of topological relations, and its performance gain\ncompared to a baseline technique in the case of proximity-based LD.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 09:56:39 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Santipantakis", "Georgios", ""], ["Doulkeridis", "Christos", ""], ["Vouros", "George A.", ""], ["Vlachou", "Akrivi", ""]]}, {"id": "1803.01248", "submitter": "Giovanni Vincenti", "authors": "Giovanni Vincenti", "title": "Imprecise temporal associations and decision support systems", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.procs.2018.04.096", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quick and pervasive infiltration of decision support systems, artificial\nintelligence, and data mining in consumer electronics and everyday life in\ngeneral has been significant in recent years. Fields such as UX have been\nfacilitating the integration of such technologies into software and hardware,\nbut the back-end processing is still based on binary foundations. This article\ndescribes an approach to mining for imprecise temporal associations among\nevents in data streams, taking into account the very natural concept of\napproximation. This type of association analysis is likely to lead to more\nmeaningful and actionable decision support systems.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 21:40:33 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Vincenti", "Giovanni", ""]]}, {"id": "1803.01384", "submitter": "Saravanan Thirumuruganathan", "authors": "Saravanan Thirumuruganathan, Nan Tang, Mourad Ouzzani, AnHai Doan", "title": "Data Curation with Deep Learning [Vision]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data curation - the process of discovering, integrating, and cleaning data -\nis one of the oldest, hardest, yet inevitable data management problems. Despite\ndecades of efforts from both researchers and practitioners, it is still one of\nthe most time consuming and least enjoyable work of data scientists. In most\norganizations, data curation plays an important role so as to fully unlock the\nvalue of big data. Unfortunately, the current solutions are not keeping up with\nthe ever-changing data ecosystem, because they often require substantially high\nhuman cost. Meanwhile, deep learning is making strides in achieving remarkable\nsuccesses in multiple areas, such as image recognition, natural language\nprocessing, and speech recognition. In this vision paper, we explore how some\nof the fundamental innovations in deep learning could be leveraged to improve\nexisting data curation solutions and to help build new ones. In particular, we\nprovide a thorough overview of the current deep learning landscape, and\nidentify interesting research opportunities and dispel common myths. We hope\nthat the synthesis of these important domains will unleash a series of research\nactivities that will lead to significantly improved solutions for many data\ncuration tasks.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 17:08:45 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 15:09:30 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Thirumuruganathan", "Saravanan", ""], ["Tang", "Nan", ""], ["Ouzzani", "Mourad", ""], ["Doan", "AnHai", ""]]}, {"id": "1803.01390", "submitter": "Jelle Hellings", "authors": "Jelle Hellings, Marc Gyssens, Yuqing Wu, Dirk Van Gucht, Jan Van den\n  Bussche, Stijn Vansummeren, George H. L. Fletcher", "title": "Comparing Downward Fragments of the Relational Calculus with Transitive\n  Closure on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the continuing interest in the tree data model, we study the\nexpressive power of downward navigational query languages on trees and chains.\nBasic navigational queries are built from the identity relation and edge\nrelations using composition and union. We study the effects on relative\nexpressiveness when we add transitive closure, projections, coprojections,\nintersection, and difference; this for boolean queries and path queries on\nlabeled and unlabeled structures. In all cases, we present the complete Hasse\ndiagram. In particular, we establish, for each query language fragment that we\nstudy on trees, whether it is closed under difference and intersection.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 17:38:51 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Hellings", "Jelle", ""], ["Gyssens", "Marc", ""], ["Wu", "Yuqing", ""], ["Van Gucht", "Dirk", ""], ["Bussche", "Jan Van den", ""], ["Vansummeren", "Stijn", ""], ["Fletcher", "George H. L.", ""]]}, {"id": "1803.01445", "submitter": "Ali Moallemi", "authors": "G\\\"osta Grahne and Ali Moallemi", "title": "Universal (and Existential) Nulls", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incomplete Information research is quite mature when it comes to so called\n{\\em existential nulls}, where an existential null is a value stored in the\ndatabase, representing an unknown object. For some reason {\\em universal\nnulls}, that is, values representing {\\em all} possible objects, have received\nalmost no attention. We remedy the situation in this paper, by showing that a\nsuitable finite representation mechanism, called {\\em Star Cylinders}, handling\nuniversal nulls can be developed based on the {\\em Cylindric Set Algebra} of\nHenkin, Monk and Tarski. We provide a finitary version of the cylindric set\nalgebra, called {\\em Cylindric Star Algebra}, and show that our star-cylinders\nare closed under this algebra. Moreover, we show that any {\\em First Order\nRelational Calculus} query over databases containing universal nulls can be\ntranslated into an equivalent expression in our cylindric star-algebra, and\nvice versa, in time polynomial in the size of the database.\n  The representation mechanism is then extended to {\\em Naive Star Cylinders},\nwhich are star-cylinders allowing existential nulls in addition to universal\nnulls. For positive queries (with universal quantification), the well known\nnaive evaluation technique can still be applied on the existential nulls,\nthereby allowing polynomial time evaluation of certain answers on databases\ncontaining both universal and existential nulls. If precise answers are\nrequired, certain answer evaluation with universal and existential nulls\nremains in coNP. Note that the problem is coNP-hard, already for positive\nexistential queries and databases with only existential nulls. If inequalities\n$\\neg(x_i\\approx x_j)$ are allowed, reasoning over existential databases is\nknown to be $\\Pi^p_2$-complete, and it remains in $\\Pi^p_2$ when universal\nnulls and full first order queries are allowed.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 00:48:08 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Grahne", "G\u00f6sta", ""], ["Moallemi", "Ali", ""]]}, {"id": "1803.01969", "submitter": "Edward Gan", "authors": "Edward Gan, Jialin Ding, Kai Sheng Tai, Vatsal Sharan, Peter Bailis", "title": "Moment-Based Quantile Sketches for Efficient High Cardinality\n  Aggregation Queries", "comments": "Technical Report for paper to be published in VLDB 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive analytics increasingly involves querying for quantiles over\nsub-populations of high cardinality datasets. Data processing engines such as\nDruid and Spark use mergeable summaries to estimate quantiles, but summary\nmerge times can be a bottleneck during aggregation. We show how a compact and\nefficiently mergeable quantile sketch can support aggregation workloads. This\ndata structure, which we refer to as the moments sketch, operates with a small\nmemory footprint (200 bytes) and computationally efficient (50ns) merges by\ntracking only a set of summary statistics, notably the sample moments. We\ndemonstrate how we can efficiently and practically estimate quantiles using the\nmethod of moments and the maximum entropy principle, and show how the use of a\ncascade further improves query time for threshold predicates. Empirical\nevaluation on real-world datasets shows that the moments sketch can achieve\nless than 1 percent error with 15 times less merge overhead than comparable\nsummaries, improving end query time in the MacroBase engine by up to 7 times\nand the Druid engine by up to 60 times.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 00:48:59 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 23:11:49 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Gan", "Edward", ""], ["Ding", "Jialin", ""], ["Tai", "Kai Sheng", ""], ["Sharan", "Vatsal", ""], ["Bailis", "Peter", ""]]}, {"id": "1803.02601", "submitter": "Bart Scheers", "authors": "Bart Scheers, Steven Bloemen, Hannes M\\\"uhleisen, Pim Schellart, Arjen\n  van Elteren, Martin Kersten, Paul J. Groot", "title": "Fast in-database cross-matching of high-cadence, high-density source\n  lists with an up-to-date sky model", "comments": "16 pages, 5 figures; Accepted for publication in Astronomy &\n  Computing", "journal-ref": "A&C 23 (2018) 27-39", "doi": "10.1016/j.ascom.2018.02.006", "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coming high-cadence wide-field optical telescopes will image hundreds of\nthousands of sources per minute. Besides inspecting the near real-time data\nstreams for transient and variability events, the accumulated data archive is a\nwealthy laboratory for making complementary scientific discoveries.\n  The goal of this work is to optimise column-oriented database techniques to\nenable the construction of a full-source and light-curve database for\nlarge-scale surveys, that is accessible by the astronomical community.\n  We adopted LOFAR's Transients Pipeline as the baseline and modified it to\nenable the processing of optical images that have much higher source densities.\nThe pipeline adds new source lists to the archive database, while\ncross-matching them with the known cataloged sources in order to build a full\nlight-curve archive. We investigated several techniques of indexing and\npartitioning the largest tables, allowing for faster positional source look-ups\nin the cross matching algorithms. We monitored all query run times in long-term\npipeline runs where we processed a subset of IPHAS data that have image source\ndensity peaks over $170,000$ per field of view ($500,000$ deg$^{-2}$).\n  Our analysis demonstrates that horizontal table partitions of declination\nwidths of one-degree control the query run times. Usage of an index strategy\nwhere the partitions are densily sorted according to source declination yields\nanother improvement. Most queries run in sublinear time and a few (<20%) run in\nlinear time, because of dependencies on input source-list and result-set size.\nWe observed that for this logical database partitioning schema the limiting\ncadence the pipeline achieved with processing IPHAS data is 25 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 11:22:40 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Scheers", "Bart", ""], ["Bloemen", "Steven", ""], ["M\u00fchleisen", "Hannes", ""], ["Schellart", "Pim", ""], ["van Elteren", "Arjen", ""], ["Kersten", "Martin", ""], ["Groot", "Paul J.", ""]]}, {"id": "1803.03716", "submitter": "Pedram Gharani", "authors": "Pedram Gharani, Kenrick Fernande, Vineet Raghu", "title": "TRAJEDI: Trajectory Dissimilarity", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-98923-5_8", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast increase in our ability to obtain and store trajectory data\nnecessitates trajectory analytics techniques to extract useful information from\nthis data. Pair-wise distance functions are a foundation building block for\ncommon operations on trajectory datasets including constrained SELECT queries,\nk-nearest neighbors, and similarity and diversity algorithms. The accuracy and\nperformance of these operations depend heavily on the speed and accuracy of the\nunderlying trajectory distance function, which is in turn affected by\ntrajectory calibration. Current methods either require calibrated data, or\nperform calibration of the entire relevant dataset first, which is expensive\nand time consuming for large datasets. We present TRAJEDI, a calibrationaware\npair-wise distance calculation scheme that outperforms naive approaches while\npreserving accuracy. We also provide analyses of parameter tuning to trade-off\nbetween speed and accuracy. Our scheme is usable with any diversity, similarity\nor k-nearest neighbor algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 23:07:09 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Gharani", "Pedram", ""], ["Fernande", "Kenrick", ""], ["Raghu", "Vineet", ""]]}, {"id": "1803.04035", "submitter": "Richard Nock", "authors": "Richard Nock and Stephen Hardy and Wilko Henecka and Hamish Ivey-Law\n  and Giorgio Patrini and Guillaume Smith and Brian Thorne", "title": "Entity Resolution and Federated Learning get a Federated Resolution", "comments": "arXiv admin note: text overlap with arXiv:1711.10677", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider two data providers, each maintaining records of different feature\nsets about common entities. They aim to learn a linear model over the whole set\nof features. This problem of federated learning over vertically partitioned\ndata includes a crucial upstream issue: entity resolution, i.e. finding the\ncorrespondence between the rows of the datasets. It is well known that entity\nresolution, just like learning, is mistake-prone in the real world. Despite the\nimportance of the problem, there has been no formal assessment of how errors in\nentity resolution impact learning.\n  In this paper, we provide a thorough answer to this question, answering how\noptimal classifiers, empirical losses, margins and generalisation abilities are\naffected. While our answer spans a wide set of losses --- going beyond proper,\nconvex, or classification calibrated ---, it brings simple practical arguments\nto upgrade entity resolution as a preprocessing step to learning. One of these\nsuggests that entity resolution should be aimed at controlling or minimizing\nthe number of matching errors between examples of distinct classes. In our\nexperiments, we modify a simple token-based entity resolution algorithm so that\nit indeed aims at avoiding matching rows belonging to different classes, and\nperform experiments in the setting where entity resolution relies on noisy\ndata, which is very relevant to real world domains. Notably, our approach\ncovers the case where one peer \\textit{does not} have classes, or a noisy\nrecord of classes. Experiments display that using the class information during\nentity resolution can buy significant uplift for learning at little expense\nfrom the complexity standpoint.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 20:53:18 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 21:46:12 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Nock", "Richard", ""], ["Hardy", "Stephen", ""], ["Henecka", "Wilko", ""], ["Ivey-Law", "Hamish", ""], ["Patrini", "Giorgio", ""], ["Smith", "Guillaume", ""], ["Thorne", "Brian", ""]]}, {"id": "1803.04120", "submitter": "Michael Gowanlock", "authors": "Michael Gowanlock, Ben Karsin", "title": "GPU Accelerated Self-join for the Distance Similarity Metric", "comments": "Accepted for Publication in the 4th IEEE International Workshop on\n  High-Performance Big Data, Deep Learning, and Cloud Computing. To appear in\n  the Proceedings of the 32nd IEEE International Parallel and Distributed\n  Processing Symposium Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-join finds all objects in a dataset within a threshold of each other\ndefined by a similarity metric. As such, the self-join is a building block for\nthe field of databases and data mining, and is employed in Big Data\napplications. In this paper, we advance a GPU-efficient algorithm for the\nsimilarity self-join that uses the Euclidean distance metric. The\nsearch-and-refine strategy is an efficient approach for low dimensionality\ndatasets, as index searches degrade with increasing dimension (i.e., the curse\nof dimensionality). Thus, we target the low dimensionality problem, and compare\nour GPU self-join to a search-and-refine implementation, and a state-of-the-art\nparallel algorithm. In low dimensionality, there are several unique challenges\nassociated with efficiently solving the self-join problem on the GPU. Low\ndimensional data often results in higher data densities, causing a significant\nnumber of distance calculations and a large result set. As dimensionality\nincreases, index searches become increasingly exhaustive, forming a performance\nbottleneck. We advance several techniques to overcome these challenges using\nthe GPU. The techniques we propose include a GPU-efficient index that employs a\nbounded search, a batching scheme to accommodate large result set sizes, and a\nreduction in distance calculations through duplicate search removal. Our GPU\nself-join outperforms both search-and-refine and state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 05:28:44 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Gowanlock", "Michael", ""], ["Karsin", "Ben", ""]]}, {"id": "1803.04141", "submitter": "Dimitrios Vasilas", "authors": "Dimitrios Vasilas (LIP6, DELYS), Marc Shapiro (DELYS, LIP6), Bradley\n  King", "title": "A Modular Design for Geo-Distributed Querying", "comments": "5th Workshop on Principles and Practice of Consistency for\n  Distributed Data, Apr 2018, Porto, Portugal. 5th Workshop on Principles and\n  Practice of Consistency for Distributed Data April 23--26, 2018, Porto,\n  Portugal, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most distributed storage systems provide limited abilities for querying data\nby attributes other than their primary keys. Supporting efficient search on\nsecondary attributes is challenging as applications pose varying requirements\nto query processing systems, and no single system design can be suitable for\nall needs. In this paper, we show how to overcome these challenges in order to\nextend distributed data stores to support queries on secondary attributes. We\npropose a modular architecture that is flexible and allows query processing\nsystems to make trade-offs according to different use case requirements. We\ndescribe adap-tive mechanisms that make use of this flexibility to enable query\nprocessing systems to dynamically adjust to query and write operation\nworkloads.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 07:39:56 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Vasilas", "Dimitrios", "", "LIP6, DELYS"], ["Shapiro", "Marc", "", "DELYS, LIP6"], ["King", "Bradley", ""]]}, {"id": "1803.04237", "submitter": "Diego Didona Dr", "authors": "Diego Didona, Rachid Guerraoui, Jingjing Wang, Willy Zwaenepoel", "title": "Causal Consistency and Latency Optimality: Friend or Foe?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal consistency is an attractive consistency model for replicated data\nstores. It is provably the strongest model that tolerates partitions, it avoids\nthe long latencies associated with strong consistency, and, especially when\nusing read-only transactions, it prevents many of the anomalies of weaker\nconsistency models. Recent work has shown that causal consistency allows\n\"latency-optimal\" read-only transactions, that are nonblocking, single-version\nand single-round in terms of communication. On the surface, this latency\noptimality is very appealing, as the vast majority of applications are assumed\nto have read-dominated workloads.\n  In this paper, we show that such \"latency-optimal\" read-only transactions\ninduce an extra overhead on writes, the extra overhead is so high that\nperformance is actually jeopardized, even in read-dominated workloads. We show\nthis result from a practical and a theoretical angle.\n  First, we present a protocol that implements \"almost laten- cy-optimal\" ROTs\nbut does not impose on the writes any of the overhead of latency-optimal\nprotocols. In this protocol, ROTs are nonblocking, one version and can be\nconfigured to use either two or one and a half rounds of client-server\ncommunication. We experimentally show that this protocol not only provides\nbetter throughput, as expected, but also surprisingly better latencies for all\nbut the lowest loads and most read-heavy workloads.\n  Then, we prove that the extra overhead imposed on writes by latency-optimal\nread-only transactions is inherent, i.e., it is not an artifact of the design\nwe consider, and cannot be avoided by any implementation of latency-optimal\nread-only transactions. We show in particular that this overhead grows linearly\nwith the number of clients.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 13:18:05 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Didona", "Diego", ""], ["Guerraoui", "Rachid", ""], ["Wang", "Jingjing", ""], ["Zwaenepoel", "Willy", ""]]}, {"id": "1803.04292", "submitter": "Bertil Chapuis", "authors": "Bertil Chapuis, Benoit Garbinato", "title": "Geodabs: Trajectory Indexing Meets Fingerprinting at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding trajectories and discovering motifs that are similar in large\ndatasets is a central problem for a wide range of applications. Solutions\naddressing this problem usually rely on spatial indexing and on the computation\nof a similarity measure in polynomial time. Although effective in the context\nof sparse trajectory datasets, this approach is too expensive in the context of\ndense datasets, where many trajectories potentially match with a given query.\nIn this paper, we apply fingerprinting, a copy-detection mechanism used in the\ncontext of textual data, to trajectories. To this end, we fingerprint\ntrajectories with geodabs, a construction based on geohash aimed at trajectory\nfingerprinting. We demonstrate that by relying on the properties of a space\nfilling curve geodabs can be used to build sharded inverted indexes. We show\nhow normalization affects precision and recall, two key measures in information\nretrieval. We then demonstrate that the probabilistic nature of fingerprinting\nhas a marginal effect on the quality of the results. Finally, we evaluate our\nmethod in terms of performances and show that, in contrast with existing\nmethods, it is not affected by the density of the trajectory dataset and that\nit can be efficiently distributed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 14:48:06 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Chapuis", "Bertil", ""], ["Garbinato", "Benoit", ""]]}, {"id": "1803.04562", "submitter": "Babak Salimi", "authors": "Babak Salimi, Johannes Gehrke and Dan Suciu", "title": "Bias in OLAP Queries: Detection, Explanation, and Removal", "comments": "This paper is an extended version of a paper presented at SIGMOD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On line analytical processing (OLAP) is an essential element of\ndecision-support systems. OLAP tools provide insights and understanding needed\nfor improved decision making. However, the answers to OLAP queries can be\nbiased and lead to perplexing and incorrect insights. In this paper, we propose\nHypDB, a system to detect, explain, and to resolve bias in decision-support\nqueries. We give a simple definition of a \\emph{biased query}, which performs a\nset of independence tests on the data to detect bias. We propose a novel\ntechnique that gives explanations for bias, thus assisting an analyst in\nunderstanding what goes on. Additionally, we develop an automated method for\nrewriting a biased query into an unbiased query, which shows what the analyst\nintended to examine. In a thorough evaluation on several real datasets we show\nboth the quality and the performance of our techniques, including the\ncompletely automatic discovery of the revolutionary insights from a famous 1973\ndiscrimination case.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 22:54:11 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 21:35:42 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Salimi", "Babak", ""], ["Gehrke", "Johannes", ""], ["Suciu", "Dan", ""]]}, {"id": "1803.04884", "submitter": "Torsten Kilias", "authors": "Torsten Kilias, Alexander L\\\"oser, Felix A. Gers, Richard\n  Koopmanschap, Ying Zhang, Martin Kersten", "title": "IDEL: In-Database Entity Linking with Neural Embeddings", "comments": "This manuscript is a preprint for a paper submitted to VLDB2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel architecture, In-Database Entity Linking (IDEL), in which\nwe integrate the analytics-optimized RDBMS MonetDB with neural text mining\nabilities. Our system design abstracts core tasks of most neural entity linking\nsystems for MonetDB. To the best of our knowledge, this is the first defacto\nimplemented system integrating entity-linking in a database. We leverage the\nability of MonetDB to support in-database-analytics with user defined functions\n(UDFs) implemented in Python. These functions call machine learning libraries\nfor neural text mining, such as TensorFlow. The system achieves zero cost for\ndata shipping and transformation by utilizing MonetDB's ability to embed Python\nprocesses in the database kernel and exchange data in NumPy arrays. IDEL\nrepresents text and relational data in a joint vector space with neural\nembeddings and can compensate errors with ambiguous entity representations. For\ndetecting matching entities, we propose a novel similarity function based on\njoint neural embeddings which are learned via minimizing pairwise contrastive\nranking loss. This function utilizes a high dimensional index structures for\nfast retrieval of matching entities. Our first implementation and experiments\nusing the WebNLG corpus show the effectiveness and the potentials of IDEL.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 15:35:42 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Kilias", "Torsten", ""], ["L\u00f6ser", "Alexander", ""], ["Gers", "Felix A.", ""], ["Koopmanschap", "Richard", ""], ["Zhang", "Ying", ""], ["Kersten", "Martin", ""]]}, {"id": "1803.05277", "submitter": "Domagoj Vrgo\\v{c}", "authors": "Fernando Florenzano, Cristian Riveros, Martin Ugarte, Stijn\n  Vansummeren, Domagoj Vrgoc", "title": "Constant delay algorithms for regular document spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular expressions and automata models with capture variables are core tools\nin rule-based information extraction. These formalisms, also called regular\ndocument spanners, use regular languages in order to locate the data that a\nuser wants to extract from a text document, and then store this data into\nvariables. Since document spanners can easily generate large outputs, it is\nimportant to have good evaluation algorithms that can generate the extracted\ndata in a quick succession, and with relatively little precomputation time.\nTowards this goal, we present a practical evaluation algorithm that allows\nconstant delay enumeration of a spanner's output after a precomputation phase\nthat is linear in the document. While the algorithm assumes that the spanner is\nspecified in a syntactic variant of variable set automata, we also study how it\ncan be applied when the spanner is specified by general variable set automata,\nregex formulas, or spanner algebras. Finally, we study the related problem of\ncounting the number of outputs of a document spanner, providing a fine grained\nanalysis of the classes of document spanners that support efficient enumeration\nof their results.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 13:44:53 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Florenzano", "Fernando", ""], ["Riveros", "Cristian", ""], ["Ugarte", "Martin", ""], ["Vansummeren", "Stijn", ""], ["Vrgoc", "Domagoj", ""]]}, {"id": "1803.05714", "submitter": "Boyi Hou", "authors": "Boyi Hou, Qun Chen, Zhaoqiang Chen, Youcef Nafa, Zhanhuai Li", "title": "r-HUMO: A Risk-Aware Human-Machine Cooperation Framework for Entity\n  Resolution with Quality Guarantees", "comments": "12 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1710.00204", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though many approaches have been proposed for entity resolution (ER), it\nremains very challenging to find one with quality guarantees. To this end, we\nproposea risk-aware HUman-Machine cOoperation framework for ER, denoted by\nr-HUMO. Built on the existing HUMO framework, r-HUMO similarly enforces both\nprecision and recall levels by partitioning an ER workload between the human\nand the machine. However, r-HUMO is the first solution to optimize the process\nof human workload selection from a risk perspective. It iteratively selects\nhuman workload based on real-time risk analysis on human-labeled results as\nwell as prespecified machine metrics. In this paper,we first introduce the\nr-HUMO framework and then present the risk analysis technique to prioritize the\ninstances for manual labeling. Finally,we empirically evaluate r-HUMO's\nperformance on real data. Our extensive experiments show that r-HUMO is\neffective in enforcing quality guarantees,and compared with the\nstate-of-the-art alternatives, it can achieve better quality control with\nreduced human cost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 12:45:46 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 12:35:05 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 02:04:20 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Hou", "Boyi", ""], ["Chen", "Qun", ""], ["Chen", "Zhaoqiang", ""], ["Nafa", "Youcef", ""], ["Li", "Zhanhuai", ""]]}, {"id": "1803.06015", "submitter": "Sara Cohen", "authors": "Sara Cohen and Aviv Zohar", "title": "Database Perspectives on Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern blockchain systems are a fresh look at the paradigm of distributed\ncomputing, applied under assumptions of large-scale public networks. They can\nbe used to store and share information without a trusted central party. There\nhas been much effort to develop blockchain systems for a myriad of uses,\nranging from cryptocurrencies to identity control, supply chain management,\netc. None of this work has directly studied the fundamental database issues\nthat arise when using blockchains as the underlying infrastructure to manage\ndata.\n  The key difference between using blockchains to store data and centrally\ncontrolled databases is that transactions are accepted to a blockchain via a\nconsensus mechanism. Hence, once a user has issued a transaction, she cannot be\ncertain if it will be accepted. Moreover, a yet unaccepted transaction cannot\nbe retracted by the user, and may be appended to the blockchain in the future.\nThis causes difficulties as the user may wish to reissue a transaction, if it\nwas not accepted. Yet this data may then become appended twice to the\nblockchain.\n  In this paper we present a database perspective on blockchains by introducing\nformal foundations for blockchains as a storage layer that underlies a\ndatabase. The main issue that we tackle is uncertainty in transaction appending\nthat is a result of the consensus mechanism. We study two flavors of\ntransaction appending problems: (1) the complexity of determining whether it is\npossible for a denial constraint to be contradicted, given the state of the\nblockchain, pending transactions, and integrity constraints and (2) the\ncomplexity of generating transactions that are mutually (in)consistent with\ngiven subsets of pending transactions. Solving these problems is critical to\nensure that users can issue transactions consistent with their intentions.\nFinally, we chart important directions for future work.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 21:58:39 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Cohen", "Sara", ""], ["Zohar", "Aviv", ""]]}, {"id": "1803.06071", "submitter": "Hongzhi Wang", "authors": "Zhixin Qi, Hongzhi Wang, Jianzhong Li, Hong Gao", "title": "Impacts of Dirty Data: and Experimental Evaluation", "comments": "22 pages, 192 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quality issues have attracted widespread attention due to the negative\nimpacts of dirty data on data mining and machine learning results. The\nrelationship between data quality and the accuracy of results could be applied\non the selection of the appropriate algorithm with the consideration of data\nquality and the determination of the data share to clean. However, rare\nresearch has focused on exploring such relationship. Motivated by this, this\npaper conducts an experimental comparison for the effects of missing,\ninconsistent and conflicting data on classification and clustering algorithms.\nBased on the experimental findings, we provide guidelines for algorithm\nselection and data cleaning.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 04:23:00 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 07:48:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Qi", "Zhixin", ""], ["Wang", "Hongzhi", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}, {"id": "1803.06089", "submitter": "Florin Rusu", "authors": "Weijie Zhao, Florin Rusu, Bin Dong, Kesheng Wu, Anna Y. Q. Ho, and\n  Peter Nugent", "title": "Distributed Caching for Complex Querying of Raw Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As applications continue to generate multi-dimensional data at exponentially\nincreasing rates, fast analytics to extract meaningful results is becoming\nextremely important. The database community has developed array databases that\nalleviate this problem through a series of techniques. In-situ mechanisms\nprovide direct access to raw data in the original format---without loading and\npartitioning. Parallel processing scales to the largest datasets. In-memory\ncaching reduces latency when the same data are accessed across a workload of\nqueries. However, we are not aware of any work on distributed caching of\nmulti-dimensional raw arrays. In this paper, we introduce a distributed\nframework for cost-based caching of multi-dimensional arrays in native format.\nGiven a set of files that contain portions of an array and an online query\nworkload, the framework computes an effective caching plan in two stages.\nFirst, the plan identifies the cells to be cached locally from each of the\ninput files by continuously refining an evolving R-tree index. In the second\nstage, an optimal assignment of cells to nodes that collocates dependent cells\nin order to minimize the overall data transfer is determined. We design cache\neviction and placement heuristic algorithms that consider the historical query\nworkload. A thorough experimental evaluation over two real datasets in three\nfile formats confirms the superiority -- by as much as two orders of magnitude\n-- of the proposed framework over existing techniques in terms of cache\noverhead and workload execution time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 06:33:24 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Zhao", "Weijie", ""], ["Rusu", "Florin", ""], ["Dong", "Bin", ""], ["Wu", "Kesheng", ""], ["Ho", "Anna Y. Q.", ""], ["Nugent", "Peter", ""]]}, {"id": "1803.06354", "submitter": "Jimmy Lin", "authors": "Youngbin Kim and Jimmy Lin", "title": "Serverless Data Analytics with Flint", "comments": "Published in the Proceedings of the 2018 IEEE 11th International\n  Conference on Cloud Computing (CLOUD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless architectures organized around loosely-coupled function\ninvocations represent an emerging design for many applications. Recent work\nmostly focuses on user-facing products and event-driven processing pipelines.\nIn this paper, we explore a completely different part of the application space\nand examine the feasibility of analytical processing on big data using a\nserverless architecture. We present Flint, a prototype Spark execution engine\nthat takes advantage of AWS Lambda to provide a pure pay-as-you-go cost model.\nWith Flint, a developer uses PySpark exactly as before, but without needing an\nactual Spark cluster. We describe the design, implementation, and performance\nof Flint, along with the challenges associated with serverless analytics.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 18:02:27 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 00:51:26 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Kim", "Youngbin", ""], ["Lin", "Jimmy", ""]]}, {"id": "1803.06416", "submitter": "Rachel Cummings", "authors": "Rachel Cummings, Sara Krehbiel, Kevin A. Lai, Uthaipon Tantipongpipat", "title": "Differential Privacy for Growing Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of differentially private algorithms for adaptive\nanalysis of dynamically growing databases, where a database accumulates new\ndata entries while the analysis is ongoing. We provide a collection of tools\nfor machine learning and other types of data analysis that guarantee\ndifferential privacy and accuracy as the underlying databases grow arbitrarily\nlarge. We give both a general technique and a specific algorithm for adaptive\nanalysis of dynamically growing databases. Our general technique is illustrated\nby two algorithms that schedule black box access to some algorithm that\noperates on a fixed database to generically transform private and accurate\nalgorithms for static databases into private and accurate algorithms for\ndynamically growing databases. These results show that almost any private and\naccurate algorithm can be rerun at appropriate points of data growth with\nminimal loss of accuracy, even when data growth is unbounded. Our specific\nalgorithm directly adapts the private multiplicative weights algorithm to the\ndynamic setting, maintaining the accuracy guarantee of the static setting\nthrough unbounded data growth. Along the way, we develop extensions of several\nother differentially private algorithms to the dynamic setting, which may be of\nindependent interest for future work on the design of differentially private\nalgorithms for growing databases.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 22:05:44 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Cummings", "Rachel", ""], ["Krehbiel", "Sara", ""], ["Lai", "Kevin A.", ""], ["Tantipongpipat", "Uthaipon", ""]]}, {"id": "1803.06445", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi, Georg Gottlob and Reinhard Pichler", "title": "Datalog: Bag Semantics via Set Semantics", "comments": "Extended version of paper appearing in Proc. ICDT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duplicates in data management are common and problematic. In this work, we\npresent a translation of Datalog under bag semantics into a well-behaved\nextension of Datalog, the so-called {\\em warded Datalog}$^\\pm$, under set\nsemantics. From a theoretical point of view, this allows us to reason on bag\nsemantics by making use of the well-established theoretical foundations of set\nsemantics. From a practical point of view, this allows us to handle the bag\nsemantics of Datalog by powerful, existing query engines for the required\nextension of Datalog. This use of Datalog$^\\pm$ is extended to give a set\nsemantics to duplicates in Datalog$^\\pm$ itself. We investigate the properties\nof the resulting Datalog$^\\pm$ programs, the problem of deciding\nmultiplicities, and expressibility of some bag operations. Moreover, the\nproposed translation has the potential for interesting applications such as to\nMultiset Relational Algebra and the semantic web query language SPARQL with bag\nsemantics.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 02:00:47 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 18:30:24 GMT"}, {"version": "v3", "created": "Tue, 12 Feb 2019 16:16:36 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Gottlob", "Georg", ""], ["Pichler", "Reinhard", ""]]}, {"id": "1803.06632", "submitter": "Itai Dattner", "authors": "Lior Shabtay, Rami Yaari, Itai Dattner", "title": "A Guided FP-growth algorithm for multitude-targeted mining of big data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the GFP-growth (Guided FP-growth) algorithm, a novel\nmethod for multitude-targeted mining: finding the count of a given large list\nof itemsets in large data. The GFP-growth algorithm is designed to focus on the\nspecific multitude itemsets of interest and optimizes the time and memory\ncosts. We prove that the GFP-growth algorithm yields the exact frequency-counts\nfor the required itemsets. We show that for a number of different problems, a\nsolution can be devised which takes advantage of the efficient implementation\nof multitude-targeted mining for boosting the performance. In particular, we\nstudy in detail the problem of generating the minority-class rules from\nimbalanced data, a scenario that appears in many real-life domains such as\nmedical applications, failure prediction, network and cyber security, and\nmaintenance. We develop the Minority-Report Algorithm that uses the GFP-growth\nfor boosting performance. We prove some theoretical properties of the\nMinority-Report Algorithm and demonstrate its performance gain using\nsimulations and real data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 09:57:34 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 13:32:55 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Shabtay", "Lior", ""], ["Yaari", "Rami", ""], ["Dattner", "Itai", ""]]}, {"id": "1803.06674", "submitter": "Zhenjiang Hu", "authors": "Yasuhito Asano, Soichiro Hidaka, Zhenjiang Hu, Yasunori Ishihara,\n  Hiroyuki Kato, Hsiang-Shang Ko, Keisuke Nakano, Makoto Onizuka, Yuya Sasaki,\n  Toshiyuki Shimizu, Kanae Tsushima, Masatoshi Yoshikawa", "title": "A View-based Programmable Architecture for Controlling and Integrating\n  Decentralized Data", "comments": "14 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The view and the view update are known mechanism for controlling access of\ndata and for integrating data of different schemas. Despite intensive and long\nresearch on them in both the database community and the programming language\ncommunity, we are facing difficulties to use them in practice. The main reason\nis that we are lacking of control over the view update strategy to deal with\ninherited ambiguity of view update for a given view.\n  This vision paper aims to provide a new language-based approach to\ncontrolling and integrating decentralized data based on the view, and establish\na software foundation for systematic construction of such data management\nsystems. Our key observation is that a view should be defined through a view\nupdate strategy rather than a query. In other words, the view definition should\nbe extracted from the view update strategy, which is in sharp contrast to the\ntraditional approaches where the view update strategy is derived from the view\ndefinition.\n  In this paper, we present the first programmable architecture with a\ndeclarative language for specifying update strategies over views, whose unique\nview definition can be automatically derived, and show how it can be\neffectively used to control data access, integrate data generally allowing\ncoexistence of GAV (global as view) and LAV (local as view), and perform both\nanalysis and updates on the integrated data. We demonstrate its usefulness\nthrough development of a privacy-preserving ride-sharing alliance system,\ndiscuss its application scope, and highlight future challenges.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 15:05:05 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Asano", "Yasuhito", ""], ["Hidaka", "Soichiro", ""], ["Hu", "Zhenjiang", ""], ["Ishihara", "Yasunori", ""], ["Kato", "Hiroyuki", ""], ["Ko", "Hsiang-Shang", ""], ["Nakano", "Keisuke", ""], ["Onizuka", "Makoto", ""], ["Sasaki", "Yuya", ""], ["Shimizu", "Toshiyuki", ""], ["Tsushima", "Kanae", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "1803.06850", "submitter": "Richard McClatchey", "authors": "Barkha Javed, Zaheer Khan and Richard McClatchey", "title": "An Adaptable System to Support Provenance Management for the Public\n  Policy-Making Process in Smart Cities", "comments": "26 pages, 15 figures, 4 tables", "journal-ref": "Informatics Vol5 No 3 pp 1-26 2018", "doi": "10.3390/informatics5010003", "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Government policies aim to address public issues and problems and therefore\nplay a pivotal role in peoples lives. The creation of public policies, however,\nis complex given the perspective of large and diverse stakeholders involvement,\nconsiderable human participation, lengthy processes, complex task specification\nand the non-deterministic nature of the process. The inherent complexities of\nthe policy process impart challenges for designing a computing system that\nassists in supporting and automating the business process pertaining to policy\nsetup, which also raises concerns for setting up a tracking service in the\npolicy-making environment. A tracking service informs how decisions have been\ntaken during policy creation and can provide useful and intrinsic information\nregarding the policy process. At present, there exists no computing system that\nassists in tracking the complete process that has been employed for policy\ncreation. To design such a system, it is important to consider the policy\nenvironment challenges; for this a novel network and goal based approach has\nbeen framed and is covered in detail in this paper. Furthermore, smart\ngovernance objectives that include stakeholders participation and citizens\ninvolvement have been considered. Thus, the proposed approach has been devised\nby considering smart governance principles and the knowledge environment of\npolicy making where tasks are largely dependent on policy makers decisions and\non individual policy objectives. Our approach reckons the human dimension for\ndeciding and defining autonomous process activities at run time. Furthermore,\nwith the network-based approach, so-called provenance data tracking is employed\nwhich enables the capture of policy process.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 09:52:53 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Javed", "Barkha", ""], ["Khan", "Zaheer", ""], ["McClatchey", "Richard", ""]]}, {"id": "1803.07434", "submitter": "Richard McClatchey", "authors": "R McClatchey, A Branson, J Shamdasani, P. Emin", "title": "Evolvable Systems for Big Data Management in Business", "comments": "5 pages, 2 figures, Proc of the 4th International Symposium on Big\n  Data Principles, Architectures & Applications (BDAA 2017). arXiv admin note:\n  text overlap with arXiv:1502.01545, arXiv:1402.5764, arXiv:1402.5753", "journal-ref": null, "doi": "10.1109/HPCS.2017.14", "report-no": null, "categories": "cs.SE cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Big Data systems are increasingly having to be longer lasting,\nenterprise-wide and interoperable with other (legacy or new) systems.\nFurthermore many organizations operate in an external environment which\ndictates change at an unforeseeable rate and requires evolution in system\nrequirements. In these cases system development does not have a definitive end\npoint, rather it continues in a mutually constitutive cycle with the\norganization and its requirements. Also when the period of design is of such\nduration that the technology may well evolve or when the required technology is\nnot mature at the outset, then the design process becomes considerably more\ndifficult. Not only that but if the system must inter-operate with other\nsystems then the design process becomes considerably more difficult. Ideally in\nthese circumstances the design must also be able to evolve in order to react to\nchanging technologies and requirements and to ensure traceability between the\ndesign and the evolving system specification. In other words extended design\nperiods necessitate adaptable design specifications. For interoperability Big\nData systems need to be discoverable and to work with information about other\nsystems with which they need to cooperate over time. We have developed software\ncalled CRISTAL-ISE that enables dynamic system evolution and interoperability\nfor Big Data systems, it has been commercialised as the Agilium-NG BPM product\nand is described in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 09:01:38 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["McClatchey", "R", ""], ["Branson", "A", ""], ["Shamdasani", "J", ""], ["Emin", "P.", ""]]}, {"id": "1803.07480", "submitter": "Maximilian Schleich", "authors": "Mahmoud Abo Khamis and Hung Q. Ngo and XuanLong Nguyen and Dan Olteanu\n  and Maximilian Schleich", "title": "AC/DC: In-Database Learning Thunderstruck", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on the design and implementation of the AC/DC gradient descent\nsolver for a class of optimization problems over normalized databases. AC/DC\ndecomposes an optimization problem into a set of aggregates over the join of\nthe database relations. It then uses the answers to these aggregates to\niteratively improve the solution to the problem until it converges.\n  The challenges faced by AC/DC are the large database size, the mixture of\ncontinuous and categorical features, and the large number of aggregates to\ncompute. AC/DC addresses these challenges by employing a sparse data\nrepresentation, factorized computation, problem reparameterization under\nfunctional dependencies, and a data structure that supports shared computation\nof aggregates.\n  To train polynomial regression models and factorization machines of up to\n154K features over the natural join of all relations from a real-world dataset\nof up to 86M tuples, AC/DC needs up to 30 minutes on one core of a commodity\nmachine. This is up to three orders of magnitude faster than its competitors R,\nMadLib, libFM, and TensorFlow whenever they finish and thus do not exceed\nmemory limitation, 24-hour timeout, or internal design limitations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 15:17:14 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 04:35:21 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Nguyen", "XuanLong", ""], ["Olteanu", "Dan", ""], ["Schleich", "Maximilian", ""]]}, {"id": "1803.07847", "submitter": "Giacomo Kahn", "authors": "Alexandre Bazin (LIP6), Jessie Carbonnel (MAREL), Marianne Huchard\n  (MAREL), Giacomo Kahn (LIMOS)", "title": "On-demand Relational Concept Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal Concept Analysis and its associated conceptual structures have been\nused to support exploratory search through conceptual navigation. Relational\nConcept Analysis (RCA) is an extension of Formal Concept Analysis to process\nrelational datasets. RCA and its multiple interconnected structures represent\ngood candidates to support exploratory search in relational datasets, as they\nare enabling navigation within a structure as well as between the connected\nstructures. However, building the entire structures does not present an\nefficient solution to explore a small localised area of the dataset, for\ninstance to retrieve the closest alternatives to a given query. In these cases,\ngenerating only a concept and its neighbour concepts at each navigation step\nappears as a less costly alternative. In this paper, we propose an algorithm to\ncompute a concept and its neighbourhood in extended concept lattices. The\nconcepts are generated directly from the relational context family, and possess\nboth formal and relational attributes. The algorithm takes into account two RCA\nscaling operators. We illustrate it on an example.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 10:50:26 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Bazin", "Alexandre", "", "LIP6"], ["Carbonnel", "Jessie", "", "MAREL"], ["Huchard", "Marianne", "", "MAREL"], ["Kahn", "Giacomo", "", "LIMOS"]]}, {"id": "1803.08604", "submitter": "Jennifer Ortiz", "authors": "Jennifer Ortiz, Magdalena Balazinska, Johannes Gehrke, S. Sathiya\n  Keerthi", "title": "Learning State Representations for Query Optimization with Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning is quickly changing the field of artificial\nintelligence. These models are able to capture a high level understanding of\ntheir environment, enabling them to learn difficult dynamic tasks in a variety\nof domains. In the database field, query optimization remains a difficult\nproblem. Our goal in this work is to explore the capabilities of deep\nreinforcement learning in the context of query optimization. At each state, we\nbuild queries incrementally and encode properties of subqueries through a\nlearned representation. The challenge here lies in the formation of the state\ntransition function, which defines how the current subquery state combines with\nthe next query operation (action) to yield the next state. As a first step in\nthis direction, we focus the state representation problem and the formation of\nthe state transition function. We describe our approach and show preliminary\nresults. We further discuss how we can use the state representation to improve\nquery optimization using reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 22:39:32 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Ortiz", "Jennifer", ""], ["Balazinska", "Magdalena", ""], ["Gehrke", "Johannes", ""], ["Keerthi", "S. Sathiya", ""]]}, {"id": "1803.09010", "submitter": "Hanna Wallach", "authors": "Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\n  Vaughan, Hanna Wallach, Hal Daum\\'e III, and Kate Crawford", "title": "Datasheets for Datasets", "comments": "Working Paper, comments are encouraged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning community currently has no standardized process for\ndocumenting datasets, which can lead to severe consequences in high-stakes\ndomains. To address this gap, we propose datasheets for datasets. In the\nelectronics industry, every component, no matter how simple or complex, is\naccompanied with a datasheet that describes its operating characteristics, test\nresults, recommended uses, and other information. By analogy, we propose that\nevery dataset be accompanied with a datasheet that documents its motivation,\ncomposition, collection process, recommended uses, and so on. Datasheets for\ndatasets will facilitate better communication between dataset creators and\ndataset consumers, and encourage the machine learning community to prioritize\ntransparency and accountability.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 23:22:18 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 03:22:43 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2018 18:26:32 GMT"}, {"version": "v4", "created": "Sun, 14 Apr 2019 22:03:18 GMT"}, {"version": "v5", "created": "Thu, 9 Jan 2020 00:59:24 GMT"}, {"version": "v6", "created": "Tue, 14 Jan 2020 01:36:33 GMT"}, {"version": "v7", "created": "Thu, 19 Mar 2020 17:26:37 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Gebru", "Timnit", ""], ["Morgenstern", "Jamie", ""], ["Vecchione", "Briana", ""], ["Vaughan", "Jennifer Wortman", ""], ["Wallach", "Hanna", ""], ["Daum\u00e9", "Hal", "III"], ["Crawford", "Kate", ""]]}, {"id": "1803.09565", "submitter": "Alexander Titus", "authors": "Alexander J. Titus, Audrey Flower, Patrick Hagerty, Paul Gamble,\n  Charlie Lewis, Todd Stavish, Kevin P. OConnell, Greg Shipley, and Stephanie\n  M. Rogers", "title": "SIG-DB: leveraging homomorphic encryption to Securely Interrogate\n  privately held Genomic DataBases", "comments": "38 pages, 3 figures, 4 tables, 1 supplemental table, 7 supplemental\n  figures", "journal-ref": "PLoS Computational Biology; 2018 Sep 4; 14(9):e1006454", "doi": "10.1371/journal.pcbi.1006454", "report-no": "PMID: 30180163", "categories": "q-bio.QM cs.CR cs.DB q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic data are becoming increasingly valuable as we develop methods to\nutilize the information at scale and gain a greater understanding of how\ngenetic information relates to biological function. Advances in synthetic\nbiology and the decreased cost of sequencing are increasing the amount of\nprivately held genomic data. As the quantity and value of private genomic data\ngrows, so does the incentive to acquire and protect such data, which creates a\nneed to store and process these data securely. We present an algorithm for the\nSecure Interrogation of Genomic DataBases (SIG-DB). The SIG-DB algorithm\nenables databases of genomic sequences to be searched with an encrypted query\nsequence without revealing the query sequence to the Database Owner or any of\nthe database sequences to the Querier. SIG-DB is the first application of its\nkind to take advantage of locality-sensitive hashing and homomorphic encryption\nto allow generalized sequence-to-sequence comparisons of genomic data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 13:09:12 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Titus", "Alexander J.", ""], ["Flower", "Audrey", ""], ["Hagerty", "Patrick", ""], ["Gamble", "Paul", ""], ["Lewis", "Charlie", ""], ["Stavish", "Todd", ""], ["OConnell", "Kevin P.", ""], ["Shipley", "Greg", ""], ["Rogers", "Stephanie M.", ""]]}, {"id": "1803.09627", "submitter": "Francois Fouquet PhD", "authors": "Thomas Hartmann, Francois Fouquet, Assaad Moawad, Romain Rouvoy, Yves\n  Le Traon", "title": "GreyCat: Efficient What-If Analytics for Data in Motion at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, data analytics shifted from a descriptive era,\nconfined to the explanation of past events, to the emergence of predictive\ntechniques. Nonetheless, existing predictive techniques still fail to\neffectively explore alternative futures, which continuously diverge from\ncurrent situations when exploring the effects of what-if decisions. Enabling\nprescriptive analytics therefore calls for the design of scalable systems that\ncan cope with the complexity and the diversity of underlying data models. In\nthis article, we address this challenge by combining graphs and time series\nwithin a scalable storage system that can organize a massive amount of\nunstructured and continuously changing data into multi-dimensional data models,\ncalled Many-Worlds Graphs. We demonstrate that our open source implementation,\nGreyCat, can efficiently fork and update thousands of parallel worlds composed\nof millions of timestamped nodes, such as what-if exploration.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 07:48:15 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Hartmann", "Thomas", ""], ["Fouquet", "Francois", ""], ["Moawad", "Assaad", ""], ["Rouvoy", "Romain", ""], ["Traon", "Yves Le", ""]]}, {"id": "1803.09835", "submitter": "Kexin Rong", "authors": "Kexin Rong, Clara E. Yoon, Karianne J. Bergen, Hashem Elezabi, Peter\n  Bailis, Philip Levis, Gregory C. Beroza", "title": "Locality-Sensitive Hashing for Earthquake Detection: A Case Study of\n  Scaling Data-Driven Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we report on a novel application of Locality Sensitive Hashing\n(LSH) to seismic data at scale. Based on the high waveform similarity between\nreoccurring earthquakes, our application identifies potential earthquakes by\nsearching for similar time series segments via LSH. However, a straightforward\nimplementation of this LSH-enabled application has difficulty scaling beyond 3\nmonths of continuous time series data measured at a single seismic station. As\na case study of a data-driven science workflow, we illustrate how domain\nknowledge can be incorporated into the workload to improve both the efficiency\nand result quality. We describe several end-to-end optimizations of the\nanalysis pipeline from pre-processing to post-processing, which allow the\napplication to scale to time series data measured at multiple seismic stations.\nOur optimizations enable an over 100$\\times$ speedup in the end-to-end analysis\npipeline. This improved scalability enabled seismologists to perform seismic\nanalysis on more than ten years of continuous time series data from over ten\nseismic stations, and has directly enabled the discovery of 597 new earthquakes\nnear the Diablo Canyon nuclear power plant in California and 6123 new\nearthquakes in New Zealand.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 20:43:25 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 03:10:28 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Rong", "Kexin", ""], ["Yoon", "Clara E.", ""], ["Bergen", "Karianne J.", ""], ["Elezabi", "Hashem", ""], ["Bailis", "Peter", ""], ["Levis", "Philip", ""], ["Beroza", "Gregory C.", ""]]}, {"id": "1803.09930", "submitter": "Hung Ngo", "authors": "Hung Q. Ngo", "title": "Worst-Case Optimal Join Algorithms: Techniques, Results, and Open\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worst-case optimal join algorithms are the class of join algorithms whose\nruntime match the worst-case output size of a given join query. While the first\nprovably worst-case optimal join algorithm was discovered relatively recently,\nthe techniques and results surrounding these algorithms grow out of decades of\nresearch from a wide range of areas, intimately connecting graph theory,\nalgorithms, information theory, constraint satisfaction, database theory, and\ngeometric inequalities. These ideas are not just paperware: in addition to\nacademic project implementations, two variations of such algorithms are the\nwork-horse join algorithms of commercial database and data analytics engines.\n  This paper aims to be a brief introduction to the design and analysis of\nworst-case optimal join algorithms. We discuss the key techniques for proving\nruntime and output size bounds. We particularly focus on the fascinating\nconnection between join algorithms and information theoretic inequalities, and\nthe idea of how one can turn a proof into an algorithm. Finally, we conclude\nwith a representative list of fundamental open problems in this area.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 07:13:49 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 21:05:41 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 04:17:42 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Ngo", "Hung Q.", ""]]}, {"id": "1803.10311", "submitter": "Doris Xin", "authors": "Doris Xin, Litian Ma, Shuchen Song, Aditya Parameswaran", "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the\n  Applied Machine Learning Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning workflow development is anecdotally regarded to be an\niterative process of trial-and-error with humans-in-the-loop. However, we are\nnot aware of quantitative evidence corroborating this popular belief. A\nquantitative characterization of iteration can serve as a benchmark for machine\nlearning workflow development in practice, and can aid the development of\nhuman-in-the-loop machine learning systems. To this end, we conduct a\nsmall-scale survey of the applied machine learning literature from five\ndistinct application domains. We collect and distill statistics on the role of\niteration within machine learning workflow development, and report preliminary\ntrends and insights from our investigation, as a starting point towards this\nbenchmark. Based on our findings, we finally describe desiderata for effective\nand versatile human-in-the-loop machine learning systems that can cater to\nusers in diverse domains.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 20:38:05 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 22:16:31 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Xin", "Doris", ""], ["Ma", "Litian", ""], ["Song", "Shuchen", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1803.10901", "submitter": "Bikram Karmakar", "authors": "Bikram Karmakar and Indranil Mukhopadhyay", "title": "Statistical Validity and Consistency of Big Data Analytics: A General\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Informatics and technological advancements have triggered generation of huge\nvolume of data with varied complexity in its management and analysis. Big Data\nanalytics is the practice of revealing hidden aspects of such data and making\ninferences from it. Although storage, retrieval and management of Big Data seem\npossible through efficient algorithm and system development, concern about\nstatistical consistency remains to be addressed in view of its specific\ncharacteristics. Since Big Data does not conform to standard analytics, we need\nproper modification of the existing statistical theory and tools. Here we\npropose, with illustrations, a general statistical framework and an algorithmic\nprinciple for Big Data analytics that ensure statistical accuracy of the\nconclusions. The proposed framework has the potential to push forward\nadvancement of Big Data analytics in the right direction. The\npartition-repetition approach proposed here is broad enough to encompass all\npractical data analytic problems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 02:15:03 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Karmakar", "Bikram", ""], ["Mukhopadhyay", "Indranil", ""]]}, {"id": "1803.11105", "submitter": "Ran Bittmann", "authors": "Ran M. Bittmann, Philippe Nemery, Xingtian Shi, Michael Kemelmakher,\n  Mengjiao Wang", "title": "Frequent Item-set Mining without Ubiquitous Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent Item-set Mining (FIM), sometimes called Market Basket Analysis (MBA)\nor Association Rule Learning (ARL), are Machine Learning (ML) methods for\ncreating rules from datasets of transactions of items. Most methods identify\nitems likely to appear together in a transaction based on the support (i.e. a\nminimum number of relative co-occurrence of the items) for that hypothesis.\nAlthough this is a good indicator to measure the relevance of the assumption\nthat these items are likely to appear together, the phenomenon of very frequent\nitems, referred to as ubiquitous items, is not addressed in most algorithms.\nUbiquitous items have the same entropy as infrequent items, and not\ncontributing significantly to the knowledge. On the other hand, they have\nstrong effect on the performance of the algorithms and sometimes preventing the\nconvergence of the FIM algorithms and thus the provision of meaningful results.\nThis paper discusses the phenomenon of ubiquitous items and demonstrates how\nignoring these has a dramatic effect on the computation performances but with a\nlow and controlled effect on the significance of the results.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 14:52:33 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Bittmann", "Ran M.", ""], ["Nemery", "Philippe", ""], ["Shi", "Xingtian", ""], ["Kemelmakher", "Michael", ""], ["Wang", "Mengjiao", ""]]}, {"id": "1803.11328", "submitter": "Guna Prasaad", "authors": "Guna Prasaad, G. Ramalingam, Kaushik Rajan", "title": "Scaling Ordered Stream Processing on Shared-Memory Multicores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern applications require real-time processing of large volumes of\nhigh-speed data. Such data processing needs can be modeled as a streaming\ncomputation. A streaming computation is specified as a dataflow graph that\nexposes multiple opportunities for parallelizing its execution, in the form of\ndata, pipeline and task parallelism. On the other hand, many important\napplications require that processing of the stream be ordered, where inputs are\nprocessed in the same order as they arrive. There is a fundamental conflict\nbetween ordered processing and parallelizing the streaming computation. This\npaper focuses on the problem of effectively parallelizing ordered streaming\ncomputations on a shared-memory multicore machine.\n  We first address the key challenges in exploiting data parallelism in the\nordered setting. We present a low-latency, non-blocking concurrent data\nstructure to order outputs produced by concurrent workers on an operator. We\nalso propose a new approach to parallelizing partitioned stateful operators\nthat can handle load imbalance across partitions effectively and mostly avoid\ndelays due to ordering. We illustrate the trade-offs and effectiveness of our\nconcurrent data-structures on micro-benchmarks and streaming queries from the\nTPCx-BB benchmark. We then present an adaptive runtime that dynamically maps\nthe exposed parallelism in the computation to that of the machine. We propose\nseveral intuitive scheduling heuristics and compare them empirically on the\nTPCx-BB queries. We find that for streaming computations, heuristics that\nexploit as much pipeline parallelism as possible perform better than those that\nseek to exploit data parallelism.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 03:50:08 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Prasaad", "Guna", ""], ["Ramalingam", "G.", ""], ["Rajan", "Kaushik", ""]]}]