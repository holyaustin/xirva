[{"id": "1505.00212", "submitter": "Robert Piro", "authors": "Boris Motik, Yavor Nenov, Robert Piro, Ian Horrocks", "title": "Combining Rewriting and Incremental Materialisation Maintenance for\n  Datalog Programs with Equality", "comments": "All proofs contained in the appendix. 7 pages + 4 pages appendix. 7\n  algorithms and one table with evaluation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Materialisation precomputes all consequences of a set of facts and a datalog\nprogram so that queries can be evaluated directly (i.e., independently from the\nprogram). Rewriting optimises materialisation for datalog programs with\nequality by replacing all equal constants with a single representative; and\nincremental maintenance algorithms can efficiently update a materialisation for\nsmall changes in the input facts. Both techniques are critical to practical\napplicability of datalog systems; however, we are unaware of an approach that\ncombines rewriting and incremental maintenance. In this paper we present the\nfirst such combination, and we show empirically that it can speed up updates by\nseveral orders of magnitude compared to using either rewriting or incremental\nmaintenance in isolation.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 16:21:17 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Motik", "Boris", ""], ["Nenov", "Yavor", ""], ["Piro", "Robert", ""], ["Horrocks", "Ian", ""]]}, {"id": "1505.00326", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli and Michael Benedikt", "title": "Combining Existential Rules and Description Logics (Extended Version)", "comments": "32 pages. To appear in IJCAI 2015. Extended version including proofs", "journal-ref": "Proceedings of the Twenty-Fourth International Joint Conference on\n  Artificial Intelligence (IJCAI), 2015, pages 2691-2697", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query answering under existential rules -- implications with existential\nquantifiers in the head -- is known to be decidable when imposing restrictions\non the rule bodies such as frontier-guardedness [BLM10, BLMS11]. Query\nanswering is also decidable for description logics [Baa03], which further allow\ndisjunction and functionality constraints (assert that certain relations are\nfunctions), however, they are focused on ER-type schemas, where relations have\narity two.\n  This work investigates how to get the best of both worlds: having decidable\nexistential rules on arbitrary arity relations, while allowing rich description\nlogics, including functionality constraints, on arity-two relations. We first\nshow negative results on combining such decidable languages. Second, we\nintroduce an expressive set of existential rules (frontier-one rules with a\ncertain restriction) which can be combined with powerful constraints on\narity-two relations (e.g. GC 2, ALCQIb) while retaining decidable query\nanswering. Further, we provide conditions to add functionality constraints on\nthe higher-arity relations.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 08:55:51 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Benedikt", "Michael", ""]]}, {"id": "1505.00841", "submitter": "Antoine Amarilli", "authors": "Aliaksandr Talaika, Joanna Biega, Antoine Amarilli, Fabian M. Suchanek", "title": "Harvesting Entities from the Web Using Unique Identifiers -- IBEX", "comments": "30 pages, 5 figures, 9 tables. Complete technical report for A.\n  Talaika, J. A. Biega, A. Amarilli, and F. M. Suchanek. IBEX: Harvesting\n  Entities from the Web Using Unique Identifiers. WebDB workshop, 2015", "journal-ref": null, "doi": "10.1145/2767109.2767116", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the prevalence of unique entity identifiers on the\nWeb. These are, e.g., ISBNs (for books), GTINs (for commercial products), DOIs\n(for documents), email addresses, and others. We show how these identifiers can\nbe harvested systematically from Web pages, and how they can be associated with\nhuman-readable names for the entities at large scale.\n  Starting with a simple extraction of identifiers and names from Web pages, we\nshow how we can use the properties of unique identifiers to filter out noise\nand clean up the extraction result on the entire corpus. The end result is a\ndatabase of millions of uniquely identified entities of different types, with\nan accuracy of 73--96% and a very high coverage compared to existing knowledge\nbases. We use this database to compute novel statistics on the presence of\nproducts, people, and other entities on the Web.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 23:21:00 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Talaika", "Aliaksandr", ""], ["Biega", "Joanna", ""], ["Amarilli", "Antoine", ""], ["Suchanek", "Fabian M.", ""]]}, {"id": "1505.01300", "submitter": "Dominique Gay", "authors": "Dominique Gay, Romain Guigour\\`es, Marc Boull\\'e, Fabrice Cl\\'erot", "title": "Cats & Co: Categorical Time Series Coclustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a novel method of clustering and exploratory analysis of temporal\nevent sequences data (also known as categorical time series) based on\nthree-dimensional data grid models. A data set of temporal event sequences can\nbe represented as a data set of three-dimensional points, each point is defined\nby three variables: a sequence identifier, a time value and an event value.\nInstantiating data grid models to the 3D-points turns the problem into\n3D-coclustering.\n  The sequences are partitioned into clusters, the time variable is discretized\ninto intervals and the events are partitioned into clusters. The cross-product\nof the univariate partitions forms a multivariate partition of the\nrepresentation space, i.e., a grid of cells and it also represents a\nnonparametric estimator of the joint distribution of the sequences, time and\nevents dimensions. Thus, the sequences are grouped together because they have\nsimilar joint distribution of time and events, i.e., similar distribution of\nevents along the time dimension. The best data grid is computed using a\nparameter-free Bayesian model selection approach. We also suggest several\ncriteria for exploiting the resulting grid through agglomerative hierarchies,\nfor interpreting the clusters of sequences and characterizing their components\nthrough insightful visualizations. Extensive experiments on both synthetic and\nreal-world data sets demonstrate that data grid models are efficient, effective\nand discover meaningful underlying patterns of categorical time series data.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 09:50:12 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Gay", "Dominique", ""], ["Guigour\u00e8s", "Romain", ""], ["Boull\u00e9", "Marc", ""], ["Cl\u00e9rot", "Fabrice", ""]]}, {"id": "1505.01303", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Wei Ding and Abraham Bagherjeiran", "title": "XTreePath: A generalization of XPath to handle real world structural\n  variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a key problem in information extraction which deals with wrapper\nfailures due to changing content templates. A good proportion of wrapper\nfailures are due to HTML templates changing to cause wrappers to become\nincompatible after element inclusion or removal in a DOM (Tree representation\nof HTML). We perform a large-scale empirical analyses of the causes of shift\nand mathematically quantify the levels of domain difficulty based on entropy.\nWe propose the XTreePath annotation method to captures contextual node\ninformation from the training DOM. We then utilize this annotation in a\nsupervised manner at test time with our proposed Recursive Tree Matching method\nwhich locates nodes most similar in context recursively using the tree edit\ndistance. The search is based on a heuristic function that takes into account\nthe similarity of a tree compared to the structure that was present in the\ntraining data. We evaluate XTreePath using 117,422 pages from 75 diverse\nwebsites in 8 vertical markets. Our XTreePath method consistently outperforms\nXPath and a current commercial system in terms of successful extractions in a\nblackbox test. We make our code and datasets publicly available online.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 10:08:12 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 21:44:41 GMT"}, {"version": "v3", "created": "Wed, 27 Dec 2017 01:31:49 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Ding", "Wei", ""], ["Bagherjeiran", "Abraham", ""]]}, {"id": "1505.01535", "submitter": "Nguyen Cuong Ha Huy", "authors": "Van Nghia Luong, Ha Huy Cuong Nguyen, Van Son Le", "title": "An improvement on fragmentation in Distribution Database Design Based on\n  Knowledge-Oriented Clustering Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of optimizing distributed database includes: fragmentation and\npositioning data. Several different approaches and algorithms have been\nproposed to solve this problem. In this paper, we propose an algorithm that\nbuilds the initial equivalence relation based on the distance threshold. This\nthreshold is also based on knowledge- oriented clustering techniques for both\nof horizontal and vertical fragmentation. Similarity measures used in the\nalgorithms are the measures developed from the classical measures. Experimental\nresults carrying on the small data set match fragmented results based on the\nclassical algorithm. Execution time and data fragmentation significantly\nreduced while the complexity of our algorithm in the general case is stable.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 22:53:18 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Luong", "Van Nghia", ""], ["Nguyen", "Ha Huy Cuong", ""], ["Le", "Van Son", ""]]}, {"id": "1505.02441", "submitter": "Saravanan Thirumuruganathan", "authors": "Weimo Liu, Md Farhadur Rahman, Saravanan Thirumuruganathan, Nan Zhang,\n  Gautam Das", "title": "Aggregate Estimations over Location Based Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location based services (LBS) have become very popular in recent years. They\nrange from map services (e.g., Google Maps) that store geographic locations of\npoints of interests, to online social networks (e.g., WeChat, Sina Weibo,\nFourSquare) that leverage user geographic locations to enable various\nrecommendation functions. The public query interfaces of these services may be\nabstractly modeled as a kNN interface over a database of two dimensional points\non a plane: given an arbitrary query point, the system returns the k points in\nthe database that are nearest to the query point. In this paper we consider the\nproblem of obtaining approximate estimates of SUM and COUNT aggregates by only\nquerying such databases via their restrictive public interfaces. We distinguish\nbetween interfaces that return location information of the returned tuples\n(e.g., Google Maps), and interfaces that do not return location information\n(e.g., Sina Weibo). For both types of interfaces, we develop aggregate\nestimation algorithms that are based on novel techniques for precisely\ncomputing or approximately estimating the Voronoi cell of tuples. We discuss a\ncomprehensive set of real-world experiments for testing our algorithms,\nincluding experiments on Google Maps, WeChat, and Sina Weibo.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 21:37:32 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 05:07:09 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Liu", "Weimo", ""], ["Rahman", "Md Farhadur", ""], ["Thirumuruganathan", "Saravanan", ""], ["Zhang", "Nan", ""], ["Das", "Gautam", ""]]}, {"id": "1505.02444", "submitter": "Joanna Ochremiak", "authors": "Filip Mazowiecki, Joanna Ochremiak, Adam Witkowski", "title": "Eliminating Recursion from Monadic Datalog Programs on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of eliminating recursion from monadic datalog programs\non trees with an infinite set of labels. We show that the boundedness problem,\ni.e., determining whether a datalog program is equivalent to some nonrecursive\none is undecidable but the decidability is regained if the descendant relation\nis disallowed. Under similar restrictions we obtain decidability of the problem\nof equivalence to a given nonrecursive program. We investigate the connection\nbetween these two problems in more detail.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 21:47:29 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Mazowiecki", "Filip", ""], ["Ochremiak", "Joanna", ""], ["Witkowski", "Adam", ""]]}, {"id": "1505.02463", "submitter": "Yaliang Li", "authors": "Yaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su, Bo Zhao, Wei Fan,\n  Jiawei Han", "title": "A Survey on Truth Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to information explosion, data for the objects of interest can be\ncollected from increasingly more sources. However, for the same object, there\nusually exist conflicts among the collected multi-source information. To tackle\nthis challenge, truth discovery, which integrates multi-source noisy\ninformation by estimating the reliability of each source, has emerged as a hot\ntopic. Several truth discovery methods have been proposed for various\nscenarios, and they have been successfully applied in diverse application\ndomains. In this survey, we focus on providing a comprehensive overview of\ntruth discovery methods, and summarizing them from different aspects. We also\ndiscuss some future directions of truth discovery research. We hope that this\nsurvey will promote a better understanding of the current progress on truth\ndiscovery, and offer some guidelines on how to apply these approaches in\napplication domains.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 01:22:30 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 05:07:09 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Li", "Yaliang", ""], ["Gao", "Jing", ""], ["Meng", "Chuishi", ""], ["Li", "Qi", ""], ["Su", "Lu", ""], ["Zhao", "Bo", ""], ["Fan", "Wei", ""], ["Han", "Jiawei", ""]]}, {"id": "1505.02681", "submitter": "Chih-Ya Shen", "authors": "Chih-Ya Shen, De-Nian Yang, Liang-Hao Huang, Wang-Chien Lee, and\n  Ming-Syan Chen", "title": "Socio-Spatial Group Queries for Impromptu Activity Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development and integration of social networking services and smartphones\nhave made it easy for individuals to organize impromptu social activities\nanywhere and anytime. Main challenges arising in organizing impromptu\nactivities are mostly due to the requirements of making timely invitations in\naccordance with the potential activity locations, corresponding to the\nlocations of and the relationship among the candidate attendees. Various\ncombinations of candidate attendees and activity locations create a large\nsolution space. Thus, in this paper, we propose Multiple Rally-Point Social\nSpatial Group Query (MRGQ), to select an appropriate activity location for a\ngroup of nearby attendees with tight social relationships. Although MRGQ is\nNP-hard, the number of attendees in practice is usually small enough such that\nan optimal solution can be found efficiently. Therefore, we first propose an\nInteger Linear Programming optimization model for MRGQ. We then design an\nefficient algorithm, called MAGS, which employs effective search space\nexploration and pruning strategies to reduce the running time for finding the\noptimal solution. We also propose to further optimize efficiency by indexing\nthe potential activity locations. A user study demonstrates the strength of\nusing MAGS over manual coordination in terms of both solution quality and\nefficiency. Experimental results on real datasets show that our algorithms can\nprocess MRGQ efficiently and significantly outperform other baseline\nalgorithms, including one based on the commercial parallel optimizer IBM CPLEX.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 15:58:31 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 10:35:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Shen", "Chih-Ya", ""], ["Yang", "De-Nian", ""], ["Huang", "Liang-Hao", ""], ["Lee", "Wang-Chien", ""], ["Chen", "Ming-Syan", ""]]}, {"id": "1505.02728", "submitter": "Razen Al-Harbi", "authors": "Razen Harbi, Ibrahim Abdelaziz, Panos Kalnis, Nikos Mamoulis, Yasser\n  Ebrahim, Majed Sahli", "title": "Adaptive Partitioning for Very Large RDF Data", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed RDF systems partition data across multiple computer nodes\n(workers). Some systems perform cheap hash partitioning, which may result in\nexpensive query evaluation, while others apply heuristics aiming at minimizing\ninter-node communication during query evaluation. This requires an expensive\ndata preprocessing phase, leading to high startup costs for very large RDF\nknowledge bases. Apriori knowledge of the query workload has also been used to\ncreate partitions, which however are static and do not adapt to workload\nchanges; hence, inter-node communication cannot be consistently avoided for\nqueries that are not favored by the initial data partitioning.\n  In this paper, we propose AdHash, a distributed RDF system, which addresses\nthe shortcomings of previous work. First, AdHash applies lightweight\npartitioning on the initial data, that distributes triples by hashing on their\nsubjects; this renders its startup overhead low. At the same time, the\nlocality-aware query optimizer of AdHash takes full advantage of the\npartitioning to (i)support the fully parallel processing of join patterns on\nsubjects and (ii) minimize data communication for general queries by applying\nhash distribution of intermediate results instead of broadcasting, wherever\npossible. Second, AdHash monitors the data access patterns and dynamically\nredistributes and replicates the instances of the most frequent ones among\nworkers. As a result, the communication cost for future queries is drastically\nreduced or even eliminated. To control replication, AdHash implements an\neviction policy for the redistributed patterns. Our experiments with synthetic\nand real data verify that AdHash (i) starts faster than all existing systems,\n(ii) processes thousands of queries before other systems become online, and\n(iii) gracefully adapts to the query load, being able to evaluate queries on\nbillion-scale RDF data in sub-seconds.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 18:53:22 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Harbi", "Razen", ""], ["Abdelaziz", "Ibrahim", ""], ["Kalnis", "Panos", ""], ["Mamoulis", "Nikos", ""], ["Ebrahim", "Yasser", ""], ["Sahli", "Majed", ""]]}, {"id": "1505.02891", "submitter": "Abd Elrahman Shafei", "authors": "Abdelrahman Elsayed, Hoda M. O. Mokhtar, Osama Ismail", "title": "Ontology Based Document Clustering Using MapReduce", "comments": "12 page", "journal-ref": "The International Journal of Database Management Systems (IJDMS),\n  April 2015, Volume 7, Number 2", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, document clustering is considered as a data intensive task due to\nthe dramatic, fast increase in the number of available documents. Nevertheless,\nthe features that represent those documents are also too large. The most common\nmethod for representing documents is the vector space model, which represents\ndocument features as a bag of words and does not represent semantic relations\nbetween words. In this paper we introduce a distributed implementation for the\nbisecting k-means using MapReduce programming model. The aim behind our\nproposed implementation is to solve the problem of clustering intensive data\ndocuments. In addition, we propose integrating the WordNet ontology with\nbisecting k-means in order to utilize the semantic relations between words to\nenhance document clustering results. Our presented experimental results show\nthat using lexical categories for nouns only enhances internal evaluation\nmeasures of document clustering; and decreases the documents features from\nthousands to tens features. Our experiments were conducted using Amazon Elastic\nMapReduce to deploy the Bisecting k-means algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 07:41:43 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Elsayed", "Abdelrahman", ""], ["Mokhtar", "Hoda M. O.", ""], ["Ismail", "Osama", ""]]}, {"id": "1505.03246", "submitter": "Su-Cheng Haw H", "authors": "Kok-Leong Koong, Su-Cheng Haw, Lay-Ki Soon, and Samini Subramaniam", "title": "Prefix-based Labeling Annotation for Effective XML Fragmentation", "comments": "12 pages, invited extension from conference paper. International\n  Journal of Computer Science & Information Technology (IJCSIT), Vol 7, No 2,\n  April 2015", "journal-ref": null, "doi": "10.5121/ijcsit.2015.7209", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML is gradually employed as a standard of data exchange in web environment\nsince its inception in the 90s until present. It serves as a data exchange\nbetween systems and other applications. Meanwhile the data volume has grown\nsubstantially in the web and thus effective methods of storing and retrieving\nthese data is essential. One recommended way is physically or virtually\nfragments the large chunk of data and distributes the fragments into different\nnodes. Fragmentation design of XML document contains of two parts:\nfragmentation operation and fragmentation method. The three fragmentation\noperations are Horizontal, Vertical and Hybrid. It determines how the XML\nshould be fragmented. This paper aims to give an overview on the fragmentation\ndesign consideration and subsequently, propose a fragmentation technique using\nnumber addressing.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 05:07:33 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Koong", "Kok-Leong", ""], ["Haw", "Su-Cheng", ""], ["Soon", "Lay-Ki", ""], ["Subramaniam", "Samini", ""]]}, {"id": "1505.04033", "submitter": "M\\'arta Czenky", "authors": "M\\'arta Czenky", "title": "An Examination of the Effectiveness of Teaching Data Modelling Concepts", "comments": null, "journal-ref": "International Journal of Database Management Systems ( IJDMS )\n  Vol.7, No.2, April 2015", "doi": "10.5121/ijdms.2015.7202", "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective teaching of data modelling concepts is very important; it\nconstitutes the fundament of database planning methods and the handling of\ndatabases with the help of database management lan-guages, typically SQL. We\nexamined three courses. The students of two courses prepared for the exam by\nsolving tests, while the students of the third course prepared by solving tasks\nfrom a printed exercise book. The number of task for the second course was 2.5\ntimes more than the number of task for the first course. The main purpose of\nour examination was to determine the effectiveness of the teaching of data\nmodelling concepts, and to decide if there is a significant difference between\nthe results of the three courses. According to our examination, with increasing\nthe number of test tasks and with the use of exercise book, the results became\nsignificantly better.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 11:20:45 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Czenky", "M\u00e1rta", ""]]}, {"id": "1505.04094", "submitter": "Yang Yang", "authors": "Yang Yang and Ryan N. Lichtenwalter and Nitesh V. Chawla", "title": "Evaluating Link Prediction Methods", "comments": null, "journal-ref": null, "doi": "10.1007/s10115-014-0789-0", "report-no": null, "categories": "cs.IR cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction is a popular research area with important applications in a\nvariety of disciplines, including biology, social science, security, and\nmedicine. The fundamental requirement of link prediction is the accurate and\neffective prediction of new links in networks. While there are many different\nmethods proposed for link prediction, we argue that the practical performance\npotential of these methods is often unknown because of challenges in the\nevaluation of link prediction, which impact the reliability and reproducibility\nof results. We describe these challenges, provide theoretical proofs and\nempirical examples demonstrating how current methods lead to questionable\nconclusions, show how the fallacy of these conclusions is illuminated by\nmethods we propose, and develop recommendations for consistent, standard, and\napplicable evaluation metrics. We also recommend the use of precision-recall\nthreshold curves and associated areas in lieu of receiver operating\ncharacteristic curves due to complications that arise from extreme imbalance in\nthe link prediction classification problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 15:28:58 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Yang", "Yang", ""], ["Lichtenwalter", "Ryan N.", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1505.04216", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli and Michael Benedikt", "title": "Finite Open-World Query Answering with Number Restrictions (Extended\n  Version)", "comments": "59 pages. To appear in LICS 2015. Extended version including proofs", "journal-ref": null, "doi": "10.1109/LICS.2015.37", "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-world query answering is the problem of deciding, given a set of facts,\nconjunction of constraints, and query, whether the facts and constraints imply\nthe query. This amounts to reasoning over all instances that include the facts\nand satisfy the constraints. We study finite open-world query answering (FQA),\nwhich assumes that the underlying world is finite and thus only considers the\nfinite completions of the instance. The major known decidable cases of FQA\nderive from the following: the guarded fragment of first-order logic, which can\nexpress referential constraints (data in one place points to data in another)\nbut cannot express number restrictions such as functional dependencies; and the\nguarded fragment with number restrictions but on a signature of arity only two.\nIn this paper, we give the first decidability results for FQA that combine both\nreferential constraints and number restrictions for arbitrary signatures: we\nshow that, for unary inclusion dependencies and functional dependencies, the\nfiniteness assumption of FQA can be lifted up to taking the finite implication\nclosure of the dependencies. Our result relies on new techniques to construct\nfinite universal models of such constraints, for any bound on the maximal query\nsize.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 22:56:35 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Amarilli", "Antoine", ""], ["Benedikt", "Michael", ""]]}, {"id": "1505.04746", "submitter": "Nasser Ghadiri", "authors": "Somaye Davari and Nasser Ghadiri", "title": "Spatial database implementation of fuzzy region connection calculus for\n  analysing the relationship of diseases", "comments": "ICEE2015", "journal-ref": null, "doi": "10.1109/IranianCEE.2015.7146310", "report-no": null, "categories": "cs.DB cs.AI cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing huge amounts of spatial data plays an important role in many\nemerging analysis and decision-making domains such as healthcare, urban\nplanning, agriculture and so on. For extracting meaningful knowledge from\ngeographical data, the relationships between spatial data objects need to be\nanalyzed. An important class of such relationships are topological relations\nlike the connectedness or overlap between regions. While real-world\ngeographical regions such as lakes or forests do not have exact boundaries and\nare fuzzy, most of the existing analysis methods neglect this inherent feature\nof topological relations. In this paper, we propose a method for handling the\ntopological relations in spatial databases based on fuzzy region connection\ncalculus (RCC). The proposed method is implemented in PostGIS spatial database\nand evaluated in analyzing the relationship of diseases as an important\napplication domain. We also used our fuzzy RCC implementation for fuzzification\nof the skyline operator in spatial databases. The results of the evaluation\nshow that our method provides a more realistic view of spatial relationships\nand gives more flexibility to the data analyst to extract meaningful and\naccurate results in comparison with the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 18:13:29 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 17:45:03 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Davari", "Somaye", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1505.04880", "submitter": "Nasser Ghadiri", "authors": "Amin Beiranvand and Nasser Ghadiri", "title": "ADQUEX: Adaptive Processing of Federated Queries over Linked Data based\n  on Tuple Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the distribution of linked data across the web, the methods that\nprocess federated queries through a distributed approach are more attractive to\nthe users and have gained more prosperity. In distributed processing of\nfederated queries, we need methods and procedures to execute the query in an\noptimal manner. Most of the existing methods perform the optimization task\nbased on some statistical information, whereas the query processor does not\nhave precise statistical information about their properties, since the data\nsources are autonomous. When precise statistics are not available, the\npossibility of wrong estimations will highly increase, and may lead to\ninefficient execution of the query at runtime. Another problem of the existing\nmethods is that in the optimization phase, they assume that runtime conditions\nof query execution are stable, while the environment in which federated queries\nare executed over linked data is dynamic and non-predictable. By considering\nthese two problems, there is a great potential for exploiting the federated\nquery processing techniques in an adaptive manner. In this paper, an adaptive\nmethod is proposed for processing federated queries over linked data, based on\nthe concept of routing the tuples. The proposed method, named ADQUEX, is able\nto execute the query effectively without any prior statistical information.\nThis method can change the query execution plan at runtime so that less\nintermediate results are produced. It can also adapt the execution plan to new\nsituation if unpredicted network latencies arise. Extensive evaluation of our\nmethod by running real queries over well-known linked datasets shows very good\nresults especially for complex queries.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 05:51:27 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Beiranvand", "Amin", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1505.04972", "submitter": "Arthur Ryman", "authors": "Arthur Ryman", "title": "Recursion in RDF Data Shape Languages", "comments": "31 pages, 2 figures, invited expert contribution to the W3C RDF Data\n  Shapes Working Group", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An RDF data shape is a description of the expected contents of an RDF\ndocument (aka graph) or dataset. A major part of this description is the set of\nconstraints that the document or dataset is required to satisfy. W3C recently\n(2014) chartered the RDF Data Shapes Working Group to define SHACL, a standard\nRDF data shape language. We refer to the ability to name and reference shape\nlanguage elements as recursion. This article provides a precise definition of\nthe meaning of recursion as used in Resource Shape 2.0. The definition of\nrecursion presented in this article is largely independent of language-specific\ndetails. We speculate that it also applies to ShEx and to all three of the\ncurrent proposals for SHACL. In particular, recursion is not permitted in the\nSHACL-SPARQL proposal, but we conjecture that recursion could be added by using\nthe definition proposed here as a top-level control structure.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 12:45:59 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 22:27:03 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Ryman", "Arthur", ""]]}, {"id": "1505.05136", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne", "title": "A Table-Binning Approach for Visualizing the Past", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amounts of data are available due to low-cost and high-capacity data\nstorage equipments. We propose a data exploration/visualization method for\ntabular multi-dimensional, time-varying datasets to present selected items in\ntheir global context. The approach is simple and uses a rank-based\nvisualization and a pattern matching functionality based on temporal profiles.\nRanking categories can be specified in a flexible way and are used instead of\nactual values (value reduction into bins) and plotting it over time in an\nunevenly quantized representation. Patterns that emerge are matched against a\nset of eight predefined temporal profiles. The graphical summarization of\nlarge-scale temporal data is proposed and applicability is tested qualitatively\non about eight data sets and the approach is compared to classic line plots and\nSAX representation\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 11:15:39 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Turenne", "Nicolas", ""]]}, {"id": "1505.05211", "submitter": "Souvik Bhattacherjee", "authors": "Souvik Bhattacherjee and Amit Chavan and Silu Huang and Amol Deshpande\n  and Aditya Parameswaran", "title": "Principles of Dataset Versioning: Exploring the Recreation/Storage\n  Tradeoff", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relative ease of collaborative data science and analysis has led to a\nproliferation of many thousands or millions of $versions$ of the same datasets\nin many scientific and commercial domains, acquired or constructed at various\nstages of data analysis across many users, and often over long periods of time.\nManaging, storing, and recreating these dataset versions is a non-trivial task.\nThe fundamental challenge here is the $storage-recreation\\;trade-off$: the more\nstorage we use, the faster it is to recreate or retrieve versions, while the\nless storage we use, the slower it is to recreate or retrieve versions. Despite\nthe fundamental nature of this problem, there has been a surprisingly little\namount of work on it. In this paper, we study this trade-off in a principled\nmanner: we formulate six problems under various settings, trading off these\nquantities in various ways, demonstrate that most of the problems are\nintractable, and propose a suite of inexpensive heuristics drawing from\ntechniques in delay-constrained scheduling, and spanning tree literature, to\nsolve these problems. We have built a prototype version management system, that\naims to serve as a foundation to our DATAHUB system for facilitating\ncollaborative data science. We demonstrate, via extensive experiments, that our\nproposed heuristics provide efficient solutions in practical dataset versioning\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 23:45:05 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Bhattacherjee", "Souvik", ""], ["Chavan", "Amit", ""], ["Huang", "Silu", ""], ["Deshpande", "Amol", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1505.05956", "submitter": "Xin Huang", "authors": "Xin Huang, Laks V.S. Lakshmanan, Jeffrey Xu Yu, Hong Cheng", "title": "Approximate Closest Community Search in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been significant interest in the study of the community\nsearch problem in social and information networks: given one or more query\nnodes, find densely connected communities containing the query nodes. However,\nmost existing studies do not address the \"free rider\" issue, that is, nodes far\naway from query nodes and irrelevant to them are included in the detected\ncommunity. Some state-of-the-art models have attempted to address this issue,\nbut not only are their formulated problems NP-hard, they do not admit any\napproximations without restrictive assumptions, which may not always hold in\npractice.\n  In this paper, given an undirected graph G and a set of query nodes Q, we\nstudy community search using the k-truss based community model. We formulate\nour problem of finding a closest truss community (CTC), as finding a connected\nk-truss subgraph with the largest k that contains Q, and has the minimum\ndiameter among such subgraphs. We prove this problem is NP-hard. Furthermore,\nit is NP-hard to approximate the problem within a factor $(2-\\varepsilon)$, for\nany $\\varepsilon >0 $. However, we develop a greedy algorithmic framework,\nwhich first finds a CTC containing Q, and then iteratively removes the furthest\nnodes from Q, from the graph. The method achieves 2-approximation to the\noptimal solution. To further improve the efficiency, we make use of a compact\ntruss index and develop efficient algorithms for k-truss identification and\nmaintenance as nodes get eliminated. In addition, using bulk deletion\noptimization and local exploration strategies, we propose two more efficient\nalgorithms. One of them trades some approximation quality for efficiency while\nthe other is a very efficient heuristic. Extensive experiments on 6 real-world\nnetworks show the effectiveness and efficiency of our community model and\nsearch algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 05:55:55 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2015 23:19:02 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Huang", "Xin", ""], ["Lakshmanan", "Laks V. S.", ""], ["Yu", "Jeffrey Xu", ""], ["Cheng", "Hong", ""]]}, {"id": "1505.06872", "submitter": "Wanling Gao", "authors": "Wanling Gao, Chunjie Luo, Jianfeng Zhan, Hainan Ye, Xiwen He, Lei\n  Wang, Yuqing Zhu and Xinhui Tian", "title": "Identifying Dwarfs Workloads in Big Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data benchmarking is particularly important and provides applicable\nyardsticks for evaluating booming big data systems. However, wide coverage and\ngreat complexity of big data computing impose big challenges on big data\nbenchmarking. How can we construct a benchmark suite using a minimum set of\nunits of computation to represent diversity of big data analytics workloads?\nBig data dwarfs are abstractions of extracting frequently appearing operations\nin big data computing. One dwarf represents one unit of computation, and big\ndata workloads are decomposed into one or more dwarfs. Furthermore, dwarfs\nworkloads rather than vast real workloads are more cost-efficient and\nrepresentative to evaluate big data systems. In this paper, we extensively\ninvestigate six most important or emerging application domains i.e. search\nengine, social network, e-commerce, multimedia, bioinformatics and astronomy.\nAfter analyzing forty representative algorithms, we single out eight dwarfs\nworkloads in big data analytics other than OLAP, which are linear algebra,\nsampling, logic operations, transform operations, set operations, graph\noperations, statistic operations and sort.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 09:38:08 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Gao", "Wanling", ""], ["Luo", "Chunjie", ""], ["Zhan", "Jianfeng", ""], ["Ye", "Hainan", ""], ["He", "Xiwen", ""], ["Wang", "Lei", ""], ["Zhu", "Yuqing", ""], ["Tian", "Xinhui", ""]]}, {"id": "1505.07130", "submitter": "Kemele M. Endris", "authors": "Kemele M. Endris, Sidra Faisal, Fabrizio Orlandi, S\\\"oren Auer, Simon\n  Scerri", "title": "Interest-based RDF Update Propagation", "comments": "16 pages, Keywords: Change Propagation, Dataset Dynamics, Linked\n  Data, Replication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many LOD datasets, such as DBpedia and LinkedGeoData, are voluminous and\nprocess large amounts of requests from diverse applications. Many data products\nand services rely on full or partial local LOD replications to ensure faster\nquerying and processing. While such replicas enhance the flexibility of\ninformation sharing and integration infrastructures, they also introduce data\nduplication with all the associated undesirable consequences. Given the\nevolving nature of the original and authoritative datasets, to ensure\nconsistent and up-to-date replicas frequent replacements are required at a\ngreat cost. In this paper, we introduce an approach for interest-based RDF\nupdate propagation, which propagates only interesting parts of updates from the\nsource to the target dataset. Effectively, this enables remote applications to\n`subscribe' to relevant datasets and consistently reflect the necessary changes\nlocally without the need to frequently replace the entire dataset (or a\nrelevant subset). Our approach is based on a formal definition for\ngraph-pattern-based interest expressions that is used to filter interesting\nparts of updates from the source. We implement the approach in the iRap\nframework and perform a comprehensive evaluation based on DBpedia Live updates,\nto confirm the validity and value of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 20:36:42 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Endris", "Kemele M.", ""], ["Faisal", "Sidra", ""], ["Orlandi", "Fabrizio", ""], ["Auer", "S\u00f6ren", ""], ["Scerri", "Simon", ""]]}]