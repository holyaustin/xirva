[{"id": "1208.0055", "submitter": "Sutanay Choudhury Sutanay Choudhury", "authors": "Sutanay Choudhury, Lawrence Holder, George Chin, John Feo", "title": "Large-scale continuous subgraph queries on streams", "comments": null, "journal-ref": "In Proceedings of the first annual workshop on High performance\n  computing meets databases (HPCDB 2011). ACM, New York, NY, USA, 29-32", "doi": "10.1145/2125636.2125647", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Graph pattern matching involves finding exact or approximate matches for a\nquery subgraph in a larger graph. It has been studied extensively and has\nstrong applications in domains such as computer vision, computational biology,\nsocial networks, security and finance. The problem of exact graph pattern\nmatching is often described in terms of subgraph isomorphism which is\nNP-complete. The exponential growth in streaming data from online social\nnetworks, news and video streams and the continual need for situational\nawareness motivates a solution for finding patterns in streaming updates. This\nis also the prime driver for the real-time analytics market. Development of\nincremental algorithms for graph pattern matching on streaming inputs to a\ncontinually evolving graph is a nascent area of research. Some of the\nchallenges associated with this problem are the same as found in continuous\nquery (CQ) evaluation on streaming databases. This paper reviews some of the\nrepresentative work from the exhaustively researched field of CQ systems and\nidentifies important semantics, constraints and architectural features that are\nalso appropriate for HPC systems performing real-time graph analytics. For each\nof these features we present a brief discussion of the challenge encountered in\nthe database realm, the approach to the solution and state their relevance in a\nhigh-performance, streaming graph processing framework.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2012 23:40:03 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Choudhury", "Sutanay", ""], ["Holder", "Lawrence", ""], ["Chin", "George", ""], ["Feo", "John", ""]]}, {"id": "1208.0073", "submitter": "Chin-Wan Chung", "authors": "Dong-Wan Choi, Chin-Wan Chung, Yufei Tao", "title": "A Scalable Algorithm for Maximizing Range Sum in Spatial Databases", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1088-1099 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the MaxRS problem in spatial databases. Given a set O\nof weighted points and a rectangular region r of a given size, the goal of the\nMaxRS problem is to find a location of r such that the sum of the weights of\nall the points covered by r is maximized. This problem is useful in many\nlocation-based applications such as finding the best place for a new franchise\nstore with a limited delivery range and finding the most attractive place for a\ntourist with a limited reachable range. However, the problem has been studied\nmainly in theory, particularly, in computational geometry. The existing\nalgorithms from the computational geometry community are in-memory algorithms\nwhich do not guarantee the scalability. In this paper, we propose a scalable\nexternal-memory algorithm (ExactMaxRS) for the MaxRS problem, which is optimal\nin terms of the I/O complexity. Furthermore, we propose an approximation\nalgorithm (ApproxMaxCRS) for the MaxCRS problem that is a circle version of the\nMaxRS problem. We prove the correctness and optimality of the ExactMaxRS\nalgorithm along with the approximation bound of the ApproxMaxCRS algorithm.\nFrom extensive experimental results, we show that the ExactMaxRS algorithm is\ntwo orders of magnitude faster than methods adapted from existing algorithms,\nand the approximation bound in practice is much better than the theoretical\nbound of the ApproxMaxCRS algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:42:19 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Choi", "Dong-Wan", ""], ["Chung", "Chin-Wan", ""], ["Tao", "Yufei", ""]]}, {"id": "1208.0074", "submitter": "Ahmed Aly", "authors": "Ahmed M. Aly, Walid G. Aref, Mourad Ouzzani", "title": "Spatial Queries with Two kNN Predicates", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1100-1111 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread use of location-aware devices has led to countless\nlocation-based services in which a user query can be arbitrarily complex, i.e.,\none that embeds multiple spatial selection and join predicates. Amongst these\npredicates, the k-Nearest-Neighbor (kNN) predicate stands as one of the most\nimportant and widely used predicates. Unlike related research, this paper goes\nbeyond the optimization of queries with single kNN predicates, and shows how\nqueries with two kNN predicates can be optimized. In particular, the paper\naddresses the optimization of queries with: (i) two kNN-select predicates, (ii)\ntwo kNN-join predicates, and (iii) one kNN-join predicate and one kNN-select\npredicate. For each type of queries, conceptually correct query evaluation\nplans (QEPs) and new algorithms that optimize the query execution time are\npresented. Experimental results demonstrate that the proposed algorithms\noutperform the conceptually correct QEPs by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:43:08 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Aly", "Ahmed M.", ""], ["Aref", "Walid G.", ""], ["Ouzzani", "Mourad", ""]]}, {"id": "1208.0075", "submitter": "Yufei Tao", "authors": "Cheng Sheng, Nan Zhang, Yufei Tao, Xin Jin", "title": "Optimal Algorithms for Crawling a Hidden Database in the Web", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1112-1123 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hidden database refers to a dataset that an organization makes accessible\non the web by allowing users to issue queries through a search interface. In\nother words, data acquisition from such a source is not by following static\nhyper-links. Instead, data are obtained by querying the interface, and reading\nthe result page dynamically generated. This, with other facts such as the\ninterface may answer a query only partially, has prevented hidden databases\nfrom being crawled effectively by existing search engines. This paper remedies\nthe problem by giving algorithms to extract all the tuples from a hidden\ndatabase. Our algorithms are provably efficient, namely, they accomplish the\ntask by performing only a small number of queries, even in the worst case. We\nalso establish theoretical results indicating that these algorithms are\nasymptotically optimal -- i.e., it is impossible to improve their efficiency by\nmore than a constant factor. The derivation of our upper and lower bound\nresults reveals significant insight into the characteristics of the underlying\nproblem. Extensive experiments confirm the proposed techniques work very well\non all the real datasets examined.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:43:52 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Sheng", "Cheng", ""], ["Zhang", "Nan", ""], ["Tao", "Yufei", ""], ["Jin", "Xin", ""]]}, {"id": "1208.0076", "submitter": "Lu Qin", "authors": "Lu Qin, Jeffrey Xu Yu, Lijun Chang", "title": "Diversifying Top-K Results", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1124-1135 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k query processing finds a list of k results that have largest scores\nw.r.t the user given query, with the assumption that all the k results are\nindependent to each other. In practice, some of the top-k results returned can\nbe very similar to each other. As a result some of the top-k results returned\nare redundant. In the literature, diversified top-k search has been studied to\nreturn k results that take both score and diversity into consideration. Most\nexisting solutions on diversified top-k search assume that scores of all the\nsearch results are given, and some works solve the diversity problem on a\nspecific problem and can hardly be extended to general cases. In this paper, we\nstudy the diversified top-k search problem. We define a general diversified\ntop-k search problem that only considers the similarity of the search results\nthemselves. We propose a framework, such that most existing solutions for top-k\nquery processing can be extended easily to handle diversified top-k search, by\nsimply applying three new functions, a sufficient stop condition sufficient(),\na necessary stop condition necessary(), and an algorithm for diversified top-k\nsearch on the current set of generated results, div-search-current(). We\npropose three new algorithms, namely, div-astar, div-dp, and div-cut to solve\nthe div-search-current() problem. div-astar is an A* based algorithm, div-dp is\nan algorithm that decomposes the results into components which are searched\nusing div-astar independently and combined using dynamic programming. div-cut\nfurther decomposes the current set of generated results using cut points and\ncombines the results using sophisticated operations. We conducted extensive\nperformance studies using two real datasets, enwiki and reuters. Our div-cut\nalgorithm finds the optimal solution for diversified top-k search problem in\nseconds even for k as large as 2,000.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:44:46 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Qin", "Lu", ""], ["Yu", "Jeffrey Xu", ""], ["Chang", "Lijun", ""]]}, {"id": "1208.0077", "submitter": "Xin Cao", "authors": "Xin Cao, Lisi Chen, Gao Cong, Xiaokui Xiao", "title": "Keyword-aware Optimal Route Search", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1136-1147 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying a preferable route is an important problem that finds\napplications in map services. When a user plans a trip within a city, the user\nmay want to find \"a most popular route such that it passes by shopping mall,\nrestaurant, and pub, and the travel time to and from his hotel is within 4\nhours.\" However, none of the algorithms in the existing work on route planning\ncan be used to answer such queries. Motivated by this, we define the problem of\nkeyword-aware optimal route query, denoted by KOR, which is to find an optimal\nroute such that it covers a set of user-specified keywords, a specified budget\nconstraint is satisfied, and an objective score of the route is optimal. The\nproblem of answering KOR queries is NP-hard. We devise an approximation\nalgorithm OSScaling with provable approximation bounds. Based on this\nalgorithm, another more efficient approximation algorithm BucketBound is\nproposed. We also design a greedy approximation algorithm. Results of empirical\nstudies show that all the proposed algorithms are capable of answering KOR\nqueries efficiently, while the BucketBound and Greedy algorithms run faster.\nThe empirical studies also offer insight into the accuracy of the proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:45:38 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Cao", "Xin", ""], ["Chen", "Lisi", ""], ["Cong", "Gao", ""], ["Xiao", "Xiaokui", ""]]}, {"id": "1208.0078", "submitter": "Bogdan Cautis", "authors": "Bogdan Cautis, Evgeny Kharlamov", "title": "Answering Queries using Views over Probabilistic XML: Complexity and\n  Tractability", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1148-1159 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of query answering using views in a probabilistic XML\nsetting, identifying large classes of XPath queries -- with child and\ndescendant navigation and predicates -- for which there are efficient (PTime)\nalgorithms. We consider this problem under the two possible semantics for XML\nquery results: with persistent node identifiers and in their absence.\nAccordingly, we consider rewritings that can exploit a single view, by means of\ncompensation, and rewritings that can use multiple views, by means of\nintersection. Since in a probabilistic setting queries return answers with\nprobabilities, the problem of rewriting goes beyond the classic one of\nretrieving XML answers from views. For both semantics of XML queries, we show\nthat, even when XML answers can be retrieved from views, their probabilities\nmay not be computable. For rewritings that use only compensation, we describe a\nPTime decision procedure, based on easily verifiable criteria that distinguish\nbetween the feasible cases -- when probabilistic XML results are computable --\nand the unfeasible ones. For rewritings that can use multiple views, with\ncompensation and intersection, we identify the most permissive conditions that\nmake probabilistic rewriting feasible, and we describe an algorithm that is\nsound in general, and becomes complete under fairly permissive restrictions,\nrunning in PTime modulo worst-case exponential time equivalence tests. This is\nthe best we can hope for since intersection makes query equivalence intractable\nalready over deterministic data. Our algorithm runs in PTime whenever\ndeterministic rewritings can be found in PTime.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:46:21 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Cautis", "Bogdan", ""], ["Kharlamov", "Evgeny", ""]]}, {"id": "1208.0079", "submitter": "Abhay Jha", "authors": "Abhay Jha, Dan Suciu", "title": "Probabilistic Databases with MarkoViews", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1160-1171 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the work on query evaluation in probabilistic databases has focused\non the simple tuple-independent data model, where tuples are independent random\nevents. Several efficient query evaluation techniques exists in this setting,\nsuch as safe plans, algorithms based on OBDDs, tree-decomposition and a variety\nof approximation algorithms. However, complex data analytics tasks often\nrequire complex correlations, and query evaluation then is significantly more\nexpensive, or more restrictive. In this paper, we propose MVDB as a framework\nboth for representing complex correlations and for efficient query evaluation.\nAn MVDB specifies correlations by views, called MarkoViews, on the\nprobabilistic relations and declaring the weights of the view's outputs. An\nMVDB is a (very large) Markov Logic Network. We make two sets of contributions.\nFirst, we show that query evaluation on an MVDB is equivalent to evaluating a\nUnion of Conjunctive Query(UCQ) over a tuple-independent database. The\ntranslation is exact (thus allowing the techniques developed for tuple\nindependent databases to be carried over to MVDB), yet it is novel and quite\nnon-obvious (some resulting probabilities may be negative!). This translation\nin itself though may not lead to much gain since the translated query gets\ncomplicated as we try to capture more correlations. Our second contribution is\nto propose a new query evaluation strategy that exploits offline compilation to\nspeed up online query evaluation. Here we utilize and extend our prior work on\ncompilation of UCQ. We validate experimentally our techniques on a large\nprobabilistic database with MarkoViews inferred from the DBLP data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:47:10 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Jha", "Abhay", ""], ["Suciu", "Dan", ""]]}, {"id": "1208.0080", "submitter": "Sigal Oren", "authors": "Konstantinos Mamouras, Sigal Oren, Lior Seeman, Lucja Kot, Johannes\n  Gehrke", "title": "The Complexity of Social Coordination", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1172-1183 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordination is a challenging everyday task; just think of the last time you\norganized a party or a meeting involving several people. As a growing part of\nour social and professional life goes online, an opportunity for an improved\ncoordination process arises. Recently, Gupta et al. proposed entangled queries\nas a declarative abstraction for data-driven coordination, where the difficulty\nof the coordination task is shifted from the user to the database.\nUnfortunately, evaluating entangled queries is very hard, and thus previous\nwork considered only a restricted class of queries that satisfy safety (the\ncoordination partners are fixed) and uniqueness (all queries need to be\nsatisfied). In this paper we significantly extend the class of feasible\nentangled queries beyond uniqueness and safety. First, we show that we can\nsimply drop uniqueness and still efficiently evaluate a set of safe entangled\nqueries. Second, we show that as long as all users coordinate on the same set\nof attributes, we can give an efficient algorithm for coordination even if the\nset of queries does not satisfy safety. In an experimental evaluation we show\nthat our algorithms are feasible for a wide spectrum of coordination scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:47:57 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Mamouras", "Konstantinos", ""], ["Oren", "Sigal", ""], ["Seeman", "Lior", ""], ["Kot", "Lucja", ""], ["Gehrke", "Johannes", ""]]}, {"id": "1208.0081", "submitter": "Xiaofei Zhang", "authors": "Xiaofei Zhang, Lei Chen, Min Wang", "title": "Efficient Multi-way Theta-Join Processing Using MapReduce", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1184-1195 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-way Theta-join queries are powerful in describing complex relations and\ntherefore widely employed in real practices. However, existing solutions from\ntraditional distributed and parallel databases for multi-way Theta-join queries\ncannot be easily extended to fit a shared-nothing distributed computing\nparadigm, which is proven to be able to support OLAP applications over immense\ndata volumes. In this work, we study the problem of efficient processing of\nmulti-way Theta-join queries using MapReduce from a cost-effective perspective.\nAlthough there have been some works using the (key,value) pair-based\nprogramming model to support join operations, efficient processing of multi-way\nTheta-join queries has never been fully explored. The substantial challenge\nlies in, given a number of processing units (that can run Map or Reduce tasks),\nmapping a multi-way Theta-join query to a number of MapReduce jobs and having\nthem executed in a well scheduled sequence, such that the total processing time\nspan is minimized. Our solution mainly includes two parts: 1) cost metrics for\nboth single MapReduce job and a number of MapReduce jobs executed in a certain\norder; 2) the efficient execution of a chain-typed Theta-join with only one\nMapReduce job. Comparing with the query evaluation strategy proposed in [23]\nand the widely adopted Pig Latin and Hive SQL solutions, our method achieves\nsignificant improvement of the join processing efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:48:44 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Zhang", "Xiaofei", ""], ["Chen", "Lei", ""], ["Wang", "Min", ""]]}, {"id": "1208.0082", "submitter": "Harold Lim", "authors": "Harold Lim, Herodotos Herodotou, Shivnath Babu", "title": "Stubby: A Transformation-based Optimizer for MapReduce Workflows", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1196-1207 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing trend of performing analysis on large datasets using\nworkflows composed of MapReduce jobs connected through producer-consumer\nrelationships based on data. This trend has spurred the development of a number\nof interfaces--ranging from program-based to query-based interfaces--for\ngenerating MapReduce workflows. Studies have shown that the gap in performance\ncan be quite large between optimized and unoptimized workflows. However,\nautomatic cost-based optimization of MapReduce workflows remains a challenge\ndue to the multitude of interfaces, large size of the execution plan space, and\nthe frequent unavailability of all types of information needed for\noptimization. We introduce a comprehensive plan space for MapReduce workflows\ngenerated by popular workflow generators. We then propose Stubby, a cost-based\noptimizer that searches selectively through the subspace of the full plan space\nthat can be enumerated correctly and costed based on the information available\nin any given setting. Stubby enumerates the plan space based on plan-to-plan\ntransformations and an efficient search algorithm. Stubby is designed to be\nextensible to new interfaces and new types of optimizations, which is a\ndesirable feature given how rapidly MapReduce systems are evolving. Stubby's\nefficiency and effectiveness have been evaluated using representative workflows\nfrom many domains.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:49:32 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Lim", "Harold", ""], ["Herodotou", "Herodotos", ""], ["Babu", "Shivnath", ""]]}, {"id": "1208.0083", "submitter": "Zhuowei Bao", "authors": "Zhuowei Bao, Susan B. Davidson, Tova Milo", "title": "Labeling Workflow Views with Fine-Grained Dependencies", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1208-1219 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of efficiently answering reachability\nqueries over views of provenance graphs, derived from executions of workflows\nthat may include recursion. Such views include composite modules and model\nfine-grained dependencies between module inputs and outputs. A novel\nview-adaptive dynamic labeling scheme is developed for efficient query\nevaluation, in which view specifications are labeled statically (i.e. as they\nare created) and data items are labeled dynamically as they are produced during\na workflow execution. Although the combination of fine-grained dependencies and\nrecursive workflows entail, in general, long (linear-size) data labels, we show\nthat for a large natural class of workflows and views, labels are compact\n(logarithmic-size) and reachability queries can be evaluated in constant time.\nExperimental results demonstrate the benefit of this approach over the\nstate-of-the-art technique when applied for labeling multiple views.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:50:17 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Bao", "Zhuowei", ""], ["Davidson", "Susan B.", ""], ["Milo", "Tova", ""]]}, {"id": "1208.0084", "submitter": "Jaroslaw Szlichta", "authors": "Jaroslaw Szlichta, Parke Godfrey, Jarek Gryz", "title": "Fundamentals of Order Dependencies", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1220-1231 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependencies have played a significant role in database design for many\nyears. They have also been shown to be useful in query optimization. In this\npaper, we discuss dependencies between lexicographically ordered sets of\ntuples. We introduce formally the concept of order dependency and present a set\nof axioms (inference rules) for them. We show how query rewrites based on these\naxioms can be used for query optimization. We present several interesting\ntheorems that can be derived using the inference rules. We prove that\nfunctional dependencies are subsumed by order dependencies and that our set of\naxioms for order dependencies is sound and complete.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:51:05 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Szlichta", "Jaroslaw", ""], ["Godfrey", "Parke", ""], ["Gryz", "Jarek", ""]]}, {"id": "1208.0086", "submitter": "Yu Cao", "authors": "Yu Cao, Chee-Yong Chan, Jie Li, Kian-Lee Tan", "title": "Optimization of Analytic Window Functions", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1244-1255 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic functions represent the state-of-the-art way of performing complex\ndata analysis within a single SQL statement. In particular, an important class\nof analytic functions that has been frequently used in commercial systems to\nsupport OLAP and decision support applications is the class of window\nfunctions. A window function returns for each input tuple a value derived from\napplying a function over a window of neighboring tuples. However, existing\nwindow function evaluation approaches are based on a naive sorting scheme. In\nthis paper, we study the problem of optimizing the evaluation of window\nfunctions. We propose several efficient techniques, and identify optimization\nopportunities that allow us to optimize the evaluation of a set of window\nfunctions. We have integrated our scheme into PostgreSQL. Our comprehensive\nexperimental study on the TPC-DS datasets as well as synthetic datasets and\nqueries demonstrate significant speedup over existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:52:40 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Cao", "Yu", ""], ["Chan", "Chee-Yong", ""], ["Li", "Jie", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1208.0087", "submitter": "Fabian Hueske", "authors": "Fabian Hueske, Mathias Peters, Matthias Sax, Astrid Rheinl\\\"ander,\n  Rico Bergmann, Aljoscha Krettek, Kostas Tzoumas", "title": "Opening the Black Boxes in Data Flow Optimization", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1256-1267 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many systems for big data analytics employ a data flow abstraction to define\nparallel data processing tasks. In this setting, custom operations expressed as\nuser-defined functions are very common. We address the problem of performing\ndata flow optimization at this level of abstraction, where the semantics of\noperators are not known. Traditionally, query optimization is applied to\nqueries with known algebraic semantics. In this work, we find that a handful of\nproperties, rather than a full algebraic specification, suffice to establish\nreordering conditions for data processing operators. We show that these\nproperties can be accurately estimated for black box operators by statically\nanalyzing the general-purpose code of their user-defined functions. We design\nand implement an optimizer for parallel data flows that does not assume\nknowledge of semantics or algebraic properties of operators. Our evaluation\nconfirms that the optimizer can apply common rewritings such as selection\nreordering, bushy join-order enumeration, and limited forms of aggregation\npush-down, hence yielding similar rewriting power as modern relational DBMS\noptimizers. Moreover, it can optimize the operator order of non-relational data\nflows, a unique feature among today's systems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:53:27 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Hueske", "Fabian", ""], ["Peters", "Mathias", ""], ["Sax", "Matthias", ""], ["Rheinl\u00e4nder", "Astrid", ""], ["Bergmann", "Rico", ""], ["Krettek", "Aljoscha", ""], ["Tzoumas", "Kostas", ""]]}, {"id": "1208.0088", "submitter": "Stephan Ewen", "authors": "Stephan Ewen, Kostas Tzoumas, Moritz Kaufmann, Volker Markl", "title": "Spinning Fast Iterative Data Flows", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1268-1279 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel dataflow systems are a central part of most analytic pipelines for\nbig data. The iterative nature of many analysis and machine learning\nalgorithms, however, is still a challenge for current systems. While certain\ntypes of bulk iterative algorithms are supported by novel dataflow frameworks,\nthese systems cannot exploit computational dependencies present in many\nalgorithms, such as graph algorithms. As a result, these algorithms are\ninefficiently executed and have led to specialized systems based on other\nparadigms, such as message passing or shared memory. We propose a method to\nintegrate incremental iterations, a form of workset iterations, with parallel\ndataflows. After showing how to integrate bulk iterations into a dataflow\nsystem and its optimizer, we present an extension to the programming model for\nincremental iterations. The extension alleviates for the lack of mutable state\nin dataflows and allows for exploiting the sparse computational dependencies\ninherent in many iterative algorithms. The evaluation of a prototypical\nimplementation shows that those aspects lead to up to two orders of magnitude\nspeedup in algorithm runtime, when exploited. In our experiments, the improved\ndataflow system is highly competitive with specialized systems while\nmaintaining a transparent and unified dataflow abstraction.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:54:11 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Ewen", "Stephan", ""], ["Tzoumas", "Kostas", ""], ["Kaufmann", "Moritz", ""], ["Markl", "Volker", ""]]}, {"id": "1208.0089", "submitter": "Svilen Mihaylov", "authors": "Svilen R. Mihaylov, Zachary G. Ives, Sudipto Guha", "title": "REX: Recursive, Delta-Based Data-Centric Computation", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1280-1291 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's Web and social network environments, query workloads include ad\nhoc and OLAP queries, as well as iterative algorithms that analyze data\nrelationships (e.g., link analysis, clustering, learning). Modern DBMSs support\nad hoc and OLAP queries, but most are not robust enough to scale to large\nclusters. Conversely, \"cloud\" platforms like MapReduce execute chains of batch\ntasks across clusters in a fault tolerant way, but have too much overhead to\nsupport ad hoc queries.\n  Moreover, both classes of platform incur significant overhead in executing\niterative data analysis algorithms. Most such iterative algorithms repeatedly\nrefine portions of their answers, until some convergence criterion is reached.\nHowever, general cloud platforms typically must reprocess all data in each\nstep. DBMSs that support recursive SQL are more efficient in that they\npropagate only the changes in each step -- but they still accumulate each\niteration's state, even if it is no longer useful. User-defined functions are\nalso typically harder to write for DBMSs than for cloud platforms.\n  We seek to unify the strengths of both styles of platforms, with a focus on\nsupporting iterative computations in which changes, in the form of deltas, are\npropagated from iteration to iteration, and state is efficiently updated in an\nextensible way. We present a programming model oriented around deltas, describe\nhow we execute and optimize such programs in our REX runtime system, and\nvalidate that our platform also handles failures gracefully. We experimentally\nvalidate our techniques, and show speedups over the competing methods ranging\nfrom 2.5 to nearly 100 times.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:54:58 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Mihaylov", "Svilen R.", ""], ["Ives", "Zachary G.", ""], ["Guha", "Sudipto", ""]]}, {"id": "1208.0090", "submitter": "James Cheng", "authors": "James Cheng, Zechao Shang, Hong Cheng, Haixun Wang, Jeffrey Xu Yu", "title": "K-Reach: Who is in Your Small World", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1292-1303 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of answering k-hop reachability queries in a directed\ngraph, i.e., whether there exists a directed path of length k, from a source\nquery vertex to a target query vertex in the input graph. The problem of k-hop\nreachability is a general problem of the classic reachability (where\nk=infinity). Existing indexes for processing classic reachability queries, as\nwell as for processing shortest path queries, are not applicable or not\nefficient for processing k-hop reachability queries. We propose an index for\nprocessing k-hop reachability queries, which is simple in design and efficient\nto construct. Our experimental results on a wide range of real datasets show\nthat our index is more efficient than the state-of-the-art indexes even for\nprocessing classic reachability queries, for which these indexes are primarily\ndesigned. We also show that our index is efficient in answering k-hop\nreachability queries.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:55:46 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Cheng", "James", ""], ["Shang", "Zechao", ""], ["Cheng", "Hong", ""], ["Wang", "Haixun", ""], ["Yu", "Jeffrey Xu", ""]]}, {"id": "1208.0091", "submitter": "Yinghui Wu", "authors": "Wenfei Fan, Xin Wang, Yinghui Wu", "title": "Performance Guarantees for Distributed Reachability Queries", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1304-1315 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world a graph is often fragmented and distributed across\ndifferent sites. This highlights the need for evaluating queries on distributed\ngraphs. This paper proposes distributed evaluation algorithms for three classes\nof queries: reachability for determining whether one node can reach another,\nbounded reachability for deciding whether there exists a path of a bounded\nlength between a pair of nodes, and regular reachability for checking whether\nthere exists a path connecting two nodes such that the node labels on the path\nform a string in a given regular expression. We develop these algorithms based\non partial evaluation, to explore parallel computation. When evaluating a query\nQ on a distributed graph G, we show that these algorithms possess the following\nperformance guarantees, no matter how G is fragmented and distributed: (1) each\nsite is visited only once; (2) the total network traffic is determined by the\nsize of Q and the fragmentation of G, independent of the size of G; and (3) the\nresponse time is decided by the largest fragment of G rather than the entire G.\nIn addition, we show that these algorithms can be readily implemented in the\nMapReduce framework. Using synthetic and real-life data, we experimentally\nverify that these algorithms are scalable on large graphs, regardless of how\nthe graphs are distributed.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:56:31 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Fan", "Wenfei", ""], ["Wang", "Xin", ""], ["Wu", "Yinghui", ""]]}, {"id": "1208.0092", "submitter": "Pirooz Chubak", "authors": "Pirooz Chubak, Davood Rafiei", "title": "Efficient Indexing and Querying over Syntactically Annotated Trees", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1316-1327 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language text corpora are often available as sets of syntactically\nparsed trees. A wide range of expressive tree queries are possible over such\nparsed trees that open a new avenue in searching over natural language text.\nThey not only allow for querying roles and relationships within sentences, but\nalso improve search effectiveness compared to flat keyword queries. One major\ndrawback of current systems supporting querying over parsed text is the\nperformance of evaluating queries over large data. In this paper we propose a\nnovel indexing scheme over unique subtrees as index keys. We also propose a\nnovel root-split coding scheme that stores subtree structural information only\npartially, thus reducing index size and improving querying performance. Our\nextensive set of experiments show that root-split coding reduces the index size\nof any interval coding which stores individual node numbers by a factor of 50%\nto 80%, depending on the sizes of subtrees indexed. Moreover, We show that our\nindex using root-split coding, outperforms previous approaches by at least an\norder of magnitude in terms of the response time of queries.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:57:16 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Chubak", "Pirooz", ""], ["Rafiei", "Davood", ""]]}, {"id": "1208.0093", "submitter": "Wahbeh Qardaji", "authors": "Ninghui Li, Wahbeh Qardaji, Dong Su, Jianneng Cao", "title": "PrivBasis: Frequent Itemset Mining with Differential Privacy", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1340-1351 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of frequent itemsets can serve valuable economic and research\npurposes. Releasing discovered frequent itemsets, however, presents privacy\nchallenges. In this paper, we study the problem of how to perform frequent\nitemset mining on transaction databases while satisfying differential privacy.\nWe propose an approach, called PrivBasis, which leverages a novel notion called\nbasis sets. A theta-basis set has the property that any itemset with frequency\nhigher than theta is a subset of some basis. We introduce algorithms for\nprivately constructing a basis set and then using it to find the most frequent\nitemsets. Experiments show that our approach greatly outperforms the current\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:58:50 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Li", "Ninghui", ""], ["Qardaji", "Wahbeh", ""], ["Su", "Dong", ""], ["Cao", "Jianneng", ""]]}, {"id": "1208.0094", "submitter": "Zhenjie Zhang", "authors": "Ganzhao Yuan, Zhenjie Zhang, Marianne Winslett, Xiaokui Xiao, Yin\n  Yang, Zhifeng Hao", "title": "Low-Rank Mechanism: Optimizing Batch Queries under Differential Privacy", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1352-1363 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a promising privacy-preserving paradigm for\nstatistical query processing over sensitive data. It works by injecting random\nnoise into each query result, such that it is provably hard for the adversary\nto infer the presence or absence of any individual record from the published\nnoisy results. The main objective in differentially private query processing is\nto maximize the accuracy of the query results, while satisfying the privacy\nguarantees. Previous work, notably the matrix mechanism, has suggested that\nprocessing a batch of correlated queries as a whole can potentially achieve\nconsiderable accuracy gains, compared to answering them individually. However,\nas we point out in this paper, the matrix mechanism is mainly of theoretical\ninterest; in particular, several inherent problems in its design limit its\naccuracy in practice, which almost never exceeds that of naive methods. In\nfact, we are not aware of any existing solution that can effectively optimize a\nquery batch under differential privacy. Motivated by this, we propose the\nLow-Rank Mechanism (LRM), the first practical differentially private technique\nfor answering batch queries with high accuracy, based on a low rank\napproximation of the workload matrix. We prove that the accuracy provided by\nLRM is close to the theoretical lower bound for any mechanism to answer a batch\nof queries under differential privacy. Extensive experiments using real data\ndemonstrate that LRM consistently outperforms state-of-the-art query processing\nsolutions under differential privacy, by large margins.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 03:59:34 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Yuan", "Ganzhao", ""], ["Zhang", "Zhenjie", ""], ["Winslett", "Marianne", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Yin", ""], ["Hao", "Zhifeng", ""]]}, {"id": "1208.0153", "submitter": "Mohamed Salah  Gouider Dr", "authors": "Saida Aissi and Mohamed Salah Gouider", "title": "Personalization in Geographic information systems: A survey", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 4, No 3, July 2012 , pp 291-298", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geographic Information Systems (GIS) are widely used in different domains of\napplications, such as maritime navigation, museums visits and route planning,\nas well as ecological, demographical and economical applications. Nowadays,\norganizations need sophisticated and adapted GIS-based Decision Support System\n(DSS) to get quick access to relevant information and to analyze data with\nrespect to geographic information, represented not only as spatial objects, but\nalso as maps.\n  Several research works on GIS personalization was proposed: Face the great\nchallenge of developing both the theory and practice to provide personalization\nGIS visualization systems. This paper aims to provide a comprehensive review of\nliterature on presented GIS personalization approaches. A benchmarking study of\nGIS personalization methods is proposed. Several evaluation criteria are used\nto identify the existence of trends as well as potential needs for further\ninvestigations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 09:45:07 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Aissi", "Saida", ""], ["Gouider", "Mohamed Salah", ""]]}, {"id": "1208.0163", "submitter": "Mohamed Salah  Gouider Dr", "authors": "Saida Aissi and Mohamed Salah Gouider", "title": "Spatial and Spatio-Temporal Multidimensional Data Modelling: A Survey", "comments": null, "journal-ref": "International Journal of Advanced Research in Computer Science and\n  Software (IJARCCE), Volume 1, Issue 1, March 2012", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data warehouse store and provide access to large volume of historical data\nsupporting the strategic decisions of organisations. Data warehouse is based on\na multidimensional model which allow to express user's needs for supporting the\ndecision making process. Since it is estimated that 80% of data used for\ndecision making has a spatial or location component [1, 2], spatial data have\nbeen widely integrated in Data Warehouses and in OLAP systems. Extending a\nmultidimensional data model by the inclusion of spatial data provides a concise\nand organised spatial datawarehouse representation. This paper aims to provide\na comprehensive review of litterature on developed and suggested spatial and\nspatio-temporel multidimensional models. A benchmarking study of the proposed\nmodels is presented. Several evaluation criterias are used to identify the\nexistence of trends as well as potential needs for further investigations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 10:21:55 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Aissi", "Saida", ""], ["Gouider", "Mohamed Salah", ""]]}, {"id": "1208.0203", "submitter": "Mohamed Salah  Gouider Dr", "authors": "Saida Aissi and Mohamed Salah Gouider", "title": "Towards the Next Generation of Data Warehouse Personalization System: A\n  Survey and a Comparative Study", "comments": "8 pages", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol 9,\n  Issue 3, No 2, May 2012, pages 561-568", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional databases are a great asset for decision making. Their users\nexpress complex OLAP (On-Line Analytical Processing) queries, often returning\nhuge volumes of facts, sometimes providing little or no information.\nFurthermore, due to the huge volume of historical data stored in DWs, the OLAP\napplications may return a big amount of irrelevant information that could make\nthe data exploration process not efficient and tardy. OLAP personalization\nsystems play a major role in reducing the effort of decision-makers to find the\nmost interesting information. Several works dealing with OLAP personalization\nwere presented in the last few years. This paper aims to provide a\ncomprehensive review of literature on OLAP personalization approaches. A\nbenchmarking study of OLAP personalization methods is proposed. Several\nevaluation criteria are used to identify the existence of trends as well as\npotential needs for further investigations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 13:27:20 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Aissi", "Saida", ""], ["Gouider", "Mohamed Salah", ""]]}, {"id": "1208.0219", "submitter": "Jun Zhang", "authors": "Jun Zhang, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, Marianne Winslett", "title": "Functional Mechanism: Regression Analysis under Differential Privacy", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1364-1375 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\epsilon-differential privacy is the state-of-the-art model for releasing\nsensitive information while protecting privacy. Numerous methods have been\nproposed to enforce epsilon-differential privacy in various analytical tasks,\ne.g., regression analysis. Existing solutions for regression analysis, however,\nare either limited to non-standard types of regression or unable to produce\naccurate regression results. Motivated by this, we propose the Functional\nMechanism, a differentially private method designed for a large class of\noptimization-based analyses. The main idea is to enforce epsilon-differential\nprivacy by perturbing the objective function of the optimization problem,\nrather than its results. As case studies, we apply the functional mechanism to\naddress two most widely used regression models, namely, linear regression and\nlogistic regression. Both theoretical analysis and thorough experimental\nevaluations show that the functional mechanism is highly effective and\nefficient, and it significantly outperforms existing solutions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 14:11:04 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Zhang", "Jun", ""], ["Zhang", "Zhenjie", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Yin", ""], ["Winslett", "Marianne", ""]]}, {"id": "1208.0220", "submitter": "Panagiotis Karras", "authors": "Jianneng Cao, Panagiotis Karras", "title": "Publishing Microdata with a Robust Privacy Guarantee", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1388-1399 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, the publication of microdata poses a privacy threat. Vast research has\nstriven to define the privacy condition that microdata should satisfy before it\nis released, and devise algorithms to anonymize the data so as to achieve this\ncondition. Yet, no method proposed to date explicitly bounds the percentage of\ninformation an adversary gains after seeing the published data for each\nsensitive value therein. This paper introduces beta-likeness, an appropriately\nrobust privacy model for microdata anonymization, along with two anonymization\nschemes designed therefor, the one based on generalization, and the other based\non perturbation. Our model postulates that an adversary's confidence on the\nlikelihood of a certain sensitive-attribute (SA) value should not increase, in\nrelative difference terms, by more than a predefined threshold. Our techniques\naim to satisfy a given beta threshold with little information loss. We\nexperimentally demonstrate that (i) our model provides an effective privacy\nguarantee in a way that predecessor models cannot, (ii) our generalization\nscheme is more effective and efficient in its task than methods adapting\nalgorithms for the k-anonymity model, and (iii) our perturbation method\noutperforms a baseline approach. Moreover, we discuss in detail the resistance\nof our model and methods to attacks proposed in previous research.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 14:11:45 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Cao", "Jianneng", ""], ["Karras", "Panagiotis", ""]]}, {"id": "1208.0221", "submitter": "Ziyu Guan", "authors": "Ziyu Guan, Xifeng Yan, Lance M. Kaplan", "title": "Measuring Two-Event Structural Correlations on Graphs", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1400-1411 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-life graphs usually have various kinds of events happening on them,\ne.g., product purchases in online social networks and intrusion alerts in\ncomputer networks. The occurrences of events on the same graph could be\ncorrelated, exhibiting either attraction or repulsion. Such structural\ncorrelations can reveal important relationships between different events.\nUnfortunately, correlation relationships on graph structures are not well\nstudied and cannot be captured by traditional measures. In this work, we design\na novel measure for assessing two-event structural correlations on graphs.\nGiven the occurrences of two events, we choose uniformly a sample of \"reference\nnodes\" from the vicinity of all event nodes and employ the Kendall's tau rank\ncorrelation measure to compute the average concordance of event density\nchanges. Significance can be efficiently assessed by tau's nice property of\nbeing asymptotically normal under the null hypothesis. In order to compute the\nmeasure in large scale networks, we develop a scalable framework using\ndifferent sampling strategies. The complexity of these strategies is analyzed.\nExperiments on real graph datasets with both synthetic and real events\ndemonstrate that the proposed framework is not only efficacious, but also\nefficient and scalable.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 14:12:02 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Guan", "Ziyu", ""], ["Yan", "Xifeng", ""], ["Kaplan", "Lance M.", ""]]}, {"id": "1208.0222", "submitter": "Feifei Li", "authors": "Jeffrey Jestes, Jeff M. Phillips, Feifei Li, Mingwang Tang", "title": "Ranking Large Temporal Data", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1412-1423 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking temporal data has not been studied until recently, even though\nranking is an important operator (being promoted as a firstclass citizen) in\ndatabase systems. However, only the instant top-k queries on temporal data were\nstudied in, where objects with the k highest scores at a query time instance t\nare to be retrieved. The instant top-k definition clearly comes with\nlimitations (sensitive to outliers, difficult to choose a meaningful query time\nt). A more flexible and general ranking operation is to rank objects based on\nthe aggregation of their scores in a query interval, which we dub the aggregate\ntop-k query on temporal data. For example, return the top-10 weather stations\nhaving the highest average temperature from 10/01/2010 to 10/07/2010; find the\ntop-20 stocks having the largest total transaction volumes from 02/05/2011 to\n02/07/2011. This work presents a comprehensive study to this problem by\ndesigning both exact and approximate methods (with approximation quality\nguarantees). We also provide theoretical analysis on the construction cost, the\nindex size, the update and the query costs of each approach. Extensive\nexperiments on large real datasets clearly demonstrate the efficiency, the\neffectiveness, and the scalability of our methods compared to the baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 14:12:21 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Jestes", "Jeffrey", ""], ["Phillips", "Jeff M.", ""], ["Li", "Feifei", ""], ["Tang", "Mingwang", ""]]}, {"id": "1208.0224", "submitter": "Florian Funke", "authors": "Florian Funke, Alfons Kemper, Thomas Neumann", "title": "Compacting Transactional Data in Hybrid OLTP & OLAP Databases", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1424-1435 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing main memory sizes have facilitated database management systems that\nkeep the entire database in main memory. The drastic performance improvements\nthat came along with these in-memory systems have made it possible to reunite\nthe two areas of online transaction processing (OLTP) and online analytical\nprocessing (OLAP): An emerging class of hybrid OLTP and OLAP database systems\nallows to process analytical queries directly on the transactional data. By\noffering arbitrarily current snapshots of the transactional data for OLAP,\nthese systems enable real-time business intelligence. Despite memory sizes of\nseveral Terabytes in a single commodity server, RAM is still a precious\nresource: Since free memory can be used for intermediate results in query\nprocessing, the amount of memory determines query performance to a large\nextent. Consequently, we propose the compaction of memory-resident databases.\nCompaction consists of two tasks: First, separating the mutable working set\nfrom the immutable \"frozen\" data. Second, compressing the immutable data and\noptimizing it for efficient, memory-consumption-friendly snapshotting. Our\napproach reorganizes and compresses transactional data online and yet hardly\naffects the mission-critical OLTP throughput. This is achieved by unburdening\nthe OLTP threads from all additional processing and performing these tasks\nasynchronously.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 14:13:12 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Funke", "Florian", ""], ["Kemper", "Alfons", ""], ["Neumann", "Thomas", ""]]}, {"id": "1208.0225", "submitter": "Alexander Hall", "authors": "Alexander Hall, Olaf Bachmann, Robert B\\\"ussow, Silviu G\\u{a}nceanu,\n  Marc Nunkesser", "title": "Processing a Trillion Cells per Mouse Click", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1436-1446 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Column-oriented database systems have been a real game changer for the\nindustry in recent years. Highly tuned and performant systems have evolved that\nprovide users with the possibility of answering ad hoc queries over large\ndatasets in an interactive manner. In this paper we present the column-oriented\ndatastore developed as one of the central components of PowerDrill. It combines\nthe advantages of columnar data layout with other known techniques (such as\nusing composite range partitions) and extensive algorithmic engineering on key\ndata structures. The main goal of the latter being to reduce the main memory\nfootprint and to increase the efficiency in processing typical user queries. In\nthis combination we achieve large speed-ups. These enable a highly interactive\nWeb UI where it is common that a single mouse click leads to processing a\ntrillion values in the underlying dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 14:13:23 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Hall", "Alexander", ""], ["Bachmann", "Olaf", ""], ["B\u00fcssow", "Robert", ""], ["G\u0103nceanu", "Silviu", ""], ["Nunkesser", "Marc", ""]]}, {"id": "1208.0227", "submitter": "Danica Porobic", "authors": "Danica Porobic, Ippokratis Pandis, Miguel Branco, P{\\i}nar T\\\"oz\\\"un,\n  Anastasia Ailamaki", "title": "OLTP on Hardware Islands", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1447-1458 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern hardware is abundantly parallel and increasingly heterogeneous. The\nnumerous processing cores have non-uniform access latencies to the main memory\nand to the processor caches, which causes variability in the communication\ncosts. Unfortunately, database systems mostly assume that all processing cores\nare the same and that microarchitecture differences are not significant enough\nto appear in critical database execution paths. As we demonstrate in this\npaper, however, hardware heterogeneity does appear in the critical path and\nconventional database architectures achieve suboptimal and even worse,\nunpredictable performance. We perform a detailed performance analysis of OLTP\ndeployments in servers with multiple cores per CPU (multicore) and multiple\nCPUs per server (multisocket). We compare different database deployment\nstrategies where we vary the number and size of independent database instances\nrunning on a single server, from a single shared-everything instance to\nfine-grained shared-nothing configurations. We quantify the impact of\nnon-uniform hardware on various deployments by (a) examining how efficiently\neach deployment uses the available hardware resources and (b) measuring the\nimpact of distributed transactions and skewed requests on different workloads.\nFinally, we argue in favor of shared-nothing deployments that are topology- and\nworkload-aware and take advantage of fast on-chip communication between islands\nof cores on the same socket.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 14:13:33 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Porobic", "Danica", ""], ["Pandis", "Ippokratis", ""], ["Branco", "Miguel", ""], ["T\u00f6z\u00fcn", "P\u0131nar", ""], ["Ailamaki", "Anastasia", ""]]}, {"id": "1208.0270", "submitter": "Stacy Patterson", "authors": "Stacy Patterson, Aaron J. Elmore, Faisal Nawab, Divyakant Agrawal, Amr\n  El Abbadi", "title": "Serializability, not Serial: Concurrency Control and Availability in\n  Multi-Datacenter Datastores", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1459-1470 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for concurrency control and availability in\nmulti-datacenter datastores. While we consider Google's Megastore as our\nmotivating example, we define general abstractions for key components, making\nour solution extensible to any system that satisfies the abstraction\nproperties. We first develop and analyze a transaction management and\nreplication protocol based on a straightforward implementation of the Paxos\nalgorithm. Our investigation reveals that this protocol acts as a concurrency\nprevention mechanism rather than a concurrency control mechanism. We then\npropose an enhanced protocol called Paxos with Combination and Promotion\n(Paxos-CP) that provides true transaction concurrency while requiring the same\nper instance message complexity as the basic Paxos protocol. Finally, we\ncompare the performance of Paxos and Paxos-CP in a multi-datacenter\nexperimental study, and we demonstrate that Paxos-CP results in significantly\nfewer aborted transactions than basic Paxos.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 16:48:57 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Patterson", "Stacy", ""], ["Elmore", "Aaron J.", ""], ["Nawab", "Faisal", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1208.0271", "submitter": "Alvin Cheung", "authors": "Alvin Cheung, Owen Arden, Samuel Madden, Andrew C. Myers", "title": "Automatic Partitioning of Database Applications", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1471-1482 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database-backed applications are nearly ubiquitous in our daily lives.\nApplications that make many small accesses to the database create two\nchallenges for developers: increased latency and wasted resources from numerous\nnetwork round trips. A well-known technique to improve transactional database\napplication performance is to convert part of the application into stored\nprocedures that are executed on the database server. Unfortunately, this\nconversion is often difficult. In this paper we describe Pyxis, a system that\ntakes database-backed applications and automatically partitions their code into\ntwo pieces, one of which is executed on the application server and the other on\nthe database server. Pyxis profiles the application and server loads,\nstatically analyzes the code's dependencies, and produces a partitioning that\nminimizes the number of control transfers as well as the amount of data sent\nduring each transfer. Our experiments using TPC-C and TPC-W show that Pyxis is\nable to generate partitions with up to 3x reduction in latency and 1.7x\nimprovement in throughput when compared to a traditional non-partitioned\nimplementation and has comparable performance to that of a custom stored\nprocedure implementation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 16:49:06 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Cheung", "Alvin", ""], ["Arden", "Owen", ""], ["Madden", "Samuel", ""], ["Myers", "Andrew C.", ""]]}, {"id": "1208.0273", "submitter": "Chen CAO", "authors": "Caleb Chen Cao, Jieying She, Yongxin Tong, Lei Chen", "title": "Whom to Ask? Jury Selection for Decision Making Tasks on Micro-blog\n  Services", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1495-1506 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is universal to see people obtain knowledge on micro-blog services by\nasking others decision making questions. In this paper, we study the Jury\nSelection Problem(JSP) by utilizing crowdsourcing for decision making tasks on\nmicro-blog services. Specifically, the problem is to enroll a subset of crowd\nunder a limited budget, whose aggregated wisdom via Majority Voting scheme has\nthe lowest probability of drawing a wrong answer(Jury Error Rate-JER). Due to\nvarious individual error-rates of the crowd, the calculation of JER is\nnon-trivial. Firstly, we explicitly state that JER is the probability when the\nnumber of wrong jurors is larger than half of the size of a jury. To avoid the\nexponentially increasing calculation of JER, we propose two efficient\nalgorithms and an effective bounding technique. Furthermore, we study the Jury\nSelection Problem on two crowdsourcing models, one is for altruistic\nusers(AltrM) and the other is for incentive-requiring users(PayM) who require\nextra payment when enrolled into a task. For the AltrM model, we prove the\nmonotonicity of JER on individual error rate and propose an efficient exact\nalgorithm for JSP. For the PayM model, we prove the NP-hardness of JSP on PayM\nand propose an efficient greedy-based heuristic algorithm. Finally, we conduct\na series of experiments to investigate the traits of JSP, and validate the\nefficiency and effectiveness of our proposed algorithms on both synthetic and\nreal micro-blog data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 16:49:31 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Cao", "Caleb Chen", ""], ["She", "Jieying", ""], ["Tong", "Yongxin", ""], ["Chen", "Lei", ""]]}, {"id": "1208.0274", "submitter": "Xiaochun Yang", "authors": "Xiaochun Yang, Honglei Liu, Bin Wang", "title": "ALAE: Accelerating Local Alignment with Affine Gap Exactly in\n  Biosequence Databases", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1507-1518 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of local alignment, which is finding pairs of similar\nsubsequences with gaps. The problem exists in biosequence databases. BLAST is a\ntypical software for finding local alignment based on heuristic, but could miss\nresults. Using the Smith-Waterman algorithm, we can find all local alignments\nin O(mn) time, where m and n are lengths of a query and a text, respectively. A\nrecent exact approach BWT-SW improves the complexity of the Smith-Waterman\nalgorithm under constraints, but still much slower than BLAST. This paper takes\non the challenge of designing an accurate and efficient algorithm for\nevaluating local-alignment searches, especially for long queries. In this\npaper, we propose an efficient software called ALAE to speed up BWT-SW using a\ncompressed suffix array. ALAE utilizes a family of filtering techniques to\nprune meaningless calculations and an algorithm for reusing score calculations.\nWe also give a mathematical analysis and show that the upper bound of the total\nnumber of calculated entries using ALAE could vary from 4.50mn0.520 to\n9.05mn0.896 for random DNA sequences and vary from 8.28mn0.364 to 7.49mn0.723\nfor random protein sequences. We demonstrate the significant performance\nimprovement of ALAE on BWT-SW using a thorough experimental study on real\nbiosequences. ALAE guarantees correctness and accelerates BLAST for most of\nparameters.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 16:49:39 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Yang", "Xiaochun", ""], ["Liu", "Honglei", ""], ["Wang", "Bin", ""]]}, {"id": "1208.0275", "submitter": "K. Selcuk Candan", "authors": "K. Sel\\c{c}uk Candan, Rosaria Rossini, Maria Luisa Sapino, Xiaolan\n  Wang", "title": "sDTW: Computing DTW Distances using Locally Relevant Constraints based\n  on Salient Feature Alignments", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1519-1530 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications generate and consume temporal data and retrieval of time\nseries is a key processing step in many application domains. Dynamic time\nwarping (DTW) distance between time series of size N and M is computed relying\non a dynamic programming approach which creates and fills an NxM grid to search\nfor an optimal warp path. Since this can be costly, various heuristics have\nbeen proposed to cut away the potentially unproductive portions of the DTW\ngrid. In this paper, we argue that time series often carry structural features\nthat can be used for identifying locally relevant constraints to eliminate\nredundant work. Relying on this observation, we propose salient feature based\nsDTW algorithms which first identify robust salient features in the given time\nseries and then find a consistent alignment of these to establish the\nboundaries for the warp path search. More specifically, we propose alternative\nfixed core&adaptive width, adaptive core&fixed width, and adaptive\ncore&adaptive width strategies which enforce different constraints reflecting\nthe high level structural characteristics of the series in the data set.\nExperiment results show that the proposed sDTW algorithms help achieve much\nhigher accuracy in DTWcomputation and time series retrieval than fixed core &\nfixed width algorithms that do not leverage local features of the given time\nseries.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 16:49:49 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Candan", "K. Sel\u00e7uk", ""], ["Rossini", "Rosaria", ""], ["Sapino", "Maria Luisa", ""], ["Wang", "Xiaolan", ""]]}, {"id": "1208.0276", "submitter": "Farhan Tauheed", "authors": "Farhan Tauheed, Thomas Heinis, Felix Sh\\\"urmann, Henry Markram,\n  Anastasia Ailamaki", "title": "SCOUT: Prefetching for Latent Feature Following Queries", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1531-1542 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's scientists are quickly moving from in vitro to in silico\nexperimentation: they no longer analyze natural phenomena in a petri dish, but\ninstead they build models and simulate them. Managing and analyzing the massive\namounts of data involved in simulations is a major task. Yet, they lack the\ntools to efficiently work with data of this size. One problem many scientists\nshare is the analysis of the massive spatial models they build. For several\ntypes of analysis they need to interactively follow the structures in the\nspatial model, e.g., the arterial tree, neuron fibers, etc., and issue range\nqueries along the way. Each query takes long to execute, and the total time for\nexecuting a sequence of queries significantly delays data analysis. Prefetching\nthe spatial data reduces the response time considerably, but known approaches\ndo not prefetch with high accuracy. We develop SCOUT, a structure-aware method\nfor prefetching data along interactive spatial query sequences. SCOUT uses an\napproximate graph model of the structures involved in past queries and attempts\nto identify what particular structure the user follows. Our experiments with\nneuroscience data show that SCOUT prefetches with an accuracy from 71% to 92%,\nwhich translates to a speedup of 4x-15x. SCOUT also improves the prefetching\naccuracy on datasets from other scientific domains, such as medicine and\nbiology.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 16:49:56 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Tauheed", "Farhan", ""], ["Heinis", "Thomas", ""], ["Sh\u00fcrmann", "Felix", ""], ["Markram", "Henry", ""], ["Ailamaki", "Anastasia", ""]]}, {"id": "1208.0277", "submitter": "Kaibo Wang", "authors": "Kaibo Wang, Yin Huai, Rubao Lee, Fusheng Wang, Xiaodong Zhang, Joel H.\n  Saltz", "title": "Accelerating Pathology Image Data Cross-Comparison on CPU-GPU Hybrid\n  Systems", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1543-1554 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important application of spatial databases in pathology imaging\nanalysis, cross-comparing the spatial boundaries of a huge amount of segmented\nmicro-anatomic objects demands extremely data- and compute-intensive\noperations, requiring high throughput at an affordable cost. However, the\nperformance of spatial database systems has not been satisfactory since their\nimplementations of spatial operations cannot fully utilize the power of modern\nparallel hardware. In this paper, we provide a customized software solution\nthat exploits GPUs and multi-core CPUs to accelerate spatial cross-comparison\nin a cost-effective way. Our solution consists of an efficient GPU algorithm\nand a pipelined system framework with task migration support. Extensive\nexperiments with real-world data sets demonstrate the effectiveness of our\nsolution, which improves the performance of spatial cross-comparison by over 18\ntimes compared with a parallelized spatial database approach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 16:50:06 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Wang", "Kaibo", ""], ["Huai", "Yin", ""], ["Lee", "Rubao", ""], ["Wang", "Fusheng", ""], ["Zhang", "Xiaodong", ""], ["Saltz", "Joel H.", ""]]}, {"id": "1208.0278", "submitter": "Arnd K\\\"onig", "authors": "Jiexing Li, Arnd Christian K\\\"onig, Vivek Narasayya, Surajit Chaudhuri", "title": "Robust Estimation of Resource Consumption for SQL Queries using\n  Statistical Techniques", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1555-1566 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to estimate resource consumption of SQL queries is crucial for a\nnumber of tasks in a database system such as admission control, query\nscheduling and costing during query optimization. Recent work has explored the\nuse of statistical techniques for resource estimation in place of the manually\nconstructed cost models used in query optimization. Such techniques, which\nrequire as training data examples of resource usage in queries, offer the\npromise of superior estimation accuracy since they can account for factors such\nas hardware characteristics of the system or bias in cardinality estimates.\nHowever, the proposed approaches lack robustness in that they do not generalize\nwell to queries that are different from the training examples, resulting in\nsignificant estimation errors. Our approach aims to address this problem by\ncombining knowledge of database query processing with statistical models. We\nmodel resource-usage at the level of individual operators, with different\nmodels and features for each operator type, and explicitly model the asymptotic\nbehavior of each operator. This results in significantly better estimation\naccuracy and the ability to estimate resource usage of arbitrary plans, even\nwhen they are very different from the training instances. We validate our\napproach using various large scale real-life and benchmark workloads on\nMicrosoft SQL Server.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 16:50:15 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Li", "Jiexing", ""], ["K\u00f6nig", "Arnd Christian", ""], ["Narasayya", "Vivek", ""], ["Chaudhuri", "Surajit", ""]]}, {"id": "1208.0285", "submitter": "Mahashweta Das", "authors": "Mahashweta Das, Saravanan Thirumuruganathan, Sihem Amer-Yahia, Gautam\n  Das, Cong Yu", "title": "Who Tags What? An Analysis Framework", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1567-1578 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of Web 2.0 is signaled by sites such as Flickr, del.icio.us, and\nYouTube, and social tagging is essential to their success. A typical tagging\naction involves three components, user, item (e.g., photos in Flickr), and tags\n(i.e., words or phrases). Analyzing how tags are assigned by certain users to\ncertain items has important implications in helping users search for desired\ninformation. In this paper, we explore common analysis tasks and propose a dual\nmining framework for social tagging behavior mining. This framework is centered\naround two opposing measures, similarity and diversity, being applied to one or\nmore tagging components, and therefore enables a wide range of analysis\nscenarios such as characterizing similar users tagging diverse items with\nsimilar tags, or diverse users tagging similar items with diverse tags, etc. By\nadopting different concrete measures for similarity and diversity in the\nframework, we show that a wide range of concrete analysis problems can be\ndefined and they are NP-Complete in general. We design efficient algorithms for\nsolving many of those problems and demonstrate, through comprehensive\nexperiments over real data, that our algorithms significantly out-perform the\nexact brute-force approach without compromising analysis result quality.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 17:20:05 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Das", "Mahashweta", ""], ["Thirumuruganathan", "Saravanan", ""], ["Amer-Yahia", "Sihem", ""], ["Das", "Gautam", ""], ["Yu", "Cong", ""]]}, {"id": "1208.0286", "submitter": "Haohan Zhu", "authors": "Haohan Zhu, George Kollios, Vassilis Athitsos", "title": "A Generic Framework for Efficient and Effective Subsequence Retrieval", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1579-1590 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general framework for matching similar subsequences in\nboth time series and string databases. The matching results are pairs of query\nsubsequences and database subsequences. The framework finds all possible pairs\nof similar subsequences if the distance measure satisfies the \"consistency\"\nproperty, which is a property introduced in this paper. We show that most\npopular distance functions, such as the Euclidean distance, DTW, ERP, the\nFrechet distance for time series, and the Hamming distance and Levenshtein\ndistance for strings, are all \"consistent\". We also propose a generic index\nstructure for metric spaces named \"reference net\". The reference net occupies\nO(n) space, where n is the size of the dataset and is optimized to work well\nwith our framework. The experiments demonstrate the ability of our method to\nimprove retrieval performance when combined with diverse distance measures. The\nexperiments also illustrate that the reference net scales well in terms of\nspace overhead and query time.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 17:20:11 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Zhu", "Haohan", ""], ["Kollios", "George", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "1208.0287", "submitter": "Jens Dittrich", "authors": "Jens Dittrich, Jorge-Arnulfo Quian\\'e-Ruiz, Stefan Richter, Stefan\n  Schuh, Alekh Jindal, J\\\"org Schad", "title": "Only Aggressive Elephants are Fast Elephants", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1591-1602 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yellow elephants are slow. A major reason is that they consume their inputs\nentirely before responding to an elephant rider's orders. Some clever riders\nhave trained their yellow elephants to only consume parts of the inputs before\nresponding. However, the teaching time to make an elephant do that is high. So\nhigh that the teaching lessons often do not pay off. We take a different\napproach. We make elephants aggressive; only this will make them very fast. We\npropose HAIL (Hadoop Aggressive Indexing Library), an enhancement of HDFS and\nHadoop MapReduce that dramatically improves runtimes of several classes of\nMapReduce jobs. HAIL changes the upload pipeline of HDFS in order to create\ndifferent clustered indexes on each data block replica. An interesting feature\nof HAIL is that we typically create a win-win situation: we improve both data\nupload to HDFS and the runtime of the actual Hadoop MapReduce job. In terms of\ndata upload, HAIL improves over HDFS by up to 60% with the default replication\nfactor of three. In terms of query execution, we demonstrate that HAIL runs up\nto 68x faster than Hadoop. In our experiments, we use six clusters including\nphysical and EC2 clusters of up to 100 nodes. A series of scalability\nexperiments also demonstrates the superiority of HAIL.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 17:20:18 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Dittrich", "Jens", ""], ["Quian\u00e9-Ruiz", "Jorge-Arnulfo", ""], ["Richter", "Stefan", ""], ["Schuh", "Stefan", ""], ["Jindal", "Alekh", ""], ["Schad", "J\u00f6rg", ""]]}, {"id": "1208.0288", "submitter": "Rui Li", "authors": "Rui Li, Shengjie Wang, Kevin Chen-Chuan Chang", "title": "Multiple Location Profiling for Users and Relationships from Social\n  Network and Content", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1603-1614 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users' locations are important for many applications such as personalized\nsearch and localized content delivery. In this paper, we study the problem of\nprofiling Twitter users' locations with their following network and tweets. We\npropose a multiple location profiling model (MLP), which has three key\nfeatures: 1) it formally models how likely a user follows another user given\ntheir locations and how likely a user tweets a venue given his location, 2) it\nfundamentally captures that a user has multiple locations and his following\nrelationships and tweeted venues can be related to any of his locations, and\nsome of them are even noisy, and 3) it novelly utilizes the home locations of\nsome users as partial supervision. As a result, MLP not only discovers users'\nlocations accurately and completely, but also \"explains\" each following\nrelationship by revealing users' true locations in the relationship.\nExperiments on a large-scale data set demonstrate those advantages.\nParticularly, 1) for predicting users' home locations, MLP successfully places\n62% users and outperforms two state-of-the-art methods by 10% in accuracy, 2)\nfor discovering users' multiple locations, MLP improves the baseline methods by\n14% in recall, and 3) for explaining following relationships, MLP achieves 57%\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 17:20:26 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Li", "Rui", ""], ["Wang", "Shengjie", ""], ["Chang", "Kevin Chen-Chuan", ""]]}, {"id": "1208.0289", "submitter": "Bongki Moon", "authors": "Woon-Hak Kang, Sang-Won Lee, Bongki Moon", "title": "Flash-based Extended Cache for Higher Throughput and Faster Recovery", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1615-1626 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the current price gap between disk and flash memory drives, for\napplications dealing with large scale data, it will be economically more\nsensible to use flash memory drives to supplement disk drives rather than to\nreplace them. This paper presents FaCE, which is a new low-overhead caching\nstrategy that uses flash memory as an extension to the DRAM buffer. FaCE aims\nat improving the transaction throughput as well as shortening the recovery time\nfrom a system failure. To achieve the goals, we propose two novel algorithms\nfor flash cache management, namely, Multi-Version FIFO replacement and Group\nSecond Chance. One striking result from FaCE is that using a small flash memory\ndrive as a caching device could deliver even higher throughput than using a\nlarge flash memory drive to store the entire database tables. This was possible\ndue to flash write optimization as well as disk access reduction obtained by\nthe FaCE caching methods. In addition, FaCE takes advantage of the\nnon-volatility of flash memory to fully support database recovery by extending\nthe scope of a persistent database to include the data pages stored in the\nflash cache. We have implemented FaCE in the PostgreSQL open source database\nserver and demonstrated its effectiveness for TPC-C benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 17:20:34 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Kang", "Woon-Hak", ""], ["Lee", "Sang-Won", ""], ["Moon", "Bongki", ""]]}, {"id": "1208.0290", "submitter": "Rob Johnson", "authors": "Michael A. Bender, Martin Farach-Colton, Rob Johnson, Russell Kraner,\n  Bradley C. Kuszmaul, Dzejla Medjedovic, Pablo Montes, Pradeep Shetty, Richard\n  P. Spillane, Erez Zadok", "title": "Don't Thrash: How to Cache Your Hash on Flash", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1627-1637 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new alternatives to the well-known Bloom filter data\nstructure. The Bloom filter, a compact data structure supporting set insertion\nand membership queries, has found wide application in databases, storage\nsystems, and networks. Because the Bloom filter performs frequent random reads\nand writes, it is used almost exclusively in RAM, limiting the size of the sets\nit can represent. This paper first describes the quotient filter, which\nsupports the basic operations of the Bloom filter, achieving roughly comparable\nperformance in terms of space and time, but with better data locality.\nOperations on the quotient filter require only a small number of contiguous\naccesses. The quotient filter has other advantages over the Bloom filter: it\nsupports deletions, it can be dynamically resized, and two quotient filters can\nbe efficiently merged. The paper then gives two data structures, the buffered\nquotient filter and the cascade filter, which exploit the quotient filter\nadvantages and thus serve as SSD-optimized alternatives to the Bloom filter.\nThe cascade filter has better asymptotic I/O performance than the buffered\nquotient filter, but the buffered quotient filter outperforms the cascade\nfilter on small to medium data sets. Both data structures significantly\noutperform recently-proposed SSD-optimized Bloom filter variants, such as the\nelevator Bloom filter, buffered Bloom filter, and forest-structured Bloom\nfilter. In experiments, the cascade filter and buffered quotient filter\nperformed insertions 8.6-11 times faster than the fastest Bloom filter variant\nand performed lookups 0.94-2.56 times faster.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 17:21:23 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Bender", "Michael A.", ""], ["Farach-Colton", "Martin", ""], ["Johnson", "Rob", ""], ["Kraner", "Russell", ""], ["Kuszmaul", "Bradley C.", ""], ["Medjedovic", "Dzejla", ""], ["Montes", "Pablo", ""], ["Shetty", "Pradeep", ""], ["Spillane", "Richard P.", ""], ["Zadok", "Erez", ""]]}, {"id": "1208.0291", "submitter": "Robert Isele", "authors": "Robert Isele, Christian Bizer", "title": "Learning Expressive Linkage Rules using Genetic Programming", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1638-1649 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in data integration and data cleansing is to find entities\nin different data sources that describe the same real-world object. Many\nexisting methods for identifying such entities rely on explicit linkage rules\nwhich specify the conditions that entities must fulfill in order to be\nconsidered to describe the same real-world object. In this paper, we present\nthe GenLink algorithm for learning expressive linkage rules from a set of\nexisting reference links using genetic programming. The algorithm is capable of\ngenerating linkage rules which select discriminative properties for comparison,\napply chains of data transformations to normalize property values, choose\nappropriate distance measures and thresholds and combine the results of\nmultiple comparisons using non-linear aggregation functions. Our experiments\nshow that the GenLink algorithm outperforms the state-of-the-art genetic\nprogramming approach to learning linkage rules recently presented by Carvalho\net. al. and is capable of learning linkage rules which achieve a similar\naccuracy as human written rules for the same problem.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 17:21:32 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Isele", "Robert", ""], ["Bizer", "Christian", ""]]}, {"id": "1208.0292", "submitter": "Yongxin Tong", "authors": "Yongxin Tong, Lei Chen, Yurong Cheng, Philip S. Yu", "title": "Mining Frequent Itemsets over Uncertain Databases", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1650-1661 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, due to the wide applications of uncertain data, mining\nfrequent itemsets over uncertain databases has attracted much attention. In\nuncertain databases, the support of an itemset is a random variable instead of\na fixed occurrence counting of this itemset. Thus, unlike the corresponding\nproblem in deterministic databases where the frequent itemset has a unique\ndefinition, the frequent itemset under uncertain environments has two different\ndefinitions so far. The first definition, referred as the expected\nsupport-based frequent itemset, employs the expectation of the support of an\nitemset to measure whether this itemset is frequent. The second definition,\nreferred as the probabilistic frequent itemset, uses the probability of the\nsupport of an itemset to measure its frequency. Thus, existing work on mining\nfrequent itemsets over uncertain databases is divided into two different groups\nand no study is conducted to comprehensively compare the two different\ndefinitions. In addition, since no uniform experimental platform exists,\ncurrent solutions for the same definition even generate inconsistent results.\nIn this paper, we firstly aim to clarify the relationship between the two\ndifferent definitions. Through extensive experiments, we verify that the two\ndefinitions have a tight connection and can be unified together when the size\nof data is large enough. Secondly, we provide baseline implementations of eight\nexisting representative algorithms and test their performances with uniform\nmeasures fairly. Finally, according to the fair tests over many different\nbenchmark data sets, we clarify several existing inconsistent conclusions and\ndiscuss some new findings.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 17:22:19 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Tong", "Yongxin", ""], ["Chen", "Lei", ""], ["Cheng", "Yurong", ""], ["Yu", "Philip S.", ""]]}, {"id": "1208.0684", "submitter": "Hamed Hassanzadeh", "authors": "Mahnoosh Kholghi and MohammadReza Keyvanpour", "title": "Comparative Evaluation of Data Stream Indexing Models", "comments": null, "journal-ref": "International Journal of Machine Learning and Computing vol. 2,\n  no. 3, pp. 257-260, 2012", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In recent years, the management and processing of data streams has become a\ntopic of active research in several fields of computer science such as,\ndistributed systems, database systems, and data mining. A data stream can be\nthought of as a transient, continuously increasing sequence of data. In data\nstreams' applications, because of online monitoring, answering to the user's\nqueries should be time and space efficient. In this paper, we consider the\nspecial requirements of indexing to determine the performance of different\ntechniques in data stream processing environments. Stream indexing has main\ndifferences with approaches in traditional databases. Also, we compare data\nstream indexing models analytically that can provide a suitable method for\nstream indexing.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 08:17:57 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Kholghi", "Mahnoosh", ""], ["Keyvanpour", "MohammadReza", ""]]}, {"id": "1208.1231", "submitter": "Sebastian Michel", "authors": "Foteini Alvanaki and Sebastian Michel and Aleksandar Stupar", "title": "Building and Maintaining Halls of Fame over a Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Halls of Fame are fascinating constructs. They represent the elite of an\noften very large amount of entities---persons, companies, products, countries\netc. Beyond their practical use as static rankings, changes to them are\nparticularly interesting---for decision making processes, as input to common\nmedia or novel narrative science applications, or simply consumed by users. In\nthis work, we aim at detecting events that can be characterized by changes to a\nHall of Fame ranking in an automated way. We describe how the schema and data\nof a database can be used to generate Halls of Fame. In this database scenario,\nby Hall of Fame we refer to distinguished tuples; entities, whose\ncharacteristics set them apart from the majority. We define every Hall of Fame\nas one specific instance of an SQL query, such that a change in its result is\nconsidered a noteworthy event. Identified changes (i.e., events) are ranked\nusing lexicographic tradeoffs over event and query properties and presented to\nusers or fed in higher-level applications. We have implemented a full-fledged\nprototype system that uses either database triggers or a Java based middleware\nfor event identification. We report on an experimental evaluation using a\nreal-world dataset of basketball statistics.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 18:26:17 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Alvanaki", "Foteini", ""], ["Michel", "Sebastian", ""], ["Stupar", "Aleksandar", ""]]}, {"id": "1208.1860", "submitter": "Benjamin Rubinstein", "authors": "Sahand Negahban, Benjamin I. P. Rubinstein and Jim Gemmell", "title": "Scaling Multiple-Source Entity Resolution using Statistically Efficient\n  Transfer Learning", "comments": "Short version to appear in CIKM'2012; 10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a serious, previously-unexplored challenge facing almost all\napproaches to scaling up entity resolution (ER) to multiple data sources: the\nprohibitive cost of labeling training data for supervised learning of\nsimilarity scores for each pair of sources. While there exists a rich\nliterature describing almost all aspects of pairwise ER, this new challenge is\narising now due to the unprecedented ability to acquire and store data from\nonline sources, features driven by ER such as enriched search verticals, and\nthe uniqueness of noisy and missing data characteristics for each source. We\nshow on real-world and synthetic data that for state-of-the-art techniques, the\nreality of heterogeneous sources means that the number of labeled training data\nmust scale quadratically in the number of sources, just to maintain constant\nprecision/recall. We address this challenge with a brand new transfer learning\nalgorithm which requires far less training data (or equivalently, achieves\nsuperior accuracy with the same data) and is trained using fast convex\noptimization. The intuition behind our approach is to adaptively share\nstructure learned about one scoring problem with all other scoring problems\nsharing a data source in common. We demonstrate that our theoretically\nmotivated approach incurs no runtime cost while it can maintain constant\nprecision/recall with the cost of labeling increasing only linearly with the\nnumber of sources.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 10:02:35 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Negahban", "Sahand", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Gemmell", "Jim", ""]]}, {"id": "1208.1927", "submitter": "Jiannan Wang", "authors": "Jiannan Wang, Tim Kraska, Michael J. Franklin, Jianhua Feng", "title": "CrowdER: Crowdsourcing Entity Resolution", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1483-1494 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution is central to data integration and data cleaning.\nAlgorithmic approaches have been improving in quality, but remain far from\nperfect. Crowdsourcing platforms offer a more accurate but expensive (and slow)\nway to bring human insight into the process. Previous work has proposed\nbatching verification tasks for presentation to human workers but even with\nbatching, a human-only approach is infeasible for data sets of even moderate\nsize, due to the large numbers of matches to be tested. Instead, we propose a\nhybrid human-machine approach in which machines are used to do an initial,\ncoarse pass over all the data, and people are used to verify only the most\nlikely matching pairs. We show that for such a hybrid system, generating the\nminimum number of verification tasks of a given size is NP-Hard, but we develop\na novel two-tiered heuristic approach for creating batched tasks. We describe\nthis method, and present the results of extensive experiments on real data sets\nusing a popular crowdsourcing platform. The experiments show that our hybrid\napproach achieves both good efficiency and high accuracy compared to\nmachine-only or human-only alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 14:46:38 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Wang", "Jiannan", ""], ["Kraska", "Tim", ""], ["Franklin", "Michael J.", ""], ["Feng", "Jianhua", ""]]}, {"id": "1208.1931", "submitter": "Michele Dallachiesa", "authors": "Michele Dallachiesa, Besmira Nushi, Katsiaryna Mirylenka, Themis\n  Palpanas", "title": "Uncertain Time-Series Similarity: Return to the Basics", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1662-1673 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years there has been a considerable increase in the availability\nof continuous sensor measurements in a wide range of application domains, such\nas Location-Based Services (LBS), medical monitoring systems, manufacturing\nplants and engineering facilities to ensure efficiency, product quality and\nsafety, hydrologic and geologic observing systems, pollution management, and\nothers. Due to the inherent imprecision of sensor observations, many\ninvestigations have recently turned into querying, mining and storing uncertain\ndata. Uncertainty can also be due to data aggregation, privacy-preserving\ntransforms, and error-prone mining algorithms. In this study, we survey the\ntechniques that have been proposed specifically for modeling and processing\nuncertain time series, an important model for temporal data. We provide an\nanalytical evaluation of the alternatives that have been proposed in the\nliterature, highlighting the advantages and disadvantages of each approach, and\nfurther compare these alternatives with two additional techniques that were\ncarefully studied before. We conduct an extensive experimental evaluation with\n17 real datasets, and discuss some surprising results, which suggest that a\nfruitful research direction is to take into account the temporal correlations\nin the time series. Based on our evaluations, we also provide guidelines useful\nfor the practitioners in the field.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 14:52:01 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Dallachiesa", "Michele", ""], ["Nushi", "Besmira", ""], ["Mirylenka", "Katsiaryna", ""], ["Palpanas", "Themis", ""]]}, {"id": "1208.1932", "submitter": "Tamraparni Dasu", "authors": "Tamraparni Dasu, Ji Meng Loh", "title": "Statistical Distortion: Consequences of Data Cleaning", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1674-1683 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of statistical distortion as an essential metric for\nmeasuring the effectiveness of data cleaning strategies. We use this metric to\npropose a widely applicable yet scalable experimental framework for evaluating\ndata cleaning strategies along three dimensions: glitch improvement,\nstatistical distortion and cost-related criteria. Existing metrics focus on\nglitch improvement and cost, but not on the statistical impact of data cleaning\nstrategies. We illustrate our framework on real world data, with a\ncomprehensive suite of experiments and analyses.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 14:52:19 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Dasu", "Tamraparni", ""], ["Loh", "Ji Meng", ""]]}, {"id": "1208.1933", "submitter": "Willis Lang", "authors": "Willis Lang, Stavros Harizopoulos, Jignesh M. Patel, Mehul A. Shah,\n  Dimitris Tsirogiannis", "title": "Towards Energy-Efficient Database Cluster Design", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1684-1695 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy is a growing component of the operational cost for many \"big data\"\ndeployments, and hence has become increasingly important for practitioners of\nlarge-scale data analysis who require scale-out clusters or parallel DBMS\nappliances. Although a number of recent studies have investigated the energy\nefficiency of DBMSs, none of these studies have looked at the architectural\ndesign space of energy-efficient parallel DBMS clusters. There are many\nchallenges to increasing the energy efficiency of a DBMS cluster, including\ndealing with the inherent scaling inefficiency of parallel data processing, and\nchoosing the appropriate energy-efficient hardware. In this paper, we\nexperimentally examine and analyze a number of key parameters related to these\nchallenges for designing energy-efficient database clusters. We explore the\ncluster design space using empirical results and propose a model that considers\nthe key bottlenecks to energy efficiency in a parallel DBMS. This paper\nrepresents a key first step in designing energy-efficient database clusters,\nwhich is increasingly important given the trend toward parallel database\nappliances.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 14:54:33 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Lang", "Willis", ""], ["Harizopoulos", "Stavros", ""], ["Patel", "Jignesh M.", ""], ["Shah", "Mehul A.", ""], ["Tsirogiannis", "Dimitris", ""]]}, {"id": "1208.2013", "submitter": "Alvin Cheung", "authors": "Alvin Cheung and Armando Solar-Lezama and Samuel Madden", "title": "Inferring SQL Queries Using Program Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing high-performance applications that interact with databases is a\ndifficult task, as developers need to understand both the details of the\nlanguage in which their applications are written in, and also the intricacies\nof the relational model. One popular solution to this problem is the use of\nobject-relational mapping (ORM) libraries that provide transparent access to\nthe database using the same language that the application is written in.\nUnfortunately, using such frameworks can easily lead to applications with poor\nperformance because developers often end up implementing relational operations\nin application code, and doing so usually does not take advantage of the\noptimized implementations of relational operations, efficient query plans, or\npush down of predicates that database systems provide. In this paper we present\nQBS, an algorithm that automatically identifies fragments of application logic\nthat can be pushed into SQL queries. The QBS algorithm works by automatically\nsynthesizing invariants and postconditions for the original code fragment. The\npostconditions and invariants are expressed using a theory of ordered relations\nthat allows us to reason precisely about the contents and order of the records\nproduced even by complex code fragments that compute joins and aggregates. The\ntheory is close in expressiveness to SQL, so the synthesized postconditions can\nbe readily translated to SQL queries. Using 40 code fragments extracted from\nover 120k lines of open-source code written using the Java Hibernate ORM, we\ndemonstrate that our approach can convert a variety of imperative constructs\ninto relational specifications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 19:26:29 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Cheung", "Alvin", ""], ["Solar-Lezama", "Armando", ""], ["Madden", "Samuel", ""]]}, {"id": "1208.2448", "submitter": "Yong Zeng", "authors": "Yong Zeng, Zhifeng Bao, Guoliang Li, Tok Wang Ling, Jiaheng Lu", "title": "Breaking Out The XML MisMatch Trap", "comments": "The article is already withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In keyword search, when user cannot get what she wants, query refinement is\nneeded and reason can be various. We first give a thorough categorization of\nthe reason, then focus on solving one category of query refinement problem in\nthe context of XML keyword search, where what user searches for does not exist\nin the data. We refer to it as the MisMatch problem in this paper. Then we\npropose a practical way to detect the MisMatch problem and generate helpful\nsuggestions to users. Our approach can be viewed as a post-processing job of\nquery evaluation, and has three main features: (1) it adopts both the suggested\nqueries and their sample results as the output to user, helping user judge\nwhether the MisMatch problem is solved without consuming all query results; (2)\nit is portable in the sense that it can work with any LCA-based matching\nsemantics and orthogonal to the choice of result retrieval method adopted; (3)\nit is lightweight in the way that it occupies a very small proportion of the\nwhole query evaluation time. Extensive experiments on three real datasets\nverify the effectiveness, efficiency and scalability of our approach. An online\nXML keyword search engine called XClear that embeds the MisMatch problem\ndetector and suggester has been built.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2012 18:51:23 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 03:09:15 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2012 07:34:13 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Zeng", "Yong", ""], ["Bao", "Zhifeng", ""], ["Li", "Guoliang", ""], ["Ling", "Tok Wang", ""], ["Lu", "Jiaheng", ""]]}, {"id": "1208.2478", "submitter": "Sreenivas Gollapudi", "authors": "Sreenivas Gollapudi, Samuel Ieong, Anitha Kannan", "title": "Structured Query Reformulations in Commerce Search", "comments": "A shorter version appeared in CIKM 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in commerce search has shown that understanding the semantics in\nuser queries enables more effective query analysis and retrieval of relevant\nproducts. However, due to lack of sufficient domain knowledge, user queries\noften include terms that cannot be mapped directly to any product attribute.\nFor example, a user looking for {\\tt designer handbags} might start with such a\nquery because she is not familiar with the manufacturers, the price ranges,\nand/or the material that gives a handbag designer appeal. Current commerce\nsearch engines treat terms such as {\\tt designer} as keywords and attempt to\nmatch them to contents such as product reviews and product descriptions, often\nresulting in poor user experience.\n  In this study, we propose to address this problem by reformulating queries\ninvolving terms such as {\\tt designer}, which we call \\emph{modifiers}, to\nqueries that specify precise product attributes. We learn to rewrite the\nmodifiers to attribute values by analyzing user behavior and leveraging\nstructured data sources such as the product catalog that serves the queries. We\nfirst produce a probabilistic mapping between the modifiers and attribute\nvalues based on user behavioral data. These initial associations are then used\nto retrieve products from the catalog, over which we infer sets of attribute\nvalues that best describe the semantics of the modifiers. We evaluate the\neffectiveness of our approach based on a comprehensive Mechanical Turk study.\nWe find that users agree with the attribute values selected by our approach in\nabout 95% of the cases and they prefer the results surfaced for our\nreformulated queries to ones for the original queries in 87% of the time.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 01:00:24 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Gollapudi", "Sreenivas", ""], ["Ieong", "Samuel", ""], ["Kannan", "Anitha", ""]]}, {"id": "1208.2773", "submitter": "Luca Bonomi", "authors": "Luca Bonomi, Li Xiong, Rui Chen, and Benjamin C. M. Fung", "title": "Privacy Preserving Record Linkage via grams Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage has been extensively used in various data mining applications\ninvolving sharing data. While the amount of available data is growing, the\nconcern of disclosing sensitive information poses the problem of utility vs\nprivacy. In this paper, we study the problem of private record linkage via\nsecure data transformations. In contrast to the existing techniques in this\narea, we propose a novel approach that provides strong privacy guarantees under\nthe formal framework of differential privacy. We develop an embedding strategy\nbased on frequent variable length grams mined in a private way from the\noriginal data. We also introduce personalized threshold for matching individual\nrecords in the embedded space which achieves better linkage accuracy than the\nexisting global threshold approach. Compared with the state-of-the-art secure\nmatching schema, our approach provides formal, provable privacy guarantees and\nachieves better scalability while providing comparable utility.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 02:42:01 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Bonomi", "Luca", ""], ["Xiong", "Li", ""], ["Chen", "Rui", ""], ["Fung", "Benjamin C. M.", ""]]}, {"id": "1208.2925", "submitter": "Alvin Cheung", "authors": "Alvin Cheung, Armando Solar-Lezama, Samuel Madden", "title": "Using Program Synthesis for Social Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIT-CSAIL-TR-2012-025", "categories": "cs.LG cs.DB cs.PL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to select events of interest to a user in\na social media setting where events are generated by the activities of the\nuser's friends through their mobile devices. We argue that given the unique\nrequirements of the social media setting, the problem is best viewed as an\ninductive learning problem, where the goal is to first generalize from the\nusers' expressed \"likes\" and \"dislikes\" of specific events, then to produce a\nprogram that can be manipulated by the system and distributed to the collection\ndevices to collect only data of interest. The key contribution of this paper is\na new algorithm that combines existing machine learning techniques with new\nprogram synthesis technology to learn users' preferences. We show that when\ncompared with the more standard approaches, our new algorithm provides up to\norder-of-magnitude reductions in model training time, and significantly higher\nprediction accuracies for our target application. The approach also improves on\nstandard machine learning techniques in that it produces clear programs that\ncan be manipulated to optimize data collection and filtering.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 17:04:19 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Cheung", "Alvin", ""], ["Solar-Lezama", "Armando", ""], ["Madden", "Samuel", ""]]}, {"id": "1208.3307", "submitter": "Evgeniy Grigoriev A.", "authors": "Grigoriev Evgeny", "title": "Impedance mismatch is not an \"Objects vs. Relations\" problem", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem of impedance mismatch between applications written in OO languages\nand relational DB is not a problem of discrepancy between object-oriented and\nrelational approaches themselves. Its real causes can be found in usual\nimplementation of the OO approach. Direct comparison of the two approaches\ncannot be used as a base for the conclusion that they are discrepant or\nmismatched. Experimental proof of absence of contradiction between\nobject-oriented paradigm and relational data model is also presented\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2012 07:43:28 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2012 07:29:56 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2013 22:21:40 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Evgeny", "Grigoriev", ""]]}, {"id": "1208.3533", "submitter": "Marina Drosou", "authors": "Marina Drosou, Evaggelia Pitoura", "title": "DisC Diversity: Result Diversification based on Dissimilarity and\n  Coverage", "comments": "To appear at the 39th International Conference on Very Large Data\n  Bases (VLDB), August 26-31, 2013, Riva del Garda, Trento, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, result diversification has attracted a lot of attention as a means\nto improve the quality of results retrieved by user queries. In this paper, we\npropose a new, intuitive definition of diversity called DisC diversity. A DisC\ndiverse subset of a query result contains objects such that each object in the\nresult is represented by a similar object in the diverse subset and the objects\nin the diverse subset are dissimilar to each other. We show that locating a\nminimum DisC diverse subset is an NP-hard problem and provide heuristics for\nits approximation. We also propose adapting DisC diverse subsets to a different\ndegree of diversification. We call this operation zooming. We present efficient\nimplementations of our algorithms based on the M-tree, a spatial index\nstructure, and experimentally evaluate their performance.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 05:45:18 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 06:02:16 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Drosou", "Marina", ""], ["Pitoura", "Evaggelia", ""]]}, {"id": "1208.3812", "submitter": "Pritam Chanda", "authors": "Pritam Chanda, Aidong Zhang, and Murali Ramanathan", "title": "Algorithms for Efficient Mining of Statistically Significant Attribute\n  Association Information", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of the association information between the attributes in a data set\nprovides insight into the underlying structure of the data and explains the\nrelationships (independence, synergy, redundancy) between the attributes and\nclass (if present). Complex models learnt computationally from the data are\nmore interpretable to a human analyst when such interdependencies are known. In\nthis paper, we focus on mining two types of association information among the\nattributes - correlation information and interaction information for both\nsupervised (class attribute present) and unsupervised analysis (class attribute\nabsent). Identifying the statistically significant attribute associations is a\ncomputationally challenging task - the number of possible associations\nincreases exponentially and many associations contain redundant information\nwhen a number of correlated attributes are present. In this paper, we explore\nefficient data mining methods to discover non-redundant attribute sets that\ncontain significant association information indicating the presence of\ninformative patterns in the data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 06:09:49 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Chanda", "Pritam", ""], ["Zhang", "Aidong", ""], ["Ramanathan", "Murali", ""]]}, {"id": "1208.3876", "submitter": "Saravanan Thirumuruanathan", "authors": "Saravanan Thirumuruganathan, Nan Zhang, Gautam Das", "title": "Digging Deeper into Deep Web Databases by Breaking Through the Top-k\n  Barrier", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of web databases are only accessible through proprietary\nform-like interfaces which require users to query the system by entering\ndesired values for a few attributes. A key restriction enforced by such an\ninterface is the top-k output constraint - i.e., when there are a large number\nof matching tuples, only a few (top-k) of them are preferentially selected and\nreturned by the website, often according to a proprietary ranking function.\nSince most web database owners set k to be a small value, the top-k output\nconstraint prevents many interesting third-party (e.g., mashup) services from\nbeing developed over real-world web databases. In this paper we consider the\nnovel problem of \"digging deeper\" into such web databases. Our main\ncontribution is the meta-algorithm GetNext that can retrieve the next ranked\ntuple from the hidden web database using only the restrictive interface of a\nweb database without any prior knowledge of its ranking function. This\nalgorithm can then be called iteratively to retrieve as many top ranked tuples\nas necessary. We develop principled and efficient algorithms that are based on\ngenerating and executing multiple reformulated queries and inferring the next\nranked tuple from their returned results. We provide theoretical analysis of\nour algorithms, as well as extensive experimental results over synthetic and\nreal-world databases that illustrate the effectiveness of our techniques.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 18:34:28 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Thirumuruganathan", "Saravanan", ""], ["Zhang", "Nan", ""], ["Das", "Gautam", ""]]}, {"id": "1208.3943", "submitter": "Jay Gholap B.Tech.(Computer Engineering)", "authors": "Jay Gholap", "title": "Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility", "comments": "5 Pages", "journal-ref": "Published in Asian Journal of Computer Science and Information\n  Technology,Vol 2,No. 8 (2012)", "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining involves the systematic analysis of large data sets, and data\nmining in agricultural soil datasets is exciting and modern research area. The\nproductive capacity of a soil depends on soil fertility. Achieving and\nmaintaining appropriate levels of soil fertility, is of utmost importance if\nagricultural land is to remain capable of nourishing crop production. In this\nresearch, Steps for building a predictive model of soil fertility have been\nexplained.\n  This paper aims at predicting soil fertility class using decision tree\nalgorithms in data mining . Further, it focuses on performance tuning of J48\ndecision tree algorithm with the help of meta-techniques such as attribute\nselection and boosting.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2012 08:48:40 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Gholap", "Jay", ""]]}, {"id": "1208.4016", "submitter": "Gowri Shankar Ramaswamy", "authors": "Gowri Shankar Ramaswamy and F Sagayaraj Francis", "title": "Concept driven framework for Latent Table Discovery", "comments": null, "journal-ref": "JOURNAL OF COMPUTING, VOLUME 4, ISSUE 7, JULY 2012, ISSN (Online)\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database systems have to cater to the growing demands of the information age.\nThe growth of the new age information retrieval powerhouses like search engines\nhas thrown a challenge to the data management community to come up with novel\nmechanisms for feeding information to end users. The burgeoning use of natural\nlanguage query interfaces compels system designers to present meaningful and\ncustomised information. Conventional query languages like SQL do not cater to\nthese requirements due to syntax oriented design. Providing a semantic cover\nover these systems was the aim of latent table discovery focusing on\nsemantically connecting unrelated tables that were not syntactically related by\ndesign and document the discovered knowledge. This paper throws a new direction\ntowards improving the semantic capabilities of database systems by introducing\na concept driven framework over the latent table discovery method.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2012 14:09:01 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Ramaswamy", "Gowri Shankar", ""], ["Francis", "F Sagayaraj", ""]]}, {"id": "1208.4145", "submitter": "Francesco Bonchi", "authors": "Paolo Boldi, Francesco Bonchi, Aris Gionis, Tamir Tassa", "title": "Injecting Uncertainty in Graphs for Identity Obfuscation", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.\n  1376-1387 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collected nowadays by social-networking applications create fascinating\nopportunities for building novel services, as well as expanding our\nunderstanding about social structures and their dynamics. Unfortunately,\npublishing social-network graphs is considered an ill-advised practice due to\nprivacy concerns. To alleviate this problem, several anonymization methods have\nbeen proposed, aiming at reducing the risk of a privacy breach on the published\ndata, while still allowing to analyze them and draw relevant conclusions. In\nthis paper we introduce a new anonymization approach that is based on injecting\nuncertainty in social graphs and publishing the resulting uncertain graphs.\nWhile existing approaches obfuscate graph data by adding or removing edges\nentirely, we propose using a finer-grained perturbation that adds or removes\nedges partially: this way we can achieve the same desired level of obfuscation\nwith smaller changes in the data, thus maintaining higher utility. Our\nexperiments on real-world networks confirm that at the same level of identity\nobfuscation our method provides higher usefulness than existing randomized\nmethods that publish standard graphs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 00:13:56 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Boldi", "Paolo", ""], ["Bonchi", "Francesco", ""], ["Gionis", "Aris", ""], ["Tassa", "Tamir", ""]]}, {"id": "1208.4165", "submitter": "Joseph Hellerstein", "authors": "Joe Hellerstein, Christopher R\\'e, Florian Schoppmann, Daisy Zhe Wang,\n  Eugene Fratkin, Aleksander Gorajek, Kee Siong Ng, Caleb Welton, Xixuan Feng,\n  Kun Li, Arun Kumar", "title": "The MADlib Analytics Library or MAD Skills, the SQL", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1700-1711 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MADlib is a free, open source library of in-database analytic methods. It\nprovides an evolving suite of SQL-based algorithms for machine learning, data\nmining and statistics that run at scale within a database engine, with no need\nfor data import/export to other tools. The goal is for MADlib to eventually\nserve a role for scalable database systems that is similar to the CRAN library\nfor R: a community repository of statistical methods, this time written with\nscale and parallelism in mind. In this paper we introduce the MADlib project,\nincluding the background that led to its beginnings, and the motivation for its\nopen source nature. We provide an overview of the library's architecture and\ndesign patterns, and provide a description of various statistical methods in\nthat context. We include performance and speedup results of a core design\npattern from one of those methods over the Greenplum parallel DBMS on a\nmodest-sized test cluster. We then report on two initial efforts at\nincorporating academic research into MADlib, which is one of the project's\ngoals. MADlib is freely available at http://madlib.net, and the project is open\nfor contributions of both new methods, and ports to additional database\nplatforms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:52:27 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Hellerstein", "Joe", ""], ["R\u00e9", "Christopher", ""], ["Schoppmann", "Florian", ""], ["Wang", "Daisy Zhe", ""], ["Fratkin", "Eugene", ""], ["Gorajek", "Aleksander", ""], ["Ng", "Kee Siong", ""], ["Welton", "Caleb", ""], ["Feng", "Xixuan", ""], ["Li", "Kun", ""], ["Kumar", "Arun", ""]]}, {"id": "1208.4166", "submitter": "Avrilia Floratou", "authors": "Avrilia Floratou, Nikhil Teletia, David J. Dewitt, Jignesh M. Patel,\n  Donghui Zhang", "title": "Can the Elephants Handle the NoSQL Onslaught?", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1712-1723 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this new era of \"big data\", traditional DBMSs are under attack from two\nsides. At one end of the spectrum, the use of document store NoSQL systems\n(e.g. MongoDB) threatens to move modern Web 2.0 applications away from\ntraditional RDBMSs. At the other end of the spectrum, big data DSS analytics\nthat used to be the domain of parallel RDBMSs is now under attack by another\nclass of NoSQL data analytics systems, such as Hive on Hadoop. So, are the\ntraditional RDBMSs, aka \"big elephants\", doomed as they are challenged from\nboth ends of this \"big data\" spectrum? In this paper, we compare one\nrepresentative NoSQL system from each end of this spectrum with SQL Server, and\nanalyze the performance and scalability aspects of each of these approaches\n(NoSQL vs. SQL) on two workloads (decision support analysis and interactive\ndata-serving) that represent the two ends of the application spectrum. We\npresent insights from this evaluation and speculate on potential trends for the\nfuture.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:52:46 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Floratou", "Avrilia", ""], ["Teletia", "Nikhil", ""], ["Dewitt", "David J.", ""], ["Patel", "Jignesh M.", ""], ["Zhang", "Donghui", ""]]}, {"id": "1208.4167", "submitter": "Tilmann Rabl", "authors": "Tilmann Rabl, Mohammad Sadoghi, Hans-Arno Jacobsen, Sergio\n  G\\'omez-Villamor, Victor Munt\\'es-Mulero, Serge Mankowskii", "title": "Solving Big Data Challenges for Enterprise Application Performance\n  Management", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1724-1735 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the complexity of enterprise systems increases, the need for monitoring\nand analyzing such systems also grows. A number of companies have built\nsophisticated monitoring tools that go far beyond simple resource utilization\nreports. For example, based on instrumentation and specialized APIs, it is now\npossible to monitor single method invocations and trace individual transactions\nacross geographically distributed systems. This high-level of detail enables\nmore precise forms of analysis and prediction but comes at the price of high\ndata rates (i.e., big data). To maximize the benefit of data monitoring, the\ndata has to be stored for an extended period of time for ulterior analysis.\nThis new wave of big data analytics imposes new challenges especially for the\napplication performance monitoring systems. The monitoring data has to be\nstored in a system that can sustain the high data rates and at the same time\nenable an up-to-date view of the underlying infrastructure. With the advent of\nmodern key-value stores, a variety of data storage systems have emerged that\nare built with a focus on scalability and high data rates as predominant in\nthis monitoring use case. In this work, we present our experience and a\ncomprehensive performance evaluation of six modern (open-source) data stores in\nthe context of application performance monitoring as part of CA Technologies\ninitiative. We evaluated these systems with data and workloads that can be\nfound in application performance monitoring, as well as, on-line advertisement,\npower monitoring, and many other use cases. We present our insights not only as\nperformance results but also as lessons learned and our experience relating to\nthe setup and configuration complexity of these data stores in an industry\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:52:55 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Rabl", "Tilmann", ""], ["Sadoghi", "Mohammad", ""], ["Jacobsen", "Hans-Arno", ""], ["G\u00f3mez-Villamor", "Sergio", ""], ["Munt\u00e9s-Mulero", "Victor", ""], ["Mankowskii", "Serge", ""]]}, {"id": "1208.4168", "submitter": "Avraham Shinnar", "authors": "Avraham Shinnar, David Cunningham, Benjamin Herta, Vijay Saraswat", "title": "M3R: Increased performance for in-memory Hadoop jobs", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1736-1747 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Main Memory Map Reduce (M3R) is a new implementation of the Hadoop Map Reduce\n(HMR) API targeted at online analytics on high mean-time-to-failure clusters.\nIt does not support resilience, and supports only those workloads which can fit\ninto cluster memory. In return, it can run HMR jobs unchanged -- including jobs\nproduced by compilers for higher-level languages such as Pig, Jaql, and\nSystemML and interactive front-ends like IBM BigSheets -- while providing\nsignificantly better performance than the Hadoop engine on several workloads\n(e.g. 45x on some input sizes for sparse matrix vector multiply). M3R also\nsupports extensions to the HMR API which can enable Map Reduce jobs to run\nfaster on the M3R engine, while not affecting their performance under the\nHadoop engine.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:53:00 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Shinnar", "Avraham", ""], ["Cunningham", "David", ""], ["Herta", "Benjamin", ""], ["Saraswat", "Vijay", ""]]}, {"id": "1208.4169", "submitter": "Philipp R\\\"osch", "authors": "Philipp R\\\"osch, Lars Dannecker, Gregor Hackenbroich, Franz Faerber", "title": "A Storage Advisor for Hybrid-Store Databases", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1748-1758 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the SAP HANA database, SAP offers a high-performance in-memory\nhybrid-store database. Hybrid-store databases---that is, databases supporting\nrow- and column-oriented data management---are getting more and more prominent.\nWhile the columnar management offers high-performance capabilities for\nanalyzing large quantities of data, the row-oriented store can handle\ntransactional point queries as well as inserts and updates more efficiently. To\neffectively take advantage of both stores at the same time the novel question\nwhether to store the given data row- or column-oriented arises. We tackle this\nproblem with a storage advisor tool that supports database administrators at\nthis decision. Our proposed storage advisor recommends the optimal store based\non data and query characteristics; its core is a cost model to estimate and\ncompare query execution times for the different stores. Besides a per-table\ndecision, our tool also considers to horizontally and vertically partition the\ndata and manage the partitions on different stores. We evaluated the storage\nadvisor for the use in the SAP HANA database; we show the recommendation\nquality as well as the benefit of having the data in the optimal store with\nrespect to increased query performance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:53:09 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["R\u00f6sch", "Philipp", ""], ["Dannecker", "Lars", ""], ["Hackenbroich", "Gregor", ""], ["Faerber", "Franz", ""]]}, {"id": "1208.4170", "submitter": "Micha{\\l} \\'Switakowski", "authors": "Micha{\\l} \\'Switakowski, Peter Boncz, Marcin \\.Zukowski", "title": "From Cooperative Scans to Predictive Buffer Management", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1759-1770 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analytical applications, database systems often need to sustain workloads\nwith multiple concurrent scans hitting the same table. The Cooperative Scans\n(CScans) framework, which introduces an Active Buffer Manager (ABM) component\ninto the database architecture, has been the most effective and elaborate\nresponse to this problem, and was initially developed in the X100 research\nprototype. We now report on the the experiences of integrating Cooperative\nScans into its industrial-strength successor, the Vectorwise database product.\nDuring this implementation we invented a simpler optimization of concurrent\nscan buffer management, called Predictive Buffer Management (PBM). PBM is based\non the observation that in a workload with long-running scans, the buffer\nmanager has quite a bit of information on the workload in the immediate future,\nsuch that an approximation of the ideal OPT algorithm becomes feasible. In the\nevaluation on both synthetic benchmarks as well as a TPC-H throughput run we\ncompare the benefits of naive buffer management (LRU) versus CScans, PBM and\nOPT; showing that PBM achieves benefits close to Cooperative Scans, while\nincurring much lower architectural impact.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:53:17 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["\u015awitakowski", "Micha\u0142", ""], ["Boncz", "Peter", ""], ["\u017bukowski", "Marcin", ""]]}, {"id": "1208.4171", "submitter": "Jimmy Lin", "authors": "George Lee, Jimmy Lin, Chuang Liu, Andrew Lorek, Dmitriy Ryaboy", "title": "The Unified Logging Infrastructure for Data Analytics at Twitter", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1771-1780 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a substantial amount of work on large-scale\ndata analytics using Hadoop-based platforms running on large clusters of\ncommodity machines. A less-explored topic is how those data, dominated by\napplication logs, are collected and structured to begin with. In this paper, we\npresent Twitter's production logging infrastructure and its evolution from\napplication-specific logging to a unified \"client events\" log format, where\nmessages are captured in common, well-formatted, flexible Thrift messages.\nSince most analytics tasks consider the user session as the basic unit of\nanalysis, we pre-materialize \"session sequences\", which are compact summaries\nthat can answer a large class of common queries quickly. The development of\nthis infrastructure has streamlined log collection and data analysis, thereby\nimproving our ability to rapidly experiment and iterate on various aspects of\nthe service.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:53:45 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Lee", "George", ""], ["Lin", "Jimmy", ""], ["Liu", "Chuang", ""], ["Lorek", "Andrew", ""], ["Ryaboy", "Dmitriy", ""]]}, {"id": "1208.4172", "submitter": "Tomas Talius", "authors": "Tomas Talius, Robin Dhamankar, Andrei Dumitrache, Hanuma Kodavalla", "title": "Transaction Log Based Application Error Recovery and Point In-Time Query", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1781-1789 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database backups have traditionally been used as the primary mechanism to\nrecover from hardware and user errors. High availability solutions maintain\nredundant copies of data that can be used to recover from most failures except\nuser or application errors. Database backups are neither space nor time\nefficient for recovering from user errors which typically occur in the recent\npast and affect a small portion of the database. Moreover periodic full backups\nimpact user workload and increase storage costs. In this paper we present a\nscheme that can be used for both user and application error recovery starting\nfrom the current state and rewinding the database back in time using the\ntransaction log. While we provide a consistent view of the entire database as\nof a point in time in the past, the actual prior versions are produced only for\ndata that is accessed. We make the as of data accessible to arbitrary point in\ntime queries by integrating with the database snapshot feature in Microsoft SQL\nServer.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:53:49 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Talius", "Tomas", ""], ["Dhamankar", "Robin", ""], ["Dumitrache", "Andrei", ""], ["Kodavalla", "Hanuma", ""]]}, {"id": "1208.4173", "submitter": "Andrew Lamb", "authors": "Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, Nga Tran, Ben\n  Vandier, Lyric Doshi, Chuck Bear", "title": "The Vertica Analytic Database: C-Store 7 Years Later", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1790-1801 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the system architecture of the Vertica Analytic Database\n(Vertica), a commercialization of the design of the C-Store research prototype.\nVertica demonstrates a modern commercial RDBMS system that presents a classical\nrelational interface while at the same time achieving the high performance\nexpected from modern \"web scale\" analytic systems by making appropriate\narchitectural choices. Vertica is also an instructive lesson in how academic\nsystems research can be directly commercialized into a successful product.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:53:52 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Lamb", "Andrew", ""], ["Fuller", "Matt", ""], ["Varadarajan", "Ramakrishna", ""], ["Tran", "Nga", ""], ["Vandier", "Ben", ""], ["Doshi", "Lyric", ""], ["Bear", "Chuck", ""]]}, {"id": "1208.4174", "submitter": "Yanpei Chen", "authors": "Yanpei Chen, Sara Alspaugh, Randy Katz", "title": "Interactive Analytical Processing in Big Data Systems: A Cross-Industry\n  Study of MapReduce Workloads", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1802-1813 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the past few years, organizations in diverse industries have adopted\nMapReduce-based systems for large-scale data processing. Along with these new\nusers, important new workloads have emerged which feature many small, short,\nand increasingly interactive jobs in addition to the large, long-running batch\njobs for which MapReduce was originally designed. As interactive, large-scale\nquery processing is a strength of the RDBMS community, it is important that\nlessons from that field be carried over and applied where possible in this new\ndomain. However, these new workloads have not yet been described in the\nliterature. We fill this gap with an empirical analysis of MapReduce traces\nfrom six separate business-critical deployments inside Facebook and at Cloudera\ncustomers in e-commerce, telecommunications, media, and retail. Our key\ncontribution is a characterization of new MapReduce workloads which are driven\nin part by interactive analysis, and which make heavy use of query-like\nprogramming frameworks on top of MapReduce. These workloads display diverse\nbehaviors which invalidate prior assumptions about MapReduce such as uniform\ndata access, regular diurnal patterns, and prevalence of large jobs. A\nsecondary contribution is a first step towards creating a TPC-like data\nprocessing benchmark for MapReduce.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:53:55 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Chen", "Yanpei", ""], ["Alspaugh", "Sara", ""], ["Katz", "Randy", ""]]}, {"id": "1208.4175", "submitter": "Wang Lam", "authors": "Wang Lam, Lu Liu, STS Prasad, Anand Rajaraman, Zoheb Vacheri, AnHai\n  Doan", "title": "Muppet: MapReduce-Style Processing of Fast Data", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1814-1825 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce has emerged as a popular method to process big data. In the past\nfew years, however, not just big data, but fast data has also exploded in\nvolume and availability. Examples of such data include sensor data streams, the\nTwitter Firehose, and Facebook updates. Numerous applications must process fast\ndata. Can we provide a MapReduce-style framework so that developers can quickly\nwrite such applications and execute them over a cluster of machines, to achieve\nlow latency and high scalability? In this paper we report on our investigation\nof this question, as carried out at Kosmix and WalmartLabs. We describe\nMapUpdate, a framework like MapReduce, but specifically developed for fast\ndata. We describe Muppet, our implementation of MapUpdate. Throughout the\ndescription we highlight the key challenges, argue why MapReduce is not well\nsuited to address them, and briefly describe our current solutions. Finally, we\ndescribe our experience and lessons learned with Muppet, which has been used\nextensively at Kosmix and WalmartLabs to power a broad range of applications in\nsocial media and e-commerce.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:53:58 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Lam", "Wang", ""], ["Liu", "Lu", ""], ["Prasad", "STS", ""], ["Rajaraman", "Anand", ""], ["Vacheri", "Zoheb", ""], ["Doan", "AnHai", ""]]}, {"id": "1208.4176", "submitter": "Gabriela Jacques-Silva", "authors": "Gabriela Jacques-Silva, Bu\\u{g}ra Gedik, Rohit Wagle, Kun-Lung Wu,\n  Vibhore Kumar", "title": "Building User-defined Runtime Adaptation Routines for Stream Processing\n  Applications", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1826-1837 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream processing applications are deployed as continuous queries that run\nfrom the time of their submission until their cancellation. This deployment\nmode limits developers who need their applications to perform runtime\nadaptation, such as algorithmic adjustments, incremental job deployment, and\napplication-specific failure recovery. Currently, developers do runtime\nadaptation by using external scripts and/or by inserting operators into the\nstream processing graph that are unrelated to the data processing logic. In\nthis paper, we describe a component called orchestrator that allows users to\nwrite routines for automatically adapting the application to runtime\nconditions. Developers build an orchestrator by registering and handling events\nas well as specifying actuations. Events can be generated due to changes in the\nsystem state (e.g., application component failures), built-in system metrics\n(e.g., throughput of a connection), or custom application metrics (e.g.,\nquality score). Once the orchestrator receives an event, users can take\nadaptation actions by using the orchestrator actuation APIs. We demonstrate the\nuse of the orchestrator in IBM's System S in the context of three different\napplications, illustrating application adaptation to changes on the incoming\ndata distribution, to application failures, and on-demand dynamic composition.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:54:03 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Jacques-Silva", "Gabriela", ""], ["Gedik", "Bu\u011fra", ""], ["Wagle", "Rohit", ""], ["Wu", "Kun-Lung", ""], ["Kumar", "Vibhore", ""]]}, {"id": "1208.4178", "submitter": "Hongji Bao", "authors": "Junchen Jiang, Hongji Bao, Edward Y. Chang, Yuqian Li", "title": "MOIST: A Scalable and Parallel Moving Object Indexer with School\n  Tracking", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1838-1849 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location-Based Service (LBS) is rapidly becoming the next ubiquitous\ntechnology for a wide range of mobile applications. To support applications\nthat demand nearest-neighbor and history queries, an LBS spatial indexer must\nbe able to efficiently update, query, archive and mine location records, which\ncan be in contention with each other. In this work, we propose MOIST, whose\nbaseline is a recursive spatial partitioning indexer built upon BigTable. To\nreduce update and query contention, MOIST groups nearby objects of similar\ntrajectory into the same school, and keeps track of only the history of school\nleaders. This dynamic clustering scheme can eliminate redundant updates and\nhence reduce update latency. To improve history query processing, MOIST keeps\nsome history data in memory, while it flushes aged data onto parallel disks in\na locality-preserving way. Through experimental studies, we show that MOIST can\nsupport highly efficient nearest-neighbor and history queries and can scale\nwell with an increasing number of users and update frequency.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:56:43 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Jiang", "Junchen", ""], ["Bao", "Hongji", ""], ["Chang", "Edward Y.", ""], ["Li", "Yuqian", ""]]}, {"id": "1208.4179", "submitter": "Dan Ports", "authors": "Dan R. K. Ports, Kevin Grittner", "title": "Serializable Snapshot Isolation in PostgreSQL", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.\n  1850-1861 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our experience implementing PostgreSQL's new\nserializable isolation level. It is based on the recently-developed\nSerializable Snapshot Isolation (SSI) technique. This is the first\nimplementation of SSI in a production database release as well as the first in\na database that did not previously have a lock-based serializable isolation\nlevel. We reflect on our experience and describe how we overcame some of the\nresulting challenges, including the implementation of a new lock manager, a\ntechnique for ensuring memory usage is bounded, and integration with other\nPostgreSQL features. We also introduce an extension to SSI that improves\nperformance for read-only transactions. We evaluate PostgreSQL's serializable\nisolation level using several benchmarks and show that it achieves performance\nonly slightly below that of snapshot isolation, and significantly outperforms\nthe traditional two-phase locking approach on read-intensive workloads.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 02:56:46 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Ports", "Dan R. K.", ""], ["Grittner", "Kevin", ""]]}, {"id": "1208.4270", "submitter": "Tae-Seob Yun", "authors": "Kyu-Young Whang, Tae-Seob Yun, Yeon-Mi Yeo, Il-Yeol Song, Hyuk-Yoon\n  Kwon, In-Joong Kim", "title": "ODYS: A Massively-Parallel Search Engine Using a DB-IR\n  Tightly-Integrated Parallel DBMS", "comments": "34 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, parallel search engines have been implemented based on scalable\ndistributed file systems such as Google File System. However, we claim that\nbuilding a massively-parallel search engine using a parallel DBMS can be an\nattractive alternative since it supports a higher-level (i.e., SQL-level)\ninterface than that of a distributed file system for easy and less error-prone\napplication development while providing scalability. In this paper, we propose\na new approach of building a massively-parallel search engine using a DB-IR\ntightly-integrated parallel DBMS and demonstrate its commercial-level\nscalability and performance. In addition, we present a hybrid (i.e., analytic\nand experimental) performance model for the parallel search engine. We have\nbuilt a five-node parallel search engine according to the proposed architecture\nusing a DB-IR tightly-integrated DBMS. Through extensive experiments, we show\nthe correctness of the model by comparing the projected output with the\nexperimental results of the five-node engine. Our model demonstrates that ODYS\nis capable of handling 1 billion queries per day (81 queries/sec) for 30\nbillion web pages by using only 43,472 nodes with an average query response\ntime of 211 ms, which is equivalent to or better than those of commercial\nsearch engines. We also show that, by using twice as many (86,944) nodes, ODYS\ncan provide an average query response time of 162 ms, which is significantly\nlower than those of commercial search engines.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 14:01:36 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Whang", "Kyu-Young", ""], ["Yun", "Tae-Seob", ""], ["Yeo", "Yeon-Mi", ""], ["Song", "Il-Yeol", ""], ["Kwon", "Hyuk-Yoon", ""], ["Kim", "In-Joong", ""]]}, {"id": "1208.4634", "submitter": "EPTCS", "authors": "Gabriel Ciobanu (Romanian Academy, Institute of Computer Science),\n  Ross Horne (Romanian Academy, Institute of Computer Science)", "title": "A Provenance Tracking Model for Data Updates", "comments": "In Proceedings FOCLASA 2012, arXiv:1208.4327", "journal-ref": "EPTCS 91, 2012, pp. 31-44", "doi": "10.4204/EPTCS.91.3", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For data-centric systems, provenance tracking is particularly important when\nthe system is open and decentralised, such as the Web of Linked Data. In this\npaper, a concise but expressive calculus which models data updates is\npresented. The calculus is used to provide an operational semantics for a\nsystem where data and updates interact concurrently. The operational semantics\nof the calculus also tracks the provenance of data with respect to updates.\nThis provides a new formal semantics extending provenance diagrams which takes\ninto account the execution of processes in a concurrent setting. Moreover, a\nsound and complete model for the calculus based on ideals of series-parallel\nDAGs is provided. The notion of provenance introduced can be used as a\nsubjective indicator of the quality of data in concurrent interacting systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2012 22:00:51 GMT"}], "update_date": "2012-08-24", "authors_parsed": [["Ciobanu", "Gabriel", "", "Romanian Academy, Institute of Computer Science"], ["Horne", "Ross", "", "Romanian Academy, Institute of Computer Science"]]}, {"id": "1208.4809", "submitter": "Husnabad Venkateswara  Reddy", "authors": "H. Venkateswara Reddy, Dr.S.Viswanadha Raju, B.Ramasubba Reddy", "title": "Comparing N-Node Set Importance Representative results with Node\n  Importance Representative results for Categorical Clustering: An exploratory\n  study", "comments": "16 pages, 4 figures, 3 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The proportionate increase in the size of the data with increase in space\nimplies that clustering a very large data set becomes difficult and is a time\nconsuming process.Sampling is one important technique to scale down the size of\ndataset and to improve the efficiency of clustering. After sampling allocating\nunlabeled objects into proper clusters is impossible in the categorical\ndomain.To address the problem, Chen employed a method called MAximal\nRepresentative Data Labeling to allocate each unlabeled data point to the\nappropriate cluster based on Node Importance Representative and N-Node\nImportance Representative algorithms. This paper took off from Chen s\ninvestigation and analyzed and compared the results of NIR and NNIR leading to\nthe conclusion that the two processes contradict each other when it comes to\nfinding the resemblance between an unlabeled data point and a cluster.A new and\nbetter way of solving the problem was arrived at that finds resemblance between\nunlabeled data point within all clusters, while also providing maximal\nresemblance for allocation of data in the required cluster.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2012 17:32:32 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Reddy", "H. Venkateswara", ""], ["Raju", "Dr. S. Viswanadha", ""], ["Reddy", "B. Ramasubba", ""]]}, {"id": "1208.5258", "submitter": "Daniel Yang Li", "authors": "Chao Li and Daniel Yang Li and Gerome Miklau and Dan Suciu", "title": "A Theory of Pricing Private Data", "comments": "25 pages, 2 figures. Best Paper Award, to appear in the 16th\n  International Conference on Database Theory (ICDT), 2013", "journal-ref": "ICDT '13 Proceedings of the 16th International Conference on\n  Database Theory Pages 33-44, 2013", "doi": "10.1145/2448496.2448502", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal data has value to both its owner and to institutions who would like\nto analyze it. Privacy mechanisms protect the owner's data while releasing to\nanalysts noisy versions of aggregate query results. But such strict protections\nof individual's data have not yet found wide use in practice. Instead, Internet\ncompanies, for example, commonly provide free services in return for valuable\nsensitive information from users, which they exploit and sometimes sell to\nthird parties.\n  As the awareness of the value of the personal data increases, so has the\ndrive to compensate the end user for her private information. The idea of\nmonetizing private data can improve over the narrower view of hiding private\ndata, since it empowers individuals to control their data through financial\nmeans.\n  In this paper we propose a theoretical framework for assigning prices to\nnoisy query answers, as a function of their accuracy, and for dividing the\nprice amongst data owners who deserve compensation for their loss of privacy.\nOur framework adopts and extends key principles from both differential privacy\nand query pricing in data markets. We identify essential properties of the\nprice function and micro-payments, and characterize valid solutions.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2012 22:08:11 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2012 20:55:11 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Li", "Chao", ""], ["Li", "Daniel Yang", ""], ["Miklau", "Gerome", ""], ["Suciu", "Dan", ""]]}, {"id": "1208.5443", "submitter": "Bing-Rong Lin", "authors": "Bing-Rong Lin and Daniel Kifer", "title": "A Framework for Extracting Semantic Guarantees from Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical privacy views privacy definitions as contracts that guide the\nbehavior of algorithms that take in sensitive data and produce sanitized data.\nFor most existing privacy definitions, it is not clear what they actually\nguarantee.\n  In this paper, we propose the first (to the best of our knowledge) framework\nfor extracting semantic guarantees from privacy definitions. That is, instead\nof answering narrow questions such as \"does privacy definition Y protect X?\"\nthe goal is to answer the more general question \"what does privacy definition Y\nprotect?\"\n  The privacy guarantees we can extract are Bayesian in nature and deal with\nchanges in an attacker's beliefs. The key to our framework is an object we call\nthe row cone. Every privacy definition has a row cone, which is a convex set\nthat describes all the ways an attacker's prior beliefs can be turned into\nposterior beliefs after observing an output of an algorithm satisfying that\nprivacy definition.\n  The framework can be applied to privacy definitions or even to individual\nalgorithms to identify the types of inferences they defend against. We\nillustrate the use of our framework with analyses of several definitions and\nalgorithms for which we can derive previously unknown semantics. These include\nrandomized response, FRAPP, and several algorithms that add integer-valued\nnoise to their inputs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2012 16:50:53 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Lin", "Bing-Rong", ""], ["Kifer", "Daniel", ""]]}, {"id": "1208.5745", "submitter": "Sushovan De", "authors": "Rohit Raghunathan, Sushovan De, Subbarao Kambhampati", "title": "Bayes Networks for Supporting Query Processing Over Incomplete\n  Autonomous Databases", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the information available to lay users through autonomous data sources\ncontinues to increase, mediators become important to ensure that the wealth of\ninformation available is tapped effectively. A key challenge that these\ninformation mediators need to handle is the varying levels of incompleteness in\nthe underlying databases in terms of missing attribute values. Existing\napproaches such as QPIAD aim to mine and use Approximate Functional\nDependencies (AFDs) to predict and retrieve relevant incomplete tuples. These\napproaches make independence assumptions about missing values---which\ncritically hobbles their performance when there are tuples containing missing\nvalues for multiple correlated attributes. In this paper, we present a\nprincipled probabilistic alternative that views an incomplete tuple as defining\na distribution over the complete tuples that it stands for. We learn this\ndistribution in terms of Bayes networks. Our approach involves\nmining/\"learning\" Bayes networks from a sample of the database, and using it to\ndo both imputation (predict a missing value) and query rewriting (retrieve\nrelevant results with incompleteness on the query-constrained attributes, when\nthe data sources are autonomous). We present empirical studies to demonstrate\nthat (i) at higher levels of incompleteness, when multiple attribute values are\nmissing, Bayes networks do provide a significantly higher classification\naccuracy and (ii) the relevant possible answers retrieved by the queries\nreformulated using Bayes networks provide higher precision and recall than AFDs\nwhile keeping query processing costs manageable.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2012 18:52:12 GMT"}], "update_date": "2012-08-29", "authors_parsed": [["Raghunathan", "Rohit", ""], ["De", "Sushovan", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1208.6416", "submitter": "Samson Abramsky", "authors": "Samson Abramsky", "title": "Relational Databases and Bell's Theorem", "comments": "19 pages. To appear in Festschrift for Peter Buneman", "journal-ref": "In Search of Elegance in the Theory and Practice of Computation:\n  Essays dedicated to Peter Buneman, ed. V. Tannen, L. Wong, L. Libkin, W. Fan,\n  W.C. Tan and M. Fourman, Springer, pages 13-35, 2013", "doi": null, "report-no": null, "categories": "cs.LO cs.DB quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim in this paper is to point out a surprising formal connection, between\ntwo topics which seem on face value to have nothing to do with each other:\nrelational database theory, and the study of non-locality and contextuality in\nthe foundations of quantum mechanics. We shall show that there is a remarkably\ndirect correspondence between central results such as Bell's theorem in the\nfoundations of quantum mechanics, and questions which arise naturally and have\nbeen well-studied in relational database theory.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 08:20:32 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2013 10:51:01 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["Abramsky", "Samson", ""]]}]