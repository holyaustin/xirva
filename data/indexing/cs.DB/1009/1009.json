[{"id": "1009.0255", "submitter": "Flavio Rizzolo", "authors": "Flavio Rizzolo, Iluju Kiringa, Rachel Pottinger, Kwok Wong", "title": "The Conceptual Integration Modeling Framework: Abstracting from the\n  Multidimensional Model", "comments": "Technical report, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data warehouses are overwhelmingly built through a bottom-up process, which\nstarts with the identification of sources, continues with the extraction and\ntransformation of data from these sources, and then loads the data into a set\nof data marts according to desired multidimensional relational schemas. End\nuser business intelligence tools are added on top of the materialized\nmultidimensional schemas to drive decision making in an organization.\nUnfortunately, this bottom-up approach is costly both in terms of the skilled\nusers needed and the sheer size of the warehouses. This paper proposes a\ntop-down framework in which data warehousing is driven by a conceptual model.\nThe framework offers both design time and run time environments. At design\ntime, a business user first uses the conceptual modeling language as a\nmultidimensional object model to specify what business information is needed;\nthen she maps the conceptual model to a pre-existing logical multidimensional\nrepresentation. At run time, a system will transform the user conceptual model\ntogether with the mappings into views over the logical multidimensional\nrepresentation. We focus on how the user can conceptually abstract from an\nexisting data warehouse, and on how this conceptual model can be mapped to the\nlogical multidimensional representation. We also give an indication of what\nquery language is used over the conceptual model. Finally, we argue that our\nframework is a step along the way to allowing automatic generation of the data\nwarehouse.\n", "versions": [{"version": "v1", "created": "Wed, 1 Sep 2010 19:27:40 GMT"}], "update_date": "2010-09-02", "authors_parsed": [["Rizzolo", "Flavio", ""], ["Kiringa", "Iluju", ""], ["Pottinger", "Rachel", ""], ["Wong", "Kwok", ""]]}, {"id": "1009.0368", "submitter": "Sandeep Singh rawat", "authors": "Sandeep Singh Rawat and Lakshmi Rajamani", "title": "Discovering potential user browsing behaviors using custom-built apriori\n  algorithm", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the organizations put information on the web because they want it to\nbe seen by the world. Their goal is to have visitors come to the site, feel\ncomfortable and stay a while and try to know completely about the running\norganization. As educational system increasingly requires data mining, the\nopportunity arises to mine the resulting large amounts of student information\nfor hidden useful information (patterns like rule, clustering, and\nclassification, etc). The education domain offers ground for many interesting\nand challenging data mining applications like astronomy, chemistry,\nengineering, climate studies, geology, oceanography, ecology, physics, biology,\nhealth sciences and computer science. Collecting the interesting patterns using\nthe required interestingness measures, which help us in discovering the\nsophisticated patterns that are ultimately used for developing the site. We\nstudy the application of data mining to educational log data collected from\nGuru Nanak Institute of Technology, Ibrahimpatnam, India. We have proposed a\ncustom-built apriori algorithm to find the effective pattern analysis. Finally,\nanalyzing web logs for usage and access trends can not only provide important\ninformation to web site developers and administrators, but also help in\ncreating adaptive web sites.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 10:00:51 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Rawat", "Sandeep Singh", ""], ["Rajamani", "Lakshmi", ""]]}, {"id": "1009.0384", "submitter": "Rahmat Widia Sembiring", "authors": "Rahmat Widia Sembiring, Jasni Mohamad Zain, Abdullah Embong", "title": "Clustering high dimensional data using subspace and projected clustering\n  algorithms", "comments": "9 pages, 6 figures", "journal-ref": "International journal of computer science & information Technology\n  (IJCSIT) Vol.2, No.4, August 2010, p.162-170", "doi": "10.5121/ijcsit.2010.2414", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem statement: Clustering has a number of techniques that have been\ndeveloped in statistics, pattern recognition, data mining, and other fields.\nSubspace clustering enumerates clusters of objects in all subspaces of a\ndataset. It tends to produce many over lapping clusters. Approach: Subspace\nclustering and projected clustering are research areas for clustering in high\ndimensional spaces. In this research we experiment three clustering oriented\nalgorithms, PROCLUS, P3C and STATPC. Results: In general, PROCLUS performs\nbetter in terms of time of calculation and produced the least number of\nun-clustered data while STATPC outperforms PROCLUS and P3C in the accuracy of\nboth cluster points and relevant attributes found. Conclusions/Recommendations:\nIn this study, we analyze in detail the properties of different data clustering\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 10:47:11 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Sembiring", "Rahmat Widia", ""], ["Zain", "Jasni Mohamad", ""], ["Embong", "Abdullah", ""]]}, {"id": "1009.0397", "submitter": "Wided Oueslati wided", "authors": "wided oueslati and jalel akaichi", "title": "Mobile Information Collectors' Trajectory Data Warehouse Design", "comments": "20 pages, 9 figures", "journal-ref": "international journal of managing information technology august\n  2010", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  To analyze complex phenomena which involve moving objects, Trajectory Data\nWarehouse (TDW) seems to be an answer for many recent decision problems related\nto various professions (physicians, commercial representatives, transporters,\necologists ...) concerned with mobility. This work aims to make trajectories as\na first class concept in the trajectory data conceptual model and to design a\nTDW, in which data resulting from mobile information collectors' trajectory are\ngathered. These data will be analyzed, according to trajectory characteristics,\nfor decision making purposes, such as new products commercialization, new\ncommerce implementation, etc.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 11:41:29 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["oueslati", "wided", ""], ["akaichi", "jalel", ""]]}, {"id": "1009.0827", "submitter": "Hassan Rashidi", "authors": "Hamed khataeimaragheh and Hassan Rashidi", "title": "A Novel Watermarking Scheme for Detecting and Recovering Distortions in\n  Database Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a novel fragile watermarking scheme is proposed to detect,\nlocalize and recover malicious modifications in relational databases. In the\nproposed scheme, all tuples in the database are first securely divided into\ngroups. Then watermarks are embedded and verified group-by-group independently.\nBy using the embedded watermark, we are able to detect and localize the\nmodification made to the database and even we recover the true data from the\ndatabase modified locations. Our experimental results show that this scheme is\nso qualified; i.e. distortion detection and true data recovery both are\nperformed successfully.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 11:29:55 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["khataeimaragheh", "Hamed", ""], ["Rashidi", "Hassan", ""]]}, {"id": "1009.0929", "submitter": "Hao-En Chueh", "authors": "Hao-En Chueh", "title": "Mining Target-Oriented Sequential Patterns with Time-Intervals", "comments": "11 pages, 9 tables", "journal-ref": "International journal of computer science & information Technology\n  (IJCSIT) Vol.2, No.4, August 2010", "doi": "10.5121/ijcsit.2010.2410", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A target-oriented sequential pattern is a sequential pattern with a concerned\nitemset in the end of pattern. A time-interval sequential pattern is a\nsequential pattern with time-intervals between every pair of successive\nitemsets. In this paper we present an algorithm to discover target-oriented\nsequential pattern with time-intervals. To this end, the original sequences are\nreversed so that the last itemsets can be arranged in front of the sequences.\nThe contrasts between reversed sequences and the concerned itemset are then\nused to exclude the irrelevant sequences. Clustering analysis is used with\ntypical sequential pattern mining algorithm to extract the sequential patterns\nwith time-intervals between successive itemsets. Finally, the discovered\ntime-interval sequential patterns are reversed again to the original order for\nsearching the target patterns.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 16:48:59 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Chueh", "Hao-En", ""]]}, {"id": "1009.0971", "submitter": "Kiran Kumar B", "authors": "B. Kiran Kumar and A. Bhaskar (Department of M.C.A., Kakatiya\n  Institute of Technology & Science, A.P., INDIA)", "title": "ETP-Mine: An Efficient Method for Mining Transitional Patterns", "comments": "11 pages", "journal-ref": "International Journal of Database Management Systems (IJDMS) ISSN\n  : 0975-5705, August,2010", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Transaction database contains a set of transactions along with items and\ntheir associated timestamps. Transitional patterns are the patterns which\nspecify the dynamic behavior of frequent patterns in a transaction database. To\ndiscover transitional patterns and their significant milestones, first we have\nto extract all frequent patterns and their supports using any frequent pattern\ngeneration algorithm. These frequent patterns are used in the generation of\ntransitional patterns. The existing algorithm (TP-Mine) generates frequent\npatterns, some of which cannot be used in generation of transitional patterns.\nIn this paper, we propose a modification to the existing algorithm, which\nprunes the candidate items to be used in the generation of frequent patterns.\nThis method drastically reduces the number of frequent patterns which are used\nin discovering transitional patterns. Extensive simulation test is done to\nevaluate the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 6 Sep 2010 05:46:18 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Kumar", "B. Kiran", "", "Department of M.C.A., Kakatiya\n  Institute of Technology & Science, A.P., INDIA"], ["Bhaskar", "A.", "", "Department of M.C.A., Kakatiya\n  Institute of Technology & Science, A.P., INDIA"]]}, {"id": "1009.1166", "submitter": "David Spivak", "authors": "David I. Spivak", "title": "Functorial Data Migration", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB math.CT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we present a simple database definition language: that of\ncategories and functors. A database schema is a small category and an instance\nis a set-valued functor on it. We show that morphisms of schemas induce three\n\"data migration functors\", which translate instances from one schema to the\nother in canonical ways. These functors parameterize projections, unions, and\njoins over all tables simultaneously and can be used in place of conjunctive\nand disjunctive queries. We also show how to connect a database and a\nfunctional programming language by introducing a functorial connection between\nthe schema and the category of types for that language. We begin the paper with\na multitude of examples to motivate the definitions, and near the end we\nprovide a dictionary whereby one can translate database concepts into\ncategory-theoretic concepts and vice-versa.\n", "versions": [{"version": "v1", "created": "Mon, 6 Sep 2010 21:05:23 GMT"}, {"version": "v2", "created": "Tue, 14 Sep 2010 13:49:04 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2012 18:37:02 GMT"}, {"version": "v4", "created": "Sun, 3 Feb 2013 00:46:59 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Spivak", "David I.", ""]]}, {"id": "1009.2021", "submitter": "Alexandra Meliou", "authors": "Alexandra Meliou and Wolfgang Gatterbauer and Katherine F. Moore and\n  Dan Suciu", "title": "The Complexity of Causality and Responsibility for Query Answers and\n  non-Answers", "comments": "15 pages, 12 figures, PVLDB 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An answer to a query has a well-defined lineage expression (alternatively\ncalled how-provenance) that explains how the answer was derived. Recent work\nhas also shown how to compute the lineage of a non-answer to a query. However,\nthe cause of an answer or non-answer is a more subtle notion and consists, in\ngeneral, of only a fragment of the lineage. In this paper, we adapt Halpern,\nPearl, and Chockler's recent definitions of causality and responsibility to\ndefine the causes of answers and non-answers to queries, and their degree of\nresponsibility. Responsibility captures the notion of degree of causality and\nserves to rank potentially many causes by their relative contributions to the\neffect. Then, we study the complexity of computing causes and responsibilities\nfor conjunctive queries. It is known that computing causes is NP-complete in\ngeneral. Our first main result shows that all causes to conjunctive queries can\nbe computed by a relational query which may involve negation. Thus, causality\ncan be computed in PTIME, and very efficiently so. Next, we study computing\nresponsibility. Here, we prove that the complexity depends on the conjunctive\nquery and demonstrate a dichotomy between PTIME and NP-complete cases. For the\nPTIME cases, we give a non-trivial algorithm, consisting of a reduction to the\nmax-flow computation problem. Finally, we prove that, even when it is in PTIME,\nresponsibility is complete for LOGSPACE, implying that, unlike causality, it\ncannot be computed by a relational query.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 14:35:30 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2011 00:18:41 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Meliou", "Alexandra", ""], ["Gatterbauer", "Wolfgang", ""], ["Moore", "Katherine F.", ""], ["Suciu", "Dan", ""]]}, {"id": "1009.2270", "submitter": "Luciano Caroprese", "authors": "L. Caroprese and M. Truszczynski", "title": "Active Integrity Constraints and Revision Programming", "comments": "48 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study active integrity constraints and revision programming, two\nformalisms designed to describe integrity constraints on databases and to\nspecify policies on preferred ways to enforce them. Unlike other more commonly\naccepted approaches, these two formalisms attempt to provide a declarative\nsolution to the problem. However, the original semantics of founded repairs for\nactive integrity constraints and justified revisions for revision programs\ndiffer. Our main goal is to establish a comprehensive framework of semantics\nfor active integrity constraints, to find a parallel framework for revision\nprograms, and to relate the two. By doing so, we demonstrate that the two\nformalisms proposed independently of each other and based on different\nintuitions when viewed within a broader semantic framework turn out to be\nnotational variants of each other. That lends support to the adequacy of the\nsemantics we develop for each of the formalisms as the foundation for a\ndeclarative approach to the problem of database update and repair. In the paper\nwe also study computational properties of the semantics we consider and\nestablish results concerned with the concept of the minimality of change and\nthe invariance under the shifting transformation.\n", "versions": [{"version": "v1", "created": "Sun, 12 Sep 2010 22:14:04 GMT"}], "update_date": "2010-09-14", "authors_parsed": [["Caroprese", "L.", ""], ["Truszczynski", "M.", ""]]}, {"id": "1009.2764", "submitter": "Karl Malbrain", "authors": "Karl Malbrain", "title": "A Blink Tree latch method and protocol to support synchronous node\n  deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Blink Tree latch method and protocol supports synchronous node deletion in\na high concurrency environment. Full source code is available.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 20:15:14 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 19:55:33 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Malbrain", "Karl", ""]]}, {"id": "1009.3665", "submitter": "Amitabh Chaudhary", "authors": "Tanu Malik, Xiaodan Wang, Philip Little, Amitabh Chaudhary, and Ani\n  Thakar", "title": "A Dynamic Data Middleware Cache for Rapidly-growing Scientific\n  Repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern scientific repositories are growing rapidly in size. Scientists are\nincreasingly interested in viewing the latest data as part of query results.\nCurrent scientific middleware cache systems, however, assume repositories are\nstatic. Thus, they cannot answer scientific queries with the latest data. The\nqueries, instead, are routed to the repository until data at the cache is\nrefreshed. In data-intensive scientific disciplines, such as astronomy,\nindiscriminate query routing or data refreshing often results in runaway\nnetwork costs. This severely affects the performance and scalability of the\nrepositories and makes poor use of the cache system. We present Delta, a\ndynamic data middleware cache system for rapidly-growing scientific\nrepositories. Delta's key component is a decision framework that adaptively\ndecouples data objects---choosing to keep some data object at the cache, when\nthey are heavily queried, and keeping some data objects at the repository, when\nthey are heavily updated. Our algorithm profiles incoming workload to search\nfor optimal data decoupling that reduces network costs. It leverages formal\nconcepts from the network flow problem, and is robust to evolving scientific\nworkloads. We evaluate the efficacy of Delta, through a prototype\nimplementation, by running query traces collected from a real astronomy survey.\n", "versions": [{"version": "v1", "created": "Sun, 19 Sep 2010 22:26:23 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Malik", "Tanu", ""], ["Wang", "Xiaodan", ""], ["Little", "Philip", ""], ["Chaudhary", "Amitabh", ""], ["Thakar", "Ani", ""]]}, {"id": "1009.3712", "submitter": "EPTCS", "authors": "Raymond Mui, Phyllis Frankl", "title": "Preventing SQL Injection through Automatic Query Sanitization with\n  ASSIST", "comments": "In Proceedings TAV-WEB 2010, arXiv:1009.3306", "journal-ref": "EPTCS 35, 2010, pp. 27-38", "doi": "10.4204/EPTCS.35.3", "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web applications are becoming an essential part of our everyday lives. Many\nof our activities are dependent on the functionality and security of these\napplications. As the scale of these applications grows, injection\nvulnerabilities such as SQL injection are major security challenges for\ndevelopers today. This paper presents the technique of automatic query\nsanitization to automatically remove SQL injection vulnerabilities in code. In\nour technique, a combination of static analysis and program transformation are\nused to automatically instrument web applications with sanitization code. We\nhave implemented this technique in a tool named ASSIST (Automatic and Static\nSQL Injection Sanitization Tool) for protecting Java-based web applications.\nOur experimental evaluation showed that our technique is effective against SQL\ninjection vulnerabilities and has a low overhead.\n", "versions": [{"version": "v1", "created": "Mon, 20 Sep 2010 07:19:32 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Mui", "Raymond", ""], ["Frankl", "Phyllis", ""]]}, {"id": "1009.3713", "submitter": "EPTCS", "authors": "Xiang Fu", "title": "Relational Constraint Driven Test Case Synthesis for Web Applications", "comments": "In Proceedings TAV-WEB 2010, arXiv:1009.3306", "journal-ref": "EPTCS 35, 2010, pp. 39-50", "doi": "10.4204/EPTCS.35.4", "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a relational constraint driven technique that synthesizes\ntest cases automatically for web applications. Using a static analysis,\nservlets can be modeled as relational transducers, which manipulate backend\ndatabases. We present a synthesis algorithm that generates a sequence of HTTP\nrequests for simulating a user session. The algorithm relies on backward\nsymbolic image computation for reaching a certain database state, given a code\ncoverage objective. With a slight adaptation, the technique can be used for\ndiscovering workflow attacks on web applications.\n", "versions": [{"version": "v1", "created": "Mon, 20 Sep 2010 07:19:38 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Fu", "Xiang", ""]]}, {"id": "1009.4582", "submitter": "S. M. Kamruzzaman", "authors": "Chowdhury Mofizur Rahman, Ferdous Ahmed Sohel, Parvez Naushad, and S.\n  M. Kamruzzaman", "title": "Text Classification using the Concept of Association Rule of Data Mining", "comments": "8 Pages, International Conference", "journal-ref": "Proc. International Conference on Information Technology,\n  Kathmandu, Nepal, pp. 234-241, May. 2003", "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the amount of online text increases, the demand for text classification to\naid the analysis and management of text is increasing. Text is cheap, but\ninformation, in the form of knowing what classes a text belongs to, is\nexpensive. Automatic classification of text can provide this information at low\ncost, but the classifiers themselves must be built with expensive human effort,\nor trained from texts which have themselves been manually classified. In this\npaper we will discuss a procedure of classifying text using the concept of\nassociation rule of data mining. Association rule mining technique has been\nused to derive feature set from pre-classified text documents. Naive Bayes\nclassifier is then used on derived features for final classification.\n", "versions": [{"version": "v1", "created": "Thu, 23 Sep 2010 11:32:16 GMT"}], "update_date": "2010-09-27", "authors_parsed": [["Rahman", "Chowdhury Mofizur", ""], ["Sohel", "Ferdous Ahmed", ""], ["Naushad", "Parvez", ""], ["Kamruzzaman", "S. M.", ""]]}, {"id": "1009.4976", "submitter": "S. M. Kamruzzaman", "authors": "S. M. Kamruzzaman, Farhana Haider, and Ahmed Ryadh Hasan", "title": "Text Classification using Association Rule with a Hybrid Concept of\n  Naive Bayes Classifier and Genetic Algorithm", "comments": "6 Pages, International Conference", "journal-ref": "Proc. 7th International Conference on Computer and Information\n  Technology (ICCIT-2004), Dhaka, Bangladesh, pp. 682-687, Dec. 2004", "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is the automated assignment of natural language texts to\npredefined categories based on their content. Text classification is the\nprimary requirement of text retrieval systems, which retrieve texts in response\nto a user query, and text understanding systems, which transform text in some\nway such as producing summaries, answering questions or extracting data. Now a\nday the demand of text classification is increasing tremendously. Keeping this\ndemand into consideration, new and updated techniques are being developed for\nthe purpose of automated text classification. This paper presents a new\nalgorithm for text classification. Instead of using words, word relation i.e.\nassociation rules is used to derive feature set from pre-classified text\ndocuments. The concept of Naive Bayes Classifier is then used on derived\nfeatures and finally a concept of Genetic Algorithm has been added for final\nclassification. A system based on the proposed algorithm has been implemented\nand tested. The experimental results show that the proposed system works as a\nsuccessful text classifier.\n", "versions": [{"version": "v1", "created": "Sat, 25 Sep 2010 06:10:33 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Kamruzzaman", "S. M.", ""], ["Haider", "Farhana", ""], ["Hasan", "Ahmed Ryadh", ""]]}, {"id": "1009.4987", "submitter": "S. M. Kamruzzaman", "authors": "S. M. Kamruzzaman, Farhana Haider, and Ahmed Ryadh Hasan", "title": "Text Classification using Data Mining", "comments": "19 Pages, International Conference", "journal-ref": "Proc. International Conference on Information and Communication\n  Technology in Management (ICTM-2005), Multimedia University, Malaysia, May\n  2005", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is the process of classifying documents into predefined\ncategories based on their content. It is the automated assignment of natural\nlanguage texts to predefined categories. Text classification is the primary\nrequirement of text retrieval systems, which retrieve texts in response to a\nuser query, and text understanding systems, which transform text in some way\nsuch as producing summaries, answering questions or extracting data. Existing\nsupervised learning algorithms to automatically classify text need sufficient\ndocuments to learn accurately. This paper presents a new algorithm for text\nclassification using data mining that requires fewer documents for training.\nInstead of using words, word relation i.e. association rules from these words\nis used to derive feature set from pre-classified text documents. The concept\nof Naive Bayes classifier is then used on derived features and finally only a\nsingle concept of Genetic Algorithm has been added for final classification. A\nsystem based on the proposed algorithm has been implemented and tested. The\nexperimental results show that the proposed system works as a successful text\nclassifier.\n", "versions": [{"version": "v1", "created": "Sat, 25 Sep 2010 07:27:46 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Kamruzzaman", "S. M.", ""], ["Haider", "Farhana", ""], ["Hasan", "Ahmed Ryadh", ""]]}, {"id": "1009.4994", "submitter": "S. M. Kamruzzaman", "authors": "S M Kamruzzaman and Chowdhury Mofizur Rahman", "title": "Text Categorization using Association Rule and Naive Bayes Classifier", "comments": "9 Pages, International Journal", "journal-ref": "Asian Journal of Information Technology, Vol. 3, No. 9, pp\n  657-665, Sep. 2004", "doi": "10.3923/ajit.2004.657.665", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the amount of online text increases, the demand for text categorization to\naid the analysis and management of text is increasing. Text is cheap, but\ninformation, in the form of knowing what classes a text belongs to, is\nexpensive. Automatic categorization of text can provide this information at low\ncost, but the classifiers themselves must be built with expensive human effort,\nor trained from texts which have themselves been manually classified. Text\ncategorization using Association Rule and Na\\\"ive Bayes Classifier is proposed\nhere. Instead of using words word relation i.e association rules from these\nwords is used to derive feature set from pre-classified text documents. Naive\nBayes Classifier is then used on derived features for final categorization.\n", "versions": [{"version": "v1", "created": "Sat, 25 Sep 2010 08:05:21 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Kamruzzaman", "S M", ""], ["Rahman", "Chowdhury Mofizur", ""]]}, {"id": "1009.5149", "submitter": "Eya Ben Ahmed", "authors": "Eya ben Ahmed and Mohamed Salah Gouider", "title": "Towards an incremental maintenance of cyclic association rules", "comments": null, "journal-ref": "International Journal of Database Management Systems (IJDMS),\n  November 2010, Volume 2, Number 4", "doi": null, "report-no": "November 2010, Volume 2, Number 4", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the cyclic association rules have been introduced in order to\ndiscover rules from items characterized by their regular variation over time.\nIn real life situations, temporal databases are often appended or updated.\nRescanning the whole database every time is highly expensive while existing\nincremental mining techniques can efficiently solve such a problem. In this\npaper, we propose an incremental algorithm for cyclic association rules\nmaintenance. The carried out experiments of our proposal stress on its\nefficiency and performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 03:10:28 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Ahmed", "Eya ben", ""], ["Gouider", "Mohamed Salah", ""]]}, {"id": "1009.5233", "submitter": "Nassib Nassar", "authors": "Nassib Nassar", "title": "A Simple Abstraction for Data Modeling", "comments": "10 pages, 2 figures, LaTeX; added two definitions in section 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problems that scientists face in creating well designed databases\nintersect with the concerns of data curation. Entity-relationship modeling and\nits variants have been the basis of most relational data modeling for decades.\nHowever, these abstractions and the relational model itself are intricate and\nhave proved not to be very accessible among scientists with limited resources\nfor data management. This paper explores one aspect of relational data models,\nthe meaning of foreign key relationships. We observe that a foreign key\nproduces a table relationship that generally references either an entity or\nrepeating attributes. This paper proposes constructing foreign keys based on\nthese two cases, and suggests that the method promotes intuitive data modeling\nand normalization.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 12:00:32 GMT"}, {"version": "v2", "created": "Wed, 29 Sep 2010 19:11:56 GMT"}, {"version": "v3", "created": "Mon, 4 Oct 2010 16:34:26 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Nassar", "Nassib", ""]]}]