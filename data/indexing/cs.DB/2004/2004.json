[{"id": "2004.00130", "submitter": "Amine Mhedhbi", "authors": "Amine Mhedhbi, Pranjal Gupta, Shahid Khaliq and Semih Salihoglu", "title": "A+ Indexes: Tunable and Space-Efficient Adjacency Lists in Graph\n  Database Management Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Graph database management systems (GDBMSs) are highly optimized to perform\nfast traversals, i.e., joins of vertices with their neighbours, by indexing the\nneighbourhoods of vertices in adjacency lists. However, existing GDBMSs have\nsystem-specific and fixed adjacency list structures, which makes each system\nefficient on only a fixed set of workloads. We describe a new tunable indexing\nsubsystem for GDBMSs, we call A+ indexes, with materialized view support. The\nsubsystem consists of two types of indexes: (i) vertex-partitioned indexes that\npartition 1-hop materialized views into adjacency lists on either the source or\ndestination vertex IDs; and (ii) edge-partitioned indexes that partition 2-hop\nviews into adjacency lists on one of the edge IDs. As in existing GDBMSs, a\nsystem by default requires one forward and one backward vertex-partitioned\nindex, which we call the primary A+ index. Users can tune the primary index or\nsecondary indexes by adding nested partitioning and sorting criteria. Our\nsecondary indexes are space-efficient and use a technique we call offset lists.\nOur indexing subsystem allows a wider range of applications to benefit from\nGDBMSs' fast join capabilities. We demonstrate the tunability and space\nefficiency of A+ indexes through extensive experiments on three workloads.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 21:38:33 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 01:52:31 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 07:51:16 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 21:17:53 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Mhedhbi", "Amine", ""], ["Gupta", "Pranjal", ""], ["Khaliq", "Shahid", ""], ["Salihoglu", "Semih", ""]]}, {"id": "2004.00190", "submitter": "Vijay Gadepally", "authors": "Vijay Gadepally, Jeremy Kepner", "title": "Technical Report: Developing a Working Data Hub", "comments": "Fixes typographical errors; references updated; minor content updates\n  in Section 4. arXiv admin note: substantial text overlap with\n  arXiv:1905.03592", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data forms a key component of any enterprise. The need for high quality and\neasy access to data is further amplified by organizations wishing to leverage\nmachine learning or artificial intelligence for their operations. To this end,\nmany organizations are building resources for managing heterogenous data,\nproviding end-users with an organization wide view of available data, and\nacting as a centralized repository for data owned/collected by an organization.\nVery broadly, we refer to these class of techniques as a \"data hub.\" While\nthere is no clear definition of what constitutes a data hub, some of the key\ncharacteristics include: data catalog; links to data sets or owners of data\nsets or centralized data repository; basic ability to serve / visualize data\nsets; access control policies that ensure secure data access and respects\npolicies of data owners; and computing capabilities tied with data hub\ninfrastructure. Of course, developing such a data hub entails numerous\nchallenges. This document provides background in databases, data management and\noutlines best practices and recommendations for developing and deploying a\nworking data hub.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 01:52:11 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 16:52:48 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""]]}, {"id": "2004.00253", "submitter": "Karim Ba\\\"ina", "authors": "Karim Ba\\\"ina", "title": "Leveraging Data Preparation, HBase NoSQL Storage, and HiveQL Querying\n  for COVID-19 Big Data Analytics Projects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiologist, Scientists, Statisticians, Historians, Data engineers and\nData scientists are working on finding descriptive models and theories to\nexplain COVID-19 expansion phenomena or on building analytics predictive models\nfor learning the apex of COVID-19 confimed cases, recovered cases, and deaths\nevolution curves. In CRISP-DM life cycle, 75% of time is consumed only by data\npreparation phase causing lot of pressions and stress on scientists and data\nscientists building machine learning models. This paper aims to help reducing\ndata preparation efforts by presenting detailed schemas design and data\npreparation technical scripts for formatting and storing Johns Hopkins\nUniversity COVID-19 daily data in HBase NoSQL data store, and enabling HiveQL\nCOVID-19 data querying in a relational Hive SQL-like style.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 06:45:50 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Ba\u00efna", "Karim", ""]]}, {"id": "2004.00481", "submitter": "Qi Zhou", "authors": "Qi Zhou, Joy Arulraj, Shamkant Navathe, William Harris, Jinpeng Wu", "title": "A Symbolic Approach to Proving Query Equivalence Under Bag Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In database-as-a-service platforms, automated verification of query\nequivalence helps eliminate redundant computation in the form of overlapping\nsub-queries. Researchers have proposed two pragmatic techniques to tackle this\nproblem. The first approach consists of reducing the queries to algebraic\nexpressions and proving their equivalence using an algebraic theory. The\nlimitations of this technique are threefold. It cannot prove the equivalence of\nqueries with significant differences in the attributes of their relational\noperators. It does not support certain widely-used SQL features. Its\nverification procedure is computationally intensive. The second approach\ntransforms this problem to a constraint satisfaction problem and leverages a\ngeneral-purpose solver to determine query equivalence. This technique consists\nof deriving the symbolic representation of the queries and proving their\nequivalence by determining the query containment relationship between the\nsymbolic expressions. While the latter approach addresses all the limitations\nof the former technique, it only proves the equivalence of queries under set\nsemantics. However, in practice, database applications use bag semantics. In\nthis paper, we introduce a novel symbolic approach for proving query\nequivalence under bag semantics. We transform the problem of proving query\nequivalence under bag semantics to that of proving the existence of a\nbijective, identity map between tuples returned by the queries on all valid\ninputs. We implement this symbolic approach in SPES and demonstrate that SPES\nproves the equivalence of a larger set of query pairs (95/232) under bag\nsemantics compared to the state-of-the-art tools based on algebraic (30/232)\nand symbolic approaches (67/232) under set and bag semantics, respectively.\nFurthermore, SPES is 3X faster than the symbolic tool that proves equivalence\nunder set semantics.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 14:56:21 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 01:54:56 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhou", "Qi", ""], ["Arulraj", "Joy", ""], ["Navathe", "Shamkant", ""], ["Harris", "William", ""], ["Wu", "Jinpeng", ""]]}, {"id": "2004.00584", "submitter": "Yuliang Li", "authors": "Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, Wang-Chiew Tan", "title": "Deep Entity Matching with Pre-Trained Language Models", "comments": "To appear in VLDB 2021", "journal-ref": null, "doi": "10.14778/3421424.3421431", "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Ditto, a novel entity matching system based on pre-trained\nTransformer-based language models. We fine-tune and cast EM as a sequence-pair\nclassification problem to leverage such models with a simple architecture. Our\nexperiments show that a straightforward application of language models such as\nBERT, DistilBERT, or RoBERTa pre-trained on large text corpora already\nsignificantly improves the matching quality and outperforms previous\nstate-of-the-art (SOTA), by up to 29% of F1 score on benchmark datasets. We\nalso developed three optimization techniques to further improve Ditto's\nmatching capability. Ditto allows domain knowledge to be injected by\nhighlighting important pieces of input information that may be of interest when\nmaking matching decisions. Ditto also summarizes strings that are too long so\nthat only the essential information is retained and used for EM. Finally, Ditto\nadapts a SOTA technique on data augmentation for text to EM to augment the\ntraining data with (difficult) examples. This way, Ditto is forced to learn\n\"harder\" to improve the model's matching capability. The optimizations we\ndeveloped further boost the performance of Ditto by up to 9.8%. Perhaps more\nsurprisingly, we establish that Ditto can achieve the previous SOTA results\nwith at most half the number of labeled data. Finally, we demonstrate Ditto's\neffectiveness on a real-world large-scale EM task. On matching two company\ndatasets consisting of 789K and 412K records, Ditto achieves a high F1 score of\n96.5%.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:14:10 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 05:40:21 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 19:19:08 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Li", "Yuliang", ""], ["Li", "Jinfeng", ""], ["Suhara", "Yoshihiko", ""], ["Doan", "AnHai", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2004.00756", "submitter": "Benjamin Killeen", "authors": "Benjamin D. Killeen, Jie Ying Wu, Kinjal Shah, Anna Zapaishchykova,\n  Philipp Nikutta, Aniruddha Tamhane, Shreya Chakraborty, Jinchi Wei, Tiger\n  Gao, Mareike Thies, Mathias Unberath", "title": "A County-level Dataset for Informing the United States' Response to\n  COVID-19", "comments": "Updated 10 September 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the coronavirus disease 2019 (COVID-19) continues to be a global pandemic,\npolicy makers have enacted and reversed non-pharmaceutical interventions with\nvarious levels of restrictions to limit its spread. Data driven approaches that\nanalyze temporal characteristics of the pandemic and its dependence on regional\nconditions might supply information to support the implementation of mitigation\nand suppression strategies. To facilitate research in this direction on the\nexample of the United States, we present a machine-readable dataset that\naggregates relevant data from governmental, journalistic, and academic sources\non the U.S. county level. In addition to county-level time-series data from the\nJHU CSSE COVID-19 Dashboard, our dataset contains more than 300 variables that\nsummarize population estimates, demographics, ethnicity, housing, education,\nemployment and income, climate, transit scores, and healthcare system-related\nmetrics. Furthermore, we present aggregated out-of-home activity information\nfor various points of interest for each county, including grocery stores and\nhospitals, summarizing data from SafeGraph and Google mobility reports. We\ncompile information from IHME, state and county-level government, and\nnewspapers for dates of the enactment and reversal of non-pharmaceutical\ninterventions. By collecting these data, as well as providing tools to read\nthem, we hope to accelerate research that investigates how the disease spreads\nand why spread may be different across regions. Our dataset and associated code\nare available at github.com/JieYingWu/COVID-19_US_County-level_Summaries.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 05:07:27 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 02:58:18 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Killeen", "Benjamin D.", ""], ["Wu", "Jie Ying", ""], ["Shah", "Kinjal", ""], ["Zapaishchykova", "Anna", ""], ["Nikutta", "Philipp", ""], ["Tamhane", "Aniruddha", ""], ["Chakraborty", "Shreya", ""], ["Wei", "Jinchi", ""], ["Gao", "Tiger", ""], ["Thies", "Mareike", ""], ["Unberath", "Mathias", ""]]}, {"id": "2004.00803", "submitter": "Guanyu Feng", "authors": "Guanyu Feng, Zixuan Ma, Daixuan Li, Shengqi Chen, Xiaowei Zhu, Wentao\n  Han, Wenguang Chen", "title": "RisGraph: A Real-Time Streaming System for Evolving Graphs to Support\n  Sub-millisecond Per-update Analysis at Millions Ops/s", "comments": null, "journal-ref": "SIGMOD Conference 2021: 513-527", "doi": "10.1145/3448016.3457263", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolving graphs in the real world are large-scale and constantly changing, as\nhundreds of thousands of updates may come every second. Monotonic algorithms\nsuch as Reachability and Shortest Path are widely used in real-time analytics\nto gain both static and temporal insights and can be accelerated by incremental\ncomputing. Existing streaming systems adopt the incremental computing model and\nachieve either low latency or high throughput, but not both. However, both high\nthroughput and low latency are required in real scenarios such as financial\nfraud detection. This paper presents RisGraph, a real-time streaming system\nthat provides low-latency analysis for each update with high throughput.\nRisGraph addresses the challenge with localized data access and inter-update\nparallelism. We propose a data structure named Indexed Adjacency Lists and use\nsparse arrays and Hybrid Parallel Mode to enable localized data access. To\nachieve inter-update parallelism, we propose a domain-specific concurrency\ncontrol mechanism based on the classification of safe and unsafe updates.\nExperiments show that RisGraph can ingest millions of updates per second for\ngraphs with several hundred million vertices and billions of edges, and the\nP999 processing time latency is within 20 milliseconds. RisGraph achieves\norders-of-magnitude improvement on throughput when analyses are executed for\neach update without batching and performs better than existing systems with\nbatches of up to 20 million updates.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 04:13:21 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 09:41:31 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Feng", "Guanyu", ""], ["Ma", "Zixuan", ""], ["Li", "Daixuan", ""], ["Chen", "Shengqi", ""], ["Zhu", "Xiaowei", ""], ["Han", "Wentao", ""], ["Chen", "Wenguang", ""]]}, {"id": "2004.00827", "submitter": "Daniel Kang", "authors": "Daniel Kang, Edward Gan, Peter Bailis, Tatsunori Hashimoto, Matei\n  Zaharia", "title": "Approximate Selection with Guarantees using Proxies", "comments": null, "journal-ref": "PVLDB 2020", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the falling costs of data acquisition and storage, researchers and\nindustry analysts often want to find all instances of rare events in large\ndatasets. For instance, scientists can cheaply capture thousands of hours of\nvideo, but are limited by the need to manually inspect long videos to identify\nrelevant objects and events. To reduce this cost, recent work proposes to use\ncheap proxy models, such as image classifiers, to identify an approximate set\nof data points satisfying a data selection filter. Unfortunately, this recent\nwork does not provide the statistical accuracy guarantees necessary in\nscientific and production settings.\n  In this work, we introduce novel algorithms for approximate selection queries\nwith statistical accuracy guarantees. Namely, given a limited number of exact\nidentifications from an oracle, often a human or an expensive machine learning\nmodel, our algorithms meet a minimum precision or recall target with high\nprobability. In contrast, existing approaches can catastrophically fail in\nsatisfying these recall and precision targets. We show that our algorithms can\nimprove query result quality by up to 30x for both the precision and recall\ntargets in both real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 05:36:10 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 19:08:32 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 20:33:35 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Kang", "Daniel", ""], ["Gan", "Edward", ""], ["Bailis", "Peter", ""], ["Hashimoto", "Tatsunori", ""], ["Zaharia", "Matei", ""]]}, {"id": "2004.01124", "submitter": "Jongik Kim", "authors": "Jongik Kim", "title": "Nass: A New Approach to Graph Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of graph similarity search with graph\nedit distance (GED) constraints. Due to the NP-hardness of GED computation,\nexisting solutions to this problem adopt the filtering-and-verification\nframework with a main focus on the filtering phase to generate a small number\nof candidate graphs. However, they have a limitation that the number of\ncandidates grows extremely rapidly as a GED threshold increases. To address the\nlimitation, we propose a new approach that utilizes GED computation results in\ngenerating candidate graphs. The main idea is that whenever we identify a\nresult graph of the query, we immediately regenerate candidate graphs using a\nsubset of pre-computed graphs similar to the identified result graph. To speed\nup GED computation, we also develop a novel GED computation algorithm. The\nproposed algorithm reduces the search space for GED computation by utilizing a\nseries of filtering techniques, which have been used to generate candidates in\nexisting solutions. Experimental results on real datasets demonstrate the\nproposed approach significantly outperforms the state-of-the art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:53:04 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Kim", "Jongik", ""]]}, {"id": "2004.01816", "submitter": "Juan Reutter L", "authors": "Aidan Hogan and Juan Reutter and Adrian Soto", "title": "Recursive SPARQL for Graph Analytics", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work on knowledge graphs and graph-based data management often focus either\non declarative graph query languages or on frameworks for graph analytics,\nwhere there has been little work in trying to combine both approaches. However,\nmany real-world tasks conceptually involve combinations of these approaches: a\ngraph query can be used to select the appropriate data, which is then enriched\nwith analytics, and then possibly filtered or combined again with other data by\nmeans of a query language. In this paper we propose a declarative language that\nis well suited to perform graph querying and analytical tasks. We do this by\nproposing a minimalistic extension of SPARQL to allow for expressing analytical\ntasks; in particular, we propose to extend SPARQL with recursive features, and\nprovide a formal syntax and semantics for our language. We show that this\nlanguage can express key analytical tasks on graphs (in fact, it is Turing\ncomplete), offering a more declarative alternative to existing frameworks and\nlanguages. We show how procedures in our language can be implemented over an\noff-the-shelf SPARQL engine with a specialised client that allows\nparallelisation and batch-based processing when memory is limited. Results show\nthat with such an implementation, procedures for popular analytics currently\nrun in seconds or minutes for selective sub-graphs (our target use-case) but\nstruggle at larger scales.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 23:57:04 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Hogan", "Aidan", ""], ["Reutter", "Juan", ""], ["Soto", "Adrian", ""]]}, {"id": "2004.01833", "submitter": "Martin Weise", "authors": "Martin Weise", "title": "On the Efficient Design of LSM Stores", "comments": "7 pages, 3 figures, course material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, key-value data storage systems have gained significantly\nmore interest from academia and industry. These systems face numerous\nchallenges concerning storage space- and read optimization. There exists a\nlarge potential for improving current solutions by introducing new management\ntechniques and algorithms.\n  In this paper we give an overview of the basic concept of key-value data\nstorage systems and provide an explanation for bottlenecks. Further we\nintroduce two new memory management algorithms and a improved index structure.\nFinally, these solutions are compared to each other and discussed.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 01:47:35 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Weise", "Martin", ""]]}, {"id": "2004.01908", "submitter": "Ingo M\\\"uller", "authors": "Ingo M\\\"uller (1) and Renato Marroqu\\'in (2) and Dimitrios Koutsoukos\n  (1) and Mike Wawrzoniak (1) and Sabir Akhadov (3) and Gustavo Alonso (1) ((1)\n  Systems Group, Department of Computer Science, ETH Zurich, (2) Oracle Labs,\n  (3) Databricks)", "title": "The Collection Virtual Machine: An Abstraction for Multi-Frontend\n  Multi-Backend Data Analysis", "comments": "This paper is currently under review at DaMoN'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Getting the best performance from the ever-increasing number of hardware\nplatforms has been a recurring challenge for data processing systems. In recent\nyears, the advent of data science with its increasingly numerous and complex\ntypes of analytics has made this challenge even more difficult. In practice,\nsystem designers are overwhelmed by the number of combinations and typically\nimplement only one analysis/platform combination, leading to repeated\nimplementation effort -- and a plethora of semi-compatible tools for data\nscientists.\n  In this paper, we propose the \"Collection Virtual Machine\" (or CVM) -- an\nextensible compiler framework designed to keep the specialization process of\ndata analytics systems tractable. It can capture at the same time the essence\nof a large span of low-level, hardware-specific implementation techniques as\nwell as high-level operations of different types of analyses. At its core lies\na language for defining nested, collection-oriented intermediate\nrepresentations (IRs). Frontends produce programs in their IR flavors defined\nin that language, which get optimized through a series of rewritings (possibly\nchanging the IR flavor multiple times) until the program is finally expressed\nin an IR of platform-specific operators. While reducing the overall\nimplementation effort, this also improves the interoperability of both analyses\nand hardware platforms. We have used CVM successfully to build specialized\nbackends for platforms as diverse as multi-core CPUs, RDMA clusters, and\nserverless computing infrastructure in the cloud and expect similar results for\nmany more frontends and hardware platforms in the near future.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 11:02:36 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 19:48:05 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["M\u00fcller", "Ingo", ""], ["Marroqu\u00edn", "Renato", ""], ["Koutsoukos", "Dimitrios", ""], ["Wawrzoniak", "Mike", ""], ["Akhadov", "Sabir", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2004.02012", "submitter": "Anil Pacaci", "authors": "Anil Pacaci, Angela Bonifati, M. Tamer \\\"Ozsu", "title": "Regular Path Query Evaluation on Streaming Graphs", "comments": "A shorter version of this paper has been accepted for publication in\n  2020 International Conference on Management of Data (SIGMOD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study persistent query evaluation over streaming graphs, which is becoming\nincreasingly important. We focus on navigational queries that determine if\nthere exists a path between two entities that satisfies a user-specified\nconstraint. We adopt the Regular Path Query (RPQ) model that specifies\nnavigational patterns with labeled constraints. We propose deterministic\nalgorithms to efficiently evaluate persistent RPQs under both arbitrary and\nsimple path semantics in a uniform manner. Experimental analysis on real and\nsynthetic streaming graphs shows that the proposed algorithms can process up to\ntens of thousands of edges per second and efficiently answer RPQs that are\ncommonly used in real-world workloads.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 20:35:30 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Pacaci", "Anil", ""], ["Bonifati", "Angela", ""], ["\u00d6zsu", "M. Tamer", ""]]}, {"id": "2004.02308", "submitter": "Jose Picado", "authors": "Jose Picado, John Davis, Arash Termehchy, Ga Young Lee", "title": "Learning Over Dirty Data Without Cleaning", "comments": "To be published in Proceedings of the 2020 ACM SIGMOD International\n  Conference on Management of Data (SIGMOD'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world datasets are dirty and contain many errors. Examples of these\nissues are violations of integrity constraints, duplicates, and inconsistencies\nin representing data values and entities. Learning over dirty databases may\nresult in inaccurate models. Users have to spend a great deal of time and\neffort to repair data errors and create a clean database for learning.\nMoreover, as the information required to repair these errors is not often\navailable, there may be numerous possible clean versions for a dirty database.\nWe propose DLearn, a novel relational learning system that learns directly over\ndirty databases effectively and efficiently without any preprocessing. DLearn\nleverages database constraints to learn accurate relational models over\ninconsistent and heterogeneous data. Its learned models represent patterns over\nall possible clean instances of the data in a usable form. Our empirical study\nindicates that DLearn learns accurate models over large real-world databases\nefficiently.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 20:21:13 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Picado", "Jose", ""], ["Davis", "John", ""], ["Termehchy", "Arash", ""], ["Lee", "Ga Young", ""]]}, {"id": "2004.02369", "submitter": "Keval Vora", "authors": "Kasra Jamshidi, Rakesh Mahadasa, Keval Vora", "title": "Peregrine: A Pattern-Aware Graph Mining System", "comments": "This is the full version of the paper appearing in the European\n  Conference on Computer Systems (EuroSys), 2020", "journal-ref": null, "doi": "10.1145/3342195.3387548", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph mining workloads aim to extract structural properties of a graph by\nexploring its subgraph structures. General purpose graph mining systems provide\na generic runtime to explore subgraph structures of interest with the help of\nuser-defined functions that guide the overall exploration process. However, the\nstate-of-the-art graph mining systems remain largely oblivious to the shape (or\npattern) of the subgraphs that they mine. This causes them to: (a) explore\nunnecessary subgraphs; (b) perform expensive computations on the explored\nsubgraphs; and, (c) hold intermediate partial subgraphs in memory; all of which\naffect their overall performance. Furthermore, their programming models are\noften tied to their underlying exploration strategies, which makes it difficult\nfor domain users to express complex mining tasks.\n  In this paper, we develop Peregrine, a pattern-aware graph mining system that\ndirectly explores the subgraphs of interest while avoiding exploration of\nunnecessary subgraphs, and simultaneously bypassing expensive computations\nthroughout the mining process. We design a pattern-based programming model that\ntreats \"graph patterns\" as first class constructs and enables Peregrine to\nextract the semantics of patterns, which it uses to guide its exploration. Our\nevaluation shows that Peregrine outperforms state-of-the-art distributed and\nsingle machine graph mining systems, and scales to complex mining tasks on\nlarger graphs, while retaining simplicity and expressivity with its\n\"pattern-first\" programming approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 01:33:55 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Jamshidi", "Kasra", ""], ["Mahadasa", "Rakesh", ""], ["Vora", "Keval", ""]]}, {"id": "2004.02564", "submitter": "Suman Banerjee", "authors": "Suman Banerjee and Bithika Pal", "title": "DySky: Dynamic Skyline Queries on Uncertain Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph, and a set of query vertices (subset of the vertices), the\ndynamic skyline query problem returns a subset of data vertices (other than\nquery vertices) which are not dominated by other data vertices based on certain\ndistance measure. In this paper, we study the dynamic skyline query problem on\nuncertain graphs (DySky). The input to this problem is an uncertain graph, a\nsubset of its nodes as query vertices, and the goal here is to return all the\ndata vertices which are not dominated by others. We employ two distance\nmeasures in uncertain graphs, namely, \\emph{Majority Distance}, and\n\\emph{Expected Distance}. Our approach is broadly divided into three steps:\n\\emph{Pruning}, \\emph{Distance Computation}, and \\emph{Skyline Vertex Set\nGeneration}. We implement the proposed methodology with three publicly\navailable datasets and observe that it can find out skyline vertex set without\ntaking much time even for million sized graphs if expected distance is\nconcerned. Particularly, the pruning strategy reduces the computational time\nsignificantly.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:22:43 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Banerjee", "Suman", ""], ["Pal", "Bithika", ""]]}, {"id": "2004.02570", "submitter": "Hui Luo", "authors": "Hui Luo, Zhifeng Bao, Farhana M. Choudhury, and J. Shane Culpepper", "title": "Dynamic Ridesharing in Peak Travel Periods", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2019.2961341", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a variant of the dynamic ridesharing problem with a\nspecific focus on peak hours: Given a set of drivers and rider requests, we aim\nto match drivers to each rider request by achieving two objectives: maximizing\nthe served rate and minimizing the total additional distance, subject to a\nseries of spatio-temporal constraints. Our problem can be distinguished from\nexisting work in three aspects: (1) Previous work did not fully explore the\nimpact of peak travel periods where the number of rider requests is much\ngreater than the number of available drivers. (2) Existing solutions usually\nrely on single objective optimization techniques, such as minimizing the total\ntravel cost. (3) When evaluating the overall system performance, the runtime\nspent on updating drivers' trip schedules as per incoming rider requests should\nbe incorporated, while it is excluded by most existing solutions. We propose an\nindex structure together with a set of pruning rules and an efficient algorithm\nto include new riders into drivers' existing trip schedule. To answer new rider\nrequests effectively, we propose two algorithms that match drivers with rider\nrequests. Finally, we perform extensive experiments on a large-scale test\ncollection to validate the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:34:26 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Luo", "Hui", ""], ["Bao", "Zhifeng", ""], ["Choudhury", "Farhana M.", ""], ["Culpepper", "J. Shane", ""]]}, {"id": "2004.02580", "submitter": "Ziqiang Yu", "authors": "Ziqiang Yu, Xiaohui Yu, Nick Koudas, Yang Liu, Yifan Li, Yueting Chen,\n  Dingyu Yang", "title": "Distributed Processing of k Shortest Path Queries over Dynamic Road\n  Networks", "comments": "A shorter version of this technical report has been accepted for\n  publication as a full paper in ACM SIGMOD 2020: International Conference on\n  Management of Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying the k-shortest paths (KSPs for short) in a dynamic\nroad network is essential to many location-based services. Road networks are\ndynamic in the sense that the weights of the edges in the corresponding graph\nconstantly change over time, representing evolving traffic conditions. Very\noften such services have to process numerous KSP queries over large road\nnetworks at the same time, thus there is a pressing need to identify\ndistributed solutions for this problem. However, most existing approaches are\ndesigned to identify KSPs on a static graph in a sequential manner (i.e., the\n(i+1)-th shortest path is generated based on the i-th shortest path),\nrestricting their scalability and applicability in a distributed setting. We\ntherefore propose KSP-DG, a distributed algorithm for identifying k-shortest\npaths in a dynamic graph. It is based on partitioning the entire graph into\nsmaller subgraphs, and reduces the problem of determining KSPs into the\ncomputation of partial KSPs in relevant subgraphs, which can execute in\nparallel on a cluster of servers. A distributed two-level index called DTLP is\ndeveloped to facilitate the efficient identification of relevant subgraphs. A\nsalient feature of DTLP is that it indexes a set of virtual paths that are\ninsensitive to varying traffic conditions, leading to very low maintenance cost\nin dynamic road networks. This is the first treatment of the problem of\nprocessing KSP queries over dynamic road networks. Extensive experiments\nconducted on real road networks confirm the superiority of our proposal over\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:52:00 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 00:28:10 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Yu", "Ziqiang", ""], ["Yu", "Xiaohui", ""], ["Koudas", "Nick", ""], ["Liu", "Yang", ""], ["Li", "Yifan", ""], ["Chen", "Yueting", ""], ["Yang", "Dingyu", ""]]}, {"id": "2004.03352", "submitter": "Salman Shaikh Dr", "authors": "Salman Ahmed Shaikh, Komal Mariam, Hiroyuki Kitagawa, Kyoung-Sook Kim", "title": "GeoFlink: A Distributed and Scalable Framework for the Real-time\n  Processing of Spatial Streams", "comments": "CIKM 2020 Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Flink is an open-source system for scalable processing of batch and\nstreaming data. Flink does not natively support efficient processing of spatial\ndata streams, which is a requirement of many applications dealing with spatial\ndata. Besides Flink, other scalable spatial data processing platforms including\nGeoSpark, Spatial Hadoop, etc. do not support streaming workloads and can only\nhandle static/batch workloads. To fill this gap, we present GeoFlink, which\nextends Apache Flink to support spatial data types, indexes and continuous\nqueries over spatial data streams. To enable the efficient processing of\nspatial continuous queries and for the effective data distribution across Flink\ncluster nodes, a gird-based index is introduced. GeoFlink currently supports\nspatial range, spatial $k$NN and spatial join queries on point data type. An\nextensive experimental study on real spatial data streams shows that GeoFlink\nachieves significantly higher query throughput than ordinary Flink processing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:27:02 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 05:06:13 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 01:25:46 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Shaikh", "Salman Ahmed", ""], ["Mariam", "Komal", ""], ["Kitagawa", "Hiroyuki", ""], ["Kim", "Kyoung-Sook", ""]]}, {"id": "2004.03398", "submitter": "Berthold Reinwald", "authors": "Shivam Srivastava, Prithviraj Sen, Berthold Reinwald", "title": "Forecasting in multivariate irregularly sampled time series with missing\n  values", "comments": "arXiv admin note: text overlap with arXiv:1905.12374 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse and irregularly sampled multivariate time series are common in\nclinical, climate, financial and many other domains. Most recent approaches\nfocus on classification, regression or forecasting tasks on such data. In\nforecasting, it is necessary to not only forecast the right value but also to\nforecast when that value will occur in the irregular time series. In this work,\nwe present an approach to forecast not only the values but also the time at\nwhich they are expected to occur.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 01:49:46 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Srivastava", "Shivam", ""], ["Sen", "Prithviraj", ""], ["Reinwald", "Berthold", ""]]}, {"id": "2004.03436", "submitter": "Shaoxu Song", "authors": "Aoqian Zhang, Shaoxu Song, Yu Sun, Jianmin Wang", "title": "Learning Individual Models for Imputation (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing numerical values are prevalent, e.g., owing to unreliable sensor\nreading, collection and transmission among heterogeneous sources. Unlike\ncategorized data imputation over a limited domain, the numerical values suffer\nfrom two issues: (1) sparsity problem, the incomplete tuple may not have\nsufficient complete neighbors sharing the same/similar values for imputation,\nowing to the (almost) infinite domain; (2) heterogeneity problem, different\ntuples may not fit the same (regression) model. In this study, enlightened by\nthe conditional dependencies that hold conditionally over certain tuples rather\nthan the whole relation, we propose to learn a regression model individually\nfor each complete tuple together with its neighbors. Our IIM, Imputation via\nIndividual Models, thus no longer relies on sharing similar values among the k\ncomplete neighbors for imputation, but utilizes their regression results by the\naforesaid learned individual (not necessary the same) models. Remarkably, we\nshow that some existing methods are indeed special cases of our IIM, under the\nextreme settings of the number l of learning neighbors considered in individual\nlearning. In this sense, a proper number l of neighbors is essential to learn\nthe individual models (avoid over-fitting or under-fitting). We propose to\nadaptively learn individual models over various number l of neighbors for\ndifferent complete tuples. By devising efficient incremental computation, the\ntime complexity of learning a model reduces from linear to constant.\nExperiments on real data demonstrate that our IIM with adaptive learning\nachieves higher imputation accuracy than the existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:36:54 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zhang", "Aoqian", ""], ["Song", "Shaoxu", ""], ["Sun", "Yu", ""], ["Wang", "Jianmin", ""]]}, {"id": "2004.03477", "submitter": "Ciro Medeiros", "authors": "Ciro M. Medeiros, Martin A. Musicante, Umberto S. Costa", "title": "An Algorithm for Context-Free Path Queries over Graph Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDF (Resource Description Framework) is a standard language to represent\ngraph databases. Query languages for RDF databases usually include primitives\nto support path queries, linking pairs of vertices of the graph that are\nconnected by a path of labels belonging to a given language. Languages such as\nSPARQL include support for paths defined by regular languages (by means of\nRegular Expressions). A context-free path query is a path query whose language\ncan be defined by a context-free grammar. Context-free path queries can be used\nto implement queries such as the \"same generation queries\", that are not\nexpressible by Regular Expressions. In this paper, we present a novel algorithm\nfor context-free path query processing. We prove the correctness of our\napproach and show its run-time and memory complexity. We show the viability of\nour approach by means of a prototype implemented in Go. We run our prototype\nusing the same cases of study as proposed in recent works, comparing our\nresults with another, recently published algorithm. The experiments include\nboth synthetic and real RDF databases. Our algorithm can be seen as a step\nforward, towards the implementation of more expressive query languages.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:26:50 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Medeiros", "Ciro M.", ""], ["Musicante", "Martin A.", ""], ["Costa", "Umberto S.", ""]]}, {"id": "2004.03488", "submitter": "Dimitrios Koutsoukos", "authors": "Dimitrios Koutsoukos and Ingo M\\\"uller and Renato Marroqu\\'in and\n  Gustavo Alonso", "title": "Modularis: Modular Data Analytics for Hardware, Software, and Platform\n  Heterogeneity", "comments": "Under review at VLDB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Today's data analytics displays an overwhelming diversity along many\ndimensions: data types, platforms, hardware acceleration, etc. As a result,\nsystem design often has to choose between depth and breadth: high efficiency\nfor a narrow set of use cases or generality at a lower performance. In this\npaper, we pave the way to get the best of both worlds: We present Modularis-an\nexecution layer for data analytics based on fine-grained, composable building\nblocks that are as generic and simple as possible. These building blocks are\nsimilar to traditional database operators, but at a finer granularity, so we\ncall them sub-operators. Sub-operators can be freely and easily combined. As we\ndemonstrate with concrete examples in the context of RDMA-based databases,\nModularis' sub-operators can be combined to perform the same task as a complex,\nmonolithic operator. Sub-operators, however, can be reused, can be offloaded to\ndifferent layers or accelerators, and can be customized to specialized\nhardware. In the use cases we have tested so far, sub-operators reduce the\namount of code significantly-or example, for a distributed, RDMA-based join by\na factor of four-while having minimal performance overhead. Modularis is an\norder of magnitude faster on SQL-style analytics compared to a commonly used\nframework for generic data processing (Presto) and on par with a commercial\ncluster database (MemSQL).\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:39:34 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Koutsoukos", "Dimitrios", ""], ["M\u00fcller", "Ingo", ""], ["Marroqu\u00edn", "Renato", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2004.03630", "submitter": "Harish Doraiswamy", "authors": "Harish Doraiswamy and Juliana Freire", "title": "A GPU-friendly Geometric Data Model and Algebra for Spatial Queries:\n  Extended Version", "comments": "This is the extended version of the paper published in SIGMOD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of low cost sensors has led to an unprecedented growth in\nthe volume of spatial data. However, the time required to evaluate even simple\nspatial queries over large data sets greatly hampers our ability to\ninteractively explore these data sets and extract actionable insights. Graphics\nProcessing Units~(GPUs) are increasingly being used to speedup spatial queries.\nHowever, existing GPU-based solutions have two important drawbacks: they are\noften tightly coupled to the specific query types they target, making it hard\nto adapt them for other queries; and since their design is based on CPU-based\napproaches, it can be difficult to effectively utilize all the benefits\nprovided by the GPU. As a first step towards making GPU spatial query\nprocessing mainstream, we propose a new model that represents spatial data as\ngeometric objects and define an algebra consisting of GPU-friendly composable\noperators that operate over these objects. We demonstrate the expressiveness of\nthe proposed algebra by formulating standard spatial queries as algebraic\nexpressions. We also present a proof-of-concept prototype that supports a\nsubset of the operators and show that it is at least two orders of magnitude\nfaster than a CPU-based implementation. This performance gain is obtained both\nusing a discrete Nvidia mobile GPU and the less powerful integrated GPUs common\nin commodity laptops.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 18:10:53 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Doraiswamy", "Harish", ""], ["Freire", "Juliana", ""]]}, {"id": "2004.03644", "submitter": "Babak Salimi", "authors": "Babak Salimi, Harsh Parikh, Moe Kayali, Sudeepa Roy, Lise Getoor, and\n  Dan Suciu", "title": "Causal Relational Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference is at the heart of empirical research in natural and social\nsciences and is critical for scientific discovery and informed decision making.\nThe gold standard in causal inference is performing randomized controlled\ntrials; unfortunately these are not always feasible due to ethical, legal, or\ncost constraints. As an alternative, methodologies for causal inference from\nobservational data have been developed in statistical studies and social\nsciences. However, existing methods critically rely on restrictive assumptions\nsuch as the study population consisting of homogeneous elements that can be\nrepresented in a single flat table, where each row is referred to as a unit. In\ncontrast, in many real-world settings, the study domain naturally consists of\nheterogeneous elements with complex relational structure, where the data is\nnaturally represented in multiple related tables. In this paper, we present a\nformal framework for causal inference from such relational data. We propose a\ndeclarative language called CaRL for capturing causal background knowledge and\nassumptions and specifying causal queries using simple Datalog-like rules.CaRL\nprovides a foundation for inferring causality and reasoning about the effect of\ncomplex interventions in relational domains. We present an extensive\nexperimental evaluation on real relational data to illustrate the applicability\nof CaRL in social sciences and healthcare.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 18:33:05 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Salimi", "Babak", ""], ["Parikh", "Harsh", ""], ["Kayali", "Moe", ""], ["Roy", "Sudeepa", ""], ["Getoor", "Lise", ""], ["Suciu", "Dan", ""]]}, {"id": "2004.03710", "submitter": "Suhas Somnath", "authors": "Dale Stansberry, Suhas Somnath, Jessica Breet, Gregory Shutt, and\n  Mallikarjun Shankar", "title": "DataFed: Towards Reproducible Research via Federated Data Management", "comments": "Part of conference proceedings at the 6th Annual Conference on\n  Computational Science & Computational Intelligence held at Las Vegas, NV, USA\n  on Dec 05-07 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly collaborative, globalized nature of scientific research\ncombined with the need to share data and the explosion in data volumes present\nan urgent need for a scientific data management system (SDMS). An SDMS presents\na logical and holistic view of data that greatly simplifies and empowers data\norganization, curation, searching, sharing, dissemination, etc. We present\nDataFed -- a lightweight, distributed SDMS that spans a federation of storage\nsystems within a loosely-coupled network of scientific facilities. Unlike\nexisting SDMS offerings, DataFed uses high-performance and scalable user\nmanagement and data transfer technologies that simplify deployment,\nmaintenance, and expansion of DataFed. DataFed provides web-based and\ncommand-line interfaces to manage data and integrate with complex scientific\nworkflows. DataFed represents a step towards reproducible scientific research\nby enabling reliable staging of the correct data at the desired environment.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 21:05:22 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Stansberry", "Dale", ""], ["Somnath", "Suhas", ""], ["Breet", "Jessica", ""], ["Shutt", "Gregory", ""], ["Shankar", "Mallikarjun", ""]]}, {"id": "2004.03716", "submitter": "Ahmet Kara", "authors": "Ahmet Kara, Milos Nikolic, Hung Q. Ngo, Dan Olteanu, Haozhe Zhang", "title": "Maintaining Triangle Queries under Updates", "comments": "47 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of incrementally maintaining the triangle queries\nwith arbitrary free variables under single-tuple updates to the input\nrelations. We introduce an approach called IVM$^\\epsilon$ that exhibits a\ntrade-off between the update time, the space, and the delay for the enumeration\nof the query result, such that the update time ranges from the square root to\nlinear in the database size while the delay ranges from constant to linear\ntime. IVM$^\\epsilon$ achieves Pareto worst-case optimality in the update-delay\nspace conditioned on the Online Matrix-Vector Multiplication conjecture. It is\nstrongly Pareto optimal for the triangle queries with zero or three free\nvariables and weakly Pareto optimal for the triangle queries with one or two\nfree variables.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 21:09:59 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Kara", "Ahmet", ""], ["Nikolic", "Milos", ""], ["Ngo", "Hung Q.", ""], ["Olteanu", "Dan", ""], ["Zhang", "Haozhe", ""]]}, {"id": "2004.03814", "submitter": "Ryan Marcus", "authors": "Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad\n  Alizadeh, Tim Kraska", "title": "Bao: Learning to Steer Query Optimizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query optimization remains one of the most challenging problems in data\nmanagement systems. Recent efforts to apply machine learning techniques to\nquery optimization challenges have been promising, but have shown few practical\ngains due to substantive training overhead, inability to adapt to changes, and\npoor tail performance. Motivated by these difficulties and drawing upon a long\nhistory of research in multi-armed bandits, we introduce Bao (the BAndit\nOptimizer). Bao takes advantage of the wisdom built into existing query\noptimizers by providing per-query optimization hints. Bao combines modern tree\nconvolutional neural networks with Thompson sampling, a decades-old and\nwell-studied reinforcement learning algorithm. As a result, Bao automatically\nlearns from its mistakes and adapts to changes in query workloads, data, and\nschema. Experimentally, we demonstrate that Bao can quickly (an order of\nmagnitude faster than previous approaches) learn strategies that improve\nend-to-end query execution performance, including tail latency. In cloud\nenvironments, we show that Bao can offer both reduced costs and better\nperformance compared with a sophisticated commercial system.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 05:15:47 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Marcus", "Ryan", ""], ["Negi", "Parimarjan", ""], ["Mao", "Hongzi", ""], ["Tatbul", "Nesime", ""], ["Alizadeh", "Mohammad", ""], ["Kraska", "Tim", ""]]}, {"id": "2004.04139", "submitter": "Xi Liang", "authors": "Xi Liang, Zechao Shang, Aaron J. Elmore, Sanjay Krishnan, Michael J.\n  Franklin", "title": "Fast and Reliable Missing Data Contingency Analysis with\n  Predicate-Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, data analysts largely rely on intuition to determine whether missing\nor withheld rows of a dataset significantly affect their analyses. We propose a\nframework that can produce automatic contingency analysis, i.e., the range of\nvalues an aggregate SQL query could take, under formal constraints describing\nthe variation and frequency of missing data tuples. We describe how to process\nSUM, COUNT, AVG, MIN, and MAX queries in these conditions resulting in hard\nerror bounds with testable constraints. We propose an optimization algorithm\nbased on an integer program that reconciles a set of such constraints, even if\nthey are overlapping, conflicting, or unsatisfiable, into such bounds. Our\nexperiments on real-world datasets against several statistical imputation and\ninference baselines show that statistical techniques can have a deceptively\nhigh error rate that is often unpredictable. In contrast, our framework offers\nhard bounds that are guaranteed to hold if the constraints are not violated. In\nspite of these hard bounds, we show competitive accuracy to statistical\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:50:18 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Liang", "Xi", ""], ["Shang", "Zechao", ""], ["Elmore", "Aaron J.", ""], ["Krishnan", "Sanjay", ""], ["Franklin", "Michael J.", ""]]}, {"id": "2004.04286", "submitter": "Masoud Salehpour", "authors": "Masoud Salehpour and Joseph G. Davis", "title": "The Effects of Different JSON Representations on Querying Knowledge\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs (KGs) have emerged as the de-facto standard for modeling and\nquerying datasets with a graph-like structure in the Semantic Web domain. Our\nfocus is on the performance challenges associated with querying KGs. We\ndeveloped three informationally equivalent JSON-based representations for KGs,\nnamely, Subject-based Name/Value (JSON-SNV), Documents of Triples (JSON-DT),\nand Chain-based Name/Value (JSON-CNV). We analyzed the effects of these\nrepresentations on query performance by storing them on two prominent\ndocument-based Data Management Systems (DMSs), namely, MongoDB and Couchbase\nand executing a set of benchmark queries over them. We also compared the\nexecution times with row-store Virtuoso, column-store Virtuoso, and\n\\mbox{Blazegraph} as three major DMSs with different architectures (aka,\nRDF-stores). Our results indicate that the representation type has a\nsignificant performance impact on query execution. For instance, the JSON-SNV\noutperforms others by nearly one order of magnitude to execute subject-subject\njoin queries. This and the other results presented in this paper can assist in\nmore accurate benchmarking of the emerging DMSs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 22:37:39 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Salehpour", "Masoud", ""], ["Davis", "Joseph G.", ""]]}, {"id": "2004.04656", "submitter": "Yuchao Tao", "authors": "Yuchao Tao (1), Xi He (2), Ashwin Machanavajjhala (1), Sudeepa Roy (1)\n  ((1) Duke University, (2) University of Waterloo)", "title": "Computing Local Sensitivities of Counting Queries with Joins", "comments": "To be published in Proceedings of the 2020 ACM SIGMOD International\n  Conference on Management of Data", "journal-ref": null, "doi": "10.1145/3318464.3389762", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local sensitivity of a query Q given a database instance D, i.e. how much the\noutput Q(D) changes when a tuple is added to D or deleted from D, has many\napplications including query analysis, outlier detection, and in differential\nprivacy. However, it is NP-hard to find local sensitivity of a conjunctive\nquery in terms of the size of the query, even for the class of acyclic queries.\nAlthough the complexity is polynomial when the query size is fixed, the naive\nalgorithms are not efficient for large databases and queries involving multiple\njoins. In this paper, we present a novel approach to compute local sensitivity\nof counting queries involving join operations by tracking and summarizing tuple\nsensitivities -- the maximum change a tuple can cause in the query result when\nit is added or removed. We give algorithms for the sensitivity problem for full\nacyclic join queries using join trees, that run in polynomial time in both the\nsize of the database and query for an interesting sub-class of queries, which\nwe call 'doubly acyclic queries' that include path queries, and in polynomial\ntime in combined complexity when the maximum degree in the join tree is\nbounded. Our algorithms can be extended to certain non-acyclic queries using\ngeneralized hypertree decompositions. We evaluate our approach experimentally,\nand show applications of our algorithms to obtain better results for\ndifferential privacy by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 16:44:01 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Tao", "Yuchao", "", "Duke University"], ["He", "Xi", "", "University of Waterloo"], ["Machanavajjhala", "Ashwin", "", "Duke University"], ["Roy", "Sudeepa", "", "Duke University"]]}, {"id": "2004.05065", "submitter": "Amir Gilad", "authors": "Amir Gilad, Daniel Deutch, Sudeepa Roy", "title": "On Multiple Semantics for Declarative Database Repairs", "comments": null, "journal-ref": "SIGMOD 2020", "doi": "10.1145/3318464.3389721", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of database repairs through a rule-based framework that\nwe refer to as Delta Rules. Delta Rules are highly expressive and allow\nspecifying complex, cross-relations repair logic associated with Denial\nConstraints, Causal Rules, and allowing to capture Database Triggers of\ninterest. We show that there are no one-size-fits-all semantics for repairs in\nthis inclusive setting, and we consequently introduce multiple alternative\nsemantics, presenting the case for using each of them. We then study the\nrelationships between the semantics in terms of their output and the complexity\nof computation. Our results formally establish the tradeoff between the\npermissiveness of the semantics and its computational complexity. We\ndemonstrate the usefulness of the framework in capturing multiple data repair\nscenarios for an Academic Search database and the TPC-H databases, showing how\nusing different semantics affects the repair in terms of size and runtime, and\nexamining the relationships between the repairs. We also compare our approach\nwith SQL triggers and a state-of-the-art data repair system.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 15:00:29 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 01:15:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gilad", "Amir", ""], ["Deutch", "Daniel", ""], ["Roy", "Sudeepa", ""]]}, {"id": "2004.05076", "submitter": "Ran Ben Basat", "authors": "Muhammad Tirmazi, Ran Ben Basat, Jiaqi Gao, Minlan Yu", "title": "Cheetah: Accelerating Database Queries with Switch Pruning", "comments": "To appear in ACM SIGMOD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern database systems are growing increasingly distributed and struggle to\nreduce query completion time with a large volume of data. In this paper, we\nleverage programmable switches in the network to partially offload query\ncomputation to the switch. While switches provide high performance, they have\nresource and programming constraints that make implementing diverse queries\ndifficult. To fit in these constraints, we introduce the concept of data\n\\emph{pruning} -- filtering out entries that are guaranteed not to affect\noutput. The database system then runs the same query but on the pruned data,\nwhich significantly reduces processing time. We propose pruning algorithms for\na variety of queries. We implement our system, Cheetah, on a Barefoot Tofino\nswitch and Spark. Our evaluation on multiple workloads shows $40 - 200\\%$\nimprovement in the query completion time compared to Spark.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 15:34:15 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 17:46:52 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Tirmazi", "Muhammad", ""], ["Basat", "Ran Ben", ""], ["Gao", "Jiaqi", ""], ["Yu", "Minlan", ""]]}, {"id": "2004.05297", "submitter": "Siddhartha Sahu", "authors": "Siddhartha Sahu and Semih Salihoglu", "title": "Graphsurge: Graph Analytics on View Collections Using Differential\n  Computation", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3452837", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design and implementation of a new open-source\nview-based graph analytics system called Graphsurge. Graphsurge is designed to\nsupport applications that analyze multiple snapshots or views of a large-scale\ngraph. Users program Graphsurge through a declarative graph view definition\nlanguage (GVDL) to create views over input graphs and a Differential\nDataflow-based programming API to write analytics computations. A key feature\nof GVDL is the ability to organize views into view collections, which allows\nGraphsurge to automatically share computation across views, without users\nwriting any incrementalization code, by performing computations differentially.\nWe then introduce two optimization problems that naturally arise in our\nsetting. First is the collection ordering problem to determine the order of\nviews that leads to minimum differences across consecutive views. We prove this\nproblem is NP-hard and show a constant-factor approximation algorithm drawn\nfrom literature. Second is the collection splitting problem to decide on which\nviews to run computations differentially vs from scratch, for which we present\nan adaptive solution that makes decisions at runtime. We present extensive\nexperiments to demonstrate the benefits of running computations differentially\nfor view collections and our collection ordering and splitting optimizations.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 03:47:27 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 16:55:40 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Sahu", "Siddhartha", ""], ["Salihoglu", "Semih", ""]]}, {"id": "2004.05345", "submitter": "Qiang Huang", "authors": "Yifan Lei, Qiang Huang, Mohan Kankanhalli, Anthony K. H. Tung", "title": "Locality-Sensitive Hashing Scheme based on Longest Circular Co-Substring", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality-Sensitive Hashing (LSH) is one of the most popular methods for\n$c$-Approximate Nearest Neighbor Search ($c$-ANNS) in high-dimensional spaces.\nIn this paper, we propose a novel LSH scheme based on the Longest Circular\nCo-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.\nWe introduce a novel concept of LCCS and a new data structure named Circular\nShift Array (CSA) for $k$-LCCS search. The insight of LCCS search framework is\nthat close data objects will have a longer LCCS than the far-apart ones with\nhigh probability. LCCS-LSH is \\emph{LSH-family-independent}, and it supports\n$c$-ANNS with different kinds of distance metrics. We also introduce a\nmulti-probe version of LCCS-LSH and conduct extensive experiments over five\nreal-life datasets. The experimental results demonstrate that LCCS-LSH\noutperforms state-of-the-art LSH schemes.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 09:24:51 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Lei", "Yifan", ""], ["Huang", "Qiang", ""], ["Kankanhalli", "Mohan", ""], ["Tung", "Anthony K. H.", ""]]}, {"id": "2004.05366", "submitter": "Len Du", "authors": "Len Du", "title": "In-Machine-Learning Database: Reimagining Deep Learning with Old-School\n  SQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-database machine learning has been very popular, almost being a cliche.\nHowever, can we do it the other way around? In this work, we say \"yes\" by\napplying plain old SQL to deep learning, in a sense implementing deep learning\nalgorithms with SQL. Most deep learning frameworks, as well as generic machine\nlearning ones, share a de facto standard of multidimensional array operations,\nunderneath fancier infrastructure such as automatic differentiation. As SQL\ntables can be regarded as generalisations of (multi-dimensional) arrays, we\nhave found a way to express common deep learning operations in SQL, encouraging\na different way of thinking and thus potentially novel models. In particular,\none of the latest trend in deep learning was the introduction of sparsity in\nthe name of graph convolutional networks, whereas we take sparsity almost for\ngranted in the database world. As both databases and machine learning involve\ntransformation of datasets, we hope this work can inspire further works\nutilizing the large body of existing wisdom, algorithms and technologies in the\ndatabase field to advance the state of the art in machine learning, rather than\nmerely integerating machine learning into databases.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 11:00:26 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 18:08:28 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Du", "Len", ""]]}, {"id": "2004.05378", "submitter": "Surabhi Gupta", "authors": "Surabhi Gupta, Sanket Purandare, Karthik Ramachandra", "title": "Optimizing Cursor Loops in Relational Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loops that iterate over SQL query results are quite common, both in\napplication programs that run outside the DBMS, as well as User Defined\nFunctions (UDFs) and stored procedures that run within the DBMS. It can be\nargued that set-oriented operations are more efficient and should be preferred\nover iteration; but from real-world use cases, it is clear that loops over\nquery results are inevitable in many situations, and are preferred by many\nusers. Such loops, known as cursor loops, come with huge trade-offs and\noverheads w.r.t. performance, resource consumption and concurrency.\n  We present Aggify, a technique for optimizing loops over query results that\novercomes all these overheads. It achieves this by automatically generating\ncustom aggregates that are equivalent in semantics to the loop. Thereby, Aggify\ncompletely eliminates the loop by rewriting the query to use this generated\naggregate. This technique has several advantages such as: (i) pipelining of\nentire cursor loop operations instead of materialization, (ii) pushing down\nloop computation from the application layer into the DBMS, closer to the data,\n(iii) leveraging existing work on optimization of aggregate functions,\nresulting in efficient query plans. We describe the technique underlying Aggify\nand present our experimental evaluation over benchmarks as well as real\nworkloads that demonstrate the significant benefits of this technique.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 12:00:06 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 10:51:59 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 09:47:06 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Gupta", "Surabhi", ""], ["Purandare", "Sanket", ""], ["Ramachandra", "Karthik", ""]]}, {"id": "2004.05437", "submitter": "Aunn Raza", "authors": "Aunn Raza, Periklis Chrysogelos, Angelos Christos Anadiotis, Anastasia\n  Ailamaki", "title": "Adaptive HTAP through Elastic Resource Scheduling", "comments": "Technical report accompanying the paper in SIGMOD 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Hybrid Transactional/Analytical Processing (HTAP) systems use an\nintegrated data processing engine that performs analytics on fresh data, which\nare ingested from a transactional engine. HTAP systems typically consider data\nfreshness at design time, and are optimized for a fixed range of freshness\nrequirements, addressed at a performance cost for either OLTP or OLAP. The data\nfreshness and the performance requirements of both engines, however, may vary\nwith the workload.\n  We approach HTAP as a scheduling problem, addressed at runtime through\nelastic resource management. We model an HTAP system as a set of three\nindividual engines: an OLTP, an OLAP and a Resource and Data Exchange (RDE)\nengine. We devise a scheduling algorithm which traverses the HTAP design\nspectrum through elastic resource management, to meet the data freshness\nrequirements of the workload. We propose an in-memory system design which is\nnon-intrusive to the current state-of-art OLTP and OLAP engines, and we use it\nto evaluate the performance of our approach. Our evaluation shows that the\nperformance benefit of our system for OLAP queries increases over time,\nreaching up to 50% compared to static schedules for 100 query sequences, while\nmaintaining a small, and controlled, drop in the OLTP throughput.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 16:16:27 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 10:01:46 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Raza", "Aunn", ""], ["Chrysogelos", "Periklis", ""], ["Anadiotis", "Angelos Christos", ""], ["Ailamaki", "Anastasia", ""]]}, {"id": "2004.05517", "submitter": "Oksana Dolmatova", "authors": "Oksana Dolmatova, Nikolaus Augsten, Michael H. Boehlen", "title": "A Relational Matrix Algebra and its Implementation in a Column Store", "comments": "16 pages, 18 figures", "journal-ref": null, "doi": "10.1145/3318464.3389747", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytical queries often require a mixture of relational and linear algebra\noperations applied to the same data. This poses a challenge to analytic systems\nthat must bridge the gap between relations and matrices. Previous work has\nmainly strived to fix the problem at the implementation level. This paper\nproposes a principled solution at the logical level. We introduce the\nrelational matrix algebra (RMA), which seamlessly integrates linear algebra\noperations into the relational model and eliminates the dichotomy between\nmatrices and relations. RMA is closed: All our relational matrix operations are\nperformed on relations and result in relations; no additional data structure is\nrequired. Our implementation in MonetDB shows the feasibility of our approach,\nand empirical evaluations suggest that in-database analytics performs well for\nmixed workloads.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 00:59:09 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Dolmatova", "Oksana", ""], ["Augsten", "Nikolaus", ""], ["Boehlen", "Michael H.", ""]]}, {"id": "2004.05648", "submitter": "Masoud Salehpour", "authors": "Masoud Salehpour and Joseph G. Davis", "title": "A Comparative Analysis of Knowledge Graph Query Performance", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.04286", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Knowledge Graphs (KGs) continue to gain widespread momentum for use in\ndifferent domains, storing the relevant KG content and efficiently executing\nqueries over them are becoming increasingly important. A range of Data\nManagement Systems (DMSs) have been employed to process KGs. This paper aims to\nprovide an in-depth analysis of query performance across diverse DMSs and KG\nquery types. Our aim is to provide a fine-grained, comparative analysis of four\nmajor DMS types, namely, row-, column-, graph-, and document-stores, against\nmajor query types, namely, subject-subject, subject-object, tree-like, and\noptional joins. In particular, we analyzed the performance of row-store\nVirtuoso, column-store Virtuoso, Blazegraph (i.e., graph-store), and MongoDB\n(i.e., document-store) using five well-known benchmarks, namely, BSBM, WatDiv,\nFishMark, BowlognaBench, and BioBench-Allie. Our results show that no single\nDMS displays superior query performance across the four query types. In\nparticular, row- and column-store Virtuoso are a factor of 3-8 faster for\ntree-like joins, Blazegraph performs around one order of magnitude faster for\nsubject-object joins, and MongoDB performs over one order of magnitude faster\nfor high-selective queries.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 23:04:16 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Salehpour", "Masoud", ""], ["Davis", "Joseph G.", ""]]}, {"id": "2004.05712", "submitter": "Chiranjeeb Buragohain", "authors": "Chiranjeeb Buragohain, Knut Magne Risvik, Paul Brett, Miguel Castro,\n  Wonhee Cho, Joshua Cowhig, Nikolas Gloy, Karthik Kalyanaraman, Richendra\n  Khanna, John Pao, Matthew Renzelmann, Alex Shamis, Timothy Tan and Shuheng\n  Zheng", "title": "A1: A Distributed In-Memory Graph Database", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3386135", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A1 is an in-memory distributed database used by the Bing search engine to\nsupport complex queries over structured data. The key enablers for A1 are\navailability of cheap DRAM and high speed RDMA (Remote Direct Memory Access)\nnetworking in commodity hardware. A1 uses FaRM as its underlying storage layer\nand builds the graph abstraction and query engine on top. The combination of\nin-memory storage and RDMA access requires rethinking how data is allocated,\norganized and queried in a large distributed system. A single A1 cluster can\nstore tens of billions of vertices and edges and support a throughput of 350+\nmillion of vertex reads per second with end to end query latency in single\ndigit milliseconds. In this paper we describe the A1 data model, RDMA optimized\ndata structures and query execution.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 22:58:46 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Buragohain", "Chiranjeeb", ""], ["Risvik", "Knut Magne", ""], ["Brett", "Paul", ""], ["Castro", "Miguel", ""], ["Cho", "Wonhee", ""], ["Cowhig", "Joshua", ""], ["Gloy", "Nikolas", ""], ["Kalyanaraman", "Karthik", ""], ["Khanna", "Richendra", ""], ["Pao", "John", ""], ["Renzelmann", "Matthew", ""], ["Shamis", "Alex", ""], ["Tan", "Timothy", ""], ["Zheng", "Shuheng", ""]]}, {"id": "2004.05722", "submitter": "Weiyuan Wu", "authors": "Weiyuan Wu, Lampros Flokas, Eugene Wu, Jiannan Wang", "title": "Complaint-driven Training Data Debugging for Query 2.0", "comments": "Proceedings of the 2020 ACM SIGMOD International Conference on\n  Management of Data", "journal-ref": null, "doi": "10.1145/3318464.3389696", "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the need for machine learning (ML) increases rapidly across all industry\nsectors, there is a significant interest among commercial database providers to\nsupport \"Query 2.0\", which integrates model inference into SQL queries.\nDebugging Query 2.0 is very challenging since an unexpected query result may be\ncaused by the bugs in training data (e.g., wrong labels, corrupted features).\nIn response, we propose Rain, a complaint-driven training data debugging\nsystem. Rain allows users to specify complaints over the query's intermediate\nor final output, and aims to return a minimum set of training examples so that\nif they were removed, the complaints would be resolved. To the best of our\nknowledge, we are the first to study this problem. A naive solution requires\nretraining an exponential number of ML models. We propose two novel heuristic\napproaches based on influence functions which both require linear retraining\nsteps. We provide an in-depth analytical and empirical analysis of the two\napproaches and conduct extensive experiments to evaluate their effectiveness\nusing four real-world datasets. Results show that Rain achieves the highest\nrecall@k among all the baselines while still returns results interactively.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 23:56:06 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Wu", "Weiyuan", ""], ["Flokas", "Lampros", ""], ["Wu", "Eugene", ""], ["Wang", "Jiannan", ""]]}, {"id": "2004.05935", "submitter": "Suman Banerjee", "authors": "Suman Banerjee and Bithika Pal", "title": "First Stretch then Shrink and Bulk: A Two Phase Approach for Enumeration\n  of Maximal $(\\Delta, \\gamma)$\\mbox{-}Cliques of a Temporal Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \\emph{Temporal Network} (also known as \\emph{Link Stream} or\n\\emph{Time-Varying Graph}) is often used to model a time-varying relationship\namong a group of agents. It is typically represented as a collection of\ntriplets of the form $(u,v,t)$ that denotes the interaction between the agents\n$u$ and $v$ at time $t$. For analyzing the contact patterns of the agents\nforming a temporal network, recently the notion of classical \\textit{clique} of\na \\textit{static graph} has been generalized as \\textit{$\\Delta$\\mbox{-}Clique}\nof a Temporal Network. In the same direction, one of our previous studies\nintroduces the notion of \\textit{$(\\Delta, \\gamma)$\\mbox{-}Clique}, which is\nbasically a \\textit{vertex set}, \\textit{time interval} pair, in which every\npair of the clique vertices are linked at least $\\gamma$ times in every\n$\\Delta$ duration of the time interval. In this paper, we propose a different\nmethodology for enumerating all the maximal $(\\Delta, \\gamma)$\\mbox{-}Cliques\nof a given temporal network. The proposed methodology is broadly divided into\ntwo phases. In the first phase, each temporal link is processed for\nconstructing $(\\Delta, \\gamma)$\\mbox{-}Clique(s) with maximum duration. In the\nsecond phase, these initial cliques are expanded by vertex addition to form the\nmaximal cliques. From the experimentation carried out on $5$ real\\mbox{-}world\ntemporal network datasets, we observe that the proposed methodology enumerates\nall the maximal $(\\Delta,\\gamma)$\\mbox{-}Cliques efficiently, particularly when\nthe dataset is sparse. As a special case ($\\gamma=1$), the proposed methodology\nis also able to enumerate $(\\Delta,1) \\equiv \\Delta$\\mbox{-}cliques with much\nless time compared to the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 18:42:47 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Banerjee", "Suman", ""], ["Pal", "Bithika", ""]]}, {"id": "2004.05951", "submitter": "Fuat Basik", "authors": "Fuat Bas{\\i}k, Hakan Ferhatosmano\\u{g}lu, Bu\\u{g}ra Gedik", "title": "SLIM: Scalable Linkage of Mobility Data", "comments": "To Appear in Sigmod 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable solution to link entities across mobility datasets\nusing their spatio-temporal information. This is a fundamental problem in many\napplications such as linking user identities for security, understanding\nprivacy limitations of location based services, or producing a unified dataset\nfrom multiple sources for urban planning. Such integrated datasets are also\nessential for service providers to optimise their services and improve business\nintelligence. In this paper, we first propose a mobility based representation\nand similarity computation for entities. An efficient matching process is then\ndeveloped to identify the final linked pairs, with an automated mechanism to\ndecide when to stop the linkage. We scale the process with a locality-sensitive\nhashing (LSH) based approach that significantly reduces candidate pairs for\nmatching. To realize the effectiveness and efficiency of our techniques in\npractice, we introduce an algorithm called SLIM. In the experimental\nevaluation, SLIM outperforms the two existing state-of-the-art approaches in\nterms of precision and recall. Moreover, the LSH-based approach brings two to\nfour orders of magnitude speedup.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:07:28 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Bas\u0131k", "Fuat", ""], ["Ferhatosmano\u011flu", "Hakan", ""], ["Gedik", "Bu\u011fra", ""]]}, {"id": "2004.06101", "submitter": "Mirek Riedewald", "authors": "Rundong Li, Wolfgang Gatterbauer, Mirek Riedewald", "title": "Near-Optimal Distributed Band-Joins through Recursive Partitioning", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3389750", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider running-time optimization for band-joins in a distributed system,\ne.g., the cloud. To balance load across worker machines, input has to be\npartitioned, which causes duplication. We explore how to resolve this tension\nbetween maximum load per worker and input duplication for band-joins between\ntwo relations. Previous work suffered from high optimization cost or considered\npartitionings that were too restricted (resulting in suboptimal join\nperformance). Our main insight is that recursive partitioning of the\njoin-attribute space with the appropriate split scoring measure can achieve\nboth low optimization cost and low join cost. It is the first approach that is\nnot only effective for one-dimensional band-joins but also for joins on\nmultiple attributes. Experiments indicate that our method is able to find\npartitionings that are within 10% of the lower bound for both maximum load per\nworker and input duplication for a broad range of settings, significantly\nimproving over previous work.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 17:59:27 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Li", "Rundong", ""], ["Gatterbauer", "Wolfgang", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2004.06203", "submitter": "Masoud Salehpour", "authors": "Masoud Salehpour and Joseph G. Davis", "title": "Knowledge Graphs for Processing Scientific Data: Challenges and\n  Prospects", "comments": "arXiv admin note: text overlap with arXiv:2004.04286,\n  arXiv:2004.05648", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in the use of Knowledge Graphs (KGs) for the\nrepresentation, exchange, and reuse of scientific data. While KGs offer the\nprospect of improving the infrastructure for working with scalable and reusable\nscholarly data consistent with the FAIR (Findability, Accessibility,\nInteroperability, and Reusability) principles, the state-of-the-art Data\nManagement Systems (DMSs) for processing large KGs leave somewhat to be\ndesired. In this paper, we studied the performance of some of the major DMSs in\nthe context of querying KGs with the goal of providing a finely-grained,\ncomparative analysis of DMSs representing each of the four major DMS types. We\nexperimented with four well-known scientific KGs, namely, Allie, Cellcycle,\nDrugBank, and LinkedSPL against Virtuoso, Blazegraph, RDF-3X, and MongoDB as\nthe representative DMSs. Our results suggest that the DMSs display limitations\nin processing complex queries on the KG datasets. Depending on the query type,\nthe performance differentials can be several orders of magnitude. Also, no\nsingle DMS appears to offer consistently superior performance. We present an\nanalysis of the underlying issues and outline two integrated approaches and\nproposals for resolving the problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 23:12:12 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Salehpour", "Masoud", ""], ["Davis", "Joseph G.", ""]]}, {"id": "2004.06530", "submitter": "Raoni Louren\\c{c}o", "authors": "Raoni Louren\\c{c}o, Juliana Freire, Dennis Shasha", "title": "BugDoc: Algorithms to Debug Computational Processes", "comments": "To appear in SIGMOD 2020. arXiv admin note: text overlap with\n  arXiv:2002.04640", "journal-ref": null, "doi": "10.1145/3318464.3389763", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis for scientific experiments and enterprises, large-scale\nsimulations, and machine learning tasks all entail the use of complex\ncomputational pipelines to reach quantitative and qualitative conclusions. If\nsome of the activities in a pipeline produce erroneous outputs, the pipeline\nmay fail to execute or produce incorrect results. Inferring the root cause(s)\nof such failures is challenging, usually requiring time and much human thought,\nwhile still being error-prone. We propose a new approach that makes use of\niteration and provenance to automatically infer the root causes and derive\nsuccinct explanations of failures. Through a detailed experimental evaluation,\nwe assess the cost, precision, and recall of our approach compared to the state\nof the art. Our experimental data and processing software is available for use,\nreproducibility, and enhancement.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 20:13:23 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Louren\u00e7o", "Raoni", ""], ["Freire", "Juliana", ""], ["Shasha", "Dennis", ""]]}, {"id": "2004.06653", "submitter": "Huajun He", "authors": "Huajun He, Ruiyuan Li, Rubin Wang, Jie Bao, Yu Zheng and Tianrui Li", "title": "Efficient Suspected Infected Crowds Detection Based on Spatio-Temporal\n  Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virus transmission from person to person is an emergency event facing the\nglobal public. Early detection and isolation of potentially susceptible crowds\ncan effectively control the epidemic of its disease. Existing metrics can not\ncorrectly address the infected rate on trajectories. To solve this problem, we\npropose a novel spatio-temporal infected rate (IR) measure based on human\nmoving trajectories that can adequately describe the risk of being infected by\na given query trajectory of a patient. Then, we manage source data through an\nefficient spatio-temporal index to make our system more scalable, and can\nquickly query susceptible crowds from massive trajectories. Besides, we design\nseveral pruning strategies that can effectively reduce calculations. Further,\nwe design a spatial first time (SFT) index, which enables us to quickly query\nmultiple trajectories without much I/O consumption and data redundancy. The\nperformance of the solutions is demonstrated in experiments based on real and\nsynthetic trajectory datasets that have shown the effectiveness and efficiency\nof our solutions.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 08:35:54 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["He", "Huajun", ""], ["Li", "Ruiyuan", ""], ["Wang", "Rubin", ""], ["Bao", "Jie", ""], ["Zheng", "Yu", ""], ["Li", "Tianrui", ""]]}, {"id": "2004.07009", "submitter": "Rojeh Hayek", "authors": "Rojeh Hayek, Oded Shmueli", "title": "NN-based Transformation of Any SQL Cardinality Estimator for Handling\n  DISTINCT, AND, OR and NOT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SQL queries, with the AND, OR, and NOT operators, constitute a broad class of\nhighly used queries. Thus, their cardinality estimation is important for query\noptimization. In addition, a query planner requires the set-theoretic\ncardinality (i.e., without duplicates) for queries with DISTINCT as well as in\nplanning; for example, when considering sorting options. Yet, despite the\nimportance of estimating query cardinalities in the presence of DISTINCT, AND,\nOR, and NOT, many cardinality estimation methods are limited to estimating\ncardinalities of only conjunctive queries with duplicates counted.\n  The focus of this work is on two methods for handling this deficiency that\ncan be applied to any limited cardinality estimation model. First, we describe\na specialized deep learning scheme, PUNQ, which is tailored to representing\nconjunctive SQL queries and predicting the percentage of unique rows in the\nquery's result with duplicate rows. Using the predicted percentages obtained\nvia PUNQ, we are able to transform any cardinality estimation method that only\nestimates for conjunctive queries, and which estimates cardinalities with\nduplicates (e.g., MSCN), to a method that estimates queries cardinalities\nwithout duplicates. This enables estimating cardinalities of queries with the\nDISTINCT keyword. In addition, we describe a recursive algorithm, GenCrd, for\nextending any cardinality estimation method M that only handles conjunctive\nqueries to one that estimates cardinalities for more general queries (that\ninclude AND, OR, and NOT), without changing the method M itself.\n  Our evaluation is carried out on a challenging, real-world database with\ngeneral queries that include either the DISTINCT keyword or the AND, OR, and\nNOT operators. Experimentally, we show that the proposed methods obtain\naccurate cardinality estimates with the same level of accuracy as that of the\noriginal transformed methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 11:20:06 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Hayek", "Rojeh", ""], ["Shmueli", "Oded", ""]]}, {"id": "2004.07498", "submitter": "Primal Pappachan", "authors": "Primal Pappachan, Roberto Yus, Sharad Mehrotra, Johann-Christoph\n  Freytag", "title": "Sieve: A Middleware Approach to Scalable Access Control for Database\n  Management Systems", "comments": "Extended version of the paper submitted to Very Large Data Bases\n  (VLDB) and is now under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current approaches of enforcing FGAC in Database Management Systems (DBMS) do\nnot scale in scenarios when the number of policies are in the order of\nthousands. This paper identifies such a use case in the context of emerging\nsmart spaces wherein systems may be required by legislation, such as Europe's\nGDPR and California's CCPA, to empower users to specify who may have access to\ntheir data and for what purposes. We present Sieve, a layered approach of\nimplementing FGAC in existing database systems, that exploits a variety of it's\nfeatures such as UDFs, index usage hints, query explain; to scale to large\nnumber of policies. Given a query, Sieve exploits it's context to filter the\npolicies that need to be checked. Sieve also generates guarded expressions that\nsaves on evaluation cost by grouping the policies and cuts the read cost by\nexploiting database indices. Our experimental results, on two DBMS and two\ndifferent datasets, show that Sieve scales to large data sets and to large\npolicy corpus thus supporting real-time access in applications including\nemerging smart environments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 07:37:24 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 20:33:26 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Pappachan", "Primal", ""], ["Yus", "Roberto", ""], ["Mehrotra", "Sharad", ""], ["Freytag", "Johann-Christoph", ""]]}, {"id": "2004.07585", "submitter": "Qian Lin", "authors": "Qian Lin, Kaiyuan Yang, Tien Tuan Anh Dinh, Qingchao Cai, Gang Chen,\n  Beng Chin Ooi, Pingcheng Ruan, Sheng Wang, Zhongle Xie, Meihui Zhang, Olafs\n  Vandans", "title": "ForkBase: Immutable, Tamper-evident Storage Substrate for Branchable\n  Applications", "comments": "In Proceedings of the IEEE International Conference on Data\n  Engineering (ICDE), 2020 (Demo)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collaboration activities typically require systematic or protocol-based\ncoordination to be scalable. Git, an effective enabler for collaborative\ncoding, has been attested for its success in countless projects around the\nworld. Hence, applying the Git philosophy to general data collaboration beyond\ncoding is motivating. We call it Git for data. However, the original Git design\nhandles data at the file granule, which is considered too coarse-grained for\nmany database applications. We argue that Git for data should be co-designed\nwith database systems. To this end, we developed ForkBase to make Git for data\npractical. ForkBase is a distributed, immutable storage system designed for\ndata version management and data collaborative operation. In this\ndemonstration, we show how ForkBase can greatly facilitate collaborative data\nmanagement and how its novel data deduplication technique can improve storage\nefficiency for archiving massive data versions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:52:25 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Lin", "Qian", ""], ["Yang", "Kaiyuan", ""], ["Dinh", "Tien Tuan Anh", ""], ["Cai", "Qingchao", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""], ["Ruan", "Pingcheng", ""], ["Wang", "Sheng", ""], ["Xie", "Zhongle", ""], ["Zhang", "Meihui", ""], ["Vandans", "Olafs", ""]]}, {"id": "2004.07668", "submitter": "Wim Martens", "authors": "Angela Bonifati, Giovanna Guerrini, Carsten Lutz, Wim Martens, Lara\n  Mazilu, Norman Paton, Marcos Antonio Vaz Salles, Marc H. Scholl, Yongluan\n  Zhou", "title": "Holding a Conference Online and Live due to COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint EDBT/ICDT conference (International Conference on Extending\nDatabase Technology/International Conference on Database Theory) is a well\nestablished conference series on data management, with annual meetings in the\nsecond half of March that attract 250 to 300 delegates. Three weeks before\nEDBT/ICDT 2020 was planned to take place in Copenhagen, the rapidly developing\nCovid-19 pandemic led to the decision to cancel the face-to-face event. In the\ninterest of the research community, it was decided to move the conference\nonline while trying to preserve as much of the real-life experience as\npossible. As far as we know, we are one of the first conferences that moved to\na fully synchronous online experience due to the COVID-19 outbreak. With fully\nsynchronous, we mean that participants jointly listened to presentations, had\nlive Q&A, and attended other live events associated with the conference. In\nthis report, we share our decisions, experiences, and lessons learned.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:05:24 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 05:52:37 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 07:03:53 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bonifati", "Angela", ""], ["Guerrini", "Giovanna", ""], ["Lutz", "Carsten", ""], ["Martens", "Wim", ""], ["Mazilu", "Lara", ""], ["Paton", "Norman", ""], ["Salles", "Marcos Antonio Vaz", ""], ["Scholl", "Marc H.", ""], ["Zhou", "Yongluan", ""]]}, {"id": "2004.07812", "submitter": "Songsong Mo", "authors": "Songsong Mo, Zhifeng Bao, Baihua Zheng, and Zhiyong Peng", "title": "Bus Frequency Optimization: When Waiting Time Matters in User\n  Satisfaction", "comments": null, "journal-ref": "International Conference on Database Systems for Advanced\n  Applications 2020", "doi": "10.1007/978-3-030-59416-9_12", "report-no": null, "categories": "cs.SI cs.DB eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reorganizing bus frequency to cater for the actual travel demand can save the\ncost of the public transport system significantly. Many, if not all, existing\nstudies formulate this as a bus frequency optimization problem which tries to\nminimize passengers' average waiting time. However, many investigations have\nconfirmed that the user satisfaction drops faster as the waiting time\nincreases. Consequently, this paper studies the bus frequency optimization\nproblem considering the user satisfaction. Specifically, for the first time to\nour best knowledge, we study how to schedule the buses such that the total\nnumber of passengers who could receive their bus services within the waiting\ntime threshold is maximized. We prove that this problem is NP-hard, and present\nan index-based algorithm with $(1-1/e)$ approximation ratio. By exploiting the\nlocality property of routes in a bus network, we propose a partition-based\ngreedy method which achieves a $(1-\\rho)(1-1/e)$ approximation ratio. Then we\npropose a progressive partition-based greedy method to further improve the\nefficiency while achieving a $(1-\\rho)(1-1/e-\\varepsilon)$ approximation ratio.\nExperiments on a real city-wide bus dataset in Singapore verify the efficiency,\neffectiveness, and scalability of our methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 15:39:07 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Mo", "Songsong", ""], ["Bao", "Zhifeng", ""], ["Zheng", "Baihua", ""], ["Peng", "Zhiyong", ""]]}, {"id": "2004.07917", "submitter": "George  Fletcher", "authors": "George Fletcher, Paul Groth, Juan Sequeda", "title": "Knowledge Scientists: Unlocking the data-driven organization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations across all sectors are increasingly undergoing deep\ntransformation and restructuring towards data-driven operations. The central\nrole of data highlights the need for reliable and clean data. Unreliable,\nerroneous, and incomplete data lead to critical bottlenecks in processing\npipelines and, ultimately, service failures, which are disastrous for the\ncompetitive performance of the organization. Given its central importance,\nthose organizations which recognize and react to the need for reliable data\nwill have the advantage in the coming decade. We argue that the technologies\nfor reliable data are driven by distinct concerns and expertise which\ncomplement those of the data scientist and the data engineer. Those\norganizations which identify the central importance of meaningful, explainable,\nreproducible, and maintainable data will be at the forefront of the\ndemocratization of reliable data. We call the new role which must be developed\nto fill this critical need the Knowledge Scientist. The organizational\nstructures, tools, methodologies and techniques to support and make possible\nthe work of knowledge scientists are still in their infancy. As organizations\nnot only use data but increasingly rely on data, it is time to empower the\npeople who are central to this transformation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 20:14:20 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Fletcher", "George", ""], ["Groth", "Paul", ""], ["Sequeda", "Juan", ""]]}, {"id": "2004.08015", "submitter": "Takuya Takagi", "authors": "Hiroaki Iwashita, Takuya Takagi, Hirofumi Suzuki, Keisuke Goto, Kotaro\n  Ohori, Hiroki Arimura", "title": "Efficient Constrained Pattern Mining Using Dynamic Item Ordering for\n  Explainable Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning of interpretable classification models has been attracting much\nattention for the last few years. Discovery of succinct and contrasting\npatterns that can highlight the differences between the two classes is very\nimportant. Such patterns are useful for human experts, and can be used to\nconstruct powerful classifiers. In this paper, we consider mining of minimal\nemerging patterns from high-dimensional data sets under a variety of\nconstraints in a supervised setting. We focus on an extension in which patterns\ncan contain negative items that designate the absence of an item. In such a\ncase, a database becomes highly dense, and it makes mining more challenging\nsince popular pattern mining techniques such as fp-tree and occurrence deliver\ndo not efficiently work. To cope with this difficulty, we present an efficient\nalgorithm for mining minimal emerging patterns by combining two techniques:\ndynamic variable-ordering during pattern search for enhancing pruning effect,\nand the use of a pointer-based dynamic data structure, called dancing links,\nfor efficiently maintaining occurrence lists. Experiments on benchmark data\nsets showed that our algorithm achieves significant speed-ups over emerging\npattern mining approach based on LCM, a very fast depth-first frequent itemset\nminer using static variable-ordering.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 01:22:33 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Iwashita", "Hiroaki", ""], ["Takagi", "Takuya", ""], ["Suzuki", "Hirofumi", ""], ["Goto", "Keisuke", ""], ["Ohori", "Kotaro", ""], ["Arimura", "Hiroki", ""]]}, {"id": "2004.08255", "submitter": "Zhiwei Ckhen", "authors": "Zhiwei Chen, Aoqian Zhang", "title": "A Survey of Approximate Quantile Computation on Large-scale Data\n  (Technical Report)", "comments": null, "journal-ref": "IEEE Access 8 (2020): 34585-34597", "doi": "10.1109/ACCESS.2020.2974919", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data volume grows extensively, data profiling helps to extract metadata of\nlarge-scale data. However, one kind of metadata, order statistics, is difficult\nto be computed because they are not mergeable or incremental. Thus, the\nlimitation of time and memory space does not support their computation on\nlarge-scale data. In this paper, we focus on an order statistic, quantiles, and\npresent a comprehensive analysis of studies on approximate quantile\ncomputation. Both deterministic algorithms and randomized algorithms that\ncompute approximate quantiles over streaming models or distributed models are\ncovered. Then, multiple techniques for improving the efficiency and performance\nof approximate quantile algorithms in various scenarios, such as skewed data\nand high-speed data streams, are presented. Finally, we conclude with coverage\nof existing packages in different languages and with a brief discussion of the\nfuture direction in this area.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 14:10:00 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Chen", "Zhiwei", ""], ["Zhang", "Aoqian", ""]]}, {"id": "2004.08257", "submitter": "Elwin Huaman", "authors": "Elwin Huaman, Elias K\\\"arle and Dieter Fensel", "title": "Duplication Detection in Knowledge Graphs: Literature and Tools", "comments": "Submitted to EKAW 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, an increasing amount of knowledge graphs (KGs) have been\ncreated as a means to store cross-domain knowledge and billion of facts, which\nare the basis of costumers' applications like search engines. However, KGs\ninevitably have inconsistencies such as duplicates that might generate\nconflicting property values. Duplication detection (DD) aims to identify\nduplicated entities and resolve their conflicting property values effectively\nand efficiently. In this paper, we perform a literature review on DD methods\nand tools, and an evaluation of them. Our main contributions are a performance\nevaluation of DD tools in KGs, improvement suggestions, and a DD workflow to\nsupport future development of DD tools, which are based on desirable features\ndetected through this study.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 14:12:40 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Huaman", "Elwin", ""], ["K\u00e4rle", "Elias", ""], ["Fensel", "Dieter", ""]]}, {"id": "2004.08284", "submitter": "Wang Xi", "authors": "Xi Wang, Chen Wang", "title": "Time Series Data Cleaning with Regular and Irregular Time Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Errors are prevalent in time series data, especially in the industrial field.\nData with errors could not be stored in the database, which results in the loss\nof data assets. Handling the dirty data in time series is non-trivial, when\ngiven irregular time intervals. At present, to deal with these time series\ncontaining errors, besides keeping original erroneous data, discarding\nerroneous data and manually checking erroneous data, we can also use the\ncleaning algorithm widely used in the database to automatically clean the time\nseries data. This survey provides a classification of time series data cleaning\ntechniques and comprehensively reviews the state-of-the-art methods of each\ntype. In particular, we have a special focus on the irregular time intervals.\nBesides we summarize data cleaning tools, systems and evaluation criteria from\nresearch and industry. Finally, we highlight possible directions time series\ndata cleaning.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 14:53:59 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 10:26:34 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 21:00:58 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wang", "Xi", ""], ["Wang", "Chen", ""]]}, {"id": "2004.08425", "submitter": "David Daly", "authors": "Henrik Ingo and David Daly", "title": "Automated System Performance Testing at MongoDB", "comments": "Author Preprint. Appearing in DBTest.io 2020", "journal-ref": null, "doi": "10.1145/3395032.3395323", "report-no": null, "categories": "cs.DB cs.PF cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed Systems Infrastructure (DSI) is MongoDB's framework for running\nfully automated system performance tests in our Continuous Integration (CI)\nenvironment. To run in CI it needs to automate everything end-to-end:\nprovisioning and deploying multi-node clusters, executing tests, tuning the\nsystem for repeatable results, and collecting and analyzing the results. Today\nDSI is MongoDB's most used and most useful performance testing tool. It runs\nalmost 200 different benchmarks in daily CI, and we also use it for manual\nperformance investigations. As we can alert the responsible engineer in a\ntimely fashion, all but one of the major regressions were fixed before the\n4.2.0 release. We are also able to catch net new improvements, of which DSI\ncaught 17. We open sourced DSI in March 2020.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 19:14:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ingo", "Henrik", ""], ["Daly", "David", ""]]}, {"id": "2004.08473", "submitter": "Dongfang Zhao", "authors": "Dongfang Zhao", "title": "Fork-Resilient Cross-Blockchain Transactions through Algebraic Topology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-blockchain transaction (CBT) serves as a cornerstone for the\nnext-generation, blockchain-based data management systems. However,\nstate-of-the-art CBT models do not address the effect of the possible local\nfork suspension that might invalidate the entire CBT. This paper takes an\nalgebraic-topological approach to abstract the blockchains and their\ntransactions into simplicial complexes and shows that CBTs cannot complete in\neither a \\textit{committed} or an \\textit{aborted} status by a $t$-resilient\nmessage-passing protocol. This result implies that a more sophisticated model\nis in need to support CBTs and, thus, sheds light on the future blockchain\ndesigns.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 22:30:52 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 19:47:35 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhao", "Dongfang", ""]]}, {"id": "2004.08604", "submitter": "Massimo Cafaro", "authors": "Italo Epicoco, Catiuscia Melle, Massimo Cafaro, Marco Pulimeno and\n  Giuseppe Morleo", "title": "UDDSketch: Accurate Tracking of Quantiles in Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present UDDSketch (Uniform DDSketch), a novel sketch for fast and accurate\ntracking of quantiles in data streams. This sketch is heavily inspired by the\nrecently introduced DDSketch, and is based on a novel bucket collapsing\nprocedure that allows overcoming the intrinsic limits of the corresponding\nDDSketch procedures. Indeed, the DDSketch bucket collapsing procedure does not\nallow the derivation of formal guarantees on the accuracy of quantile\nestimation for data which does not follow a sub-exponential distribution. On\nthe contrary, UDDSketch is designed so that accuracy guarantees can be given\nover the full range of quantiles and for arbitrary distribution in input.\nMoreover, our algorithm fully exploits the budgeted memory adaptively in order\nto guarantee the best possible accuracy over the full range of quantiles.\nExtensive experimental results on synthetic datasets confirm the validity of\nour approach.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 12:18:32 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Epicoco", "Italo", ""], ["Melle", "Catiuscia", ""], ["Cafaro", "Massimo", ""], ["Pulimeno", "Marco", ""], ["Morleo", "Giuseppe", ""]]}, {"id": "2004.08771", "submitter": "Florin Rusu", "authors": "Yujing Ma and Florin Rusu", "title": "Heterogeneous CPU+GPU Stochastic Gradient Descent Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely-adopted practice is to train deep learning models with specialized\nhardware accelerators, e.g., GPUs or TPUs, due to their superior performance on\nlinear algebra operations. However, this strategy does not employ effectively\nthe extensive CPU and memory resources -- which are used only for\npreprocessing, data transfer, and scheduling -- available by default on the\naccelerated servers. In this paper, we study training algorithms for deep\nlearning on heterogeneous CPU+GPU architectures. Our two-fold objective --\nmaximize convergence rate and resource utilization simultaneously -- makes the\nproblem challenging. In order to allow for a principled exploration of the\ndesign space, we first introduce a generic deep learning framework that\nexploits the difference in computational power and memory hierarchy between CPU\nand GPU through asynchronous message passing. Based on insights gained through\nexperimentation with the framework, we design two heterogeneous asynchronous\nstochastic gradient descent (SGD) algorithms. The first algorithm -- CPU+GPU\nHogbatch -- combines small batches on CPU with large batches on GPU in order to\nmaximize the utilization of both resources. However, this generates an\nunbalanced model update distribution which hinders the statistical convergence.\nThe second algorithm -- Adaptive Hogbatch -- assigns batches with continuously\nevolving size based on the relative speed of CPU and GPU. This balances the\nmodel updates ratio at the expense of a customizable decrease in utilization.\nWe show that the implementation of these algorithms in the proposed CPU+GPU\nframework achieves both faster convergence and higher resource utilization than\nTensorFlow on several real datasets and on two computing architectures -- an\non-premises server and a cloud instance.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 05:21:20 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ma", "Yujing", ""], ["Rusu", "Florin", ""]]}, {"id": "2004.09005", "submitter": "Kien Nguyen", "authors": "Gabriel Ghinita, Kien Nguyen, Mihai Maruseac, Cyrus Shahabi", "title": "A Secure Location-based Alert System with Tunable Privacy-Performance\n  Trade-off", "comments": "32 pages, GeoInformatica 2020", "journal-ref": null, "doi": "10.1007/s10707-020-00410-1", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring location updates from mobile users has important applications in\nmany areas, ranging from public safety and national security to social networks\nand advertising. However, sensitive information can be derived from movement\npatterns, thus protecting the privacy of mobile users is a major concern. Users\nmay only be willing to disclose their locations when some condition is met, for\ninstance in proximity of a disaster area or an event of interest. Currently,\nsuch functionality can be achieved using searchable encryption. Such\ncryptographic primitives provide provable guarantees for privacy, and allow\ndecryption only when the location satisfies some predicate. Nevertheless, they\nrely on expensive pairing-based cryptography (PBC), of which direct application\nto the domain of location updates leads to impractical solutions. We propose\nsecure and efficient techniques for private processing of location updates that\ncomplement the use of PBC and lead to significant gains in performance by\nreducing the amount of required pairing operations. We implement two\noptimizations that further improve performance: materialization of results to\nexpensive mathematical operations, and parallelization. We also propose an\nheuristic that brings down the computational overhead through enlarging an\nalert zone by a small factor (given as system parameter), therefore trading off\na small and controlled amount of privacy for significant performance gains.\nExtensive experimental results show that the proposed techniques significantly\nimprove performance compared to the baseline, and reduce the searchable\nencryption overhead to a level that is practical in a computing environment\nwith reasonable resources, such as the cloud.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 00:57:44 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Ghinita", "Gabriel", ""], ["Nguyen", "Kien", ""], ["Maruseac", "Mihai", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "2004.09045", "submitter": "Longbin Lai", "authors": "Lu Qin, Longbin Lai, Kongzhang Hao, Zhongxin Zhou, Yiwei Zhao, Yuxing\n  Han, Xuemin Lin, Zhengping Qian, Jingren Zhou", "title": "Taming the Expressiveness and Programmability of Graph Analytical\n  Queries", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph database has enjoyed a boom in the last decade, and graph queries\naccordingly gain a lot of attentions from both the academia and industry. We\nfocus on analytical queries in this paper. While analyzing existing\ndomain-specific languages (DSLs) for analytical queries regarding the\nperspectives of completeness, expressiveness and programmability, we find out\nthat none of existing work has achieved a satisfactory coverage of these\nperspectives. Motivated by this, we propose the \\flash DSL, which is named\nafter the three primitive operators Filter, LocAl and PuSH. We prove that\n\\flash is Turing complete (completeness), and show that it achieves both good\nexpressiveness and programmability for analytical queries. We provide an\nimplementation of \\flash based on code generation, and compare it with native\nC++ codes and existing DSL using representative queries. The experiment results\ndemonstrate \\flash's expressiveness, and its capability of programming complex\nalgorithms that achieve satisfactory runtime.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 04:08:28 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 07:28:06 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Qin", "Lu", ""], ["Lai", "Longbin", ""], ["Hao", "Kongzhang", ""], ["Zhou", "Zhongxin", ""], ["Zhao", "Yiwei", ""], ["Han", "Yuxing", ""], ["Lin", "Xuemin", ""], ["Qian", "Zhengping", ""], ["Zhou", "Jingren", ""]]}, {"id": "2004.09350", "submitter": "Patrick Damme", "authors": "Patrick Damme, Annett Ungeth\\\"um, Johannes Pietrzyk, Alexander Krause,\n  Dirk Habich, Wolfgang Lehner", "title": "MorphStore: Analytical Query Engine with a Holistic Compression-Enabled\n  Processing Model", "comments": "Submitted to PVLDB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present MorphStore, an open-source in-memory columnar\nanalytical query engine with a novel holistic compression-enabled processing\nmodel. Basically, compression using lightweight integer compression algorithms\nalready plays an important role in existing in-memory column-store database\nsystems, but mainly for base data. In particular, during query processing,\nthese systems only keep the data compressed until an operator cannot process\nthe compressed data directly, whereupon the data is decompressed, but not\nrecompressed. Thus, the full potential of compression during query processing\nis not exploited. To overcome that, we developed a novel compression-enabled\nprocessing model as presented in this paper. As we are going to show, the\ncontinuous usage of compression for all base data and all intermediates is very\nbeneficial to reduce the overall memory footprint as well as to improve the\nquery performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:50:50 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Damme", "Patrick", ""], ["Ungeth\u00fcm", "Annett", ""], ["Pietrzyk", "Johannes", ""], ["Krause", "Alexander", ""], ["Habich", "Dirk", ""], ["Lehner", "Wolfgang", ""]]}, {"id": "2004.09625", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Kanthi Sannappa Komar, Sanjukta Bhowmick and Sharma\n  Chakravarthy", "title": "A New Community Definition For MultiLayer Networks And A Novel Approach\n  For Its Efficient Computation", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.01737,\n  arXiv:1903.02641", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the use of MultiLayer Networks (or MLNs) for modeling and analysis is\ngaining popularity, it is becoming increasingly important to propose a\ncommunity definition that encompasses the multiple features represented by MLNs\nand develop algorithms for efficiently computing communities on MLNs.\nCurrently, communities for MLNs, are based on aggregating the networks into\nsingle graphs using different techniques (type independent, projection-based,\netc.) and applying single graph community detection algorithms, such as Louvain\nand Infomap on these graphs. This process results in different types of\ninformation loss (semantics and structure). To the best of our knowledge, in\nthis paper we propose, for the first time, a definition of community for\nheterogeneous MLNs (or HeMLNs) which preserves semantics as well as the\nstructure. Additionally, our basic definition can be extended to appropriately\nmatch the analysis objectives as needed.\n  In this paper, we present a structure and semantics preserving community\ndefinition for HeMLNs that is compatible with and is an extension of the\ntraditional definition for single graphs. We also present a framework for its\nefficient computation using a newly proposed decoupling approach. First, we\ndefine a k-community for connected k layers of a HeMLN. Then we propose a\nfamily of algorithms for its computation using the concept of bipartite graph\npairings. Further, for a broader analysis, we introduce several pairing\nalgorithms and weight metrics for composing binary HeMLN communities using\nparticipating community characteristics. Essentially, this results in an\nextensible family of community computations. We provide extensive experimental\nresults for showcasing the efficiency and analysis flexibility of the proposed\ncomputation using popular IMDb and DBLP data sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 20:38:09 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Santra", "Abhishek", ""], ["Komar", "Kanthi Sannappa", ""], ["Bhowmick", "Sanjukta", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "2004.09676", "submitter": "Yiming Lin", "authors": "Yiming Lin, Daokun Jiang, Roberto Yus, Georgios Bouloukakis, Andrew\n  Chio, Sharad Mehrotra, Nalini Venkatasubramanian", "title": "LOCATER: Cleaning WiFi Connectivity Datasets for Semantic Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the data cleaning challenges that arise in using WiFi\nconnectivity data to locate users to semantic indoor locations such as\nbuildings, regions, rooms. WiFi connectivity data consists of sporadic\nconnections between devices and nearby WiFi access points (APs), each of which\nmay cover a relatively large area within a building. Our system, entitled\nsemantic LOCATion cleanER (LOCATER), postulates semantic localization as a\nseries of data cleaning tasks - first, it treats the problem of determining the\nAP to which a device is connected between any two of its connection events as a\nmissing value detection and repair problem. It then associates the device with\nthe semantic subregion (e.g., a conference room in the region) by postulating\nit as a location disambiguation problem. LOCATER uses a bootstrapping\nsemi-supervised learning method for coarse localization and a probabilistic\nmethod to achieve finer localization. The paper shows that LOCATER can achieve\nsignificantly high accuracy at both the coarse and fine levels.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 23:34:48 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 06:32:35 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2020 00:48:48 GMT"}, {"version": "v4", "created": "Sat, 12 Sep 2020 22:27:22 GMT"}, {"version": "v5", "created": "Mon, 21 Sep 2020 19:31:24 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Lin", "Yiming", ""], ["Jiang", "Daokun", ""], ["Yus", "Roberto", ""], ["Bouloukakis", "Georgios", ""], ["Chio", "Andrew", ""], ["Mehrotra", "Sharad", ""], ["Venkatasubramanian", "Nalini", ""]]}, {"id": "2004.09990", "submitter": "Alexander Martin Mussgnug", "authors": "Alexander M. Mussgnug", "title": "A Philosophy of Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that while this discourse on data ethics is of critical importance,\nit is missing one fundamental point: If more and more efforts in business,\ngovernment, science, and our daily lives are data-driven, we should pay more\nattention to what exactly we are driven by. Therefore, we need more debate on\nwhat fundamental properties constitute data. In the first section of the paper,\nwe work from the fundamental properties necessary for statistical computation\nto a definition of statistical data. We define a statistical datum as the\ncoming together of substantive and numerical properties and differentiate\nbetween qualitative and quantitative data. Subsequently, we qualify our\ndefinition by arguing that for data to be practically useful, it needs to be\ncommensurable in a manner that reveals meaningful differences that allow for\nthe generation of relevant insights through statistical methodologies. In the\nsecond section, we focus on what our conception of data can contribute to the\ndiscourse on data ethics and beyond. First, we hold that the need for useful\ndata to be commensurable rules out an understanding of properties as\nfundamentally unique or equal. Second, we argue that practical concerns lead us\nto increasingly standardize how we operationalize a substantive property; in\nother words, how we formalize the relationship between the substantive and\nnumerical properties of data. Thereby, we also standardize the interpretation\nof a property. With our increasing reliance on data and data technologies,\nthese two characteristics of data affect our collective conception of reality.\nStatistical data's exclusion of the fundamentally unique and equal influences\nour perspective on the world, and the standardization of substantive properties\ncan be viewed as profound ontological practice, entrenching ever more pervasive\ninterpretations of phenomena in our everyday lives.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 14:47:24 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 12:36:57 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Mussgnug", "Alexander M.", ""]]}, {"id": "2004.10232", "submitter": "Visweswara Sai Prashanth Dintyala", "authors": "Visweswara Sai Prashanth Dintyala, Arpit Narechania, Joy Arulraj", "title": "SQLCheck: Automated Detection and Diagnosis of SQL Anti-Patterns", "comments": "18 pages (14 page paper, 1 page references, 2 page Appendix), 12\n  figures, Conference: SIGMOD'20", "journal-ref": null, "doi": "10.1145/3318464.3389754", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of database-as-a-service platforms has made deploying database\napplications easier than before. Now, developers can quickly create scalable\napplications. However, designing performant, maintainable, and accurate\napplications is challenging. Developers may unknowingly introduce anti-patterns\nin the application's SQL statements. These anti-patterns are design decisions\nthat are intended to solve a problem, but often lead to other problems by\nviolating fundamental design principles.\n  In this paper, we present SQLCheck, a holistic toolchain for automatically\nfinding and fixing anti-patterns in database applications. We introduce\ntechniques for automatically (1) detecting anti-patterns with high precision\nand recall, (2) ranking the anti-patterns based on their impact on performance,\nmaintainability, and accuracy of applications, and (3) suggesting alternative\nqueries and changes to the database design to fix these anti-patterns. We\ndemonstrate the prevalence of these anti-patterns in a large collection of\nqueries and databases collected from open-source repositories. We introduce an\nanti-pattern detection algorithm that augments query analysis with data\nanalysis. We present a ranking model for characterizing the impact of\nfrequently occurring anti-patterns. We discuss how SQLCheck suggests fixes for\nhigh-impact anti-patterns using rule-based query refactoring techniques. Our\nexperiments demonstrate that SQLCheck enables developers to create more\nperformant, maintainable, and accurate applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 18:34:05 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Dintyala", "Visweswara Sai Prashanth", ""], ["Narechania", "Arpit", ""], ["Arulraj", "Joy", ""]]}, {"id": "2004.10247", "submitter": "Larissa Shimomura", "authors": "Larissa C. Shimomura (Eindhoven University of Technology), George\n  Fletcher (Eindhoven University of Technology), Nikolay Yakovets (Eindhoven\n  University of Technology)", "title": "GGDs: Graph Generating Dependencies", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Graph Generating Dependencies (GGDs), a new class of dependencies\nfor property graphs. Extending the expressivity of state of the art constraint\nlanguages, GGDs can express both tuple- and equality-generating dependencies on\nproperty graphs, both of which find broad application in graph data management.\nWe provide the formal definition of GGDs, analyze the validation problem for\nGGDs, and demonstrate the practical utility of GGDs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 19:20:02 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 08:54:20 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 13:48:46 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Shimomura", "Larissa C.", "", "Eindhoven University of Technology"], ["Fletcher", "George", "", "Eindhoven University of Technology"], ["Yakovets", "Nikolay", "", "Eindhoven\n  University of Technology"]]}, {"id": "2004.10360", "submitter": "Chen Luo", "authors": "Chen Luo and Michael J. Carey", "title": "Breaking Down Memory Walls: Adaptive Memory Management in LSM-based\n  Storage Systems (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-Structured Merge-trees (LSM-trees) have been widely used in modern NoSQL\nsystems. Due to their out-of-place update design, LSM-trees have introduced\nmemory walls among the memory components of multiple LSM-trees and between the\nwrite memory and the buffer cache. Optimal memory allocation among these\nregions is non-trivial because it is highly workload-dependent. Existing\nLSM-tree implementations instead adopt static memory allocation schemes due to\ntheir simplicity and robustness, sacrificing performance. In this paper, we\nattempt to break down these memory walls in LSM-based storage systems. We first\npresent a memory management architecture that enables adaptive memory\nmanagement. We then present a partitioned memory component structure with new\nflush policies to better exploit the write memory to minimize the write cost.\nTo break down the memory wall between the write memory and the buffer cache, we\nfurther introduce a memory tuner that tunes the memory allocation between these\ntwo regions. We have conducted extensive experiments in the context of Apache\nAsterixDB using the YCSB and TPC-C benchmarks and we present the results here.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 01:34:58 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 23:59:30 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Luo", "Chen", ""], ["Carey", "Michael J.", ""]]}, {"id": "2004.10667", "submitter": "Yutaka Nagashima", "authors": "Yutaka Nagashima", "title": "Simple Dataset for Proof Method Recommendation in Isabelle/HOL (Dataset\n  Description)", "comments": "This is the preprint of our short paper accepted at the 13th\n  Conference on Intelligent Computer Mathematics (CICM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a growing number of researchers have applied machine learning to\nassist users of interactive theorem provers. However, the expressive nature of\nunderlying logics and esoteric structures of proof documents impede machine\nlearning practitioners, who often do not have much expertise in formal logic,\nlet alone Isabelle/HOL, from achieving a large scale success in this field. In\nthis data description, we present a simple dataset that contains data on over\n400k proof method applications along with over 100 extracted features for each\nin a format that can be processed easily without any knowledge about formal\nlogic. Our simple data format allows machine learning practitioners to try\nmachine learning tools to predict proof methods in Isabelle/HOL without\nrequiring domain expertise in logic.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 12:00:11 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 06:38:37 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 07:46:04 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Nagashima", "Yutaka", ""]]}, {"id": "2004.10898", "submitter": "Badrish Chandramouli", "authors": "Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan\n  Li, Umar Farooq Minhas, Per-{\\AA}ke Larson, Donald Kossmann, Rajeev Acharya", "title": "Qd-tree: Learning Data Layouts for Big Data Analytics", "comments": "ACM SIGMOD 2020", "journal-ref": null, "doi": "10.1145/3318464.3389770", "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corporations today collect data at an unprecedented and accelerating scale,\nmaking the need to run queries on large datasets increasingly important.\nTechnologies such as columnar block-based data organization and compression\nhave become standard practice in most commercial database systems. However, the\nproblem of best assigning records to data blocks on storage is still open. For\nexample, today's systems usually partition data by arrival time into row\ngroups, or range/hash partition the data based on selected fields. For a given\nworkload, however, such techniques are unable to optimize for the important\nmetric of the number of blocks accessed by a query. This metric directly\nrelates to the I/O cost, and therefore performance, of most analytical queries.\nFurther, they are unable to exploit additional available storage to drive this\nmetric down further.\n  In this paper, we propose a new framework called a query-data routing tree,\nor qd-tree, to address this problem, and propose two algorithms for their\nconstruction based on greedy and deep reinforcement learning techniques.\nExperiments over benchmark and real workloads show that a qd-tree can provide\nphysical speedups of more than an order of magnitude compared to current\nblocking schemes, and can reach within 2X of the lower bound for data skipping\nbased on selectivity, while providing complete semantic descriptions of created\nblocks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 23:42:59 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yang", "Zongheng", ""], ["Chandramouli", "Badrish", ""], ["Wang", "Chi", ""], ["Gehrke", "Johannes", ""], ["Li", "Yinan", ""], ["Minhas", "Umar Farooq", ""], ["Larson", "Per-\u00c5ke", ""], ["Kossmann", "Donald", ""], ["Acharya", "Rajeev", ""]]}, {"id": "2004.11375", "submitter": "Wolfgang Gatterbauer", "authors": "Aristotelis Leventidis, Jiahui Zhang, Cody Dunne, Wolfgang\n  Gatterbauer, H.V. Jagadish, Mirek Riedewald", "title": "QueryVis: Logic-based diagrams help users understand complicated SQL\n  queries faster", "comments": "Full version of paper appearing in SIGMOD 2020", "journal-ref": null, "doi": "10.1145/3318464.3389767", "report-no": null, "categories": "cs.DB cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the meaning of existing SQL queries is critical for code\nmaintenance and reuse. Yet SQL can be hard to read, even for expert users or\nthe original creator of a query. We conjecture that it is possible to capture\nthe logical intent of queries in \\emph{automatically-generated visual diagrams}\nthat can help users understand the meaning of queries faster and more\naccurately than SQL text alone. We present initial steps in that direction with\nvisual diagrams that are based on the first-order logic foundation of SQL and\ncan capture the meaning of deeply nested queries. Our diagrams build upon a\nrich history of diagrammatic reasoning systems in logic and were designed using\na large body of human-computer interaction best practices: they are\n\\emph{minimal} in that no visual element is superfluous; they are\n\\emph{unambiguous} in that no two queries with different semantics map to the\nsame visualization; and they \\emph{extend} previously existing visual\nrepresentations of relational schemata and conjunctive queries in a natural\nway. An experimental evaluation involving 42 users on Amazon Mechanical Turk\nshows that with only a 2--3 minute static tutorial, participants could\ninterpret queries meaningfully faster with our diagrams than when reading SQL\nalone. Moreover, we have evidence that our visual diagrams result in\nparticipants making fewer errors than with SQL. We believe that more regular\nexposure to diagrammatic representations of SQL can give rise to a\n\\emph{pattern-based} and thus more intuitive use and re-use of SQL. All details\non the experimental study, the evaluation stimuli, raw data, and analyses, and\nsource code are available at https://osf.io/mycr2\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:55:32 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Leventidis", "Aristotelis", ""], ["Zhang", "Jiahui", ""], ["Dunne", "Cody", ""], ["Gatterbauer", "Wolfgang", ""], ["Jagadish", "H. V.", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2004.11870", "submitter": "Domenico Fabio Savo", "authors": "Gianluca Cima (1), Domenico Lembo (1), Riccardo Rosati (1), Domenico\n  Fabio Savo (2) ((1) Sapienza Universit\\`a di Roma, (2) Universit\\`a degli\n  Studi di Bergamo)", "title": "CQE in Description Logics Through Instance Indistinguishability\n  (extended version)", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study privacy-preserving query answering in Description Logics (DLs).\nSpecifically, we consider the approach of controlled query evaluation (CQE)\nbased on the notion of instance indistinguishability. We derive data complexity\nresults for query answering over DL-Lite$_{\\mathcal{R}}$ ontologies, through a\ncomparison with an alternative, existing confidentiality-preserving approach to\nCQE. Finally, we identify a semantically well-founded notion of approximated\nquery answering for CQE, and prove that, for DL-Lite$_{\\mathcal{R}}$\nontologies, this form of CQE is tractable with respect to data complexity and\nis first-order rewritable, i.e., it is always reducible to the evaluation of a\nfirst-order query over the data instance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:28:24 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Cima", "Gianluca", ""], ["Lembo", "Domenico", ""], ["Rosati", "Riccardo", ""], ["Savo", "Domenico Fabio", ""]]}, {"id": "2004.12108", "submitter": "Mahawaga Arachchige Pathum Chamikara", "authors": "M.A.P. Chamikara, P.Bertok, I.Khalil, D.Liu, S.Camtepe", "title": "Privacy Preserving Distributed Machine Learning with Federated Learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.comcom.2021.02.014", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing and distributed machine learning have advanced to a level that\ncan revolutionize a particular organization. Distributed devices such as the\nInternet of Things (IoT) often produce a large amount of data, eventually\nresulting in big data that can be vital in uncovering hidden patterns, and\nother insights in numerous fields such as healthcare, banking, and policing.\nData related to areas such as healthcare and banking can contain potentially\nsensitive data that can become public if they are not appropriately sanitized.\nFederated learning (FedML) is a recently developed distributed machine learning\n(DML) approach that tries to preserve privacy by bringing the learning of an ML\nmodel to data owners'. However, literature shows different attack methods such\nas membership inference that exploit the vulnerabilities of ML models as well\nas the coordinating servers to retrieve private data. Hence, FedML needs\nadditional measures to guarantee data privacy. Furthermore, big data often\nrequires more resources than available in a standard computer. This paper\naddresses these issues by proposing a distributed perturbation algorithm named\nas DISTPAB, for privacy preservation of horizontally partitioned data. DISTPAB\nalleviates computational bottlenecks by distributing the task of privacy\npreservation utilizing the asymmetry of resources of a distributed environment,\nwhich can have resource-constrained devices as well as high-performance\ncomputers. Experiments show that DISTPAB provides high accuracy, high\nefficiency, high scalability, and high attack resistance. Further experiments\non privacy-preserving FedML show that DISTPAB is an excellent solution to stop\nprivacy leaks in DML while preserving high data utility.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 10:51:36 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 02:48:00 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Chamikara", "M. A. P.", ""], ["Bertok", "P.", ""], ["Khalil", "I.", ""], ["Liu", "D.", ""], ["Camtepe", "S.", ""]]}, {"id": "2004.12424", "submitter": "Yajun Yang", "authors": "Yajun Yang, Hang Zhang, Hong Gao, Qinghua Hu, Xin Wang", "title": "An Efficient Index Method for the Optimal Route Query over Multi-Cost\n  Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart city has been consider the wave of the future and the route\nrecommendation in networks is a fundamental problem in it. Most existing\napproaches for the shortest route problem consider that there is only one kind\nof cost in networks. However, there always are several kinds of cost in\nnetworks and users prefer to select an optimal route under the global\nconsideration of these kinds of cost. In this paper, we study the problem of\nfinding the optimal route in the multi-cost networks. We prove this problem is\nNP-hard and the existing index techniques cannot be used to this problem. We\npropose a novel partition-based index with contour skyline techniques to find\nthe optimal route. We propose a vertex-filtering algorithm to facilitate the\nquery processing. We conduct extensive experiments on six real-life networks\nand the experimental results show that our method has an improvement in\nefficiency by an order of magnitude compared to the previous heuristic\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 16:04:53 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yang", "Yajun", ""], ["Zhang", "Hang", ""], ["Gao", "Hong", ""], ["Hu", "Qinghua", ""], ["Wang", "Xin", ""]]}, {"id": "2004.12628", "submitter": "Sven Hertling", "authors": "Jan Portisch, Sven Hertling, Heiko Paulheim", "title": "Visual Analysis of Ontology Matching Results with the MELT Dashboard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this demo, we introduce MELT Dashboard, an interactive Web user interface\nfor ontology alignment evaluation which is created with the existing Matching\nEvaLuation Toolkit (MELT). Compared to existing, static evaluation interfaces\nin the ontology matching domain, our dashboard allows for interactive\nself-service analyses such as a drill down into the matcher performance for\ndata type properties or into the performance of matchers within a certain\nconfidence threshold. In addition, the dashboard offers detailed group\nevaluation capabilities that allow for the application in broad evaluation\ncampaigns such as the Ontology Alignment Evaluation Initiative (OAEI).\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:11:00 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Portisch", "Jan", ""], ["Hertling", "Sven", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2004.12821", "submitter": "Romain Rouvoy", "authors": "Sacha Brisset and Romain Rouvoy and Renaud Pawlak and Lionel\n  Seinturier", "title": "SFTM: Fast Comparison of Web Documents using Similarity-based Flexible\n  Tree Matching", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree matching techniques have been investigated in many fields, including web\ndata mining and extraction, as a key component to analyze the content of web\ndocuments, existing tree matching approaches, like Tree-Edit Distance (TED) or\nFlexible Tree Matching (FTM), fail to scale beyond a few hundreds of nodes,\nwhich is far below the average complexity of existing web online documents and\napplications. In this paper, we therefore propose a novel Similarity-based\nFlexible Tree Matching algorithm (SFTM), which is the first algorithm to enable\ntree matching on real-life web documents with practical computation times. In\nparticular, we approach tree matching as an optimisation problem and we\nleverage node labels and local topology similarity in order to avoid any\ncombinatorial explosion. Our practical evaluation demonstrates that our\napproach compares to the reference implementation of TED qualitatively, while\nimproving the computation times by two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:02:09 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Brisset", "Sacha", ""], ["Rouvoy", "Romain", ""], ["Pawlak", "Renaud", ""], ["Seinturier", "Lionel", ""]]}, {"id": "2004.12929", "submitter": "Alfredo Nazabal", "authors": "Alfredo Nazabal, Christopher K.I. Williams, Giovanni Colavizza, Camila\n  Rangel Smith, Angus Williams", "title": "Data Engineering for Data Analytics: A Classification of the Issues, and\n  Case Studies", "comments": "24 pages, 1 figure, submitted to IEEE Transactions on Knowledge and\n  Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the situation where a data analyst wishes to carry out an analysis\non a given dataset. It is widely recognized that most of the analyst's time\nwill be taken up with \\emph{data engineering} tasks such as acquiring,\nunderstanding, cleaning and preparing the data. In this paper we provide a\ndescription and classification of such tasks into high-levels groups, namely\ndata organization, data quality and feature engineering. We also make available\nfour datasets and example analyses that exhibit a wide variety of these\nproblems, to help encourage the development of tools and techniques to help\nreduce this burden and push forward research towards the automation or\nsemi-automation of the data engineering process.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 16:42:40 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Nazabal", "Alfredo", ""], ["Williams", "Christopher K. I.", ""], ["Colavizza", "Giovanni", ""], ["Smith", "Camila Rangel", ""], ["Williams", "Angus", ""]]}, {"id": "2004.13021", "submitter": "Mahdi Bohlouli", "authors": "Mahdi Bohlouli, Frank Schulz, Lefteris Angelis, David Pahor, Ivona\n  Brandic, David Atlan, Rosemary Tate", "title": "Towards an Integrated Platform for Big Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of data in the world is expanding rapidly. Every day, huge amounts\nof data are created by scientific experiments, companies, and end users'\nactivities. These large data sets have been labeled as \"Big Data\", and their\nstorage, processing and analysis presents a plethora of new challenges to\ncomputer science researchers and IT professionals. In addition to efficient\ndata management, additional complexity arises from dealing with semi-structured\nor unstructured data, and from time critical processing requirements. In order\nto understand these massive amounts of data, advanced visualization and data\nexploration techniques are required. Innovative approaches to these challenges\nhave been developed during recent years, and continue to be a hot topic for\nre-search and industry in the future. An investigation of current approaches\nreveals that usually only one or two aspects are ad-dressed, either in the data\nmanagement, processing, analysis or visualization. This paper presents the\nvision of an integrated plat-form for big data analysis that combines all these\naspects. Main benefits of this approach are an enhanced scalability of the\nwhole platform, a better parameterization of algorithms, a more efficient usage\nof system resources, and an improved usability during the end-to-end data\nanalysis process.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 03:15:23 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Bohlouli", "Mahdi", ""], ["Schulz", "Frank", ""], ["Angelis", "Lefteris", ""], ["Pahor", "David", ""], ["Brandic", "Ivona", ""], ["Atlan", "David", ""], ["Tate", "Rosemary", ""]]}, {"id": "2004.13115", "submitter": "Shantanu Sharma", "authors": "Peeyush Gupta, Yin Li, Sharad Mehrotra, Nisha Panwar, Shantanu Sharma,\n  Sumaya Almanee", "title": "Obscure: Information-Theoretically Secure, Oblivious, and Verifiable\n  Aggregation Queries on Secret-Shared Outsourced Data -- Full Version", "comments": "A preliminary version of this work was accepted in VLDB 2019. This\n  version has been accepted in IEEE Transactions on Knowledge and Data\n  Engineering (TKDE). The final published version of this paper may differ from\n  this accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite exciting progress on cryptography, secure and efficient query\nprocessing over outsourced data remains an open challenge. We develop a\ncommunication-efficient and information-theoretically secure system, entitled\nObscure for aggregation queries with conjunctive or disjunctive predicates,\nusing secret-sharing. Obscure is strongly secure (i.e., secure regardless of\nthe computational-capabilities of an adversary) and prevents the network, as\nwell as, the (adversarial) servers to learn the user's queries, results, or the\ndatabase. In addition, Obscure provides additional security features, such as\nhiding access-patterns (i.e., hiding the identity of the tuple satisfying a\nquery) and hiding query-patterns (i.e., hiding which two queries are\nidentical). Also, Obscure does not require any communication between any two\nservers that store the secret-shared data before/during/after the query\nexecution. Moreover, our techniques deal with the secret-shared data that is\noutsourced by a single or multiple database owners, as well as, allows a user,\nwhich may not be the database owner, to execute the query over secret-shared\ndata. We further develop (non-mandatory) privacy-preserving result verification\nalgorithms that detect malicious behaviors, and experimentally validate the\nefficiency of Obscure on large datasets, the size of which prior approaches of\nsecret-sharing or multi-party computation systems have not scaled to.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 19:27:21 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Gupta", "Peeyush", ""], ["Li", "Yin", ""], ["Mehrotra", "Sharad", ""], ["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Almanee", "Sumaya", ""]]}, {"id": "2004.13237", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi", "title": "An ASP-Based Approach to Counterfactual Explanations for Classification", "comments": "Revised and extended version. To appear in Proc. RuleML+RR, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose answer-set programs that specify and compute counterfactual\ninterventions as a basis for causality-based explanations to decisions produced\nby classification models. They can be applied with black-box models and models\nthat can be specified as logic programs, such as rule-based classifiers. The\nmain focus in on the specification and computation of maximum responsibility\ncausal explanations. The use of additional semantic knowledge is investigated.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 01:36:26 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 03:56:13 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Bertossi", "Leopoldo", ""]]}, {"id": "2004.13495", "submitter": "Ana Nunes Alonso", "authors": "Ana Nunes Alonso, Jo\\~ao Abreu, David Nunes, Andr\\'e Vieira, Luiz\n  Santos, T\\'ercio Soares, Jos\\'e Pereira", "title": "Towards a Polyglot Data Access Layer for a Low-Code Application\n  Development Platform", "comments": "Extended version of \"Building a Polyglot Data Access Layer for a\n  Low-Code Application Development Platform\", to appear in conference DAIS'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-code application development as proposed by the OutSystems Platform\nenables fast mobile and desktop application development and deployment. It\nhinges on visual development of the interface and business logic but also on\neasy integration with data stores and services while delivering robust\napplications that scale. Data integration increasingly means accessing a\nvariety of NoSQL stores. Unfortunately, the diversity of data and processing\nmodels, that make them useful in the first place, is difficult to reconcile\nwith the simplification of abstractions exposed to developers in a low-code\nplatform. Moreover, NoSQL data stores also rely on a variety of general purpose\nand custom scripting languages as their main interfaces. In this paper we\npropose a polyglot data access layer for the OutSystems Platform that uses SQL\nwith optional embedded script snippets to bridge the gap between low-code and\nfull access to NoSQL stores. In detail, we characterize the challenges for\nintegrating a variety of NoSQL data stores; we describe the architecture and\nproof-of-concept implementation; and evaluate it with a sample application.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:27:33 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Alonso", "Ana Nunes", ""], ["Abreu", "Jo\u00e3o", ""], ["Nunes", "David", ""], ["Vieira", "Andr\u00e9", ""], ["Santos", "Luiz", ""], ["Soares", "T\u00e9rcio", ""], ["Pereira", "Jos\u00e9", ""]]}, {"id": "2004.13527", "submitter": "Daniel Leite", "authors": "Leticia Decker, Daniel Leite, Luca Giommi, Daniele Bonacorsi", "title": "Real-Time Anomaly Detection in Data Centers for Log-based Predictive\n  Maintenance using an Evolving Fuzzy-Rule-Based Approach", "comments": "9 pages, 6 figures, 1 table, IEEE World Congress on Computational\n  Intelligence (WCCI 2020). arXiv admin note: substantial text overlap with\n  arXiv:2004.09986", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of anomalous behaviors in data centers is crucial to predictive\nmaintenance and data safety. With data centers, we mean any computer network\nthat allows users to transmit and exchange data and information. In particular,\nwe focus on the Tier-1 data center of the Italian Institute for Nuclear Physics\n(INFN), which supports the high-energy physics experiments at the Large Hadron\nCollider (LHC) in Geneva. The center provides resources and services needed for\ndata processing, storage, analysis, and distribution. Log records in the data\ncenter is a stochastic and non-stationary phenomenon in nature. We propose a\nreal-time approach to monitor and classify log records based on sliding time\nwindows, and a time-varying evolving fuzzy-rule-based classification model. The\nmost frequent log pattern according to a control chart is taken as the normal\nsystem status. We extract attributes from time windows to gradually develop and\nupdate an evolving Gaussian Fuzzy Classifier (eGFC) on the fly. The real-time\nanomaly monitoring system has to provide encouraging results in terms of\naccuracy, compactness, and real-time operation.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 21:19:44 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Decker", "Leticia", ""], ["Leite", "Daniel", ""], ["Giommi", "Luca", ""], ["Bonacorsi", "Daniele", ""]]}, {"id": "2004.13843", "submitter": "Ram G Athreya", "authors": "Ram G Athreya, Srividya Bansal, Axel-Cyrille Ngonga Ngomo, Ricardo\n  Usbeck", "title": "Template-based Question Answering using Recursive Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network-based approach to automatically learn and\nclassify natural language questions into its corresponding template using\nrecursive neural networks. An obvious advantage of using neural networks is the\nelimination of the need for laborious feature engineering that can be\ncumbersome and error-prone. The input question is encoded into a vector\nrepresentation. The model is trained and evaluated on the LC-QuAD dataset\n(Large-scale Complex Question Answering Dataset). The LC-QuAD queries are\nannotated based on 38 unique templates that the model attempts to classify. The\nresulting model is evaluated against both the LC-QuAD dataset and the 7th\nQuestion Answering Over Linked Data (QALD-7) dataset. The recursive neural\nnetwork achieves template classification accuracy of 0.828 on the LC-QuAD\ndataset and an accuracy of 0.618 on the QALD-7 dataset. When the top-2 most\nlikely templates were considered the model achieves an accuracy of 0.945 on the\nLC-QuAD dataset and 0.786 on the QALD-7 dataset. After slot filling, the\noverall system achieves a macro F-score 0.419 on the LC-QuAD dataset and a\nmacro F-score of 0.417 on the QALD-7 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 18:14:39 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 00:26:26 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 01:41:26 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Athreya", "Ram G", ""], ["Bansal", "Srividya", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""], ["Usbeck", "Ricardo", ""]]}, {"id": "2004.14171", "submitter": "Gengchen Mai", "authors": "Gengchen Mai, Krzysztof Janowicz, Ling Cai, Rui Zhu, Blake Regalia, Bo\n  Yan, Meilin Shi, Ni Lao", "title": "SE-KGE: A Location-Aware Knowledge Graph Embedding Model for Geographic\n  Question Answering and Spatial Semantic Lifting", "comments": "Accepted to Transactions in GIS", "journal-ref": "Transactions in GIS, 2020", "doi": "10.1111/TGIS.12629", "report-no": null, "categories": "cs.DB cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning knowledge graph (KG) embeddings is an emerging technique for a\nvariety of downstream tasks such as summarization, link prediction, information\nretrieval, and question answering. However, most existing KG embedding models\nneglect space and, therefore, do not perform well when applied to (geo)spatial\ndata and tasks. For those models that consider space, most of them primarily\nrely on some notions of distance. These models suffer from higher computational\ncomplexity during training while still losing information beyond the relative\ndistance between entities. In this work, we propose a location-aware KG\nembedding model called SE-KGE. It directly encodes spatial information such as\npoint coordinates or bounding boxes of geographic entities into the KG\nembedding space. The resulting model is capable of handling different types of\nspatial reasoning. We also construct a geographic knowledge graph as well as a\nset of geographic query-answer pairs called DBGeo to evaluate the performance\nof SE-KGE in comparison to multiple baselines. Evaluation results show that\nSE-KGE outperforms these baselines on the DBGeo dataset for geographic logic\nquery answering task. This demonstrates the effectiveness of our\nspatially-explicit model and the importance of considering the scale of\ndifferent geographic entities. Finally, we introduce a novel downstream task\ncalled spatial semantic lifting which links an arbitrary location in the study\narea to entities in the KG via some relations. Evaluation on DBGeo shows that\nour model outperforms the baseline by a substantial margin.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 17:46:31 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Mai", "Gengchen", ""], ["Janowicz", "Krzysztof", ""], ["Cai", "Ling", ""], ["Zhu", "Rui", ""], ["Regalia", "Blake", ""], ["Yan", "Bo", ""], ["Shi", "Meilin", ""], ["Lao", "Ni", ""]]}, {"id": "2004.14471", "submitter": "Tianyu Li", "authors": "Tianyu Li, Matthew Butrovich, Amadou Ngom, Wan Shen Lim, Wes McKinney,\n  Andrew Pavlo", "title": "Mainlining Databases: Supporting Fast Transactional Workloads on\n  Universal Columnar Data File Formats", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of modern data processing tools has given rise to\nopen-source columnar data formats. The advantage of these formats is that they\nhelp organizations avoid repeatedly converting data to a new format for each\napplication. These formats, however, are read-only, and organizations must use\na heavy-weight transformation process to load data from on-line transactional\nprocessing (OLTP) systems. We aim to reduce or even eliminate this process by\ndeveloping a storage architecture for in-memory database management systems\n(DBMSs) that is aware of the eventual usage of its data and emits columnar\nstorage blocks in a universal open-source format. We introduce relaxations to\ncommon analytical data formats to efficiently update records and rely on a\nlightweight transformation process to convert blocks to a read-optimized layout\nwhen they are cold. We also describe how to access data from third-party\nanalytical tools with minimal serialization overhead. To evaluate our work, we\nimplemented our storage engine based on the Apache Arrow format and integrated\nit into the DB-X DBMS. Our experiments show that our approach achieves\ncomparable performance with dedicated OLTP DBMSs while enabling\norders-of-magnitude faster data exports to external data science and machine\nlearning tools than existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 20:55:52 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Li", "Tianyu", ""], ["Butrovich", "Matthew", ""], ["Ngom", "Amadou", ""], ["Lim", "Wan Shen", ""], ["McKinney", "Wes", ""], ["Pavlo", "Andrew", ""]]}, {"id": "2004.14478", "submitter": "Congcong Ge", "authors": "Congcong Ge, Yunjun Gao, Honghui Weng, Chong Zhang, Xiaoye Miao,\n  Baihua Zheng", "title": "KGClean: An Embedding Powered Knowledge Graph Cleaning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality assurance of the knowledge graph is a prerequisite for various\nknowledge-driven applications. We propose KGClean, a novel cleaning framework\npowered by knowledge graph embedding, to detect and repair the heterogeneous\ndirty data. In contrast to previous approaches that either focus on filling\nmissing data or clean errors violated limited rules, KGClean enables (i)\ncleaning both missing data and other erroneous values, and (ii) mining\npotential rules automatically, which expands the coverage of error detecting.\nKGClean first learns data representations by TransGAT, an effective knowledge\ngraph embedding model, which gathers the neighborhood information of each data\nand incorporates the interactions among data for casting data to continuous\nvector spaces with rich semantics. KGClean integrates an active learning-based\nclassification model, which identifies errors with a small seed of labels.\nKGClean utilizes an efficient PRO-repair strategy to repair errors using a\nnovel concept of propagation power. Extensive experiments on four typical\nknowledge graphs demonstrate the effectiveness of KGClean in practice.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 12:19:04 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Ge", "Congcong", ""], ["Gao", "Yunjun", ""], ["Weng", "Honghui", ""], ["Zhang", "Chong", ""], ["Miao", "Xiaoye", ""], ["Zheng", "Baihua", ""]]}, {"id": "2004.14541", "submitter": "Andreas Kipf", "authors": "Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons\n  Kemper, Tim Kraska, Thomas Neumann", "title": "RadixSpline: A Single-Pass Learned Index", "comments": "Third International Workshop on Exploiting Artificial Intelligence\n  Techniques for Data Management (aiDM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that learned models can outperform state-of-the-art\nindex structures in size and lookup performance. While this is a very promising\nresult, existing learned structures are often cumbersome to implement and are\nslow to build. In fact, most approaches that we are aware of require multiple\ntraining passes over the data.\n  We introduce RadixSpline (RS), a learned index that can be built in a single\npass over the data and is competitive with state-of-the-art learned index\nmodels, like RMI, in size and lookup performance. We evaluate RS using the SOSD\nbenchmark and show that it achieves competitive results on all datasets,\ndespite the fact that it only has two parameters.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 01:56:54 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 21:01:04 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kipf", "Andreas", ""], ["Marcus", "Ryan", ""], ["van Renen", "Alexander", ""], ["Stoian", "Mihail", ""], ["Kemper", "Alfons", ""], ["Kraska", "Tim", ""], ["Neumann", "Thomas", ""]]}, {"id": "2004.14619", "submitter": "Zheng Tang", "authors": "Milind Naphade, Shuo Wang, David Anastasiu, Zheng Tang, Ming-Ching\n  Chang, Xiaodong Yang, Liang Zheng, Anuj Sharma, Rama Chellappa, Pranamesh\n  Chakraborty", "title": "The 4th AI City Challenge", "comments": "Organization summary of the 4th AI City Challenge Workshop @ CVPR\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AI City Challenge was created to accelerate intelligent video analysis\nthat helps make cities smarter and safer. Transportation is one of the largest\nsegments that can benefit from actionable insights derived from data captured\nby sensors, where computer vision and deep learning have shown promise in\nachieving large-scale practical deployment. The 4th annual edition of the AI\nCity Challenge has attracted 315 participating teams across 37 countries, who\nleveraged city-scale real traffic data and high-quality synthetic data to\ncompete in four challenge tracks. Track 1 addressed video-based automatic\nvehicle counting, where the evaluation is conducted on both algorithmic\neffectiveness and computational efficiency. Track 2 addressed city-scale\nvehicle re-identification with augmented synthetic data to substantially\nincrease the training set for the task. Track 3 addressed city-scale\nmulti-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly\ndetection. The evaluation system shows two leader boards, in which a general\nleader board shows all submitted results, and a public leader board shows\nresults limited to our contest participation rules, that teams are not allowed\nto use external data in their work. The public leader board shows results more\nclose to real-world situations where annotated data are limited. Our results\nshow promise that AI technology can enable smarter and safer transportation\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 07:47:14 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Naphade", "Milind", ""], ["Wang", "Shuo", ""], ["Anastasiu", "David", ""], ["Tang", "Zheng", ""], ["Chang", "Ming-Ching", ""], ["Yang", "Xiaodong", ""], ["Zheng", "Liang", ""], ["Sharma", "Anuj", ""], ["Chellappa", "Rama", ""], ["Chakraborty", "Pranamesh", ""]]}, {"id": "2004.14794", "submitter": "Stefania Dumbrava", "authors": "Angela Bonifati, Stefania Dumbrava, and Haridimos Kondylakis", "title": "Graph Summarization", "comments": "To appear in the Encyclopedia of Big Data Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous and rapid growth of highly interconnected datasets, which are\nboth voluminous and complex, calls for the development of adequate processing\nand analytical techniques. One method for condensing and simplifying such\ndatasets is graph summarization. It denotes a series of application-specific\nalgorithms designed to transform graphs into more compact representations while\npreserving structural patterns, query answers, or specific property\ndistributions. As this problem is common to several areas studying graph\ntopologies, different approaches, such as clustering, compression, sampling, or\ninfluence detection, have been proposed, primarily based on statistical and\noptimization methods. The focus of our chapter is to pinpoint the main graph\nsummarization methods, but especially to focus on the most recent approaches\nand novel research trends on this topic, not yet covered by previous surveys.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:07:39 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 09:06:29 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 14:32:07 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Bonifati", "Angela", ""], ["Dumbrava", "Stefania", ""], ["Kondylakis", "Haridimos", ""]]}]