[{"id": "1305.0297", "submitter": "David Spivak", "authors": "David I. Spivak", "title": "The operad of wiring diagrams: formalizing a graphical language for\n  databases, recursion, and plug-and-play circuits", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB math.CT math.LO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Wiring diagrams, as seen in digital circuits, can be nested hierarchically\nand thus have an aspect of self-similarity. We show that wiring diagrams form\nthe morphisms of an operad $\\mcT$, capturing this self-similarity. We discuss\nthe algebra $\\Rel$ of mathematical relations on $\\mcT$, and in so doing use\nwiring diagrams as a graphical language with which to structure queries on\nrelational databases. We give the example of circuit diagrams as a special\ncase. We move on to show how plug-and-play devices and also recursion can be\nformulated in the operadic framework as well. Throughout we include many\nexamples and figures.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 21:24:17 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Spivak", "David I.", ""]]}, {"id": "1305.0502", "submitter": "Ruoming Jin", "authors": "Ruoming Jin and Guan Wang", "title": "Simple, Fast, and Scalable Reachability Oracle", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reachability oracle (or hop labeling) assigns each vertex v two sets of\nvertices: Lout(v) and Lin(v), such that u reaches v iff Lout(u) \\cap Lin(v)\n\\neq \\emptyset. Despite their simplicity and elegance, reachability oracles\nhave failed to achieve efficiency in more than ten years since their\nintroduction: the main problem is high construction cost, which stems from a\nset-cover framework and the need to materialize transitive closure. In this\npaper, we present two simple and efficient labeling algorithms,\nHierarchical-Labeling and Distribution-Labeling, which can work onmassive\nreal-world graphs: their construction time is an order of magnitude faster than\nthe setcover based labeling approach, and transitive closure materialization is\nnot needed. On large graphs, their index sizes and their query performance can\nnow beat the state-of-the-art transitive closure compression and online search\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 16:39:04 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2013 06:15:28 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Jin", "Ruoming", ""], ["Wang", "Guan", ""]]}, {"id": "1305.0507", "submitter": "Ruoming Jin", "authors": "Ruoming Jin, Ning Ruan, Bo You, Haixun Wang", "title": "Hub-Accelerator: Fast and Exact Shortest Path Computation in Large\n  Social Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortest path computation is one of the most fundamental operations for\nmanaging and analyzing large social networks. Though existing techniques are\nquite effective for finding the shortest path on large but sparse road\nnetworks, social graphs have quite different characteristics: they are\ngenerally non-spatial, non-weighted, scale-free, and they exhibit small-world\nproperties in addition to their massive size. In particular, the existence of\nhubs, those vertices with a large number of connections, explodes the search\nspace, making the shortest path computation surprisingly challenging. In this\npaper, we introduce a set of novel techniques centered around hubs,\ncollectively referred to as the Hub-Accelerator framework, to compute the\nk-degree shortest path (finding the shortest path between two vertices if their\ndistance is within k). These techniques enable us to significantly reduce the\nsearch space by either greatly limiting the expansion scope of hubs (using the\nnovel distance- preserving Hub-Network concept) or completely pruning away the\nhubs in the online search (using the Hub2-Labeling approach). The\nHub-Accelerator approaches are more than two orders of magnitude faster than\nBFS and the state-of-the-art approximate shortest path method Sketch for the\nshortest path computation. The Hub- Network approach does not introduce\nadditional index cost with light pre-computation cost; the index size and index\nconstruction cost of Hub2-Labeling are also moderate and better than or\ncomparable to the approximation indexing Sketch method.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 16:47:27 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Jin", "Ruoming", ""], ["Ruan", "Ning", ""], ["You", "Bo", ""], ["Wang", "Haixun", ""]]}, {"id": "1305.0699", "submitter": "Jimmy Lin", "authors": "Nima Asadi and Jimmy Lin", "title": "Fast, Incremental Inverted Indexing in Main Memory for Web-Scale\n  Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For text retrieval systems, the assumption that all data structures reside in\nmain memory is increasingly common. In this context, we present a novel\nincremental inverted indexing algorithm for web-scale collections that directly\nconstructs compressed postings lists in memory. Designing efficient in-memory\nalgorithms requires understanding modern processor architectures and memory\nhierarchies: in this paper, we explore the issue of postings lists contiguity.\nNaturally, postings lists that occupy contiguous memory regions are preferred\nfor retrieval, but maintaining contiguity increases complexity and slows\nindexing. On the other hand, allowing discontiguous index segments simplifies\nindex construction but decreases retrieval performance. Understanding this\ntradeoff is our main contribution: We find that co-locating small groups of\ninverted list segments yields query evaluation performance that is\nstatistically indistinguishable from fully-contiguous postings lists. In other\nwords, it is not necessary to lay out in-memory data structures such that all\npostings for a term are contiguous; we can achieve ideal performance with a\nrelatively small amount of effort.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 13:28:02 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Asadi", "Nima", ""], ["Lin", "Jimmy", ""]]}, {"id": "1305.1371", "submitter": "Fan Min", "authors": "Fan Min and William Zhu", "title": "Granular association rules for multi-valued data", "comments": "Proceedings of The 2013 Canadian Conference on Electrical and\n  Computer Engineering (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granular association rule is a new approach to reveal patterns hide in\nmany-to-many relationships of relational databases. Different types of data\nsuch as nominal, numeric and multi-valued ones should be dealt with in the\nprocess of rule mining. In this paper, we study multi-valued data and develop\ntechniques to filter out strong however uninteresting rules. An example of such\nrule might be \"male students rate movies released in 1990s that are NOT\nthriller.\" This kind of rules, called negative granular association rules,\noften overwhelms positive ones which are more useful. To address this issue, we\nfilter out negative granules such as \"NOT thriller\" in the process of granule\ngeneration. In this way, only positive granular association rules are generated\nand strong ones are mined. Experimental results on the movielens data set\nindicate that most rules are negative, and our technique is effective to filter\nthem out.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 01:08:05 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Min", "Fan", ""], ["Zhu", "William", ""]]}, {"id": "1305.1609", "submitter": "Florin Rusu", "authors": "Yu Cheng and Florin Rusu", "title": "Formal Representation of the SS-DB Benchmark and Experimental Evaluation\n  in EXTASCID", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the performance of scientific data processing systems is a\ndifficult task considering the plethora of application-specific solutions\navailable in this landscape and the lack of a generally-accepted benchmark. The\ndual structure of scientific data coupled with the complex nature of processing\ncomplicate the evaluation procedure further. SS-DB is the first attempt to\ndefine a general benchmark for complex scientific processing over raw and\nderived data. It fails to draw sufficient attention though because of the\nambiguous plain language specification and the extraordinary SciDB results. In\nthis paper, we remedy the shortcomings of the original SS-DB specification by\nproviding a formal representation in terms of ArrayQL algebra operators and\nArrayQL/SciQL constructs. These are the first formal representations of the\nSS-DB benchmark. Starting from the formal representation, we give a reference\nimplementation and present benchmark results in EXTASCID, a novel system for\nscientific data processing. EXTASCID is complete in providing native support\nboth for array and relational data and extensible in executing any user code\ninside the system by the means of a configurable metaoperator. These features\nresult in an order of magnitude improvement over SciDB at data loading,\nextracting derived data, and operations over derived data.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 19:07:28 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Cheng", "Yu", ""], ["Rusu", "Florin", ""]]}, {"id": "1305.1713", "submitter": "Meenesh  bhardwaj", "authors": "Meenesh Bhardwaj", "title": "Optimization of stochastic database cracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Variant Stochastic cracking is a significantly more resilient approach to\nadaptive indexing. It showed [1]that Stochastic cracking uses each query as a\nhint on how to reorganize data, but not blindly so; it gains resilience and\navoids performance bottlenecks by deliberately applying certain arbitrary\nchoices in its decision making. Therefore bring, adaptive indexing forward to a\nmature formulation that confers the workload-robustness that previous\napproaches lacked. Original cracking relies on the randomness of the workloads\nto converge well. [2][3] However, where the workload is non-random, cracking\nneeds to introduce randomness on its own. Stochastic Cracking clearly improves\nover original cracking by being robust in workload changes while maintaining\nall original cracking features when it comes to adaptation. But looking at both\ntypes of cracking, it conveyed an incomplete picture as at some point of time\nit is must to know whether the workload is random or sequential. In this paper\nour focus is on optimization of variant stochastic cracking, that could be\nachieved in two ways either by reducing the initialization cost to make\nstochastic cracking even more transparent to the user, especially for queries\nthat initiate a workload change and hence incur a higher cost or by combining\nthe strengths of the various stochastic cracking algorithms via a dynamic\ncomponent that decides which algorithm to choose for a query on the fly. The\nefforts have been put in to make an algorithm that reduces the initialization\ncost by using the main notion of both cracking, while considering the\nrequirements of adaptive indexing [2].\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 04:26:10 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Bhardwaj", "Meenesh", ""]]}, {"id": "1305.2103", "submitter": "Jacek Sroka", "authors": "Jacek Sroka and Adrian Panasiuk and Krzysztof Stencel and Jerzy\n  Tyszkiewicz", "title": "Translating Relational Queries into Spreadsheets", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2015.2397440", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreadsheets are among the most commonly used applications for data\nmanagement and analysis. Perhaps they are even among the most widely used\ncomputer applications of all kinds. They combine in a natural and intuitive way\ndata processing with very diverse supplementary features: statistical\nfunctions, visualization tools, pivot tables, pivot charts, linear programming\nsolvers, Web queries periodically downloading data from external sources, etc.\nHowever, the spreadsheet paradigm of computation still lacks sufficient\nanalysis.\n  In this article we demonstrate that a spreadsheet can implement all data\ntransformations definable in SQL, without any use of macros or built-in\nprogramming languages, merely by utilizing spreadsheet formulas. We provide a\nquery compiler, which translates any given SQL query into a worksheet of the\nsame semantics, including NULL values.\n  Thereby database operations become available to the users who do not want to\nmigrate to a database. They can define their queries using a high-level\nlanguage and then get their execution plans in a plain vanilla spreadsheet. No\nsophisticated database system, no spreadsheet plugins or macros are needed.\n  The functions available in spreadsheets impose severe limitations on the\nalgorithms one can implement. In this paper we offer $O(n\\log^2n)$ sorting\nspreadsheet, but using a non-constant number of rows, improving on the\npreviously known $O(n^2)$ ones.\n  It is therefore surprising, that a spreadsheet can implement, as we\ndemonstrate, Depth-First-Search and Breadth-First-Search on graphs, thereby\nreaching beyond queries definable in SQL-92.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 14:30:12 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 16:25:44 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Sroka", "Jacek", ""], ["Panasiuk", "Adrian", ""], ["Stencel", "Krzysztof", ""], ["Tyszkiewicz", "Jerzy", ""]]}, {"id": "1305.2758", "submitter": "Oumair Naseer", "authors": "1Oumair Naseer, 2Ayesha Naseer, 3Atif Ali Khan, 4Humza Naseer", "title": "Using Page Size for Controlling Duplicate Query Results in Semantic Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Semantic web is a web of future. The Resource Description Framework (RDF) is\na language to represent resources in the World Wide Web. When these resources\nare queried the problem of duplicate query results occurs. The present\ntechniques used hash index comparison to remove duplicate query results. The\nmajor drawback of using the hash index to remove duplicate query results is\nthat, if there is a slight change in formatting or word order, then hash index\nis changed and query results are no more considered as duplicate even though\nthey have same contents. We presented an algorithm for detection and\nelimination of duplicate query results from semantic web using hash index and\npage size comparisons. Experimental results showed that the proposed technique\nremoved duplicate query results from semantic web efficiently, solved the\nproblems of using hash index for duplicate handling and could be embedded in\nexisting SQL-Based query system for semantic web. Research could be carried out\nfor certain flexibilities in existing SQL-Based query system of semantic web to\naccommodate other duplicate detection techniques as well.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 12:33:58 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Naseer", "1Oumair", ""], ["Naseer", "2Ayesha", ""], ["Khan", "3Atif Ali", ""], ["Naseer", "4Humza", ""]]}, {"id": "1305.3014", "submitter": "Ali Jalali", "authors": "Ali Jalali, Santanu Kolay, Peter Foldes and Ali Dasdan", "title": "Scalable Audience Reach Estimation in Real-time Online Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertising has been introduced as one of the most efficient methods\nof advertising throughout the recent years. Yet, advertisers are concerned\nabout the efficiency of their online advertising campaigns and consequently,\nwould like to restrict their ad impressions to certain websites and/or certain\ngroups of audience. These restrictions, known as targeting criteria, limit the\nreachability for better performance. This trade-off between reachability and\nperformance illustrates a need for a forecasting system that can quickly\npredict/estimate (with good accuracy) this trade-off. Designing such a system\nis challenging due to (a) the huge amount of data to process, and, (b) the need\nfor fast and accurate estimates. In this paper, we propose a distributed fault\ntolerant system that can generate such estimates fast with good accuracy. The\nmain idea is to keep a small representative sample in memory across multiple\nmachines and formulate the forecasting problem as queries against the sample.\nThe key challenge is to find the best strata across the past data, perform\nmultivariate stratified sampling while ensuring fuzzy fall-back to cover the\nsmall minorities. Our results show a significant improvement over the uniform\nand simple stratified sampling strategies which are currently widely used in\nthe industry.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 03:48:09 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Jalali", "Ali", ""], ["Kolay", "Santanu", ""], ["Foldes", "Peter", ""], ["Dasdan", "Ali", ""]]}, {"id": "1305.3058", "submitter": "Emilien Antoine", "authors": "Serge Abiteboul (LSV), \\'Emilien Antoine (LSV), Gerome Miklau (LSV,\n  UMASS), Julia Stoyanovich, Jules Testard (LSV, McGill)", "title": "Rule-Based Application Development using Webdamlog", "comments": "SIGMOD - Special Interest Group on Management Of Data (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the WebdamLog system for managing distributed data on the Web in a\npeer-to-peer manner. We demonstrate the main features of the system through an\napplication called Wepic for sharing pictures between attendees of the sigmod\nconference. Using Wepic, the attendees will be able to share, download, rate\nand annotate pictures in a highly decentralized manner. We show how WebdamLog\nhandles heterogeneity of the devices and services used to share data in such a\nWeb setting. We exhibit the simple rules that define the Wepic application and\nshow how to easily modify the Wepic application.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 08:31:10 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Abiteboul", "Serge", "", "LSV"], ["Antoine", "\u00c9milien", "", "LSV"], ["Miklau", "Gerome", "", "LSV,\n  UMASS"], ["Stoyanovich", "Julia", "", "LSV, McGill"], ["Testard", "Jules", "", "LSV, McGill"]]}, {"id": "1305.3082", "submitter": "Jialong Han", "authors": "Jialong Han, Ji-Rong Wen", "title": "Mining Frequent Neighborhood Patterns in Large Labeled Graphs", "comments": "9 pages", "journal-ref": null, "doi": "10.1145/2505515.2505530", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, frequent subgraphs have been an important sort of targeted\npatterns in the pattern mining literatures, where most works deal with\ndatabases holding a number of graph transactions, e.g., chemical structures of\ncompounds. These methods rely heavily on the downward-closure property (DCP) of\nthe support measure to ensure an efficient pruning of the candidate patterns.\nWhen switching to the emerging scenario of single-graph databases such as\nGoogle Knowledge Graph and Facebook social graph, the traditional support\nmeasure turns out to be trivial (either 0 or 1). However, to the best of our\nknowledge, all attempts to redefine a single-graph support resulted in measures\nthat either lose DCP, or are no longer semantically intuitive.\n  This paper targets mining patterns in the single-graph setting. We resolve\nthe \"DCP-intuitiveness\" dilemma by shifting the mining target from frequent\nsubgraphs to frequent neighborhoods. A neighborhood is a specific topological\npattern where a vertex is embedded, and the pattern is frequent if it is shared\nby a large portion (above a given threshold) of vertices. We show that the new\npatterns not only maintain DCP, but also have equally significant semantics as\nsubgraph patterns. Experiments on real-life datasets display the feasibility of\nour algorithms on relatively large graphs, as well as the capability of mining\ninteresting knowledge that is not discovered in prior works.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 09:46:17 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Han", "Jialong", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1305.3103", "submitter": "Hassan Rashidi", "authors": "Hassan Rashidi", "title": "A fast method for implementation of the property lists in programming\n  languages", "comments": "9 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in programming languages is to support different\ndata structures and their variations in both static and dynamic aspects. One of\nthe these data structures is the property list which applications use it as a\nconvenient way to store, organize, and access standard types of data. In this\npaper, the standards methods for implementation of the Property Lists,\nincluding the Static Array, Link List, Hash and Tree are reviewed. Then an\nefficient method to implement the property list is presented. The experimental\nresults shows that our method is fast compared with the existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 10:45:16 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Rashidi", "Hassan", ""]]}, {"id": "1305.3407", "submitter": "Johannes Niedermayer", "authors": "Johannes Niedermayer, Andreas Z\\\"ufle, Tobias Emrich, Matthias Renz,\n  Nikos Mamoulis, Lei Chen, Hans-Peter Kriegel", "title": "Probabilistic Nearest Neighbor Queries on Uncertain Moving Object\n  Trajectories", "comments": "12 pages", "journal-ref": "PVLDB 7(3): 205-216 (2013)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor (NN) queries in trajectory databases have received\nsignificant attention in the past, due to their application in spatio-temporal\ndata analysis. Recent work has considered the realistic case where the\ntrajectories are uncertain; however, only simple uncertainty models have been\nproposed, which do not allow for accurate probabilistic search. In this paper,\nwe fill this gap by addressing probabilistic nearest neighbor queries in\ndatabases with uncertain trajectories modeled by stochastic processes,\nspecifically the Markov chain model. We study three nearest neighbor query\nsemantics that take as input a query state or trajectory $q$ and a time\ninterval. For some queries, we show that no polynomial time solution can be\nfound. For problems that can be solved in PTIME, we present exact query\nevaluation algorithms, while for the general case, we propose a sophisticated\nsampling approach, which uses Bayesian inference to guarantee that sampled\ntrajectories conform to the observation data stored in the database. This\nsampling approach can be used in Monte-Carlo based approximation solutions. We\ninclude an extensive experimental study to support our theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 09:54:58 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 15:18:04 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Niedermayer", "Johannes", ""], ["Z\u00fcfle", "Andreas", ""], ["Emrich", "Tobias", ""], ["Renz", "Matthias", ""], ["Mamoulis", "Nikos", ""], ["Chen", "Lei", ""], ["Kriegel", "Hans-Peter", ""]]}, {"id": "1305.4195", "submitter": "Xiaocheng Huang", "authors": "Susan B. Davidson and Xiaocheng Huang and Julia Stoyanovich and\n  Xiaojie Yuan", "title": "Search and Result Presentation in Scientific Workflow Repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of searching a repository of complex hierarchical\nworkflows whose component modules, both composite and atomic, have been\nannotated with keywords. Since keyword search does not use the graph structure\nof a workflow, we develop a model of workflows using context-free bag grammars.\nWe then give efficient polynomial-time algorithms that, given a workflow and a\nkeyword query, determine whether some execution of the workflow matches the\nquery. Based on these algorithms we develop a search and ranking solution that\nefficiently retrieves the top-k grammars from a repository. Finally, we propose\na novel result presentation method for grammars matching a keyword query, based\non representative parse-trees. The effectiveness of our approach is validated\nthrough an extensive experimental evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 21:22:14 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2013 18:21:55 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Davidson", "Susan B.", ""], ["Huang", "Xiaocheng", ""], ["Stoyanovich", "Julia", ""], ["Yuan", "Xiaojie", ""]]}, {"id": "1305.5653", "submitter": "George Garbis", "authors": "George Garbis, Kostis Kyzirakos, and Manolis Koubarakis", "title": "Geographica: A Benchmark for Geospatial RDF Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial extensions of SPARQL like GeoSPARQL and stSPARQL have recently\nbeen defined and corresponding geospatial RDF stores have been implemented.\nHowever, there is no widely used benchmark for evaluating geospatial RDF stores\nwhich takes into account recent advances to the state of the art in this area.\nIn this paper, we develop a benchmark, called Geographica, which uses both\nreal-world and synthetic data to test the offered functionality and the\nperformance of some prominent geospatial RDF stores.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 08:54:46 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Garbis", "George", ""], ["Kyzirakos", "Kostis", ""], ["Koubarakis", "Manolis", ""]]}, {"id": "1305.5824", "submitter": "Slim Bouker", "authors": "Slim Bouker, Rabie Saidi, Sadok Ben Yahia, Engelbert Mephu Nguifo", "title": "Towards a semantic and statistical selection of association rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The increasing growth of databases raises an urgent need for more accurate\nmethods to better understand the stored data. In this scope, association rules\nwere extensively used for the analysis and the comprehension of huge amounts of\ndata. However, the number of generated rules is too large to be efficiently\nanalyzed and explored in any further process. Association rules selection is a\nclassical topic to address this issue, yet, new innovated approaches are\nrequired in order to provide help to decision makers. Hence, many interesting-\nness measures have been defined to statistically evaluate and filter the\nassociation rules. However, these measures present two major problems. On the\none hand, they do not allow eliminating irrelevant rules, on the other hand,\ntheir abun- dance leads to the heterogeneity of the evaluation results which\nleads to confusion in decision making. In this paper, we propose a two-winged\napproach to select statistically in- teresting and semantically incomparable\nrules. Our statis- tical selection helps discovering interesting association\nrules without favoring or excluding any measure. The semantic comparability\nhelps to decide if the considered association rules are semantically related\ni.e comparable. The outcomes of our experiments on real datasets show promising\nresults in terms of reduction in the number of rules.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 18:46:34 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Bouker", "Slim", ""], ["Saidi", "Rabie", ""], ["Yahia", "Sadok Ben", ""], ["Nguifo", "Engelbert Mephu", ""]]}, {"id": "1305.6146", "submitter": "Tien Tuan Anh Dinh", "authors": "Tien Tuan Anh Dinh and Anwitaman Datta", "title": "Streamforce: outsourcing access control enforcement for stream data to\n  the clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As tremendous amount of data being generated everyday from human activity and\nfrom devices equipped with sensing capabilities, cloud computing emerges as a\nscalable and cost-effective platform to store and manage the data. While\nbenefits of cloud computing are numerous, security concerns arising when data\nand computation are outsourced to a third party still hinder the complete\nmovement to the cloud. In this paper, we focus on the problem of data privacy\non the cloud, particularly on access controls over stream data. The nature of\nstream data and the complexity of sharing data make access control a more\nchallenging issue than in traditional archival databases. We present\nStreamforce - a system allowing data owners to securely outsource their data to\nthe cloud. The owner specifies fine-grained policies which are enforced by the\ncloud. The latter performs most of the heavy computations, while learning\nnothing about the data. To this end, we employ a number of encryption schemes,\nincluding deterministic encryption, proxy-based attribute based encryption and\nsliding-window encryption. In Streamforce, access control policies are modeled\nas secure continuous queries, which entails minimal changes to existing stream\nprocessing engines, and allows for easy expression of a wide-range of policies.\nIn particular, Streamforce comes with a number of secure query operators\nincluding Map, Filter, Join and Aggregate. Finally, we implement Streamforce\nover an open source stream processing engine (Esper) and evaluate its\nperformance on a cloud platform. The results demonstrate practical performance\nfor many real-world applications, and although the security overhead is\nvisible, Streamforce is highly scalable.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 08:58:44 GMT"}, {"version": "v2", "created": "Tue, 28 May 2013 11:05:07 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Dinh", "Tien Tuan Anh", ""], ["Datta", "Anwitaman", ""]]}, {"id": "1305.6506", "submitter": "Michael Hausenblas", "authors": "Michael Hausenblas", "title": "Notes on Physical & Logical Data Layouts", "comments": "5 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this short note I review and discuss fundamental options for physical and\nlogical data layouts as well as the impact of the choices on data processing. I\nshould say in advance that these notes offer no new insights, that is,\neverything stated here has already been published elsewhere. In fact, it has\nbeen published in so many different places, such as blog posts, in the\nliterature, etc. that the main contribution is to bring it all together in one\nplace.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 14:16:11 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Hausenblas", "Michael", ""]]}, {"id": "1305.7006", "submitter": "Walaa Eldin Moustafa", "authors": "Walaa Eldin Moustafa, Angelika Kimmig, Amol Deshpande, Lise Getoor", "title": "Subgraph Pattern Matching over Uncertain Graphs with Identity Linkage\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing need for methods which can capture uncertainties and\nanswer queries over graph-structured data. Two common types of uncertainty are\nuncertainty over the attribute values of nodes and uncertainty over the\nexistence of edges. In this paper, we combine those with identity uncertainty.\nIdentity uncertainty represents uncertainty over the mapping from objects\nmentioned in the data, or references, to the underlying real-world entities. We\npropose the notion of a probabilistic entity graph (PEG), a probabilistic graph\nmodel that defines a distribution over possible graphs at the entity level. The\nmodel takes into account node attribute uncertainty, edge existence\nuncertainty, and identity uncertainty, and thus enables us to systematically\nreason about all three types of uncertainties in a uniform manner. We introduce\na general framework for constructing a PEG given uncertain data at the\nreference level and develop highly efficient algorithms to answer subgraph\npattern matching queries in this setting. Our algorithms are based on two novel\nideas: context-aware path indexing and reduction by join-candidates, which\ndrastically reduce the query search space. A comprehensive experimental\nevaluation shows that our approach outperforms baseline implementations by\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 05:27:23 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Moustafa", "Walaa Eldin", ""], ["Kimmig", "Angelika", ""], ["Deshpande", "Amol", ""], ["Getoor", "Lise", ""]]}]