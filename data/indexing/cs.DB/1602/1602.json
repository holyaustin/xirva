[{"id": "1602.00033", "submitter": "Chunbin Lin", "authors": "Chunbin Lin, Benjamin Mandel, Yannis Papakonstantinou, Matthias\n  Springer", "title": "Fast In-Memory SQL Analytics on Graphs", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of graph analytics SQL queries, which we call relationship\nqueries. Relationship queries are a wide superset of fixed-length graph\nreachability queries and of tree pattern queries. Intuitively, it discovers\ntarget entities that are reachable from source entities specified by the query.\nIt usually also finds aggregated scores, which correspond to the target\nentities and are calculated by applying aggregation functions on measure\nattributes, which are found on the target entities, the source entities and the\npaths from the sources to the targets. We present real-world OLAP scenarios,\nwhere efficient relationship queries are needed. However, row stores, column\nstores and graph databases are unacceptably slow in such OLAP scenarios. We\nbriefly comment on the straightforward extension of relationship queries that\nallows accessing arbitrary schemas.\n  The GQ-Fast in-memory analytics engine utilizes a bottom-up fully pipelined\nquery execution model running on a novel data organization that combines\nsalient features of column-based organization, indexing and compression.\nFurthermore, GQ-Fast compiles its query plans into executable C++ source codes.\nBesides achieving runtime efficiency, GQ-Fast also reduces main memory\nrequirements because, unlike column databases, GQ-Fast selectively allows more\ndense forms of compression including heavy-weighted compressions, which do not\nsupport random access.\n  We used GQ-Fast to accelerate queries for two OLAP dashboards in the\nbiomedical field. It outperforms Postgres by 2-4 orders of magnitude and\noutperforms MonetDB and Neo4j by 1-3 orders of magnitude when all of them are\nrunning on RAM. In addition, it generally saves space due to the appropriate\nuse of compression methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 22:58:35 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 18:54:09 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 15:49:52 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Lin", "Chunbin", ""], ["Mandel", "Benjamin", ""], ["Papakonstantinou", "Yannis", ""], ["Springer", "Matthias", ""]]}, {"id": "1602.00363", "submitter": "Jianzhong Qi", "authors": "Chuanwen Li, Yu Gu, Jianzhong Qi, Ge Yu, Rui Zhang, Qingxu Deng", "title": "INSQ: An Influential Neighbor Set Based Moving kNN Query Processing\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the moving k nearest neighbor (MkNN) query, which computes one's k\nnearest neighbor set and maintains it while at move. Existing MkNN algorithms\nare mostly safe region based, which lack efficiency due to either computing\nsmall safe regions with a high recomputation frequency or computing larger safe\nregions but with a high cost for each computation. In this demonstration, we\nshowcase a system named INSQ that adopts a novel algorithm called the\nInfluential Neighbor Set (INS) algorithm to process the MkNN query in both\ntwo-dimensional Euclidean space and road networks. This algorithm uses a small\nset of safe guarding objects instead of safe regions. As long as the the\ncurrent k nearest neighbors are closer to the query object than the safe\nguarding objects are, the current k nearest neighbors stay valid and no\nrecomputation is required. Meanwhile, the region defined by the safe guarding\nobjects is the largest possible safe region. This means that the recomputation\nfrequency is also minimized and hence, the INS algorithm achieves high overall\nquery processing efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 02:17:58 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 22:45:20 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 04:19:22 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Li", "Chuanwen", ""], ["Gu", "Yu", ""], ["Qi", "Jianzhong", ""], ["Yu", "Ge", ""], ["Zhang", "Rui", ""], ["Deng", "Qingxu", ""]]}, {"id": "1602.00389", "submitter": "Jianzhong Qi", "authors": "Yu Sun, Rui Zhang, Andy Yuan Xue, Jianzhong Qi, Xiaoyong Du", "title": "Reverse Nearest Neighbor Heat Maps: A Tool for Influence Exploration", "comments": "Accepted to appear in ICDE 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of constructing a reverse nearest neighbor (RNN) heat\nmap by finding the RNN set of every point in a two-dimensional space. Based on\nthe RNN set of a point, we obtain a quantitative influence (i.e., heat) for the\npoint. The heat map provides a global view on the influence distribution in the\nspace, and hence supports exploratory analyses in many applications such as\nmarketing and resource management. To construct such a heat map, we first\nreduce it to a problem called Region Coloring (RC), which divides the space\ninto disjoint regions within which all the points have the same RNN set. We\nthen propose a novel algorithm named CREST that efficiently solves the RC\nproblem by labeling each region with the heat value of its containing points.\nIn CREST, we propose innovative techniques to avoid processing expensive RNN\nqueries and greatly reduce the number of region labeling operations. We perform\ndetailed analyses on the complexity of CREST and lower bounds of the RC\nproblem, and prove that CREST is asymptotically optimal in the worst case.\nExtensive experiments with both real and synthetic data sets demonstrate that\nCREST outperforms alternative algorithms by several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 04:27:39 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 07:55:13 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Sun", "Yu", ""], ["Zhang", "Rui", ""], ["Xue", "Andy Yuan", ""], ["Qi", "Jianzhong", ""], ["Du", "Xiaoyong", ""]]}, {"id": "1602.00503", "submitter": "Amine Ghrab", "authors": "Amine Ghrab, Oscar Romero, Sabri Skhiri, Alejandro Vaisman, Esteban\n  Zim\\'anyi", "title": "GRAD: On Graph Database Modeling", "comments": "28 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph databases have emerged as the fundamental technology underpinning\ntrendy application domains where traditional databases are not well-equipped to\nhandle complex graph data. However, current graph databases support basic graph\nstructures and integrity constraints with no standard algebra. In this paper,\nwe introduce GRAD, a native and generic graph database model. GRAD goes beyond\ntraditional graph database models, which support simple graph structures and\nconstraints. Instead, GRAD presents a complete graph database model supporting\nadvanced graph structures, a set of well-defined constraints over these\nstructures and a powerful graph analysis-oriented algebra.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 12:49:25 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Ghrab", "Amine", ""], ["Romero", "Oscar", ""], ["Skhiri", "Sabri", ""], ["Vaisman", "Alejandro", ""], ["Zim\u00e1nyi", "Esteban", ""]]}, {"id": "1602.00563", "submitter": "Ioana Ileana", "authors": "Angela Bonifati, Ioana Ileana, Michele Linardi", "title": "Functional Dependencies Unleashed for Scalable Data Exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of efficiently evaluating target functional\ndependencies (fds) in the Data Exchange (DE) process. Target fds naturally\noccur in many DE scenarios, including the ones in Life Sciences in which\nmultiple source relations need to be structured under a constrained target\nschema. However, despite their wide use, target fds' evaluation is still a\nbottleneck in the state-of-the-art DE engines. Systems relying on an all-SQL\napproach typically do not support target fds unless additional information is\nprovided. Alternatively, DE engines that do include these dependencies\ntypically pay the price of a significant drop in performance and scalability.\nIn this paper, we present a novel chase-based algorithm that can efficiently\nhandle arbitrary fds on the target. Our approach essentially relies on\nexploiting the interactions between source-to-target (s-t) tuple-generating\ndependencies (tgds) and target fds. This allows us to tame the size of the\nintermediate chase results, by playing on a careful ordering of chase steps\ninterleaving fds and (chosen) tgds. As a direct consequence, we importantly\ndiminish the fd application scope, often a central cause of the dramatic\noverhead induced by target fds. Moreover, reasoning on dependency interaction\nfurther leads us to interesting parallelization opportunities, yielding\nadditional scalability gains. We provide a proof-of-concept implementation of\nour chase-based algorithm and an experimental study aiming at gauging its\nscalability with respect to a number of parameters, among which the size of\nsource instances and the number of dependencies of each tested scenario.\nFinally, we empirically compare with the latest DE engines, and show that our\nalgorithm outperforms them.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 15:32:24 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 18:06:39 GMT"}, {"version": "v3", "created": "Sat, 16 Apr 2016 11:46:21 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Bonifati", "Angela", ""], ["Ileana", "Ioana", ""], ["Linardi", "Michele", ""]]}, {"id": "1602.00773", "submitter": "Vera Moffitt", "authors": "Vera Zaychik Moffitt and Julia Stoyanovich", "title": "Querying Evolving Graphs with Portal", "comments": "12 pages plus appendix. Submitted to SIGMOD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are used to represent a plethora of phenomena, from the Web and social\nnetworks, to biological pathways, to semantic knowledge bases. Arguably the\nmost interesting and important questions one can ask about graphs have to do\nwith their evolution. Which Web pages are showing an increasing popularity\ntrend? How does influence propagate in social networks? How does knowledge\nevolve?\n  This paper proposes a logical model of an evolving graph called a TGraph,\nwhich captures evolution of graph topology and of its vertex and edge\nattributes. We present a compositional temporal graph algebra TGA, and show a\nreduction of TGA to temporal relational algebra with graph-specific primitives.\nWe formally study the properties of TGA, and also show that it is sufficient to\nconcisely express a wide range of common use cases. We describe an\nimplementation of our model and algebra in Portal, built on top of Apache Spark\n/ GraphX. We conduct extensive experiments on real datasets, and show that\nPortal scales.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 03:10:45 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 04:25:11 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Moffitt", "Vera Zaychik", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "1602.01040", "submitter": "HyeongSik Kim HyeongSik Kim", "authors": "HyeongSik Kim and Kemafor Anyanwu", "title": "Scalable Ontological Query Processing over Semantically Integrated Life\n  Science Datasets using MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the requirement of enabling a comprehensive perspective of\nlife-sciences data, Semantic Web technologies have been adopted for\nstandardized representations of data and linkages between data. This has\nresulted in data warehouses such as UniProt, Bio2RDF, and Chem2Bio2RDF, that\nintegrate different kinds of biological and chemical data using ontologies.\nUnfortunately, the ability to process queries over ontologically-integrated\ncollections remains a challenge, particularly when data is large. The reason is\nthat besides the traditional challenges of processing graph-structured data,\ncomplete query answering requires inferencing to explicate implicitly\nrepresented facts. Since traditional inferencing techniques like forward\nchaining are difficult to scale up, and need to be repeated each time data is\nupdated, recent focus has been on inferencing that can be supported using\ndatabase technologies via query rewriting. However, due to the richness of most\nbiomedical ontologies relative to other domain ontologies, the queries\nresulting from the query rewriting technique are often more complex than\nexisting query optimization techniques can cope with. This is particularly so\nwhen using the emerging class of cloud data processing platforms for big data\nprocessing due to some additional overhead which they introduce. In this paper,\nwe present an approach for dealing such complex queries on big data using\nMapReduce, along with an evaluation on existing real-world datasets and\nbenchmark queries.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 18:45:22 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Kim", "HyeongSik", ""], ["Anyanwu", "Kemafor", ""]]}, {"id": "1602.01248", "submitter": "Spyros Sioutas SS", "authors": "Nikolaos Nodarakis, Spyros Sioutas, Athanasios Tsakalidis and Giannis\n  Tzimas", "title": "Using Hadoop for Large Scale Analysis on Twitter: A Technical Report", "comments": "8 pages, 3 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis (or opinion mining) on Twitter data has attracted much\nattention recently. One of the system's key features, is the immediacy in\ncommunication with other users in an easy, user-friendly and fast way.\nConsequently, people tend to express their feelings freely, which makes Twitter\nan ideal source for accumulating a vast amount of opinions towards a wide\ndiversity of topics. This amount of information offers huge potential and can\nbe harnessed to receive the sentiment tendency towards these topics. However,\nsince none can invest an infinite amount of time to read through these tweets,\nan automated decision making approach is necessary. Nevertheless, most existing\nsolutions are limited in centralized environments only. Thus, they can only\nprocess at most a few thousand tweets. Such a sample, is not representative to\ndefine the sentiment polarity towards a topic due to the massive number of\ntweets published daily. In this paper, we go one step further and develop a\nnovel method for sentiment learning in the MapReduce framework. Our algorithm\nexploits the hashtags and emoticons inside a tweet, as sentiment labels, and\nproceeds to a classification procedure of diverse sentiment types in a parallel\nand distributed manner. Moreover, we utilize Bloom filters to compact the\nstorage size of intermediate data and boost the performance of our algorithm.\nThrough an extensive experimental evaluation, we prove that our solution is\nefficient, robust and scalable and confirm the quality of our sentiment\nidentification.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 10:19:19 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Nodarakis", "Nikolaos", ""], ["Sioutas", "Spyros", ""], ["Tsakalidis", "Athanasios", ""], ["Tzimas", "Giannis", ""]]}, {"id": "1602.01366", "submitter": "Andreas Pieris", "authors": "Pablo Barcelo and Georg Gottlob and Andreas Pieris", "title": "Semantic Acyclicity Under Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conjunctive query (CQ) is semantically acyclic if it is equivalent to an\nacyclic one. Semantic acyclicity has been studied in the constraint-free case,\nand deciding whether a query enjoys this property is NP-complete. However, in\ncase the database is subject to constraints such as tuple-generating\ndependencies (tgds) that can express, e.g., inclusion dependencies, or\nequality-generating dependencies (egds) that capture, e.g., functional\ndependencies, a CQ may turn out to be semantically acyclic under the\nconstraints while not semantically acyclic in general. This opens avenues to\nnew query optimization techniques. In this paper we initiate and develop the\ntheory of semantic acyclicity under constraints. More precisely, we study the\nfollowing natural problem: Given a CQ and a set of constraints, is the query\nsemantically acyclic under the constraints, or, in other words, is the query\nequivalent to an acyclic one over all those databases that satisfy the set of\nconstraints? We show that, contrary to what one might expect, decidability of\nCQ containment is a necessary but not sufficient condition for the decidability\nof semantic acyclicity. In particular, we show that semantic acyclicity is\nundecidable in the presence of full tgds (i.e., Datalog rules). In view of this\nfact, we focus on the main classes of tgds for which CQ containment is\ndecidable, and do not capture the class of full tgds, namely guarded,\nnon-recursive and sticky tgds. For these classes we show that semantic\nacyclicity is decidable, and its complexity coincides with the complexity of CQ\ncontainment. In the case of egds, we show that for keys over unary and binary\npredicates semantic acyclicity is decidable (NP-complete). We finally consider\nthe problem of evaluating a semantically acyclic query over a database that\nsatisfies a set of constraints; for guarded tgds and functional dependencies\nthis problem is tractable.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 17:01:36 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 16:39:28 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Barcelo", "Pablo", ""], ["Gottlob", "Georg", ""], ["Pieris", "Andreas", ""]]}, {"id": "1602.01443", "submitter": "Jonathan Ullman", "authors": "Jeffrey D. Ullman, Jonathan Ullman", "title": "Some Pairs Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common form of MapReduce application involves discovering relationships\nbetween certain pairs of inputs. Similarity joins serve as a good example of\nthis type of problem, which we call a \"some-pairs\" problem. In the framework of\nAfrati et al. (VLDB 2013), algorithms are measured by the tradeoff between\nreducer size (maximum number of inputs a reducer can handle) and the\nreplication rate (average number of reducers to which an input must be sent.\nThere are two obvious approaches to solving some-pairs problems in general. We\nshow that no general-purpose MapReduce algorithm can beat both of these two\nalgorithms in the worst case. We then explore a recursive algorithm for solving\nsome-pairs problems and heuristics for beating the lower bound on common\ninstances of the some-pairs class of problems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 20:14:25 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Ullman", "Jeffrey D.", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1602.01537", "submitter": "Vachik Dave", "authors": "Vachik S. Dave and Mohammad Al Hasan", "title": "TopCom: Index for Shortest Distance Query in Directed Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding shortest distance between two vertices in a graph is an important\nproblem due to its numerous applications in diverse domains, including\ngeo-spatial databases, social network analysis, and information retrieval.\nClassical algorithms (such as, Dijkstra) solve this problem in polynomial time,\nbut these algorithms cannot provide real-time response for a large number of\nbursty queries on a large graph. So, indexing based solutions that pre-process\nthe graph for efficiently answering (exactly or approximately) a large number\nof distance queries in real-time is becoming increasingly popular. Existing\nsolutions have varying performance in terms of index size, index building time,\nquery time, and accuracy. In this work, we propose T OP C OM , a novel\nindexing-based solution for exactly answering distance queries. Our experiments\nwith two of the existing state-of-the-art methods (IS-Label and TreeMap) show\nthe superiority of T OP C OM over these two methods considering scalability and\nquery time. Besides, indexing of T OP C OM exploits the DAG (directed acyclic\ngraph) structure in the graph, which makes it significantly faster than the\nexisting methods if the SCCs (strongly connected component) of the input graph\nare relatively small.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 02:02:05 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 02:56:53 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Dave", "Vachik S.", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1602.01871", "submitter": "Jiamin Huang", "authors": "Jiamin Huang, Barzan Mozafari, Grant Schoenebeck, Thomas Wenisch", "title": "Identifying the Major Sources of Variance in Transaction Latencies:\n  Towards More Predictable Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decades of research have sought to improve transaction processing performance\nand scalability in database management systems (DBMSs). However, significantly\nless attention has been dedicated to the predictability of performance: how\noften individual transactions exhibit execution latency far from the mean?\nPerformance predictability is vital when transaction processing lies on the\ncritical path of a complex enterprise software or an interactive web service,\nas well as in emerging database-as-a-service markets where customers contract\nfor guaranteed levels of performance. In this paper, we take several steps\ntowards achieving more predictable database systems. First, we propose a\nprofiling framework called VProfiler that, given the source code of a DBMS, is\nable to identify the dominant sources of variance in transaction latency.\nVProfiler automatically instruments the DBMS source code to deconstruct the\noverall variance of transaction latencies into variances and covariances of the\nexecution time of individual functions, which in turn provide insight into the\nroot causes of variance. Second, we use VProfiler to analyze MySQL and Postgres\n- two of the most popular and complex open-source database systems. Our case\nstudies reveal that the primary causes of variance in MySQL and Postgres are\nlock scheduling and centralized logging, respectively. Finally, based on\nVProfiler's findings, we further focus on remedying the performance variance of\nMySQL by (1) proposing a new lock scheduling algorithm, called Variance-Aware\nTransaction Scheduling (VATS), (2) enhancing the buffer pool replacement\npolicy, and (3) identifying tuning parameters that can reduce variance\nsignificantly. Our experimental results show that our schemes reduce overall\ntransaction latency variance by 37% on average (and up to 64%) without\ncompromising throughput or mean latency.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 22:20:59 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 20:33:06 GMT"}, {"version": "v3", "created": "Thu, 3 Mar 2016 07:24:53 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Huang", "Jiamin", ""], ["Mozafari", "Barzan", ""], ["Schoenebeck", "Grant", ""], ["Wenisch", "Thomas", ""]]}, {"id": "1602.02334", "submitter": "Leopoldo Bertossi", "authors": "Zeinab Bahmani, Leopoldo Bertossi and Nikolaos Vasiloglou", "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity\n  Resolution", "comments": "Final journal version, with some minor technical corrections.\n  Extended version of arXiv:1508.06013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER), an important and common data cleaning problem, is\nabout detecting data duplicate representations for the same external entities,\nand merging them into single representations. Relatively recently, declarative\nrules called \"matching dependencies\" (MDs) have been proposed for specifying\nsimilarity conditions under which attribute values in database records are\nmerged. In this work we show the process and the benefits of integrating four\ncomponents of ER: (a) Building a classifier for duplicate/non-duplicate record\npairs built using machine learning (ML) techniques; (b) Use of MDs for\nsupporting the blocking phase of ML; (c) Record merging on the basis of the\nclassifier results; and (d) The use of the declarative language \"LogiQL\" -an\nextended form of Datalog supported by the \"LogicBlox\" platform- for all\nactivities related to data processing, and the specification and enforcement of\nMDs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 03:06:40 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 21:09:37 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 17:43:43 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Bahmani", "Zeinab", ""], ["Bertossi", "Leopoldo", ""], ["Vasiloglou", "Nikolaos", ""]]}, {"id": "1602.02358", "submitter": "Haohan Zhu", "authors": "Haohan Zhu, Xianrui Meng and George Kollios", "title": "NED: An Inter-Graph Node Metric Based On Edit Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node similarity is a fundamental problem in graph analytics. However, node\nsimilarity between nodes in different graphs (inter-graph nodes) has not\nreceived a lot of attention yet. The inter-graph node similarity is important\nin learning a new graph based on the knowledge of an existing graph (transfer\nlearning on graphs) and has applications in biological, communication, and\nsocial networks. In this paper, we propose a novel distance function for\nmeasuring inter-graph node similarity with edit distance, called NED. In NED,\ntwo nodes are compared according to their local neighborhood structures which\nare represented as unordered k-adjacent trees, without relying on labels or\nother assumptions. Since the computation problem of tree edit distance on\nunordered trees is NP-Complete, we propose a modified tree edit distance,\ncalled TED*, for comparing neighborhood trees. TED* is a metric distance, as\nthe original tree edit distance, but more importantly, TED* is polynomially\ncomputable. As a metric distance, NED admits efficient indexing, provides\ninterpretable results, and shows to perform better than existing approaches on\na number of data analysis tasks, including graph de-anonymization. Finally, the\nefficiency and effectiveness of NED are empirically demonstrated using\nreal-world graphs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 11:00:03 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 21:14:57 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2016 04:24:07 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Zhu", "Haohan", ""], ["Meng", "Xianrui", ""], ["Kollios", "George", ""]]}, {"id": "1602.02620", "submitter": "Ninh Pham", "authors": "Ninh Pham, Rasmus Pagh", "title": "Scalability and Total Recall with Fast CoveringLSH", "comments": "Short version appears in Proceedings of CIKM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic\ntechnique for similarity search with strong performance guarantees in\nhigh-dimensional spaces. A drawback of traditional LSH schemes is that they may\nhave \\emph{false negatives}, i.e., the recall is less than 100\\%. This limits\nthe applicability of LSH in settings requiring precise performance guarantees.\nBuilding on the recent theoretical \"CoveringLSH\" construction that eliminates\nfalse negatives, we propose a fast and practical covering LSH scheme for\nHamming space called \\emph{Fast CoveringLSH (fcLSH)}. Inheriting the design\nbenefits of CoveringLSH our method avoids false negatives and always reports\nall near neighbors. Compared to CoveringLSH we achieve an asymptotic\nimprovement to the hash function computation time from $\\mathcal{O}(dL)$ to\n$\\mathcal{O}(d + L\\log{L})$, where $d$ is the dimensionality of data and $L$ is\nthe number of hash tables. Our experiments on synthetic and real-world data\nsets demonstrate that \\emph{fcLSH} is comparable (and often superior) to\ntraditional hashing-based approaches for search radius up to 20 in\nhigh-dimensional Hamming space.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 16:03:11 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 10:46:19 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Pham", "Ninh", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1602.03072", "submitter": "Sabeur Aridhi", "authors": "Sabeur Aridhi and Engelbert Mephu Nguifo", "title": "Big Graph Mining: Frameworks and Techniques", "comments": "Submitted to Big Data Research, Elsevier", "journal-ref": null, "doi": "10.1016/j.bdr.2016.07.002", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big graph mining is an important research area and it has attracted\nconsiderable attention. It allows to process, analyze, and extract meaningful\ninformation from large amounts of graph data. Big graph mining has been highly\nmotivated not only by the tremendously increasing size of graphs but also by\nits huge number of applications. Such applications include bioinformatics,\nchemoinformatics and social networks. One of the most challenging tasks in big\ngraph mining is pattern mining in big graphs. This task consists on using data\nmining algorithms to discover interesting, unexpected and useful patterns in\nlarge amounts of graph data. It aims also to provide deeper understanding of\ngraph data. In this context, several graph processing frameworks and scaling\ndata mining/pattern mining techniques have been proposed to deal with very big\ngraphs. This paper gives an overview of existing data mining and graph\nprocessing frameworks that deal with very big graphs. Then it presents a survey\nof current researches in the field of data mining / pattern mining in big\ngraphs and discusses the main research issues related to this field. It also\ngives a categorization of both distributed data mining and machine learning\ntechniques, graph processing frameworks and large scale pattern mining\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 16:53:08 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Aridhi", "Sabeur", ""], ["Nguifo", "Engelbert Mephu", ""]]}, {"id": "1602.03110", "submitter": "Akhil Arora", "authors": "Sainyam Galhotra, Akhil Arora, Shourya Roy", "title": "Holistic Influence Maximization: Combining Scalability and Efficiency\n  with Opinion-Aware Models", "comments": "ACM SIGMOD Conference 2016, 18 pages, 29 figures", "journal-ref": null, "doi": "10.1145/2882903.2882929", "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The steady growth of graph data from social networks has resulted in\nwide-spread research in finding solutions to the influence maximization\nproblem. In this paper, we propose a holistic solution to the influence\nmaximization (IM) problem. (1) We introduce an opinion-cum-interaction (OI)\nmodel that closely mirrors the real-world scenarios. Under the OI model, we\nintroduce a novel problem of Maximizing the Effective Opinion (MEO) of\ninfluenced users. We prove that the MEO problem is NP-hard and cannot be\napproximated within a constant ratio unless P=NP. (2) We propose a heuristic\nalgorithm OSIM to efficiently solve the MEO problem. To better explain the OSIM\nheuristic, we first introduce EaSyIM - the opinion-oblivious version of OSIM, a\nscalable algorithm capable of running within practical compute times on\ncommodity hardware. In addition to serving as a fundamental building block for\nOSIM, EaSyIM is capable of addressing the scalability aspect - memory\nconsumption and running time, of the IM problem as well.\n  Empirically, our algorithms are capable of maintaining the deviation in the\nspread always within 5% of the best known methods in the literature. In\naddition, our experiments show that both OSIM and EaSyIM are effective,\nefficient, scalable and significantly enhance the ability to analyze real\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 18:21:41 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Arora", "Akhil", ""], ["Roy", "Shourya", ""]]}, {"id": "1602.03501", "submitter": "Patrick Schultz", "authors": "Patrick Schultz, David I. Spivak, Christina Vasilakopoulou, Ryan\n  Wisnesky", "title": "Algebraic Databases", "comments": "80 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Databases have been studied category-theoretically for decades. The database\nschema---whose purpose is to arrange high-level conceptual entities---is\ngenerally modeled as a category or sketch. The data itself, often called an\ninstance, is generally modeled as a set-valued functor, assigning to each\nconceptual entity a set of examples. While mathematically elegant, these\ncategorical models have typically struggled with representing concrete data\nsuch as integers or strings.\n  In the present work, we propose an extension of the set-valued functor model,\nmaking use of multisorted algebraic theories (a.k.a. Lawvere theories) to\nincorporate concrete data in a principled way. This also allows constraints and\nqueries to make use of operations on data, such as multiplication or comparison\nof numbers, helping to bridge the gap between traditional databases and\nprogramming languages.\n  We also show how all of the components of our model---including schemas,\ninstances, change-of-schema functors, and queries - fit into a single double\ncategorical structure called a proarrow equipment (a.k.a. framed bicategory).\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 20:22:40 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 16:05:35 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Schultz", "Patrick", ""], ["Spivak", "David I.", ""], ["Vasilakopoulou", "Christina", ""], ["Wisnesky", "Ryan", ""]]}, {"id": "1602.03557", "submitter": "Christopher Aberger", "authors": "Christopher R. Aberger, Susan Tu, Kunle Olukotun, and Christopher R\\'e", "title": "Old Techniques for New Join Algorithms: A Case Study in RDF Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been significant interest around designing specialized RDF\nengines, as traditional query processing mechanisms incur orders of magnitude\nperformance gaps on many RDF workloads. At the same time researchers have\nreleased new worst-case optimal join algorithms which can be asymptotically\nbetter than the join algorithms in traditional engines. In this paper we apply\nworst-case optimal join algorithms to a standard RDF workload, the LUBM\nbenchmark, for the first time. We do so using two worst-case optimal engines:\n(1) LogicBlox, a commercial database engine, and (2) EmptyHeaded, our prototype\nresearch engine with enhanced worst-case optimal join algorithms. We show that\nwithout any added optimizations both LogicBlox and EmptyHeaded outperform two\nstate-of-the-art specialized RDF engines, RDF-3X and TripleBit, by up to 6x on\ncyclic join queries-the queries where traditional optimizers are suboptimal. On\nthe remaining, less complex queries in the LUBM benchmark, we show that three\nclassic query optimization techniques enable EmptyHeaded to compete with RDF\nengines, even when there is no asymptotic advantage to the worst-case optimal\napproach. We validate that our design has merit as EmptyHeaded outperforms\nMonetDB by three orders of magnitude and LogicBlox by two orders of magnitude,\nwhile remaining within an order of magnitude of RDF-3X and TripleBit.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 22:10:10 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Aberger", "Christopher R.", ""], ["Tu", "Susan", ""], ["Olukotun", "Kunle", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1602.03730", "submitter": "Saravanan Thirumuruganathan", "authors": "Md Farhadur Rahman, Weimo Liu, Saad Bin Suhaim, Saravanan\n  Thirumuruganathan, Nan Zhang, Gautam Das", "title": "HDBSCAN: Density based Clustering over Location Based Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location Based Services (LBS) have become extremely popular and used by\nmillions of users. Popular LBS run the entire gamut from mapping services (such\nas Google Maps) to restaurants (such as Yelp) and real-estate (such as Redfin).\nThe public query interfaces of LBS can be abstractly modeled as a kNN interface\nover a database of two dimensional points: given an arbitrary query point, the\nsystem returns the k points in the database that are nearest to the query\npoint. Often, k is set to a small value such as 20 or 50. In this paper, we\nconsider the novel problem of enabling density based clustering over an LBS\nwith only a limited, kNN query interface. Due to the query rate limits imposed\nby LBS, even retrieving every tuple once is infeasible. Hence, we seek to\nconstruct a cluster assignment function f(.) by issuing a small number of kNN\nqueries, such that for any given tuple t in the database which may or may not\nhave been accessed, f(.) outputs the cluster assignment of t with high\naccuracy. We conduct a comprehensive set of experiments over benchmark datasets\nand popular real-world LBS such as Yahoo! Flickr, Zillow, Redfin and Google\nMaps.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 14:06:02 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 07:22:37 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Rahman", "Md Farhadur", ""], ["Liu", "Weimo", ""], ["Suhaim", "Saad Bin", ""], ["Thirumuruganathan", "Saravanan", ""], ["Zhang", "Nan", ""], ["Das", "Gautam", ""]]}, {"id": "1602.03819", "submitter": "Amir Gilad", "authors": "Daniel Deutch, Amir Gilad", "title": "Query By Provenance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assist non-specialists in formulating database queries, multiple\nframeworks that automatically infer queries from a set of examples have been\nproposed. While highly useful, a shortcoming of the approach is that if users\ncan only provide a small set of examples, many inherently different queries may\nqualify, and only some of these actually match the user intentions. Our main\nobservation is that if users further explain their examples, the set of\nqualifying queries may be significantly more focused. We develop a novel\nframework where users explain example tuples by choosing input tuples that are\nintuitively the \"cause\" for their examples. Their explanations are\nautomatically \"compiled\" into a formal model for explanations, based on\npreviously developed models of data provenance. Then, our novel algorithms\ninfer conjunctive queries from the examples and their explanations. We prove\nthe computational efficiency of the algorithms and favorable properties of\ninferred queries. We have further implemented our solution in a system\nprototype with an interface that assists users in formulating explanations in\nan intuitive way. Our experimental results, including a user study as well as\nexperiments using the TPC-H benchmark, indicate the effectiveness of our\nsolution.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 18:32:47 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 19:48:37 GMT"}, {"version": "v3", "created": "Mon, 16 May 2016 15:26:30 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Deutch", "Daniel", ""], ["Gilad", "Amir", ""]]}, {"id": "1602.04256", "submitter": "Yihan Gao", "authors": "Yihan Gao, Aditya Parameswaran", "title": "Squish: Near-Optimal Compression for Archival of Relational Datasets", "comments": null, "journal-ref": null, "doi": "10.1145/2939672.2939867", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational datasets are being generated at an alarmingly rapid rate across\norganizations and industries. Compressing these datasets could significantly\nreduce storage and archival costs. Traditional compression algorithms, e.g.,\ngzip, are suboptimal for compressing relational datasets since they ignore the\ntable structure and relationships between attributes.\n  We study compression algorithms that leverage the relational structure to\ncompress datasets to a much greater extent. We develop Squish, a system that\nuses a combination of Bayesian Networks and Arithmetic Coding to capture\nmultiple kinds of dependencies among attributes and achieve near-entropy\ncompression rate. Squish also supports user-defined attributes: users can\ninstantiate new data types by simply implementing five functions for a new\nclass interface. We prove the asymptotic optimality of our compression\nalgorithm and conduct experiments to show the effectiveness of our system:\nSquish achieves a reduction of over 50\\% in storage size relative to systems\ndeveloped in prior work on a variety of real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 22:46:57 GMT"}, {"version": "v2", "created": "Sun, 19 Jun 2016 16:09:39 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Gao", "Yihan", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1602.04268", "submitter": "Robert Kent", "authors": "Robert E. Kent", "title": "The ERA of FOLE: Superstructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the representation of ontologies in the first-order\nlogical environment FOLE (Kent 2013). An ontology defines the primitives with\nwhich to model the knowledge resources for a community of discourse (Gruber\n2009). These primitives, consisting of classes, relationships and properties,\nare represented by the ERA (entity-relationship-attribute) data model (Chen\n1976). An ontology uses formal axioms to constrain the interpretation of these\nprimitives. In short, an ontology specifies a logical theory. This paper is the\nsecond in a series of three papers that provide a rigorous mathematical\nrepresentation for the ERA data model in particular, and ontologies in general,\nwithin the first-order logical environment FOLE. The first two papers show how\nFOLE represents the formalism and semantics of (many-sorted) first-order logic\nin a classification form corresponding to ideas discussed in the Information\nFlow Framework (IFF). In particular, the first paper (Kent 2015) provided a\n\"foundation\" that connected elements of the ERA data model with components of\nthe first-order logical environment FOLE, and this second paper provides a\n\"superstructure\" that extends FOLE to the formalisms of first-order logic. The\nthird paper will define an \"interpretation\" of FOLE in terms of the\ntransformational passage, first described in (Kent 2013), from the\nclassification form of first-order logic to an equivalent interpretation form,\nthereby defining the formalism and semantics of first-order logical/relational\ndatabase systems (Kent 2011). The FOLE representation follows a conceptual\nstructures approach, that is completely compatible with Formal Concept Analysis\n(Ganter and Wille 1999) and Information Flow (Barwise and Seligman 1997).\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 00:03:39 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Kent", "Robert E.", ""]]}, {"id": "1602.04301", "submitter": "Dingxiong Deng", "authors": "Dingxiong Deng, Cyrus Shahabi, Ugur Demiryurek, Linhong Zhu, Rose Yu,\n  Yan Liu", "title": "Latent Space Model for Road Networks to Predict Time-Varying Traffic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time traffic prediction from high-fidelity spatiotemporal traffic sensor\ndatasets is an important problem for intelligent transportation systems and\nsustainability. However, it is challenging due to the complex topological\ndependencies and high dynamics associated with changing road conditions. In\nthis paper, we propose a Latent Space Model for Road Networks (LSM-RN) to\naddress these challenges. In particular, given a series of road network\nsnapshots, we learn the attributes of vertices in latent spaces which capture\nboth topological and temporal properties. As these latent attributes are\ntime-dependent, they can estimate how traffic patterns form and evolve. In\naddition, we present an incremental online algorithm which sequentially and\nadaptively learn the latent attributes from the temporal graph changes. Our\nframework enables real-time traffic prediction by 1) exploiting real-time\nsensor readings to adjust/update the existing latent spaces, and 2) training as\ndata arrives and making predictions on-the-fly with given data. By conducting\nextensive experiments with a large volume of real-world traffic sensor data, we\ndemonstrate the utility superiority of our framework for real-time traffic\nprediction on large road networks over competitors as well as a baseline\ngraph-based LSM.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 08:18:07 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 07:39:45 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 01:36:57 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Deng", "Dingxiong", ""], ["Shahabi", "Cyrus", ""], ["Demiryurek", "Ugur", ""], ["Zhu", "Linhong", ""], ["Yu", "Rose", ""], ["Liu", "Yan", ""]]}, {"id": "1602.04302", "submitter": "Ganzhao Yuan", "authors": "Ganzhao Yuan, Yin Yang, Zhenjie Zhang, Zhifeng Hao", "title": "Convex Optimization for Linear Query Processing under Approximate\n  Differential Privacy", "comments": "to appear in ACM SIGKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy enables organizations to collect accurate aggregates\nover sensitive data with strong, rigorous guarantees on individuals' privacy.\nPrevious work has found that under differential privacy, computing multiple\ncorrelated aggregates as a batch, using an appropriate \\emph{strategy}, may\nyield higher accuracy than computing each of them independently. However,\nfinding the best strategy that maximizes result accuracy is non-trivial, as it\ninvolves solving a complex constrained optimization program that appears to be\nnon-linear and non-convex. Hence, in the past much effort has been devoted in\nsolving this non-convex optimization program. Existing approaches include\nvarious sophisticated heuristics and expensive numerical solutions. None of\nthem, however, guarantees to find the optimal solution of this optimization\nproblem.\n  This paper points out that under ($\\epsilon$, $\\delta$)-differential privacy,\nthe optimal solution of the above constrained optimization problem in search of\na suitable strategy can be found, rather surprisingly, by solving a simple and\nelegant convex optimization program. Then, we propose an efficient algorithm\nbased on Newton's method, which we prove to always converge to the optimal\nsolution with linear global convergence rate and quadratic local convergence\nrate. Empirical evaluations demonstrate the accuracy and efficiency of the\nproposed solution.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 08:31:14 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 06:51:59 GMT"}, {"version": "v3", "created": "Mon, 16 May 2016 23:20:13 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Yuan", "Ganzhao", ""], ["Yang", "Yin", ""], ["Zhang", "Zhenjie", ""], ["Hao", "Zhifeng", ""]]}, {"id": "1602.04431", "submitter": "Vikram  Singh", "authors": "Vikash Mishra, Vikram Singh", "title": "Distributed Query Processing Plans generation using Teacher Learner\n  Based Optimization", "comments": "12 pages", "journal-ref": "International Journal of Information Processing,9(4),\n  46-60,2015,ISSN : 0973-8215", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing popularity, the number of data sources and the amount of\ndata has been growing very fast in recent years. The distribution of\noperational data on disperse data sources impose a challenge on processing user\nqueries. In such database systems, the database relations required by a query\nto answer may be stored at multiple sites. This leads to an exponential\nincrease in the number of possible equivalent or alternatives of a user query.\nThough it is not computationally reasonable to explore exhaustively all\npossible query plans in a large search space, thus a strategy is requisite to\nproduce optimal query plans in distributed database systems. The query plan\nwith most cost-effective option for query processing is measured necessary and\nmust be generated for a given query. This paper attempts to generate such\noptimal query plans using a parameter less optimization technique\nTeaching-Learner Based Optimization(TLBO). The TLBO algorithm was experiential\nto go one better than the other optimization algorithms for the multi objective\nunconstrained and constrained benchmark problems. Experimental comparisons of\nTLBO based optimal plan generation with the multiobjective genetic algorithm\nbased distributed query plan generation algorithm shows that for higher number\nof relations, the TLBO based algorithm is able to generate comparatively better\nquality Top K query plans.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 08:44:29 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Mishra", "Vikash", ""], ["Singh", "Vikram", ""]]}, {"id": "1602.04473", "submitter": "Michael Ruster", "authors": "Michael Ruster", "title": "Large-Scale Reasoning with OWL", "comments": "Part of the \"Knowledge Representation in the Semantic Web\" Seminar by\n  Matthias Thimm, Koblenz 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With the growth of the Semantic Web in size and importance, more and more\nknowledge is stored in machine-readable formats such as the Web Ontology\nLanguage OWL. This paper outlines common approaches for efficient reasoning on\nlarge-scale data consisting of billions ($10^9$) of triples. Therefore, OWL and\nits sublanguages, as well as forward and backward chaining techniques are\npresented. The WebPIE reasoner is discussed in detail as an example for forward\nchaining using MapReduce for materialisation. Moreover, the QueryPIE reasoner\nis presented as a backward chaining/hybrid approach which uses query rewriting.\nFurthermore, an overview on other reasoners is given such as OWLIM and TrOWL.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 16:18:32 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Ruster", "Michael", ""]]}, {"id": "1602.05100", "submitter": "Abolfazl Asudeh", "authors": "Abolfazl Asudeh and Nan Zhang and Gautam Das", "title": "Query Reranking As A Service", "comments": "Proceedings of the VLDB Endowment (PVLDB), Vol. 9, No. 11, 2016", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol 9, No 11, 2016", "doi": "10.14778/2983200.2983205", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ranked retrieval model has rapidly become the de facto way for search\nquery processing in client-server databases, especially those on the web.\nDespite of the extensive efforts in the database community on designing better\nranking functions/mechanisms, many such databases in practice still fail to\naddress the diverse and sometimes contradicting preferences of users on tuple\nranking, perhaps (at least partially) due to the lack of expertise and/or\nmotivation for the database owner to design truly effective ranking functions.\nThis paper takes a different route on addressing the issue by defining a novel\n{\\em query reranking problem}, i.e., we aim to design a third-party service\nthat uses nothing but the public search interface of a client-server database\nto enable the on-the-fly processing of queries with any user-specified ranking\nfunctions (with or without selection conditions), no matter if the ranking\nfunction is supported by the database or not. We analyze the worst-case\ncomplexity of the problem and introduce a number of ideas, e.g., on-the-fly\nindexing, domination detection and virtual tuple pruning, to reduce the\naverage-case cost of the query reranking algorithm. We also present extensive\nexperimental results on real-world datasets, in both offline and live online\nsystems, that demonstrate the effectiveness of our proposed techniques.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 04:03:26 GMT"}, {"version": "v2", "created": "Sat, 16 Jul 2016 18:47:43 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Asudeh", "Abolfazl", ""], ["Zhang", "Nan", ""], ["Das", "Gautam", ""]]}, {"id": "1602.06236", "submitter": "Paraschos Koutris", "authors": "Paul Beame, Paraschos Koutris and Dan Suciu", "title": "Communication Cost in Parallel Query Processing", "comments": "arXiv admin note: substantial text overlap with arXiv:1306.5972", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing conjunctive queries over large databases on\nparallel architectures without shared storage. Using the structure of such a\nquery $q$ and the skew in the data, we study tradeoffs between the number of\nprocessors, the number of rounds of communication, and the per-processor load\n-- the number of bits each processor can send or can receive in a single round\n-- that are required to compute $q$.\n  When the data is free of skew, we obtain essentially tight upper and lower\nbounds for one round algorithms and we show how the bounds degrade when there\nis skew in the data. In the case of skewed data, we show how to improve the\nalgorithms when approximate degrees of the heavy-hitter elements are available,\nobtaining essentially optimal algorithms for queries such as simple joins and\ntriangle join queries.\n  For queries that we identify as tree-like, we also prove nearly matching\nupper and lower bounds for multi-round algorithms for a natural class of\nskew-free databases. One consequence of these latter lower bounds is that for\nany $\\varepsilon>0$, using $p$ processors to compute the connected components\nof a graph, or to output the path, if any, between a specified pair of vertices\nof a graph with $m$ edges and per-processor load that is\n$O(m/p^{1-\\varepsilon})$ requires $\\Omega(\\log p)$ rounds of communication.\n  Our upper bounds are given by simple structured algorithms using MapReduce.\nOur one-round lower bounds are proved in a very general model, which we call\nthe Massively Parallel Communication (MPC) model, that allows processors to\ncommunicate arbitrary bits. Our multi-round lower bounds apply in a restricted\nversion of the MPC model in which processors in subsequent rounds after the\nfirst communication round are only allowed to send tuples.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 17:42:11 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Beame", "Paul", ""], ["Koutris", "Paraschos", ""], ["Suciu", "Dan", ""]]}, {"id": "1602.06401", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, John Liagouris, Maria Krommyda, George Papastefanatos,\n  Timos Sellis", "title": "graphVizdb: A Scalable Platform for Interactive Large Graph\n  Visualization", "comments": "32nd IEEE International Conference on Data Engineering (ICDE '16)", "journal-ref": null, "doi": "10.1109/ICDE.2016.7498340", "report-no": null, "categories": "cs.HC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel platform for the interactive visualization of very large\ngraphs. The platform enables the user to interact with the visualized graph in\na way that is very similar to the exploration of maps at multiple levels. Our\napproach involves an offline preprocessing phase that builds the layout of the\ngraph by assigning coordinates to its nodes with respect to a Euclidean plane.\nThe respective points are indexed with a spatial data structure, i.e., an\nR-tree, and stored in a database. Multiple abstraction layers of the graph\nbased on various criteria are also created offline, and they are indexed\nsimilarly so that the user can explore the dataset at different levels of\ngranularity, depending on her particular needs. Then, our system translates\nuser operations into simple and very efficient spatial operations (i.e., window\nqueries) in the backend. This technique allows for a fine-grained access to\nvery large graphs with extremely low latency and memory requirements and\nwithout compromising the functionality of the tool. Our web-based prototype\nsupports three main operations: (1) interactive navigation, (2) multi-level\nexploration, and (3) keyword search on the graph metadata.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 12:49:09 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Bikakis", "Nikos", ""], ["Liagouris", "John", ""], ["Krommyda", "Maria", ""], ["Papastefanatos", "George", ""], ["Sellis", "Timos", ""]]}, {"id": "1602.06458", "submitter": "Leopoldo Bertossi", "authors": "Babak Salimi and Leopoldo Bertossi", "title": "Causes for Query Answers from Databases, Datalog Abduction and\n  View-Updates: The Presence of Integrity Constraints", "comments": "To appear in Proceedings Flairs, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality has been recently introduced in databases, to model, characterize\nand possibly compute causes for query results (answers). Connections between\nqueryanswer causality, consistency-based diagnosis, database repairs (wrt.\nintegrity constraint violations), abductive diagnosis and the view-update\nproblem have been established. In this work we further investigate connections\nbetween query-answer causality and abductive diagnosis and the view-update\nproblem. In this context, we also define and investigate the notion of\nquery-answer causality in the presence of integrity constraints.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 20:57:59 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Salimi", "Babak", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1602.06643", "submitter": "Shaunak Bopardikar", "authors": "Alberto Speranzon and Shaunak D. Bopardikar", "title": "An Algebraic Topological Approach to Privacy: Numerical and Categorical\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we cast the classic problem of achieving k-anonymity for a\ngiven database as a problem in algebraic topology. Using techniques from this\nfield of mathematics, we propose a framework for k-anonymity that brings new\ninsights and algorithms to anonymize a database. We begin by addressing the\nsimpler case when the data lies in a metric space. This case is instrumental to\nintroduce the main ideas and notation. Specifically, by mapping a database to\nthe Euclidean space and by considering the distance between datapoints, we\nintroduce a simplicial representation of the data and show how concepts from\nalgebraic topology, such as the nerve complex and persistent homology, can be\napplied to efficiently obtain the entire spectrum of k-anonymity of the\ndatabase for various values of k and levels of generalization. For this\nrepresentation, we provide an analytic characterization of conditions under\nwhich a given representation of the dataset is k-anonymous. We introduce a\nweighted barcode diagram which, in this context, becomes a computational tool\nto tradeoff data anonymity with data loss expressed as level of generalization.\nSome simulations results are used to illustrate the main idea of the paper. We\nconclude the paper with a discussion on how to extend this method to address\nthe general case of a mix of categorical and metric data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 04:24:23 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Speranzon", "Alberto", ""], ["Bopardikar", "Shaunak D.", ""]]}, {"id": "1602.06844", "submitter": "Hao Wu", "authors": "Hao Wu, Yue Ning, Prithwish Chakraborty, Jilles Vreeken, Nikolaj Tatti\n  and Naren Ramakrishnan", "title": "Generating Realistic Synthetic Population Datasets", "comments": "The conference version of the paper is submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern studies of societal phenomena rely on the availability of large\ndatasets capturing attributes and activities of synthetic, city-level,\npopulations. For instance, in epidemiology, synthetic population datasets are\nnecessary to study disease propagation and intervention measures before\nimplementation. In social science, synthetic population datasets are needed to\nunderstand how policy decisions might affect preferences and behaviors of\nindividuals. In public health, synthetic population datasets are necessary to\ncapture diagnostic and procedural characteristics of patient records without\nviolating confidentialities of individuals. To generate such datasets over a\nlarge set of categorical variables, we propose the use of the maximum entropy\nprinciple to formalize a generative model such that in a statistically\nwell-founded way we can optimally utilize given prior information about the\ndata, and are unbiased otherwise. An efficient inference algorithm is designed\nto estimate the maximum entropy model, and we demonstrate how our approach is\nadept at estimating underlying data distributions. We evaluate this approach\nagainst both simulated data and on US census datasets, and demonstrate its\nfeasibility using an epidemic simulation application.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 16:28:17 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 16:50:26 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2016 17:20:42 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Wu", "Hao", ""], ["Ning", "Yue", ""], ["Chakraborty", "Prithwish", ""], ["Vreeken", "Jilles", ""], ["Tatti", "Nikolaj", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1602.07064", "submitter": "Jorge Martinez Gil", "authors": "Jorge Martinez-Gil", "title": "SIFT: An Algorithm for Extracting Structural Information From Taxonomies", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present SIFT, a 3-step algorithm for the analysis of the\nstructural information represented by means of a taxonomy. The major advantage\nof this algorithm is the capability to leverage the information inherent to the\nhierarchical structures of taxonomies to infer correspondences which can allow\nto merge them in a later step. This method is particular relevant in scenarios\nwhere taxonomy alignment techniques exploiting textual information from\ntaxonomy nodes cannot operate successfully.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 07:33:02 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Martinez-Gil", "Jorge", ""]]}, {"id": "1602.07168", "submitter": "Mikhail Egorov", "authors": "Michael Egorov and MacLane Wilkison", "title": "ZeroDB white paper", "comments": "Website of the project: https://www.zerodb.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ZeroDB is an end-to-end encrypted database that enables clients to operate on\n(search, sort, query, and share) encrypted data without exposing encryption\nkeys or cleartext data to the database server. The familiar client-server\narchitecture is unchanged, but query logic and encryption keys are pushed\nclient-side. Since the server has no insight into the nature of the data, the\nrisk of data being exposed via a server-side data breach is eliminated. Even if\nthe server is successfully infiltrated, adversaries would not have access to\nthe cleartext data and cannot derive anything useful out of disk or RAM\nsnapshots.\n  ZeroDB provides end-to-end encryption while maintaining much of the\nfunctionality expected of a modern database, such as full-text search, sort,\nand range queries. Additionally, ZeroDB uses proxy re-encryption and/or delta\nkey technology to enable secure, granular sharing of encrypted data without\nexposing keys to the server and without sharing the same encryption key between\nusers of the database.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 14:33:23 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 08:35:34 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 18:01:18 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Egorov", "Michael", ""], ["Wilkison", "MacLane", ""]]}, {"id": "1602.07424", "submitter": "Matteo Riondato", "authors": "Lorenzo De Stefani, Alessandro Epasto, Matteo Riondato, Eli Upfal", "title": "TRI\\`EST: Counting Local and Global Triangles in Fully-dynamic Streams\n  with Fixed Memory Size", "comments": "49 pages, 7 figures, extended version of the paper appeared at ACM\n  KDD'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TRI\\`EST, a suite of one-pass streaming algorithms to compute\nunbiased, low-variance, high-quality approximations of the global and local\n(i.e., incident to each vertex) number of triangles in a fully-dynamic graph\nrepresented as an adversarial stream of edge insertions and deletions. Our\nalgorithms use reservoir sampling and its variants to exploit the\nuser-specified memory space at all times. This is in contrast with previous\napproaches which use hard-to-choose parameters (e.g., a fixed sampling\nprobability) and offer no guarantees on the amount of memory they will use. We\nshow a full analysis of the variance of the estimations and novel concentration\nbounds for these quantities. Our experimental results on very large graphs show\nthat TRI\\`EST outperforms state-of-the-art approaches in accuracy and exhibits\na small update time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 07:39:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 18:51:06 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["De Stefani", "Lorenzo", ""], ["Epasto", "Alessandro", ""], ["Riondato", "Matteo", ""], ["Upfal", "Eli", ""]]}, {"id": "1602.07807", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Benjamin Strauss", "title": "Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly\n  Detection", "comments": "8 pages, 4 figures, 5 tables; published in Proceedings of the 2016\n  IEEE Tenth International Conference on Semantic Computing (ICSC), Laguna\n  Hills, CA, USA, pages 79-86, February 2016", "journal-ref": "In Proceedings of the 2016 IEEE Tenth International Conference on\n  Semantic Computing (ICSC), pages 79-86, Laguna Hills, CA, USA, February 2016.\n  IEEE", "doi": "10.1109/ICSC.2016.38", "report-no": null, "categories": "cs.DB cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important forms of data are stored digitally in XML format. Errors can\noccur in the textual content of the data in the fields of the XML. Fixing these\nerrors manually is time-consuming and expensive, especially for large amounts\nof data. There is increasing interest in the research, development, and use of\nautomated techniques for assisting with data cleaning. Electronic dictionaries\nare an important form of data frequently stored in XML format that frequently\nhave errors introduced through a mixture of manual typographical entry errors\nand optical character recognition errors. In this paper we describe methods for\nflagging statistical anomalies as likely errors in electronic dictionaries\nstored in XML format. We describe six systems based on different sources of\ninformation. The systems detect errors using various signals in the data\nincluding uncommon characters, text length, character-based language models,\nword-based language models, tied-field length ratios, and tied-field\ntransliteration models. Four of the systems detect errors based on expectations\nautomatically inferred from content within elements of a single field type. We\ncall these single-field systems. Two of the systems detect errors based on\ncorrespondence expectations automatically inferred from content within elements\nof multiple related field types. We call these tied-field systems. For each\nsystem, we provide an intuitive analysis of the type of error that it is\nsuccessful at detecting. Finally, we describe two larger-scale evaluations\nusing crowdsourcing with Amazon's Mechanical Turk platform and using the\nannotations of a domain expert. The evaluations consistently show that the\nsystems are useful for improving the efficiency with which errors in XML\nelectronic dictionaries can be detected.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 05:49:36 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 04:01:43 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Bloodgood", "Michael", ""], ["Strauss", "Benjamin", ""]]}, {"id": "1602.08349", "submitter": "Luigi Santocanale", "authors": "Luigi Santocanale (LIF)", "title": "Relational lattices via duality", "comments": "Coalgebraic Methods in Computer Science 2016, Apr 2016, Eindhoven,\n  Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural join and the inner union combine in different ways tables of a\nrelational database. Tropashko [18] observed that these two operations are the\nmeet and join in a class of lattices-called the relational lattices- and\nproposed lattice theory as an alternative algebraic approach to databases.\nAiming at query optimization, Litak et al. [12] initiated the study of the\nequational theory of these lattices. We carry on with this project, making use\nof the duality theory developed in [16]. The contributions of this paper are as\nfollows. Let A be a set of column's names and D be a set of cell values; we\ncharacterize the dual space of the relational lattice R(D, A) by means of a\ngeneralized ultrametric space, whose elements are the functions from A to D,\nwith the P (A)-valued distance being the Hamming one but lifted to subsets of\nA. We use the dual space to present an equational axiomatization of these\nlattices that reflects the combinatorial properties of these generalized\nultrametric spaces: symmetry and pairwise completeness. Finally, we argue that\nthese equations correspond to combinatorial properties of the dual spaces of\nlattices, in a technical sense analogous of correspondence theory in modal\nlogic. In particular, this leads to an exact characterization of the finite\nlattices satisfying these equations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 14:42:03 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Santocanale", "Luigi", "", "LIF"]]}, {"id": "1602.08721", "submitter": "Oren Kalinsky", "authors": "Oren Kalinsky, Yoav Etsion, Benny Kimelfeld", "title": "Flexible Caching in Trie Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional algorithms for multiway join computation are based on rewriting\nthe order of joins and combining results of intermediate subqueries. Recently,\nseveral approaches have been proposed for algorithms that are \"worst-case\noptimal\" wherein all relations are scanned simultaneously. An example is\nVeldhuizen's Leapfrog Trie Join (LFTJ). An important advantage of LFTJ is its\nsmall memory footprint, due to the fact that intermediate results are full\ntuples that can be dumped immediately. However, since the algorithm does not\nstore intermediate results, recurring joins must be reconstructed from the\nsource relations, resulting in excessive memory traffic. In this paper, we\naddress this problem by incorporating caches into LFTJ. We do so by adopting\nrecent developments on join optimization, tying variable ordering to tree\ndecomposition. While the traditional usage of tree decomposition computes the\nresult for each bag in advance, our proposed approach incorporates caching\ndirectly into LFTJ and can dynamically adjust the size of the cache.\nConsequently, our solution balances memory usage and repeated computation, as\nconfirmed by our experiments over SNAP datasets.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 14:26:08 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Kalinsky", "Oren", ""], ["Etsion", "Yoav", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "1602.08791", "submitter": "Vijay Gadepally", "authors": "Vijay Gadepally, Jennie Duggan, Aaron Elmore, Jeremy Kepner, Samuel\n  Madden, Tim Mattson, Michael Stonebraker", "title": "The BigDAWG Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BigDAWG is a polystore system designed to work on complex problems that\nnaturally span across different processing or storage engines. BigDAWG provides\nan architecture that supports diverse database systems working with different\ndata models, support for the competing notions of location transparency and\nsemantic completeness via islands of information and a middleware that provides\na uniform multi-island interface. In this article, we describe the current\narchitecture of BigDAWG, its application on the MIMIC II medical dataset, and\nour plans for the mechanics of cross-system queries. During the presentation,\nwe will also deliver a brief demonstration of the current version of BigDAWG.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 00:49:11 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Gadepally", "Vijay", ""], ["Duggan", "Jennie", ""], ["Elmore", "Aaron", ""], ["Kepner", "Jeremy", ""], ["Madden", "Samuel", ""], ["Mattson", "Tim", ""], ["Stonebraker", "Michael", ""]]}, {"id": "1602.08845", "submitter": "Florin Rusu", "authors": "Chengjie Qin and Florin Rusu", "title": "Dot-Product Join: An Array-Relation Join Operator for Big Model\n  Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Model analytics tackles the training of massive models that go beyond the\navailable memory of a single computing device, e.g., CPU or GPU. It generalizes\nBig Data analytics which is targeted at how to train memory-resident models\nover out-of-memory training data. In this paper, we propose an in-database\nsolution for Big Model analytics. We identify dot-product as the primary\noperation for training generalized linear models and introduce the first\narray-relation dot-product join database operator between a set of sparse\narrays and a dense relation. This is a constrained formulation of the\nextensively studied sparse matrix vector multiplication (SpMV) kernel. The\nparamount challenge in designing the dot-product join operator is how to\noptimally schedule access to the dense relation based on the non-contiguous\nentries in the sparse arrays. We prove that this problem is NP-hard and propose\na practical solution characterized by two technical contributions---dynamic\nbatch processing and array reordering. We devise three heuristics -- LSH,\nRadix, and K-center -- for array reordering and analyze them thoroughly. We\nexecute extensive experiments over synthetic and real data that confirm the\nminimal overhead the operator incurs when sufficient memory is available and\nthe graceful degradation it suffers as memory becomes scarce. Moreover,\ndot-product join achieves an order of magnitude reduction in execution time\nover alternative in-database solutions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 07:41:28 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 19:55:20 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Qin", "Chengjie", ""], ["Rusu", "Florin", ""]]}]