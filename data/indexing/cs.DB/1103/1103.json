[{"id": "1103.0083", "submitter": "Hao-En Chueh", "authors": "Hao-En Chueh", "title": "Mining Target-Oriented Fuzzy Correlation Rules to Optimize Telecom\n  Service Management", "comments": "10 pages, 7 tables", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT), Vol 3, No 1, Feb 2011, PP.74-83", "doi": "10.5121/ijcsit.2011.3106", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To optimize telecom service management, it is necessary that information\nabout telecom services is highly related to the most popular telecom service.\nTo this end, we propose an algorithm for mining target-oriented fuzzy\ncorrelation rules. In this paper, we show that by using the fuzzy statistics\nanalysis and the data mining technology, the target-oriented fuzzy correlation\nrules can be obtained from a given database. We conduct an experiment by using\na sample database from a telecom service provider in Taiwan. Our work can be\nused to assist the telecom service provider in providing the appropriate\nservices to the customers for better customer relationship management.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2011 05:17:30 GMT"}], "update_date": "2011-03-02", "authors_parsed": [["Chueh", "Hao-En", ""]]}, {"id": "1103.0172", "submitter": "Andreas Zuefle", "authors": "Thomas Bernecker, Tobias Emrich, Hans-Peter Kriegel, Nikos Mamoulis,\n  Matthias Renz, Shiming Zhang, Andreas Z\\\"ufle", "title": "Inverse Queries For Multidimensional Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional spatial queries return, for a given query object $q$, all\ndatabase objects that satisfy a given predicate, such as epsilon range and\n$k$-nearest neighbors. This paper defines and studies {\\em inverse} spatial\nqueries, which, given a subset of database objects $Q$ and a query predicate,\nreturn all objects which, if used as query objects with the predicate, contain\n$Q$ in their result. We first show a straightforward solution for answering\ninverse spatial queries for any query predicate. Then, we propose a\nfilter-and-refinement framework that can be used to improve efficiency. We show\nhow to apply this framework on a variety of inverse queries, using appropriate\nspace pruning strategies. In particular, we propose solutions for inverse\nepsilon range queries, inverse $k$-nearest neighbor queries, and inverse\nskyline queries. Our experiments show that our framework is significantly more\nefficient than naive approaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2011 14:07:04 GMT"}, {"version": "v2", "created": "Thu, 5 May 2011 15:03:08 GMT"}], "update_date": "2011-05-06", "authors_parsed": [["Bernecker", "Thomas", ""], ["Emrich", "Tobias", ""], ["Kriegel", "Hans-Peter", ""], ["Mamoulis", "Nikos", ""], ["Renz", "Matthias", ""], ["Zhang", "Shiming", ""], ["Z\u00fcfle", "Andreas", ""]]}, {"id": "1103.0248", "submitter": "Zoran Majkic", "authors": "Zoran Majkic", "title": "DB Category: Denotational Semantics for View-based Database Mappings", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a categorical denotational semantics for a database mapping, based\non views, in the most general framework of a database integration/exchange.\nDeveloped database category DB, for databases (objects) and view-based mappings\n(morphisms) between them, is different from Set category: the morphisms (based\non a set of complex query computations) are not functions, while the objects\nare database instances (sets of relations). The logic based schema mappings\nbetween databases, usually written in a highly expressive logical language (ex.\nLAV, GAV, GLAV mappings, or tuple generating dependency) may be functorially\ntranslated into this \"computation\" category DB. A new approach is adopted,\nbased on the behavioral point of view for databases, and behavioral\nequivalences for databases and their mappings are established. By introduction\nof view-based observations for databases, which are computations without\nside-effects, we define a fundamental (Universal algebra) monad with a\npower-view endofunctor T. The resulting 2-category DB is symmetric, so that any\nmapping can be represented as an object (database instance) as well, where a\nhigher-level mapping between mappings is a 2-cell morphism. Database category\nDB has the following properties: it is equal to its dual, complete and\ncocomplete. Special attention is devoted to practical examples: a query\ndefinition, a query rewriting in GAV Database-integration environment, and the\nfixpoint solution of a canonical data integration model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 16:43:20 GMT"}], "update_date": "2011-03-02", "authors_parsed": [["Majkic", "Zoran", ""]]}, {"id": "1103.0490", "submitter": "Zoran Majkic", "authors": "Zoran Majkic", "title": "Sound and Complete Query Answering in Intensional P2P Data Integration", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary use of the term 'intension' derives from the traditional logical\ndoctrine that an idea has both an extension and an intension. In this paper we\nintroduce an intensional FOL (First-order-logic) for P2P systems by fusing the\nBealer's intensional algebraic FOL with the S5 possible-world semantics of the\nMontague, we define the intensional equivalence relation for this logic and the\nweak deductive inference for it. The notion of ontology has become widespread\nin semantic Web. The meaning of concepts and views defined over some database\nontology can be considered as intensional objects which have particular\nextension in some possible world: for instance in the actual world. Thus, non\ninvasive mapping between completely independent peer databases in a P2P systems\ncan be naturally specified by the set of couples of intensionally equivalent\nviews, which have the same meaning (intension), over two different peers. Such\na kind of mapping has very different semantics from the standard view-based\nmappings based on the material implication commonly used for Data Integration.\nWe show how a P2P database system may be embedded into this intensional modal\nFOL, and how we are able to obtain a weak non-omniscient inference, which can\nbe effectively implemented. For a query answering we consider non omniscient\nquery agents and we define object-oriented class for them which implements as\nmethod the query rewriting algorithm. Finally, we show that this query\nanswering algorithm is sound and complete w.r.t. the weak deduction of the P2P\nintensional logic.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 17:18:22 GMT"}], "update_date": "2011-03-07", "authors_parsed": [["Majkic", "Zoran", ""]]}, {"id": "1103.0633", "submitter": "Yashwant Dongre prof", "authors": "Y. V. Dongare, P. S. Dhabe and S. V. Deshmukh", "title": "RDBNorma: - A semi-automated tool for relational database schema\n  normalization up to third normal form", "comments": "22 pages and international journal", "journal-ref": "International Journal of Database Management Systems ( IJDMS ),\n  Vol.3, No.1, February 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a tool called RDBNorma is proposed, that uses a novel approach\nto represent a relational database schema and its functional dependencies in\ncomputer memory using only one linked list and used for semi-automating the\nprocess of relational database schema normalization up to third normal form.\nThis paper addresses all the issues of representing a relational schema along\nwith its functional dependencies using one linked list along with the\nalgorithms to convert a relation into second and third normal form by using\nabove representation. We have compared performance of RDBNorma with existing\ntool called Micro using standard relational schemas collected from various\nresources. It is observed that proposed tool is at least 2.89 times faster than\nthe Micro and requires around half of the space than Micro to represent a\nrelation. Comparison is done by entering all the attributes and functional\ndependencies holds on a relation in the same order and implementing both the\ntools in same language and on same machine.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 09:44:14 GMT"}], "update_date": "2011-03-04", "authors_parsed": [["Dongare", "Y. V.", ""], ["Dhabe", "P. S.", ""], ["Deshmukh", "S. V.", ""]]}, {"id": "1103.0686", "submitter": "Mohamed Mkaouar", "authors": "Mohamed Mkaouar, Rafik Bouaziz, and Mohamed Moalla", "title": "Querying and Manipulating Temporal Databases", "comments": null, "journal-ref": "International Journal of Database Management Systems (IJDMS),\n  February 2011, Volume 3, Number 1", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many works have focused, for over twenty five years, on the integration of\nthe time dimension in databases (DB). However, the standard SQL3 does not yet\nallow easy definition, manipulation and querying of temporal DBs. In this\npaper, we study how we can simplify querying and manipulating temporal facts in\nSQL3, using a model that integrates time in a native manner. To do this, we\npropose new keywords and syntax to define different temporal versions for many\nrelational operators and functions used in SQL. It then becomes possible to\nperform various queries and updates appropriate to temporal facts. We\nillustrate the use of these proposals on many examples from a real application.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 14:09:24 GMT"}], "update_date": "2011-03-04", "authors_parsed": [["Mkaouar", "Mohamed", ""], ["Bouaziz", "Rafik", ""], ["Moalla", "Mohamed", ""]]}, {"id": "1103.0711", "submitter": "Marco Piccioni", "authors": "Marco Piccioni, Manuel Oriol, Bertrand Meyer", "title": "Class Schema Evolution for Persistent Object-Oriented Software: Model,\n  Empirical Study, and Automated Support", "comments": "14 pages, to appear in IEEE Transactions on Software Engineering\n  (TSE)", "journal-ref": null, "doi": "10.1109/TSE.2011.123", "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide support for object serialization in object-oriented programming\nlanguages, persistent objects have become common place and most large\nobject-oriented software systems rely on extensive amounts of persistent data.\nSuch systems also evolve over time. Retrieving previously persisted objects\nfrom classes whose schema has changed is however difficult, and may lead to\ninvalidating the consistency of the application. The ESCHER framework addresses\nthese issues through an IDE-integrated approach that handles class schema\nevolution by managing versions of the code and generating transformation\nfunctions automatically. The infrastructure also enforces class invariants to\nprevent the introduction of potentially corrupt objects. This article describes\na model for class attribute changes, a measure for class evolution robustness,\nfour empirical studies, and the design and implementation of the ESCHER system.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 15:18:55 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2011 11:09:10 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2011 20:24:08 GMT"}, {"version": "v4", "created": "Thu, 9 Feb 2012 13:29:38 GMT"}, {"version": "v5", "created": "Thu, 21 Jun 2012 09:28:23 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Piccioni", "Marco", ""], ["Oriol", "Manuel", ""], ["Meyer", "Bertrand", ""]]}, {"id": "1103.0825", "submitter": "Thanh Tran", "authors": "Graham Cormode, Magda Procopiuc, Divesh Srivastava, Thanh T. L. Tran", "title": "Differentially Private Publication of Sparse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The problem of privately releasing data is to provide a version of a dataset\nwithout revealing sensitive information about the individuals who contribute to\nthe data. The model of differential privacy allows such private release while\nproviding strong guarantees on the output. A basic mechanism achieves\ndifferential privacy by adding noise to the frequency counts in the contingency\ntables (or, a subset of the count data cube) derived from the dataset. However,\nwhen the dataset is sparse in its underlying space, as is the case for most\nmulti-attribute relations, then the effect of adding noise is to vastly\nincrease the size of the published data: it implicitly creates a huge number of\ndummy data points to mask the true data, making it almost impossible to work\nwith.\n  We present techniques to overcome this roadblock and allow efficient private\nrelease of sparse data, while maintaining the guarantees of differential\nprivacy. Our approach is to release a compact summary of the noisy data.\nGenerating the noisy data and then summarizing it would still be very costly,\nso we show how to shortcut this step, and instead directly generate the summary\nfrom the input data, without materializing the vast intermediate noisy data. We\ninstantiate this outline for a variety of sampling and filtering methods, and\nshow how to use the resulting summary for approximate, private, query\nanswering. Our experimental study shows that this is an effective, practical\nsolution, with comparable and occasionally improved utility over the costly\nmaterialization approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 05:02:47 GMT"}], "update_date": "2011-03-07", "authors_parsed": [["Cormode", "Graham", ""], ["Procopiuc", "Magda", ""], ["Srivastava", "Divesh", ""], ["Tran", "Thanh T. L.", ""]]}, {"id": "1103.0921", "submitter": "Hela Limam", "authors": "Hela Limam and Jalel Akaichi", "title": "Managing and Querying Web Services Communities: A Survey", "comments": "16 pages, 2 figures", "journal-ref": "International Journal of Database Management Systems ( IJDMS ),\n  Vol.3, No.1, February 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance of Web Services technologies and the emergence of Web\nServices into the information space, tremendous opportunities for empowering\nusers and organizations appear in various application domains including\nelectronic commerce, travel, intelligence information gathering and analysis,\nhealth care, digital government, etc. However, the technology to organize,\nsearch, integrate these Web Services has not kept pace with the rapid growth of\nthe available information space. The number of Web Services to be integrated\nmay be large and continuously changing. To ease and improve the process of Web\nservices discovery in an open environment like the Internet, it is suggested to\ngather similar Web services into groups known as communities. Although Web\nservices are intensively investigated, the community management issues have not\nbeen addressed yet In this paper we draw an overview of several Web services\nCommunities' management approaches based on some currently existing communities\nplatforms and frameworks. We also discuss different approaches for querying and\nselecting Web services under the umbrella of Web services communities'. We\ncompare the current approaches among each others with respect to some key\nrequirements.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 10:45:11 GMT"}], "update_date": "2011-03-07", "authors_parsed": [["Limam", "Hela", ""], ["Akaichi", "Jalel", ""]]}, {"id": "1103.1255", "submitter": "Nuno Lopes", "authors": "Antoine Zimmermann, Nuno Lopes, Axel Polleres, Umberto Straccia", "title": "A General Framework for Representing, Reasoning and Querying with\n  Annotated Semantic Web Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a generic framework for representing and reasoning with annotated\nSemantic Web data, a task becoming more important with the recent increased\namount of inconsistent and non-reliable meta-data on the web. We formalise the\nannotated language, the corresponding deductive system and address the query\nanswering problem. Previous contributions on specific RDF annotation domains\nare encompassed by our unified reasoning formalism as we show by instantiating\nit on (i) temporal, (ii) fuzzy, and (iii) provenance annotations. Moreover, we\nprovide a generic method for combining multiple annotation domains allowing to\nrepresent, e.g. temporally-annotated fuzzy RDF. Furthermore, we address the\ndevelopment of a query language -- AnQL -- that is inspired by SPARQL,\nincluding several features of SPARQL 1.1 (subqueries, aggregates, assignment,\nsolution modifiers) along with the formal definitions of their semantics.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2011 11:43:58 GMT"}], "update_date": "2011-03-08", "authors_parsed": [["Zimmermann", "Antoine", ""], ["Lopes", "Nuno", ""], ["Polleres", "Axel", ""], ["Straccia", "Umberto", ""]]}, {"id": "1103.1367", "submitter": "Chao Li", "authors": "Chao Li, Gerome Miklau", "title": "Efficient Batch Query Answering Under Differential Privacy", "comments": "6 figues, 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a rigorous privacy condition achieved by randomizing\nquery answers. This paper develops efficient algorithms for answering multiple\nqueries under differential privacy with low error. We pursue this goal by\nadvancing a recent approach called the matrix mechanism, which generalizes\nstandard differentially private mechanisms. This new mechanism works by first\nanswering a different set of queries (a strategy) and then inferring the\nanswers to the desired workload of queries. Although a few strategies are known\nto work well on specific workloads, finding the strategy which minimizes error\non an arbitrary workload is intractable. We prove a new lower bound on the\noptimal error of this mechanism, and we propose an efficient algorithm that\napproaches this bound for a wide range of workloads.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2011 20:22:38 GMT"}], "update_date": "2011-03-08", "authors_parsed": [["Li", "Chao", ""], ["Miklau", "Gerome", ""]]}, {"id": "1103.1604", "submitter": "Georg Gottlob", "authors": "Georg Gottlob", "title": "On Minimal Constraint Networks", "comments": "Preprint - to appear in Artificial Intelligence. (Full version of the\n  CP'2011 paper with same title)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a minimal binary constraint network, every tuple of a constraint relation\ncan be extended to a solution. The tractability or intractability of computing\na solution to such a minimal network was a long standing open question. Dechter\nconjectured this computation problem to be NP-hard. We prove this conjecture.\nWe also prove a conjecture by Dechter and Pearl stating that for k\\geq2 it is\nNP-hard to decide whether a single constraint can be decomposed into an\nequivalent k-ary constraint network. We show that this holds even in case of\nbi-valued constraints where k\\geq3, which proves another conjecture of Dechter\nand Pearl. Finally, we establish the tractability frontier for this problem\nwith respect to the domain cardinality and the parameter k.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2011 19:02:33 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2011 23:04:43 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2011 10:00:57 GMT"}, {"version": "v4", "created": "Mon, 20 Jun 2011 15:15:25 GMT"}, {"version": "v5", "created": "Sat, 12 May 2012 00:10:11 GMT"}, {"version": "v6", "created": "Wed, 25 Jul 2012 13:48:03 GMT"}], "update_date": "2012-07-26", "authors_parsed": [["Gottlob", "Georg", ""]]}, {"id": "1103.2406", "submitter": "Nilesh Dalvi", "authors": "Nilesh Dalvi (Yahoo! Research), Ravi Kumar (Yahoo! Research), Mohamed\n  Soliman (U. of Waterloo)", "title": "Automatic Wrappers for Large Scale Web Extraction", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 4, pp.\n  219-230 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic framework to make wrapper induction algorithms tolerant\nto noise in the training data. This enables us to learn wrappers in a\ncompletely unsupervised manner from automatically and cheaply obtained noisy\ntraining data, e.g., using dictionaries and regular expressions. By removing\nthe site-level supervision that wrapper-based techniques require, we are able\nto perform information extraction at web-scale, with accuracy unattained with\nexisting unsupervised extraction techniques. Our system is used in production\nat Yahoo! and powers live applications.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2011 01:05:38 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Dalvi", "Nilesh", "", "Yahoo! Research"], ["Kumar", "Ravi", "", "Yahoo! Research"], ["Soliman", "Mohamed", "", "U. of Waterloo"]]}, {"id": "1103.2408", "submitter": "Sandeep Tata", "authors": "Jun Rao (LinkedIn), Eugene J. Shekita (IBM Research), Sandeep Tata\n  (IBM Research)", "title": "Using Paxos to Build a Scalable, Consistent, and Highly Available\n  Datastore", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 4, pp.\n  243-254 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spinnaker is an experimental datastore that is designed to run on a large\ncluster of commodity servers in a single datacenter. It features key-based\nrange partitioning, 3-way replication, and a transactional get-put API with the\noption to choose either strong or timeline consistency on reads. This paper\ndescribes Spinnaker's Paxos-based replication protocol. The use of Paxos\nensures that a data partition in Spinnaker will be available for reads and\nwrites as long a majority of its replicas are alive. Unlike traditional\nmaster-slave replication, this is true regardless of the failure sequence that\noccurs. We show that Paxos replication can be competitive with alternatives\nthat provide weaker consistency guarantees. Compared to an eventually\nconsistent datastore, we show that Spinnaker can be as fast or even faster on\nreads and only 5% to 10% slower on writes.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2011 01:06:32 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Rao", "Jun", "", "LinkedIn"], ["Shekita", "Eugene J.", "", "IBM Research"], ["Tata", "Sandeep", "", "IBM Research"]]}, {"id": "1103.2409", "submitter": "Bolin Ding", "authors": "Bolin Ding (UIUC), Arnd Christian K\\\"onig (Microsoft Research)", "title": "Fast Set Intersection in Memory", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 4, pp.\n  255-266 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set intersection is a fundamental operation in information retrieval and\ndatabase systems. This paper introduces linear space data structures to\nrepresent sets such that their intersection can be computed in a worst-case\nefficient way. In general, given k (preprocessed) sets, with totally n\nelements, we will show how to compute their intersection in expected time\nO(n/sqrt(w)+kr), where r is the intersection size and w is the number of bits\nin a machine-word. In addition,we introduce a very simple version of this\nalgorithm that has weaker asymptotic guarantees but performs even better in\npractice; both algorithms outperform the state of the art techniques in terms\nof execution time for both synthetic and real data sets and workloads.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2011 01:08:36 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Ding", "Bolin", "", "UIUC"], ["K\u00f6nig", "Arnd Christian", "", "Microsoft Research"]]}, {"id": "1103.2410", "submitter": "Vibhor Rastogi", "authors": "Vibhor Rastogi (Yahoo! Research), Nilesh Dalvi (Yahoo! Research),\n  Minos Garofalakis (Technical University of Crete)", "title": "Large-Scale Collective Entity Matching", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 4, pp.\n  208-218 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been several recent advancements in Machine Learning community on\nthe Entity Matching (EM) problem. However, their lack of scalability has\nprevented them from being applied in practical settings on large real-life\ndatasets. Towards this end, we propose a principled framework to scale any\ngeneric EM algorithm. Our technique consists of running multiple instances of\nthe EM algorithm on small neighborhoods of the data and passing messages across\nneighborhoods to construct a global solution. We prove formal properties of our\nframework and experimentally demonstrate the effectiveness of our approach in\nscaling EM algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2011 01:09:30 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Rastogi", "Vibhor", "", "Yahoo! Research"], ["Dalvi", "Nilesh", "", "Yahoo! Research"], ["Garofalakis", "Minos", "", "Technical University of Crete"]]}, {"id": "1103.2566", "submitter": "Andrew Twigg", "authors": "Andrew Byde, Andy Twigg", "title": "Optimal query/update tradeoffs in versioned dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  External-memory dictionaries are a fundamental data structure in file systems\nand databases. Versioned (or fully-persistent) dictionaries have an associated\nversion tree where queries can be performed at any version, updates can be\nperformed on leaf versions, and any version can be `cloned' by adding a child.\nVarious query/update tradeoffs are known for unversioned dictionaries, many of\nthem with matching upper and lower bounds. No fully-versioned external-memory\ndictionaries are known with optimal space/query/update tradeoffs. In\nparticular, no versioned constructions are known that offer updates in $o(1)$\nI/Os using O(N) space. We present the first cache-oblivious and cache-aware\nconstructions that achieve a wide range of optimal points on this tradeoff.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2011 23:35:48 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2011 12:17:18 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Byde", "Andrew", ""], ["Twigg", "Andy", ""]]}, {"id": "1103.2635", "submitter": "Lawrence Cayton", "authors": "Lawrence Cayton", "title": "Accelerating Nearest Neighbor Search on Manycore Systems", "comments": null, "journal-ref": "In Proceedings of the 2012 IEEE 26th International Parallel and\n  Distributed Processing Symposium (IPDPS '12). IEEE Computer Society,\n  Washington, DC, USA, 402-413", "doi": "10.1109/IPDPS.2012.45", "report-no": null, "categories": "cs.DB cs.CG cs.DC cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for accelerating metric similarity search that are\neffective on modern hardware. Our algorithms factor into easily parallelizable\ncomponents, making them simple to deploy and efficient on multicore CPUs and\nGPUs. Despite the simple structure of our algorithms, their search performance\nis provably sublinear in the size of the database, with a factor dependent only\non its intrinsic dimensionality. We demonstrate that our methods provide\nsubstantial speedups on a range of datasets and hardware platforms. In\nparticular, we present results on a 48-core server machine, on graphics\nhardware, and on a multicore desktop.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 11:39:23 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2011 18:26:44 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cayton", "Lawrence", ""]]}, {"id": "1103.2651", "submitter": "Yanwei Xu", "authors": "Yanwei Xu", "title": "Efficient Continual Top-$k$ Keyword Search in Relational Databases", "comments": "This paper has been withdrawn by the author due to a crucial error of\n  the algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword search in relational databases has been widely studied in recent\nyears because it does not require users neither to master a certain structured\nquery language nor to know the complex underlying data schemas. Most of\nexisting methods focus on answering snapshot keyword queries in static\ndatabases. In practice, however, databases are updated frequently, and users\nmay have long-term interests on specific topics. To deal with such a situation,\nit is necessary to build effective and efficient facility in database systems\nto support continual keyword queries evaluation.\n  In this paper, we propose an efficient method for continual keyword queries\nanswering over relational databases. The proposed method consists of two core\nalgorithms. The first one computes a set of potential top-$k$ results by\nevaluating the ranges of the future relevance score for every query result and\ncreate a light-weight state for each keyword query. The second one uses these\nstates to maintain the top-$k$ results of keyword queries when the database is\ncontinually growing. Experimental results validate the effectiveness and\nefficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 12:50:48 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2011 18:28:41 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Xu", "Yanwei", ""]]}, {"id": "1103.3102", "submitter": "Aditya Parameswaran", "authors": "Aditya Parameswaran (Stanford University), Anish Das Sarma (Yahoo!\n  Research), Hector Garcia-Molina (Stanford University), Neoklis Polyzotis (UC\n  Santa Cruz), Jennifer Widom (Stanford University)", "title": "Human-Assisted Graph Search: It's Okay to Ask Questions", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 5, pp.\n  267-278 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of human-assisted graph search: given a directed\nacyclic graph with some (unknown) target node(s), we consider the problem of\nfinding the target node(s) by asking an omniscient human questions of the form\n\"Is there a target node that is reachable from the current node?\". This general\nproblem has applications in many domains that can utilize human intelligence,\nincluding curation of hierarchies, debugging workflows, image segmentation and\ncategorization, interactive search and filter synthesis. To our knowledge, this\nwork provides the first formal algorithmic study of the optimization of human\ncomputation for this problem. We study various dimensions of the problem space,\nproviding algorithms and complexity results. Our framework and algorithms can\nbe used in the design of an optimizer for crowd-sourcing platforms such as\nMechanical Turk.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 05:51:08 GMT"}], "update_date": "2011-03-17", "authors_parsed": [["Parameswaran", "Aditya", "", "Stanford University"], ["Sarma", "Anish Das", "", "Yahoo!\n  Research"], ["Garcia-Molina", "Hector", "", "Stanford University"], ["Polyzotis", "Neoklis", "", "UC\n  Santa Cruz"], ["Widom", "Jennifer", "", "Stanford University"]]}, {"id": "1103.3103", "submitter": "Mohamed Yakout", "authors": "Mohamed Yakout (Purdue University), Ahmed K. Elmagarmid (Qatar\n  Computing Research Institute), Jennifer Neville (Purdue University), Mourad\n  Ouzzani (Purdue University), Ihab F. Ilyas (University of Waterloo)", "title": "Guided Data Repair", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 5, pp.\n  279-289 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present GDR, a Guided Data Repair framework that\nincorporates user feedback in the cleaning process to enhance and accelerate\nexisting automatic repair techniques while minimizing user involvement. GDR\nconsults the user on the updates that are most likely to be beneficial in\nimproving data quality. GDR also uses machine learning methods to identify and\napply the correct updates directly to the database without the actual\ninvolvement of the user on these specific updates. To rank potential updates\nfor consultation by the user, we first group these repairs and quantify the\nutility of each group using the decision-theory concept of value of information\n(VOI). We then apply active learning to order updates within a group based on\ntheir ability to improve the learned model. User feedback is used to repair the\ndatabase and to adaptively refine the training set for the model. We\nempirically evaluate GDR on a real-world dataset and show significant\nimprovement in data quality using our user guided repairing process. We also,\nassess the trade-off between the user efforts and the resulting data quality.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 05:51:51 GMT"}], "update_date": "2011-03-17", "authors_parsed": [["Yakout", "Mohamed", "", "Purdue University"], ["Elmagarmid", "Ahmed K.", "", "Qatar\n  Computing Research Institute"], ["Neville", "Jennifer", "", "Purdue University"], ["Ouzzani", "Mourad", "", "Purdue University"], ["Ilyas", "Ihab F.", "", "University of Waterloo"]]}, {"id": "1103.3105", "submitter": "Bingsheng He", "authors": "Bingsheng He (Nanyang Technological University), Jeffrey Xu Yu\n  (Chinese University of Hong Kong)", "title": "High-Throughput Transaction Executions on Graphics Processors", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 5, pp.\n  314-325 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OLTP (On-Line Transaction Processing) is an important business system sector\nin various traditional and emerging online services. Due to the increasing\nnumber of users, OLTP systems require high throughput for executing tens of\nthousands of transactions in a short time period. Encouraged by the recent\nsuccess of GPGPU (General-Purpose computation on Graphics Processors), we\npropose GPUTx, an OLTP engine performing high-throughput transaction executions\non the GPU for in-memory databases. Compared with existing GPGPU studies\nusually optimizing a single task, transaction executions require handling many\nsmall tasks concurrently. Specifically, we propose the bulk execution model to\ngroup multiple transactions into a bulk and to execute the bulk on the GPU as a\nsingle task. The transactions within the bulk are executed concurrently on the\nGPU. We study three basic execution strategies (one with locks and the other\ntwo lock-free), and optimize them with the GPU features including the hardware\nsupport of atomic operations, the massive thread parallelism and the SPMD\n(Single Program Multiple Data) execution. We evaluate GPUTx on a recent NVIDIA\nGPU in comparison with its counterpart on a quad-core CPU. Our experimental\nresults show that optimizations on GPUTx significantly improve the throughput,\nand the optimized GPUTx achieves 4-10 times higher throughput than its\nCPU-based counterpart on public transaction processing benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 05:52:51 GMT"}], "update_date": "2011-03-17", "authors_parsed": [["He", "Bingsheng", "", "Nanyang Technological University"], ["Yu", "Jeffrey Xu", "", "Chinese University of Hong Kong"]]}, {"id": "1103.3107", "submitter": "Christopher Re", "authors": "Mehmet Levent Koc (University of Wisconsin-Madiso), Christopher R\\'e\n  (University of Wisconsin-Madison)", "title": "Incrementally Maintaining Classification using an RDBMS", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 5, pp.\n  302-313 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of imprecise data has motivated both researchers and the\ndatabase industry to push statistical techniques into relational database\nmanagement systems (RDBMSs). We study algorithms to maintain model-based views\nfor a popular statistical technique, classification, inside an RDBMS in the\npresence of updates to the training examples. We make three technical\ncontributions: (1) An algorithm that incrementally maintains classification\ninside an RDBMS. (2) An analysis of the above algorithm that shows that our\nalgorithm is optimal among all deterministic algorithms (and asymptotically\nwithin a factor of 2 of a nondeterministic optimal). (3) An index structure\nbased on the technical ideas that underlie the above algorithm which allows us\nto store only a fraction of the entities in memory. We apply our techniques to\ntext processing, and we demonstrate that our algorithms provide several orders\nof magnitude improvement over non-incremental approaches to classification on a\nvariety of data sets: such as the Cora, UCI Machine Learning Repository data\nsets, Citeseer, and DBLife.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 05:57:58 GMT"}, {"version": "v2", "created": "Sat, 16 Apr 2011 08:57:50 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Koc", "Mehmet Levent", "", "University of Wisconsin-Madiso"], ["R\u00e9", "Christopher", "", "University of Wisconsin-Madison"]]}, {"id": "1103.3753", "submitter": "Istv\\'an Sz\\'epk\\'uti", "authors": "Istv\\'an Sz\\'epk\\'uti", "title": "On the Scalability of Multidimensional Databases", "comments": "18 pages, 1 figure, 8 tables. Paper presented at the Second\n  Conference of PhD Students in Computer Science, Szeged, Hungary, 20 - 23 July\n  2000. For further details, please refer to\n  http://www.inf.u-szeged.hu/~szepkuti/papers.html#scalability", "journal-ref": "Periodica Polytechnica Electrical Engineering, Vol. 44, Number 1,\n  pp. 103-119, 2000", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly accepted in the practice of on-line analytical processing of\ndatabases that the multidimensional database organization is less scalable than\nthe relational one. It is easy to see that the size of the multidimensional\norganization may increase very quickly. For example, if we introduce one\nadditional dimension, then the total number of possible cells will be at least\ndoubled. However, this reasoning does not takethe fact into account that the\nmultidimensional organization can be compressed. There are compression\ntechniques, which can remove all or at least a part of the empty cells from the\nmultidimensional organization, while maintaining a good retrieval performance.\nRelational databases often use B-tree indices to speed up the access to given\nrows of tables. It can be proven, under some reasonable assumptions, that the\ntotal size of the table and the B-tree index is bigger than a compressed\nmultidimensional representation. This implies that the compressed array results\nin a smaller database and faster access at the same time. This paper compares\nseveral compression techniques and shows when we should and should not apply\ncompressed arrays instead of relational tables.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2011 06:12:56 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2011 22:55:55 GMT"}], "update_date": "2011-04-27", "authors_parsed": [["Sz\u00e9pk\u00fati", "Istv\u00e1n", ""]]}, {"id": "1103.3857", "submitter": "Istv\\'an Sz\\'epk\\'uti", "authors": "Istv\\'an Sz\\'epk\\'uti", "title": "Difference Sequence Compression of Multidimensional Databases", "comments": "22 pages, 4 figures, 5 tables. Paper presented at the Third\n  Conference of PhD Students in Computer Science, Szeged, Hungary, 1 - 4 July\n  2002. For further details, please refer to\n  http://www.inf.u-szeged.hu/~szepkuti/papers.html#differencesequence", "journal-ref": "Periodica Polytechnica Electrical Engineering, Vol. 48, Number\n  3-4, pp. 197-218, 2004", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multidimensional databases often use compression techniques in order to\ndecrease the size of the database. This paper introduces a new method called\ndifference sequence compression. Under some conditions, this new technique is\nable to create a smaller size multidimensional database than others like single\ncount header compression, logical position compression or base-offset\ncompression. Keywords: compression, multidimensional database, On-line\nAnalytical Processing, OLAP.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2011 15:35:03 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2011 20:17:21 GMT"}], "update_date": "2011-04-28", "authors_parsed": [["Sz\u00e9pk\u00fati", "Istv\u00e1n", ""]]}, {"id": "1103.3863", "submitter": "Istv\\'an Sz\\'epk\\'uti", "authors": "Istv\\'an Sz\\'epk\\'uti", "title": "Multidimensional or Relational? / How to Organize an On-line Analytical\n  Processing Database", "comments": "28 pages, 2 figures, 13 tables. Talk presented at the XIV\n  International Conference on Mathematical Programming, Matrahaza, Hungary, 28\n  - 31 March 1999; Computing Research Repository, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, the number of OLAP applications increased quickly.\nThese applications use two significantly different DB structures:\nmultidimensional (MD) and table-based. One can show that the traditional model\nof relational databases cannot make difference between these two structures.\nAnother model is necessary to make the differences visible. One of these is the\nspeed of the system. It can be proven that the multidimensional DB organization\nresults in shorter response times. And it is crucial, since a manager may\nbecome impatient, if he or she has to wait say more than 20 seconds for the\nnext screen. On the other hand, we have to pay for the speed with a bigger DB\nsize. Why does the size of MD databases grow so quickly? The reason is the\nsparsity of data: The MD matrix contains many empty cells. Efficient handling\nof sparse matrices is indispensable in an OLAP application. One way to handle\nsparsity is to take the structure closer to the table-based one. Thus the DB\nsize decreases, while the application gets slower. Therefore, other methods are\nneeded. This paper deals with the comparison of the two DB structures and the\nlimits of their usage. The new results of the paper: (1) It gives a\nconstructive proof that all relations can be represented in MD arrays. (2) It\nalso shows when the MD array representation is quicker than the table-based\none. (3) The MD representation results in smaller DB size under some\nconditions. One such sufficient condition is proved in the paper. (4) A\nvariation of the single count header compression scheme is described with an\nalgorithm, which creates the compressed array from the ordered table without\nmaterializing the uncompressed array. (5) The speed of the two different\ndatabase organizations is tested with experiments, as well. The tests are done\non benchmark as well as real life data. The experiments support the theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2011 17:02:03 GMT"}, {"version": "v2", "created": "Sun, 17 Apr 2011 20:28:38 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2011 20:42:22 GMT"}], "update_date": "2011-04-21", "authors_parsed": [["Sz\u00e9pk\u00fati", "Istv\u00e1n", ""]]}, {"id": "1103.4168", "submitter": "Istv\\'an Sz\\'epk\\'uti", "authors": "Istv\\'an Sz\\'epk\\'uti", "title": "Caching in Multidimensional Databases", "comments": "14 pages, 5 figures, 8 tables. Paper presented at the Fifth\n  Conference of PhD Students in Computer Science, Szeged, Hungary, 27 - 30 June\n  2006. For further details, please refer to\n  http://www.inf.u-szeged.hu/~szepkuti/papers.html#caching", "journal-ref": "Periodica Polytechnica Electrical Engineering, Vol. 51, Number\n  3-4, pp. 119-132, 2007", "doi": "10.3311/pp.ee.2007-3-4.06", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One utilisation of multidimensional databases is the field of On-line\nAnalytical Processing (OLAP). The applications in this area are designed to\nmake the analysis of shared multidimensional information fast [9]. On one hand,\nspeed can be achieved by specially devised data structures and algorithms. On\nthe other hand, the analytical process is cyclic. In other words, the user of\nthe OLAP application runs his or her queries one after the other. The output of\nthe last query may be there (at least partly) in one of the previous results.\nTherefore caching also plays an important role in the operation of these\nsystems. However, caching itself may not be enough to ensure acceptable\nperformance. Size does matter: The more memory is available, the more we gain\nby loading and keeping information in there. Oftentimes, the cache size is\nfixed. This limits the performance of the multidimensional database, as well,\nunless we compress the data in order to move a greater proportion of them into\nthe memory. Caching combined with proper compression methods promise further\nperformance improvements. In this paper, we investigate how caching influences\nthe speed of OLAP systems. Different physical representations (multidimensional\nand table) are evaluated. For the thorough comparison, models are proposed. We\ndraw conclusions based on these models, and the conclusions are verified with\nempirical data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 22:07:08 GMT"}, {"version": "v2", "created": "Mon, 2 May 2011 23:03:44 GMT"}], "update_date": "2011-05-04", "authors_parsed": [["Sz\u00e9pk\u00fati", "Istv\u00e1n", ""]]}, {"id": "1103.4169", "submitter": "Istv\\'an Sz\\'epk\\'uti", "authors": "Istv\\'an Sz\\'epk\\'uti", "title": "Difference-Huffman Coding of Multidimensional Databases", "comments": "23 pages, 3 figures, 6 tables. Revised version of this paper appeared\n  in Periodica Polytechnica Electrical Engineering. Please refer to\n  http://arxiv.org/abs/1103.4168; Computing Research Repository, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new compression method called difference-Huffman coding (DHC) is introduced\nin this paper. It is verified empirically that DHC results in a smaller\nmultidimensional physical representation than those for other previously\npublished techniques (single count header compression, logical position\ncompression, base-offset compression and difference sequence compression). The\narticle examines how caching influences the expected retrieval time of the\nmultidimensional and table representations of relations. A model is proposed\nfor this, which is then verified with empirical data. Conclusions are drawn,\nbased on the model and the experiment, about when one physical representation\noutperforms another in terms of retrieval time. Over the tested range of\navailable memory, the performance for the multidimensional representation was\nalways much quicker than for the table representation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 22:12:10 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2011 19:02:52 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2011 17:19:48 GMT"}], "update_date": "2011-04-21", "authors_parsed": [["Sz\u00e9pk\u00fati", "Istv\u00e1n", ""]]}, {"id": "1103.4282", "submitter": "Andrew Twigg", "authors": "Andy Twigg, Andrew Byde, Grzegorz Milos, Tim Moreton, John Wilkes, Tom\n  Wilkie", "title": "Stratified B-trees and versioning dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic versioned data structure in storage and computer science is the\ncopy-on-write (CoW) B-tree -- it underlies many of today's file systems and\ndatabases, including WAFL, ZFS, Btrfs and more. Unfortunately, it doesn't\ninherit the B-tree's optimality properties; it has poor space utilization,\ncannot offer fast updates, and relies on random IO to scale. Yet, nothing\nbetter has been developed since. We describe the `stratified B-tree', which\nbeats all known semi-external memory versioned B-trees, including the CoW\nB-tree. In particular, it is the first versioned dictionary to achieve optimal\ntradeoffs between space, query and update performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2011 14:58:20 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2011 14:07:56 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Twigg", "Andy", ""], ["Byde", "Andrew", ""], ["Milos", "Grzegorz", ""], ["Moreton", "Tim", ""], ["Wilkes", "John", ""], ["Wilkie", "Tom", ""]]}, {"id": "1103.4410", "submitter": "Zhao Cao", "authors": "Zhao Cao (University of Massachusetts), Charles Sutton (University of\n  Edinburgh), Yanlei Diao (University of Massachusetts), Prashant Shenoy\n  (University of Massachusetts)", "title": "Distributed Inference and Query Processing for RFID Tracking and\n  Monitoring", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 5, pp.\n  326-337 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the design of a scalable, distributed stream\nprocessing system for RFID tracking and monitoring. Since RFID data lacks\ncontainment and location information that is key to query processing, we\npropose to combine location and containment inference with stream query\nprocessing in a single architecture, with inference as an enabling mechanism\nfor high-level query processing. We further consider challenges in\ninstantiating such a system in large distributed settings and design techniques\nfor distributed inference and query processing. Our experimental results, using\nboth real-world data and large synthetic traces, demonstrate the accuracy,\nefficiency, and scalability of our proposed techniques.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2011 23:23:20 GMT"}], "update_date": "2011-03-24", "authors_parsed": [["Cao", "Zhao", "", "University of Massachusetts"], ["Sutton", "Charles", "", "University of\n  Edinburgh"], ["Diao", "Yanlei", "", "University of Massachusetts"], ["Shenoy", "Prashant", "", "University of Massachusetts"]]}, {"id": "1103.4778", "submitter": "Jos\\'e L Balc\\'azar Navarro", "authors": "Jos\\'e L. Balc\\'azar", "title": "Formal and Computational Properties of the Confidence Boost of\n  Association Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some existing notions of redundancy among association rules allow for a\nlogical-style characterization and lead to irredundant bases of absolutely\nminimum size. One can push the intuition of redundancy further and find an\nintuitive notion of interest of an association rule, in terms of its \"novelty\"\nwith respect to other rules. Namely: an irredundant rule is so because its\nconfidence is higher than what the rest of the rules would suggest; then, one\ncan ask: how much higher? We propose to measure such a sort of \"novelty\"\nthrough the confidence boost of a rule, which encompasses two previous similar\nnotions (confidence width and rule blocking, of which the latter is closely\nrelated to the earlier measure \"improvement\"). Acting as a complement to\nconfidence and support, the confidence boost helps to obtain small and crisp\nsets of mined association rules, and solves the well-known problem that, in\ncertain cases, rules of negative correlation may pass the confidence bound. We\nanalyze the properties of two versions of the notion of confidence boost, one\nof them a natural generalization of the other. We develop efficient\nalgorithmics to filter rules according to their confidence boost, compare the\nconcept to some similar notions in the bibliography, and describe the results\nof some experimentation employing the new notions on standard benchmark\ndatasets. We describe an open-source association mining tool that embodies one\nof our variants of confidence boost in such a way that the data mining process\ndoes not require the user to select any value for any parameter.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2011 14:45:50 GMT"}], "update_date": "2011-03-25", "authors_parsed": [["Balc\u00e1zar", "Jos\u00e9 L.", ""]]}, {"id": "1103.4916", "submitter": "Kodge B. G.", "authors": "B. G. Kodge, P. S. Hiremath", "title": "Detection of Spatial Changes using Spatial Data Mining", "comments": "This paper has been withdrawn by the author", "journal-ref": "Advances in Information Mining, ISSN: 0975-3265, Volume 2, Issue\n  2, 2010, pp-14-18", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Change detection based on analysis and samples are analyzed. Land\nuse/cover change detection based on SDM is discussed.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2011 07:10:50 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2012 10:40:21 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Kodge", "B. G.", ""], ["Hiremath", "P. S.", ""]]}, {"id": "1103.4979", "submitter": "K. Viswanathan Iyer", "authors": "K. Viswanathan Iyer", "title": "An Introduction to Functional dependency in Relational Databases", "comments": "Revised 2nd version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This write-up is the suggested lecture notes for a second level course on\nadvanced topics in database systems for master's students of Computer Science\nwith a theoretical focus. A prerequisite in algorithms and an exposure to\ndatabase systems are required. Additional reading may require exposure to\nmathematical logic. The starting point for these notes are from M.Y.Vardi's\nsurvey listed herein as a reference - some of the proofs are presented as such\n. This select rewrite on functional dependency is intended to provide a few\nclarifications even though radically new design approaches are now being\nproposed.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2011 14:24:18 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2011 01:51:23 GMT"}, {"version": "v3", "created": "Tue, 1 Mar 2016 04:55:06 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Iyer", "K. Viswanathan", ""]]}, {"id": "1103.5170", "submitter": "Entong Shen", "authors": "Graham Cormode, Magda Procopiuc, Entong Shen, Divesh Srivastava, Ting\n  Yu", "title": "Differentially Private Spatial Decompositions", "comments": "ICDE 2012 (supplementary acknowledgments)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy has recently emerged as the de facto standard for\nprivate data release. This makes it possible to provide strong theoretical\nguarantees on the privacy and utility of released data. While it is well-known\nhow to release data based on counts and simple functions under this guarantee,\nit remains to provide general purpose techniques to release different kinds of\ndata. In this paper, we focus on spatial data such as locations and more\ngenerally any data that can be indexed by a tree structure. Directly applying\nexisting differential privacy methods to this type of data simply generates\nnoise. Instead, we introduce a new class of \"private spatial decompositions\":\nthese adapt standard spatial indexing methods such as quadtrees and kd-trees to\nprovide a private description of the data distribution. Equipping such\nstructures with differential privacy requires several steps to ensure that they\nprovide meaningful privacy guarantees. Various primitives, such as choosing\nsplitting points and describing the distribution of points within a region,\nmust be done privately, and the guarantees of the different building blocks\ncomposed to provide an overall guarantee. Consequently, we expose the design\nspace for private spatial decompositions, and analyze some key examples. Our\nexperimental study demonstrates that it is possible to build such\ndecompositions efficiently, and use them to answer a variety of queries\nprivately with high accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2011 22:47:47 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2011 04:12:04 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2012 19:49:50 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Cormode", "Graham", ""], ["Procopiuc", "Magda", ""], ["Shen", "Entong", ""], ["Srivastava", "Divesh", ""], ["Yu", "Ting", ""]]}, {"id": "1103.5188", "submitter": "Catuscia Palamidessi", "authors": "M\\'ario S. Alvim, Miguel E. Andr\\'es, Konstantinos Chatzikokolakis,\n  Pierpaolo Degano, Catuscia Palamidessi", "title": "Differential Privacy: on the trade-off between Utility and Information\n  Leakage", "comments": "30 pages; HAL repository", "journal-ref": "Proceedings of the 8th International Workshop on Formal Aspects of\n  Security & Trust (FAST'11), Springer, LNCS 7140, pp. 39-54, 2011", "doi": "10.1007/978-3-642-29420-4_3", "report-no": "inria-00580122", "categories": "cs.CR cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a notion of privacy that has become very popular in\nthe database community. Roughly, the idea is that a randomized query mechanism\nprovides sufficient privacy protection if the ratio between the probabilities\nthat two adjacent datasets give the same answer is bound by e^epsilon. In the\nfield of information flow there is a similar concern for controlling\ninformation leakage, i.e. limiting the possibility of inferring the secret\ninformation from the observables. In recent years, researchers have proposed to\nquantify the leakage in terms of R\\'enyi min mutual information, a notion\nstrictly related to the Bayes risk. In this paper, we show how to model the\nquery system in terms of an information-theoretic channel, and we compare the\nnotion of differential privacy with that of mutual information. We show that\ndifferential privacy implies a bound on the mutual information (but not\nvice-versa). Furthermore, we show that our bound is tight. Then, we consider\nthe utility of the randomization mechanism, which represents how close the\nrandomized answers are, in average, to the real ones. We show that the notion\nof differential privacy implies a bound on utility, also tight, and we propose\na method that under certain conditions builds an optimal randomization\nmechanism, i.e. a mechanism which provides the best utility while guaranteeing\ndifferential privacy.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2011 06:41:12 GMT"}, {"version": "v2", "created": "Mon, 9 May 2011 00:04:26 GMT"}, {"version": "v3", "created": "Thu, 25 Aug 2011 04:12:17 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Alvim", "M\u00e1rio S.", ""], ["Andr\u00e9s", "Miguel E.", ""], ["Chatzikokolakis", "Konstantinos", ""], ["Degano", "Pierpaolo", ""], ["Palamidessi", "Catuscia", ""]]}, {"id": "1103.5795", "submitter": "M. Shahriar Hossain", "authors": "M. Shahriar Hossain, Rafal A. Angryk", "title": "Heuristic Algorithm for Interpretation of Non-Atomic Categorical\n  Attributes in Similarity-based Fuzzy Databases - Scalability Evaluation", "comments": null, "journal-ref": "M. S. Hossain, R. A. Angryk, Heuristic Algorithm for\n  Interpretation of Non-Atomic Categorical Attributes in Similarity-based Fuzzy\n  Databases - Scalability Evaluation, NAFIPS 2007: 233-238", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we are analyzing scalability of the heuristic algorithm we used\nin the past to discover knowledge from multi-valued symbolic attributes in\nfuzzy databases. The non-atomic descriptors, characterizing a single attribute\nof a database record, are commonly used in fuzzy databases to reflect\nuncertainty about the recorded observation. In this paper, we present\nimplementation details and scalability tests of the algorithm, which we\ndeveloped to precisely interpret such non-atomic values and to transfer (i.e.\ndefuzzify) the fuzzy tuples to the forms acceptable for many regular (i.e.\natomic values based) data mining algorithms. Important advantages of our\napproach are: (1) its linear scalability, and (2) its unique capability of\nincorporating background knowledge, implicitly stored in the fuzzy database\nmodels in the form of fuzzy similarity hierarchy, into the\ninterpretation/defuzzification process.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 23:46:06 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Hossain", "M. Shahriar", ""], ["Angryk", "Rafal A.", ""]]}]