[{"id": "1806.00227", "submitter": "Thomas Minier", "authors": "Thomas Minier, Hala Skaf-Molli, Pascal Molli", "title": "SaGe: Preemptive Query Execution for High Data Availability on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Web applications require querying available RDF Data with high\nperformance and reliability. However, ensuring both data availability and\nperformant SPARQL query execution in the context of public SPARQL servers are\nchallenging problems. Queries could have arbitrary execution time and unknown\narrival rates. In this paper, we propose SaGe, a preemptive server-side SPARQL\nquery engine. SaGe relies on a preemptable physical query execution plan and\npreemptable physical operators. SaGe stops query execution after a given slice\nof time, saves the state of the plan and sends the saved plan back to the\nclient with retrieved results. Later, the client can continue the query\nexecution by resubmitting the saved plan to the server. By ensuring a fair\nquery execution, SaGe maintains server availability and provides high query\nthroughput. Experimental results demonstrate that SaGe outperforms the state of\nthe art SPARQL query engines in terms of query throughput, query timeout and\nanswer completeness.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 07:56:22 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Minier", "Thomas", ""], ["Skaf-Molli", "Hala", ""], ["Molli", "Pascal", ""]]}, {"id": "1806.00571", "submitter": "Chengyuan Zhang", "authors": "Jun Long, Lei Zhu, Chengyuan Zhang, Zhan Yang, Yunwu Lin and Ruipeng\n  Chen", "title": "Efficient Interactive Search for Geo-tagged Multimedia Data", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advances in mobile computing and multimedia techniques, there are\nvast amount of multimedia data with geographical information collected in\nmultifarious applications. In this paper, we propose a novel type of image\nsearch named interactive geo-tagged image search which aims to find out a set\nof images based on geographical proximity and similarity of visual content, as\nwell as the preference of users. Existing approaches for spatial keyword query\nand geo-image query cannot address this problem effectively since they do not\nconsider these three type of information together for query. In order to solve\nthis challenge efficiently, we propose the definition of interactive top-$k$\ngeo-tagged image query and then present a framework including candidate search\nstage , interaction stage and termination stage. To enhance the searching\nefficiency in a large-scale database, we propose the candidate search algorithm\nnamed GI-SUPER Search based on a new notion called superior relationship and\nGIR-Tree, a novel index structure. Furthermore, two candidate selection methods\nare proposed for learning the preferences of the user during the interaction.\nAt last, the termination procedure and estimation procedure are introduced in\nbrief. Experimental evaluation on real multimedia dataset demonstrates that our\nsolution has a really high performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 02:11:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Long", "Jun", ""], ["Zhu", "Lei", ""], ["Zhang", "Chengyuan", ""], ["Yang", "Zhan", ""], ["Lin", "Yunwu", ""], ["Chen", "Ruipeng", ""]]}, {"id": "1806.00637", "submitter": "Peng Cheng", "authors": "Jiayang Tu, Peng Cheng, Lei Chen", "title": "Quality-Assured Synchronized Task Assignment in Crowdsourcing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of crowdsourcing platforms that aggregate the\nintelligence of Internet workers, crowdsourcing has been widely utilized to\naddress problems that require human cognitive abilities. Considering great\ndynamics of worker arrival and departure, it is of vital importance to design a\ntask assignment scheme to adaptively select the most beneficial tasks for the\navailable workers. In this paper, in order to make the most efficient\nutilization of the worker labor and balance the accuracy of answers and the\noverall latency, we a) develop a parameter estimation model that assists in\nestimating worker expertise, question easiness and answer confidence; b)\npropose a \\textit{quality-assured synchronized task assignment scheme} that\nexecutes in batches and maximizes the number of potentially completed questions\n(MCQ) within each batch. We prove that MCQ problem is NP-hard and present two\ngreedy approximation solutions to address the problem. The effectiveness and\nefficiency of the approximation solutions are further evaluated through\nextensive experiments on synthetic and real datasets. The experimental results\nshow that the accuracy and the overall latency of the MCQ approaches outperform\nthe existing online task assignment algorithms in the synchronized task\nassignment scenario.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 14:01:49 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Tu", "Jiayang", ""], ["Cheng", "Peng", ""], ["Chen", "Lei", ""]]}, {"id": "1806.01168", "submitter": "Jinfei Liu", "authors": "Jinfei Liu, Juncheng Yang, Li Xiong, Jian Pei", "title": "Secure and Efficient Skyline Queries on Encrypted Data", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outsourcing data and computation to cloud server provides a cost-effective\nway to support large scale data storage and query processing. However, due to\nsecurity and privacy concerns, sensitive data (e.g., medical records) need to\nbe protected from the cloud server and other unauthorized users. One approach\nis to outsource encrypted data to the cloud server and have the cloud server\nperform query processing on the encrypted data only. It remains a challenging\ntask to support various queries over encrypted data in a secure and efficient\nway such that the cloud server does not gain any knowledge about the data,\nquery, and query result. In this paper, we study the problem of secure skyline\nqueries over encrypted data. The skyline query is particularly important for\nmulti-criteria decision making but also presents significant challenges due to\nits complex computations. We propose a fully secure skyline query protocol on\ndata encrypted using semantically-secure encryption. As a key subroutine, we\npresent a new secure dominance protocol, which can be also used as a building\nblock for other queries. Furthermore, we demonstrate two optimizations, data\npartitioning and lazy merging, to further reduce the computation load. Finally,\nwe provide both serial and parallelized implementations and empirically study\nthe protocols in terms of efficiency and scalability under different parameter\nsettings, verifying the feasibility of our proposed solutions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:07:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Liu", "Jinfei", ""], ["Yang", "Juncheng", ""], ["Xiong", "Li", ""], ["Pei", "Jian", ""]]}, {"id": "1806.01217", "submitter": "Chengsheng Mao", "authors": "Chengsheng Mao, Alal Eran, Yuan Luo", "title": "Efficient Genomic Interval Queries Using Augmented Range Trees", "comments": "4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient large-scale annotation of genomic intervals is essential for\npersonal genome interpretation in the realm of precision medicine. There are 13\npossible relations between two intervals according to Allen's interval algebra.\nConventional interval trees are routinely used to identify the genomic\nintervals satisfying a coarse relation with a query interval, but cannot\nsupport efficient query for more refined relations such as all Allen's\nrelations. We design and implement a novel approach to address this unmet need.\nThrough rewriting Allen's interval relations, we transform an interval query to\na range query, then adapt and utilize the range trees for querying. We\nimplement two types of range trees: a basic 2-dimensional range tree (2D-RT)\nand an augmented range tree with fractional cascading (RTFC) and compare them\nwith the conventional interval tree (IT). Theoretical analysis shows that RTFC\ncan achieve the best time complexity for interval queries regarding all Allen's\nrelations among the three trees. We also perform comparative experiments on the\nefficiency of RTFC, 2D-RT and IT in querying noncoding element annotations in a\nlarge collection of personal genomes. Our experimental results show that 2D-RT\nis more efficient than IT for interval queries regarding most of Allen's\nrelations, RTFC is even more efficient than 2D-RT. The results demonstrate that\nRTFC is an efficient data structure for querying large-scale datasets regarding\nAllen's relations between genomic intervals, such as those required by\ninterpreting genome-wide variation in large populations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:59:15 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Mao", "Chengsheng", ""], ["Eran", "Alal", ""], ["Luo", "Yuan", ""]]}, {"id": "1806.01270", "submitter": "Kai Rothauge", "authors": "Alex Gittens, Kai Rothauge, Shusen Wang, Michael W. Mahoney, Jey\n  Kottalam, Lisa Gerhardt, Prabhat, Michael Ringenburg, Kristyn Maschhoff", "title": "Alchemist: An Apache Spark <=> MPI Interface", "comments": "Accepted for publication in Concurrency and Computation: Practice and\n  Experience, Special Issue on the Cray User Group 2018. arXiv admin note: text\n  overlap with arXiv:1805.11800", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Apache Spark framework for distributed computation is popular in the data\nanalytics community due to its ease of use, but its MapReduce-style programming\nmodel can incur significant overheads when performing computations that do not\nmap directly onto this model. One way to mitigate these costs is to off-load\ncomputations onto MPI codes. In recent work, we introduced Alchemist, a system\nfor the analysis of large-scale data sets. Alchemist calls MPI-based libraries\nfrom within Spark applications, and it has minimal coding, communication, and\nmemory overheads. In particular, Alchemist allows users to retain the\nproductivity benefits of working within the Spark software ecosystem without\nsacrificing performance efficiency in linear algebra, machine learning, and\nother related computations.\n  In this paper, we discuss the motivation behind the development of Alchemist,\nand we provide a detailed overview its design and usage. We also demonstrate\nthe efficiency of our approach on medium-to-large data sets, using some\nstandard linear algebra operations, namely matrix multiplication and the\ntruncated singular value decomposition of a dense matrix, and we compare the\nperformance of Spark with that of Spark+Alchemist. These computations are run\non the NERSC supercomputer Cori Phase 1, a Cray XC40.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 23:25:29 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Gittens", "Alex", ""], ["Rothauge", "Kai", ""], ["Wang", "Shusen", ""], ["Mahoney", "Michael W.", ""], ["Kottalam", "Jey", ""], ["Gerhardt", "Lisa", ""], ["Prabhat", "", ""], ["Ringenburg", "Michael", ""], ["Maschhoff", "Kristyn", ""]]}, {"id": "1806.01657", "submitter": "Alifah Syamsiyah", "authors": "Alifah Syamsiyah, Boudewijn F. van Dongen, Remco M. Dijkman", "title": "Native Directly Follows Operator", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical legacy information systems store data in relational databases.\nProcess mining is a research discipline that analyzes this data to obtain\ninsights into processes. Many different process mining techniques can be\napplied to data. In current techniques, an XES event log serves as a basis for\nanalysis. However, because of the static characteristic of an XES event log, we\nneed to create one XES file for each process mining question, which leads to\noverhead and inflexibility. As an alternative, people attempt to perform\nprocess mining directly on the data source using so-called intermediate\nstructures. In previous work, we investigated methods to build intermediate\nstructures on source data by executing a basic SQL query on the database.\nHowever, the nested form in the SQL query can cause performance issues on the\ndatabase side. Therefore, in this paper, we propose a native SQL operator for\ndirect process discovery on relational databases. We define a native operator\nfor the simplest form of the intermediate structure, called the \"directly\nfollows relation\". This approach has been evaluated with big event data and the\nexperimental results show that it performs faster than the state-of-the-art of\ndatabase approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 12:42:45 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Syamsiyah", "Alifah", ""], ["van Dongen", "Boudewijn F.", ""], ["Dijkman", "Remco M.", ""]]}, {"id": "1806.02075", "submitter": "Paul Francis", "authors": "Paul Francis, Sebastian Probst-Eide, Pawel Obrok, Cristian Berneanu,\n  Sasa Juric, Reinhard Munz", "title": "Diffix-Birch: Extending Diffix-Aspen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A longstanding open problem is that of how to get high quality statistics\nthrough direct queries to databases containing information about individuals\nwithout revealing information specific to those individuals. Diffix is a\nframework for anonymous database query that adds noise based on the filter\nconditions in the query. A previous paper described the first version, called\ndiffix-aspen. This version, diffix-birch, extends that description to include a\nwide variety of common features found in SQL. It describes attacks associated\nwith various features, and the anonymization steps used to defend against those\nattacks. This paper describes diffix-birch, which was used for the bounty\nprogram sponsored by Aircloak starting December 2017.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 09:11:13 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 15:04:26 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Francis", "Paul", ""], ["Probst-Eide", "Sebastian", ""], ["Obrok", "Pawel", ""], ["Berneanu", "Cristian", ""], ["Juric", "Sasa", ""], ["Munz", "Reinhard", ""]]}, {"id": "1806.02227", "submitter": "Thomas Moyer", "authors": "Warren Smith and Thomas Moyer and Charles Munson", "title": "Curator: Provenance Management for Modern Distributed Systems", "comments": "Published at TaPP 2018, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data provenance is a valuable tool for protecting and troubleshooting\ndistributed systems. Careful design of the provenance components reduces the\nimpact on the design, implementation, and operation of the distributed system.\nIn this paper, we present Curator, a provenance management toolkit that can be\neasily integrated with microservice-based systems and other modern distributed\nsystems. This paper describes the design of Curator and discusses how we have\nused Curator to add provenance to distributed systems. We find that our\napproach results in no changes to the design of these distributed systems and\nminimal additional code and dependencies to manage. In addition, Curator uses\nthe same scalable infrastructure as the distributed system and can therefore\nscale with the distributed system.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 14:45:46 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Smith", "Warren", ""], ["Moyer", "Thomas", ""], ["Munson", "Charles", ""]]}, {"id": "1806.02290", "submitter": "Peng Gao", "authors": "Peng Gao, Xusheng Xiao, Zhichun Li, Kangkook Jee, Fengyuan Xu, Sanjeev\n  R. Kulkarni, Prateek Mittal", "title": "AIQL: Enabling Efficient Attack Investigation from System Monitoring\n  Data", "comments": "Accepted paper at USENIX ATC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for countering Advanced Persistent Threat (APT) attacks has led to\nthe solutions that ubiquitously monitor system activities in each host, and\nperform timely attack investigation over the monitoring data for analyzing\nattack provenance. However, existing query systems based on relational\ndatabases and graph databases lack language constructs to express key\nproperties of major attack behaviors, and often execute queries inefficiently\nsince their semantics-agnostic design cannot exploit the properties of system\nmonitoring data to speed up query execution.\n  To address this problem, we propose a novel query system built on top of\nexisting monitoring tools and databases, which is designed with novel types of\noptimizations to support timely attack investigation. Our system provides (1)\ndomain-specific data model and storage for scaling the storage, (2) a\ndomain-specific query language, Attack Investigation Query Language (AIQL) that\nintegrates critical primitives for attack investigation, and (3) an optimized\nquery engine based on the characteristics of the data and the semantics of the\nqueries to efficiently schedule the query execution. We deployed our system in\nNEC Labs America comprising 150 hosts and evaluated it using 857 GB of real\nsystem monitoring data (containing 2.5 billion events). Our evaluations on a\nreal-world APT attack and a broad set of attack behaviors show that our system\nsurpasses existing systems in both efficiency (124x over PostgreSQL, 157x over\nNeo4j, and 16x over Greenplum) and conciseness (SQL, Neo4j Cypher, and Splunk\nSPL contain at least 2.4x more constraints than AIQL).\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 16:31:09 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 03:47:46 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Gao", "Peng", ""], ["Xiao", "Xusheng", ""], ["Li", "Zhichun", ""], ["Jee", "Kangkook", ""], ["Xu", "Fengyuan", ""], ["Kulkarni", "Sanjeev R.", ""], ["Mittal", "Prateek", ""]]}, {"id": "1806.03168", "submitter": "Lei Huang", "authors": "Lei Huang, Guangjie Ren, Shun Jiang, Raphael Arar, Eric Young Liu", "title": "Data-driven Analytics for Business Architectures: Proposed Use of Graph\n  Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business Architecture (BA) plays a significant role in helping organizations\nunderstand enterprise structures and processes, and align them with strategic\nobjectives. However, traditional BAs are represented in fixed structure with\nstatic model elements and fail to dynamically capture business insights based\non internal and external data. To solve this problem, this paper introduces the\ngraph theory into BAs with aim of building extensible data-driven analytics and\nautomatically generating business insights. We use IBM's Component Business\nModel (CBM) as an example to illustrate various ways in which graph theory can\nbe leveraged for data-driven analytics, including what and how business\ninsights can be obtained. Future directions for applying graph theory to\nbusiness architecture analytics are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 06:25:25 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Huang", "Lei", ""], ["Ren", "Guangjie", ""], ["Jiang", "Shun", ""], ["Arar", "Raphael", ""], ["Liu", "Eric Young", ""]]}, {"id": "1806.03313", "submitter": "Davood Rafiei", "authors": "Davood Rafiei and Fan Deng", "title": "Similarity Join and Similarity Self-Join Size Estimation in a Streaming\n  Environment", "comments": "IEEE Transactions on Knowledge and Data Engineering (to appear)", "journal-ref": "IEEE Trans. Knowl. Data Eng. 32(4): 768-781 (2020)", "doi": "10.1109/TKDE.2019.2893175", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of similarity self-join and similarity join size\nestimation in a streaming setting where the goal is to estimate, in one scan of\nthe input and with sublinear space in the input size, the number of record\npairs that have a similarity within a given threshold. The problem has many\napplications in data cleaning and query plan generation, where the cost of a\nsimilarity join may be estimated before actually doing the join. On unary input\nwhere two records either match or don't match, the problem becomes join and\nself-join size estimation for which one-pass algorithms are readily available.\nOur work addresses the problem for d-ary input, for d >= 1, where the degree of\nsimilarity can vary from 1 to d. We show that our proposed algorithm gives an\naccurate estimate and scales well with the input size. We provide error bounds\nand time and space costs, and conduct an extensive experimental evaluation of\nour algorithm, comparing its estimation accuracy to a few competitors,\nincluding some multi-pass algorithms. Our results show that given the same\nspace, the proposed algorithm has an order of magnitude less error for a large\nrange of similarity thresholds.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 18:18:18 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 18:48:12 GMT"}, {"version": "v3", "created": "Sat, 2 Feb 2019 02:07:48 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Rafiei", "Davood", ""], ["Deng", "Fan", ""]]}, {"id": "1806.03384", "submitter": "Mahmoud Mohammadi", "authors": "Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia,\n  Hongkyu Park, Youngmin Kim", "title": "Data Synthesis based on Generative Adversarial Networks", "comments": "Full Version - VLDB 2018", "journal-ref": null, "doi": "10.14778/3231751.3231757", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy is an important concern for our society where sharing data with\npartners or releasing data to the public is a frequent occurrence. Some of the\ntechniques that are being used to achieve privacy are to remove identifiers,\nalter quasi-identifiers, and perturb values. Unfortunately, these approaches\nsuffer from two limitations. First, it has been shown that private information\ncan still be leaked if attackers possess some background knowledge or other\ninformation sources. Second, they do not take into account the adverse impact\nthese methods will have on the utility of the released data. In this paper, we\npropose a method that meets both requirements. Our method, called table-GAN,\nuses generative adversarial networks (GANs) to synthesize fake tables that are\nstatistically similar to the original table yet do not incur information\nleakage. We show that the machine learning models trained using our synthetic\ntables exhibit performance that is similar to that of models trained using the\noriginal table for unknown testing cases. We call this property model\ncompatibility. We believe that anonymization/perturbation/synthesis methods\nwithout model compatibility are of little value. We used four real-world\ndatasets from four different domains for our experiments and conducted in-depth\ncomparisons with state-of-the-art anonymization, perturbation, and generation\ntechniques. Throughout our experiments, only our method consistently shows a\nbalance between privacy level and model compatibility.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 00:23:15 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 19:32:02 GMT"}, {"version": "v3", "created": "Sun, 17 Jun 2018 21:29:13 GMT"}, {"version": "v4", "created": "Tue, 19 Jun 2018 18:22:07 GMT"}, {"version": "v5", "created": "Mon, 2 Jul 2018 19:20:02 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Park", "Noseong", ""], ["Mohammadi", "Mahmoud", ""], ["Gorde", "Kshitij", ""], ["Jajodia", "Sushil", ""], ["Park", "Hongkyu", ""], ["Kim", "Youngmin", ""]]}, {"id": "1806.04004", "submitter": "Nicolas Fiorini", "authors": "Nicolas Fiorini, Kathi Canese, Rostyslav Bryzgunov, Ievgeniia\n  Radetska, Asta Gindulyte, Martin Latterner, Vadim Miller, Maxim Osipov,\n  Michael Kholodov, Grisha Starchenko, Evgeny Kireev, Zhiyong Lu", "title": "PubMed Labs: An experimental platform for improving biomedical\n  literature search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PubMed is a freely accessible system for searching the biomedical literature,\nwith approximately 2.5 million users worldwide on an average workday. We have\nrecently developed PubMed Labs (www.pubmed.gov/labs), an experimental platform\nfor users to test new features/tools and provide feedback, which enables us to\nmake more informed decisions about potential changes to improve the search\nquality and overall usability of PubMed. In doing so, we hope to better meet\nour user needs in an era of information overload. Another novel aspect of\nPubMed Labs lies in its mobile-first and responsive layout, which offers better\nsupport for accessing PubMed on the increasingly popular use of mobile and\nsmall-screen devices. Currently, PubMed Labs only includes a core subset of\nPubMed functionalities, e.g. search, facets. We encourage users to test PubMed\nLabs and share their experience with us, based on which we expect to\ncontinuously improve PubMed Labs with more advanced features and better user\nexperience.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 14:10:01 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Fiorini", "Nicolas", ""], ["Canese", "Kathi", ""], ["Bryzgunov", "Rostyslav", ""], ["Radetska", "Ievgeniia", ""], ["Gindulyte", "Asta", ""], ["Latterner", "Martin", ""], ["Miller", "Vadim", ""], ["Osipov", "Maxim", ""], ["Kholodov", "Michael", ""], ["Starchenko", "Grisha", ""], ["Kireev", "Evgeny", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1806.04226", "submitter": "Michael Anderson", "authors": "Michael R. Anderson, Michael Cafarella, German Ros, Thomas F. Wenisch", "title": "Physical Representation-based Predicate Optimization for a Visual\n  Analytics Database", "comments": "Camera-ready version of the paper submitted to ICDE 2019, In\n  Proceedings of the 35th IEEE International Conference on Data Engineering\n  (ICDE 2019)", "journal-ref": "Proceedings of the 35th IEEE International Conference on Data\n  Engineering (ICDE 2019), 1466-1477", "doi": "10.1109/ICDE.2019.00132", "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying the content of images, video, and other non-textual data sources\nrequires expensive content extraction methods. Modern extraction techniques are\nbased on deep convolutional neural networks (CNNs) and can classify objects\nwithin images with astounding accuracy. Unfortunately, these methods are slow:\nprocessing a single image can take about 10 milliseconds on modern GPU-based\nhardware. As massive video libraries become ubiquitous, running a content-based\nquery over millions of video frames is prohibitive.\n  One promising approach to reduce the runtime cost of queries of visual\ncontent is to use a hierarchical model, such as a cascade, where simple cases\nare handled by an inexpensive classifier. Prior work has sought to design\ncascades that optimize the computational cost of inference by, for example,\nusing smaller CNNs. However, we observe that there are critical factors besides\nthe inference time that dramatically impact the overall query time. Notably, by\ntreating the physical representation of the input image as part of our query\noptimization---that is, by including image transforms, such as resolution\nscaling or color-depth reduction, within the cascade---we can optimize data\nhandling costs and enable drastically more efficient classifier cascades.\n  In this paper, we propose Tahoma, which generates and evaluates many\npotential classifier cascades that jointly optimize the CNN architecture and\ninput data representation. Our experiments on a subset of ImageNet show that\nTahoma's input transformations speed up cascades by up to 35 times. We also\nfind up to a 98x speedup over the ResNet50 classifier with no loss in accuracy,\nand a 280x speedup if some accuracy is sacrificed.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 20:28:12 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 21:36:12 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 18:08:09 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Anderson", "Michael R.", ""], ["Cafarella", "Michael", ""], ["Ros", "German", ""], ["Wenisch", "Thomas F.", ""]]}, {"id": "1806.04467", "submitter": "Loic Baron", "authors": "Lo\\\"ic Baron (NPA, CNRS), Radomir Klacza (UPMC, NPA), Pauline\n  Gaudet-Chardonnet (NPA, UPMC), Amira Bradai (UPMC, NPA), Ciro Scognamiglio\n  (UPMC, NPA), Serge Fdida (NPA, LINCS)", "title": "Next generation portal for federated testbeds MySlice v2: from prototype\n  to production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of projects in computer science around the world have contributed to\nbuild federated experimental facilities providing access to a large set of\ndistributed compute, storage and network resources for the research community.\nSeveral tools have been developed to provide users an easy access to the\nfederated testbeds. This paper presents the architecture of the new version of\nthe MySlice web portal, that has evolved from a prototype to a production ready\nsoftware.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 12:39:00 GMT"}], "update_date": "2018-06-16", "authors_parsed": [["Baron", "Lo\u00efc", "", "NPA, CNRS"], ["Klacza", "Radomir", "", "UPMC, NPA"], ["Gaudet-Chardonnet", "Pauline", "", "NPA, UPMC"], ["Bradai", "Amira", "", "UPMC, NPA"], ["Scognamiglio", "Ciro", "", "UPMC, NPA"], ["Fdida", "Serge", "", "NPA, LINCS"]]}, {"id": "1806.04761", "submitter": "Ciprian-Octavian Truica", "authors": "Ciprian-Octavian Truic\\u{a} and Florin R\\u{a}dulescu and Alexandru\n  Boicea and Ion Bucur", "title": "Performance evaluation for CRUD operations in asynchronously replicated\n  document oriented database", "comments": null, "journal-ref": null, "doi": "10.1109/CSCS.2015.32", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NoSQL databases are becoming increasingly popular as more developers seek new\nways for storing information. The popularity of these databases has risen due\nto their flexibility and scalability needed in domains like Big Data and Cloud\nComputing. This paper examines asynchronous replication, one of the key\nfeatures for a scalable and flexible system. Three of the most popular\nDocument-Oriented Databases, MongoDB, CouchDB, and Couchbase, are examined. For\ntesting, the execution time for CRUD operations for a single database instance\nand for a distributed environment with two nodes is taken into account and the\nresults are compared with tests outcomes obtained for three relational database\nmanagement systems: Microsoft SQL Server, MySQL, and PostgreSQL.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 20:38:56 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Truic\u0103", "Ciprian-Octavian", ""], ["R\u0103dulescu", "Florin", ""], ["Boicea", "Alexandru", ""], ["Bucur", "Ion", ""]]}, {"id": "1806.04808", "submitter": "Guansong Pang", "authors": "Guansong Pang, Longbing Cao, Ling Chen, Huan Liu", "title": "Learning Representations of Ultrahigh-dimensional Data for Random\n  Distance-based Outlier Detection", "comments": "10 pages, 4 figures, 3 tables. To appear in the proceedings of KDD18,\n  Long presentation (oral)", "journal-ref": null, "doi": "10.1145/3219819.3220042", "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning expressive low-dimensional representations of ultrahigh-dimensional\ndata, e.g., data with thousands/millions of features, has been a major way to\nenable learning methods to address the curse of dimensionality. However,\nexisting unsupervised representation learning methods mainly focus on\npreserving the data regularity information and learning the representations\nindependently of subsequent outlier detection methods, which can result in\nsuboptimal and unstable performance of detecting irregularities (i.e.,\noutliers).\n  This paper introduces a ranking model-based framework, called RAMODO, to\naddress this issue. RAMODO unifies representation learning and outlier\ndetection to learn low-dimensional representations that are tailored for a\nstate-of-the-art outlier detection approach - the random distance-based\napproach. This customized learning yields more optimal and stable\nrepresentations for the targeted outlier detectors. Additionally, RAMODO can\nleverage little labeled data as prior knowledge to learn more expressive and\napplication-relevant representations. We instantiate RAMODO to an efficient\nmethod called REPEN to demonstrate the performance of RAMODO.\n  Extensive empirical results on eight real-world ultrahigh dimensional data\nsets show that REPEN (i) enables a random distance-based detector to obtain\nsignificantly better AUC performance and two orders of magnitude speedup; (ii)\nperforms substantially better and more stably than four state-of-the-art\nrepresentation learning methods; and (iii) leverages less than 1% labeled data\nto achieve up to 32% AUC improvement.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 00:53:56 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Pang", "Guansong", ""], ["Cao", "Longbing", ""], ["Chen", "Ling", ""], ["Liu", "Huan", ""]]}, {"id": "1806.04952", "submitter": "Markus Schr\\\"oder", "authors": "Markus Schr\\\"oder and Christian Jilek and J\\\"orn Hees and Andreas\n  Dengel", "title": "Towards Semantically Enhanced Data Understanding", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of machine learning, data understanding is the practice of\ngetting initial insights in unknown datasets. Such knowledge-intensive tasks\nrequire a lot of documentation, which is necessary for data scientists to grasp\nthe meaning of the data. Usually, documentation is separate from the data in\nvarious external documents, diagrams, spreadsheets and tools which causes\nconsiderable look up overhead. Moreover, other supporting applications are not\nable to consume and utilize such unstructured data. That is why we propose a\nmethodology that uses a single semantic model that interlinks data with its\ndocumentation. Hence, data scientists are able to directly look up the\nconnected information about the data by simply following links. Equally, they\ncan browse the documentation which always refers to the data. Furthermore, the\nmodel can be used by other approaches providing additional support, like\nsearching, comparing, integrating or visualizing data. To showcase our approach\nwe also demonstrate an early prototype.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 11:19:33 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Schr\u00f6der", "Markus", ""], ["Jilek", "Christian", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "1806.04968", "submitter": "Chengliang Chai", "authors": "Chengliang Chai, Ju Fan, Guoliang Li, Jiannan Wang, Yudian Zheng", "title": "Crowd-Powered Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data mining tasks cannot be completely addressed by auto- mated\nprocesses, such as sentiment analysis and image classification. Crowdsourcing\nis an effective way to harness the human cognitive ability to process these\nmachine-hard tasks. Thanks to public crowdsourcing platforms, e.g., Amazon\nMechanical Turk and Crowd- Flower, we can easily involve hundreds of thousands\nof ordinary workers (i.e., the crowd) to address these machine-hard tasks. In\nthis tutorial, we will survey and synthesize a wide spectrum of existing\nstudies on crowd-powered data mining. We first give an overview of\ncrowdsourcing, and then summarize the fundamental techniques, including quality\ncontrol, cost control, and latency control, which must be considered in\ncrowdsourced data mining. Next we review crowd-powered data mining operations,\nincluding classification, clustering, pattern mining, machine learning using\nthe crowd (including deep learning, transfer learning and semi-supervised\nlearning) and knowledge discovery. Finally, we provide the emerging challenges\nin crowdsourced data mining.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 12:04:20 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 02:31:26 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Chai", "Chengliang", ""], ["Fan", "Ju", ""], ["Li", "Guoliang", ""], ["Wang", "Jiannan", ""], ["Zheng", "Yudian", ""]]}, {"id": "1806.04973", "submitter": "Michael Bommarito II", "authors": "Michael J Bommarito II and Daniel Martin Katz and Eric M Detterman", "title": "OpenEDGAR: Open Source Software for SEC EDGAR Analysis", "comments": "12 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenEDGAR is an open source Python framework designed to rapidly construct\nresearch databases based on the Electronic Data Gathering, Analysis, and\nRetrieval (EDGAR) system operated by the US Securities and Exchange Commission\n(SEC). OpenEDGAR is built on the Django application framework, supports\ndistributed compute across one or more servers, and includes functionality to\n(i) retrieve and parse index and filing data from EDGAR, (ii) build tables for\nkey metadata like form type and filer, (iii) retrieve, parse, and update CIK to\nticker and industry mappings, (iv) extract content and metadata from filing\ndocuments, and (v) search filing document contents. OpenEDGAR is designed for\nuse in both academic research and industrial applications, and is distributed\nunder MIT License at https://github.com/LexPredict/openedgar.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 12:16:37 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Bommarito", "Michael J", "II"], ["Katz", "Daniel Martin", ""], ["Detterman", "Eric M", ""]]}, {"id": "1806.05918", "submitter": "Roman Kontchakov", "authors": "Guohui Xiao, Roman Kontchakov, Benjamin Cogrel, Diego Calvanese, Elena\n  Botoeva", "title": "Efficient Handling of SPARQL OPTIONAL for OBDA (Extended Version)", "comments": "technical report for ISWC 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OPTIONAL is a key feature in SPARQL for dealing with missing information.\nWhile this operator is used extensively, it is also known for its complexity,\nwhich can make efficient evaluation of queries with OPTIONAL challenging. We\ntackle this problem in the Ontology-Based Data Access (OBDA) setting, where the\ndata is stored in a SQL relational database and exposed as a virtual RDF graph\nby means of an R2RML mapping. We start with a succinct translation of a SPARQL\nfragment into SQL. It fully respects bag semantics and three-valued logic and\nrelies on the extensive use of the LEFT JOIN operator and COALESCE function. We\nthen propose optimisation techniques for reducing the size and improving the\nstructure of generated SQL queries. Our optimisations capture interactions\nbetween JOIN, LEFT JOIN, COALESCE and integrity constraints such as attribute\nnullability, uniqueness and foreign key constraints. Finally, we empirically\nverify effectiveness of our techniques on the BSBM OBDA benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 11:48:39 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 06:16:19 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Xiao", "Guohui", ""], ["Kontchakov", "Roman", ""], ["Cogrel", "Benjamin", ""], ["Calvanese", "Diego", ""], ["Botoeva", "Elena", ""]]}, {"id": "1806.05983", "submitter": "Yuan Liang", "authors": "Yuan Liang", "title": "Online Variant of Parcel Allocation in Last-mile Delivery", "comments": "15 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the problem of last-mile delivery, where a large pool of\ncitizen crowd-workers are hired to perform a variety of location-specific urban\nlogistics parcel delivering tasks. Current approaches focus on offline\nscenarios, where all the spatio temporal information of parcels and workers are\ngiven. However, the offline scenarios can be impractical since parcels and\nworkers appear dynamically in real applications, and their information is\nunknown in advance. In this paper, in order to solve the shortcomings of the\noffline setting, we first formalize the online parcel allocation in last-mile\ndelivery problem, where all parcels were put in pop-stations in advance, while\nworkers arrive dynamically. Then we propose an algorithm which provides\ntheoretical guarantee for the parcel allocation in last-mile delivery. Finally,\nwe verify the effectiveness and efficiency of the proposed method through\nextensive experiments on real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:25:28 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 03:01:15 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 16:03:41 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Liang", "Yuan", ""]]}, {"id": "1806.06151", "submitter": "Mahawaga Arachchige Pathum Chamikara", "authors": "M.A.P. Chamikara, P. Bertok, D. Liu, S. Camtepe, I. Khalil", "title": "Efficient Data Perturbation for Privacy Preserving and Accurate Data\n  Stream Mining", "comments": "Pervasive and Mobile Computing 2018", "journal-ref": null, "doi": "10.1016/j.pmcj.2018.05.003", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The widespread use of the Internet of Things (IoT) has raised many concerns,\nincluding the protection of private information. Existing privacy preservation\nmethods cannot provide a good balance between data utility and privacy, and\nalso have problems with efficiency and scalability. This paper proposes an\nefficient data stream perturbation method (named as $P^2RoCAl$). $P^2RoCAl$\noffers better data utility than similar methods: classification accuracies of\n$P^2RoCAl$ perturbed data streams are very close to those of the original data\nstreams. $P^2RoCAl$ also provides higher resilience against data reconstruction\nattacks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 23:51:52 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 09:59:49 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Chamikara", "M. A. P.", ""], ["Bertok", "P.", ""], ["Liu", "D.", ""], ["Camtepe", "S.", ""], ["Khalil", "I.", ""]]}, {"id": "1806.06205", "submitter": "Xiaowang Zhang", "authors": "Lijing Zhang and Xiaowang Zhang and Zhiyong Feng", "title": "TrQuery: An Embedding-based Framework for Recommanding SPARQL Queries", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an embedding-based framework (TrQuery) for\nrecommending solutions of a SPARQL query, including approximate solutions when\nexact querying solutions are not available due to incompleteness or\ninconsistencies of real-world RDF data. Within this framework, embedding is\napplied to score solutions together with edit distance so that we could obtain\nmore fine-grained recommendations than those recommendations via edit distance.\nFor instance, graphs of two querying solutions with a similar structure can be\ndistinguished in our proposed framework while the edit distance depending on\nstructural difference becomes unable. To this end, we propose a novel score\nmodel built on vector space generated in embedding system to compute the\nsimilarity between an approximate subgraph matching and a whole graph matching.\nFinally, we evaluate our approach on large RDF datasets DBpedia and YAGO, and\nexperimental results show that TrQuery exhibits an excellent behavior in terms\nof both effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:17:08 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Zhang", "Lijing", ""], ["Zhang", "Xiaowang", ""], ["Feng", "Zhiyong", ""]]}, {"id": "1806.06452", "submitter": "Dai-Hai Ton That", "authors": "Zhihao Yuan, Dai Hai Ton That, Siddhant Kothari, Gabriel Fils, Tanu\n  Malik", "title": "Utilizing Provenance in Reusable Research Objects", "comments": "25 pages", "journal-ref": "Informatics 2018, 5(1), 14", "doi": "10.3390/informatics5010014", "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Science is conducted collaboratively, often requiring the sharing of\nknowledge about computational experiments. When experiments include only\ndatasets, they can be shared using Uniform Resource Identifiers (URIs) or\nDigital Object Identifiers (DOIs). An experiment, however, seldom includes only\ndatasets, but more often includes software, its past execution, provenance, and\nassociated documentation. The Research Object has recently emerged as a\ncomprehensive and systematic method for aggregation and identification of\ndiverse elements of computational experiments. While a necessary method, mere\naggregation is not sufficient for the sharing of computational experiments.\nOther users must be able to easily recompute on these shared research objects.\nComputational provenance is often the key to enable such reuse. In this paper,\nwe show how reusable research objects can utilize provenance to correctly\nrepeat a previous reference execution, to construct a subset of a research\nobject for partial reuse, and to reuse existing contents of a research object\nfor modified reuse. We describe two methods to summarize provenance that aid in\nunderstanding the contents and past executions of a research object. The first\nmethod obtains a process-view by collapsing low-level system information, and\nthe second method obtains a summary graph by grouping related nodes and edges\nwith the goal to obtain a graph view similar to application workflow. Through\ndetailed experiments, we show the efficacy and efficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 21:47:58 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Yuan", "Zhihao", ""], ["That", "Dai Hai Ton", ""], ["Kothari", "Siddhant", ""], ["Fils", "Gabriel", ""], ["Malik", "Tanu", ""]]}, {"id": "1806.06978", "submitter": "Isaac Sheff", "authors": "Isaac Sheff, Xinwen Wang, Andrew C. Myers, Robbert van Renesse", "title": "A Web of Blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchains offer a useful abstraction: a trustworthy, decentralized log of\ntotally ordered transactions. Traditional blockchains have problems with\nscalability and efficiency, preventing their use for many applications. These\nlimitations arise from the requirement that all participants agree on the total\nordering of transactions. To address this fundamental shortcoming, we introduce\nCharlotte, a system for maintaining decentralized, authenticated data\nstructures, including transaction logs. Each data structurestructure -- indeed,\neach block -- specifies its own availability and integrity properties, allowing\nCharlotte applications to retain the full benefits of permissioned or\npermissionless blockchains. In Charlotte, a block can be atomically appended to\nmultiple logs, allowing applications to be interoperable when they want to,\nwithout inefficiently forcing all applications to share one big log. We call\nthis open graph of interconnected blocks a blockweb. We allow new kinds of\nblockweb applications that operate beyond traditional chains. We demonstrate\nthe viability of Charlotte applications with proof-of-concept servers running\ninteroperable blockchains. Using performance data from our prototype, we\nestimate that when compared with traditional blockchains, Charlotte offers\nmultiple orders of magnitude improvement in speed and energy efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 23:01:41 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Sheff", "Isaac", ""], ["Wang", "Xinwen", ""], ["Myers", "Andrew C.", ""], ["van Renesse", "Robbert", ""]]}, {"id": "1806.07084", "submitter": "Kong Hyeok", "authors": "Hyeok Kong, Dokjun An, Jihyang Ri", "title": "Itemsets of interest for negative association rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far, most of association rule minings have considered about positive\nassociation rules based on frequent itemsets in databases[2,5-7], but they have\nnot considered the problem of mining negative association rules correlated with\nfrequent and infrequent itemsets. Negative association rule mining is much more\ndifficult than positive association rule mining because it needs infrequent\nitemsets, and only the rare association rule mining which is a kind of negative\nassociation rule minings has been studied. This paper presents a mathematical\nmodel to mine positive and negative association rules precisely, for which in a\npoint of view that negation of a frequent itemset is an infrequent itemset, we\nmake clear the importance of the problem of mining negative association rules\nbased on certain infrequent itemsets and study on what conditions infrequent\nitemsets of interest should satisfy for negative association rules.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 07:50:48 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Kong", "Hyeok", ""], ["An", "Dokjun", ""], ["Ri", "Jihyang", ""]]}, {"id": "1806.07344", "submitter": "G\\'abor Sz\\'arnyas", "authors": "G\\'abor Sz\\'arnyas, J\\'ozsef Marton, J\\'anos Maginecz, D\\'aniel\n  Varr\\'o", "title": "Reducing Property Graph Queries to Relational Algebra for Incremental\n  View Maintenance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The property graph data model of modern graph database systems is\nincreasingly adapted for storing and processing heterogeneous datasets like\nnetworks. Many challenging applications with near real-time requirements --\ne.g. financial fraud detection, recommendation systems, and on-the-fly\nvalidation -- can be captured with graph queries, which are evaluated\nrepeatedly. To ensure quick response time for a changing data set, these\napplications would benefit from applying incremental view maintenance (IVM)\ntechniques, which can perform continuous evaluation of queries and calculate\nthe changes in the result set upon updates. However, currently, no graph\ndatabases provide support for incremental views. While IVM problems have been\nstudied extensively over relational databases, views on property graph queries\nrequire operators outside the scope of standard relational algebra. Hence,\ntackling this problem requires the integration of numerous existing IVM\ntechniques and possibly further extensions. In this paper, we present an\napproach to perform IVM on property graphs, using a nested relational algebraic\nrepresentation for property graphs and graph operations. Then we define a chain\nof transformations to reduce most property graph queries to flat relational\nalgebra and use techniques from discrimination networks (used in rule-based\nexpert systems) to evaluate them. We demonstrate the approach using our\nprototype tool, ingraph, which uses openCypher, an open graph query language\nspecified as part of an industry initiative. However, several aspects of our\napproach can be generalised to other graph query languages such as G-CORE and\nPGQL.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 16:51:31 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Sz\u00e1rnyas", "G\u00e1bor", ""], ["Marton", "J\u00f3zsef", ""], ["Maginecz", "J\u00e1nos", ""], ["Varr\u00f3", "D\u00e1niel", ""]]}, {"id": "1806.07524", "submitter": "Yichen Hu Mr", "authors": "Yichen Hu, Qing Wang, Peter Christen", "title": "Developing a Temporal Bibliographic Data Set for Entity Resolution", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution is the process of identifying groups of records within or\nacross data sets where each group represents a real-world entity. Novel\ntechniques that consider temporal features to improve the quality of entity\nresolution have recently attracted significant attention. However, there are\ncurrently no large data sets available that contain both temporal information\nas well as ground truth information to evaluate the quality of temporal entity\nresolution approaches. In this paper, we describe the preparation of a temporal\ndata set based on author profiles extracted from the Digital Bibliography and\nLibrary Project (DBLP). We completed missing links between publications and\nauthor profiles in the DBLP data set using the DBLP public API. We then used\nthe Microsoft Academic Graph (MAG) to link temporal affiliation information for\nDBLP authors. We selected around 80K (1%) of author profiles that cover 2\nmillion (50%) publications using information in DBLP such as alternative author\nnames and personal web profile to improve the reliability of the resulting\nground truth, while at the same time keeping the data set challenging for\ntemporal entity resolution research.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 02:14:09 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Hu", "Yichen", ""], ["Wang", "Qing", ""], ["Christen", "Peter", ""]]}, {"id": "1806.07598", "submitter": "Kasper Green Larsen", "authors": "Shunhua Jiang, Kasper Green Larsen", "title": "A Faster External Memory Priority Queue with DecreaseKeys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A priority queue is a fundamental data structure that maintains a dynamic set\nof (key, priority)-pairs and supports Insert, Delete, ExtractMin and\nDecreaseKey operations. In the external memory model, the current best priority\nqueue supports each operation in amortized $O(\\frac{1}{B}\\log \\frac{N}{B})$\nI/Os. If the DecreaseKey operation does not need to be supported, one can\ndesign a more efficient data structure that supports the Insert, Delete and\nExtractMin operations in $O(\\frac{1}{B}\\log \\frac{N}{B}/ \\log \\frac{M}{B})$\nI/Os. A recent result shows that a degradation in performance is inevitable by\nproving a lower bound of $\\Omega(\\frac{1}{B}\\log B/\\log\\log N)$ I/Os for\npriority queues with DecreaseKeys. In this paper we tighten the gap between the\nlower bound and the upper bound by proposing a new priority queue which\nsupports the DecreaseKey operation and has an expected amortized I/O complexity\nof $O(\\frac{1}{B}\\log \\frac{N}{B}/\\log\\log N)$. Our result improves the\nexternal memory priority queue with DecreaseKeys for the first time in over a\ndecade, and also gives the fastest external memory single source shortest path\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 07:57:10 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Jiang", "Shunhua", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1806.07691", "submitter": "Kong Hyeok", "authors": "Hyeok Kong, Dokjun An, Douk Han", "title": "Searching of interesting itemsets for negative association rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an algorithm of searching for both positive and\nnegative itemsets of interest which should be given at the first stage for\npositive and negative association rules mining. Traditional association rule\nmining algorithms extract positive association rules based on frequent\nitemsets, for which the frequent itemsets, i.e. only positive itemsets of\ninterest are searched. Further, there are useful itemsets among the frequent\nitemsets pruned from the traditional algorithms to reduce the search space, for\nmining of negative association rules. Therefore, the traditional algorithms\nhave not come true to find negative itemsets needed in mining of negative\nassociation rules. Our new algorithm to search for both positive and negative\nitemsets of interest prepares preconditions for mining of all positive and\nnegative association rules.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 12:24:20 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Kong", "Hyeok", ""], ["An", "Dokjun", ""], ["Han", "Douk", ""]]}, {"id": "1806.07728", "submitter": "Shigeyuki Sato", "authors": "Shigeyuki Sato, Wei Hao, and Kiminori Matsuzaki", "title": "Parallelization of XPath Queries using Modern XQuery Processors", "comments": "This is the full version of our publication to appear at ADBIS 2018\n  as a short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical and promising approach to parallelizing XPath queries was\nproposed by Bordawekar et al. in 2009, which enables parallelization on top of\nexisting XML database engines. Although they experimentally demonstrated the\nspeedup by their approach, their practice has already been out of date because\nthe software environment has largely changed with the capability of XQuery\nprocessing. In this work, we implement their approach in two ways on top of a\nstate-of-the-art XML database engine and experimentally demonstrate that our\nimplementations can bring significant speedup on a commodity server.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 13:45:24 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Sato", "Shigeyuki", ""], ["Hao", "Wei", ""], ["Matsuzaki", "Kiminori", ""]]}, {"id": "1806.08182", "submitter": "Frederik Mallmann-Trenn", "authors": "Vincent Cohen-Addad, Frederik Mallmann-Trenn, Claire Mathieu", "title": "Instance-Optimality in the Noisy Value-and Comparison-Model --- Accept,\n  Accept, Strong Accept: Which Papers get in?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by crowdsourced computation, peer-grading, and recommendation\nsystems, Braverman, Mao and Weinberg [STOC'16] studied the \\emph{query} and\n\\emph{round} complexity of fundamental problems such as finding the maximum\n(\\textsc{max}), finding all elements above a certain value\n(\\textsc{threshold-$v$}) or computing the top$-k$ elements (\\textsc{Top}-$k$)\nin a noisy environment.\n  For example, consider the task of selecting papers for a conference. This\ntask is challenging due the crowdsourcing nature of peer reviews: the results\nof reviews are noisy and it is necessary to parallelize the review process as\nmuch as possible. We study the noisy value model and the noisy comparison\nmodel: In the \\emph{noisy value model}, a reviewer is asked to evaluate a\nsingle element: \"What is the value of paper $i$?\" (\\eg accept). In the\n\\emph{noisy comparison model} (introduced in the seminal work of Feige, Peleg,\nRaghavan and Upfal [SICOMP'94]) a reviewer is asked to do a pairwise\ncomparison: \"Is paper $i$ better than paper $j$?\"\n  In this paper, we show optimal worst-case query complexity for the\n\\textsc{max},\\textsc{threshold-$v$} and \\textsc{Top}-$k$ problems. For\n\\textsc{max} and \\textsc{Top}-$k$, we obtain optimal worst-case upper and lower\nbounds on the round vs query complexity in both models. For\n\\textsc{threshold}-$v$, we obtain optimal query complexity and nearly-optimal\nround complexity, where $k$ is the size of the output) for both models.\n  We then go beyond the worst-case and address the question of the importance\nof knowledge of the instance by providing, for a large range of parameters,\ninstance-optimal algorithms with respect to the query complexity. Furthermore,\nwe show that the value model is strictly easier than the comparison model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 11:50:39 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 17:35:16 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Mallmann-Trenn", "Frederik", ""], ["Mathieu", "Claire", ""]]}, {"id": "1806.08245", "submitter": "Ching Tarn", "authors": "Ching Tarn, Yinan Zhang, Ye Feng", "title": "Reductive Clustering: An Efficient Linear-time Graph-based Divisive\n  Cluster Analysis Approach", "comments": "http://res.ctarn.io/reductive-clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient linear-time graph-based divisive cluster analysis\napproach called Reductive Clustering. The approach tries to reveal the\nhierarchical structural information through reducing the graph into a more\nconcise one repeatedly. With the reductions, the original graph can be divided\ninto subgraphs recursively, and a lite informative dendrogram is constructed\nbased on the divisions. The reduction consists of three steps: selection,\nconnection, and partition. First a subset of vertices of the graph are selected\nas representatives to build a concise graph. The representatives are\nre-connected to maintain a consistent structure with the previous graph. If\npossible, the concise graph is divided into subgraphs, and each subgraph is\nfurther reduced recursively until the termination condition is met. We discuss\nthe approach, along with several selection and connection methods, in detail\nboth theoretically and experimentally in this paper. Our implementations run in\nlinear time and achieve outstanding performance on various types of datasets.\nExperimental results show that they outperform state-of-the-art clustering\nalgorithms with significantly less computing resource requirements.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 13:44:17 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 06:40:56 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2020 12:20:22 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Tarn", "Ching", ""], ["Zhang", "Yinan", ""], ["Feng", "Ye", ""]]}, {"id": "1806.08384", "submitter": "Jun Hyung Shin", "authors": "Jun Hyung Shin", "title": "Novel Selectivity Estimation Strategy for Modern DBMS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selectivity estimation is important in query optimization, however accurate\nestimation is difficult when predicates are complex. Instead of existing\ndatabase synopses and statistics not helpful for such cases, we introduce a new\napproach to compute the exact selectivity by running an aggregate query during\nthe optimization phase. Exact selectivity can be achieved without significant\noverhead for in-memory and GPU-accelerated databases by adding extra query\nexecution calls. We implement a selection push-down extension based on the\nnovel selectivity estimation strategy in the MapD database system. Our approach\nrecords constant and less than 30 millisecond overheads in any circumstances\nwhile running on GPU. The novel strategy successfully generates better query\nexecution plans which result in performance improvement up to 4.8 times from\nTPC-H benchmark SF-50 queries and 7.3 times from star schema benchmark SF-80\nqueries.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 18:34:03 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Shin", "Jun Hyung", ""]]}, {"id": "1806.08658", "submitter": "Behrooz Razeghi", "authors": "Behrooz Razeghi, Slava Voloshynovskiy, Sohrab Ferdowsi and Dimche\n  Kostadinov", "title": "Privacy-Preserving Identification via Layered Sparse Code Design:\n  Distributed Servers and Multiple Access Authorization", "comments": "EUSIPCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB cs.DC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new computationally efficient privacy-preserving identification\nframework based on layered sparse coding. The key idea of the proposed\nframework is a sparsifying transform learning with ambiguization, which\nconsists of a trained linear map, a component-wise nonlinearity and a privacy\namplification. We introduce a practical identification framework, which\nconsists of two phases: public and private identification. The public untrusted\nserver provides the fast search service based on the sparse privacy protected\ncodebook stored at its side. The private trusted server or the local client\napplication performs the refined accurate similarity search using the results\nof the public search and the layered sparse codebooks stored at its side. The\nprivate search is performed in the decoded domain and also the accuracy of\nprivate search is chosen based on the authorization level of the client. The\nefficiency of the proposed method is in computational complexity of encoding,\ndecoding, \"encryption\" (ambiguization) and \"decryption\" (purification) as well\nas storage complexity of the codebooks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 18:06:55 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Razeghi", "Behrooz", ""], ["Voloshynovskiy", "Slava", ""], ["Ferdowsi", "Sohrab", ""], ["Kostadinov", "Dimche", ""]]}, {"id": "1806.09029", "submitter": "Jonathan K Kummerfeld", "authors": "Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik\n  Ramanathan, Sesh Sadasivam, Rui Zhang, Dragomir Radev", "title": "Improving Text-to-SQL Evaluation Methodology", "comments": "To appear at ACL 2018", "journal-ref": "ACL (2018) 351-360", "doi": "10.18653/v1/P18-1033", "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To be informative, an evaluation must measure how well systems generalize to\nrealistic unseen data. We identify limitations of and propose improvements to\ncurrent evaluations of text-to-SQL systems. First, we compare human-generated\nand automatically generated questions, characterizing properties of queries\nnecessary for real-world applications. To facilitate evaluation on multiple\ndatasets, we release standardized and improved versions of seven existing\ndatasets and one new text-to-SQL dataset. Second, we show that the current\ndivision of data into training and test sets measures robustness to variations\nin the way questions are asked, but only partially tests how well systems\ngeneralize to new queries; therefore, we propose a complementary dataset split\nfor evaluation of future work. Finally, we demonstrate how the common practice\nof anonymizing variables during evaluation removes an important challenge of\nthe task. Our observations highlight key difficulties, and our methodology\nenables effective measurement of future development.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 20:02:55 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Finegan-Dollak", "Catherine", ""], ["Kummerfeld", "Jonathan K.", ""], ["Zhang", "Li", ""], ["Ramanathan", "Karthik", ""], ["Sadasivam", "Sesh", ""], ["Zhang", "Rui", ""], ["Radev", "Dragomir", ""]]}, {"id": "1806.09256", "submitter": "Cagatay Demiralp", "authors": "Marco Cavallo and \\c{C}a\\u{g}atay Demiralp", "title": "Track Xplorer: A System for Visual Analysis of Sensor-based Motor\n  Activity Predictions", "comments": "EuroVis'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid commoditization of wearable sensors, detecting human movements\nfrom sensor datasets has become increasingly common over a wide range of\napplications. To detect activities, data scientists iteratively experiment with\ndifferent classifiers before deciding which model to deploy. Effective\nreasoning about and comparison of alternative classifiers are crucial in\nsuccessful model development. This is, however, inherently difficult in\ndeveloping classifiers for sensor data, where the intricacy of long temporal\nsequences, high prediction frequency, and imprecise labeling make standard\nevaluation methods relatively ineffective and even misleading. We introduce\nTrack Xplorer, an interactive visualization system to query, analyze, and\ncompare the predictions of sensor-data classifiers. Track Xplorer enables users\nto interactively explore and compare the results of different classifiers, and\nassess their accuracy with respect to the ground-truth labels and video.\nThrough integration with a version control system, Track Xplorer supports\ntracking of models and their parameters without additional workload on model\ndevelopers. Track Xplorer also contributes an extensible algebra over track\nrepresentations to filter, compose, and compare classification outputs,\nenabling users to reason effectively about classifier performance. We apply\nTrack Xplorer in a collaborative project to develop classifiers to detect\nmovements from multisensor data gathered from Parkinson's disease patients. We\ndemonstrate how Track Xplorer helps identify early on possible systemic data\nerrors, effectively track and compare the results of different classifiers, and\nreason about and pinpoint the causes of misclassifications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 02:19:24 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Cavallo", "Marco", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1806.09339", "submitter": "Peng Gao", "authors": "Peng Gao, Xusheng Xiao, Ding Li, Zhichun Li, Kangkook Jee, Zhenyu Wu,\n  Chung Hwan Kim, Sanjeev R. Kulkarni, Prateek Mittal", "title": "SAQL: A Stream-based Query System for Real-Time Abnormal System Behavior\n  Detection", "comments": "Accepted paper at USENIX Security Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, advanced cyber attacks, which consist of a sequence of steps that\ninvolve many vulnerabilities and hosts, compromise the security of many\nwell-protected businesses. This has led to the solutions that ubiquitously\nmonitor system activities in each host (big data) as a series of events, and\nsearch for anomalies (abnormal behaviors) for triaging risky events. Since\nfighting against these attacks is a time-critical mission to prevent further\ndamage, these solutions face challenges in incorporating expert knowledge to\nperform timely anomaly detection over the large-scale provenance data.\n  To address these challenges, we propose a novel stream-based query system\nthat takes as input, a real-time event feed aggregated from multiple hosts in\nan enterprise, and provides an anomaly query engine that queries the event feed\nto identify abnormal behaviors based on the specified anomalies. To facilitate\nthe task of expressing anomalies based on expert knowledge, our system provides\na domain-specific query language, SAQL, which allows analysts to express models\nfor (1) rule-based anomalies, (2) time-series anomalies, (3) invariant-based\nanomalies, and (4) outlier-based anomalies. We deployed our system in NEC Labs\nAmerica comprising 150 hosts and evaluated it using 1.1TB of real system\nmonitoring data (containing 3.3 billion events). Our evaluations on a broad set\nof attack behaviors and micro-benchmarks show that our system has a low\ndetection latency (<2s) and a high system throughput (110,000 events/s;\nsupporting ~4000 hosts), and is more efficient in memory utilization than the\nexisting stream-based complex event processing systems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 09:15:11 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Gao", "Peng", ""], ["Xiao", "Xusheng", ""], ["Li", "Ding", ""], ["Li", "Zhichun", ""], ["Jee", "Kangkook", ""], ["Wu", "Zhenyu", ""], ["Kim", "Chung Hwan", ""], ["Kulkarni", "Sanjeev R.", ""], ["Mittal", "Prateek", ""]]}, {"id": "1806.09447", "submitter": "Giulio Ermanno Pibiri", "authors": "Giulio Ermanno Pibiri, Rossano Venturini", "title": "Handling Massive N-Gram Datasets Efficiently", "comments": "Published in ACM Transactions on Information Systems (TOIS), February\n  2019, Article No: 25", "journal-ref": null, "doi": "10.1145/3302913", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the two fundamental problems concerning the handling of\nlarge n-gram language models: indexing, that is compressing the n-gram strings\nand associated satellite data without compromising their retrieval speed; and\nestimation, that is computing the probability distribution of the strings from\na large textual source. Regarding the problem of indexing, we describe\ncompressed, exact and lossless data structures that achieve, at the same time,\nhigh space reductions and no time degradation with respect to state-of-the-art\nsolutions and related software packages. In particular, we present a compressed\ntrie data structure in which each word following a context of fixed length k,\ni.e., its preceding k words, is encoded as an integer whose value is\nproportional to the number of words that follow such context. Since the number\nof words following a given context is typically very small in natural\nlanguages, we lower the space of representation to compression levels that were\nnever achieved before. Despite the significant savings in space, our technique\nintroduces a negligible penalty at query time. Regarding the problem of\nestimation, we present a novel algorithm for estimating modified Kneser-Ney\nlanguage models, that have emerged as the de-facto choice for language modeling\nin both academia and industry, thanks to their relatively low perplexity\nperformance. Estimating such models from large textual sources poses the\nchallenge of devising algorithms that make a parsimonious use of the disk. The\nstate-of-the-art algorithm uses three sorting steps in external memory: we show\nan improved construction that requires only one sorting step thanks to\nexploiting the properties of the extracted n-gram strings. With an extensive\nexperimental analysis performed on billions of n-grams, we show an average\nimprovement of 4.5X on the total running time of the state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:23:12 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 09:20:53 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Pibiri", "Giulio Ermanno", ""], ["Venturini", "Rossano", ""]]}, {"id": "1806.09793", "submitter": "Khuong Vo An", "authors": "Khanh Dang, Khuong Vo and Josef K\\\"ung", "title": "A NoSQL Data-based Personalized Recommendation System for C2C e-Commerce", "comments": "Accepted to DEXA 2017", "journal-ref": null, "doi": "10.1007/978-3-319-64471-4_25", "report-no": null, "categories": "cs.IR cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the considerable development of customer-to-customer (C2C) e-commerce in\nthe recent years, there is a big demand for an effective recommendation system\nthat suggests suitable websites for users to sell their items with some\nspecified needs. Nonetheless, e-commerce recommendation systems are mostly\ndesigned for business-to-customer (B2C) websites, where the systems offer the\nconsumers the products that they might like to buy. Almost none of the related\nresearch works focus on choosing selling sites for target items. In this paper,\nwe introduce an approach that recommends the selling websites based upon the\nitem's description, category, and desired selling price. This approach employs\nNoSQL data-based machine learning techniques for building and training topic\nmodels and classification models. The trained models can then be used to rank\nthe websites dynamically with respect to the user needs. The experimental\nresults with real-world datasets from Vietnam C2C websites will demonstrate the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 05:02:30 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Dang", "Khanh", ""], ["Vo", "Khuong", ""], ["K\u00fcng", "Josef", ""]]}, {"id": "1806.09823", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Piotr Indyk, Ilya Razenshteyn", "title": "Approximate Nearest Neighbor Search in High Dimensions", "comments": "27 pages, no figures; to appear in the proceedings of ICM 2018\n  (accompanying the talk by P. Indyk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nearest neighbor problem is defined as follows: Given a set $P$ of $n$\npoints in some metric space $(X,D)$, build a data structure that, given any\npoint $q$, returns a point in $P$ that is closest to $q$ (its \"nearest\nneighbor\" in $P$). The data structure stores additional information about the\nset $P$, which is then used to find the nearest neighbor without computing all\ndistances between $q$ and $P$. The problem has a wide range of applications in\nmachine learning, computer vision, databases and other fields.\n  To reduce the time needed to find nearest neighbors and the amount of memory\nused by the data structure, one can formulate the {\\em approximate} nearest\nneighbor problem, where the the goal is to return any point $p' \\in P$ such\nthat the distance from $q$ to $p'$ is at most $c \\cdot \\min_{p \\in P} D(q,p)$,\nfor some $c \\geq 1$. Over the last two decades, many efficient solutions to\nthis problem were developed. In this article we survey these developments, as\nwell as their connections to questions in geometric functional analysis and\ncombinatorial geometry.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 07:35:45 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Andoni", "Alexandr", ""], ["Indyk", "Piotr", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1806.09967", "submitter": "Eric Leclercq", "authors": "Eric Leclercq, Marinette Savonnet", "title": "A Tensor Based Data Model for Polystore: An Application to Social\n  Networks Data", "comments": null, "journal-ref": "IDEAS 2018 22nd International Database Engineering \\& Applications\n  Symposium}{June 18--20, 2018}{Villa San Giovanni, Italy", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we show how the mathematical object tensor can be used to\nbuild a multi-paradigm model for the storage of social data in data warehouses.\nFrom an architectural point of view, our approach allows to link different\nstorage systems (polystore) and limits the impact of ETL tools performing model\ntransformations required to feed different analysis algorithms. Therefore,\nsystems can take advantage of multiple data models both in terms of query\nexecution performance and the semantic expressiveness of data representation.\nThe proposed model allows to reach the logical independence between data and\nprograms implementing analysis algorithms. With a concrete case study on\nmessage virality on Twitter during the French presidential election of 2017, we\nhighlight some of the contributions of our model.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 13:33:20 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Leclercq", "Eric", ""], ["Savonnet", "Marinette", ""]]}, {"id": "1806.10078", "submitter": "Maarten Van den Heuvel", "authors": "Maarten Van den Heuvel, Floris Geerts, Wolfgang Gatterbauer and Martin\n  Theobald", "title": "A General Framework for Anytime Approximation in Probabilistic Databases", "comments": "3 pages, 2 figures, submitted to StarAI 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anytime approximation algorithms that compute the probabilities of queries\nover probabilistic databases can be of great use to statistical learning tasks.\nThose approaches have been based so far on either (i) sampling or (ii)\nbranch-and-bound with model-based bounds. We present here a more general\nbranch-and-bound framework that extends the possible bounds by using\n'dissociation', which yields tighter bounds.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:55:37 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 09:46:41 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Heuvel", "Maarten Van den", ""], ["Geerts", "Floris", ""], ["Gatterbauer", "Wolfgang", ""], ["Theobald", "Martin", ""]]}, {"id": "1806.10478", "submitter": "Tommaso Soru", "authors": "Tommaso Soru, Edgard Marx, Andr\\'e Valdestilhas, Diego Esteves, Diego\n  Moussallem, Gustavo Publio", "title": "Neural Machine Translation for Query Construction and Composition", "comments": "ICML workshop on Neural Abstract Machines & Program Induction v2\n  (NAMPI), extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on question answering with knowledge base has recently seen an\nincreasing use of deep architectures. In this extended abstract, we study the\napplication of the neural machine translation paradigm for question parsing. We\nemploy a sequence-to-sequence model to learn graph patterns in the SPARQL graph\nquery language and their compositions. Instead of inducing the programs through\nquestion-answer pairs, we expect a semi-supervised approach, where alignments\nbetween questions and queries are built through templates. We argue that the\ncoverage of language utterances can be expanded using late notable works in\nnatural language generation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 13:40:49 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 14:25:46 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Soru", "Tommaso", ""], ["Marx", "Edgard", ""], ["Valdestilhas", "Andr\u00e9", ""], ["Esteves", "Diego", ""], ["Moussallem", "Diego", ""], ["Publio", "Gustavo", ""]]}, {"id": "1806.10928", "submitter": "Bahare Fatemi", "authors": "Bahare Fatemi, Seyed Mehran Kazemi, David Poole", "title": "Record Linkage to Match Customer Names: A Probabilistic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following problem: given a database of records indexed by names\n(e.g., name of companies, restaurants, businesses, or universities) and a new\nname, determine whether the new name is in the database, and if so, which\nrecord it refers to. This problem is an instance of record linkage problem and\nis a challenging problem because people do not consistently use the official\nname, but use abbreviations, synonyms, different order of terms, different\nspelling of terms, short form of terms, and the name can contain typos or\nspacing issues. We provide a probabilistic model using relational logistic\nregression to find the probability of each record in the database being the\ndesired record for a given query and find the best record(s) with respect to\nthe probabilities. Building on term-matching and translational approaches for\nsearch, our model addresses many of the aforementioned challenges and provides\ngood results when existing baselines fail. Using the probabilities outputted by\nthe model, we can automate the search process for a portion of queries whose\ndesired documents get a probability higher than a trust threshold. We evaluate\nour model on a large real-world dataset from a telecommunications company and\ncompare it to several state-of-the-art baselines. The obtained results show\nthat our model is a promising probabilistic model for record linkage for names.\nWe also test if the knowledge learned by our model on one domain can be\neffectively transferred to a new domain. For this purpose, we test our model on\nan unseen test set from the business names of the secondString dataset.\nPromising results show that our model can be effectively applied to unseen\ndatasets. Finally, we study the sensitivity of our model to the statistics of\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:26:47 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Fatemi", "Bahare", ""], ["Kazemi", "Seyed Mehran", ""], ["Poole", "David", ""]]}, {"id": "1806.10961", "submitter": "Philipp Probst", "authors": "Daniel K\\\"uhn, Philipp Probst, Janek Thomas and Bernd Bischl", "title": "Automatic Exploration of Machine Learning Experiments on OpenML", "comments": "6 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the influence of hyperparameters on the performance of a\nmachine learning algorithm is an important scientific topic in itself and can\nhelp to improve automatic hyperparameter tuning procedures. Unfortunately,\nexperimental meta data for this purpose is still rare. This paper presents a\nlarge, free and open dataset addressing this problem, containing results on 38\nOpenML data sets, six different machine learning algorithms and many different\nhyperparameter configurations. Results where generated by an automated random\nsampling strategy, termed the OpenML Random Bot. Each algorithm was\ncross-validated up to 20.000 times per dataset with different hyperparameters\nsettings, resulting in a meta dataset of around 2.5 million experiments\noverall.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 13:37:10 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 08:48:38 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 13:06:28 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["K\u00fchn", "Daniel", ""], ["Probst", "Philipp", ""], ["Thomas", "Janek", ""], ["Bischl", "Bernd", ""]]}]