[{"id": "1711.00121", "submitter": "Weiren Yu", "authors": "Weiren Yu, Xuemin Lin, Wenjie Zhang, Julie A. McCann", "title": "Dynamical SimRank Search on Time-Varying Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the efficient dynamical computation of all-pairs\nSimRanks on time-varying graphs. Li {\\em et al}.'s approach requires $O(r^4\nn^2)$ time and $O(r^2 n^2)$ memory in a graph with $n$ nodes, where $r$ is the\ntarget rank of the low-rank SVD. (1) We first consider edge update that does\nnot accompany new node insertions. We show that the SimRank update $\\Delta S$\nin response to every link update is expressible as a rank-one Sylvester matrix\nequation. This provides an incremental method requiring $O(Kn^2)$ time and\n$O(n^2)$ memory in the worst case to update all pairs of similarities for $K$\niterations. (2) To speed up the computation further, we propose a lossless\npruning strategy that captures the \"affected areas\" of $\\Delta S$ to eliminate\nunnecessary retrieval. This reduces the time of the incremental SimRank to\n$O(K(m+|AFF|))$, where $m$ is the number of edges in the old graph, and $|AFF|\n(\\le n^2)$ is the size of \"affected areas\" in $\\Delta S$, and in practice,\n$|AFF| \\ll n^2$. (3) We also consider edge updates that accompany node\ninsertions, and categorize them into three cases, according to which end of the\ninserted edge is a new node. For each case, we devise an efficient incremental\nalgorithm that can support new node insertions. (4) We next design an efficient\nbatch incremental method that handles \"similar sink edges\" simultaneously and\neliminates redundant edge updates. (5) To achieve linear memory, we devise a\nmemory-efficient strategy that dynamically updates all pairs of SimRanks column\nby column in just $O(Kn+m)$ memory, without the need to store all $(n^2)$ pairs\nof old SimRank scores. Experimental studies on various datasets demonstrate\nthat our solution substantially outperforms the existing incremental SimRank\nmethods, and is faster and more memory-efficient than its competitors on\nmillion-scale graphs.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 21:50:10 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Yu", "Weiren", ""], ["Lin", "Xuemin", ""], ["Zhang", "Wenjie", ""], ["McCann", "Julie A.", ""]]}, {"id": "1711.01034", "submitter": "Minghui Qiu", "authors": "Xu Hu, Jun Huang, Minghui Qiu, Cen Chen and Wei Chu", "title": "PS-DBSCAN: An Efficient Parallel DBSCAN Algorithm Based on Platform Of\n  AI (PAI)", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present PS-DBSCAN, a communication efficient parallel DBSCAN algorithm\nthat combines the disjoint-set data structure and Parameter Server framework in\nPlatform of AI (PAI). Since data points within the same cluster may be\ndistributed over different workers which result in several disjoint-sets,\nmerging them incurs large communication costs. In our algorithm, we employ a\nfast global union approach to union the disjoint-sets to alleviate the\ncommunication burden. Experiments over the datasets of different scales\ndemonstrate that PS-DBSCAN outperforms the PDSDBSCAN with 2-10 times speedup on\ncommunication efficiency.\n  We have released our PS-DBSCAN in an algorithm platform called Platform of AI\n(PAI - https://pai.base.shuju.aliyun.com/) in Alibaba Cloud. We have also\ndemonstrated how to use the method in PAI.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 06:36:20 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Hu", "Xu", ""], ["Huang", "Jun", ""], ["Qiu", "Minghui", ""], ["Chen", "Cen", ""], ["Chu", "Wei", ""]]}, {"id": "1711.01046", "submitter": "Li Wang", "authors": "Li Wang, Tom Z. J. Fu, Richard T. B. Ma, Marianne Winslett, Zhenjie\n  Zhang", "title": "Elasticutor: Rapid Elasticity for Realtime Stateful Stream Processing", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elasticity is highly desirable for stream processing systems to guarantee low\nlatency against workload dynamics, such as surges in data arrival rate and\nfluctuations in data distribution. Existing systems achieve elasticity\nfollowing a resource-centric approach that uses dynamic key partitioning across\nthe parallel instances, i.e. executors, to balance the workload and scale\noperators. However, such operator-level key repartitioning needs global\nsynchronization and prohibits rapid elasticity. To address this problem, we\npropose an executor-centric approach, whose core idea is to avoid\noperator-level key repartitioning while implementing each executor as the\nbuilding block of elasticity. Following this new approach, we design the\nElasticutor framework with two level of optimizations: i) a novel\nimplementation of executors, i.e., elastic executors, that perform elastic\nmulti-core execution via efficient intra-executor load balancing and executor\nscaling and ii) a global model-based scheduler that dynamically allocates CPU\ncores to executors based on the instantaneous workloads. We implemented a\nprototype of Elasticutor and conducted extensive experiments. Our results show\nthat Elasticutor doubles the throughput and achieves an average processing\nlatency up to 2 orders of magnitude lower than previous methods, for a dynamic\nworkload of real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 07:44:07 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Wang", "Li", ""], ["Fu", "Tom Z. J.", ""], ["Ma", "Richard T. B.", ""], ["Winslett", "Marianne", ""], ["Zhang", "Zhenjie", ""]]}, {"id": "1711.01283", "submitter": "Tommaso Soru", "authors": "Tommaso Soru, Diego Esteves, Edgard Marx, Axel-Cyrille Ngonga Ngomo", "title": "Mandolin: A Knowledge Discovery Framework for the Web of Data", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Logic Networks join probabilistic modeling with first-order logic and\nhave been shown to integrate well with the Semantic Web foundations. While\nseveral approaches have been devised to tackle the subproblems of rule mining,\ngrounding, and inference, no comprehensive workflow has been proposed so far.\nIn this paper, we fill this gap by introducing a framework called Mandolin,\nwhich implements a workflow for knowledge discovery specifically on RDF\ndatasets. Our framework imports knowledge from referenced graphs, creates\nsimilarity relationships among similar literals, and relies on state-of-the-art\ntechniques for rule mining, grounding, and inference computation. We show that\nour best configuration scales well and achieves at least comparable results\nwith respect to other statistical-relational-learning algorithms on link\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 18:04:06 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Soru", "Tommaso", ""], ["Esteves", "Diego", ""], ["Marx", "Edgard", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1711.01287", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Wil M. P. van der Aalst", "title": "Discovering More Precise Process Models from Event Logs by Filtering Out\n  Chaotic Activities", "comments": null, "journal-ref": "Journal of Intelligent Information Systems, (2018), 1-33", "doi": "10.1007/s10844-018-0507-6", "report-no": null, "categories": "cs.DB cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process Discovery is concerned with the automatic generation of a process\nmodel that describes a business process from execution data of that business\nprocess. Real life event logs can contain chaotic activities. These activities\nare independent of the state of the process and can, therefore, happen at\nrather arbitrary points in time. We show that the presence of such chaotic\nactivities in an event log heavily impacts the quality of the process models\nthat can be discovered with process discovery techniques. The current modus\noperandi for filtering activities from event logs is to simply filter out\ninfrequent activities. We show that frequency-based filtering of activities\ndoes not solve the problems that are caused by chaotic activities. Moreover, we\npropose a novel technique to filter out chaotic activities from event logs. We\nevaluate this technique on a collection of seventeen real-life event logs that\noriginate from both the business process management domain and the smart home\nenvironment domain. As demonstrated, the developed activity filtering methods\nenable the discovery of process models that are more behaviorally specific\ncompared to process models that are discovered using standard frequency-based\nfiltering.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 18:13:36 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1711.01299", "submitter": "Eugene Wu", "authors": "Sanjay Krishnan, Michael J. Franklin, Ken Goldberg, Eugene Wu", "title": "BoostClean: Automated Error Detection and Repair for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models based on machine learning can be highly sensitive to data\nerror. Training data are often combined with a variety of different sources,\neach susceptible to different types of inconsistencies, and new data streams\nduring prediction time, the model may encounter previously unseen\ninconsistencies. An important class of such inconsistencies is domain value\nviolations that occur when an attribute value is outside of an allowed domain.\nWe explore automatically detecting and repairing such violations by leveraging\nthe often available clean test labels to determine whether a given detection\nand repair combination will improve model accuracy. We present BoostClean which\nautomatically selects an ensemble of error detection and repair combinations\nusing statistical boosting. BoostClean selects this ensemble from an extensible\nlibrary that is pre-populated general detection functions, including a novel\ndetector based on the Word2Vec deep learning model, which detects errors across\na diverse set of domains. Our evaluation on a collection of 12 datasets from\nKaggle, the UCI repository, real-world data analyses, and production datasets\nthat show that Boost- Clean can increase absolute prediction accuracy by up to\n9% over the best non-ensembled alternatives. Our optimizations including\nparallelism, materialization, and indexing techniques show a 22.2x end-to-end\nspeedup on a 16-core machine.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 18:50:08 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Krishnan", "Sanjay", ""], ["Franklin", "Michael J.", ""], ["Goldberg", "Ken", ""], ["Wu", "Eugene", ""]]}, {"id": "1711.01960", "submitter": "Jialin Wan", "authors": "Shanshan Han, Hongzhi Wang, Jialin Wan, Jianzhong Li", "title": "An Iterative Scheme for Leverage-based Approximate Aggregation", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current data explosion poses great challenges to the approximate\naggregation with an efficiency and accuracy. To address this problem, we\npropose a novel approach to calculate the aggregation answers with a high\naccuracy using only a small portion of the data. We introduce leverages to\nreflect individual differences in the samples from a statistical perspective.\nTwo kinds of estimators, the leverage-based estimator, and the sketch estimator\n(a \"rough picture\" of the aggregation answer), are in constraint relations and\niteratively improved according to the actual conditions until their difference\nis below a threshold. Due to the iteration mechanism and the leverages, our\napproach achieves a high accuracy. Moreover, some features, such as not\nrequiring recording the sampled data and easy to extend to various execution\nmodes (e.g., the online mode), make our approach well suited to deal with big\ndata. Experiments show that our approach has an extraordinary performance, and\nwhen compared with the uniform sampling, our approach can achieve high-quality\nanswers with only 1/3 of the same sample size.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 15:34:35 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 13:38:36 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 15:18:04 GMT"}, {"version": "v4", "created": "Tue, 22 Jan 2019 08:44:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Han", "Shanshan", ""], ["Wang", "Hongzhi", ""], ["Wan", "Jialin", ""], ["Li", "Jianzhong", ""]]}, {"id": "1711.02476", "submitter": "Willi Mann", "authors": "Willi Mann, Nikolaus Augsten, Christian S. Jensen", "title": "SWOOP: Top-k Similarity Joins over Set Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide efficient support for applications that aim to continuously find\npairs of similar sets in rapid streams of sets. A prototypical example setting\nis that of tweets. A tweet is a set of words, and Twitter emits about half a\nbillion tweets per day. Our solution makes it possible to efficiently maintain\nthe top-$k$ most similar tweets from a pair of rapid Twitter streams, e.g., to\ndiscover similar trends in two cities if the streams concern cities.\n  Using a sliding window model, the top-$k$ result changes as new sets in the\nstream enter the window or existing ones leave the window. Maintaining the\ntop-$k$ result under rapid streams is challenging. First, when a set arrives,\nit may form a new pair for the top-$k$ result with any set already in the\nwindow. Second, when a set leaves the window, all its pairings in the top-$k$\nare invalidated and must be replaced. It is not enough to maintain the $k$ most\nsimilar pairs, as less similar pairs may eventually be promoted to the top-$k$\nresult. A straightforward solution that pairs every new set with all sets in\nthe window and keeps all pairs for maintaining the top-$k$ result is memory\nintensive and too slow. We propose SWOOP, a highly scalable stream join\nalgorithm that solves these issues. Novel indexing techniques and sophisticated\nfilters efficiently prune useless pairs as new sets enter the window. SWOOP\nincrementally maintains a stock of similar pairs to update the top-$k$ result\nat any time, and the stock is shown to be minimal. Our experiments confirm that\nSWOOP can deal with stream rates that are orders of magnitude faster than the\nrates of existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 14:20:30 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 20:09:19 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Mann", "Willi", ""], ["Augsten", "Nikolaus", ""], ["Jensen", "Christian S.", ""]]}, {"id": "1711.02952", "submitter": "Tejas Kulkarni", "authors": "Tejas Kulkarni, Graham Cormode, Divesh Srivastava", "title": "Marginal Release Under Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many analysis and machine learning tasks require the availability of marginal\nstatistics on multidimensional datasets while providing strong privacy\nguarantees for the data subjects. Applications for these statistics range from\nfinding correlations in the data to fitting sophisticated prediction models. In\nthis paper, we provide a set of algorithms for materializing marginal\nstatistics under the strong model of local differential privacy. We prove the\nfirst tight theoretical bounds on the accuracy of marginals compiled under each\napproach, perform empirical evaluation to confirm these bounds, and evaluate\nthem for tasks such as modeling and correlation testing. Our results show that\nreleasing information based on (local) Fourier transformations of the input is\npreferable to alternatives based directly on (local) marginals.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 14:15:53 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Kulkarni", "Tejas", ""], ["Cormode", "Graham", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1711.03537", "submitter": "Janneth Chicaiza", "authors": "Nelson Piedra, Janneth Chicaiza, Jorge Lopez-Vargas, Edmundo Tovar", "title": "Discovery of potential collaboration networks from open knowledge\n  sources", "comments": "2 pages, International Conference on Knowledge Engineering and\n  Semantic Web", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scientific publishing conveys the outputs of an academic or research\nactivity, in this sense; it also reflects the efforts and issues in which\npeople engage. To identify potential collaborative networks one of the simplest\napproaches is to leverage the co-authorship relations. In this approach,\nsemantic and hierarchic relationships defined by a Knowledge Organization\nSystem are used in order to improve the system's ability to recommend potential\nnetworks beyond the lexical or syntactic analysis of the topics or concepts\nthat are of interest to academics.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:58:16 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Piedra", "Nelson", ""], ["Chicaiza", "Janneth", ""], ["Lopez-Vargas", "Jorge", ""], ["Tovar", "Edmundo", ""]]}, {"id": "1711.03860", "submitter": "D\\'aniel Marx", "authors": "Albert Atserias, Martin Grohe, D\\'aniel Marx", "title": "Size bounds and query plans for relational joins", "comments": "Conference version in FOCS 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational joins are at the core of relational algebra, which in turn is the\ncore of the standard database query language SQL. As their evaluation is\nexpensive and very often dominated by the output size, it is an important task\nfor database query optimisers to compute estimates on the size of joins and to\nfind good execution plans for sequences of joins. We study these problems from\na theoretical perspective, both in the worst-case model, and in an average-case\nmodel where the database is chosen according to a known probability\ndistribution. In the former case, our first key observation is that the\nworst-case size of a query is characterised by the fractional edge cover number\nof its underlying hypergraph, a combinatorial parameter previously known to\nprovide an upper bound. We complete the picture by proving a matching lower\nbound, and by showing that there exist queries for which the join-project plan\nsuggested by the fractional edge cover approach may be substantially better\nthan any join plan that does not use intermediate projections. On the other\nhand, we show that in the average-case model, every join-project plan can be\nturned into a plan containing no projections in such a way that the expected\ntime to evaluate the plan increases only by a constant factor independent of\nthe size of the database. Not surprisingly, the key combinatorial parameter in\nthis context is the maximum density of the underlying hypergraph. We show how\nto make effective use of this parameter to eliminate the projections.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:16:52 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Atserias", "Albert", ""], ["Grohe", "Martin", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1711.03955", "submitter": "Sandeep Singh Sandha", "authors": "Sandeep Singh Sandha", "title": "StreetX: Spatio-Temporal Access Control Model for Data", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cities are a big source of spatio-temporal data that is shared across\nentities to drive potential use cases. Many of the Spatio-temporal datasets are\nconfidential and are selectively shared. To allow selective sharing, several\naccess control models exist, however user cannot express arbitrary space and\ntime constraints on data attributes using them. In this paper we focus on\nspatio-temporal access control model. We show that location and time attributes\nof data may decide its confidentiality via a motivating example and thus can\naffect user's access control policy. In this paper, we present StreetX which\nenables user to represent constraints on multiple arbitrary space regions and\ntime windows using a simple abstract language. StreetX is scalable and is\ndesigned to handle large amount of spatio-temporal data from multiple users.\nMultiple space and time constraints can affect performance of the query and may\nalso result in conflicts. StreetX automatically resolve conflicts and optimizes\nthe query evaluation with access control to improve performance. We implemented\nand tested prototype of StreetX using space constraints by defining region\nhaving 1749 polygon coordinates on 10 million data records. Our testing shows\nthat StreetX extends the current access control with spatio-temporal\ncapabilities.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 18:32:17 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Sandha", "Sandeep Singh", ""]]}, {"id": "1711.03987", "submitter": "Pan Hu", "authors": "Pan Hu, Boris Motik, Ian Horrocks", "title": "Optimised Maintenance of Datalog Materialisations", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To efficiently answer queries, datalog systems often materialise all\nconsequences of a datalog program, so the materialisation must be updated\nwhenever the input facts change. Several solutions to the materialisation\nupdate problem have been proposed. The Delete/Rederive (DRed) and the\nBackward/Forward (B/F) algorithms solve this problem for general datalog, but\nboth contain steps that evaluate rules 'backwards' by matching their heads to a\nfact and evaluating the partially instantiated rule bodies as queries. We show\nthat this can be a considerable source of overhead even on very small updates.\nIn contrast, the Counting algorithm does not evaluate the rules 'backwards',\nbut it can handle only nonrecursive rules. We present two hybrid approaches\nthat combine DRed and B/F with Counting so as to reduce or even eliminate\n'backward' rule evaluation while still handling arbitrary datalog programs. We\nshow empirically that our hybrid algorithms are usually significantly faster\nthan existing approaches, sometimes by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 19:10:20 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 14:06:40 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Hu", "Pan", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1711.04001", "submitter": "Navid Yaghmazadeh", "authors": "Navid Yaghmazadeh, Xinyu Wang, Isil Dillig", "title": "Automated Migration of Hierarchical Data to Relational Tables using\n  Programming-by-Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many applications export data in hierarchical formats like XML and\nJSON, it is often necessary to convert such hierarchical documents to a\nrelational representation. This paper presents a novel programming-by-example\napproach, and its implementation in a tool called Mitra, for automatically\nmigrating tree-structured documents to relational tables. We have evaluated the\nproposed technique using two sets of experiments. In the first experiment, we\nused Mitra to automate 98 data transformation tasks collected from\nStackOverflow. Our method can generate the desired program for 94% of these\nbenchmarks with an average synthesis time of 3.8 seconds. In the second\nexperiment, we used Mitra to generate programs that can convert real-world XML\nand JSON datasets to full-fledged relational databases. Our evaluation shows\nthat Mitra can automate the desired transformation for all datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 20:08:18 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Yaghmazadeh", "Navid", ""], ["Wang", "Xinyu", ""], ["Dillig", "Isil", ""]]}, {"id": "1711.04013", "submitter": "Alessandro Ronca", "authors": "Alessandro Ronca, Mark Kaminski, Bernardo Cuenca Grau, Boris Motik,\n  Ian Horrocks", "title": "Stream Reasoning in Temporal Datalog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an increasing interest in extending\ntraditional stream processing engines with logical, rule-based, reasoning\ncapabilities. This poses significant theoretical and practical challenges since\nrules can derive new information and propagate it both towards past and future\ntime points; as a result, streamed query answers can depend on data that has\nnot yet been received, as well as on data that arrived far in the past. Stream\nreasoning algorithms, however, must be able to stream out query answers as soon\nas possible, and can only keep a limited number of previous input facts in\nmemory. In this paper, we propose novel reasoning problems to deal with these\nchallenges, and study their computational properties on Datalog extended with a\ntemporal sort and the successor function (a core rule-based language for stream\nreasoning applications).\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 21:11:17 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 11:06:52 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Ronca", "Alessandro", ""], ["Kaminski", "Mark", ""], ["Grau", "Bernardo Cuenca", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1711.04436", "submitter": "Xiaojun Xu", "authors": "Xiaojun Xu, Chang Liu, Dawn Song", "title": "SQLNet: Generating Structured Queries From Natural Language Without\n  Reinforcement Learning", "comments": "Submitting to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing SQL queries from natural language is a long-standing open\nproblem and has been attracting considerable interest recently. Toward solving\nthe problem, the de facto approach is to employ a sequence-to-sequence-style\nmodel. Such an approach will necessarily require the SQL queries to be\nserialized. Since the same SQL query may have multiple equivalent\nserializations, training a sequence-to-sequence-style model is sensitive to the\nchoice from one of them. This phenomenon is documented as the \"order-matters\"\nproblem. Existing state-of-the-art approaches rely on reinforcement learning to\nreward the decoder when it generates any of the equivalent serializations.\nHowever, we observe that the improvement from reinforcement learning is\nlimited.\n  In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally\nsolve this problem by avoiding the sequence-to-sequence structure when the\norder does not matter. In particular, we employ a sketch-based approach where\nthe sketch contains a dependency graph so that one prediction can be done by\ntaking into consideration only the previous predictions that it depends on. In\naddition, we propose a sequence-to-set model as well as the column attention\nmechanism to synthesize the query based on the sketch. By combining all these\nnovel techniques, we show that SQLNet can outperform the prior art by 9% to 13%\non the WikiSQL task.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 06:41:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Xu", "Xiaojun", ""], ["Liu", "Chang", ""], ["Song", "Dawn", ""]]}, {"id": "1711.04452", "submitter": "Georgina Nugent Folan", "authors": "Jennifer Edmond, Georgina Nugent Folan", "title": "Digitising Cultural Complexity: Representing Rich Cultural Data in a Big\n  Data environment", "comments": "Ways of Being in a Digital Age - A Review Conference, Oct 2017,\n  Liverpool, United Kingdom. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major terminological forces driving ICT integration in research\ntoday is that of \"big data.\" While the phrase sounds inclusive and integrative,\n\"big data\" approaches are highly selective, excluding input that cannot be\neffectively structured, represented, or digitised. Data of this complex sort is\nprecisely the kind that human activity produces, but the technological\nimperative to enhance signal through the reduction of noise does not\naccommodate this richness. Data and the computational approaches that\nfacilitate \"big data\" have acquired a perceived objectivity that belies their\ncurated, malleable, reactive, and performative nature. In an input environment\nwhere anything can \"be data\" once it is entered into the system as \"data,\" data\ncleaning and processing, together with the metadata and information\narchitectures that structure and facilitate our cultural archives acquire a\ncapacity to delimit what data are. This engenders a process of simplification\nthat has major implications for the potential for future innovation within\nresearch environments that depend on rich material yet are increasingly\nmediated by digital technologies. This paper presents the preliminary findings\nof the European-funded KPLEX (Knowledge Complexity) project which investigates\nthe delimiting effect digital mediation and datafication has on rich, complex\ncultural data. The paper presents a systematic review of existing implicit\ndefinitions of data, elaborating on the implications of these definitions and\nhighlighting the ways in which metadata and computational technologies can\nrestrict the interpretative potential of data. It sheds light on the gap\nbetween analogue or augmented digital practices and fully computational ones,\nand the strategies researchers have developed to deal with this gap. The paper\nproposes a reconceptualisation of data as it is functionally employed within\ndigitally-mediated research so as to incorporate and acknowledge the richness\nand complexity of our source materials.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 07:31:24 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Edmond", "Jennifer", ""], ["Folan", "Georgina Nugent", ""]]}, {"id": "1711.04595", "submitter": "Zhou Yang", "authors": "Zhou Yang, Heli Sun, Jianbin Huang, Xiaolin Jia, Ziyu Guan, Zhongmeng\n  Zhao", "title": "Efficient Destination Prediction Based on Route Choices with Transition\n  Matrix Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Destination prediction is an essential task in a variety of mobile\napplications. In this paper, we optimize the matrix operation and adapt a\nsemi-lazy framework to improve the prediction accuracy and efficiency of a\nstate-of-the-art approach. To this end, we employ efficient dynamic-programming\nby devising several data constructs including Efficient Transition Probability\nand Transition Probabilities with Detours that are capable of pinpointing the\nminimum amount of computation. We prove that our method achieves one order of\ncut in both time and space complexity. The experimental results on real-world\nand synthetic datasets have shown that our solution consistently outperforms\nits state-of-the-art counterparts in terms of both efficiency (approximately\nover 100 times faster) and accuracy (above 30% increase).\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 06:40:28 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 06:34:20 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Yang", "Zhou", ""], ["Sun", "Heli", ""], ["Huang", "Jianbin", ""], ["Jia", "Xiaolin", ""], ["Guan", "Ziyu", ""], ["Zhao", "Zhongmeng", ""]]}, {"id": "1711.04710", "submitter": "Anuj Karpatne", "authors": "Gowtham Atluri, Anuj Karpatne, and Vipin Kumar", "title": "Spatio-Temporal Data Mining: A Survey of Problems and Methods", "comments": "Accepted for publication at ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large volumes of spatio-temporal data are increasingly collected and studied\nin diverse domains including, climate science, social sciences, neuroscience,\nepidemiology, transportation, mobile health, and Earth sciences.\nSpatio-temporal data differs from relational data for which computational\napproaches are developed in the data mining community for multiple decades, in\nthat both spatial and temporal attributes are available in addition to the\nactual measurements/attributes. The presence of these attributes introduces\nadditional challenges that needs to be dealt with. Approaches for mining\nspatio-temporal data have been studied for over a decade in the data mining\ncommunity. In this article we present a broad survey of this relatively young\nfield of spatio-temporal data mining. We discuss different types of\nspatio-temporal data and the relevant data mining questions that arise in the\ncontext of analyzing each of these datasets. Based on the nature of the data\nmining problem studied, we classify literature on spatio-temporal data mining\ninto six major categories: clustering, predictive learning, change detection,\nfrequent pattern mining, anomaly detection, and relationship mining. We discuss\nthe various forms of spatio-temporal data mining problems in each of these\ncategories.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:17:29 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 17:31:54 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Atluri", "Gowtham", ""], ["Karpatne", "Anuj", ""], ["Kumar", "Vipin", ""]]}, {"id": "1711.04971", "submitter": "Srikanta Bedathur", "authors": "Rema Ananthanarayanan and Pranay Kr. Lohia and Srikanta Bedathur", "title": "DataVizard: Recommending Visual Presentations for Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the appropriate visual presentation of the data such that it\npreserves the semantics of the underlying data and at the same time provides an\nintuitive summary of the data is an important, often the final step of data\nanalytics. Unfortunately, this is also a step involving significant human\neffort starting from selection of groups of columns in the structured results\nfrom analytics stages, to the selection of right visualization by experimenting\nwith various alternatives. In this paper, we describe our \\emph{DataVizard}\nsystem aimed at reducing this overhead by automatically recommending the most\nappropriate visual presentation for the structured result. Specifically, we\nconsider the following two scenarios: first, when one needs to visualize the\nresults of a structured query such as SQL; and the second, when one has\nacquired a data table with an associated short description (e.g., tables from\nthe Web). Using a corpus of real-world database queries (and their results) and\na number of statistical tables crawled from the Web, we show that DataVizard is\ncapable of recommending visual presentations with high accuracy. We also\npresent the results of a user survey that we conducted in order to assess user\nviews of the suitability of the presented charts vis-a-vis the plain text\ncaptions of the data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 06:43:30 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ananthanarayanan", "Rema", ""], ["Lohia", "Pranay Kr.", ""], ["Bedathur", "Srikanta", ""]]}, {"id": "1711.05090", "submitter": "Thomas Guyet", "authors": "Thomas Guyet (LACODAM), Yves Moinard (LACODAM), Ren\\'e Quiniou\n  (LACODAM), Torsten Schaub (LACODAM)", "title": "Efficiency Analysis of ASP Encodings for Sequential Pattern Mining Tasks", "comments": null, "journal-ref": "Bruno Pinaud; Fabrice Guillet; Bruno Cremilleux; Cyril de Runz.\n  Advances in Knowledge Discovery and Management, 7, Springer, pp.41--81, 2017,\n  978-3-319-65405-8", "doi": null, "report-no": null, "categories": "cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the use of Answer Set Programming (ASP) to mine\nsequential patterns. ASP is a high-level declarative logic programming paradigm\nfor high level encoding combinatorial and optimization problem solving as well\nas knowledge representation and reasoning. Thus, ASP is a good candidate for\nimplementing pattern mining with background knowledge, which has been a data\nmining issue for a long time. We propose encodings of the classical sequential\npattern mining tasks within two representations of embeddings (fill-gaps vs\nskip-gaps) and for various kinds of patterns: frequent, constrained and\ncondensed. We compare the computational performance of these encodings with\neach other to get a good insight into the efficiency of ASP encodings. The\nresults show that the fill-gaps strategy is better on real problems due to\nlower memory consumption. Finally, compared to a constraint programming\napproach (CPSM), another declarative programming paradigm, our proposal showed\ncomparable performance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:09:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Guyet", "Thomas", "", "LACODAM"], ["Moinard", "Yves", "", "LACODAM"], ["Quiniou", "Ren\u00e9", "", "LACODAM"], ["Schaub", "Torsten", "", "LACODAM"]]}, {"id": "1711.05573", "submitter": "Jia Zou", "authors": "Jia Zou, R. Matthew Barnett, Tania Lorido-Botran, Shangyu Luo, Carlos\n  Monroy, Sourav Sikdar, Kia Teymourian, Binhang Yuan, Chris Jermaine", "title": "PlinyCompute: A Platform for High-Performance, Distributed,\n  Data-Intensive Tool Development", "comments": "48 pages, including references and Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes PlinyCompute, a system for development of\nhigh-performance, data-intensive, distributed computing tools and libraries. In\nthe large, PlinyCompute presents the programmer with a very high-level,\ndeclarative interface, relying on automatic, relational-database style\noptimization to figure out how to stage distributed computations. However, in\nthe small, PlinyCompute presents the capable systems programmer with a\npersistent object data model and API (the \"PC object model\") and associated\nmemory management system that has been designed from the ground-up for high\nperformance, distributed, data-intensive computing. This contrasts with most\nother Big Data systems, which are constructed on top of the Java Virtual\nMachine (JVM), and hence must at least partially cede performance-critical\nconcerns such as memory management (including layout and de/allocation) and\nvirtual method/function dispatch to the JVM. This hybrid approach---declarative\nin the large, trusting the programmer's ability to utilize PC object model\nefficiently in the small---results in a system that is ideal for the\ndevelopment of reusable, data-intensive tools and libraries. Through extensive\nbenchmarking, we show that implementing complex objects manipulation and\nnon-trivial, library-style computations on top of PlinyCompute can result in a\nspeedup of 2x to more than 50x or more compared to equivalent implementations\non Spark.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 14:01:06 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 02:30:18 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Zou", "Jia", ""], ["Barnett", "R. Matthew", ""], ["Lorido-Botran", "Tania", ""], ["Luo", "Shangyu", ""], ["Monroy", "Carlos", ""], ["Sikdar", "Sourav", ""], ["Teymourian", "Kia", ""], ["Yuan", "Binhang", ""], ["Jermaine", "Chris", ""]]}, {"id": "1711.05787", "submitter": "Rishabh Singh", "authors": "Jeevana Priya Inala and Rishabh Singh", "title": "WebRelate: Integrating Web Data with Spreadsheets using Examples", "comments": "To appear in POPL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration between web sources and relational data is a key challenge\nfaced by data scientists and spreadsheet users. There are two main challenges\nin programmatically joining web data with relational data. First, most websites\ndo not expose a direct interface to obtain tabular data, so the user needs to\nformulate a logic to get to different webpages for each input row in the\nrelational table. Second, after reaching the desired webpage, the user needs to\nwrite complex scripts to extract the relevant data, which is often conditioned\non the input data. Since many data scientists and end-users come from diverse\nbackgrounds, writing such complex regular-expression based logical scripts to\nperform data integration tasks is unfortunately often beyond their programming\nexpertise.\n  We present WebRelate, a system that allows users to join semi-structured web\ndata with relational data in spreadsheets using input-output examples.\nWebRelate decomposes the web data integration task into two sub-tasks of i) URL\nlearning and ii) input-dependent web extraction. The first sub-task generates\nthe URLs for the webpages containing the desired data for all rows in the\nrelational table. WebRelate achieves this by learning a string transformation\nprogram using a few example URLs. The second sub-task uses examples of desired\ndata to be extracted from the corresponding webpages and learns a program to\nextract the data for the other rows. We design expressive domain-specific\nlanguages for URL generation and web data extraction, and present efficient\nsynthesis algorithms for learning programs in these DSLs from few input-output\nexamples. We evaluate WebRelate on 88 real-world web data integration tasks\ntaken from online help forums and Excel product team, and show that WebRelate\ncan learn the desired programs within few seconds using only 1 example for the\nmajority of the tasks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 20:17:07 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Inala", "Jeevana Priya", ""], ["Singh", "Rishabh", ""]]}, {"id": "1711.05857", "submitter": "Lijun Chang", "authors": "Fei Bi, Lijun Chang, Xuemin Lin, Wenjie Zhang", "title": "An Optimal and Progressive Approach to Online Search of Top-k\n  Influential Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Community search over large graphs is a fundamental problem in graph\nanalysis. Recent studies propose to compute top-k influential communities,\nwhere each reported community not only is a cohesive subgraph but also has a\nhigh influence value. The existing approaches to the problem of top-k\ninfluential community search can be categorized as index-based algorithms and\nonline search algorithms without indexes. The index-based algorithms, although\nbeing very efficient in conducting community searches, need to pre-compute a\nspecial-purpose index and only work for one built-in vertex weight vector. In\nthis paper, we investigate on-line search approaches and propose an\ninstance-optimal algorithm LocalSearch whose time complexity is linearly\nproportional to the size of the smallest subgraph that a correct algorithm\nneeds to access without indexes. In addition, we also propose techniques to\nmake LocalSearch progressively compute and report the communities in decreasing\ninfluence value order such that k does not need to be specified. Moreover, we\nextend our framework to the general case of top-k influential community search\nregarding other cohesiveness measures. Extensive empirical studies on real\ngraphs demonstrate that our algorithms outperform the existing online search\nalgorithms by several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 00:12:25 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 22:35:46 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Bi", "Fei", ""], ["Chang", "Lijun", ""], ["Lin", "Xuemin", ""], ["Zhang", "Wenjie", ""]]}, {"id": "1711.06608", "submitter": "Hugo Firth", "authors": "Hugo Firth, Paolo Missier, Jack Aiston", "title": "Loom: Query-aware Partitioning of Online Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As with general graph processing systems, partitioning data over a cluster of\nmachines improves the scalability of graph database management systems.\nHowever, these systems will incur additional network cost during the execution\nof a query workload, due to inter-partition traversals. Workload-agnostic\npartitioning algorithms typically minimise the likelihood of any edge crossing\npartition boundaries. However, these partitioners are sub-optimal with respect\nto many workloads, especially queries, which may require more frequent\ntraversal of specific subsets of inter-partition edges. Furthermore, they\nlargely unsuited to operating incrementally on dynamic, growing graphs.\n  We present a new graph partitioning algorithm, Loom, that operates on a\nstream of graph updates and continuously allocates the new vertices and edges\nto partitions, taking into account a query workload of graph pattern\nexpressions along with their relative frequencies.\n  First we capture the most common patterns of edge traversals which occur when\nexecuting queries. We then compare sub-graphs, which present themselves\nincrementally in the graph update stream, against these common patterns.\nFinally we attempt to allocate each match to single partitions, reducing the\nnumber of inter-partition edges within frequently traversed sub-graphs and\nimproving average query performance.\n  Loom is extensively evaluated over several large test graphs with realistic\nquery workloads and various orderings of the graph updates. We demonstrate\nthat, given a workload, our prototype produces partitionings of significantly\nbetter quality than existing streaming graph partitioning algorithms Fennel and\nLDG.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 16:06:04 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Firth", "Hugo", ""], ["Missier", "Paolo", ""], ["Aiston", "Jack", ""]]}, {"id": "1711.07295", "submitter": "Edans Flavius De Oliveira Sandes", "authors": "Edans F. O. Sandes, George Teodoro and Alba C. M. A. Melo", "title": "Bitmap Filter: Speeding up Exact Set Similarity Joins with Bitwise\n  Operations", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Exact Set Similarity Join problem aims to find all similar sets between\ntwo collections of sets, with respect to a threshold and a similarity function\nsuch as overlap, Jaccard, dice or cosine. The naive approach verifies all pairs\nof sets and it is often considered impractical due the high number of\ncombinations. So, Exact Set Similarity Join algorithms are usually based on the\nFilter-Verification Framework, that applies a series of filters to reduce the\nnumber of verified pairs. This paper presents a new filtering technique called\nBitmap Filter, which is able to accelerate state-of-the-art algorithms for the\nexact Set Similarity Join problem. The Bitmap Filter uses hash functions to\ncreate bitmaps of fixed b bits, representing characteristics of the sets. Then,\nit applies bitwise operations (such as xor and population count) on the bitmaps\nin order to infer a similarity upper bound for each pair of sets. If the upper\nbound is below a given similarity threshold, the pair of sets is pruned. The\nBitmap Filter benefits from the fact that bitwise operations are efficiently\nimplemented by many modern general-purpose processors and it was easily applied\nto four state-of-the-art algorithms implemented in CPU: AllPairs, PPJoin,\nAdaptJoin and GroupJoin. Furthermore, we propose a Graphic Processor Unit (GPU)\nalgorithm based on the naive approach but using the Bitmap Filter to speedup\nthe computation. The experiments considered 9 collections containing from 100\nthousands up to 10 million sets and the joins were made using Jaccard\nthresholds from 0.50 to 0.95. The Bitmap Filter was able to improve 90% of the\nexperiments in CPU, with speedups of up to 4.50x and 1.43x on average. Using\nthe GPU algorithm, the experiments were able to speedup the original CPU\nalgorithms by up to 577x using an Nvidia Geforce GTX 980 Ti.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 13:06:10 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sandes", "Edans F. O.", ""], ["Teodoro", "George", ""], ["Melo", "Alba C. M. A.", ""]]}, {"id": "1711.07581", "submitter": "Madhulika Mohanty", "authors": "Madhulika Mohanty, Maya Ramanath, Mohamed Yahya, Gerhard Weikum", "title": "Spec-QP: Speculative Query Planning for Joins over Knowledge Graphs", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organisations store huge amounts of data from multiple heterogeneous sources\nin the form of Knowledge Graphs (KGs). One of the ways to query these KGs is to\nuse SPARQL queries over a database engine. Since SPARQL follows exact match\nsemantics, the queries may return too few or no results. Recent works have\nproposed query relaxation where the query engine judiciously replaces a query\npredicate with similar predicates using weighted relaxation rules mined from\nthe KG. The space of possible relaxations is potentially too large to fully\nexplore and users are typically interested in only top-k results, so such query\nengines use top-k algorithms for query processing. However, they may still\nprocess all the relaxations, many of whose answers do not contribute towards\ntop-k answers. This leads to computation overheads and delayed response times.\n  We propose Spec-QP, a query planning framework that speculatively determines\nwhich relaxations will have their results in the top-k answers. Only these\nrelaxations are processed using the top-k operators. We, therefore, reduce the\ncomputation overheads and achieve faster response times without adversely\naffecting the quality of results. We tested Spec-QP over two datasets - XKG and\nTwitter, to demonstrate the efficiency of our planning framework at reducing\nruntimes with reasonable accuracy for query engines supporting relaxations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 23:47:07 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Mohanty", "Madhulika", ""], ["Ramanath", "Maya", ""], ["Yahya", "Mohamed", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1711.08330", "submitter": "Oleg Ivanov", "authors": "Oleg Ivanov, Sergey Bartunov", "title": "Adaptive Cardinality Estimation", "comments": "12 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address cardinality estimation problem which is an important\nsubproblem in query optimization. Query optimization is a part of every\nrelational DBMS responsible for finding the best way of the execution for the\ngiven query. These ways are called plans. The execution time of different plans\nmay differ by several orders, so query optimizer has a great influence on the\nwhole DBMS performance. We consider cost-based query optimization approach as\nthe most popular one. It was observed that cost-based optimization quality\ndepends much on cardinality estimation quality. Cardinality of the plan node is\nthe number of tuples returned by it.\n  In the paper we propose a novel cardinality estimation approach with the use\nof machine learning methods. The main point of the approach is using query\nexecution statistics of the previously executed queries to improve cardinality\nestimations. We called this approach adaptive cardinality estimation to reflect\nthis point. The approach is general, flexible, and easy to implement. The\nexperimental evaluation shows that this approach significantly increases the\nquality of cardinality estimation, and therefore increases the DBMS performance\nfor some queries by several times or even by several dozens of times.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 15:20:19 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Ivanov", "Oleg", ""], ["Bartunov", "Sergey", ""]]}, {"id": "1711.09189", "submitter": "Lijun Chang", "authors": "Lijun Chang", "title": "A Near-optimal Algorithm for Edge Connectivity-based Hierarchical Graph\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Driven by many applications in graph analytics, the problem of computing\n$k$-edge connected components ($k$-ECCs) of a graph $G$ for a user-given $k$\nhas been extensively studied recently. In this paper, we investigate the\nproblem of constructing the hierarchy of edge connectivity-based graph\ndecomposition, which compactly represents the $k$-ECCs of a graph for all\npossible $k$ values. This is based on the fact that each $k$-ECC is entirely\ncontained in a $(k-1)$-ECC. In contrast to the existing approaches that conduct\nthe computation either in a bottom-up or a top-down manner, we propose a binary\nsearch-based framework which invokes a $k$-ECC computation algorithm as a black\nbox. Let $T_{kecc}(G)$ be the time complexity of computing all $k$-ECCs of $G$\nfor a specific $k$ value. We prove that the time complexity of our framework is\n${\\cal O}\\big( (\\log \\delta(G))\\times T_{kecc}(G)\\big)$, where $\\delta(G)$ is\nthe degeneracy of $G$ and equals the maximum value among the minimum vertex\ndegrees of all subgraphs of $G$. As $\\delta(G)$ is typically small for\nreal-world graphs, this time complexity is optimal up to a logarithmic factor.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 04:43:12 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Chang", "Lijun", ""]]}, {"id": "1711.09279", "submitter": "Ritvik Shrivastava", "authors": "Anand Gupta, Hardeo Thakur, Ritvik Shrivastava, Pulkit Kumar, Sreyashi\n  Nag", "title": "A Big Data Analysis Framework Using Apache Spark and Deep Learning", "comments": "To be published in IEEE ICDM 2017 (International Conference on Data\n  Mining) Workshop on Data Science and Big Data Analytics (DSBDA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the spreading prevalence of Big Data, many advances have recently been\nmade in this field. Frameworks such as Apache Hadoop and Apache Spark have\ngained a lot of traction over the past decades and have become massively\npopular, especially in industries. It is becoming increasingly evident that\neffective big data analysis is key to solving artificial intelligence problems.\nThus, a multi-algorithm library was implemented in the Spark framework, called\nMLlib. While this library supports multiple machine learning algorithms, there\nis still scope to use the Spark setup efficiently for highly time-intensive and\ncomputationally expensive procedures like deep learning. In this paper, we\npropose a novel framework that combines the distributive computational\nabilities of Apache Spark and the advanced machine learning architecture of a\ndeep multi-layer perceptron (MLP), using the popular concept of Cascade\nLearning. We conduct empirical analysis of our framework on two real world\ndatasets. The results are encouraging and corroborate our proposed framework,\nin turn proving that it is an improvement over traditional big data analysis\nmethods that use either Spark or Deep learning as individual elements.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 20:11:41 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Gupta", "Anand", ""], ["Thakur", "Hardeo", ""], ["Shrivastava", "Ritvik", ""], ["Kumar", "Pulkit", ""], ["Nag", "Sreyashi", ""]]}, {"id": "1711.09409", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang, Congying Xia, Chenwei Zhang, Limeng Cui, Yanjie Fu and\n  Philip S. Yu", "title": "BL-MNE: Emerging Heterogeneous Social Network Embedding through Broad\n  Learning with Aligned Autoencoder", "comments": "10 pages, 9 figures, 4 tables. Full paper is accepted by ICDM 2017,\n  In: Proceedings of the 2017 IEEE International Conference on Data MiningS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding aims at projecting the network data into a low-dimensional\nfeature space, where the nodes are represented as a unique feature vector and\nnetwork structure can be effectively preserved. In recent years, more and more\nonline application service sites can be represented as massive and complex\nnetworks, which are extremely challenging for traditional machine learning\nalgorithms to deal with. Effective embedding of the complex network data into\nlow-dimension feature representation can both save data storage space and\nenable traditional machine learning algorithms applicable to handle the network\ndata. Network embedding performance will degrade greatly if the networks are of\na sparse structure, like the emerging networks with few connections. In this\npaper, we propose to learn the embedding representation for a target emerging\nnetwork based on the broad learning setting, where the emerging network is\naligned with other external mature networks at the same time. To solve the\nproblem, a new embedding framework, namely \"Deep alIgned autoencoder based\neMbEdding\" (DIME), is introduced in this paper. DIME handles the diverse link\nand attribute in a unified analytic based on broad learning, and introduces the\nmultiple aligned attributed heterogeneous social network concept to model the\nnetwork structure. A set of meta paths are introduced in the paper, which\ndefine various kinds of connections among users via the heterogeneous link and\nattribute information. The closeness among users in the networks are defined as\nthe meta proximity scores, which will be fed into DIME to learn the embedding\nvectors of users in the emerging network. Extensive experiments have been done\non real-world aligned social networks, which have demonstrated the\neffectiveness of DIME in learning the emerging network embedding vectors.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 15:44:49 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Zhang", "Jiawei", ""], ["Xia", "Congying", ""], ["Zhang", "Chenwei", ""], ["Cui", "Limeng", ""], ["Fu", "Yanjie", ""], ["Yu", "Philip S.", ""]]}, {"id": "1711.09411", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang, Limeng Cui, Philip S. Yu and Yuanhua Lv", "title": "BL-ECD: Broad Learning based Enterprise Community Detection via\n  Hierarchical Structure Fusion", "comments": "10 Pages, 12 Figures. Full paper has been accepted by CIKM 2017,In:\n  Proceedings of the 2017 International Conference on Information and Knowledge\n  Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employees in companies can be divided into di erent communities, and those\nwho frequently socialize with each other will be treated as close friends and\nare grouped in the same community. In the enterprise context, a large amount of\ninformation about the employees is available in both (1) o ine company internal\nsources and (2) online enterprise social networks (ESNs). Each of the\ninformation sources also contain multiple categories of employees'\nsocialization activities at the same time. In this paper, we propose to detect\nthe social communities of the employees in companies based on the broad\nlearning se ing with both these online and o ine information sources\nsimultaneously, and the problem is formally called the \"Broad Learning based\nEnterprise Community Detection\" (BL-Ecd) problem. To address the problem, a\nnovel broad learning based community detection framework named \"HeterogeneoUs\nMulti-sOurce ClusteRing\" (Humor) is introduced in this paper. Based on the\nvarious enterprise social intimacy measures introduced in this paper, Humor\ndetects a set of micro community structures of the employees based on each of\nthe socialization activities respectively. To obtain the (globally) consistent\ncommunity structure of employees in the company, Humor further fuses these\nmicro community structures via two broad learning phases: (1) intra-fusion of\nmicro community structures to obtain the online and o ine (locally) consistent\ncommunities respectively, and (2) inter-fusion of the online and o ine\ncommunities to achieve the (globally) consistent community structure of\nemployees. Extensive experiments conducted on real-world enterprise datasets\ndemonstrate our method can perform very well in addressing the BL-Ecd problem.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 15:56:06 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Zhang", "Jiawei", ""], ["Cui", "Limeng", ""], ["Yu", "Philip S.", ""], ["Lv", "Yuanhua", ""]]}, {"id": "1711.09543", "submitter": "Ning Gao", "authors": "Ning Gao, Zhang Liu, Dirk Grunwald", "title": "DTranx: A SEDA-based Distributed and Transactional Key Value Store with\n  Persistent Memory Log", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current distributed key value stores achieve scalability by trading off\nconsistency. As persistent memory technologies evolve tremendously, it is not\nnecessary to sacrifice consistency for performance. This paper proposes DTranx,\na distributed key value store based on a persistent memory aware log. DTranx\nintegrates a state transition based garbage collection mechanism in the log\ndesign to effectively and efficiently reclaim old logs. In addition, DTranx\nadopts the SEDA architecture to exploit higher concurrency in multi-core\nenvironments and employs the optimal core binding strategy to minimize context\nswitch overhead. Moreover, we customize a hybrid commit protocol that combines\noptimistic concurrency control and two-phase commit to reduce critical section\nof distributed locking and introduce a locking mechanism to avoid deadlocks and\nlivelocks.\n  In our evaluations, DTranx reaches 514.11k transactions per second with 36\nservers and 95\\% read workloads. The persistent memory aware log is 30 times\nfaster than the SSD based system. And, our state transition based garbage\ncollection mechanism is efficient and effective. It does not affect normal\ntransactions and log space usage is steadily low.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 05:38:10 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Gao", "Ning", ""], ["Liu", "Zhang", ""], ["Grunwald", "Dirk", ""]]}, {"id": "1711.09731", "submitter": "Douglas Teodoro", "authors": "Sergio Miranda Freire, Luciana Tricai Cavalini, Douglas Teodoro, Erik\n  Sundvall", "title": "Archetypes for Representing Data about the Brazilian Public Hospital\n  Information System and Outpatient High Complexity Procedures System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Brazilian Ministry of Health has selected the openEHR model as a standard\nfor electronic health record systems. This paper presents a set of archetypes\nto represent the main data from the Brazilian Public Hospital Information\nSystem and the High Complexity Procedures Module of the Brazilian public\nOutpatient Health Information System. The archetypes from the public openEHR\nClinical Knowledge Manager (CKM), were examined in order to select archetypes\nthat could be used to represent the data of the above mentioned systems. For\nseveral concepts, it was necessary to specialize the CKM archetypes, or design\nnew ones. A total of 22 archetypes were used: 8 new, 5 specialized and 9 reused\nfrom CKM. This set of archetypes can be used not only for information exchange,\nbut also for generating a big anonymized dataset for testing openEHR-based\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 02:18:53 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Freire", "Sergio Miranda", ""], ["Cavalini", "Luciana Tricai", ""], ["Teodoro", "Douglas", ""], ["Sundvall", "Erik", ""]]}, {"id": "1711.10933", "submitter": "Koninika Pal", "authors": "Koninika Pal and Sebastian Michel", "title": "Learning Interesting Categorical Attributes for Refined Data Exploration", "comments": "13 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes and evaluates a novel approach to determine interesting\ncategorical attributes for lists of entities. Once identified, such categories\nare of immense value to allow constraining (filtering) a current view of a user\nto subsets of entities. We show how a classifier is trained that is able to\ntell whether or not a categorical attribute can act as a constraint, in the\nsense of human-perceived interestingness. The training data is harnessed from\nWeb tables, treating the presence or absence of a table as an indication that\nthe attribute used as a filter constraint is reasonable or not. For learning\nthe classification model, we review four well-known statistical measures\n(features) for categorical attributes---entropy, unalikeability, peculiarity,\nand coverage. We additionally propose three new statistical measures to capture\nthe distribution of data, tailored to our main objective. The learned model is\nevaluated by relevance assessments obtained through a user study, reflecting\nthe applicability of the approach as a whole and, further, demonstrates the\nsuperiority of the proposed diversity measures over existing statistical\nmeasures like information entropy.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:14:52 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Pal", "Koninika", ""], ["Michel", "Sebastian", ""]]}, {"id": "1711.11071", "submitter": "Antonia Korba", "authors": "Antonia Korba", "title": "HSC: A Novel Method for Clustering Hierarchies of Networked Data", "comments": "This is a thesis project, it isn't sufficiently exhaustive", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is one of the most powerful solutions to the problem\nof clustering, on the grounds that it performs a multi scale organization of\nthe data. In recent years, research on hierarchical clustering methods has\nattracted considerable interest due to the demanding modern application\ndomains.\n  We present a novel divisive hierarchical clustering framework called\nHierarchical Stochastic Clustering (HSC), that acts in two stages. In the first\nstage, it finds a primary hierarchy of clustering partitions in a dataset. In\nthe second stage, feeds a clustering algorithm with each one of the clusters of\nthe very detailed partition, in order to settle the final result. The output is\na hierarchy of clusters. Our method is based on the previous research of Meyer\nand Weissel Stochastic Data Clustering and the theory of Simon and Ando on\nVariable Aggregation.\n  Our experiments show that our framework builds a meaningful hierarchy of\nclusters and benefits consistently the clustering algorithm that acts in the\nsecond stage, not only computationally but also in terms of cluster quality.\nThis result suggest that HSC framework is ideal for obtaining hierarchical\nsolutions of large volumes of data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:29:16 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 11:56:28 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Korba", "Antonia", ""]]}, {"id": "1711.11436", "submitter": "Yang Cao", "authors": "Yang Cao, Masatoshi Yoshikawa, Yonghui Xiao, Li Xiong", "title": "Quantifying Differential Privacy in Continuous Data Release under\n  Temporal Correlations", "comments": "accepted in TKDE special issue \"Best of ICDE 2017\". arXiv admin note:\n  substantial text overlap with arXiv:1610.07543", "journal-ref": null, "doi": "10.1109/TKDE.2018.2824328", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential Privacy (DP) has received increasing attention as a rigorous\nprivacy framework. Many existing studies employ traditional DP mechanisms\n(e.g., the Laplace mechanism) as primitives to continuously release private\ndata for protecting privacy at each time point (i.e., event-level privacy),\nwhich assume that the data at different time points are independent, or that\nadversaries do not have knowledge of correlation between data. However,\ncontinuously generated data tend to be temporally correlated, and such\ncorrelations can be acquired by adversaries. In this paper, we investigate the\npotential privacy loss of a traditional DP mechanism under temporal\ncorrelations. First, we analyze the privacy leakage of a DP mechanism under\ntemporal correlation that can be modeled using Markov Chain. Our analysis\nreveals that, the event-level privacy loss of a DP mechanism may\n\\textit{increase over time}. We call the unexpected privacy loss\n\\textit{temporal privacy leakage} (TPL). Although TPL may increase over time,\nwe find that its supremum may exist in some cases. Second, we design efficient\nalgorithms for calculating TPL. Third, we propose data releasing mechanisms\nthat convert any existing DP mechanism into one against TPL. Experiments\nconfirm that our approach is efficient and effective.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 01:25:05 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 16:06:02 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 07:12:53 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Cao", "Yang", ""], ["Yoshikawa", "Masatoshi", ""], ["Xiao", "Yonghui", ""], ["Xiong", "Li", ""]]}, {"id": "1711.11466", "submitter": "Jiawei Zhang", "authors": "Lin Meng, Jiyang Bai, Jiawei Zhang", "title": "LATTE: Application Oriented Social Network Embedding", "comments": "11 Pages, 12 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many research works propose to embed the network structured\ndata into a low-dimensional feature space, where each node is represented as a\nfeature vector. However, due to the detachment of embedding process with\nexternal tasks, the learned embedding results by most existing embedding models\ncan be ineffective for application tasks with specific objectives, e.g.,\ncommunity detection or information diffusion. In this paper, we propose study\nthe application oriented heterogeneous social network embedding problem.\nSignificantly different from the existing works, besides the network structure\npreservation, the problem should also incorporate the objectives of external\napplications in the objective function. To resolve the problem, in this paper,\nwe propose a novel network embedding framework, namely the \"appLicAtion\norienTed neTwork Embedding\" (Latte) model. In Latte, the heterogeneous network\nstructure can be applied to compute the node \"diffusive proximity\" scores,\nwhich capture both local and global network structures. Based on these computed\nscores, Latte learns the network representation feature vectors by extending\nthe autoencoder model model to the heterogeneous network scenario, which can\nalso effectively unite the objectives of network embedding and external\napplication tasks. Extensive experiments have been done on real-world\nheterogeneous social network datasets, and the experimental results have\ndemonstrated the outstanding performance of Latte in learning the\nrepresentation vectors for specific application tasks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 15:44:14 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 02:34:15 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 15:48:40 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Meng", "Lin", ""], ["Bai", "Jiyang", ""], ["Zhang", "Jiawei", ""]]}]