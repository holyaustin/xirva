[{"id": "1302.0103", "submitter": "Florin Rusu", "authors": "Florin Rusu and Yu Cheng", "title": "A Survey on Array Storage, Query Languages, and Systems", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since scientific investigation is one of the most important providers of\nmassive amounts of ordered data, there is a renewed interest in array data\nprocessing in the context of Big Data. To the best of our knowledge, a unified\nresource that summarizes and analyzes array processing research over its long\nexistence is currently missing. In this survey, we provide a guide for past,\npresent, and future research in array processing. The survey is organized along\nthree main topics. Array storage discusses all the aspects related to array\npartitioning into chunks. The identification of a reduced set of array\noperators to form the foundation for an array query language is analyzed across\nmultiple such proposals. Lastly, we survey real systems for array processing.\nThe result is a thorough survey on array data storage and processing that\nshould be consulted by anyone interested in this research topic, independent of\nexperience level. The survey is not complete though. We greatly appreciate\npointers towards any work we might have forgotten to mention.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 07:51:58 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 06:34:36 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Rusu", "Florin", ""], ["Cheng", "Yu", ""]]}, {"id": "1302.0309", "submitter": "Peter Bailis", "authors": "Peter Bailis, Aaron Davidson, Alan Fekete, Ali Ghodsi, Joseph M.\n  Hellerstein, Ion Stoica", "title": "Highly Available Transactions: Virtues and Limitations (Extended\n  Version)", "comments": "Extended version of \"Highly Available Transactions: Virtues and\n  Limitations\" to appear in VLDB 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  To minimize network latency and remain online during server failures and\nnetwork partitions, many modern distributed data storage systems eschew\ntransactional functionality, which provides strong semantic guarantees for\ngroups of multiple operations over multiple data items. In this work, we\nconsider the problem of providing Highly Available Transactions (HATs):\ntransactional guarantees that do not suffer unavailability during system\npartitions or incur high network latency. We introduce a taxonomy of highly\navailable systems and analyze existing ACID isolation and distributed data\nconsistency guarantees to identify which can and cannot be achieved in HAT\nsystems. This unifies the literature on weak transactional isolation, replica\nconsistency, and highly available systems. We analytically and experimentally\nquantify the availability and performance benefits of HATs--often two to three\norders of magnitude over wide-area networks--and discuss their necessary\nsemantic compromises.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 22:29:28 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2013 02:43:30 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Bailis", "Peter", ""], ["Davidson", "Aaron", ""], ["Fekete", "Alan", ""], ["Ghodsi", "Ali", ""], ["Hellerstein", "Joseph M.", ""], ["Stoica", "Ion", ""]]}, {"id": "1302.0337", "submitter": "Leon Abdillah", "authors": "Leon Andretti Abdillah", "title": "Perancangan basisdata sistem informasi penggajian", "comments": "18 pages", "journal-ref": "MATRIK. 8 (2006) 135-152", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this research is to design database scheme of information\nsystem at XYZ University. By using database design methods (conceptual scheme,\nlogical scheme, & physical scheme) the writer designs payroll information\nsystem. The physical scheme is compatible with Borland Delphi Database Engine\nScheme to support the implementation of the I.S. After 3 (three) steps we get 7\n(seven) tables, dan 6 (six) forms. By using this shemce, the system can produce\nseveral reports quickly, accurately, efficiently, and effectively.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 03:36:37 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Abdillah", "Leon Andretti", ""]]}, {"id": "1302.0351", "submitter": "Gaurav Saxena", "authors": "Gaurav Saxena, Ruchi Narula, Manish Mishra", "title": "New Dimension Value Introduction for In-Memory What-If Analysis", "comments": "Changes from previous version: 1. Changed references format 2. Added\n  a few more references 3. Added an algorithm to create sub-cube 4. Modified\n  algorithm to process a query for a few errors 5. Rephrased sentences for\n  clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OLAP systems operate on historical data and provide answers to analysts\nqueries. Recent in-memory implementations provide significant performance\nimprovement for real time ad-hoc analysis. Philosophy and techniques of what-if\nanalysis on data warehouse and in-memory data store based OLAP systems have\nbeen covered in great detail before but exploration of new dimension value\n(attribute) introduction has been limited in the context of what-if analysis.\nWe extend the approach of Andrey Balmin et al of using select modify operator\non data graph to introduce new values for dimensions and measures in a\nread-only in-memory data store as scenarios. Our system constructs scenarios\nwithout materializing the rows and stores the row information as queries. The\nrows associated with the scenarios are constructed as and when required by an\nad-hoc query.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 06:51:20 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2013 03:42:48 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Saxena", "Gaurav", ""], ["Narula", "Ruchi", ""], ["Mishra", "Manish", ""]]}, {"id": "1302.0914", "submitter": "Hung Ngo", "authors": "Hung Q. Ngo and Dung T. Nguyen and Christopher R\\'e and Atri Rudra", "title": "Beyond Worst-Case Analysis for Joins with Minesweeper", "comments": "[This is the full version of our PODS'2014 paper.]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new algorithm, Minesweeper, that is able to satisfy stronger\nruntime guarantees than previous join algorithms (colloquially, `beyond\nworst-case guarantees') for data in indexed search trees. Our first\ncontribution is developing a framework to measure this stronger notion of\ncomplexity, which we call {\\it certificate complexity}, that extends notions of\nBarbay et al. and Demaine et al.; a certificate is a set of propositional\nformulae that certifies that the output is correct. This notion captures a\nnatural class of join algorithms. In addition, the certificate allows us to\ndefine a strictly stronger notion of runtime complexity than traditional\nworst-case guarantees. Our second contribution is to develop a dichotomy\ntheorem for the certificate-based notion of complexity. Roughly, we show that\nMinesweeper evaluates $\\beta$-acyclic queries in time linear in the certificate\nplus the output size, while for any $\\beta$-cyclic query there is some instance\nthat takes superlinear time in the certificate (and for which the output is no\nlarger than the certificate size). We also extend our certificate-complexity\nanalysis to queries with bounded treewidth and the triangle query.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 01:24:12 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2013 18:16:36 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2013 03:35:42 GMT"}, {"version": "v4", "created": "Tue, 3 Dec 2013 14:38:38 GMT"}, {"version": "v5", "created": "Fri, 28 Mar 2014 20:01:57 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Ngo", "Hung Q.", ""], ["Nguyen", "Dung T.", ""], ["R\u00e9", "Christopher", ""], ["Rudra", "Atri", ""]]}, {"id": "1302.0971", "submitter": "Leon Abdillah", "authors": "Leon Andretti Abdillah", "title": "Validasi data dengan menggunakan objek lookup pada borland delphi 7.0", "comments": "16 pages", "journal-ref": "MATRIK. 7 (2005) 1-16", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing an application with some tables must concern the validation of\ninput (specially in Table Child). In order to maximize the accuracy and data\ninput validation. Its called lookup (took data from other dataset). There are\ntwo ways to look up data from Table Parent: 1) Using Objects (DBLookupComboBox\nand DBookupListBox), or 2) Arranging the properties of data types fields (shown\nby using DBGrid). In this article is using Borland Delphi software (Inprise\nproduct). The method is offered using 5 (five) practise steps: 1) Relational\nDatabase Scheme, 2) Form Design, 3) Object DatabasesRelationships Scheme, 4)\nProperties and Field Type Arrangement, and 5) Procedures. The result of this\npaper are: 1) The relationship that using lookup objects are valid, and 2)\nDelphi Lookup Objects can be used for 1-1, 1-N, and M-N relationship.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 09:32:54 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Abdillah", "Leon Andretti", ""]]}, {"id": "1302.1638", "submitter": "Jnanamurthy Hk", "authors": "Jnanamurthy H. K.", "title": "Discovery of Maximal Frequent Item Sets using Subset Creation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is the practice to search large amount of data to discover data\npatterns. Data mining uses mathematical algorithms to group the data and\nevaluate the future events. Association rule is a research area in the field of\nknowledge discovery. Many data mining researchers had improved upon the quality\nof association rule for business development by incorporating influential\nfactors like utility, number of items sold and for the mining of association\ndata patterns. In this paper, we propose an efficient algorithm to find maximal\nfrequent itemset first. Most of the association rule algorithms used to find\nminimal frequent item first, then with the help of minimal frequent itemsets\nderive the maximal frequent itemsets, these methods consume more time to find\nmaximal frequent itemsets. To overcome this problem, we propose a new approach\nto find maximal frequent itemset directly using the concepts of subsets. The\nproposed method is found to be efficient in finding maximal frequent itemsets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2013 04:29:39 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["K.", "Jnanamurthy H.", ""]]}, {"id": "1302.1923", "submitter": "Jixue Liu", "authors": "Jixue Liu and Chengfei Liu and Theo Haerder and Jeffery Xu Yu", "title": "Update XML Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  View update is the problem of translating an update to a view to some updates\nto the source data of the view. In this paper, we show the factors determining\nXML view update translation, propose a translation procedure, and propose\ntranslated updates to the source document for different types of views. We\nfurther show that the translated updates are precise. The proposed solution\nmakes it possible for users who do not have access privileges to the source\ndata to update the source data via a view.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 01:22:54 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Liu", "Jixue", ""], ["Liu", "Chengfei", ""], ["Haerder", "Theo", ""], ["Yu", "Jeffery Xu", ""]]}, {"id": "1302.2654", "submitter": "Murali Mani", "authors": "Murali Mani, Kinnari Shah, Manikanta Gunda", "title": "Enabling Secure Database as a Service using Fully Homomorphic\n  Encryption: Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The database community, at least for the last decade, has been grappling with\nquerying encrypted data, which would enable secure database as a service\nsolutions. A recent breakthrough in the cryptographic community (in 2009)\nrelated to fully homomorphic encryption (FHE) showed that arbitrary computation\non encrypted data is possible. Successful adoption of FHE for query processing\nis, however, still a distant dream, and numerous challenges have to be\naddressed. One challenge is how to perform algebraic query processing of\nencrypted data, where we produce encrypted intermediate results and operations\non encrypted data can be composed. In this paper, we describe our solution for\nalgebraic query processing of encrypted data, and also outline several other\nchallenges that need to be addressed, while also describing the lessons that\ncan be learnt from a decade of work by the database community in querying\nencrypted data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 21:51:00 GMT"}], "update_date": "2013-02-13", "authors_parsed": [["Mani", "Murali", ""], ["Shah", "Kinnari", ""], ["Gunda", "Manikanta", ""]]}, {"id": "1302.2966", "submitter": "Sherif Sakr", "authors": "Sherif Sakr, Anna Liu, Ayman G. Fayoumi", "title": "The Family of MapReduce and Large Scale Data Processing Systems", "comments": "arXiv admin note: text overlap with arXiv:1105.4252 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, the continuous increase of computational power has\nproduced an overwhelming flow of data which has called for a paradigm shift in\nthe computing architecture and large scale data processing mechanisms.\nMapReduce is a simple and powerful programming model that enables easy\ndevelopment of scalable parallel applications to process vast amounts of data\non large clusters of commodity machines. It isolates the application from the\ndetails of running a distributed program such as issues on data distribution,\nscheduling and fault tolerance. However, the original implementation of the\nMapReduce framework had some limitations that have been tackled by many\nresearch efforts in several followup works after its introduction. This article\nprovides a comprehensive survey for a family of approaches and mechanisms of\nlarge scale data processing mechanisms that have been implemented based on the\noriginal idea of the MapReduce framework and are currently gaining a lot of\nmomentum in both research and industrial communities. We also cover a set of\nintroduced systems that have been implemented to provide declarative\nprogramming interfaces on top of the MapReduce framework. In addition, we\nreview several large scale data processing systems that resemble some of the\nideas of the MapReduce framework for different purposes and application\nscenarios. Finally, we discuss some of the future research directions for\nimplementing the next generation of MapReduce-like solutions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 01:35:03 GMT"}], "update_date": "2013-02-14", "authors_parsed": [["Sakr", "Sherif", ""], ["Liu", "Anna", ""], ["Fayoumi", "Ayman G.", ""]]}, {"id": "1302.3860", "submitter": "M\\'arton Trencs\\'eni", "authors": "M\\'arton Trencs\\'eni, Attila Gazs\\'o", "title": "ScalienDB: Designing and Implementing a Distributed Database using Paxos", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ScalienDB is a scalable, replicated database built on top of the Paxos\nalgorithm. It was developed from 2010 to 2012, when the startup backing it\nfailed. This paper discusses the design decisions of the distributed database,\ndescribes interesting parts of the C++ codebase and enumerates lessons learned\nputting ScalienDB into production at a handful of clients. The source code is\navailable on Github under the AGPL license, but it is no longer developed or\nmaintained.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 20:04:10 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Trencs\u00e9ni", "M\u00e1rton", ""], ["Gazs\u00f3", "Attila", ""]]}, {"id": "1302.4168", "submitter": "Ashwin Kayyoor", "authors": "K. Ashwin Kumar, Amol Deshpande, Samir Khuller", "title": "Data Placement and Replica Selection for Improving Co-location in\n  Distributed Environments", "comments": "12 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing need for large-scale data analytics in a number of application\ndomains has led to a dramatic rise in the number of distributed data management\nsystems, both parallel relational databases, and systems that support\nalternative frameworks like MapReduce. There is thus an increasing contention\non scarce data center resources like network bandwidth; further, the energy\nrequirements for powering the computing equipment are also growing\ndramatically. As we show empirically, increasing the execution parallelism by\nspreading out data across a large number of machines may achieve the intended\ngoal of decreasing query latencies, but in most cases, may increase the total\nresource and energy consumption significantly. For many analytical workloads,\nhowever, minimizing query latencies is often not critical; in such scenarios,\nwe argue that we should instead focus on minimizing the average query span,\ni.e., the average number of machines that are involved in processing of a\nquery, through colocation of data items that are frequently accessed together.\nIn this work, we exploit the fact that most distributed environments need to\nuse replication for fault tolerance, and we devise workload-driven replica\nselection and placement algorithms that attempt to minimize the average query\nspan. We model a historical query workload trace as a hypergraph over a set of\ndata items, and formulate and analyze the problem of replica placement by\ndrawing connections to several well-studied graph theoretic concepts. We\ndevelop a series of algorithms to decide which data items to replicate, and\nwhere to place the replicas. We show effectiveness of our proposed approach by\npresenting results on a collection of synthetic and real workloads. Our\nexperiments show that careful data placement and replication can dramatically\nreduce the average query spans resulting in significant reductions in the\nresource consumption.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 07:09:14 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Kumar", "K. Ashwin", ""], ["Deshpande", "Amol", ""], ["Khuller", "Samir", ""]]}, {"id": "1302.4462", "submitter": "Oscar Martinez Rubi", "authors": "O. Martinez-Rubi, V. K. Veligatla, A. G. de Bruyn, P. Lampropoulos, A.\n  R. Offringa, V. Jelic, S. Yatawatta, L. V. E. Koopmans, and S. Zaroubi", "title": "LEDDB: LOFAR Epoch of Reionization Diagnostic Database", "comments": "ADASS XXII proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key science projects of the Low-Frequency Array (LOFAR) is the\ndetection of the cosmological signal coming from the Epoch of Reionization\n(EoR). Here we present the LOFAR EoR Diagnostic Database (LEDDB) that is used\nin the storage, management, processing and analysis of the LOFAR EoR\nobservations. It stores referencing information of the observations and\ndiagnostic parameters extracted from their calibration. This stored data is\nused to ease the pipeline processing, monitor the performance of the telescope\nand visualize the diagnostic parameters which facilitates the analysis of the\nseveral contamination effects on the signals. It is implemented with PostgreSQL\nand accessed through the psycopg2 python module. We have developed a very\nflexible query engine, which is used by a web user interface to access the\ndatabase, and a very extensive set of tools for the visualization of the\ndiagnostic parameters through all their multiple dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 21:21:35 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Martinez-Rubi", "O.", ""], ["Veligatla", "V. K.", ""], ["de Bruyn", "A. G.", ""], ["Lampropoulos", "P.", ""], ["Offringa", "A. R.", ""], ["Jelic", "V.", ""], ["Yatawatta", "S.", ""], ["Koopmans", "L. V. E.", ""], ["Zaroubi", "S.", ""]]}, {"id": "1302.5302", "submitter": "Jimmy Lin", "authors": "Nima Asadi, Jimmy Lin, and Michael Busch", "title": "Dynamic Memory Allocation Policies for Postings in Real-Time Twitter\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a real-time Twitter search application where tweets are arriving\nat a rate of several thousands per second. Real-time search demands that they\nbe indexed and searchable immediately, which leads to a number of\nimplementation challenges. In this paper, we focus on one aspect: dynamic\npostings allocation policies for index structures that are completely held in\nmain memory. The core issue can be characterized as a \"Goldilocks Problem\".\nBecause memory remains today a scare resource, an allocation policy that is too\naggressive leads to inefficient utilization, while a policy that is too\nconservative is slow and leads to fragmented postings lists. We present a\ndynamic postings allocation policy that allocates memory in increasingly-larger\n\"slices\" from a small number of large, fixed pools of memory. Through\nanalytical models and experiments, we explore different settings that balance\ntime (query evaluation speed) and space (memory utilization).\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 15:24:53 GMT"}], "update_date": "2013-02-22", "authors_parsed": [["Asadi", "Nima", ""], ["Lin", "Jimmy", ""], ["Busch", "Michael", ""]]}, {"id": "1302.5549", "submitter": "Georgia Koloniari", "authors": "Georgia Koloniari, Dimitris Souravlias, Evaggelia Pitoura", "title": "On Graph Deltas for Historical Queries", "comments": "6 pages, 1 figure, WOSS 2012, Istanbul, Turkey", "journal-ref": "Proceedings of 1st Workshop on Online Social Systems (WOSS) 2012,\n  in conjunction with VLDB 2012", "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of evaluating historical queries on\ngraphs. To this end, we investigate the use of graph deltas, i.e., a log of\ntime-annotated graph operations. Our storage model maintains the current graph\nsnapshot and the delta. We reconstruct past snapshots by applying appropriate\nparts of the graph delta on the current snapshot. Query evaluation proceeds on\nthe reconstructed snapshots but we also propose algorithms based mostly on\ndeltas for efficiency. We introduce various techniques for improving\nperformance, including materializing intermediate snapshots, partial\nreconstruction and indexing deltas.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 11:01:11 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Koloniari", "Georgia", ""], ["Souravlias", "Dimitris", ""], ["Pitoura", "Evaggelia", ""]]}, {"id": "1302.6556", "submitter": "Theodoros Rekatsinas", "authors": "Theodoros Rekatsinas, Amol Deshpande, Ashwin Machanavajjhala", "title": "On Sharing Private Data with Multiple Non-Colluding Adversaries", "comments": "14 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SPARSI, a theoretical framework for partitioning sensitive data\nacross multiple non-colluding adversaries. Most work in privacy-aware data\nsharing has considered disclosing summaries where the aggregate information\nabout the data is preserved, but sensitive user information is protected.\nNonetheless, there are applications, including online advertising, cloud\ncomputing and crowdsourcing markets, where detailed and fine-grained user-data\nmust be disclosed. We consider a new data sharing paradigm and introduce the\nproblem of privacy-aware data partitioning, where a sensitive dataset must be\npartitioned among k untrusted parties (adversaries). The goal is to maximize\nthe utility derived by partitioning and distributing the dataset, while\nminimizing the amount of sensitive information disclosed. The data should be\ndistributed so that an adversary, without colluding with other adversaries,\ncannot draw additional inferences about the private information, by linking\ntogether multiple pieces of information released to her. The assumption of no\ncollusion is both reasonable and necessary in the above application domains\nthat require release of private user information. SPARSI enables us to formally\ndefine privacy-aware data partitioning using the notion of sensitive properties\nfor modeling private information and a hypergraph representation for describing\nthe interdependencies between data entries and private information. We show\nthat solving privacy-aware partitioning is, in general, NP-hard, but for\nspecific information disclosure functions, good approximate solutions can be\nfound using relaxation techniques. Finally, we present a local search algorithm\napplicable to generic information disclosure functions. We apply SPARSI\ntogether with the proposed algorithms on data from a real advertising scenario\nand show that we can partition data with no disclosure to any single\nadvertiser.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 19:49:55 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2013 20:48:52 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2013 15:41:40 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Rekatsinas", "Theodoros", ""], ["Deshpande", "Amol", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "1302.7039", "submitter": "Mounira Taileb", "authors": "Mounira Taileb", "title": "Content Based Image Retrieval System Using NOHIS-tree", "comments": "6 pages, 10th International Conference on Advances in Mobile\n  Computing & Multimedia (MoMM2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval (CBIR) has been one of the most important\nresearch areas in computer vision. It is a widely used method for searching\nimages in huge databases. In this paper we present a CBIR system called\nNOHIS-Search. The system is based on the indexing technique NOHIS-tree. The two\nphases of the system are described and the performance of the system is\nillustrated with the image database ImagEval. NOHIS-Search system was compared\nto other two CBIR systems; the first that using PDDP indexing algorithm and the\nsecond system is that using the sequential search. Results show that\nNOHIS-Search system outperforms the two other systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 00:21:38 GMT"}], "update_date": "2013-03-01", "authors_parsed": [["Taileb", "Mounira", ""]]}]