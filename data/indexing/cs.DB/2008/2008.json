[{"id": "2008.00129", "submitter": "Austin Silveria", "authors": "Austin Silveria", "title": "GraphQL Live Querying with DynamoDB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of implementing GraphQL live queries at the database\nlevel. Our DynamoDB simulation in Go mimics a distributed key-value store and\nimplements live queries to expose possible pitfalls. Two key components for\nimplementing live queries are storing fields selected in a live query and\ndetermining which object fields have been updated in each database write. A\nstream(key, fields) request to the system contains fields to include in the\nlive query stream and on subsequent put(key, object) operations, the database\nasynchronously determines which fields were updated and pushes a new query view\nto the stream if those fields overlap with the stream() request. Following a\ndiscussion of our implementation, we explore motivations for using live queries\nsuch as simplifying software communication, minimizing data transfer, and\nenabling real-time data and describe an architecture for building software with\nGraphQL and live queries.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 00:15:44 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Silveria", "Austin", ""]]}, {"id": "2008.00297", "submitter": "Evgenios Kornaropoulos", "authors": "Evgenios M. Kornaropoulos, Silei Ren, Roberto Tamassia", "title": "The Price of Tailoring the Index to Your Data: Poisoning Attacks on\n  Learned Index Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of learned index structures relies on the idea that the\ninput-output functionality of a database index can be viewed as a prediction\ntask and, thus, be implemented using a machine learning model instead of\ntraditional algorithmic techniques. This novel angle for a decades-old problem\nhas inspired numerous exciting results in the intersection of machine learning\nand data structures. However, the main advantage of learned index structures,\ni.e., the ability to adjust to the data at hand via the underlying ML-model,\ncan become a disadvantage from a security perspective as it could be exploited.\n  In this work, we present the first study of poisoning attacks on learned\nindex structures. The required poisoning approach is different from all\nprevious works since the model under attack is trained on a cumulative\ndistribution function (CDF) and, thus, every injection on the training set has\na cascading impact on multiple data values. We formulate the first poisoning\nattacks on linear regression models trained on the CDF, which is a basic\nbuilding block of the proposed learned index structures. We generalize our\npoisoning techniques to attack a more advanced two-stage design of learned\nindex structures called recursive model index (RMI), which has been shown to\noutperform traditional B-Trees. We evaluate our attacks on real-world and\nsynthetic datasets under a wide variety of parameterizations of the model and\nshow that the error of the RMI increases up to $300\\times$ and the error of its\nsecond-stage models increases up to $3000\\times$.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 17:12:04 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kornaropoulos", "Evgenios M.", ""], ["Ren", "Silei", ""], ["Tamassia", "Roberto", ""]]}, {"id": "2008.00358", "submitter": "Yuyan Wang", "authors": "Benjamin Moseley, Kirk Pruhs, Alireza Samadian, Yuyan Wang", "title": "Relational Algorithms for k-means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a k-means approximation algorithm that is efficient in the\nrelational algorithms model. This is an algorithm that operates directly on a\nrelational database without performing a join to convert it to a matrix whose\nrows represent the data points. The running time is potentially exponentially\nsmaller than $N$, the number of data points to be clustered that the relational\ndatabase represents.\n  Few relational algorithms are known and this paper offers techniques for\ndesigning relational algorithms as well as characterizing their limitations. We\nshow that given two data points as cluster centers, if we cluster points\naccording to their closest centers, it is NP-Hard to approximate the number of\npoints in the clusters on a general relational input. This is trivial for\nconventional data inputs and this result exemplifies that standard algorithmic\ntechniques may not be directly applied when designing an efficient relational\nalgorithm. This paper then introduces a new method that leverages rejection\nsampling and the $k$-means++ algorithm to construct an O(1)-approximate k-means\nsolution.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 23:21:40 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 22:18:08 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Moseley", "Benjamin", ""], ["Pruhs", "Kirk", ""], ["Samadian", "Alireza", ""], ["Wang", "Yuyan", ""]]}, {"id": "2008.00368", "submitter": "Fei Chiang", "authors": "Yu Huang, Mostafa Milani, Fei Chiang", "title": "Privacy-Aware Data Cleaning-as-a-Service (Extended Version)", "comments": null, "journal-ref": null, "doi": "10.1016/j.is.2020.101608", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data cleaning is a pervasive problem for organizations as they try to reap\nvalue from their data. Recent advances in networking and cloud computing\ntechnology have fueled a new computing paradigm called Database-as-a-Service,\nwhere data management tasks are outsourced to large service providers. In this\npaper, we consider a Data Cleaning-as-a-Service model that allows a client to\ninteract with a data cleaning provider who hosts curated, and sensitive data.\nWe present PACAS: a Privacy-Aware data Cleaning-As-a-Service model that\nfacilitates interaction between the parties with client query requests for\ndata, and a service provider using a data pricing scheme that computes prices\naccording to data sensitivity. We propose new extensions to the model to define\ngeneralized data repairs that obfuscate sensitive data to allow data sharing\nbetween the client and service provider. We present a new semantic distance\nmeasure to quantify the utility of such repairs, and we re-define the notion of\nconsistency in the presence of generalized values. The PACAS model uses\n(X,Y,L)-anonymity that extends existing data publishing techniques to consider\nthe semantics in the data while protecting sensitive values. Our evaluation\nover real data show that PACAS safeguards semantically related sensitive\nvalues, and provides lower repair errors compared to existing privacy-aware\ncleaning techniques.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 00:33:43 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Huang", "Yu", ""], ["Milani", "Mostafa", ""], ["Chiang", "Fei", ""]]}, {"id": "2008.00842", "submitter": "Paris Carbone", "authors": "Marios Fragkoulis, Paris Carbone, Vasiliki Kalavri, Asterios\n  Katsifodimos", "title": "A Survey on the Evolution of Stream Processing Systems", "comments": "34 pages, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream processing has been an active research field for more than 20 years,\nbut it is now witnessing its prime time due to recent successful efforts by the\nresearch community and numerous worldwide open-source communities. This survey\nprovides a comprehensive overview of fundamental aspects of stream processing\nsystems and their evolution in the functional areas of out-of-order data\nmanagement, state management, fault tolerance, high availability, load\nmanagement, elasticity, and reconfiguration. We review noteworthy past research\nfindings, outline the similarities and differences between early ('00-'10) and\nmodern ('11-'18) streaming systems, and discuss recent trends and open\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:43:46 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Fragkoulis", "Marios", ""], ["Carbone", "Paris", ""], ["Kalavri", "Vasiliki", ""], ["Katsifodimos", "Asterios", ""]]}, {"id": "2008.00896", "submitter": "Batya Kenig", "authors": "Batya Kenig and Dan Suciu", "title": "A Dichotomy for the Generalized Model Counting Problem for Unions of\n  Conjunctive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the $generalized~model~counting~problem$, defined as follows: given\na database, and a set of deterministic tuples, count the number of subsets of\nthe database that include all deterministic tuples and satisfy the query. This\nproblem is computationally equivalent to the evaluation of the query over a\ntuple-independent probabilistic database where all tuples have probabilities in\n$\\{0,\\frac{1}{2},1\\}$. Previous work has established a dichotomy for Unions of\nConjunctive Queries (UCQ) when the probabilities are arbitrary rational\nnumbers, showing that, for each query, its complexity is either in polynomial\ntime or #P-hard. The query is called $safe$ in the first case, and $unsafe$ in\nthe second case. Here, we strengthen the hardness proof, by proving that an\nunsafe UCQ query remains #P-hard even if the probabilities are restricted to\n$\\{0,\\frac{1}{2},1\\}$. This requires a complete redesign of the hardness proof,\nusing new techniques. A related problem is the $model~counting~problem$, which\nasks for the probability of the query when the input probabilities are\nrestricted to $\\{0,\\frac{1}{2}\\}$. While our result does not extend to model\ncounting for all unsafe UCQs, we prove that model counting is #P-hard for a\nclass of unsafe queries called Type-I forbidden queries.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 14:24:08 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 16:11:20 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 07:40:31 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Kenig", "Batya", ""], ["Suciu", "Dan", ""]]}, {"id": "2008.01208", "submitter": "Bahar Ghadiri Bashardoost", "authors": "Bahar Ghadiri Bashardoost, Ren\\'ee J. Miller, Kelly Lyons, Fatemeh\n  Nargesian", "title": "Knowledge Translation: Extended Technical Report", "comments": "Extended technical report of \"Knowledge Translation\" paper, accepted\n  in VLDB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Kensho, a tool for generating mapping rules between two\nKnowledge Bases (KBs). To create the mapping rules, Kensho starts with a set of\ncorrespondences and enriches them with additional semantic information\nautomatically identified from the structure and constraints of the KBs. Our\napproach works in two phases. In the first phase, semantic associations between\nresources of each KB are captured. In the second phase, mapping rules are\ngenerated by interpreting the correspondences in a way that respects the\ndiscovered semantic associations among elements of each KB. Kensho's mapping\nrules are expressed using SPARQL queries and can be used directly to exchange\nknowledge from source to target. Kensho is able to automatically rank the\ngenerated mapping rules using a set of heuristics. We present an experimental\nevaluation of Kensho and assess our mapping generation and ranking strategies\nusing more than 50 synthesized and real world settings, chosen to showcase some\nof the most important applications of knowledge translation. In addition, we\nuse three existing benchmarks to demonstrate Kensho's ability to deal with\ndifferent mapping scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:37:38 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Bashardoost", "Bahar Ghadiri", ""], ["Miller", "Ren\u00e9e J.", ""], ["Lyons", "Kelly", ""], ["Nargesian", "Fatemeh", ""]]}, {"id": "2008.02108", "submitter": "Simona Rombo", "authors": "Mariella Bonomo and Armando La Placa and Simona E. Rombo", "title": "Identifying the $k$ Best Targets for an Advertisement Campaign via\n  Online Social Networks", "comments": "Accepted for publication in Proceedings of the 12th International\n  Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge\n  Management (KDIR2020). arXiv admin note: text overlap with arXiv:1907.01326", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for the recommendation of possible customers\n(users) to advertisers (e.g., brands) based on two main aspects: (i) the\ncomparison between On-line Social Network profiles, and (ii) neighborhood\nanalysis on the On-line Social Network. Profile matching between users and\nbrands is considered based on bag-of-words representation of textual contents\ncoming from the social media, and measures such as the Term Frequency-Inverse\nDocument Frequency are used in order to characterize the importance of words in\nthe comparison. The approach has been implemented relying on Big Data\nTechnologies, allowing this way the efficient analysis of very large Online\nSocial Networks. Results on real datasets show that the combination of profile\nmatching and neighborhood analysis is successful in identifying the most\nsuitable set of users to be used as target for a given advertisement campaign.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:52:26 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Bonomo", "Mariella", ""], ["La Placa", "Armando", ""], ["Rombo", "Simona E.", ""]]}, {"id": "2008.02352", "submitter": "Ashwini Raina", "authors": "Ashwini Raina, Asaf Cidon, Kyle Jamieson, Michael J. Freedman", "title": "PrismDB: Read-aware Log-structured Merge Trees for Heterogeneous Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, emerging hardware storage technologies have focused on\ndivergent goals: better performance or lower cost-per-bit of storage.\nCorrespondingly, data systems that employ these new technologies are optimized\neither to be fast (but expensive) or cheap (but slow). We take a different\napproach: by combining multiple tiers of fast and low-cost storage technologies\nwithin the same system, we can achieve a Pareto-efficient balance between\nperformance and cost-per-bit.\n  This paper presents the design and implementation of PrismDB, a novel\nlog-structured merge tree based key-value store that exploits a full spectrum\nof heterogeneous storage technologies (from 3D XPoint to QLC NAND). We\nintroduce the notion of \"read-awareness\" to log-structured merge trees, which\nallows hot objects to be pinned to faster storage, achieving better tiering and\nhot-cold separation of objects. Compared to the standard use of RocksDB on\nflash in datacenters today, PrismDB's average throughput on heterogeneous\nstorage is 2.3$\\times$ faster and its tail latency is more than an order of\nmagnitude better, using hardware than is half the cost.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 20:34:47 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 04:05:18 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Raina", "Ashwini", ""], ["Cidon", "Asaf", ""], ["Jamieson", "Kyle", ""], ["Freedman", "Michael J.", ""]]}, {"id": "2008.03053", "submitter": "Xuliang Zhu", "authors": "Xuliang Zhu, Xin Huang, Byron Choi, Jianliang Xu, William K. Cheung,\n  Yanchun Zhang, and Jiming Liu", "title": "Ontology-based Graph Visualization for Summarized View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data summarization that presents a small subset of a dataset to users has\nbeen widely applied in numerous applications and systems. Many datasets are\ncoded with hierarchical terminologies, e.g., the international classification\nof Diseases-9, Medical Subject Heading, and Gene Ontology, to name a few. In\nthis paper, we study the problem of selecting a diverse set of k elements to\nsummarize an input dataset with hierarchical terminologies, and visualize the\nsummary in an ontology structure. We propose an efficient greedy algorithm to\nsolve the problem with (1-1/e) = 62% -approximation guarantee. Although this\ngreedy solution achieves quality-guaranteed answers approximately but it is\nstill not optimal. To tackle the problem optimally, we further develop a\ndynamic programming algorithm to obtain optimal answers for graph visualization\nof log-data using ontology terminologies called OVDO . The complexity and\ncorrectness of OVDO are theoretically analyzed. In addition, we propose a\nuseful optimization technique of tree reduction to remove useless nodes with\nzero weights and shrink the tree into a smaller one, which ensures the\nefficiency acceleration of OVDO in many real-world applications. Extensive\nexperimental results on real-world datasets show the effectiveness and\nefficiency of our proposed approximate and exact algorithms for tree data\nsummarization.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 09:19:50 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Zhu", "Xuliang", ""], ["Huang", "Xin", ""], ["Choi", "Byron", ""], ["Xu", "Jianliang", ""], ["Cheung", "William K.", ""], ["Zhang", "Yanchun", ""], ["Liu", "Jiming", ""]]}, {"id": "2008.03230", "submitter": "Shohreh Deldari", "authors": "Shohreh Deldari, Daniel V. Smith, Amin Sadri, Flora D. Salim", "title": "ESPRESSO: Entropy and ShaPe awaRe timE-Series SegmentatiOn for\n  processing heterogeneous sensor data", "comments": "23 pages, 11 figures, accepted at IMWUT Volume(4) issue(3)", "journal-ref": null, "doi": "10.1145/3411832", "report-no": null, "categories": "cs.LG cs.CV cs.DB cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting informative and meaningful temporal segments from high-dimensional\nwearable sensor data, smart devices, or IoT data is a vital preprocessing step\nin applications such as Human Activity Recognition (HAR), trajectory\nprediction, gesture recognition, and lifelogging. In this paper, we propose\nESPRESSO (Entropy and ShaPe awaRe timE-Series SegmentatiOn), a hybrid\nsegmentation model for multi-dimensional time-series that is formulated to\nexploit the entropy and temporal shape properties of time-series. ESPRESSO\ndiffers from existing methods that focus upon particular statistical or\ntemporal properties of time-series exclusively. As part of model development, a\nnovel temporal representation of time-series $WCAC$ was introduced along with a\ngreedy search approach that estimate segments based upon the entropy metric.\nESPRESSO was shown to offer superior performance to four state-of-the-art\nmethods across seven public datasets of wearable and wear-free sensing. In\naddition, we undertake a deeper investigation of these datasets to understand\nhow ESPRESSO and its constituent methods perform with respect to different\ndataset characteristics. Finally, we provide two interesting case-studies to\nshow how applying ESPRESSO can assist in inferring daily activity routines and\nthe emotional state of humans.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 10:41:20 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Deldari", "Shohreh", ""], ["Smith", "Daniel V.", ""], ["Sadri", "Amin", ""], ["Salim", "Flora D.", ""]]}, {"id": "2008.03260", "submitter": "Anshumali Shrivastava", "authors": "Nicholas Meisburger, Anshumali Shrivastava", "title": "Distributed Tera-Scale Similarity Search with MPI: Provably Efficient\n  Similarity Search over billions without a Single Distance Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SLASH (Sketched LocAlity Sensitive Hashing), an MPI (Message\nPassing Interface) based distributed system for approximate similarity search\nover terabyte scale datasets. SLASH provides a multi-node implementation of the\npopular LSH (locality sensitive hashing) algorithm, which is generally\nimplemented on a single machine. We show how we can append the LSH algorithm\nwith heavy hitters sketches to provably solve the (high) similarity search\nproblem without a single distance computation. Overall, we mathematically show\nthat, under realistic data assumptions, we can identify the near-neighbor of a\ngiven query $q$ in sub-linear ($ \\ll O(n)$) number of simple sketch aggregation\noperations only. To make such a system practical, we offer a novel design and\nsketching solution to reduce the inter-machine communication overheads\nexponentially. In a direct comparison on comparable hardware, SLASH is more\nthan 10000x faster than the popular LSH package in PySpark. PySpark is a\nwidely-adopted distributed implementation of the LSH algorithm for large\ndatasets and is deployed in commercial platforms. In the end, we show how our\nsystem scale to Tera-scale Criteo dataset with more than 4 billion samples.\nSLASH can index this 2.3 terabyte data over 20 nodes in under an hour, with\nquery times in a fraction of milliseconds. To the best of our knowledge, there\nis no open-source system that can index and perform a similarity search on\nCriteo with a commodity cluster.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 18:15:36 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 22:48:52 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Meisburger", "Nicholas", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2008.03397", "submitter": "Lana Yeganova", "authors": "Lana Yeganova, Rezarta Islamaj, Qingyu Chen, Robert Leaman, Alexis\n  Allot, Chin-Hsuan Wei, Donald C. Comeau, Won Kim, Yifan Peng, W. John Wilbur,\n  Zhiyong Lu", "title": "Navigating the landscape of COVID-19 research through literature\n  analysis: A bird's eye view", "comments": "10 pages, 8 Figures, Submitted to KDD 2020 Health Day", "journal-ref": "KDD 2020 Health Day: AI for COVID, August 23-27, 2020, Virtual\n  Conference, CA, US", "doi": null, "report-no": null, "categories": "cs.DL cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely access to accurate scientific literature in the battle with the\nongoing COVID-19 pandemic is critical. This unprecedented public health risk\nhas motivated research towards understanding the disease in general,\nidentifying drugs to treat the disease, developing potential vaccines, etc.\nThis has given rise to a rapidly growing body of literature that doubles in\nnumber of publications every 20 days as of May 2020. Providing medical\nprofessionals with means to quickly analyze the literature and discover growing\nareas of knowledge is necessary for addressing their question and information\nneeds.\n  In this study we analyze the LitCovid collection, 13,369 COVID-19 related\narticles found in PubMed as of May 15th, 2020 with the purpose of examining the\nlandscape of literature and presenting it in a format that facilitates\ninformation navigation and understanding. We do that by applying\nstate-of-the-art named entity recognition, classification, clustering and other\nNLP techniques. By applying NER tools, we capture relevant bioentities (such as\ndiseases, internal body organs, etc.) and assess the strength of their\nrelationship with COVID-19 by the extent they are discussed in the corpus. We\nalso collect a variety of symptoms and co-morbidities discussed in reference to\nCOVID-19. Our clustering algorithm identifies topics represented by groups of\nrelated terms, and computes clusters corresponding to documents associated with\nthe topic terms. Among the topics we observe several that persist through the\nduration of multiple weeks and have numerous associated documents, as well\nseveral that appear as emerging topics with fewer documents. All the tools and\ndata are publicly available, and this framework can be applied to any\nliterature collection. Taken together, these analyses produce a comprehensive,\nsynthesized view of COVID-19 research to facilitate knowledge discovery from\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 23:39:29 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 21:01:27 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yeganova", "Lana", ""], ["Islamaj", "Rezarta", ""], ["Chen", "Qingyu", ""], ["Leaman", "Robert", ""], ["Allot", "Alexis", ""], ["Wei", "Chin-Hsuan", ""], ["Comeau", "Donald C.", ""], ["Kim", "Won", ""], ["Peng", "Yifan", ""], ["Wilbur", "W. John", ""], ["Lu", "Zhiyong", ""]]}, {"id": "2008.03545", "submitter": "Tipawan Silwattananusarn", "authors": "Tipawan Silwattananusarn and Pachisa Kulkanjanapiban", "title": "Mining and Analyzing Patron's Book-Loan Data and University Data to\n  Understand Library Use Patterns", "comments": "22 pages, 9 figures", "journal-ref": "International Journal of Information Science and Management,\n  Vol.18 No.2.(2020)", "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of this paper is to study the patron's usage behavior in an\nacademic library. This study investigates on pattern of patron's books\nborrowing in Khunying Long Athakravisunthorn Learning Resources Center, Prince\nof Songkla University that influence patron's academic achievement during on\nacademic year 2015-2018. The study collected and analyzed data from the\nlibraries, registrar, and human resources. The students' performance data was\nobtained from PSU Student Information System and the rest from ALIST library\ninformation system. WEKA was used as the data mining tool employing data mining\ntechniques of association rules and clustering. All data sets were mined and\nanalyzed to identify characteristics of the patron's book borrowing, to\ndiscover the association rules of patron's interest, and to analyze the\nrelationships between academic library use and undergraduate students'\nachievement. The results reveal patterns of patron's book loan behavior,\npatterns of book usage, patterns of interest rules with respect to patron's\ninterest in book borrowing, and patterns of relationships between patron's\nborrowing and their grade. The ability to clearly identify and describe library\npatron's behavior pattern can help library in managing resources and services\nmore effectively. This study provides a sample model as guideline or campus\npartnerships and for future collaborations that will take advantage of the\nacademic library information and data mining to improve library management and\nlibrary services.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 15:46:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Silwattananusarn", "Tipawan", ""], ["Kulkanjanapiban", "Pachisa", ""]]}, {"id": "2008.03719", "submitter": "Daniel Ritter", "authors": "Daniel Ritter and Jan Bro{\\ss}", "title": "A Rule-based Language for Application Integration", "comments": "14 pages, work from 2013/14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although message-based (business) application integration is based on\norchestrated message flows, current modeling languages exclusively cover (parts\nof) the control flow, while under-specifying the data flow. Especially for more\ndata-intensive integration scenarios, this fact adds to the inherent data\nprocessing weakness in conventional integration systems.\n  We argue that with a more data-centric integration language and a relational\nlogic based implementation of integration semantics, optimizations from the\ndata management domain(e.g., data partitioning, parallelization) can be\ncombined with common integration processing (e.g., scatter/gather,\nsplitter/gather). With the Logic Integration Language (LiLa) we redefine\nintegration logic tailored for data-intensive processing and propose a novel\napproach to data-centric integration modeling, from which we derive the\ncontrol-and data flow and apply them to a conventional integration system.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 13:02:12 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ritter", "Daniel", ""], ["Bro\u00df", "Jan", ""]]}, {"id": "2008.03891", "submitter": "Stephen Macke", "authors": "Stephen Macke, Maryam Aliakbarpour, Ilias Diakonikolas, Aditya\n  Parameswaran, Ronitt Rubinfeld", "title": "Rapid Approximate Aggregation with Distribution-Sensitive Interval\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregating data is fundamental to data analytics, data exploration, and\nOLAP. Approximate query processing (AQP) techniques are often used to\naccelerate computation of aggregates using samples, for which confidence\nintervals (CIs) are widely used to quantify the associated error. CIs used in\npractice fall into two categories: techniques that are tight but not correct,\ni.e., they yield tight intervals but only offer asymptotic guarantees, making\nthem unreliable, or techniques that are correct but not tight, i.e., they offer\nrigorous guarantees, but are overly conservative, leading to confidence\nintervals that are too loose to be useful. In this paper, we develop a CI\ntechnique that is both correct and tighter than traditional approaches.\nStarting from conservative CIs, we identify two issues they often face:\npessimistic mass allocation (PMA) and phantom outlier sensitivity (PHOS). By\ndeveloping a novel range-trimming technique for eliminating PHOS and pairing it\nwith known CI techniques without PMA, we develop a technique for computing CIs\nwith strong guarantees that requires fewer samples for the same width. We\nimplement our techniques underneath a sampling-optimized in-memory column store\nand show how to accelerate queries involving aggregates on a real dataset with\nspeedups of up to 124x over traditional AQP-with-guarantees and more than 1000x\nover exact methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 04:01:48 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Macke", "Stephen", ""], ["Aliakbarpour", "Maryam", ""], ["Diakonikolas", "Ilias", ""], ["Parameswaran", "Aditya", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "2008.04045", "submitter": "Giorgos Santipantakis", "authors": "Georgios M. Santipantakis and George A. Vouros and Christos\n  Doulkeridis", "title": "Towards Integrated and Open COVID-19 Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the global unrest related to the COVID-19 pandemic, we present a\nsystem prototype for ontology-based, integration of national data published\nfrom various countries. COVID-related data is published from different\nauthorities, in different formats, at varying spatio-temporal granularity, and\nirregularly. Consequently, this hinders the joint data exploration and\nexploitation, which could lead scientists to acquire important insights,\nwithout having to deal with the cumbersome task of data acquisition and\nintegration. Motivated by this shortcoming, we propose an approach for data\nacquisition, ontology-based data representation, and data transformation to\nRDF, which also enables interlinking with other publicly available data\nsources. Currently, data coming from the following European countries has been\nsuccessfully integrated: Austria, Belgium, France, Germany, Greece, Italy, and\nSweden. The knowledge base is automatically being updated, and it is available\nto the public through a SPARQL endpoint and a direct download link.\nFurthermore, we showcase how data integration enables spatio-temporal data\nanalysis and knowledge discovery, by means of meaningful queries that would not\nbe feasible to process otherwise.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 23:37:31 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Santipantakis", "Georgios M.", ""], ["Vouros", "George A.", ""], ["Doulkeridis", "Christos", ""]]}, {"id": "2008.04054", "submitter": "Kai Wang", "authors": "Yizhang He, Kai Wang, Wenjie Zhang, Xuemin Lin, Ying Zhang", "title": "Exploring Cohesive Subgraphs with Vertex Engagement and Tie Strength in\n  Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel cohesive subgraph model called $\\tau$-strengthened\n$(\\alpha,\\beta)$-core (denoted as $(\\alpha,\\beta)_{\\tau}$-core), which is the\nfirst to consider both tie strength and vertex engagement on bipartite graphs.\nAn edge is a strong tie if contained in at least $\\tau$ butterflies\n($2\\times2$-bicliques). $(\\alpha,\\beta)_{\\tau}$-core requires each vertex on\nthe upper or lower level to have at least $\\alpha$ or $\\beta$ strong ties,\ngiven strength level $\\tau$. To retrieve the vertices of\n$(\\alpha,\\beta)_{\\tau}$-core optimally, we construct index\n$I_{\\alpha,\\beta,\\tau}$ to store all $(\\alpha,\\beta)_{\\tau}$-cores. Effective\noptimization techniques are proposed to improve index construction. To make our\nidea practical on large graphs, we propose 2D-indexes $I_{\\alpha,\\beta},\nI_{\\beta,\\tau}$, and $I_{\\alpha,\\tau}$ that selectively store the vertices of\n$(\\alpha,\\beta)_{\\tau}$-core for some $\\alpha,\\beta$, and $\\tau$. The\n2D-indexes are more space-efficient and require less construction time, each of\nwhich can support $(\\alpha,\\beta)_{\\tau}$-core queries. As query efficiency\ndepends on input parameters and the choice of 2D-index, we propose a\nlearning-based hybrid computation paradigm by training a feed-forward neural\nnetwork to predict the optimal choice of 2D-index that minimizes the query\ntime. Extensive experiments show that ($1$) $(\\alpha,\\beta)_{\\tau}$-core is an\neffective model capturing unique and important cohesive subgraphs; ($2$) the\nproposed techniques significantly improve the efficiency of index construction\nand query processing.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:12:51 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["He", "Yizhang", ""], ["Wang", "Kai", ""], ["Zhang", "Wenjie", ""], ["Lin", "Xuemin", ""], ["Zhang", "Ying", ""]]}, {"id": "2008.04443", "submitter": "Olivier Binette", "authors": "Olivier Binette and Rebecca C. Steorts", "title": "(Almost) All of Entity Resolution", "comments": "53 pages, includes supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether the goal is to estimate the number of people that live in a\ncongressional district, to estimate the number of individuals that have died in\nan armed conflict, or to disambiguate individual authors using bibliographic\ndata, all these applications have a common theme - integrating information from\nmultiple sources. Before such questions can be answered, databases must be\ncleaned and integrated in a systematic and accurate way, commonly known as\nrecord linkage, de-duplication, or entity resolution. In this article, we\nreview motivational applications and seminal papers that have led to the growth\nof this area. Specifically, we review the foundational work that began in the\n1940's and 50's that have led to modern probabilistic record linkage. We review\nclustering approaches to entity resolution, semi- and fully supervised methods,\nand canonicalization, which are being used throughout industry and academia in\napplications such as human rights, official statistics, medicine, citation\nnetworks, among others. Finally, we discuss current research topics of\npractical importance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 22:41:20 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Binette", "Olivier", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "2008.04450", "submitter": "Jelle Hellings", "authors": "Jelle Hellings and Daniel P. Hughes and Joshua Primero and Mohammad\n  Sadoghi", "title": "Cerberus: Minimalistic Multi-shard Byzantine-resilient Transaction\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable high-performance and scalable blockchains, we need to step away\nfrom traditional consensus-based fully-replicated designs. One direction is to\nexplore the usage of sharding in which we partition the managed dataset over\nmany shards that independently operate as blockchains. Sharding requires an\nefficient fault-tolerant primitive for the ordering and execution of\nmulti-shard transactions, however.\n  In this work, we seek to design such a primitive suitable for distributed\nledger networks with high transaction throughput. To do so, we propose\nCerberus, a set of minimalistic primitives for processing single-shard and\nmulti-shard UTXO-like transactions. Cerberus aims at maximizing parallel\nprocessing at shards while minimizing coordination within and between shards.\nFirst, we propose Core-Cerberus, that uses strict environmental requirements to\nenable simple yet powerful multi-shard transaction processing. In our intended\nUTXO-environment, Core-Cerberus will operate perfectly with respect to all\ntransactions proposed and approved by well-behaved clients, but does not\nprovide any guarantees for other transactions.\n  To also support more general-purpose environments, we propose two\ngeneralizations of Core-Cerberus: we propose Optimistic-Cerberus, a protocol\nthat does not require any additional coordination phases in the well-behaved\noptimistic case, while requiring intricate coordination when recovering from\nattacks; and we propose Pessimistic-Cerberus, a protocol that adds sufficient\ncoordination to the well-behaved case of Core-Cerberus, allowing it to operate\nin a general-purpose fault-tolerant environments without significant costs to\nrecover from attacks. Finally, we compare the three protocols, showing their\npotential scalability and high transaction throughput in practical\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 23:06:29 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Hellings", "Jelle", ""], ["Hughes", "Daniel P.", ""], ["Primero", "Joshua", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "2008.04640", "submitter": "WanHong Huang", "authors": "WanHong Huang", "title": "High-concurrency Custom-build Relational Database System's design and\n  SQL parser design based on Turing-complete automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database system is an indispensable part of software projects. It plays an\nimportant role in data organization and storage. Its performance and efficiency\nare directly related to the performance of software. Nowadays, we have many\ngeneral relational database systems that can be used in our projects, such as\nSQL Server, MySQL, Oracle, etc. It is undeniable that in most cases, we can\neasily use these database systems to complete our projects, but considering the\ngenerality, the general database systems often can't play the ultimate speed\nand fully adapt to our projects. In very few projects, we will need to design a\ndatabase system that fully adapt to our projects and have a high efficiency and\nconcurrency. Therefore, it is very important to consider a feasible solution of\ndesigning a database system (We only consider the relational database system\nhere). Meanwhile, for a database system, SQL interpretation and execution\nmodule is necessary. According to the theory of formal language and automata,\nthe realization of this module can be completed by automata. In our experiment,\nwe made the following contributions: 1) We designed a small relational\ndatabase, and used the database to complete a highly concurrent student course\nselection system. 2) We design a general automaton module, which can complete\nthe operation from parsing to execution. The using of strategy model and event\ndriven design scheme is used and some improvement on general automata, for\nexample a memory like structure is added to automata to make it better to store\ncontext. All these make the automata model can be used in a variety of\noccasions, not only the parsing and execution of SQL statements.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 11:49:19 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Huang", "WanHong", ""]]}, {"id": "2008.05103", "submitter": "Xingxing Xiao", "authors": "Xingxing Xiao and Jianzhong Li", "title": "Sampling Based Approximate Skyline Calculation on Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing algorithms for processing skyline queries cannot adapt to big\ndata. This paper proposes two approximate skyline algorithms based on sampling.\nThe first algorithm obtains a fixed size sample and computes the approximate\nskyline on the sample. The error of the first algorithm is relatively small in\nmost cases, and is almost independent of the input relation size. The second\nalgorithm returns an $(\\epsilon,\\delta)$-approximation for the exact skyline.\nThe size of sample required by the second algorithm can be regarded as a\nconstant relative to the input relation size, so is the running time.\nExperiments verify the error analysis of the first algorithm and show that the\nsecond algorithm is much faster than the existing skyline algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 04:37:48 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 01:07:51 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 10:11:21 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Xiao", "Xingxing", ""], ["Li", "Jianzhong", ""]]}, {"id": "2008.05164", "submitter": "Muhammad Karam Shehzad", "authors": "Muhammad Karam Shehzad, Jam Muhammad Yousif, Muhammad Saqib Ilyas, and\n  Adnan Iqbal", "title": "The network footprint of replication in popular DBMSs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Database replication is an important component of reliable, disaster tolerant\nand highly available distributed systems. However, data replication also causes\ncommunication and processing overhead. Quantification of these overheads is\ncrucial in choosing a suitable DBMS form several available options and capacity\nplanning. In this paper, we present results from a comparative empirical\nanalysis of replication activities of three commonly used DBMSs - MySQL,\nPostgreSQL and Cassandra under text as well as image traffic. In our\nexperiments, the total traffic with two replicas (which is the norm) was as\nmuch as $300$\\% higher than the total traffic with no replica. Furthermore,\nactivation of the compression option for replication traffic, built into MySQL,\nreduced the total network traffic by as much as $20$\\%. We also found that\naverage CPU utilization and memory utilization were not impacted by the number\nof replicas or the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 08:26:42 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Shehzad", "Muhammad Karam", ""], ["Yousif", "Jam Muhammad", ""], ["Ilyas", "Muhammad Saqib", ""], ["Iqbal", "Adnan", ""]]}, {"id": "2008.05239", "submitter": "Heiko Paulheim", "authors": "Niklas L\\\"udemann, Ageda Shiba, Nikolaos Thymianis, Nicolas Heist,\n  Christopher Ludwig, and Heiko Paulheim", "title": "A Knowledge Graph for Assessing Aggressive Tax Planning Strategies", "comments": "Accepted to International Semantic Web Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The taxation of multi-national companies is a complex field, since it is\ninfluenced by the legislation of several states. Laws in different states may\nhave unforeseen interaction effects, which can be exploited by allowing\nmultinational companies to minimize taxes, a concept known as tax planning. In\nthis paper, we present a knowledge graph of multinational companies and their\nrelationships, comprising almost 1.5M business entities. We show that commonly\nknown tax planning strategies can be formulated as subgraph queries to that\ngraph, which allows for identifying companies using certain strategies.\nMoreover, we demonstrate that we can identify anomalies in the graph which hint\nat potential tax planning strategies, and we show how to enhance those analyses\nby incorporating information from Wikidata using federated queries.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 11:19:36 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 12:01:10 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 10:56:27 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["L\u00fcdemann", "Niklas", ""], ["Shiba", "Ageda", ""], ["Thymianis", "Nikolaos", ""], ["Heist", "Nicolas", ""], ["Ludwig", "Christopher", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2008.05286", "submitter": "Mustafa Ozdayi", "authors": "Md Shihabul Islam, Mustafa Safa Ozdayi, Latifur Khan, Murat\n  Kantarcioglu", "title": "Secure IoT Data Analytics in Cloud via Intel SGX", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing adoption of IoT devices in our daily life is engendering a data\ndeluge, mostly private information that needs careful maintenance and secure\nstorage system to ensure data integrity and protection. Also, the prodigious\nIoT ecosystem has provided users with opportunities to automate systems by\ninterconnecting their devices and other services with rule-based programs. The\ncloud services that are used to store and process sensitive IoT data turn out\nto be vulnerable to outside threats. Hence, sensitive IoT data and rule-based\nprograms need to be protected against cyberattacks. To address this important\nchallenge, in this paper, we propose a framework to maintain confidentiality\nand integrity of IoT data and rule-based program execution. We design the\nframework to preserve data privacy utilizing Trusted Execution Environment\n(TEE) such as Intel SGX, and end-to-end data encryption mechanism. We evaluate\nthe framework by executing rule-based programs in the SGX securely with both\nsimulated and real IoT device data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 20:26:05 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Islam", "Md Shihabul", ""], ["Ozdayi", "Mustafa Safa", ""], ["Khan", "Latifur", ""], ["Kantarcioglu", "Murat", ""]]}, {"id": "2008.05427", "submitter": "Kostas Kolomvatsos", "authors": "Kostas Kolomvatsos, Christos Anagnostopoulos", "title": "An Intelligent Edge-Centric Queries Allocation Scheme based on Ensemble\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of Internet of Things (IoT) and Edge Computing (EC) can\nassist in the delivery of novel applications that will facilitate end users\nactivities. Data collected by numerous devices present in the IoT\ninfrastructure can be hosted into a set of EC nodes becoming the subject of\nprocessing tasks for the provision of analytics. Analytics are derived as the\nresult of various queries defined by end users or applications. Such queries\ncan be executed in the available EC nodes to limit the latency in the provision\nof responses. In this paper, we propose a meta-ensemble learning scheme that\nsupports the decision making for the allocation of queries to the appropriate\nEC nodes. Our learning model decides over queries' and nodes' characteristics.\nWe provide the description of a matching process between queries and nodes\nafter concluding the contextual information for each envisioned characteristic\nadopted in our meta-ensemble scheme. We rely on widely known ensemble models,\ncombine them and offer an additional processing layer to increase the\nperformance. The aim is to result a subset of EC nodes that will host each\nincoming query. Apart from the description of the proposed model, we report on\nits evaluation and the corresponding results. Through a large set of\nexperiments and a numerical analysis, we aim at revealing the pros and cons of\nthe proposed scheme.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 16:32:46 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Kolomvatsos", "Kostas", ""], ["Anagnostopoulos", "Christos", ""]]}, {"id": "2008.06265", "submitter": "Ruben Taelman", "authors": "Ruben Taelman, Simon Steyskal, Sabrina Kirrane", "title": "Towards Querying in Decentralized Environments with Privacy-Preserving\n  Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web is a ubiquitous economic, educational, and collaborative space.\nHowever, it also serves as a haven for personal information harvesting.\nExisting decentralised Web-based ecosystems, such as Solid, aim to combat\npersonal data exploitation on the Web by enabling individuals to manage their\ndata in the personal data store of their choice. Since personal data in these\ndecentralised ecosystems are distributed across many sources, there is a need\nfor techniques to support efficient privacy-preserving query execution over\npersonal data stores. Towards this end, in this position paper we present a\nframework for efficient privacy preserving federated querying, and highlight\nopen research challenges and opportunities. The overarching goal being to\nprovide a means to position future research into privacy-preserving querying\nwithin decentralised environments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 09:41:10 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Taelman", "Ruben", ""], ["Steyskal", "Simon", ""], ["Kirrane", "Sabrina", ""]]}, {"id": "2008.06640", "submitter": "Hongzhi Wang", "authors": "Hongzhi Wang, Yan Wei and Hao Yan", "title": "Automatic Storage Structure Selection for hybrid Workload", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the use of database systems, the design of the storage engine and data\nmodel directly affects the performance of the database when performing queries.\nTherefore, the users of the database need to select the storage engine and\ndesign data model according to the workload encountered. However, in a hybrid\nworkload, the query set of the database is dynamically changing, and the design\nof its optimal storage structure is also changing. Motivated by this, we\npropose an automatic storage structure selection system based on learning cost,\nwhich is used to dynamically select the optimal storage structure of the\ndatabase under hybrid workloads. In the system, we introduce a machine learning\nmethod to build a cost model for the storage engine, and a column-oriented data\nlayout generation algorithm. Experimental results show that the proposed system\ncan choose the optimal combination of storage engine and data model according\nto the current workload, which greatly improves the performance of the default\nstorage structure. And the system is designed to be compatible with different\nstorage engines for easy use in practical applications.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 03:42:33 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Hongzhi", ""], ["Wei", "Yan", ""], ["Yan", "Hao", ""]]}, {"id": "2008.06824", "submitter": "Balder ten Cate", "authors": "Balder ten Cate and Victor Dalmau", "title": "Conjunctive Queries: Unique Characterizations and Exact Learnability", "comments": null, "journal-ref": "Proceedings of the 24th International Conference on Database\n  Theory (ICDT 2021), pp. 7:1-7:35", "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We answer the question which conjunctive queries are uniquely characterized\nby polynomially many positive and negative examples, and how to construct such\nexamples efficiently. As a consequence, we obtain a new efficient exact\nlearning algorithm for a class of conjunctive queries. At the core of our\ncontributions lie two new polynomial-time algorithms for constructing frontiers\nin the homomorphism lattice of finite structures. We also discuss implications\nfor the unique characterizability and learnability of schema mappings and of\ndescription logic concepts.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 02:54:56 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 06:52:51 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Cate", "Balder ten", ""], ["Dalmau", "Victor", ""]]}, {"id": "2008.06831", "submitter": "Tin Vu", "authors": "Tin Vu, Ahmed Eldawy", "title": "DeepSampling: Selectivity Estimation with Predicted Error and Response\n  Time", "comments": "9 pages, published in DeepSpatial 2020", "journal-ref": "ACM SIGKDD Workshop on Deep Learning for Spatiotemporal Data,\n  Applications, and Systems, 2020", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of spatial data urges the research community to find\nefficient processing techniques for interactive queries on large volumes of\ndata. Approximate Query Processing (AQP) is the most prominent technique that\ncan provide real-time answer for ad-hoc queries based on a random sample.\nUnfortunately, existing AQP methods provide an answer without providing any\naccuracy metrics due to the complex relationship between the sample size, the\nquery parameters, the data distribution, and the result accuracy. This paper\nproposes DeepSampling, a deep-learning-based model that predicts the accuracy\nof a sample-based AQP algorithm, specially selectivity estimation, given the\nsample size, the input distribution, and query parameters. The model can also\nbe reversed to measure the sample size that would produce a desired accuracy.\nDeepSampling is the first system that provides a reliable tool for existing\nspatial databases to control the accuracy of AQP.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 03:23:01 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Vu", "Tin", ""], ["Eldawy", "Ahmed", ""]]}, {"id": "2008.06835", "submitter": "Matloob Khushi Dr", "authors": "Matloob Khushi", "title": "Benchmarking database performance for genomic data", "comments": null, "journal-ref": "J Cell Biochem. 2015 Jun;116(6):877-83", "doi": "10.1002/jcb.25049", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic regions represent features such as gene annotations, transcription\nfactor binding sites and epigenetic modifications. Performing various genomic\noperations such as identifying overlapping/non-overlapping regions or nearest\ngene annotations are common research needs. The data can be saved in a database\nsystem for easy management, however, there is no comprehensive database\nbuilt-in algorithm at present to identify overlapping regions. Therefore I have\ndeveloped a region-mapping (RegMap) SQL-based algorithm to perform genomic\noperations and have benchmarked the performance of different databases.\nBenchmarking identified that PostgreSQL extracts overlapping regions much\nfaster than MySQL. Insertion and data uploads in PostgreSQL were also better,\nalthough general searching capability of both databases was almost equivalent.\nIn addition, using the algorithm pair-wise, overlaps of >1000 datasets of\ntranscription factor binding sites and histone marks, collected from previous\npublications, were reported and it was found that HNF4G significantly\nco-locates with cohesin subunit STAG1 (SA1).\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 04:08:38 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Khushi", "Matloob", ""]]}, {"id": "2008.06869", "submitter": "Ralph Foorthuis", "authors": "Ralph Foorthuis", "title": "SECODA: Segmentation- and Combination-Based Detection of Anomalies", "comments": "12 pages (including DSAA conference poster), 9 figures, 3 tables.\n  Presented at DSAA 2017, the IEEE International Conference on Data Science and\n  Advanced Analytics", "journal-ref": null, "doi": "10.1109/DSAA.2017.35", "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study introduces SECODA, a novel general-purpose unsupervised\nnon-parametric anomaly detection algorithm for datasets containing continuous\nand categorical attributes. The method is guaranteed to identify cases with\nunique or sparse combinations of attribute values. Continuous attributes are\ndiscretized repeatedly in order to correctly determine the frequency of such\nvalue combinations. The concept of constellations, exponentially increasing\nweights and discretization cut points, as well as a pruning heuristic are used\nto detect anomalies with an optimal number of iterations. Moreover, the\nalgorithm has a low memory imprint and its runtime performance scales linearly\nwith the size of the dataset. An evaluation with simulated and real-life\ndatasets shows that this algorithm is able to identify many different types of\nanomalies, including complex multidimensional instances. An evaluation in terms\nof a data quality use case with a real dataset demonstrates that SECODA can\nbring relevant and practical value to real-world settings.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 10:03:14 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Foorthuis", "Ralph", ""]]}, {"id": "2008.07159", "submitter": "Chuan-Chi Lai", "authors": "Chuan-Chi Lai, Chuan-Ming Liu, Yan-Lin Chen, Li-Chun Wang", "title": "Probabilistic Skyline Query Processing over Uncertain Data Streams in\n  Edge Computing Environments", "comments": "6 pages, 5 figures, to appear in 2020 IEEE Global Communications\n  Conference: Selected Areas in Communications: Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of technology, the data generated in our lives is\ngetting faster and faster, and the amount of data that various applications\nneed to process becomes extremely huge. Therefore, we need to put more effort\ninto analyzing data and extracting valuable information. Cloud computing used\nto be a good technology to solve a large number of data analysis problems.\nHowever, in the era of the popularity of the Internet of Things (IoT),\ntransmitting sensing data back to the cloud for centralized data analysis will\nconsume a lot of wireless communication and network transmission costs. To\nsolve the above problems, edge computing has become a promising solution. In\nthis paper, we propose a new algorithm for processing probabilistic skyline\nqueries over uncertain data streams in an edge computing environment. We use\nthe concept of a second skyline set to filter data that is unlikely to be the\nresult of the skyline. Besides, the edge server only sends the information\nneeded to update the global analysis results on the cloud server, which will\ngreatly reduce the amount of data transmitted over the network. The results\nshow that our proposed method not only reduces the response time by more than\n50% compared with the brute force method on two-dimensional data but also\nmaintains the leading processing speed on high-dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 08:53:29 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 04:01:53 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Lai", "Chuan-Chi", ""], ["Liu", "Chuan-Ming", ""], ["Chen", "Yan-Lin", ""], ["Wang", "Li-Chun", ""]]}, {"id": "2008.07176", "submitter": "David Chaves-Fraga", "authors": "Enrique Iglesias, Samaneh Jozashoori, David Chaves-Fraga, Diego\n  Collarana, Maria-Esther Vidal", "title": "SDM-RDFizer: An RML Interpreter for the Efficient Creation of RDF\n  Knowledge Graphs", "comments": null, "journal-ref": null, "doi": "10.1145/3340531.3412881", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the amount of data has increased exponentially, and\nknowledge graphs have gained attention as data structures to integrate data and\nknowledge harvested from myriad data sources. However, data complexity issues\nlike large volume, high-duplicate rate, and heterogeneity usually characterize\nthese data sources, being required data management tools able to address the\nimpact negatively of these issues on the knowledge graph creation process. In\nthis paper, we propose the SDM-RDFizer, an interpreter of the RDF Mapping\nLanguage (RML), to transform raw data in various formats into an RDF knowledge\ngraph. SDM-RDFizer implements novel algorithms to execute the logical operators\nbetween mappings in RML, allowing thus to scale up to complex scenarios where\ndata is not only broad but has a high-duplication rate. We empirically evaluate\nthe SDM-RDFizer performance against diverse testbeds with diverse\nconfigurations of data volume, duplicates, and heterogeneity. The observed\nresults indicate that SDM-RDFizer is two orders of magnitude faster than state\nof the art, thus, meaning that SDM-RDFizer an interoperable and scalable\nsolution for knowledge graph creation. SDM-RDFizer is publicly available as a\nresource through a Github repository and a DOI.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 09:32:04 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Iglesias", "Enrique", ""], ["Jozashoori", "Samaneh", ""], ["Chaves-Fraga", "David", ""], ["Collarana", "Diego", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "2008.07504", "submitter": "Karim Banawan", "authors": "Zhusheng Wang and Karim Banawan and Sennur Ulukus", "title": "Multi-Party Private Set Intersection: An Information-Theoretic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of multi-party private set intersection (MP-PSI).\nIn MP-PSI, there are $M$ parties, each storing a data set $\\mathcal{p}_i$ over\n$N_i$ replicated and non-colluding databases, and we want to calculate the\nintersection of the data sets $\\cap_{i=1}^M \\mathcal{p}_i$ without leaking any\ninformation beyond the set intersection to any of the parties. We consider a\nspecific communication protocol where one of the parties, called the leader\nparty, initiates the MP-PSI protocol by sending queries to the remaining\nparties which are called client parties. The client parties are not allowed to\ncommunicate with each other. We propose an information-theoretic scheme that\nprivately calculates the intersection $\\cap_{i=1}^M \\mathcal{p}_i$ with a\ndownload cost of $D = \\min_{t \\in \\{1, \\cdots, M\\}} \\sum_{i \\in \\{1, \\cdots\nM\\}\\setminus {t}} \\left\\lceil \\frac{|\\mathcal{p}_t|N_i}{N_i-1}\\right\\rceil$.\nSimilar to the 2-party PSI problem, our scheme builds on the connection between\nthe PSI problem and the multi-message symmetric private information retrieval\n(MM-SPIR) problem. Our scheme is a non-trivial generalization of the 2-party\nPSI scheme as it needs an intricate design of the shared common randomness.\nInterestingly, in terms of the download cost, our scheme does not incur any\npenalty due to the more stringent privacy constraints in the MP-PSI problem\ncompared to the 2-party PSI problem.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 17:51:53 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Zhusheng", ""], ["Banawan", "Karim", ""], ["Ulukus", "Sennur", ""]]}, {"id": "2008.07864", "submitter": "Dan Olteanu", "authors": "Dan Olteanu", "title": "The Relational Data Borg is Learning", "comments": "14 pages, 11 figures, VLDB 2020 keynote", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper overviews an approach that addresses machine learning over\nrelational data as a database problem. This is justified by two observations.\nFirst, the input to the learning task is commonly the result of a feature\nextraction query over the relational data. Second, the learning task requires\nthe computation of group-by aggregates.\n  This approach has been already investigated for a number of supervised and\nunsupervised learning tasks, including: ridge linear regression, factorisation\nmachines, support vector machines, decision trees, principal component\nanalysis, and k-means; and also for linear algebra over data matrices.\n  The main message of this work is that the runtime performance of machine\nlearning can be dramatically boosted by a toolbox of techniques that exploit\nthe knowledge of the underlying data. This includes theoretical development on\nthe algebraic, combinatorial, and statistical structure of relational data\nprocessing and systems development on code specialisation, low-level\ncomputation sharing, and parallelisation. These techniques aim at lowering both\nthe complexity and the constant factors of the learning time.\n  This work is the outcome of extensive collaboration of the author with\ncolleagues from RelationalAI, in particular Mahmoud Abo Khamis, Molham Aref,\nHung Ngo, and XuanLong Nguyen, and from the FDB research project, in particular\nAhmet Kara, Milos Nikolic, Maximilian Schleich, Amir Shaikhha, Jakub Zavodny,\nand Haozhe Zhang. The author would also like to thank the members of the FDB\nproject for the figures and examples used in this paper.\n  The author is grateful for support from industry: Amazon Web Services,\nGoogle, Infor, LogicBlox, Microsoft Azure, RelationalAI; and from the funding\nagencies EPSRC and ERC. This project has received funding from the European\nUnion's Horizon 2020 research and innovation programme under grant agreement No\n682588.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:25:45 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Olteanu", "Dan", ""]]}, {"id": "2008.08190", "submitter": "Wensheng Gan", "authors": "Chien-Ming Chen, Lili Chen, Wensheng Gan, Lina Qiu, and Weiping Ding", "title": "Discovering High Utility-Occupancy Patterns from Uncertain Data", "comments": "Elsevier Information Sciences, 12 figures", "journal-ref": "Information Sciences, 2021", "doi": "10.1016/j.ins.2020.10.001", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely known that there is a lot of useful information hidden in big\ndata, leading to a new saying that \"data is money.\" Thus, it is prevalent for\nindividuals to mine crucial information for utilization in many real-world\napplications. In the past, studies have considered frequency. Unfortunately,\ndoing so neglects other aspects, such as utility, interest, or risk. Thus, it\nis sensible to discover high-utility itemsets (HUIs) in transaction databases\nwhile utilizing not only the quantity but also the predefined utility. To find\npatterns that can represent the supporting transaction, a recent study was\nconducted to mine high utility-occupancy patterns whose contribution to the\nutility of the entire transaction is greater than a certain value. Moreover, in\nrealistic applications, patterns may not exist in transactions but be connected\nto an existence probability. In this paper, a novel algorithm, called\nHigh-Utility-Occupancy Pattern Mining in Uncertain databases (UHUOPM), is\nproposed. The patterns found by the algorithm are called Potential High Utility\nOccupancy Patterns (PHUOPs). This algorithm divides user preferences into three\nfactors, including support, probability, and utility occupancy. To reduce\nmemory cost and time consumption and to prune the search space in the algorithm\nas mentioned above, probability-utility-occupancy list (PUO-list) and\nprobability-frequency-utility table (PFU-table) are used, which assist in\nproviding the downward closure property. Furthermore, an original tree\nstructure, called support count tree (SC-tree), is constructed as the search\nspace of the algorithm. Finally, substantial experiments were conducted to\nevaluate the performance of proposed UHUOPM algorithm on both real-life and\nsynthetic datasets, particularly in terms of effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 23:24:40 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chen", "Chien-Ming", ""], ["Chen", "Lili", ""], ["Gan", "Wensheng", ""], ["Qiu", "Lina", ""], ["Ding", "Weiping", ""]]}, {"id": "2008.08285", "submitter": "Stephen Ash", "authors": "Andrew Borthwick, Stephen Ash, Bin Pang, Shehzad Qureshi, Timothy\n  Jones", "title": "Scalable Blocking for Very Large Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of database deduplication, the goal is to find approximately\nmatching records within a database. Blocking is a typical stage in this process\nthat involves cheaply finding candidate pairs of records that are potential\nmatches for further processing. We present here Hashed Dynamic Blocking, a new\napproach to blocking designed to address datasets larger than those studied in\nmost prior work. Hashed Dynamic Blocking (HDB) extends Dynamic Blocking, which\nleverages the insight that rare matching values and rare intersections of\nvalues are predictive of a matching relationship. We also present a novel use\nof Locality Sensitive Hashing (LSH) to build blocking key values for huge\ndatabases with a convenient configuration to control the trade-off between\nprecision and recall. HDB achieves massive scale by minimizing data movement,\nusing compact block representation, and greedily pruning ineffective candidate\nblocks using a Count-min Sketch approximate counting data structure. We\nbenchmark the algorithm by focusing on real-world datasets in excess of one\nmillion rows, demonstrating that the algorithm displays linear time complexity\nscaling in this range. Furthermore, we execute HDB on a 530 million row\nindustrial dataset, detecting 68 billion candidate pairs in less than three\nhours at a cost of $307 on a major cloud service.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 06:35:37 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Borthwick", "Andrew", ""], ["Ash", "Stephen", ""], ["Pang", "Bin", ""], ["Qureshi", "Shehzad", ""], ["Jones", "Timothy", ""]]}, {"id": "2008.08657", "submitter": "Maximilian Schleich", "authors": "Maximilian Schleich, Dan Olteanu", "title": "LMFAO: An Engine for Batches of Group-By Aggregates", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LMFAO is an in-memory optimization and execution engine for large batches of\ngroup-by aggregates over joins. Such database workloads capture the\ndata-intensive computation of a variety of data science applications.\n  We demonstrate LMFAO for three popular models: ridge linear regression with\nbatch gradient descent, decision trees with CART, and clustering with Rk-means.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 20:15:32 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Schleich", "Maximilian", ""], ["Olteanu", "Dan", ""]]}, {"id": "2008.08989", "submitter": "Amir Gilad", "authors": "Amir Gilad and Yuval Moskovitch", "title": "Towards Inferring Queries from Simple and Partial Provenance Examples", "comments": null, "journal-ref": null, "doi": "10.1145/3340531.3417451", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of query-by-example aims at inferring queries from output examples\ngiven by non-expert users, by finding the underlying logic that binds the\nexamples. However, for a very small set of examples, it is difficult to\ncorrectly infer such logic. To bridge this gap, previous work suggested\nattaching explanations to each output example, modeled as provenance, allowing\nusers to explain the reason behind their choice of example. In this paper, we\nexplore the problem of inferring queries from a few output examples and\nintuitive explanations. We propose a two step framework: (1) convert the\nexplanations into (partial) provenance and (2) infer a query that generates the\noutput examples using a novel algorithm that employs a graph based approach.\nThis framework is suitable for non-experts as it does not require the\nspecification of the provenance in its entirety or an understanding of its\nstructure. We show promising initial experimental results of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 14:22:20 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Gilad", "Amir", ""], ["Moskovitch", "Yuval", ""]]}, {"id": "2008.09268", "submitter": "Meihui Zhang", "authors": "Meihui Zhang, Zhongle Xie, Cong Yue, Ziyue Zhong", "title": "Spitz: A Verifiable Database System", "comments": null, "journal-ref": null, "doi": "10.14778/3415478.3415567", "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases in the past have helped businesses maintain and extract insights\nfrom their data. Today, it is common for a business to involve multiple\nindependent, distrustful parties. This trend towards decentralization\nintroduces a new and important requirement to databases: the integrity of the\ndata, the history, and the execution must be protected. In other words, there\nis a need for a new class of database systems whose integrity can be verified\n(or verifiable databases).\n  In this paper, we identify the requirements and the design challenges of\nverifiable databases.We observe that the main challenges come from the need to\nbalance data immutability, tamper evidence, and performance. We first consider\napproaches that extend existing OLTP and OLAP systems with support for\nverification. We next examine a clean-slate approach, by describing a new\nsystem, Spitz, specifically designed for efficiently supporting immutable and\ntamper-evident transaction management. We conduct a preliminary performance\nstudy of both approaches against a baseline system, and provide insights on\ntheir performance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 02:16:12 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Zhang", "Meihui", ""], ["Xie", "Zhongle", ""], ["Yue", "Cong", ""], ["Zhong", "Ziyue", ""]]}, {"id": "2008.09511", "submitter": "Peter Lindner", "authors": "Nofar Carmeli, Martin Grohe, Peter Lindner, Christoph Standke", "title": "Tuple-Independent Representations of Infinite Probabilistic Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic databases (PDBs) are probability spaces over database\ninstances. They provide a framework for handling uncertainty in databases, as\noccurs due to data integration, noisy data, data from unreliable sources or\nrandomized processes. Most of the existing theory literature investigated\nfinite, tuple-independent PDBs (TI-PDBs) where the occurrences of tuples are\nindependent events. Only recently, Grohe and Lindner (PODS '19) introduced\nindependence assumptions for PDBs beyond the finite domain assumption. In the\nfinite, a major argument for discussing the theoretical properties of TI-PDBs\nis that they can be used to represent any finite PDB via views. This is no\nlonger the case once the number of tuples is countably infinite. In this paper,\nwe systematically study the representability of infinite PDBs in terms of\nTI-PDBs and the related block-independent disjoint PDBs.\n  The central question is which infinite PDBs are representable as first-order\nviews over tuple-independent PDBs. We give a necessary condition for the\nrepresentability of PDBs and provide a sufficient criterion for\nrepresentability in terms of the probability distribution of a PDB. With\nvarious examples, we explore the limits of our criteria. We show that\nconditioning on first order properties yields no additional power in terms of\nexpressivity. Finally, we discuss the relation between purely logical and\narithmetic reasons for (non-)representability.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 14:39:47 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Carmeli", "Nofar", ""], ["Grohe", "Martin", ""], ["Lindner", "Peter", ""], ["Standke", "Christoph", ""]]}, {"id": "2008.09983", "submitter": "Sahaana Suri", "authors": "Sahaana Suri, Raghuveer Chanda, Neslihan Bulut, Pradyumna Narayana,\n  Yemao Zeng, Peter Bailis, Sugato Basu, Girija Narlikar, Christopher Re, and\n  Abishek Sethi", "title": "Leveraging Organizational Resources to Adapt Models to New Data\n  Modalities", "comments": null, "journal-ref": "PVLDB,13(12): 3396-3410, 2020", "doi": "10.14778/3415478.3415559", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As applications in large organizations evolve, the machine learning (ML)\nmodels that power them must adapt the same predictive tasks to newly arising\ndata modalities (e.g., a new video content launch in a social media application\nrequires existing text or image models to extend to video). To solve this\nproblem, organizations typically create ML pipelines from scratch. However,\nthis fails to utilize the domain expertise and data they have cultivated from\ndeveloping tasks for existing modalities. We demonstrate how organizational\nresources, in the form of aggregate statistics, knowledge bases, and existing\nservices that operate over related tasks, enable teams to construct a common\nfeature space that connects new and existing data modalities. This allows teams\nto apply methods for training data curation (e.g., weak supervision and label\npropagation) and model training (e.g., forms of multi-modal learning) across\nthese different data modalities. We study how this use of organizational\nresources composes at production scale in over 5 classification tasks at\nGoogle, and demonstrate how it reduces the time needed to develop models for\nnew modalities from months to weeks to days.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 07:29:00 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Suri", "Sahaana", ""], ["Chanda", "Raghuveer", ""], ["Bulut", "Neslihan", ""], ["Narayana", "Pradyumna", ""], ["Zeng", "Yemao", ""], ["Bailis", "Peter", ""], ["Basu", "Sugato", ""], ["Narlikar", "Girija", ""], ["Re", "Christopher", ""], ["Sethi", "Abishek", ""]]}, {"id": "2008.10349", "submitter": "Varun Pandey", "authors": "Varun Pandey, Alexander van Renen, Andreas Kipf, Ibrahim Sabek, Jialin\n  Ding, Alfons Kemper", "title": "The Case for Learned Spatial Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatial data is ubiquitous. Massive amounts of data are generated every day\nfrom billions of GPS-enabled devices such as cell phones, cars, sensors, and\nvarious consumer-based applications such as Uber, Tinder, location-tagged posts\nin Facebook, Twitter, Instagram, etc. This exponential growth in spatial data\nhas led the research community to focus on building systems and applications\nthat can process spatial data efficiently. In the meantime, recent research has\nintroduced learned index structures. In this work, we use techniques proposed\nfrom a state-of-the art learned multi-dimensional index structure (namely,\nFlood) and apply them to five classical multi-dimensional indexes to be able to\nanswer spatial range queries. By tuning each partitioning technique for optimal\nperformance, we show that (i) machine learned search within a partition is\nfaster by 11.79\\% to 39.51\\% than binary search when using filtering on one\ndimension, (ii) the bottleneck for tree structures is index lookup, which could\npotentially be improved by linearizing the indexed partitions (iii) filtering\non one dimension and refining using machine learned indexes is 1.23x to 1.83x\ntimes faster than closest competitor which filters on two dimensions, and (iv)\nlearned indexes can have a significant impact on the performance of low\nselectivity queries while being less effective under higher selectivities.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 12:09:55 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Pandey", "Varun", ""], ["van Renen", "Alexander", ""], ["Kipf", "Andreas", ""], ["Sabek", "Ibrahim", ""], ["Ding", "Jialin", ""], ["Kemper", "Alfons", ""]]}, {"id": "2008.10549", "submitter": "Alireza Heidari", "authors": "Alireza Heidari, Shrinu Kushagra, Ihab F. Ilyas", "title": "On sampling from data with duplicate records", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data deduplication is the task of detecting records in a database that\ncorrespond to the same real-world entity. Our goal is to develop a procedure\nthat samples uniformly from the set of entities present in the database in the\npresence of duplicates. We accomplish this by a two-stage process. In the first\nstep, we estimate the frequencies of all the entities in the database. In the\nsecond step, we use rejection sampling to obtain a (approximately) uniform\nsample from the set of entities. However, efficiently estimating the frequency\nof all the entities is a non-trivial task and not attainable in the general\ncase. Hence, we consider various natural properties of the data under which\nsuch frequency estimation (and consequently uniform sampling) is possible.\nUnder each of those assumptions, we provide sampling algorithms and give proofs\nof the complexity (both statistical and computational) of our approach. We\ncomplement our study by conducting extensive experiments on both real and\nsynthetic datasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 16:41:47 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Heidari", "Alireza", ""], ["Kushagra", "Shrinu", ""], ["Ilyas", "Ihab F.", ""]]}, {"id": "2008.10569", "submitter": "Kexin Rong", "authors": "Kexin Rong, Yao Lu, Peter Bailis, Srikanth Kandula, Philip Levis", "title": "Approximate Partition Selection for Big-Data Workloads using Summary\n  Statistics", "comments": null, "journal-ref": null, "doi": "10.14778/3407790.3407848", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many big-data clusters store data in large partitions that support access at\na coarse, partition-level granularity. As a result, approximate query\nprocessing via row-level sampling is inefficient, often requiring reads of many\npartitions. In this work, we seek to answer queries quickly and approximately\nby reading a subset of the data partitions and combining partial answers in a\nweighted manner without modifying the data layout. We illustrate how to\nefficiently perform this query processing using a set of pre-computed summary\nstatistics, which inform the choice of partitions and weights. We develop novel\nmeans of using the statistics to assess the similarity and importance of\npartitions. Our experiments on several datasets and data layouts demonstrate\nthat to achieve the same relative error compared to uniform partition sampling,\nour techniques offer from 2.7$\\times$ to $70\\times$ reduction in the number of\npartitions read, and the statistics stored per partition require fewer than\n100KB.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:15:09 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Rong", "Kexin", ""], ["Lu", "Yao", ""], ["Bailis", "Peter", ""], ["Kandula", "Srikanth", ""], ["Levis", "Philip", ""]]}, {"id": "2008.10925", "submitter": "Dimitri Braininger", "authors": "Dimitri Braininger, Wolfgang Mauerer and Stefanie Scherzinger", "title": "Replicability and Reproducibility of a Schema Evolution Study in\n  Embedded Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ascertaining the feasibility of independent falsification or repetition of\npublished results is vital to the scientific process, and replication or\nreproduction experiments are routinely performed in many disciplines.\nUnfortunately, such studies are only scarcely available in database research,\nwith few papers dedicated to re-evaluating published results. In this paper, we\nconduct a case study on replicating and reproducing a study on schema evolution\nin embedded databases. We obtain exact results for one out of four database\napplications studied, and come close in two further cases. By reporting\nresults, efforts, and obstacles encountered, we hope to increase appreciation\nfor the substantial efforts required to ensure reproducibility. By discussing\nminutiae details required for reproducible work, we argue that such important,\nbut often ignored components of scientific work should receive more credit in\nthe evaluation of future research.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 10:14:55 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 16:43:36 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 19:29:30 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Braininger", "Dimitri", ""], ["Mauerer", "Wolfgang", ""], ["Scherzinger", "Stefanie", ""]]}, {"id": "2008.10986", "submitter": "Matthew Damigos", "authors": "Foto N. Afrati and Matthew Damigos", "title": "On the complexity of query containment and computing certain answers in\n  the presence of ACs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We often add arithmetic to extend the expressiveness of query languages and\nstudy the complexity of problems such as testing query containment and finding\ncertain answers in the framework of answering queries using views. When adding\narithmetic comparisons, the complexity of such problems is higher than the\ncomplexity of their counterparts without them. It has been observed that we can\nachieve lower complexity if we restrict some of the comparisons in the\ncontaining query to be closed or open semi-interval comparisons. Here, focusing\na) on the problem of containment for conjunctive queries with arithmetic\ncomparisons (CQAC queries, for short), we prove upper bounds on its\ncomputational complexity and b) on the problem of computing certain answers, we\nfind large classes of CQAC queries and views where this problem is polynomial.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 13:26:04 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 20:07:33 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 13:46:10 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Afrati", "Foto N.", ""], ["Damigos", "Matthew", ""]]}, {"id": "2008.11015", "submitter": "Mengyu Zhou", "authors": "Mengyu Zhou, Qingtao Li, Xinyi He, Yuejiang Li, Yibo Liu, Wei Ji, Shi\n  Han, Yining Chen, Daxin Jiang, Dongmei Zhang", "title": "Table2Charts: Recommending Charts by Learning Shared Table\n  Representations", "comments": "9 + 2(appendix) pages, accepted by KDD'21 conference", "journal-ref": null, "doi": "10.1145/3447548.3467279", "report-no": null, "categories": "cs.DB cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for people to create different types of charts to explore a\nmulti-dimensional dataset (table). However, to recommend commonly composed\ncharts in real world, one should take the challenges of efficiency, imbalanced\ndata and table context into consideration. In this paper, we propose\nTable2Charts framework which learns common patterns from a large corpus of\n(table, charts) pairs. Based on deep Q-learning with copying mechanism and\nheuristic searching, Table2Charts does table-to-sequence generation, where each\nsequence follows a chart template. On a large spreadsheet corpus with 165k\ntables and 266k charts, we show that Table2Charts could learn a shared\nrepresentation of table fields so that recommendation tasks on different chart\ntypes could mutually enhance each other. Table2Charts outperforms other chart\nrecommendation systems in both multi-type task (with doubled recall numbers\nR@3=0.61 and R@1=0.43) and human evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 15:06:26 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 18:21:42 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 14:08:32 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2021 11:57:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhou", "Mengyu", ""], ["Li", "Qingtao", ""], ["He", "Xinyi", ""], ["Li", "Yuejiang", ""], ["Liu", "Yibo", ""], ["Ji", "Wei", ""], ["Han", "Shi", ""], ["Chen", "Yining", ""], ["Jiang", "Daxin", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2008.11229", "submitter": "Olumide Leshi", "authors": "Olumide Leshi", "title": "Uncovering Soccer Teams Passing Strategies Using Implication Rules", "comments": "10 pages with sbc-template.sty used, 3 PNG figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal Concept Analysis FCA has seen application in different knowledge\nareas, including Social Network Analysis SNA. In turn, research has also shown\nthe applicability of SNA in assessing team sports. In this project, to uncover\nfrequent passing sequences of a soccer team, an FCA based approach is\nintroduced. The approach relies on a minimum cover of implications, the\nDuquenne Guigues DG basis and the notion that a soccer teams passes describe a\nsocial network.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 18:34:21 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Leshi", "Olumide", ""]]}, {"id": "2008.11409", "submitter": "Jerome Darmont", "authors": "Yuzhao Yang (IRIT-SIG), J\\'er\\^ome Darmont (ERIC), Franck Ravat\n  (IRIT-SIG), Olivier Teste (IRIT-SIG)", "title": "Automatic Integration Issues of Tabular Data for On-Line Analysis\n  Processing", "comments": null, "journal-ref": "16e journ{\\'e}es EDA Business Intelligence & Big Data (EDA 2020),\n  Aug 2020, Lyon, France. pp.5-18", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Companies and individuals produce numerous tabular data. The objective of\nthis position paper is to draw up the challenges posed by the automatic\nintegration of data in the form of tables so that they can be cross-analyzed.\nWe provide a first automatic solution for the integration of such tabular data\nto allow On-Line Analysis Processing. To fulfil this task, features of tabular\ndata should be analyzed and the challenge of automatic multidimensional schema\ngeneration should be addressed. Hence, we propose a typology of tabular data\nand discuss our idea of an automatic solution.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 06:58:09 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 14:06:26 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Yang", "Yuzhao", "", "IRIT-SIG"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Ravat", "Franck", "", "IRIT-SIG"], ["Teste", "Olivier", "", "IRIT-SIG"]]}, {"id": "2008.11705", "submitter": "Francesco Lettich", "authors": "Samiul Anwar, Francesco Lettich, Mario A. Nascimento", "title": "Towards A Personal Shopper's Dilemma: Time vs Cost", "comments": "An abridged version of this paper will appear at The 28th ACM\n  SIGSPATIAL Intl Conf. on Advances in Geographic Information Systems 2020 (ACM\n  SIGSPATIAL 2020), Seattle, Washington, USA, November 3-6, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a customer who needs to fulfill a shopping list, and also a personal\nshopper who is willing to buy and resell to customers the goods in their\nshopping lists. It is in the personal shopper's best interest to find\n(shopping) routes that (i) minimize the time serving a customer, in order to be\nable to serve more customers, and (ii) minimize the price paid for the goods,\nin order to maximize his/her potential profit when reselling them. Those are\ntypically competing criteria leading to what we refer to as the Personal\nShopper's Dilemma query, i.e., to determine where to buy each of the required\ngoods while attempting to optimize both criteria at the same time. Given the\nquery's NP-hardness we propose a heuristic approach to determine a subset of\nthe sub-optimal routes under any linear combination of the aforementioned\ncriteria, i.e., the query's approximate linear skyline set. In order to measure\nthe effectiveness of our approach we also introduce two new metrics, optimality\nand coverage gaps w.r.t. an optimal, but computationally expensive, baseline\nsolution. Our experiments, using realistic city-scale datasets, show that our\nproposed approach is two orders of magnitude faster than the baseline and\nyields low values for the optimality and coverage gaps.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 17:47:08 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 15:10:00 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Anwar", "Samiul", ""], ["Lettich", "Francesco", ""], ["Nascimento", "Mario A.", ""]]}, {"id": "2008.11900", "submitter": "Alberto Laender", "authors": "Robson A. Camp\\^elo, Marco A. Casanova, Dorgival O. Guedes, Alberto H.\n  F. Laender", "title": "A Brief Survey on Replica Consistency in Cloud Environments", "comments": "13 pages, 1 figure, 2 tables, 62 references", "journal-ref": "Journal of Internet Services and Applications, Volume 11, Number\n  1, 2020", "doi": "10.1186/s13174-020-0122-y", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a general term that involves delivering hosted services\nover the Internet. With the accelerated growth of the volume of data used by\napplications, many organizations have moved their data into cloud servers to\nprovide scalable, reliable and highly available services. A particularly\nchallenging issue that arises in the context of cloud storage systems with\ngeographically-distributed data replication is how to reach a consistent state\nfor all replicas. This survey reviews major aspects related to consistency\nissues in cloud data storage systems, categorizing recently proposed methods\ninto three categories: (1) fixed consistency methods, (2) configurable\nconsistency methods and (3) consistency monitoring methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 03:30:35 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 12:03:39 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Camp\u00ealo", "Robson A.", ""], ["Casanova", "Marco A.", ""], ["Guedes", "Dorgival O.", ""], ["Laender", "Alberto H. F.", ""]]}, {"id": "2008.12330", "submitter": "Ralph Foorthuis", "authors": "Ralph Foorthuis", "title": "The Impact of Discretization Method on the Detection of Six Types of\n  Anomalies in Datasets", "comments": "16 pages, 5 figures, 2 tables. Presented at the 30th Benelux\n  Conference on Artificial Intelligence (BNAIC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is the process of identifying cases, or groups of cases,\nthat are in some way unusual and do not fit the general patterns present in the\ndataset. Numerous algorithms use discretization of numerical data in their\ndetection processes. This study investigates the effect of the discretization\nmethod on the unsupervised detection of each of the six anomaly types\nacknowledged in a recent typology of data anomalies. To this end, experiments\nare conducted with various datasets and SECODA, a general-purpose algorithm for\nunsupervised non-parametric anomaly detection in datasets with numerical and\ncategorical attributes. This algorithm employs discretization of continuous\nattributes, exponentially increasing weights and discretization cut points, and\na pruning heuristic to detect anomalies with an optimal number of iterations.\nThe results demonstrate that standard SECODA can detect all six types, but that\ndifferent discretization methods favor the discovery of certain anomaly types.\nThe main findings also hold for other detection techniques using\ndiscretization.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 18:43:55 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Foorthuis", "Ralph", ""]]}, {"id": "2008.12379", "submitter": "Wentao Wu", "authors": "Wentao Wu, Philip A. Bernstein, Alex Raizman, Christina Pavlopoulou", "title": "Cost-based Query Rewriting Techniques for Optimizing Aggregates Over\n  Correlated Windows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Window aggregates are ubiquitous in stream processing. In Azure Stream\nAnalytics (ASA), a stream processing service hosted by Microsoft's Azure cloud,\nwe see many customer queries that contain aggregate functions (such as MIN and\nMAX) over multiple correlated windows (e.g., tumbling windows of length five\nminutes and ten minutes) defined on the same event stream. In this paper, we\npresent a cost-based optimization framework for optimizing such queries by\nsharing computation among multiple windows. Since our optimization techniques\nare at the level of query rewriting, they can be implemented on any stream\nprocessing system that supports a declarative, SQL-like query language without\nchanging the underlying query execution engine. We formalize the shared\ncomputation problem, present the optimization techniques in detail, and report\nevaluation results over synthetic workloads.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 21:49:02 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 20:27:14 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Wu", "Wentao", ""], ["Bernstein", "Philip A.", ""], ["Raizman", "Alex", ""], ["Pavlopoulou", "Christina", ""]]}, {"id": "2008.12665", "submitter": "Sven Helmer", "authors": "Danila Piatov, Sven Helmer, Anton Dign\\\"os, Fabio Persia", "title": "Cache-Efficient Sweeping-Based Interval Joins for Extended Allen\n  Relation Predicates (Extended Version)", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a family of efficient plane-sweeping interval join algorithms that\ncan evaluate a wide range of interval predicates such as Allen's relationships\nand parameterized relationships. Our technique is based on a framework,\ncomponents of which can be flexibly combined in different manners to support\nthe required interval relation. In temporal databases, our algorithms can\nexploit a well-known and flexible access method, the Timeline Index, thus\nexpanding the set of operations it supports even further. Additionally,\nemploying a compact data structure, the gapless hash map, we utilize the CPU\ncache efficiently. In an experimental evaluation, we show that our approach is\nseveral times faster and scales better than state-of-the-art techniques, while\nbeing much better suited for real-time event processing.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 14:06:10 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Piatov", "Danila", ""], ["Helmer", "Sven", ""], ["Dign\u00f6s", "Anton", ""], ["Persia", "Fabio", ""]]}, {"id": "2008.12763", "submitter": "Ju Fan", "authors": "Ju Fan, Tongyu Liu, Guoliang Li, Junyou Chen, Yuwei Shen, Xiaoyong Du", "title": "Relational Data Synthesis using Generative Adversarial Networks: A\n  Design Space Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of big data has brought an urgent demand for\nprivacy-preserving data publishing. Traditional solutions to this demand have\nlimitations on effectively balancing the tradeoff between privacy and utility\nof the released data. Thus, the database community and machine learning\ncommunity have recently studied a new problem of relational data synthesis\nusing generative adversarial networks (GAN) and proposed various algorithms.\nHowever, these algorithms are not compared under the same framework and thus it\nis hard for practitioners to understand GAN's benefits and limitations. To\nbridge the gaps, we conduct so far the most comprehensive experimental study\nthat investigates applying GAN to relational data synthesis. We introduce a\nunified GAN-based framework and define a space of design solutions for each\ncomponent in the framework, including neural network architectures and training\nstrategies. We conduct extensive experiments to explore the design space and\ncompare with traditional data synthesis approaches. Through extensive\nexperiments, we find that GAN is very promising for relational data synthesis,\nand provide guidance for selecting appropriate design solutions. We also point\nout limitations of GAN and identify future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 17:41:11 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Fan", "Ju", ""], ["Liu", "Tongyu", ""], ["Li", "Guoliang", ""], ["Chen", "Junyou", ""], ["Shen", "Yuwei", ""], ["Du", "Xiaoyong", ""]]}, {"id": "2008.12905", "submitter": "Manas Joshi", "authors": "Manas Joshi, Arshdeep Singh, Sayan Ranu, Amitabha Bagchi, Priyank\n  Karia, Puneet Kala", "title": "Batching and Matching for Food Delivery in Dynamic Road Networks", "comments": "12 pages, 9 figures, Accepted in ICDE 2021 as Short Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a stream of food orders and available delivery vehicles, how should\norders be assigned to vehicles so that the delivery time is minimized? Several\ndecisions have to be made: (1) assignment of orders to vehicles, (2) grouping\norders into batches to cope with limited vehicle availability, and (3) adapting\nto dynamic positions of delivery vehicles. We show that the minimization\nproblem is not only NP-hard but inapproximable in polynomial time. To mitigate\nthis computational bottleneck, we develop an algorithm called FoodMatch, which\nmaps the vehicle assignment problem to that of minimum weight perfect matching\non a bipartite graph. To further reduce the quadratic construction cost of the\nbipartite graph, we deploy best-first search to only compute a subgraph that is\nhighly likely to contain the minimum matching. The solution quality is further\nenhanced by reducing batching to a graph clustering problem and anticipating\ndynamic positions of vehicles through angular distance. Extensive experiments\non food-delivery data from large metropolitan cities establish that FoodMatch\nis substantially better than baseline strategies on a number of metrics, while\nbeing efficient enough to handle real-world workloads.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 03:42:03 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Joshi", "Manas", ""], ["Singh", "Arshdeep", ""], ["Ranu", "Sayan", ""], ["Bagchi", "Amitabha", ""], ["Karia", "Priyank", ""], ["Kala", "Puneet", ""]]}, {"id": "2008.13028", "submitter": "Guizhen Wang", "authors": "Guizhen Wang, Jingjing Guo, Mingjie Tang, Jos\\'e Florencio de Queiroz\n  Neto, Calvin Yau, Anas Daghistani, Morteza Karimzadeh, Walid G. Aref, David\n  S. Ebert", "title": "STULL: Unbiased Online Sampling for Visual Exploration of Large\n  Spatiotemporal Data", "comments": "IEEE VIS (InfoVis/VAST/SciVis) 2020 ACM 2012 CCS - Human-centered\n  computing, Visualization, Visualization design and evaluation methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online sampling-supported visual analytics is increasingly important, as it\nallows users to explore large datasets with acceptable approximate answers at\ninteractive rates. However, existing online spatiotemporal sampling techniques\nare often biased, as most researchers have primarily focused on reducing\ncomputational latency. Biased sampling approaches select data with unequal\nprobabilities and produce results that do not match the exact data\ndistribution, leading end users to incorrect interpretations. In this paper, we\npropose a novel approach to perform unbiased online sampling of large\nspatiotemporal data. The proposed approach ensures the same probability of\nselection to every point that qualifies the specifications of a user's\nmultidimensional query. To achieve unbiased sampling for accurate\nrepresentative interactive visualizations, we design a novel data index and an\nassociated sample retrieval plan. Our proposed sampling approach is suitable\nfor a wide variety of visual analytics tasks, e.g., tasks that run aggregate\nqueries of spatiotemporal data. Extensive experiments confirm the superiority\nof our approach over a state-of-the-art spatial online sampling technique,\ndemonstrating that within the same computational time, data samples generated\nin our approach are at least 50% more accurate in representing the actual\nspatial distribution of the data and enable approximate visualizations to\npresent closer visual appearances to the exact ones.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 18:12:08 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Guizhen", ""], ["Guo", "Jingjing", ""], ["Tang", "Mingjie", ""], ["Neto", "Jos\u00e9 Florencio de Queiroz", ""], ["Yau", "Calvin", ""], ["Daghistani", "Anas", ""], ["Karimzadeh", "Morteza", ""], ["Aref", "Walid G.", ""], ["Ebert", "David S.", ""]]}, {"id": "2008.13151", "submitter": "Milan Lopuha\\\"a-Zwakenberg", "authors": "Milan Lopuha\\\"a-Zwakenberg, Haochen Tong and Boris \\v{S}kori\\'c", "title": "Data Sanitisation Protocols for the Privacy Funnel with Differential\n  Privacy Guarantees", "comments": "This preprint is an extended version of arXiv:2002.01501 (Fourteenth\n  International Conference on the Digital Society, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Open Data approach, governments and other public organisations want to\nshare their datasets with the public, for accountability and to support\nparticipation. Data must be opened in such a way that individual privacy is\nsafeguarded. The Privacy Funnel is a mathematical approach that produces a\nsanitised database that does not leak private data beyond a chosen threshold.\nThe downsides to this approach are that it does not give worst-case privacy\nguarantees, and that finding optimal sanitisation protocols can be\ncomputationally prohibitive. We tackle these problems by using differential\nprivacy metrics, and by considering local protocols which operate on one entry\nat a time. We show that under both the Local Differential Privacy and Local\nInformation Privacy leakage metrics, one can efficiently obtain optimal\nprotocols. Furthermore, Local Information Privacy is both more closely aligned\nto the privacy requirements of the Privacy Funnel scenario, and more\nefficiently computable. We also consider the scenario where each user has\nmultiple attributes, for which we define Side-channel Resistant Local\nInformation Privacy, and we give efficient methods to find protocols satisfying\nthis criterion while still offering good utility. Finally, we introduce\nConditional Reporting, an explicit LIP protocol that can be used when the\noptimal protocol is infeasible to compute, and we test this protocol on\nreal-world and synthetic data. Experiments on real-world and synthetic data\nconfirm the validity of these methods.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 12:19:34 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lopuha\u00e4-Zwakenberg", "Milan", ""], ["Tong", "Haochen", ""], ["\u0160kori\u0107", "Boris", ""]]}, {"id": "2008.13432", "submitter": "Michele Linardi", "authors": "Michele Linardi and Yan Zhu and Themis Palpanas and Eamonn Keogh", "title": "VALMOD: A Suite for Easy and Exact Detection of Variable Length Motifs\n  in Data Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data series motif discovery represents one of the most useful primitives for\ndata series mining, with applications to many domains, such as robotics,\nentomology, seismology, medicine, and climatology, and others. The\nstate-of-the-art motif discovery tools still require the user to provide the\nmotif length. Yet, in several cases, the choice of motif length is critical for\ntheir detection. Unfortunately, the obvious brute-force solution, which tests\nall lengths within a given range, is computationally untenable, and does not\nprovide any support for ranking motifs at different resolutions (i.e.,\nlengths). We demonstrate VALMOD, our scalable motif discovery algorithm that\nefficiently finds all motifs in a given range of lengths, and outputs a\nlength-invariant ranking of motifs. Furthermore, we support the analysis\nprocess by means of a newly proposed meta-data structure that helps the user to\nselect the most promising pattern length. This demo aims at illustrating in\ndetail the steps of the proposed approach, showcasing how our algorithm and\ncorresponding graphical insights enable users to efficiently identify the\ncorrect motifs. (Paper published in ACM Sigmod Conference 2018.)\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 08:44:36 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Linardi", "Michele", ""], ["Zhu", "Yan", ""], ["Palpanas", "Themis", ""], ["Keogh", "Eamonn", ""]]}, {"id": "2008.13447", "submitter": "Michele Linardi", "authors": "Michele Linardi and Yan Zhu and Themis Palpanas and Eamonn Keogh", "title": "Matrix Profile Goes MAD: Variable-Length Motif And Discord Discovery in\n  Data Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last fifteen years, data series motif and discord discovery have\nemerged as two useful and well-used primitives for data series mining, with\napplications to many domains, including robotics, entomology, seismology,\nmedicine, and climatology. Nevertheless, the state-of-the-art motif and discord\ndiscovery tools still require the user to provide the relative length. Yet, in\nseveral cases, the choice of length is critical and unforgiving. Unfortunately,\nthe obvious brute-force solution, which tests all lengths within a given range,\nis computationally untenable. In this work, we introduce a new framework, which\nprovides an exact and scalable motif and discord discovery algorithm that\nefficiently finds all motifs and discords in a given range of lengths. We\nevaluate our approach with five diverse real datasets, and demonstrate that it\nis up to 20 times faster than the state-of-the-art. Our results also show that\nremoving the unrealistic assumption that the user knows the correct length, can\noften produce more intuitive and actionable results, which could have otherwise\nbeen missed. (Paper published in Data Mining and Knowledge Discovery Journal -\n2020)\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 09:19:58 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Linardi", "Michele", ""], ["Zhu", "Yan", ""], ["Palpanas", "Themis", ""], ["Keogh", "Eamonn", ""]]}, {"id": "2008.13482", "submitter": "Samaneh Jozashoori", "authors": "Samaneh Jozashoori, David Chaves-Fraga, Enrique Iglesias, Maria-Esther\n  Vidal, Oscar Corcho", "title": "FunMap: Efficient Execution of Functional Mappings for Knowledge Graph\n  Creation", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.3993657", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data has exponentially grown in the last years, and knowledge graphs\nconstitute powerful formalisms to integrate a myriad of existing data sources.\nTransformation functions -- specified with function-based mapping languages\nlike FunUL and RML+FnO -- can be applied to overcome interoperability issues\nacross heterogeneous data sources. However, the absence of engines to\nefficiently execute these mapping languages hinders their global adoption. We\npropose FunMap, an interpreter of function-based mapping languages; it relies\non a set of lossless rewriting rules to push down and materialize the execution\nof functions in initial steps of knowledge graph creation. Although applicable\nto any function-based mapping language that supports joins between mapping\nrules, FunMap feasibility is shown on RML+FnO. FunMap reduces data redundancy,\ne.g., duplicates and unused attributes, and converts RML+FnO mappings into a\nset of equivalent rules executable on RML-compliant engines. We evaluate FunMap\nperformance over real-world testbeds from the biomedical domain. The results\nindicate that FunMap reduces the execution time of RML-compliant engines by up\nto a factor of 18, furnishing, thus, a scalable solution for knowledge graph\ncreation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 10:48:41 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 08:51:52 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Jozashoori", "Samaneh", ""], ["Chaves-Fraga", "David", ""], ["Iglesias", "Enrique", ""], ["Vidal", "Maria-Esther", ""], ["Corcho", "Oscar", ""]]}]