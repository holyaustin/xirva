[{"id": "1710.00027", "submitter": "AnHai Doan", "authors": "AnHai Doan, Adel Ardalan, Jeffrey R. Ballard, Sanjib Das, Yash Govind,\n  Pradap Konda, Han Li, Erik Paulson, Paul Suganthan G.C., Haojun Zhang", "title": "Toward a System Building Agenda for Data Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we argue that the data management community should devote far\nmore effort to building data integration (DI) systems, in order to truly\nadvance the field. Toward this goal, we make three contributions. First, we\ndraw on our recent industrial experience to discuss the limitations of current\nDI systems. Second, we propose an agenda to build a new kind of DI systems to\naddress these limitations. These systems guide users through the DI workflow,\nstep by step. They provide tools to address the \"pain points\" of the steps, and\ntools are built on top of the Python data science and Big Data ecosystem\n(PyData). We discuss how to foster an ecosystem of such tools within PyData,\nthen use it to build DI systems for collaborative/cloud/crowd/lay user\nsettings. Finally, we discuss ongoing work at Wisconsin, which suggests that\nthese DI systems are highly promising and building them raises many interesting\nresearch challenges.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 18:43:23 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Doan", "AnHai", ""], ["Ardalan", "Adel", ""], ["Ballard", "Jeffrey R.", ""], ["Das", "Sanjib", ""], ["Govind", "Yash", ""], ["Konda", "Pradap", ""], ["Li", "Han", ""], ["Paulson", "Erik", ""], ["C.", "Paul Suganthan G.", ""], ["Zhang", "Haojun", ""]]}, {"id": "1710.00204", "submitter": "Zhaoqiang Chen", "authors": "Zhaoqiang Chen, Qun Chen, Fengfeng Fan, Yanyan Wang, Zhuo Wang, Youcef\n  Nafa, Zhanhuai Li, Hailong Liu, Wei Pan", "title": "Enabling Quality Control for Entity Resolution: A Human and Machine\n  Cooperation Framework", "comments": "12 pages, 11 figures. Camera-ready version of the paper submitted to\n  ICDE 2018, In Proceedings of the 34th IEEE International Conference on Data\n  Engineering (ICDE 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though many machine algorithms have been proposed for entity resolution,\nit remains very challenging to find a solution with quality guarantees. In this\npaper, we propose a novel HUman and Machine cOoperation (HUMO) framework for\nentity resolution (ER), which divides an ER workload between the machine and\nthe human. HUMO enables a mechanism for quality control that can flexibly\nenforce both precision and recall levels. We introduce the optimization problem\nof HUMO, minimizing human cost given a quality requirement, and then present\nthree optimization approaches: a conservative baseline one purely based on the\nmonotonicity assumption of precision, a more aggressive one based on sampling\nand a hybrid one that can take advantage of the strengths of both previous\napproaches. Finally, we demonstrate by extensive experiments on real and\nsynthetic datasets that HUMO can achieve high-quality results with reasonable\nreturn on investment (ROI) in terms of human cost, and it performs considerably\nbetter than the state-of-the-art alternatives in quality control.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 14:18:24 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 07:48:07 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Chen", "Zhaoqiang", ""], ["Chen", "Qun", ""], ["Fan", "Fengfeng", ""], ["Wang", "Yanyan", ""], ["Wang", "Zhuo", ""], ["Nafa", "Youcef", ""], ["Li", "Zhanhuai", ""], ["Liu", "Hailong", ""], ["Pan", "Wei", ""]]}, {"id": "1710.00454", "submitter": "Amanpreet Singh", "authors": "Amanpreet Singh, Karthik Venkatesan and Simranjyot Singh Gill", "title": "Building a Structured Query Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding patterns in data and being able to retrieve information from those\npatterns is an important task in Information retrieval. Complex search\nrequirements which are not fulfilled by simple string matching and require\nexploring certain patterns in data demand a better query engine that can\nsupport searching via structured queries. In this article, we built a\nstructured query engine which supports searching data through structured\nqueries on the lines of ElasticSearch. We will show how we achieved real time\nindexing and retrieving of data through a RESTful API and how complex queries\ncan be created and processed using efficient data structures we created for\nstoring the data in structured way. Finally, we will conclude with an example\nof movie recommendation system built on top of this query engine.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 01:54:40 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Singh", "Amanpreet", ""], ["Venkatesan", "Karthik", ""], ["Gill", "Simranjyot Singh", ""]]}, {"id": "1710.00560", "submitter": "Jiaye Wu", "authors": "Jiaye Wu, Peng Wang, Ningting Pan, Chen Wang, Wei Wang, Jianmin Wang", "title": "KV-match: A Subsequence Matching Approach Supporting Normalization and\n  Time Warping [Extended Version]", "comments": "13 pages", "journal-ref": "2019 IEEE 35th International Conference on Data Engineering (ICDE)", "doi": "10.1109/ICDE.2019.00082", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume of time series data has exploded due to the popularity of new\napplications, such as data center management and IoT. Subsequence matching is a\nfundamental task in mining time series data. All index-based approaches only\nconsider raw subsequence matching (RSM) and do not support subsequence\nnormalization. UCR Suite can deal with normalized subsequence match problem\n(NSM), but it needs to scan full time series. In this paper, we propose a novel\nproblem, named constrained normalized subsequence matching problem (cNSM),\nwhich adds some constraints to NSM problem. The cNSM problem provides a knob to\nflexibly control the degree of offset shifting and amplitude scaling, which\nenables users to build the index to process the query. We propose a new index\nstructure, KV-index, and the matching algorithm, KV-match. With a single index,\nour approach can support both RSM and cNSM problems under either ED or DTW\ndistance. KV-index is a key-value structure, which can be easily implemented on\nlocal files or HBase tables. To support the query of arbitrary lengths, we\nextend KV-match to KV-match$_{DP}$, which utilizes multiple varied-length\nindexes to process the query. We conduct extensive experiments on synthetic and\nreal-world datasets. The results verify the effectiveness and efficiency of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 09:59:50 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 07:23:19 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 03:30:01 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wu", "Jiaye", ""], ["Wang", "Peng", ""], ["Pan", "Ningting", ""], ["Wang", "Chen", ""], ["Wang", "Wei", ""], ["Wang", "Jianmin", ""]]}, {"id": "1710.00597", "submitter": "Saravanan Thirumuruganathan", "authors": "Muhammad Ebraheem, Saravanan Thirumuruganathan, Shafiq Joty, Mourad\n  Ouzzani, Nan Tang", "title": "DeepER -- Deep Entity Resolution", "comments": "Accepted to PVLDB 2018 as \"Distributed Representations of Tuples for\n  Entity Resolution\". This version corrects a minor issue in Example 4 pointed\n  out by Andrew Borthwick and Matthias Boehm", "journal-ref": null, "doi": "10.14778/3236187.3236198", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is a key data integration problem. Despite the efforts\nin 70+ years in all aspects of ER, there is still a high demand for\ndemocratizing ER - humans are heavily involved in labeling data, performing\nfeature engineering, tuning parameters, and defining blocking functions. With\nthe recent advances in deep learning, in particular distributed representation\nof words (a.k.a. word embeddings), we present a novel ER system, called DeepER,\nthat achieves good accuracy, high efficiency, as well as ease-of-use (i.e.,\nmuch less human efforts). For accuracy, we use sophisticated composition\nmethods, namely uni- and bi-directional recurrent neural networks (RNNs) with\nlong short term memory (LSTM) hidden units, to convert each tuple to a\ndistributed representation (i.e., a vector), which can in turn be used to\neffectively capture similarities between tuples. We consider both the case\nwhere pre-trained word embeddings are available as well the case where they are\nnot; we present ways to learn and tune the distributed representations. For\nefficiency, we propose a locality sensitive hashing (LSH) based blocking\napproach that uses distributed representations of tuples; it takes all\nattributes of a tuple into consideration and produces much smaller blocks,\ncompared with traditional methods that consider only a few attributes. For\nease-of-use, DeepER requires much less human labeled data and does not need\nfeature engineering, compared with traditional machine learning based\napproaches which require handcrafted features, and similarity functions along\nwith their associated thresholds. We evaluate our algorithms on multiple\ndatasets (including benchmarks, biomedical data, as well as multi-lingual data)\nand the extensive experimental results show that DeepER outperforms existing\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 12:02:58 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 07:42:50 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2018 17:44:07 GMT"}, {"version": "v4", "created": "Fri, 6 Apr 2018 08:25:01 GMT"}, {"version": "v5", "created": "Sun, 5 Aug 2018 14:57:45 GMT"}, {"version": "v6", "created": "Mon, 18 Nov 2019 20:32:39 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Ebraheem", "Muhammad", ""], ["Thirumuruganathan", "Saravanan", ""], ["Joty", "Shafiq", ""], ["Ouzzani", "Mourad", ""], ["Tang", "Nan", ""]]}, {"id": "1710.00608", "submitter": "Tejas Kulkarni Mr", "authors": "Graham Cormode, Tejas Kulkarni, Divesh Srivastava", "title": "Constrained Differential Privacy for Count Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concern about how to aggregate sensitive user data without compromising\nindividual privacy is a major barrier to greater availability of data. The\nmodel of differential privacy has emerged as an accepted model to release\nsensitive information while giving a statistical guarantee for privacy. Many\ndifferent algorithms are possible to address different target functions. We\nfocus on the core problem of count queries, and seek to design mechanisms to\nrelease data associated with a group of n individuals. Prior work has focused\non designing mechanisms by raw optimization of a loss function, without regard\nto the consequences on the results. This can leads to mechanisms with\nundesirable properties, such as never reporting some outputs (gaps), and\noverreporting others (spikes). We tame these pathological behaviors by\nintroducing a set of desirable properties that mechanisms can obey. Any\ncombination of these can be satisfied by solving a linear program (LP) which\nminimizes a cost function, with constraints enforcing the properties. We focus\non a particular cost function, and provide explicit constructions that are\noptimal for certain combinations of properties, and show a closed form for\ntheir cost. In the end, there are only a handful of distinct optimal mechanisms\nto choose between: one is the well-known (truncated) geometric mechanism; the\nsecond a novel mechanism that we introduce here, and the remainder are found as\nthe solution to particular LPs. These all avoid the bad behaviors we identify.\nWe demonstrate in a set of experiments on real and synthetic data which is\npreferable in practice, for different combinations of data distributions,\nconstraints, and privacy parameters.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 12:37:48 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Cormode", "Graham", ""], ["Kulkarni", "Tejas", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1710.00763", "submitter": "Doris Jung-Lin Lee", "authors": "Doris Jung-Lin Lee, John Lee, Tarique Siddiqui, Jaewoo Kim, Karrie\n  Karahalios, Aditya Parameswaran", "title": "You can't always sketch what you want: Understanding Sensemaking in\n  Visual Query Systems", "comments": "Accepted for presentation at IEEE VAST 2019, to be held October 20-25\n  in Vancouver, Canada. Paper will also be published in a special issue of IEEE\n  Transactions on Visualization and Computer Graphics (TVCG) IEEE VIS\n  (InfoVis/VAST/SciVis) 2019 ACM 2012 CCS - Human-centered computing,\n  Visualization, Visualization design and evaluation methods", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934666", "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual query systems (VQSs) empower users to interactively search for line\ncharts with desired visual patterns, typically specified using intuitive\nsketch-based interfaces. Despite decades of past work on VQSs, these efforts\nhave not translated to adoption in practice, possibly because VQSs are largely\nevaluated in unrealistic lab-based settings. To remedy this gap in adoption, we\ncollaborated with experts from three diverse domains---astronomy, genetics, and\nmaterial science---via a year-long user-centered design process to develop a\nVQS that supports their workflow and analytical needs, and evaluate how VQSs\ncan be used in practice. Our study results reveal that ad-hoc sketch-only\nquerying is not as commonly used as prior work suggests, since analysts are\noften unable to precisely express their patterns of interest. In addition, we\ncharacterize three essential sensemaking processes supported by our enhanced\nVQS. We discover that participants employ all three processes, but in different\nproportions, depending on the analytical needs in each domain. Our findings\nsuggest that all three sensemaking processes must be integrated in order to\nmake future VQSs useful for a wide range of analytical inquiries.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 16:31:24 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 02:06:10 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 17:56:33 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 15:53:41 GMT"}, {"version": "v5", "created": "Sun, 30 Dec 2018 03:38:58 GMT"}, {"version": "v6", "created": "Tue, 16 Jul 2019 19:51:27 GMT"}, {"version": "v7", "created": "Thu, 3 Oct 2019 19:19:02 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Lee", "Doris Jung-Lin", ""], ["Lee", "John", ""], ["Siddiqui", "Tarique", ""], ["Kim", "Jaewoo", ""], ["Karahalios", "Karrie", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1710.00813", "submitter": "Conrad Rosenbrock", "authors": "Conred W. Rosenbrock", "title": "A Practical Python API for Querying AFLOWLIB", "comments": "7 pages, 3 code listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large databases such as aflowlib.org provide valuable data sources for\ndiscovering material trends through machine learning. Although a REST API and\nquery language are available, there is a learning curve associated with the\nAFLUX language that acts as a barrier for new users. Additionally, the data is\nstored using non-standard serialization formats. Here we present a high-level\nAPI that allows immediate access to the aflowlib data using standard python\noperators and language features. It provides an easy way to integrate aflowlib\ndata with other python materials packages such as ase and quippy, and provides\nautomatic deserialization into numpy arrays and python objects. This package is\navailable via \"pip install aflow\".\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 20:38:47 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Rosenbrock", "Conred W.", ""]]}, {"id": "1710.00867", "submitter": "Yanfeng Zhang", "authors": "Shufeng Gong, Yanfeng Zhang, Ge Yu", "title": "Clustering Stream Data by Exploring the Evolution of Density Mountain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream clustering is a fundamental problem in many streaming data analysis\napplications. Comparing to classical batch-mode clustering, there are two key\nchallenges in stream clustering: (i) Given that input data are changing\ncontinuously, how to incrementally update clustering results efficiently? (ii)\nGiven that clusters continuously evolve with the evolution of data, how to\ncapture the cluster evolution activities? Unfortunately, most of existing\nstream clustering algorithms can neither update the cluster result in real time\nnor track the evolution of clusters.\n  In this paper, we propose an stream clustering algorithm EDMStream by\nexploring the Evolution of Density Mountain. The density mountain is used to\nabstract the data distribution, the changes of which indicate data distribution\nevolution. We track the evolution of clusters by monitoring the changes of\ndensity mountains. We further provide efficient data structures and filtering\nschemes to ensure the update of density mountains in real time, which makes\nonline clustering possible. The experimental results on synthetic and real\ndatasets show that, comparing to the state-of-the-art stream clustering\nalgorithms, e.g., D-Stream, DenStream, DBSTREAM and MR-Stream, our algorithm\ncan response to a cluster update much faster (say 7-15x faster than the best of\nthe competitors) and at the same time achieve comparable cluster quality.\nFurthermore, EDMStream can successfully capture the cluster evolution\nactivities.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 18:59:09 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Gong", "Shufeng", ""], ["Zhang", "Yanfeng", ""], ["Yu", "Ge", ""]]}, {"id": "1710.01077", "submitter": "S{\\o}ren Kejser Jensen", "authors": "S{\\o}ren Kejser Jensen, Torben Bach Pedersen, Christian Thomsen", "title": "Time Series Management Systems: A Survey", "comments": "20 Pages, 15 Figures, 2 Tables, Accepted for publication in IEEE TKDE", "journal-ref": "TKDE, 29, 11, 2017, 2581-2600", "doi": "10.1109/TKDE.2017.2740932", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collection of time series data increases as more monitoring and\nautomation are being deployed. These deployments range in scale from an\nInternet of things (IoT) device located in a household to enormous distributed\nCyber-Physical Systems (CPSs) producing large volumes of data at high velocity.\nTo store and analyze these vast amounts of data, specialized Time Series\nManagement Systems (TSMSs) have been developed to overcome the limitations of\ngeneral purpose Database Management Systems (DBMSs) for times series\nmanagement. In this paper, we present a thorough analysis and classification of\nTSMSs developed through academic or industrial research and documented through\npublications. Our classification is organized into categories based on the\narchitectures observed during our analysis. In addition, we provide an overview\nof each system with a focus on the motivational use case that drove the\ndevelopment of the system, the functionality for storage and querying of time\nseries a system implements, the components the system is composed of, and the\ncapabilities of each system with regard to Stream Processing and Approximate\nQuery Processing (AQP). Last, we provide a summary of research directions\nproposed by other researchers in the field and present our vision for a next\ngeneration TSMS.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 11:16:55 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Jensen", "S\u00f8ren Kejser", ""], ["Pedersen", "Torben Bach", ""], ["Thomsen", "Christian", ""]]}, {"id": "1710.01420", "submitter": "Jose Picado", "authors": "Jose Picado, Arash Termehchy, Sudhanshu Pathak, Alan Fern, Praveen\n  Ilango, Yunqiao Cai", "title": "Usable & Scalable Learning Over Relational Data With Automatic Language\n  Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational databases are valuable resources for learning novel and\ninteresting relations and concepts. In order to constraint the search through\nthe large space of candidate definitions, users must tune the algorithm by\nspecifying a language bias. Unfortunately, specifying the language bias is done\nvia trial and error and is guided by the expert's intuitions. We propose\nAutoBias, a system that leverages information in the schema and content of the\ndatabase to automatically induce the language bias used by popular relational\nlearning systems. We show that AutoBias delivers the same accuracy as using\nmanually-written language bias by imposing only a slight overhead on the\nrunning time of the learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 23:36:31 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 18:56:51 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Picado", "Jose", ""], ["Termehchy", "Arash", ""], ["Pathak", "Sudhanshu", ""], ["Fern", "Alan", ""], ["Ilango", "Praveen", ""], ["Cai", "Yunqiao", ""]]}, {"id": "1710.01431", "submitter": "Adithya Vadapalli", "authors": "Grigory Yaroslavtsev, Adithya Vadapalli", "title": "Massively Parallel Algorithms and Hardness for Single-Linkage Clustering\n  Under $\\ell_p$-Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present massively parallel (MPC) algorithms and hardness of approximation\nresults for computing Single-Linkage Clustering of $n$ input $d$-dimensional\nvectors under Hamming, $\\ell_1, \\ell_2$ and $\\ell_\\infty$ distances. All our\nalgorithms run in $O(\\log n)$ rounds of MPC for any fixed $d$ and achieve\n$(1+\\epsilon)$-approximation for all distances (except Hamming for which we\nshow an exact algorithm). We also show constant-factor inapproximability\nresults for $o(\\log n)$-round algorithms under standard MPC hardness\nassumptions (for sufficiently large dimension depending on the distance used).\nEfficiency of implementation of our algorithms in Apache Spark is demonstrated\nthrough experiments on a variety of datasets exhibiting speedups of several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 00:48:54 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 04:08:40 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yaroslavtsev", "Grigory", ""], ["Vadapalli", "Adithya", ""]]}, {"id": "1710.01615", "submitter": "Naoise Holohan", "authors": "Naoise Holohan, Spiros Antonatos, Stefano Braghin and P\\'ol Mac\n  Aonghusa", "title": "($k$,$\\epsilon$)-Anonymity: $k$-Anonymity with $\\epsilon$-Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion in volume and variety of data offers enormous potential for\nresearch and commercial use. Increased availability of personal data is of\nparticular interest in enabling highly customised services tuned to individual\nneeds. Preserving the privacy of individuals against reidentification attacks\nin this fast-moving ecosystem poses significant challenges for a one-size fits\nall approach to anonymisation.\n  In this paper we present ($k$,$\\epsilon$)-anonymisation, an approach that\ncombines the $k$-anonymisation and $\\epsilon$-differential privacy models into\na single coherent framework, providing privacy guarantees at least as strong as\nthose offered by the individual models. Linking risks of less than 5\\% are\nobserved in experimental results, even with modest values of $k$ and\n$\\epsilon$.\n  Our approach is shown to address well-known limitations of $k$-anonymity and\n$\\epsilon$-differential privacy and is validated in an extensive experimental\ncampaign using openly available datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 14:25:39 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Holohan", "Naoise", ""], ["Antonatos", "Spiros", ""], ["Braghin", "Stefano", ""], ["Mac Aonghusa", "P\u00f3l", ""]]}, {"id": "1710.01792", "submitter": "Ashish Tapdiya", "authors": "Ashish Tapdiya, Yuan Xue, Daniel Fabbri (Vanderbilt University)", "title": "A Comparative Analysis of Materialized Views Selection and Concurrency\n  Control Mechanisms in NoSQL Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing resource demands require relational databases to scale. While\nrelational databases are well suited for vertical scaling, specialized hardware\ncan be expensive. Conversely, emerging NewSQL and NoSQL data stores are\ndesigned to scale horizontally. NewSQL databases provide ACID transaction\nsupport; however, joins are limited to the partition keys, resulting in\nrestricted query expressiveness. On the other hand, NoSQL databases are\ndesigned to scale out linearly on commodity hardware; however, they are limited\nby slow join performance. Hence, we consider if the NoSQL join performance can\nbe improved while ensuring ACID semantics and without drastically sacrificing\nwrite performance, disk utilization and query expressiveness.\n  This paper presents the Synergy system that leverages schema and workload\ndriven mechanism to identify materialized views and a specialized concurrency\ncontrol system on top of a NoSQL database to enable scalable data management\nwith familiar relational conventions. Synergy trades slight write performance\ndegradation and increased disk utilization for faster join performance\n(compared to standard NoSQL databases) and improved query expressiveness\n(compared to NewSQL databases). Experimental results using the TPC-W benchmark\nshow that, for a database populated with 1M customers, the Synergy system\nexhibits a maximum performance improvement of 80.5% as compared to other\nevaluated systems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 20:24:57 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Tapdiya", "Ashish", "", "Vanderbilt University"], ["Xue", "Yuan", "", "Vanderbilt University"], ["Fabbri", "Daniel", "", "Vanderbilt University"]]}, {"id": "1710.01854", "submitter": "Niranjan Kamat", "authors": "Niranjan Kamat, Arnab Nandi", "title": "InfiniViz: Interactive Visual Exploration using Progressive Bin\n  Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive visualizations can accelerate the data analysis loop through\nnear-instantaneous feedback. To achieve interactivity, techniques such as data\ncubes and sampling are typically employed. While data cubes can speedup\nquerying for moderate-sized datasets, they are ineffective at doing so at a\nlarger scales due to the size of the materialized data cubes. On the other\nhand, while sampling can help scale to large datasets, it adds sampling error\nand the associated issues into the process.\n  While increasing accuracy by looking at more data may sometimes be valuable,\nproviding result minutiae might not be necessary if they do not impart\nadditional significant information. Indeed, such details not only incur a\nhigher \\emph{computational} cost, but also tax the \\emph{cognitive} load of the\nanalyst with worthless trivia. To reduce both the computational and cognitive\nexpenses, we introduce \\emph{InfiniViz}. Through a novel result\nrefinement-based querying paradigm, \\emph{InfiniViz} provides error-free\nresults for large datasets by increasing bin resolutions progressively over\ntime. Through real and simulated workloads over real and benchmark datasets, we\nevaluate and demonstrate \\emph{InfiniViz}'s utility at reducing both cognitive\nand computational costs, while minimizing information loss.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 01:55:13 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Kamat", "Niranjan", ""], ["Nandi", "Arnab", ""]]}, {"id": "1710.02030", "submitter": "Ali Pesaranghader", "authors": "Ali Pesaranghader, Herna Viktor, Eric Paquet", "title": "McDiarmid Drift Detection Methods for Evolving Data Streams", "comments": "9 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly, Internet of Things (IoT) domains, such as sensor networks,\nsmart cities, and social networks, generate vast amounts of data. Such data are\nnot only unbounded and rapidly evolving. Rather, the content thereof\ndynamically evolves over time, often in unforeseen ways. These variations are\ndue to so-called concept drifts, caused by changes in the underlying data\ngeneration mechanisms. In a classification setting, concept drift causes the\npreviously learned models to become inaccurate, unsafe and even unusable.\nAccordingly, concept drifts need to be detected, and handled, as soon as\npossible. In medical applications and emergency response settings, for example,\nchange in behaviours should be detected in near real-time, to avoid potential\nloss of life. To this end, we introduce the McDiarmid Drift Detection Method\n(MDDM), which utilizes McDiarmid's inequality in order to detect concept drift.\nThe MDDM approach proceeds by sliding a window over prediction results, and\nassociate window entries with weights. Higher weights are assigned to the most\nrecent entries, in order to emphasize their importance. As instances are\nprocessed, the detection algorithm compares a weighted mean of elements inside\nthe sliding window with the maximum weighted mean observed so far. A\nsignificant difference between the two weighted means, upper-bounded by the\nMcDiarmid inequality, implies a concept drift. Our extensive experimentation\nagainst synthetic and real-world data streams show that our novel method\noutperforms the state-of-the-art. Specifically, MDDM yields shorter detection\ndelays as well as lower false negative rates, while maintaining high\nclassification accuracies.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 14:02:28 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 19:03:06 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Pesaranghader", "Ali", ""], ["Viktor", "Herna", ""], ["Paquet", "Eric", ""]]}, {"id": "1710.02035", "submitter": "Noman Islam Dr.", "authors": "Noman Islam, Zubair A. Shaikh, Aqeel-ur-Rehman, Muhammad Shahab\n  Siddiqui", "title": "HANDY: A Hybrid Association Rules Mining Approach for Network Layer\n  Discovery of Services for Mobile Ad hoc Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Ad hoc Network (MANET) is an infrastructure-less network formed\nbetween a set of mobile nodes. The discovery of services in MANET is a\nchallenging job due to the unique properties of network. In this paper, a novel\nservice discovery framework called Hybrid Association Rules Based Network Layer\nDiscovery of Services for Ad hoc Networks (HANDY) has been proposed. HANDY\nprovides three major research contributions. At first, it adopts a cross-layer\noptimized design for discovery of services that is based on simultaneous\ndiscovery of services and corresponding routes. Secondly, it provides a\nmulti-level ontology-based approach to describe the services. This resolves the\nissue of semantic interoperability among the service consumers in a scalable\nfashion. Finally, to further optimize the performance of the discovery process,\nHANDY recommends exploiting the inherent associations present among the\nservices. These associations are used in two ways. First, periodic service\nadvertisements are performed based on these associations. In addition, when a\nresponse of a service discovery request is generated, correlated services are\nalso attached with the response. The proposed service discovery scheme has been\nimplemented in JIST/SWANS simulator. The results demonstrate that the proposed\nmodifications give rise to improvement in hit ratio of the service consumers\nand latency of discovery process.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 14:49:11 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Islam", "Noman", ""], ["Shaikh", "Zubair A.", ""], ["Aqeel-ur-Rehman", "", ""], ["Siddiqui", "Muhammad Shahab", ""]]}, {"id": "1710.02261", "submitter": "Sejoon Oh", "authors": "Sejoon Oh, Namyong Park, Lee Sael, and U Kang", "title": "Scalable Tucker Factorization for Sparse Tensors - Algorithms and\n  Discoveries", "comments": "IEEE International Conference on Data Engineering (ICDE 2018)", "journal-ref": null, "doi": "10.1109/ICDE.2018.00104", "report-no": null, "categories": "cs.NA cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given sparse multi-dimensional data (e.g., (user, movie, time; rating) for\nmovie recommendations), how can we discover latent concepts/relations and\npredict missing values? Tucker factorization has been widely used to solve such\nproblems with multi-dimensional data, which are modeled as tensors. However,\nmost Tucker factorization algorithms regard and estimate missing entries as\nzeros, which triggers a highly inaccurate decomposition. Moreover, few methods\nfocusing on an accuracy exhibit limited scalability since they require huge\nmemory and heavy computational costs while updating factor matrices. In this\npaper, we propose P-Tucker, a scalable Tucker factorization method for sparse\ntensors. P-Tucker performs alternating least squares with a row-wise update\nrule in a fully parallel way, which significantly reduces memory requirements\nfor updating factor matrices. Furthermore, we offer two variants of P-Tucker: a\ncaching algorithm P-Tucker-Cache and an approximation algorithm\nP-Tucker-Approx, both of which accelerate the update process. Experimental\nresults show that P-Tucker exhibits 1.7-14.1x speed-up and 1.4-4.8x less error\ncompared to the state-of-the-art. In addition, P-Tucker scales near linearly\nwith the number of observable entries in a tensor and number of threads. Thanks\nto P-Tucker, we successfully discover hidden concepts and relations in a\nlarge-scale real-world tensor, while existing methods cannot reveal latent\nfeatures due to their limited scalability or low accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 02:54:44 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 05:24:52 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Oh", "Sejoon", ""], ["Park", "Namyong", ""], ["Sael", "Lee", ""], ["Kang", "U", ""]]}, {"id": "1710.02317", "submitter": "Wim Martens", "authors": "Wim Martens and Tina Trautner", "title": "Enumeration Problems for Regular Path Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of regular path queries (RPQs) is a central problem in graph\ndatabases. We investigate the corresponding enumeration problem, that is, given\na graph and an RPQ, enumerate all paths in the graph that match the RPQ. We\nconsider several versions of this problem, corresponding to different semantics\nof RPQs that have recently been considered: arbitrary paths, shortest paths,\nsimple paths, and trails. Whereas arbitrary and shortest paths can be\nenumerated in polynomial delay, the situation is much more intricate for simple\npaths and trails. For instance, already the question if a given graph contains\na simple path or trail of a certain length has cases with highly non-trivial\nsolutions and cases that are long-standing open problems. In this setting, we\nstudy RPQ evaluation from a parameterized complexity perspective. We define a\nclass of simple transitive expressions that is prominent in practice and for\nwhich we can prove two dichotomy-like results: one for simple paths and one for\ntrails paths. We observe that, even though simple path semantics and trail\nsemantics are intractable for RPQs in general, they are feasible for the vast\nmajority of the kinds of RPQs that users use in practice. At the heart of this\nstudy is a result of independent interest on the parameterized complexity of\nfinding disjoint paths in graphs: the two disjoint paths problem is W[1]-hard\nif parameterized by the length of one of the two paths.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 09:04:45 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Martens", "Wim", ""], ["Trautner", "Tina", ""]]}, {"id": "1710.02690", "submitter": "Rebecca Steorts", "authors": "Beidi Chen, Anshumali Shrivastava, Rebecca C. Steorts", "title": "Unique Entity Estimation with Application to the Syrian Conflict", "comments": "35 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution identifies and removes duplicate entities in large, noisy\ndatabases and has grown in both usage and new developments as a result of\nincreased data availability. Nevertheless, entity resolution has tradeoffs\nregarding assumptions of the data generation process, error rates, and\ncomputational scalability that make it a difficult task for real applications.\nIn this paper, we focus on a related problem of unique entity estimation, which\nis the task of estimating the unique number of entities and associated standard\nerrors in a data set with duplicate entities. Unique entity estimation shares\nmany fundamental challenges of entity resolution, namely, that the\ncomputational cost of all-to-all entity comparisons is intractable for large\ndatabases. To circumvent this computational barrier, we propose an efficient\n(near-linear time) estimation algorithm based on locality sensitive hashing.\nOur estimator, under realistic assumptions, is unbiased and has provably low\nvariance compared to existing random sampling based approaches. In addition, we\nempirically show its superiority over the state-of-the-art estimators on three\nreal applications. The motivation for our work is to derive an accurate\nestimate of the documented, identifiable deaths in the ongoing Syrian conflict.\nOur methodology, when applied to the Syrian data set, provides an estimate of\n$191,874 \\pm 1772$ documented, identifiable deaths, which is very close to the\nHuman Rights Data Analysis Group (HRDAG) estimate of 191,369. Our work provides\nan example of challenges and efforts involved in solving a real, noisy\nchallenging problem where modeling assumptions may not hold.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 14:30:20 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Chen", "Beidi", ""], ["Shrivastava", "Anshumali", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1710.02817", "submitter": "Sun Jizhou", "authors": "Jizhou Sun, Jianzhong Li and Hong Gao", "title": "Discovery of Paradigm Dependencies", "comments": "This paper is submitted to 34th IEEE International Conference on Data\n  Engineering (ICDE2018) On October 1, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing and incorrect values often cause serious consequences. To deal with\nthese data quality problems, a class of common employed tools are dependency\nrules, such as Functional Dependencies (FDs), Conditional Functional\nDependencies (CFDs) and Edition Rules (ERs), etc. The stronger expressing\nability a dependency has, data with the better quality can be obtained. To the\nbest of our knowledge, all previous dependencies treat each attribute value as\na non-splittable whole. Actually however, in many applications, part of a value\nmay contains meaningful information, indicating that more powerful dependency\nrules to handle data quality problems are possible. In this paper, we consider\nof discovering such type of dependencies in which the left hand side is part of\na regular-expression-like paradigm, named Paradigm Dependencies (PDs). PDs tell\nthat if a string matches the paradigm, element at the specified position can\ndecides a certain other attribute's value. We propose a framework in which\nstrings with similar coding rules and different lengths are clustered together\nand aligned vertically, from which PDs can be discovered directly. The aligning\nproblem is the key component of this framework and is proved in NP-Complete. A\ngreedy algorithm is introduced in which the clustering and aligning tasks can\nbe accomplished simultaneously. Because of the greedy algorithm's high time\ncomplexity, several pruning strategies are proposed to reduce the running time.\nIn the experimental study, three real datasets as well as several synthetical\ndatasets are employed to verify our methods' effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 09:53:57 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Sun", "Jizhou", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}, {"id": "1710.02823", "submitter": "Markku Hinkka", "authors": "Markku Hinkka, Teemu Lehto, Keijo Heljanko, Alexander Jung", "title": "Structural Feature Selection for Event Logs", "comments": "Extended version of a paper published in the proceedings of the BPM\n  2017 workshops", "journal-ref": null, "doi": "10.1007/978-3-319-74030-0_2", "report-no": null, "categories": "cs.LG cs.DB cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classifying business process instances based on\nstructural features derived from event logs. The main motivation is to provide\nmachine learning based techniques with quick response times for interactive\ncomputer assisted root cause analysis. In particular, we create structural\nfeatures from process mining such as activity and transition occurrence counts,\nand ordering of activities to be evaluated as potential features for\nclassification. We show that adding such structural features increases the\namount of information thus potentially increasing classification accuracy.\nHowever, there is an inherent trade-off as using too many features leads to too\nlong run-times for machine learning classification models. One way to improve\nthe machine learning algorithms' run-time is to only select a small number of\nfeatures by a feature selection algorithm. However, the run-time required by\nthe feature selection algorithm must also be taken into account. Also, the\nclassification accuracy should not suffer too much from the feature selection.\nThe main contributions of this paper are as follows: First, we propose and\ncompare six different feature selection algorithms by means of an experimental\nsetup comparing their classification accuracy and achievable response times.\nSecond, we discuss the potential use of feature selection results for computer\nassisted root cause analysis as well as the properties of different types of\nstructural features in the context of feature selection.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 11:38:37 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 08:54:22 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Hinkka", "Markku", ""], ["Lehto", "Teemu", ""], ["Heljanko", "Keijo", ""], ["Jung", "Alexander", ""]]}, {"id": "1710.03222", "submitter": "Kasun Bandara", "authors": "Kasun Bandara, Christoph Bergmeir, Slawek Smyl", "title": "Forecasting Across Time Series Databases using Recurrent Neural Networks\n  on Groups of Similar Series: A Clustering Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of Big Data, nowadays in many applications databases\ncontaining large quantities of similar time series are available. Forecasting\ntime series in these domains with traditional univariate forecasting procedures\nleaves great potentials for producing accurate forecasts untapped. Recurrent\nneural networks (RNNs), and in particular Long Short-Term Memory (LSTM)\nnetworks, have proven recently that they are able to outperform\nstate-of-the-art univariate time series forecasting methods in this context\nwhen trained across all available time series. However, if the time series\ndatabase is heterogeneous, accuracy may degenerate, so that on the way towards\nfully automatic forecasting methods in this space, a notion of similarity\nbetween the time series needs to be built into the methods. To this end, we\npresent a prediction model that can be used with different types of RNN models\non subgroups of similar time series, which are identified by time series\nclustering techniques. We assess our proposed methodology using LSTM networks,\na widely popular RNN variant. Our method achieves competitive results on\nbenchmarking datasets under competition evaluation procedures. In particular,\nin terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM\nmodel and outperforms all other methods on the CIF2016 forecasting competition\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 04:08:15 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 08:03:34 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Bandara", "Kasun", ""], ["Bergmeir", "Christoph", ""], ["Smyl", "Slawek", ""]]}, {"id": "1710.03289", "submitter": "Rosana Veroneze", "authors": "Rosana Veroneze and Fernando J. Von Zuben", "title": "Efficient mining of maximal biclusters in mixed-attribute datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel enumerative biclustering algorithm to directly\nmine all maximal biclusters in mixed-attribute datasets (containing both\nnumerical and categorical attributes), with or without missing values. The\nproposal is an extension of RIn-Close_CVC, which was originally conceived to\nmine perfect or perturbed biclusters with constant values on columns solely\nfrom numerical datasets, and without missing values. Even endowed with\nadditional and more general features, the extended RIn-Close_CVC retains four\nkey properties: (1) efficiency, (2) completeness, (3) correctness, and (4)\nnon-redundancy. Our proposal is the first one to deal with mixed-attribute\ndatasets without requiring any pre-processing step, such as discretization and\nitemization of real-valued attributes. This is a decisive aspect, because\ndiscretization and itemization implies a priori decisions, with information\nloss and no clear control over the consequences. On the other hand, even having\nto specify a priori an individual threshold for each numerical attribute, that\nwill be used to indicate internal consistency per attribute, each threshold\nwill be applied during the construction of the biclusters, shaping the\npeculiarities of the data distribution. We also explore the strong connection\nbetween biclustering and frequent pattern mining to (1) provide filters to\nselect a compact bicluster set that exhibits high relevance and low redundancy,\nand (2) in the case of labeled datasets, automatically present the biclusters\nin a user-friendly and intuitive form, by means of quantitative class\nassociation rules. Our experimental results showed that the biclusters yield a\nparsimonious set of relevant rules, providing useful and interpretable models\nfor five mixed-attribute labeled datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 19:59:19 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Veroneze", "Rosana", ""], ["Von Zuben", "Fernando J.", ""]]}, {"id": "1710.03439", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu, Jianxun Liu, Mengying Guo, Yungang Bao, Wenlong Ma,\n  Zhuoyue Liu, Kunpeng Song, Yingchun Yang", "title": "BestConfig: Tapping the Performance Potential of Systems via Automatic\n  Configuration Tuning", "comments": null, "journal-ref": "ACM SoCC 2017", "doi": "10.1145/3127479.3128605", "report-no": null, "categories": "cs.PF cs.DB cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ever increasing number of configuration parameters are provided to system\nusers. But many users have used one configuration setting across different\nworkloads, leaving untapped the performance potential of systems. A good\nconfiguration setting can greatly improve the performance of a deployed system\nunder certain workloads. But with tens or hundreds of parameters, it becomes a\nhighly costly task to decide which configuration setting leads to the best\nperformance. While such task requires the strong expertise in both the system\nand the application, users commonly lack such expertise.\n  To help users tap the performance potential of systems, we present\nBestConfig, a system for automatically finding a best configuration setting\nwithin a resource limit for a deployed system under a given application\nworkload. BestConfig is designed with an extensible architecture to automate\nthe configuration tuning for general systems. To tune system configurations\nwithin a resource limit, we propose the divide-and-diverge sampling method and\nthe recursive bound-and-search algorithm. BestConfig can improve the throughput\nof Tomcat by 75%, that of Cassandra by 63%, that of MySQL by 430%, and reduce\nthe running time of Hive join job by about 50% and that of Spark join job by\nabout 80%, solely by configuration adjustment.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 08:10:06 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Zhu", "Yuqing", ""], ["Liu", "Jianxun", ""], ["Guo", "Mengying", ""], ["Bao", "Yungang", ""], ["Ma", "Wenlong", ""], ["Liu", "Zhuoyue", ""], ["Song", "Kunpeng", ""], ["Yang", "Yingchun", ""]]}, {"id": "1710.03852", "submitter": "Hongwei Liang", "authors": "Hongwei Liang, Ke Wang", "title": "Top-k Route Search through Submodularity Modeling of Recurrent POI\n  Features", "comments": "11 pages, 7 figures, 2 tables", "journal-ref": "Hongwei Liang and Ke Wang. 2018. Top-k Route Search through\n  Submodularity Modeling of Recurrent POI Features. In The 41st International\n  ACM SIGIR Conference on Research & Development in Information Retrieval\n  (SIGIR '18). ACM, 545-554", "doi": "10.1145/3209978.3210038", "report-no": null, "categories": "cs.SI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a practical top-k route search problem: given a collection of\npoints of interest (POIs) with rated features and traveling costs between POIs,\na user wants to find k routes from a source to a destination and limited in a\ncost budget, that maximally match her needs on feature preferences. One\nchallenge is dealing with the personalized diversity requirement where users\nhave various trade-off between quantity (the number of POIs with a specified\nfeature) and variety (the coverage of specified features). Another challenge is\nthe large scale of the POI map and the great many alternative routes to search.\nWe model the personalized diversity requirement by the whole class of\nsubmodular functions, and present an optimal solution to the top-k route search\nproblem through indices for retrieving relevant POIs in both feature and route\nspaces and various strategies for pruning the search space using user\npreferences and constraints. We also present promising heuristic solutions and\nevaluate all the solutions on real life data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 23:00:38 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 22:22:18 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Liang", "Hongwei", ""], ["Wang", "Ke", ""]]}, {"id": "1710.04031", "submitter": "Robin Haunschild", "authors": "Robin Haunschild, Sven E. Hug, Martin P. Br\\\"andle, and Lutz Bornmann", "title": "The number of linked references of publications in Microsoft Academic in\n  comparison with the Web of Science", "comments": "6 pages", "journal-ref": null, "doi": "10.1007/s11192-017-2567-8", "report-no": null, "categories": "cs.DL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a comprehensive Microsoft Academic (MA) study, we explored\nin an initial step the quality of linked references data in MA in comparison\nwith Web of Science (WoS). Linked references are the backbone of bibliometrics,\nbecause they are the basis of the times cited information in citation indexes.\nWe found that the concordance of linked references between MA and WoS ranges\nfrom weak to non-existent for the full sample (publications of the University\nof Zurich with less than 50 linked references in MA). An analysis with a sample\nrestricted to less than 50 linked references in WoS showed a strong agreement\nbetween linked references in MA and WoS.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 12:15:55 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Haunschild", "Robin", ""], ["Hug", "Sven E.", ""], ["Br\u00e4ndle", "Martin P.", ""], ["Bornmann", "Lutz", ""]]}, {"id": "1710.04144", "submitter": "Booma Sowkarthiga Balasubramani", "authors": "Booma Sowkarthiga Balasubramani, Omar Belingheri, Eric S. Boria,\n  Isabel F. Cruz, Sybil Derrible, Michael D. Siciliano", "title": "GUIDES - Geospatial Urban Infrastructure Data Engineering Solutions", "comments": "4 pages, SIGSPATIAL'17, November 7-10, 2017, Los Angeles Area, CA,\n  USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the underground infrastructure systems of cities age, maintenance and\nrepair become an increasing concern. Cities face difficulties in planning\nmaintenance, predicting and responding to infrastructure related issues, and in\nrealizing their vision to be a smart city due to their incomplete understanding\nof the existing state of the infrastructure. Only few cities have accurate and\ncomplete digital information on their underground infrastructure (e.g.,\nelectricity, water, natural gas) systems, which poses problems to those\nplanning and performing construction projects. To address these issues, we\nintroduce GUIDES as a new data conversion and management framework for urban\nunderground infrastructure systems that enable city administrators, workers,\nand contractors along with the general public and other users to query\ndigitized and integrated data to make smarter decisions. This demo paper\npresents the GUIDES architecture and describes two of its central components:\n(i) mapping of underground infrastructure systems, and (ii) integration of\nheterogeneous geospatial data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 15:58:21 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Balasubramani", "Booma Sowkarthiga", ""], ["Belingheri", "Omar", ""], ["Boria", "Eric S.", ""], ["Cruz", "Isabel F.", ""], ["Derrible", "Sybil", ""], ["Siciliano", "Michael D.", ""]]}, {"id": "1710.04419", "submitter": "Jan Otop", "authors": "Jakub Michaliszyn, Jan Otop, Piotr Wieczorek", "title": "Querying Best Paths in Graph Databases", "comments": "A conference version fo this paper has been accepted to FSTTCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying graph databases has recently received much attention. We propose a\nnew approach to this problem, which balances competing goals of expressive\npower, language clarity and computational complexity. A distinctive feature of\nour approach is the ability to express properties of minimal (e.g. shortest)\nand maximal (e.g. most valuable) paths satisfying given criteria. To express\ncomplex properties in a modular way, we introduce labelling-generating\nontologies. The resulting formalism is computationally attractive -- queries\ncan be answered in non-deterministic logarithmic space in the size of the\ndatabase.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 09:30:34 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Michaliszyn", "Jakub", ""], ["Otop", "Jan", ""], ["Wieczorek", "Piotr", ""]]}, {"id": "1710.04469", "submitter": "Ali Shoker", "authors": "Carlos Baquero, Paulo Sergio Almeida and Ali Shoker", "title": "Pure Operation-Based Replicated Data Types", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems designed to serve clients across the world often make use\nof geo-replication to attain low latency and high availability. Conflict-free\nReplicated Data Types (CRDTs) allow the design of predictable multi-master\nreplication and support eventual consistency of replicas that are allowed to\ntransiently diverge. CRDTs come in two flavors: state-based, where a state is\nchanged locally and shipped and merged into other replicas; operation-based,\nwhere operations are issued locally and reliably causal broadcast to all other\nreplicas. However, the standard definition of op-based CRDTs is very\nencompassing, allowing even sending the full-state, and thus imposing storage\nand dissemination overheads as well as blurring the distinction from\nstate-based CRDTs. We introduce pure op-based CRDTs, that can only send\noperations to other replicas, drawing a clear distinction from state-based\nones. Data types with commutative operations can be trivially implemented as\npure op-based CRDTs using standard reliable causal delivery; whereas data types\nhaving non-commutative operations are implemented using a PO-Log, a partially\nordered log of operations, and making use of an extended API, i.e., a Tagged\nCausal Stable Broadcast (TCSB), that provides extra causality information upon\ndelivery and later informs when delivered messages become causally stable,\nallowing further PO-Log compaction. The framework is illustrated by a catalog\nof pure op-based specifications for classic CRDTs, including counters,\nmulti-value registers, add-wins and remove-wins sets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 12:18:30 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Baquero", "Carlos", ""], ["Almeida", "Paulo Sergio", ""], ["Shoker", "Ali", ""]]}, {"id": "1710.04470", "submitter": "Lior Kogan", "authors": "Lior Kogan", "title": "V1: A Visual Query Language for Property Graphs", "comments": "193 pages, 502 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The property graph is an increasingly popular data model. Pattern\nconstruction and pattern matching are important tasks when dealing with\nproperty graphs. Given a property graph schema S, a property graph G, and a\nquery pattern P, all expressed in language L, pattern matching is the process\nof finding, merging, and annotating subgraphs of G that match P. Expressive\npattern languages support topological constraints, property values constraints,\nnegations, quantifications, aggregations, and path semantics. Calculated\nproperties may be defined for vertices, edges, and subgraphs, and constraints\nmay be imposed on their evaluation result.\n  Query posers would like to construct patterns with minimal effort, minimal\ntrial and error, and in a manner that is coherent with the way they think. The\nability to express patterns in a way that is aligned with their mental\nprocesses is crucial to the flow of their work and to the quality of the\ninsights they can draw.\n  Since the capabilities of the human visual system with respect to pattern\nperception are remarkable, it is a matter of course that query patterns were to\nbe expressed visually. Visual query languages have the potential to be much\nmore 'user-friendly' than their textual counterparts in the sense that patterns\nmay be constructed and understood much more quickly and with much less mental\neffort. A long-standing challenge is to design a visual query language that is\ngeneric, has rich expressive power, and is highly receptive and productive. V1\nattempts to answer this challenge.\n  V1 is a declarative visual pattern query language for schema-based property\ngraphs. V1 supports property graphs with mixed (both directed and undirected)\nedges and unary edges, with multivalued and composite properties, and with null\nproperty values. V1 is generic, concise, has rich expressive power, and is\nhighly receptive and productive.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 12:21:17 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 18:42:36 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 19:20:02 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2021 18:13:53 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kogan", "Lior", ""]]}, {"id": "1710.05091", "submitter": "Gourab Mitra", "authors": "Gourab Mitra, Shashidhar Sundareisan and Bikash Kanti Sarkar", "title": "A simple data discretizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data discretization is an important step in the process of machine learning,\nsince it is easier for classifiers to deal with discrete attributes rather than\ncontinuous attributes. Over the years, several methods of performing\ndiscretization such as Boolean Reasoning, Equal Frequency Binning, Entropy have\nbeen proposed, explored, and implemented. In this article, a simple supervised\ndiscretization approach is introduced. The prime goal of MIL is to maximize\nclassification accuracy of classifier, minimizing loss of information while\ndiscretization of continuous attributes. The performance of the suggested\napproach is compared with the supervised discretization algorithm Minimum\nInformation Loss (MIL), using the state-of-the-art rule inductive algorithms-\nJ48 (Java implementation of C4.5 classifier). The presented approach is,\nindeed, the modified version of MIL. The empirical results show that the\nmodified approach performs better in several cases in comparison to the\noriginal MIL algorithm and Minimum Description Length Principle (MDLP) .\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 22:45:11 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Mitra", "Gourab", ""], ["Sundareisan", "Shashidhar", ""], ["Sarkar", "Bikash Kanti", ""]]}, {"id": "1710.05693", "submitter": "David Chapela-Campa", "authors": "David Chapela-Campa, Manuel Mucientes, Manuel Lama", "title": "Mining Frequent Patterns in Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining has emerged as a way to analyze the behavior of an\norganization by extracting knowledge from event logs and by offering techniques\nto discover, monitor and enhance real processes. In the discovery of process\nmodels, retrieving a complex one, i.e., a hardly readable process model, can\nhinder the extraction of information. Even in well-structured process models,\nthere is information that cannot be obtained with the current techniques. In\nthis paper, we present WoMine, an algorithm to retrieve frequent behavioural\npatterns from the model. Our approach searches in process models extracting\nstructures with sequences, selections, parallels and loops, which are\nfrequently executed in the logs. This proposal has been validated with a set of\nprocess models, including some from BPI Challenges, and compared with the state\nof the art techniques. Experiments have validated that WoMine can find all\ntypes of patterns, extracting information that cannot be mined with the state\nof the art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 18:33:19 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Chapela-Campa", "David", ""], ["Mucientes", "Manuel", ""], ["Lama", "Manuel", ""]]}, {"id": "1710.06590", "submitter": "Emeric Dynomant", "authors": "Emeric Dynomant, Mathilde Gorieu, Helene Perrin, Marion Denorme,\n  Fabien Pichon, Arnaud Desfeux", "title": "MEDOC: a Python wrapper to load MEDLINE into a local MySQL database", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Since the MEDLINE database was released, the number of documents indexed by\nthis entity has risen every year. Several tools have been developed by the\nNational Institutes of Health (NIH) to query this corpus of scientific\npublications. However, in terms of advances in big data, text-mining and data\nscience, an option to build a local relational database containing all metadata\navailable on MEDLINE would be truly useful to optimally exploit these\nresources. MEDOC (MEdline DOwnloading Contrivance) is a Python program designed\nto download data on an FTP and to load all extracted information into a local\nMySQL database. It took MEDOC 4 days and 17 hours to load the 26 million\ndocuments available on this server onto a standard computer. This indexed\nrelational database allows the user to build complex and rapid queries. All\nfields can thus be searched for desired information, a task that is difficult\nto accomplish through the PubMed graphical interface. MEDOC is free and\npublicly available at https://github.com/MrMimic/MEDOC.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 06:14:53 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Dynomant", "Emeric", ""], ["Gorieu", "Mathilde", ""], ["Perrin", "Helene", ""], ["Denorme", "Marion", ""], ["Pichon", "Fabien", ""], ["Desfeux", "Arnaud", ""]]}, {"id": "1710.07114", "submitter": "Jedrzej Potoniec", "authors": "Jedrzej Potoniec and Piotr Jakubowski and Agnieszka {\\L}awrynowicz", "title": "Swift Linked Data Miner: Mining OWL 2 EL class expressions directly from\n  online RDF datasets", "comments": null, "journal-ref": null, "doi": "10.1016/j.websem.2017.08.001", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present Swift Linked Data Miner, an interruptible algorithm\nthat can directly mine an online Linked Data source (e.g., a SPARQL endpoint)\nfor OWL 2 EL class expressions to extend an ontology with new SubClassOf:\naxioms. The algorithm works by downloading only a small part of the Linked Data\nsource at a time, building a smart index in the memory and swiftly iterating\nover the index to mine axioms. We propose a transformation function from mined\naxioms to RDF Data Shapes. We show, by means of a crowdsourcing experiment,\nthat most of the axioms mined by Swift Linked Data Miner are correct and can be\nadded to an ontology. We provide a ready to use Prot\\'eg\\'e plugin implementing\nthe algorithm, to support ontology engineers in their daily modeling work.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 12:25:06 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Potoniec", "Jedrzej", ""], ["Jakubowski", "Piotr", ""], ["\u0141awrynowicz", "Agnieszka", ""]]}, {"id": "1710.07411", "submitter": "Jyoti Leeka", "authors": "Jyoti Leeka, Srikanta Bedathur, Debajyoti Bera, Sriram\n  Lakshminarasimhan", "title": "STREAK: An Efficient Engine for Processing Top-k SPARQL Queries with\n  Spatial Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of geo-spatial data in critical applications such as emergency\nresponse, transportation, agriculture etc., has prompted the adoption of recent\nGeoSPARQL standard in many RDF processing engines. In addition to large\nrepositories of geo-spatial data -- e.g., LinkedGeoData, OpenStreetMap, etc. --\nspatial data is also routinely found in automatically constructed\nknowledgebases such as Yago and WikiData. While there have been research\nefforts for efficient processing of spatial data in RDF/SPARQL, very little\neffort has gone into building end-to-end systems that can holistically handle\ncomplex SPARQL queries along with spatial filters.\n  In this paper, we present Streak, a RDF data management system that is\ndesigned to support a wide-range of queries with spatial filters including\ncomplex joins, top-k, higher-order relationships over spatially enriched\ndatabases. Streak introduces various novel features such as a careful\nidentifier encoding strategy for spatial and non-spatial entities, the use of a\nsemantics-aware Quad-tree index that allows for early-termination and a clever\nuse of adaptive query processing with zero plan-switch cost. We show that\nStreak can scale to some of the largest publicly available semantic data\nresources such as Yago3 and LinkedGeoData which contain spatial entities and\nquantifiable predicates useful for result ranking. For experimental\nevaluations, we focus on top-k distance join queries and demonstrate that\nStreak outperforms popular spatial join algorithms as well as state of the art\nend-to-end systems like Virtuoso and PostgreSQL.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 04:12:39 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Leeka", "Jyoti", ""], ["Bedathur", "Srikanta", ""], ["Bera", "Debajyoti", ""], ["Lakshminarasimhan", "Sriram", ""]]}, {"id": "1710.07660", "submitter": "Yuepeng Wang", "authors": "Yuepeng Wang, Isil Dillig, Shuvendu K. Lahiri, William R. Cook", "title": "Verifying Equivalence of Database-Driven Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of verifying equivalence between a pair of\nprograms that operate over databases with different schemas. This problem is\nparticularly important in the context of web applications, which typically\nundergo database refactoring either for performance or maintainability reasons.\nWhile web applications should have the same externally observable behavior\nbefore and after schema migration, there are no existing tools for proving\nequivalence of such programs. This paper takes a first step towards solving\nthis problem by formalizing the equivalence and refinement checking problems\nfor database-driven applications. We also propose a proof methodology based on\nthe notion of bisimulation invariants over relational algebra with updates and\ndescribe a technique for synthesizing such bisimulation invariants. We have\nimplemented the proposed technique in a tool called Mediator for verifying\nequivalence between database-driven applications written in our intermediate\nlanguage and evaluate our tool on 21 benchmarks extracted from textbooks and\nreal-world web applications. Our results show that the proposed methodology can\nsuccessfully verify 20 of these benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 18:38:40 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Wang", "Yuepeng", ""], ["Dillig", "Isil", ""], ["Lahiri", "Shuvendu K.", ""], ["Cook", "William R.", ""]]}, {"id": "1710.07736", "submitter": "Sang-Woo Jun", "authors": "Sang-Woo Jun, Andy Wright, Sizhuo Zhang, Shuotao Xu and Arvind", "title": "BigSparse: High-performance external graph analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BigSparse, a fully external graph analytics system that picks up\nwhere semi-external systems like FlashGraph and X-Stream, which only store\nvertex data in memory, left off. BigSparse stores both edge and vertex data in\nan array of SSDs and avoids random updates to the vertex data, by first logging\nthe vertex updates and then sorting the log to sequentialize accesses to the\nSSDs. This newly introduced sorting overhead is reduced significantly by\ninterleaving sorting with vertex reduction operations. In our experiments on a\nserver with 32GB to 64GB of DRAM, BigSparse outperforms other in-memory and\nsemi-external graph analytics systems for algorithms such as PageRank,\nBreadthFirst Search, and Betweenness-Centrality for terabyte-size graphs with\nbillions of vertices. BigSparse is capable of highspeed analytics of much\nlarger graphs, on the same machine configuration.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 01:09:53 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Jun", "Sang-Woo", ""], ["Wright", "Andy", ""], ["Zhang", "Sizhuo", ""], ["Xu", "Shuotao", ""], ["Arvind", "", ""]]}, {"id": "1710.07891", "submitter": "Xin Hu", "authors": "Xin Hu, Yingting Yao, Luting Ye, Depeng Dang", "title": "Natural Language Aggregate Query over RDF Data", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2018.04.042", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language question-answering over RDF data has received widespread\nattention. Although there have been several studies that have dealt with a\nsmall number of aggregate queries, they have many restrictions (i.e.,\ninteractive information, controlled question or query template). Thus far,\nthere has been no natural language querying mechanism that can process general\naggregate queries over RDF data. Therefore, we propose a framework called NLAQ\n(Natural Language Aggregate Query). First, we propose a novel algorithm to\nautomatically understand a users query intention, which mainly contains\nsemantic relations and aggregations. Second, to build a better bridge between\nthe query intention and RDF data, we propose an extended paraphrase dictionary\nED to obtain more candidate mappings for semantic relations, and we introduce a\npredicate-type adjacent set PT to filter out inappropriate candidate mapping\ncombinations in semantic relations and basic graph patterns. Third, we design a\nsuitable translation plan for each aggregate category and effectively\ndistinguish whether an aggregate item is numeric or not, which will greatly\naffect the aggregate result. Finally, we conduct extensive experiments over\nreal datasets (QALD benchmark and DBpedia), and the experimental results\ndemonstrate that our solution is effective.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 05:35:37 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Hu", "Xin", ""], ["Yao", "Yingting", ""], ["Ye", "Luting", ""], ["Dang", "Depeng", ""]]}, {"id": "1710.08023", "submitter": "Andrew Figueroa", "authors": "Andrew Figueroa, Steven Rollo, Sean Murthy", "title": "A Brief Comparison of Two Enterprise-Class RDBMSs", "comments": "14 pages, 16 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper is an extended version of a report from a student-developed study\nto compare Microsoft SQL Server and PostgreSQL, two widely-used\nenterprise-class relational database management systems (RDBMSs). The study\nfollowed an introductory undergraduate course in relational systems and was\ndesigned to help gain practical understanding of specific DBMSs. During this\nstudy, we implemented three non-trivial schemas in each system, identified 26\ncommon database design, development, and administration activities while\nimplementing the schemas, and compared the support each system offers to carry\nout the identified activities. Where relevant, we also compared each system\nagainst the SQL standard.\n  In this report, we present a summary of the similarities and differences we\nfound between the two systems, and we provide a quantitative measure ranking\nboth systems' implementations of the 26 activities. We also briefly discuss the\n\"technical suitability\" of PostgreSQL to enterprise applications. Although this\nreport is not comprehensive and is too general to comment on the suitability of\neither system to a specific enterprise application, it can nevertheless provide\nan initial set of considerations and criteria to choose a system for most\nenterprise applications.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 21:59:47 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Figueroa", "Andrew", ""], ["Rollo", "Steven", ""], ["Murthy", "Sean", ""]]}, {"id": "1710.08436", "submitter": "Yun William Yu", "authors": "Yun William Yu and Griffin M. Weber", "title": "HyperMinHash: MinHash in LogLog space", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this extended abstract, we describe and analyze a lossy compression of\nMinHash from buckets of size $O(\\log n)$ to buckets of size $O(\\log\\log n)$ by\nencoding using floating-point notation. This new compressed sketch, which we\ncall HyperMinHash, as we build off a HyperLogLog scaffold, can be used as a\ndrop-in replacement of MinHash. Unlike comparable Jaccard index fingerprinting\nalgorithms in sub-logarithmic space (such as b-bit MinHash), HyperMinHash\nretains MinHash's features of streaming updates, unions, and cardinality\nestimation. For a multiplicative approximation error $1+ \\epsilon$ on a Jaccard\nindex $ t $, given a random oracle, HyperMinHash needs $O\\left(\\epsilon^{-2}\n\\left( \\log\\log n + \\log \\frac{1}{ t \\epsilon} \\right)\\right)$ space.\nHyperMinHash allows estimating Jaccard indices of 0.01 for set cardinalities on\nthe order of $10^{19}$ with relative error of around 10\\% using 64KiB of\nmemory; MinHash can only estimate Jaccard indices for cardinalities of\n$10^{10}$ with the same memory consumption.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 18:02:16 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 18:28:24 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 02:23:43 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2018 20:36:33 GMT"}, {"version": "v5", "created": "Sat, 13 Jul 2019 15:29:47 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Yu", "Yun William", ""], ["Weber", "Griffin M.", ""]]}, {"id": "1710.08748", "submitter": "Thorsten Wissmann", "authors": "Diego Figueira, Luc Segoufin", "title": "Bottom-up automata on data trees and vertical XPath", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 4 (November\n  6, 2017) lmcs:4047", "doi": "10.23638/LMCS-13(4:5)2017", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data tree is a finite tree whose every node carries a label from a finite\nalphabet and a datum from some infinite domain. We introduce a new model of\nautomata over unranked data trees with a decidable emptiness problem. It is\nessentially a bottom-up alternating automaton with one register that can store\none data value and can be used to perform equality tests with the data values\noccurring within the subtree of the current node. We show that it captures the\nexpressive power of the vertical fragment of XPath - containing the child,\ndescendant, parent and ancestor axes - obtaining thus a decision procedure for\nits satisfiability problem.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 13:04:12 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 11:41:58 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Figueira", "Diego", ""], ["Segoufin", "Luc", ""]]}, {"id": "1710.09420", "submitter": "Eirini Molla Mrs", "authors": "Eirini Molla, Theodoros Tzouramanis, Stefanos Gritzalis", "title": "SOPE: A Spatial Order Preserving Encryption Model for Multi-dimensional\n  Data", "comments": "24 pages, 37 figures, 2 tables, 60 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing demand for cloud services and the threat of privacy\ninvasion, the user is suggested to encrypt the data before it is outsourced to\nthe remote server. The safe storage and efficient retrieval of d-dimensional\ndata on an untrusted server has therefore crucial importance. The paper\nproposes a new encryption model which offers spatial order-preservation for\nd-dimensional data (SOPE model). The paper studies the operations for the\nconstruction of the encrypted database and suggests algorithms that exploit\nunique properties that this new model offers for the efficient execution of a\nwhole range of well-known queries over the encrypted d-dimensional data. The\nnew model utilizes well-known database indices, such as the B+-tree and the\nR-tree, as backbone structures in their traditional form, as it suggests no\nmodifications to them for loading the data and for the efficient execution of\nthe supporting query algorithms. An extensive experimental study that is also\npresented in the paper indicates the effectiveness and practicability of the\nproposed encryption model for real-life d-dimensional data applications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 20:55:50 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Molla", "Eirini", ""], ["Tzouramanis", "Theodoros", ""], ["Gritzalis", "Stefanos", ""]]}, {"id": "1710.10088", "submitter": "Rong Kang", "authors": "Rong Kang, Chen Wang, Peng Wang, Yuting Ding, Jianmin Wang", "title": "Fine-grained Pattern Matching Over Streaming Time Series", "comments": "14 pages, 14 figures, 29 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching of streaming time series with lower latency under limited\ncomputing resource comes to a critical problem, especially as the growth of\nIndustry 4.0 and Industry Internet of Things. However, against traditional\nsingle pattern matching problem, a pattern may contain multiple segments\nrepresenting different statistical properties or physical meanings for more\nprecise and expressive matching in real world. Hence, we formulate a new\nproblem, called \"fine-grained pattern matching\", which allows users to specify\nvaried granularities of matching deviation to different segments of a given\npattern, and fuzzy regions for adaptive breakpoints determination between\nconsecutive segments. In this paper, we propose a novel two-phase approach. In\nthe pruning phase, we introduce Equal-Length Block (ELB) representation\ntogether with Block-Skipping Pruning (BSP) policy, which guarantees low cost\nfeature calculation, effective pruning and no false dismissals. In the\npost-processing phase, a delta-function is proposed to enable us to conduct\nexact matching in linear complexity. Extensive experiments are conducted to\nevaluate on synthetic and real-world datasets, which illustrates that our\nalgorithm outperforms the brute-force method and MSM, a multi-step filter\nmechanism over the multi-scaled representation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 11:45:14 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 02:51:43 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 23:45:48 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Kang", "Rong", ""], ["Wang", "Chen", ""], ["Wang", "Peng", ""], ["Ding", "Yuting", ""], ["Wang", "Jianmin", ""]]}, {"id": "1710.10555", "submitter": "Wenying Ji", "authors": "Wenying Ji, Simaan M. AbouRizk, Osmar R. Zaiane, Yitong Li", "title": "Complexity Analysis Approach for Prefabricated Construction Products\n  Using Uncertain Data Clustering", "comments": null, "journal-ref": null, "doi": "10.1061/(ASCE)CO.1943-7862.0001520", "report-no": null, "categories": "cs.DB cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an uncertain data clustering approach to quantitatively\nanalyze the complexity of prefabricated construction components through the\nintegration of quality performance-based measures with associated engineering\ndesign information. The proposed model is constructed in three steps, which (1)\nmeasure prefabricated construction product complexity (hereafter referred to as\nproduct complexity) by introducing a Bayesian-based nonconforming quality\nperformance indicator; (2) score each type of product complexity by developing\na Hellinger distance-based distribution similarity measurement; and (3) cluster\nproducts into homogeneous complexity groups by using the agglomerative\nhierarchical clustering technique. An illustrative example is provided to\ndemonstrate the proposed approach, and a case study of an industrial company in\nEdmonton, Canada, is conducted to validate the feasibility and applicability of\nthe proposed model. This research inventively defines and investigates product\ncomplexity from the perspective of product quality performance with design\ninformation associated. The research outcomes provide simplified,\ninterpretable, and informative insights for practitioners to better analyze and\nmanage product complexity. In addition to this practical contribution, a novel\nhierarchical clustering technique is devised. This technique is capable of\nclustering uncertain data (i.e., beta distributions) with lower computational\ncomplexity and has the potential to be generalized to cluster all types of\nuncertain data.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 03:30:36 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 17:45:03 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ji", "Wenying", ""], ["AbouRizk", "Simaan M.", ""], ["Zaiane", "Osmar R.", ""], ["Li", "Yitong", ""]]}, {"id": "1710.11528", "submitter": "Andrew Ilyas", "authors": "Andrew Ilyas, Joana M. F. da Trindade, Raul Castro Fernandez, Samuel\n  Madden", "title": "Extracting Syntactic Patterns from Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many database columns contain string or numerical data that conforms to a\npattern, such as phone numbers, dates, addresses, product identifiers, and\nemployee ids. These patterns are useful in a number of data processing\napplications, including understanding what a specific field represents when\nfield names are ambiguous, identifying outlier values, and finding similar\nfields across data sets. One way to express such patterns would be to learn\nregular expressions for each field in the database. Unfortunately, exist- ing\ntechniques on regular expression learning are slow, taking hundreds of seconds\nfor columns of just a few thousand values. In contrast, we develop XSystem, an\nefficient method to learn patterns over database columns in significantly less\ntime. We show that these patterns can not only be built quickly, but are\nexpressive enough to capture a number of key applications, including detecting\noutliers, measuring column similarity, and assigning semantic labels to columns\n(based on a library of regular expressions). We evaluate these applications\nwith datasets that range from chemical databases (based on a collaboration with\na pharmaceutical company), our university data warehouse, and open data from\nMassData.gov.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 15:21:32 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 16:44:39 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Ilyas", "Andrew", ""], ["da Trindade", "Joana M. F.", ""], ["Fernandez", "Raul Castro", ""], ["Madden", "Samuel", ""]]}, {"id": "1710.11531", "submitter": "Varish Mulwad", "authors": "Paul Cuddihy, Justin McHugh, Jenny Weisenberg Williams, Varish Mulwad,\n  Kareem S. Aggour", "title": "SemTK: An Ontology-first, Open Source Semantic Toolkit for Managing and\n  Querying Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relatively recent adoption of Knowledge Graphs as an enabling technology\nin multiple high-profile artificial intelligence and cognitive applications has\nled to growing interest in the Semantic Web technology stack. Many\nsemantics-related tools, however, are focused on serving experts with a deep\nunderstanding of semantic technologies. For example, triplification of\nrelational data is available but there is no open source tool that allows a\nuser unfamiliar with OWL/RDF to import data into a semantic triple store in an\nintuitive manner. Further, many tools require users to have a working\nunderstanding of SPARQL to query data. Casual users interested in benefiting\nfrom the power of Knowledge Graphs have few tools available for exploring,\nquerying, and managing semantic data. We present SemTK, the Semantics Toolkit,\na user-friendly suite of tools that allow both expert and non-expert semantics\nusers convenient ingestion of relational data, simplified query generation, and\nmore. The exploration of ontologies and instance data is performed through\nSPARQLgraph, an intuitive web-based user interface in SemTK understandable and\nnavigable by a lay user. The open source version of SemTK is available at\nhttp://semtk.research.ge.com\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 15:29:35 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 01:52:39 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Cuddihy", "Paul", ""], ["McHugh", "Justin", ""], ["Williams", "Jenny Weisenberg", ""], ["Mulwad", "Varish", ""], ["Aggour", "Kareem S.", ""]]}]