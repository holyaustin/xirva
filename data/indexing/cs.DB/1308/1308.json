[{"id": "1308.0158", "submitter": "Torsten Grust", "authors": "Torsten Grust, Alexander Ulrich", "title": "First-Class Functions for First-Order Database Engines", "comments": "Proceedings of the 14th International Symposium on Database\n  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Query Defunctionalization which enables off-the-shelf first-order\ndatabase engines to process queries over first-class functions. Support for\nfirst-class functions is characterized by the ability to treat functions like\nregular data items that can be constructed at query runtime, passed to or\nreturned from other (higher-order) functions, assigned to variables, and stored\nin persistent data structures. Query defunctionalization is a non-invasive\napproach that transforms such function-centric queries into the data-centric\noperations implemented by common query processors. Experiments with XQuery and\nPL/SQL database systems demonstrate that first-order database engines can\nfaithfully and efficiently support the expressive \"functions as data\" paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 11:39:18 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Grust", "Torsten", ""], ["Ulrich", "Alexander", ""]]}, {"id": "1308.0389", "submitter": "EPTCS", "authors": "Gabriel Ciobanu (Romanian Academy, Iasi), Ross Horne (Romanian\n  Academy, Iasi), Vladimiro Sassone (University of Southampton)", "title": "Local Type Checking for Linked Data Consumers", "comments": "In Proceedings WWV 2013, arXiv:1308.0268", "journal-ref": "EPTCS 123, 2013, pp. 19-33", "doi": "10.4204/EPTCS.123.4", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web of Linked Data is the cumulation of over a decade of work by the Web\nstandards community in their effort to make data more Web-like. We provide an\nintroduction to the Web of Linked Data from the perspective of a Web developer\nthat would like to build an application using Linked Data. We identify a\nweakness in the development stack as being a lack of domain specific scripting\nlanguages for designing background processes that consume Linked Data. To\naddress this weakness, we design a scripting language with a simple but\nappropriate type system. In our proposed architecture some data is consumed\nfrom sources outside of the control of the system and some data is held\nlocally. Stronger type assumptions can be made about the local data than\nexternal data, hence our type system mixes static and dynamic typing.\nThroughout, we relate our work to the W3C recommendations that drive Linked\nData, so our syntax is accessible to Web developers.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 01:55:30 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Ciobanu", "Gabriel", "", "Romanian Academy, Iasi"], ["Horne", "Ross", "", "Romanian\n  Academy, Iasi"], ["Sassone", "Vladimiro", "", "University of Southampton"]]}, {"id": "1308.0484", "submitter": "Bin Yang", "authors": "Bin Yang, Manohar Kaul, Christian S. Jensen", "title": "Using Incomplete Information for Complete Weight Annotation of Road\n  Networks -- Extended Version", "comments": "This is an extended version of \"Using Incomplete Information for\n  Complete Weight Annotation of Road Networks,\" which is accepted for\n  publication in IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are witnessing increasing interests in the effective use of road networks.\nFor example, to enable effective vehicle routing, weighted-graph models of\ntransportation networks are used, where the weight of an edge captures some\ncost associated with traversing the edge, e.g., greenhouse gas (GHG) emissions\nor travel time. It is a precondition to using a graph model for routing that\nall edges have weights. Weights that capture travel times and GHG emissions can\nbe extracted from GPS trajectory data collected from the network. However, GPS\ntrajectory data typically lack the coverage needed to assign weights to all\nedges. This paper formulates and addresses the problem of annotating all edges\nin a road network with travel cost based weights from a set of trips in the\nnetwork that cover only a small fraction of the edges, each with an associated\nground-truth travel cost. A general framework is proposed to solve the problem.\nSpecifically, the problem is modeled as a regression problem and solved by\nminimizing a judiciously designed objective function that takes into account\nthe topology of the road network. In particular, the use of weighted PageRank\nvalues of edges is explored for assigning appropriate weights to all edges, and\nthe property of directional adjacency of edges is also taken into account to\nassign weights. Empirical studies with weights capturing travel time and GHG\nemissions on two road networks (Skagen, Denmark, and North Jutland, Denmark)\noffer insight into the design properties of the proposed techniques and offer\nevidence that the techniques are effective.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 12:56:19 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2013 20:00:22 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Yang", "Bin", ""], ["Kaul", "Manohar", ""], ["Jensen", "Christian S.", ""]]}, {"id": "1308.0502", "submitter": "James Cheney", "authors": "James Cheney", "title": "Static Enforceability of XPath-Based Access Control Policies", "comments": "Proceedings of the 14th International Symposium on Database\n  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of extending XML databases with fine-grained,\nhigh-level access control policies specified using XPath expressions. Most\nprior work checks individual updates dynamically, which is expensive (requiring\nworst-case execution time proportional to the size of the database). On the\nother hand, static enforcement can be performed without accessing the database\nbut may be incomplete, in the sense that it may forbid accesses that dynamic\nenforcement would allow. We introduce topological characterizations of XPath\nfragments in order to study the problem of determining when an access control\npolicy can be enforced statically without loss of precision. We introduce the\nnotion of fair policies that are statically enforceable, and study the\ncomplexity of determining fairness and of static enforcement itself.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 13:52:36 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Cheney", "James", ""]]}, {"id": "1308.0514", "submitter": "Stefanie Scherzinger", "authors": "Stefanie Scherzinger and Meike Klettke and Uta St\\\"orl", "title": "Managing Schema Evolution in NoSQL Data Stores", "comments": "Proceedings of the 14th International Symposium on Database\n  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NoSQL data stores are commonly schema-less, providing no means for globally\ndefining or managing the schema. While this offers great flexibility in early\nstages of application development, developers soon can experience the heavy\nburden of dealing with increasingly heterogeneous data. This paper targets\nschema evolution for NoSQL data stores, the complex task of adapting and\nchanging the implicit structure of the data stored. We discuss the\nrecommendations of the developer community on handling schema changes, and\nintroduce a simple, declarative schema evolution language. With our language,\nsoftware developers and architects can systematically manage the evolution of\ntheir production data and perform typical schema maintenance tasks. We further\nprovide a holistic NoSQL database programming language to define the semantics\nof our schema evolution language. Our solution does not require any\nmodifications to the NoSQL data store, treating the data store as a black box.\nThus, we want to address application developers that use NoSQL systems\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 14:34:14 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Scherzinger", "Stefanie", ""], ["Klettke", "Meike", ""], ["St\u00f6rl", "Uta", ""]]}, {"id": "1308.0656", "submitter": "Kian Win Ong", "authors": "Yupeng Fu, Kian Win Ong, Yannis Papakonstantinou", "title": "Declarative Ajax Web Applications through SQL++ on a Unified Application\n  State", "comments": "Proceedings of the 14th International Symposium on Database\n  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementing even a conceptually simple web application requires an\ninordinate amount of time. FORWARD addresses three problems that reduce\ndeveloper productivity: (a) Impedance mismatch across the multiple languages\nused at different tiers of the application architecture. (b) Distributed data\naccess across the multiple data sources of the application (SQL database, user\ninput of the browser page, session data in the application server, etc). (c)\nAsynchronous, incremental modification of the pages, as performed by Ajax\nactions.\n  FORWARD belongs to a novel family of web application frameworks that attack\nimpedance mismatch by offering a single unifying language. FORWARD's language\nis SQL++, a minimally extended SQL. FORWARD's architecture is based on two\nnovel cornerstones: (a) A Unified Application State (UAS), which is a virtual\ndatabase over the multiple data sources. The UAS is accessed via distributed\nSQL++ queries, therefore resolving the distributed data access problem. (b)\nDeclarative page specifications, which treat the data displayed by pages as\nrendered SQL++ page queries. The resulting pages are automatically\nincrementally modified by FORWARD. User input on the page becomes part of the\nUAS.\n  We show that SQL++ captures the semi-structured nature of web pages and\nsubsumes the data models of two important data sources of the UAS: SQL\ndatabases and JavaScript components. We show that simple markup is sufficient\nfor creating Ajax displays and for modeling user input on the page as UAS data\nsources. Finally, we discuss the page specification syntax and semantics that\nare needed in order to avoid race conditions and conflicts between the user\ninput and the automated Ajax page modifications.\n  FORWARD has been used in the development of eight commercial and academic\napplications. An alpha-release web-based IDE (itself built in FORWARD) enables\ndevelopment in the cloud.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 03:23:27 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 08:37:09 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Fu", "Yupeng", ""], ["Ong", "Kian Win", ""], ["Papakonstantinou", "Yannis", ""]]}, {"id": "1308.0769", "submitter": "Yasunori Ishihara", "authors": "Yasunori Ishihara, Nobutaka Suzuki, Kenji Hashimoto, Shogo Shimizu,\n  Toru Fujiwara", "title": "XPath Satisfiability with Parent Axes or Qualifiers Is Tractable under\n  Many of Real-World DTDs", "comments": "Proceedings of the 14th International Symposium on Database\n  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at finding a subclass of DTDs that covers many of the\nreal-world DTDs while offering a polynomial-time complexity for deciding the\nXPath satisfiability problem. In our previous work, we proposed RW-DTDs, which\ncover most of the real-world DTDs (26 out of 27 real-world DTDs and 1406 out of\n1407 DTD rules). However, under RW-DTDs, XPath satisfiability with only child,\ndescendant-or-self, and sibling axes is tractable. In this paper, we propose\nMRW-DTDs, which are slightly smaller than RW-DTDs but have tractability on\nXPath satisfiability with parent axes or qualifiers. MRW-DTDs are a proper\nsuperclass of duplicate-free DTDs proposed by Montazerian et al., and cover 24\nout of the 27 real-world DTDs and 1403 out of the 1407 DTD rules. Under\nMRW-DTDs, we show that XPath satisfiability problems with (1) child, parent,\nand sibling axes, and (2) child and sibling axes and qualifiers are both\ntractable, which are known to be intractable under RW-DTDs.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 02:20:05 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Ishihara", "Yasunori", ""], ["Suzuki", "Nobutaka", ""], ["Hashimoto", "Kenji", ""], ["Shimizu", "Shogo", ""], ["Fujiwara", "Toru", ""]]}, {"id": "1308.1440", "submitter": "L\\'aszl\\'o Dobos", "authors": "L\\'aszl\\'o Dobos and Alexander S. Szalay and Tam\\'as Budav\\'ari and\n  Istv\\'an Csabai and Nolan Li", "title": "Graywulf: A platform for federated scientific databases and services", "comments": "SSDBM 2013 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many fields of science rely on relational database management systems to\nanalyze, publish and share data. Since RDBMS are originally designed for, and\ntheir development directions are primarily driven by, business use cases they\noften lack features very important for scientific applications. Horizontal\nscalability is probably the most important missing feature which makes it\nchallenging to adapt traditional relational database systems to the ever\ngrowing data sizes. Due to the limited support of array data types and metadata\nmanagement, successful application of RDBMS in science usually requires the\ndevelopment of custom extensions. While some of these extensions are specific\nto the field of science, the majority of them could easily be generalized and\nreused in other disciplines. With the Graywulf project we intend to target\nseveral goals. We are building a generic platform that offers reusable\ncomponents for efficient storage, transformation, statistical analysis and\npresentation of scientific data stored in Microsoft SQL Server. Graywulf also\naddresses the distributed computational issues arising from current RDBMS\ntechnologies. The current version supports load balancing of simple queries and\nparallel execution of partitioned queries over a set of mirrored databases.\nUniform user access to the data is provided through a web based query interface\nand a data surface for software clients. Queries are formulated in a slightly\nmodified syntax of SQL that offers a transparent view of the distributed data.\nThe software library consists of several components that can be reused to\ndevelop complex scientific data warehouses: a system registry, administration\ntools to manage entire database server clusters, a sophisticated workflow\nexecution framework, and a SQL parser library.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 23:02:12 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Dobos", "L\u00e1szl\u00f3", ""], ["Szalay", "Alexander S.", ""], ["Budav\u00e1ri", "Tam\u00e1s", ""], ["Csabai", "Istv\u00e1n", ""], ["Li", "Nolan", ""]]}, {"id": "1308.1471", "submitter": "Arokia Paul  Rajan", "authors": "R. Arokia Paul Rajan and F. Sagayaraj Francis", "title": "Application of Inventory Management Principles for Efficient Data\n  Placement in Storage Networks", "comments": "IJCSI International Journal of Computer Science Issues, Vol. 9, Issue\n  6, No 2, November 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principles and strategies found in material management are comparable and\nanalogue with the data management. This paper concentrates on the conversion of\nproduct inventory management principles into data inventory management\nprinciples. Efforts were made to enumerate various impacting parameters that\nwould be appropriate to consider if any data inventory model could be plotted.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2013 04:12:50 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Rajan", "R. Arokia Paul", ""], ["Francis", "F. Sagayaraj", ""]]}, {"id": "1308.2147", "submitter": "Paolo Romano", "authors": "Danny Hendler, Alex Naiman, Sebastiano Peluso, Francesco Quaglia,\n  Paolo Romano, Adi Suissa", "title": "Exploiting Locality in Lease-Based Replicated Transactional Memory via\n  Task Migration", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Lilac-TM, the first locality-aware Distributed Software\nTransactional Memory (DSTM) implementation. Lilac-TM is a fully decentralized\nlease-based replicated DSTM. It employs a novel self- optimizing lease\ncirculation scheme based on the idea of dynamically determining whether to\nmigrate transactions to the nodes that own the leases required for their\nvalidation, or to demand the acquisition of these leases by the node that\noriginated the transaction. Our experimental evaluation establishes that\nLilac-TM provides significant performance gains for distributed workloads\nexhibiting data locality, while typically incurring no overhead for non-data\nlocal workloads.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 15:04:09 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Hendler", "Danny", ""], ["Naiman", "Alex", ""], ["Peluso", "Sebastiano", ""], ["Quaglia", "Francesco", ""], ["Romano", "Paolo", ""], ["Suissa", "Adi", ""]]}, {"id": "1308.2166", "submitter": "Kanat Tangwongsan", "authors": "Kanat Tangwongsan, A. Pavan, and Srikanta Tirthapura", "title": "Parallel Triangle Counting in Massive Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of triangles in a graph is a fundamental metric, used in social\nnetwork analysis, link classification and recommendation, and more. Driven by\nthese applications and the trend that modern graph datasets are both large and\ndynamic, we present the design and implementation of a fast and cache-efficient\nparallel algorithm for estimating the number of triangles in a massive\nundirected graph whose edges arrive as a stream. It brings together the\nbenefits of streaming algorithms and parallel algorithms. By building on the\nstreaming algorithms framework, the algorithm has a small memory footprint. By\nleveraging the paralell cache-oblivious framework, it makes efficient use of\nthe memory hierarchy of modern multicore machines without needing to know its\nspecific parameters. We prove theoretical bounds on accuracy, memory access\ncost, and parallel runtime complexity, as well as showing empirically that the\nalgorithm yields accurate results and substantial speedups compared to an\noptimized sequential implementation.\n  (This is an expanded version of a CIKM'13 paper of the same title.)\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 15:54:22 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Tangwongsan", "Kanat", ""], ["Pavan", "A.", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1308.2310", "submitter": "Surendiran  Rajendiran", "authors": "Rakesh Duggirala, P. Narayana", "title": "Mining Positive and Negative Association Rules Using CoherentApproach", "comments": "IJCTT-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In the data mining field, association rules are discovered having domain\nknowledge specified as a minimum support threshold. The accuracy in setting up\nthis threshold directly influences the number and the quality of association\nrules discovered. Typically, before association rules are mined, a user needs\nto determine a support threshold in order to obtain only the frequent item\nsets. Having users to determine a support threshold attracts a number of\nissues. We propose an association rule mining framework that does not require a\nper-set support threshold. Often, the number of association rules, even though\nlarge in number, misses some interesting rules and the rules quality\nnecessitates further analysis. As a result, decision making using these rules\ncould lead to risky actions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2013 13:21:20 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Duggirala", "Rakesh", ""], ["Narayana", "P.", ""]]}, {"id": "1308.2433", "submitter": "Zhiwu Xie", "authors": "Zhiwu Xie, Herbert Van de Sompel, Jinyang Liu, Johann van Reenen,\n  Ramiro Jordan", "title": "Archiving the Relaxed Consistency Web", "comments": "10 pages, 6 figures, CIKM 2013", "journal-ref": null, "doi": "10.1145/2505515.2505551", "report-no": null, "categories": "cs.DL cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The historical, cultural, and intellectual importance of archiving the web\nhas been widely recognized. Today, all countries with high Internet penetration\nrate have established high-profile archiving initiatives to crawl and archive\nthe fast-disappearing web content for long-term use. As web technologies\nevolve, established web archiving techniques face challenges. This paper\nfocuses on the potential impact of the relaxed consistency web design on\ncrawler driven web archiving. Relaxed consistent websites may disseminate,\nalbeit ephemerally, inaccurate and even contradictory information. If captured\nand preserved in the web archives as historical records, such information will\ndegrade the overall archival quality. To assess the extent of such quality\ndegradation, we build a simplified feed-following application and simulate its\noperation with synthetic workloads. The results indicate that a non-trivial\nportion of a relaxed consistency web archive may contain observable\ninconsistency, and the inconsistency window may extend significantly longer\nthan that observed at the data store. We discuss the nature of such quality\ndegradation and propose a few possible remedies.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 22:31:37 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Xie", "Zhiwu", ""], ["Van de Sompel", "Herbert", ""], ["Liu", "Jinyang", ""], ["van Reenen", "Johann", ""], ["Jordan", "Ramiro", ""]]}, {"id": "1308.3106", "submitter": "Girish Sundaram", "authors": "Sachin Kumar, Ashish Kumar, Pinaki Mitra, Girish Sundaram", "title": "System and Methods for Converting Speech to SQL", "comments": "Appeared In proceedings of International Conference ERCICA 2013 pp:\n  291-298, Published by Elsevier Ltd, ISBN:9789351071020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns with the conversion of a Spoken English Language Query\ninto SQL for retrieving data from RDBMS. A User submits a query as speech\nsignal through the user interface and gets the result of the query in the text\nformat. We have developed the acoustic and language models using which a speech\nutterance can be converted into English text query and thus natural language\nprocessing techniques can be applied on this English text query to generate an\nequivalent SQL query. For conversion of speech into English text HTK and Julius\ntools have been used and for conversion of English text query into SQL query we\nhave implemented a System which uses rule based translation to translate\nEnglish Language Query into SQL Query. The translation uses lexical analyzer,\nparser and syntax directed translation techniques like in compilers. JFLex and\nBYACC tools have been used to build lexical analyzer and parser respectively.\nSystem is domain independent i.e. system can run on different database as it\ngenerates lex files from the underlying database.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 12:54:31 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Kumar", "Sachin", ""], ["Kumar", "Ashish", ""], ["Mitra", "Pinaki", ""], ["Sundaram", "Girish", ""]]}, {"id": "1308.3357", "submitter": "Christophe Gu\\'eret", "authors": "Marat Charlaganov, Philippe Cudr\\'e-Mauroux, Cristian Dinu, Christophe\n  Gu\\'eret, Martin Grund and Teodor Macicas", "title": "The Entity Registry System: Implementing 5-Star Linked Data Without the\n  Web", "comments": "16 pages, authors are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linked Data applications often assume that connectivity to data repositories\nand entity resolution services are always available. This may not be a valid\nassumption in many cases. Indeed, there are about 4.5 billion people in the\nworld who have no or limited Web access. Many data-driven applications may have\na critical impact on the life of those people, but are inaccessible to those\npopulations due to the architecture of today's data registries. In this paper,\nwe propose and evaluate a new open-source system that can be used as a\ngeneral-purpose entity registry suitable for deployment in poorly-connected or\nad-hoc environments.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 10:53:48 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Charlaganov", "Marat", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""], ["Dinu", "Cristian", ""], ["Gu\u00e9ret", "Christophe", ""], ["Grund", "Martin", ""], ["Macicas", "Teodor", ""]]}, {"id": "1308.3679", "submitter": "Girish Sundaram", "authors": "Pinaki Mitra, Girish Sundaram, Sreedish PS", "title": "Just In Time Indexing", "comments": "Selected for proceedings of NETs2012 International Conference on\n  Internet Studies held in Bangkok, Thailand from August 17-19, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges being faced by Database managers today is to\nmanage the performance of complex SQL queries which are dynamic in nature.\nSince it is not possible to tune each and every query because of its dynamic\nnature, there is a definite possibility that these queries may cause serious\ndatabase performance issues if left alone. Conventional indexes are useful only\nfor those queries which are frequently executed or those columns which are\nfrequently joined in SQL queries. This proposal is regarding a method, a query\noptimizer for optimizing database queries in a database management system. Just\nIn Time(JIT) indexes are On Demand, temporary indexes created on the fly based\non current needs so that they would be able to satisfy any kind of queries. JIT\nindexes are created only when the configured threshold values for resource\nconsumption are exceeded for a query. JIT indexes will be stored in a temporary\nbasis and will get replaced by new JIT indexes in course of time. The proposal\nis substantiated with the help of experimental programs and with various test\ncases. The idea of parallel programming is also brought into picture as it can\nbe effectively used in a multiprocessor system. Multiple threads are employed\nwhile one set of threads proceed in the conventional way and the other set of\nthreads proceed with the proposed way. A live switch over is made when a\nsuitable stage is reached and from then onwards the proposed method will only\ncome into picture.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 17:49:24 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Mitra", "Pinaki", ""], ["Sundaram", "Girish", ""], ["PS", "Sreedish", ""]]}, {"id": "1308.3830", "submitter": "Rukshan Alexander", "authors": "Rukshan Alexander, Prashanthi Rukshan, and Sinnathamby Mahesan", "title": "Natural Language Web Interface for Database (NLWIDB)", "comments": "8 pages,5 figures, 12 tables, Proceedings of the Third International\n  Symposium, SEUSL: 6-7 July 2013, Oluvil, Sri Lanka", "journal-ref": "Proceedings of the Third International Symposium, SEUSL: 6-7 July\n  2013, Oluvil, Sri Lanka", "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a long term desire of the computer users to minimize the communication\ngap between the computer and a human. On the other hand, almost all ICT\napplications store information in to databases and retrieve from them.\nRetrieving information from the database requires knowledge of technical\nlanguages such as Structured Query Language. However majority of the computer\nusers who interact with the databases do not have a technical background and\nare intimidated by the idea of using languages such as SQL. For above reasons,\na Natural Language Web Interface for Database (NLWIDB) has been developed. The\nNLWIDB allows the user to query the database in a language more like English,\nthrough a convenient interface over the Internet.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2013 04:22:40 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Alexander", "Rukshan", ""], ["Rukshan", "Prashanthi", ""], ["Mahesan", "Sinnathamby", ""]]}, {"id": "1308.4027", "submitter": "Rada Chirkova", "authors": "Rada Chirkova", "title": "Combined-Semantics Equivalence Is Decidable for a Practical Class of\n  Conjunctive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of determining whether two conjunctive\n(\"CQ\") queries posed on relational data are combined-semantics equivalent [9].\nWe continue the tradition of [2,5,9] of studying this problem using the tool of\ncontainment between queries. We introduce a syntactic necessary and sufficient\ncondition for equivalence of queries belonging to a large natural language of\n\"explicit-wave\" combined-semantics CQ queries; this language encompasses (but\nis not limited to) all set, bag, and bag-set queries, and appears to cover all\ncombined-semantics CQ queries that are expressible in SQL. Our result solves in\nthe positive the decidability problem of determining combined-semantics\nequivalence for pairs of explicit-wave CQ queries. That is, for an arbitrary\npair of combined-semantics CQ queries, it is decidable (i) to determine whether\neach of the queries is explicit wave, and (ii) to determine, in case both\nqueries are explicit wave, whether or not they are combined-semantics\nequivalent, by using our syntactic criterion. (The problem of determining\nequivalence for general combined-semantics CQ queries remains open. Even so,\nour syntactic sufficient containment condition could still be used to determine\nthat two general CQ queries are combined-semantics equivalent.) Our equivalence\ntest, as well as our general sufficient condition for containment of\ncombined-semantics CQ queries, reduce correctly to the special cases reported\nin [2,5] for set, bag, and bag-set semantics. Our containment and equivalence\nconditions also properly generalize the results of [9], provided that the\nlatter are restricted to the language of (combined-semantics) CQ queries.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 14:06:11 GMT"}, {"version": "v2", "created": "Wed, 13 Aug 2014 20:16:37 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Chirkova", "Rada", ""]]}, {"id": "1308.4048", "submitter": "Laxmaiah Mettu", "authors": "M.Laxmaiah (1) and A.Govardhan (2) ((1) Tirumala Engineering College,\n  Keesara (M), Hyderabad, AP, India (2) School of Information Technology,\n  Jawaharlal Nehru Technological University, Hyderabad, AP, India)", "title": "Gcube Indexing", "comments": "This paper is published in IJDKP", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process, Vol.3,Number 4,July 2013", "doi": "10.5121/ijdkp.2013.3406", "report-no": "ISSN 2230-9608", "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Spatial Online Analytical Processing System involves the non-categorical\nattribute information also whereas standard Online Analytical Processing System\ndeals with only categorical attributes. Providing spatial information to the\ndata warehouse (DW); two major challenges faced are;1.Defining and Aggregation\nof Spatial/Continues values and 2.Representation, indexing, updating and\nefficient query processing. In this paper, we present GCUBE(Geographical Cube)\nstorage and indexing procedure to aggregate the spatial information/Continuous\nvalues. We employed the proposed approach storing and indexing using synthetic\nand real data sets and evaluated its build, update and Query time. It is\nobserved that the proposed procedure offers significant performance advantage.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 15:12:18 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Laxmaiah", "M.", ""], ["Govardhan", "A.", ""]]}, {"id": "1308.4687", "submitter": "Manish Sharma Mr.", "authors": "Manish Sharma, Atul Chaudhary, Santosh Kumar", "title": "Query Processing Performance and Searching Over Encrypted Data By Using\n  An Efficient Algorithm", "comments": "4 Pages, 2 Figures, 2 Tables Published With \"International Journal of\n  Computer Applications (IJCA)\"", "journal-ref": "International Journal of Computer Applications 62(10):5-8, January\n  2013", "doi": "10.5120/10114-4781 10.5120/10114-4781 10.5120/10114-4781", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is the central asset of today's dynamically operating organization and\ntheir business. This data is usually stored in database. A major consideration\nis applied on the security of that data from the unauthorized access and\nintruders. Data encryption is a strong option for security of data in database\nand especially in those organizations where security risks are high. But there\nis a potential disadvantage of performance degradation. When we apply\nencryption on database then we should compromise between the security and\nefficient query processing. The work of this paper tries to fill this gap. It\nallows the users to query over the encrypted column directly without decrypting\nall the records. It's improves the performance of the system. The proposed\nalgorithm works well in the case of range and fuzzy match queries.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 20:00:49 GMT"}], "update_date": "2013-08-25", "authors_parsed": [["Sharma", "Manish", ""], ["Chaudhary", "Atul", ""], ["Kumar", "Santosh", ""]]}, {"id": "1308.5585", "submitter": "Ioana Ileana", "authors": "Bogdan Cautis, Alin Deutsch, Ioana Ileana, Nicola Onose", "title": "Rewriting XPath Queries using View Intersections: Tractability versus\n  Completeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach for optimization of XPath queries by rewriting using\nviews techniques consists in navigating inside a view's output, thus allowing\nthe usage of only one view in the rewritten query. Algorithms for richer\nclasses of XPath rewritings, using intersection or joins on node identifiers,\nhave been proposed, but they either lack completeness guarantees, or require\nadditional information about the data. We identify the tightest restrictions\nunder which an XPath can be rewritten in polynomial time using an intersection\nof views and propose an algorithm that works for any documents or type of\nidentifiers. As a side-effect, we analyze the complexity of the related problem\nof deciding if an XPath with intersection can be equivalently rewritten as one\nwithout intersection or union. We extend our formal study of the view-based\nrewriting problem for XPath by describing also (i) algorithms for more complex\nrewrite plans, with no limitations on the number of intersection and navigation\nsteps inside view outputs they employ, and (ii) adaptations of our techniques\nto deal with XML documents without persistent node Ids, in the presence of XML\nkeys. Complementing our computational complexity study, we describe a\nproof-of-concept implementation of our techniques and possible choices that may\nspeed up execution in practice, regarding how rewrite plans are built, tested\nand executed. We also give a thorough experimental evaluation of these\ntechniques, focusing on scalability and the running time improvements achieved\nby the execution of view-based plans.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 13:45:05 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Cautis", "Bogdan", ""], ["Deutsch", "Alin", ""], ["Ileana", "Ioana", ""], ["Onose", "Nicola", ""]]}, {"id": "1308.5703", "submitter": "Gonzalo Diaz", "authors": "Marcelo Arenas, Gonzalo I. Diaz, Achille Fokoue, Anastasios\n  Kementsietsidis, Kavitha Srinivas", "title": "A Principled Approach to Bridging the Gap between Graph Data and their\n  Schemas", "comments": "18 pages, 8 figures. To be published in PVLDB Vol. 8, No. 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although RDF graphs have schema information associated with them, in practice\nit is very common to find cases in which data do not fully conform to their\nschema. A prominent example of this is DBpedia, which is RDF data extracted\nfrom Wikipedia, a publicly editable source of information. In such situations,\nit becomes interesting to study the structural properties of the actual data,\nbecause the schema gives an incomplete description of the organization of a\ndataset. In this paper we have approached the study of the structuredness of an\nRDF graph in a principled way: we propose a framework for specifying\nstructuredness functions, which gauge the degree to which an RDF graph conforms\nto a schema. In particular, we first define a formal language for specifying\nstructuredness functions with expressions we call rules. This language allows a\nuser or a database administrator to state a rule to which an RDF graph may\nfully or partially conform. Then we consider the issue of discovering a\nrefinement of a sort (type) by partitioning the dataset into subsets whose\nstructuredness is over a specified threshold. In particular, we prove that the\nnatural decision problem associated to this refinement problem is NP-complete,\nand we provide a natural translation of this problem into Integer Linear\nProgramming (ILP). Finally, we test this ILP solution with two real world\ndatasets, DBpedia Persons and WordNet Nouns, and 4 different and intuitive\nrules, which gauge the structuredness in different ways. The rules give\nmeaningful refinements of the datasets, showing that our language can be a\npowerful tool for understanding the structure of RDF data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 21:26:00 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2014 14:01:46 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Arenas", "Marcelo", ""], ["Diaz", "Gonzalo I.", ""], ["Fokoue", "Achille", ""], ["Kementsietsidis", "Anastasios", ""], ["Srinivas", "Kavitha", ""]]}, {"id": "1308.5933", "submitter": "Ala'a Al-Mughrabi MR", "authors": "Ala'a Atallah Al-Mughrabi, Hussein Owaied", "title": "Framework Model for Database Replication within the Availability Zones", "comments": null, "journal-ref": "International Journal of Computer Science Issues,VOL 10,Issue 2,NO\n  1,March 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a proposed model for database replication model in\nprivate cloud availability regions, which is an enhancement of the SQL Server\nAlwaysOn Layers of Protection Model presents by Microsoft in 2012. The\nenhancement concentrates in the database replication for private cloud\navailability regions through the use of primary and secondary servers. The\nprocesses of proposed model during the client send Write/Read Request to the\nserver, in synchronous and semi synchronous replication level has been\ndescribed in details also the processes of proposed model when the client send\nWrite/Read Request to the Primary Server presented in details. All the types of\nautomatic failover situations are presented in this thesis. Using the proposed\nmodels will increase the performance because each one of the secondary servers\nwill open for Read / Write and allow the clients to connect to the nearby\nsecondary and less loading on each server. Keywords: Availability Regions,\nCloud Computing, Database Replication, SQL Server AlwaysOn, Synchronization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2013 17:49:50 GMT"}], "update_date": "2013-08-28", "authors_parsed": [["Al-Mughrabi", "Ala'a Atallah", ""], ["Owaied", "Hussein", ""]]}, {"id": "1308.6682", "submitter": "Jerome Darmont", "authors": "Marouane Hachicha (ERIC), Chantola Kit (ERIC), J\\'er\\^ome Darmont\n  (ERIC)", "title": "A Novel Query-Based Approach for Addressing Summarizability Issues in\n  XOLAP", "comments": "18th International Conference on Management of Data (COMAD 2012),\n  Pune : India (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The business intelligence and decision-support systems used in many\napplication domains casually rely on data warehouses, which are\ndecision-oriented data repositories modeled as multidimensional (MD)\nstructures. MD structures help navigate data through hierarchical levels of\ndetail. In many real-world situations, hierarchies in MD models are complex,\nwhich causes data aggregation issues, collectively known as the summarizability\nproblem. This problem leads to incorrect analyses and critically affects\ndecision making. To enforce summarizability, existing approaches alter either\nMD models or data, and must be applied a priori, on a case-by-case basis, by an\nexpert. To alter neither models nor data, a few query-time approaches have been\nproposed recently, but they only detect summarizability issues without solving\nthem. Thus, we propose in this paper a novel approach that automatically\ndetects and processes summarizability issues at query time, without requiring\nany particular expertise from the user. Moreover, while most existing\napproaches are based on the relational model, our approach focus on an XML MD\nmodel, since XML data is customarily used to represent business data and its\nformat better copes with complex hierarchies than the relational model.\nFinally, our experiments show that our method is likely to scale better than a\nreference approach for addressing the summarizability problem in the MD\ncontext.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 09:01:25 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Hachicha", "Marouane", "", "ERIC"], ["Kit", "Chantola", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1308.6683", "submitter": "Jerome Darmont", "authors": "Chantola Kit (ERIC), Marouane Hachicha (ERIC), J\\'er\\^ome Darmont\n  (ERIC)", "title": "Benchmarking Summarizability Processing in XML Warehouses with Complex\n  Hierarchies", "comments": "15th International Workshop on Data Warehousing and OLAP (DOLAP\n  2012), Maui : United States (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business Intelligence plays an important role in decision making. Based on\ndata warehouses and Online Analytical Processing, a business intelligence tool\ncan be used to analyze complex data. Still, summarizability issues in data\nwarehouses cause ineffective analyses that may become critical problems to\nbusinesses. To settle this issue, many researchers have studied and proposed\nvarious solutions, both in relational and XML data warehouses. However, they\nfind difficulty in evaluating the performance of their proposals since the\navailable benchmarks lack complex hierarchies. In order to contribute to\nsummarizability analysis, this paper proposes an extension to the XML warehouse\nbenchmark (XWeB) with complex hierarchies. The benchmark enables us to generate\nXML data warehouses with scalable complex hierarchies as well as\nsummarizability processing. We experimentally demonstrated that complex\nhierarchies can definitely be included into a benchmark dataset, and that our\nbenchmark is able to compare two alternative approaches dealing with\nsummarizability issues.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 09:02:02 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Kit", "Chantola", "", "ERIC"], ["Hachicha", "Marouane", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1308.6701", "submitter": "Grigore Stamatescu", "authors": "Adriana Olteanu, Grigore Stamatescu, Anca Daniela Ionita, Valentin\n  Sgarciu", "title": "Enhanced Data Integration for LabVIEW Laboratory Systems", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating data is a basic concern in many accredited laboratories that\nperform a large variety of measurements. However, the present working style in\nengineering faculties does not focus much on this aspect. To deal with this\nchallenge, we developed an educational platform that allows characterization of\nacquisition ensembles, generation of Web pages for lessons, as well as\ntransformation of measured data and storage in a common format. As generally we\nhad to develop individual parsers for each instrument, we also added the\npossibility to integrate the LabVIEW workbench, often used for rapid\ndevelopment of applications in electrical engineering and automatic control.\nThis paper describes how we configure the platform for specific equipment, i.e.\nhow we model it, how we create the learning material and how we integrate the\nresults in a central database. It also introduces a case study for collecting\ndata from a thermocouple-based acquisition system based on LabVIEW, used by\nstudents for a laboratory of measurement technologies and transducers.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 10:25:51 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Olteanu", "Adriana", ""], ["Stamatescu", "Grigore", ""], ["Ionita", "Anca Daniela", ""], ["Sgarciu", "Valentin", ""]]}, {"id": "1308.6744", "submitter": "Navaz Syed A S", "authors": "A.S.Syed Navaz, M.Ravi and T.Prabhu", "title": "Preventing Disclosure of Sensitive Knowledge by Hiding Inference", "comments": "7 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Mining is a way of extracting data or uncovering hidden patterns of\ninformation from databases. So, there is a need to prevent the inference rules\nfrom being disclosed such that the more secure data sets cannot be identified\nfrom non sensitive attributes. This can be done through removing or adding\ncertain item sets in the transactions Sanitization. The purpose is to hide the\nInference rules, so that the user may not be able to discover any valuable\ninformation from other non sensitive data and any organisation can release all\nsamples of their data without the fear of Knowledge Discovery In Databases\nwhich can be achieved by investigating frequently occurring item sets, rules\nthat can be mined from them with the objective of hiding them. Another way is\nto release only limited samples in the new database so that there is no\ninformation loss and it also satisfies the legitimate needs of the users. The\nmajor problem is uncovering hidden patterns, which causes a threat to the\ndatabase security. Sensitive data are inferred from non-sensitive data based on\nthe semantics of the application the user has, commonly known as the inference\nproblem. Two fundamental approaches to protect sensitive rules from disclosure\nare that, preventing rules from being generated by hiding the frequent sets of\ndata items and reducing the importance of the rules by setting their confidence\nbelow a user-specified threshold.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 08:34:08 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Navaz", "A. S. Syed", ""], ["Ravi", "M.", ""], ["Prabhu", "T.", ""]]}]