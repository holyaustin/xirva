[{"id": "2105.00121", "submitter": "Doris Jung-Lin Lee", "authors": "Doris Jung-Lin Lee, Dixin Tang, Kunal Agarwal, Thyne Boonmark, Caitlyn\n  Chen, Jake Kang, Ujjaini Mukhopadhyay, Jerry Song, Micah Yong, Marti A.\n  Hearst, Aditya G. Parameswaran", "title": "Lux: Always-on Visualization Recommendations for Exploratory Data\n  Science", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploratory data science largely happens in computational notebooks with\ndataframe API, such as pandas, that support flexible means to transform, clean,\nand analyze data. Yet, visually exploring data in dataframes remains tedious,\nrequiring substantial programming effort for visualization and mental effort to\ndetermine what analysis to perform next. We propose Lux, an always-on framework\nfor accelerating visual insight discovery in data science workflows. When users\nprint a dataframe in their notebooks, Lux recommends visualizations to provide\na quick overview of the patterns and trends and suggests promising analysis\ndirections. Lux features a high-level language for generating visualizations\non-demand to encourage rapid visual experimentation with data. We demonstrate\nthat through the use of a careful design and three system optimizations, Lux\nadds no more than two seconds of overhead on top of pandas for over 98% of\ndatasets in the UCI repository. We evaluate Lux in terms of usability via a\ncontrolled first-use study and interviews with early adopters, finding that Lux\nhelps fulfill the needs of data scientists for visualization support within\ntheir dataframe workflows. Lux has already been embraced by data science\npractitioners, with over 1.9k stars on Github within its first 15 months.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 23:28:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lee", "Doris Jung-Lin", ""], ["Tang", "Dixin", ""], ["Agarwal", "Kunal", ""], ["Boonmark", "Thyne", ""], ["Chen", "Caitlyn", ""], ["Kang", "Jake", ""], ["Mukhopadhyay", "Ujjaini", ""], ["Song", "Jerry", ""], ["Yong", "Micah", ""], ["Hearst", "Marti A.", ""], ["Parameswaran", "Aditya G.", ""]]}, {"id": "2105.00467", "submitter": "Abdul Quamar", "authors": "Venkata Vamsikrishna Meduri, Abdul Quamar, Chuan Lei, Vasilis\n  Efthymiou, Fatma Ozcan", "title": "BI-REC: Guided Data Analysis for Conversational Business Intelligence", "comments": "16 pages, 16 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational interfaces to Business Intelligence (BI) applications enable\ndata analysis using a natural language dialog in small incremental steps. To\ntruly unleash the power of conversational BI to democratize access to data, a\nsystem needs to provide effective and continuous support for data analysis. In\nthis paper, we propose BI-REC, a conversational recommendation system for BI\napplications to help users accomplish their data analysis tasks.\n  We define the space of data analysis in terms of BI patterns, augmented with\nrich semantic information extracted from the OLAP cube definition, and use\ngraph embeddings learned using GraphSAGE to create a compact representation of\nthe analysis state. We propose a two-step approach to explore the search space\nfor useful BI pattern recommendations. In the first step, we train a\nmulti-class classifier using prior query logs to predict the next high-level\nactions in terms of a BI operation (e.g., {\\em Drill-Down} or {\\em Roll-up})\nand a measure that the user is interested in. In the second step, the\nhigh-level actions are further refined into actual BI pattern recommendations\nusing collaborative filtering. This two-step approach allows us to not only\ndivide and conquer the huge search space, but also requires less training data.\nOur experimental evaluation shows that BI-REC achieves an accuracy of 83% for\nBI pattern recommendations and up to 2X speedup in latency of prediction\ncompared to a state-of-the-art baseline. Our user study further shows that\nBI-REC provides recommendations with a precision@3 of 91.90% across several\ndifferent analysis tasks.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 13:19:38 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Meduri", "Venkata Vamsikrishna", ""], ["Quamar", "Abdul", ""], ["Lei", "Chuan", ""], ["Efthymiou", "Vasilis", ""], ["Ozcan", "Fatma", ""]]}, {"id": "2105.00618", "submitter": "Sina Shaham", "authors": "Sina Shaham, Gabriel Ghinita, Cyrus Shahabi", "title": "An Efficient and Secure Location-based Alert Protocol using Searchable\n  Encryption and Huffman Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Location data are widely used in mobile apps, ranging from location-based\nrecommendations, to social media and navigation. A specific type of interaction\nis that of location-based alerts, where mobile users subscribe to a service\nprovider (SP) in order to be notified when a certain event occurs nearby.\nConsider, for instance, the ongoing COVID-19 pandemic, where contact tracing\nhas been singled out as an effective means to control the virus spread. Users\nwish to be notified if they came in proximity to an infected individual.\nHowever, serious privacy concerns arise if the users share their location\nhistory with the SP in plaintext. To address privacy, recent work proposed\nseveral protocols that can securely implement location-based alerts. The users\nupload their encrypted locations to the SP, and the evaluation of location\npredicates is done directly on ciphertexts. When a certain individual is\nreported as infected, all matching ciphertexts are found (e.g., according to a\npredicate such as \"10 feet proximity to any of the locations visited by the\ninfected patient in the last week\"), and the corresponding users notified.\nHowever, there are significant performance issues associated with existing\nprotocols. The underlying searchable encryption primitives required to perform\nthe matching on ciphertexts are expensive, and without a proper encoding of\nlocations and search predicates, the performance can degrade a lot. In this\npaper, we propose a novel method for variable-length location encoding based on\nHuffman codes. By controlling the length required to represent encrypted\nlocations and the corresponding matching predicates, we are able to\nsignificantly speed up performance. We provide a theoretical analysis of the\ngain achieved by using Huffman codes, and we show through extensive experiments\nthat the improvement compared with fixed-length encoding methods is\nsubstantial.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 03:55:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Shaham", "Sina", ""], ["Ghinita", "Gabriel", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "2105.00639", "submitter": "Arnab Bhattacharyya", "authors": "A. Pavan and N.V. Vinodchandran and Arnab Bhattacharyya and Kuldeep S.\n  Meel", "title": "Model Counting meets F0 Estimation", "comments": "Appears in PODS '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Constraint satisfaction problems (CSP's) and data stream models are two\npowerful abstractions to capture a wide variety of problems arising in\ndifferent domains of computer science. Developments in the two communities have\nmostly occurred independently and with little interaction between them. In this\nwork, we seek to investigate whether bridging the seeming communication gap\nbetween the two communities may pave the way to richer fundamental insights. To\nthis end, we focus on two foundational problems: model counting for CSP's and\ncomputation of zeroth frequency moments ($F_0$) for data streams.\n  Our investigations lead us to observe striking similarity in the core\ntechniques employed in the algorithmic frameworks that have evolved separately\nfor model counting and $F_0$ computation. We design a recipe for translation of\nalgorithms developed for $F_0$ estimation to that of model counting, resulting\nin new algorithms for model counting. We then observe that algorithms in the\ncontext of distributed streaming can be transformed to distributed algorithms\nfor model counting. We next turn our attention to viewing streaming from the\nlens of counting and show that framing $F_0$ estimation as a special case of\n#DNF counting allows us to obtain a general recipe for a rich class of\nstreaming problems, which had been subjected to case-specific analysis in prior\nworks. In particular, our view yields a state-of-the art algorithm for\nmultidimensional range efficient $F_0$ estimation with a simpler analysis.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 06:14:14 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Pavan", "A.", ""], ["Vinodchandran", "N. V.", ""], ["Bhattacharyya", "Arnab", ""], ["Meel", "Kuldeep S.", ""]]}, {"id": "2105.00642", "submitter": "Benjamin Hilprecht", "authors": "Benjamin Hilprecht and Carsten Binnig", "title": "One Model to Rule them All: Towards Zero-Shot Learning for Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our vision of so called zero-shot learning for\ndatabases which is a new learning approach for database components. Zero-shot\nlearning for databases is inspired by recent advances in transfer learning of\nmodels such as GPT-3 and can support a new database out-of-the box without the\nneed to train a new model. As a first concrete contribution in this paper, we\nshow the feasibility of zero-shot learning for the task of physical cost\nestimation and present very promising initial results. Moreover, as a second\ncontribution we discuss the core challenges related to zero-shot learning for\ndatabases and present a roadmap to extend zero-shot learning towards many other\ntasks beyond cost estimation or even beyond classical database systems and\nworkloads.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 06:18:47 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hilprecht", "Benjamin", ""], ["Binnig", "Carsten", ""]]}, {"id": "2105.00683", "submitter": "Baotong Lu", "authors": "Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, Tianzheng Wang", "title": "APEX: A High-Performance Learned Index on Persistent Memory", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently released persistent memory (PM) has been gaining popularity. PM\noffers high performance (still lower than DRAM), persistence, and is cheaper\nthan DRAM. This opens up new possibilities for index design. Existing indexes\non PM typically evolve traditional B+tree indexes. However, this is not the\nbest choice. Recently proposed learned indexes use machine learning (ML) for\ndata distribution-aware indexing and perform significantly better than B+Trees,\nbut none support persistence and instant recovery. In this paper, we propose a\nnew learned index on PM namely APEX, with very high performance, persistence,\nconcurrency, and instant recovery. Our very careful design combines the best of\nboth worlds. Specifically, APEX is designed to reduce PM accesses, especially\nthe heavy-weight writes, flush, and memory fence instructions, while still\nexploiting ML, to achieve high performance. Our in-depth experimental\nevaluation on Intel DCPMM shows that APEX achieves up to ~15X higher throughput\nas compared to the state-of-the-art PM indexes, and can recover from failures\nin ~42ms. To the best of our knowledge, APEX is the first full-fledged and\npractical learned index on PM.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 08:36:44 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 02:45:25 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Lu", "Baotong", ""], ["Ding", "Jialin", ""], ["Lo", "Eric", ""], ["Minhas", "Umar Farooq", ""], ["Wang", "Tianzheng", ""]]}, {"id": "2105.00792", "submitter": "Genoveva Vargas-Solar", "authors": "Genoveva Vargas-Solar, Jos\\'e-Luis Zechinelli-Martini, Javier A.\n  Espinosa-Oviedo and Luis M. Vilches-Bl\\'azquez", "title": "LACLICHEV: Exploring the History of Climate Change in Latin America\n  within Newspapers Digital Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces LACLICHEV (Latin American Climate Change Evolution\nplatform ), a data collections exploration environment for exploring historical\nnewspapers searching for articles reporting meteorological events. LACLICHEV is\nbased on data collections' exploration techniques combined with information\nretrieval, data analytics, and geographic querying and visualization. This\nenvironment provides tools for curating, exploring and analyzing historical\nnewspapers articles, their description and location, and the vocabularies used\nfor referring to meteorological events. The objective being to understand the\ncontent of newspapers and identifying possible patterns and models that can\nbuild a view of the history of climate change in the Latin American region.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 12:45:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Vargas-Solar", "Genoveva", ""], ["Zechinelli-Martini", "Jos\u00e9-Luis", ""], ["Espinosa-Oviedo", "Javier A.", ""], ["Vilches-Bl\u00e1zquez", "Luis M.", ""]]}, {"id": "2105.01092", "submitter": "Johannes De Smedt", "authors": "Johannes De Smedt, Anton Yeshchenko, Artem Polyvyanyy, Jochen De\n  Weerdt, Jan Mendling", "title": "Process Model Forecasting Using Time Series Analysis of Event Sequence\n  Data", "comments": "Accepted at the International Conference on Conceptual Modeling 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process analytics is an umbrella of data-driven techniques which includes\nmaking predictions for individual process instances or overall process models.\nAt the instance level, various novel techniques have been recently devised,\ntackling next activity, remaining time, and outcome prediction. At the model\nlevel, there is a notable void. It is the ambition of this paper to fill this\ngap. To this end, we develop a technique to forecast the entire process model\nfrom historical event data. A forecasted model is a will-be process model\nrepresenting a probable future state of the overall process. Such a forecast\nhelps to investigate the consequences of drift and emerging bottlenecks. Our\ntechnique builds on a representation of event data as multiple time series,\neach capturing the evolution of a behavioural aspect of the process model, such\nthat corresponding forecasting techniques can be applied. Our implementation\ndemonstrates the accuracy of our technique on real-world event log data.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 18:00:27 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 11:56:43 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["De Smedt", "Johannes", ""], ["Yeshchenko", "Anton", ""], ["Polyvyanyy", "Artem", ""], ["De Weerdt", "Jochen", ""], ["Mendling", "Jan", ""]]}, {"id": "2105.01651", "submitter": "Shuyuan Zheng", "authors": "Shuyuan Zheng, Yang Cao, Masatoshi Yoshikawa", "title": "Trading Data with Personalized Differential Privacy and Partial\n  Arbitrage Freeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing trend regarding perceiving personal data as a commodity.\nExisting studies have built frameworks and theories about how to determine an\narbitrage-free price of a given query according to the privacy loss quantified\nby differential privacy. However, those previous works have assumed that data\nbuyers can purchase query answers with the arbitrary privacy loss of data\nowners, which may not be valid under strict privacy regulations such as GDPR\nand the increasing privacy concerns of data owners. In this paper, we study how\nto empower data owners with the control of privacy loss in regard to data\ntrading. First, we propose a modularized framework for trading personal data\nthat enables each data owner to bound her personalized privacy loss from data\ntrading. Second, since bounded privacy losses indicate bounded utilities of\nquery answers, we propose a reasonable relaxation of arbitrage freeness named\npartial arbitrage freeness, i.e., the guarantee of arbitrage-free pricing only\nfor a limited range of utilities, which provides more possibilities for our\nmarket design. Third, to avoid arbitrage behaviors, we propose a general method\nfor ensuring arbitrage freeness under personalized differential privacy.\nFourth, to make full use of data owners' personalized privacy loss bounds, we\npropose online privacy budget allocation techniques to dynamically allocate\nprivacy losses for queries under arbitrage freeness.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:53:30 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zheng", "Shuyuan", ""], ["Cao", "Yang", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "2105.01818", "submitter": "Stavros Sintos", "authors": "Pankaj K. Agarwal, Xiao Hu, Stavros Sintos, Jun Yang", "title": "Dynamic Enumeration of Similarity Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers enumerating answers to similarity-join queries under\ndynamic updates: Given two sets of $n$ points $A,B$ in $\\mathbb{R}^d$, a metric\n$\\phi(\\cdot)$, and a distance threshold $r > 0$, report all pairs of points\n$(a, b) \\in A \\times B$ with $\\phi(a,b) \\le r$. Our goal is to store $A,B$ into\na dynamic data structure that, whenever asked, can enumerate all result pairs\nwith worst-case delay guarantee, i.e., the time between enumerating two\nconsecutive pairs is bounded. Furthermore, the data structure can be\nefficiently updated when a point is inserted into or deleted from $A$ or $B$.\n  We propose several efficient data structures for answering similarity-join\nqueries in low dimension. For exact enumeration of similarity join, we present\nnear-linear-size data structures for $\\ell_1, \\ell_\\infty$ metrics with\n$\\log^{O(1)} n$ update time and delay. We show that such a data structure is\nnot feasible for the $\\ell_2$ metric for $d \\ge 4$. For approximate enumeration\nof similarity join, where the distance threshold is a soft constraint, we\nobtain a unified linear-size data structure for $\\ell_p$ metric, with\n$\\log^{O(1)} n$ delay and update time. In high dimensions, we present an\nefficient data structure with worst-case delay-guarantee using locality\nsensitive hashing (LSH).\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 01:18:27 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Hu", "Xiao", ""], ["Sintos", "Stavros", ""], ["Yang", "Jun", ""]]}, {"id": "2105.01925", "submitter": "Simon Razniewski", "authors": "Simon Razniewski", "title": "Commonsense Knowledge Base Construction in the Age of Big Data", "comments": "Manuscript for the cancelled BTW 2021 demo track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compiling commonsense knowledge is traditionally an AI topic approached by\nmanual labor. Recent advances in web data processing have enabled automated\napproaches. In this demonstration we will showcase three systems for automated\ncommonsense knowledge base construction, highlighting each time one aspect of\nspecific interest to the data management community. (i) We use Quasimodo to\nillustrate knowledge extraction systems engineering, (ii) Dice to illustrate\nthe role that schema constraints play in cleaning fuzzy commonsense knowledge,\nand (iii) Ascent to illustrate the relevance of conceptual modelling. The demos\nare available online at https://quasimodo.r2.enst.fr,\nhttps://dice.mpi-inf.mpg.de and ascent.mpi-inf.mpg.de.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 08:27:36 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Razniewski", "Simon", ""]]}, {"id": "2105.02418", "submitter": "Ziniu Wu", "authors": "Ziniu Wu, Peilun Yang, Pei Yu, Rong Zhu, Yuxing Han, Yaliang Li, Defu\n  Lian, Kai Zeng, Jingren Zhou", "title": "A Unified Transferable Model for ML-Enhanced DBMS", "comments": "7 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the database management system (DBMS) community has witnessed the\npower of machine learning (ML) solutions for DBMS tasks. Despite their\npromising performance, these existing solutions can hardly be considered\nsatisfactory. First, these ML-based methods in DBMS are not effective enough\nbecause they are optimized on each specific task, and cannot explore or\nunderstand the intrinsic connections between tasks. Second, the training\nprocess has serious limitations that hinder their practicality, because they\nneed to retrain the entire model from scratch for a new DB. Moreover, for each\nretraining, they require an excessive amount of training data, which is very\nexpensive to acquire and unavailable for a new DB. We propose to explore the\ntransferabilities of the ML methods both across tasks and across DBs to tackle\nthese fundamental drawbacks.\n  In this paper, we propose a unified model MTMLF that uses a multi-task\ntraining procedure to capture the transferable knowledge across tasks and a\npretrain finetune procedure to distill the transferable meta knowledge across\nDBs. We believe this paradigm is more suitable for cloud DB service, and has\nthe potential to revolutionize the way how ML is used in DBMS. Furthermore, to\ndemonstrate the predicting power and viability of MTMLF, we provide a concrete\nand very promising case study on query optimization tasks. Last but not least,\nwe discuss several concrete research opportunities along this line of work.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 03:31:32 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wu", "Ziniu", ""], ["Yang", "Peilun", ""], ["Yu", "Pei", ""], ["Zhu", "Rong", ""], ["Han", "Yuxing", ""], ["Li", "Yaliang", ""], ["Lian", "Defu", ""], ["Zeng", "Kai", ""], ["Zhou", "Jingren", ""]]}, {"id": "2105.03161", "submitter": "Adrian Wilke", "authors": "Adrian Wilke, Axel-Cyrille Ngonga Ngomo", "title": "Open Data Portal Germany (OPAL) Projektergebnisse", "comments": "in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Open Data Portal Germany (OPAL) project, a pipeline of the following\ndata refinement steps has been developed: requirements analysis, data\nacquisition, analysis, conversion, integration and selection. 800,000 datasets\nin DCAT format have been produced.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 10:59:16 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wilke", "Adrian", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "2105.03239", "submitter": "Bilal Abu-Salih", "authors": "Bilal Abu-Salih,Pornpit Wongthongtham, Dengya Zhu, Kit Yan Chan, Amit\n  Rudra", "title": "Semantic data discovery from Social Big Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.01624", "journal-ref": null, "doi": "10.1007/978-981-33-6652-7_4", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the large volume of data and information generated by a multitude of\nsocial data sources, it is a huge challenge to manage and extract useful\nknowledge, especially given the different forms of data, streaming data and\nuncertainty and ambiguity of data. Hence, there are still challenges in this\narea of BD analytics research to capture, store, process, visualise, query, and\nmanipulate datasets to derive meaningful information that is specific to an\napplication's domain. This chapter attempts to address this problem by studying\nSemantic Analytics and domain knowledge modelling, and to what extent these\ntechnologies can be utilised toward better understanding to the social textual\ncontents. In particular, the chapter gives an overview of semantic analysis and\ndomain ontology followed by shedding light on domain knowledge modelling,\ninference, semantic storage, and publicly available semantic tools and APIs.\nAlso, the theoretical notion of Knowledge Graphs is reported and their\ninterlinking with SBD is discussed. The utility of the semantic analytics is\ndemonstrated and evaluated through a case study on social data in the context\nof politics domain.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 19:24:34 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Abu-Salih", "Bilal", ""], ["Wongthongtham", "Pornpit", ""], ["Zhu", "Dengya", ""], ["Chan", "Kit Yan", ""], ["Rudra", "Amit", ""]]}, {"id": "2105.03313", "submitter": "Genoveva Vargas-Solar", "authors": "Raj Ratn Pranesh and Mehrdad Farokhnejad and Ambesh Shekhar and\n  Genoveva Vargas-Solar", "title": "Looking for COVID-19 misinformation in multilingual social media texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Multilingual COVID-19 Analysis Method (CMTA) for\ndetecting and observing the spread of misinformation about this disease within\ntexts. CMTA proposes a data science (DS) pipeline that applies machine learning\nmodels for processing, classifying (Dense-CNN) and analyzing (MBERT)\nmultilingual (micro)-texts. DS pipeline data preparation tasks extract features\nfrom multilingual textual data and categorize it into specific information\nclasses (i.e., 'false', 'partly false', 'misleading'). The CMTA pipeline has\nbeen experimented with multilingual micro-texts (tweets), showing\nmisinformation spread across different languages. To assess the performance of\nCMTA and put it in perspective, we performed a comparative analysis of CMTA\nwith eight monolingual models used for detecting misinformation. The comparison\nshows that CMTA has surpassed various monolingual models and suggests that it\ncan be used as a general method for detecting misinformation in multilingual\nmicro-texts. CMTA experimental results show misinformation trends about\nCOVID-19 in different languages during the first pandemic months.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:30:49 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Pranesh", "Raj Ratn", ""], ["Farokhnejad", "Mehrdad", ""], ["Shekhar", "Ambesh", ""], ["Vargas-Solar", "Genoveva", ""]]}, {"id": "2105.03371", "submitter": "Haoyu Ren", "authors": "Haoyu Ren, Darko Anicic, Thomas Runkler", "title": "The Synergy of Complex Event Processing and Tiny Machine Learning in\n  Industrial IoT", "comments": "Accepted by The 15th ACM International Conference on Distributed and\n  Event-based Systems (DEBS) 2021", "journal-ref": null, "doi": "10.1145/3465480.3466928", "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focusing on comprehensive networking, big data, and artificial intelligence,\nthe Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness\nin factory operations. Various sensors and field devices play a central role,\nas they generate a vast amount of real-time data that can provide insights into\nmanufacturing. The synergy of complex event processing (CEP) and machine\nlearning (ML) has been developed actively in the last years in IIoT to identify\npatterns in heterogeneous data streams and fuse raw data into tangible facts.\nIn a traditional compute-centric paradigm, the raw field data are continuously\nsent to the cloud and processed centrally. As IIoT devices become increasingly\npervasive and ubiquitous, concerns are raised since transmitting such amount of\ndata is energy-intensive, vulnerable to be intercepted, and subjected to high\nlatency. The data-centric paradigm can essentially solve these problems by\nempowering IIoT to perform decentralized on-device ML and CEP, keeping data\nprimarily on edge devices and minimizing communications. However, this is no\nmean feat because most IIoT edge devices are designed to be computationally\nconstrained with low power consumption. This paper proposes a framework that\nexploits ML and CEP's synergy at the edge in distributed sensor networks. By\nleveraging tiny ML and micro CEP, we shift the computation from the cloud to\nthe power-constrained IIoT devices and allow users to adapt the on-device ML\nmodel and the CEP reasoning logic flexibly on the fly without requiring to\nreupload the whole program. Lastly, we evaluate the proposed solution and show\nits effectiveness and feasibility using an industrial use case of machine\nsafety monitoring.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:58:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ren", "Haoyu", ""], ["Anicic", "Darko", ""], ["Runkler", "Thomas", ""]]}, {"id": "2105.04449", "submitter": "Hongzhi Chen", "authors": "Hongzhi Chen, Changji Li, Chenguang Zheng, Chenghuan Huang, Juncheng\n  Fang, James Cheng, Jian Zhang", "title": "G-Tran: Making Distributed Graph Transactions Fast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Graph transaction processing raises many unique challenges such as random\ndata access due to the irregularity of graph structures, low throughput and\nhigh abort rate due to the relatively large read/write sets in graph\ntransactions. To address these challenges, we present G-Tran -- an RDMA-enabled\ndistributed in-memory graph database with serializable and snapshot isolation\nsupport. First, we propose a graph-native data store to achieve good data\nlocality and fast data access for transactional updates and queries. Second,\nG-Tran adopts a fully decentralized architecture that leverages RDMA to process\ndistributed transactions with the MPP model, which can achieve high performance\nby utilizing all computing resources. In addition, we propose a new MV-OCC\nimplementation with two optimizations to address the issue of large read/write\nsets in graph transactions. Extensive experiments show that G-Tran achieves\ncompetitive performance compared with other popular graph databases on\nbenchmark workloads.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:18:27 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 11:13:11 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chen", "Hongzhi", ""], ["Li", "Changji", ""], ["Zheng", "Chenguang", ""], ["Huang", "Chenghuan", ""], ["Fang", "Juncheng", ""], ["Cheng", "James", ""], ["Zhang", "Jian", ""]]}, {"id": "2105.04486", "submitter": "Niranjan Rai", "authors": "Niranjan Rai and Xiang Lian", "title": "Probabilistic Top-k Dominating Queries in Distributed Uncertain\n  Databases (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications such as business planning and sensor data\nmonitoring, one important, yet challenging, the task is to rank objects(e.g.,\nproducts, documents, or spatial objects) based on their ranking scores and\nefficiently return those objects with the highest scores. In practice, due to\nthe unreliability of data sources, many real-world objects often contain noises\nand are thus imprecise and uncertain. In this paper, we study the problem of\nprobabilistic top-k dominating(PTD) query on such large-scale uncertain data in\na distributed environment, which retrieves k uncertain objects from distributed\nuncertain databases(on multiple distributed servers), having the largest\nranking scores with high confidences. In order to efficiently tackle the\ndistributed PTD problem, we propose a MapReduce framework for processing\ndistributed PTD queries over distributed uncertain databases. In this MapReduce\nframework, we design effective pruning strategies to filter out false alarms in\nthe distributed setting, propose cost-model-based index distribution mechanisms\nover servers, and develop efficient distributed PTD query processing\nalgorithms. Extensive experiments have demonstrated the efficiency and\neffectiveness of our proposed distributed PTD approach on both real and\nsynthetic data sets through various experimental settings.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:23:58 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 23:29:57 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Rai", "Niranjan", ""], ["Lian", "Xiang", ""]]}, {"id": "2105.04547", "submitter": "Chengdong Yao", "authors": "Chengdong Yao", "title": "Highly Efficient Memory Failure Prediction using Mcelog-based Data\n  Mining and Machine Learning", "comments": "11 pages, 2 figures, 1 table. Codes has been open source to\n  https://www.github.com/ycd2016/acaioc2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.PF cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the data center, unexpected downtime caused by memory failures can lead to\na decline in the stability of the server and even the entire information\ntechnology infrastructure, which harms the business. Therefore, whether the\nmemory failure can be accurately predicted in advance has become one of the\nmost important issues to be studied in the data center. However, for the memory\nfailure prediction in the production system, it is necessary to solve technical\nproblems such as huge data noise and extreme imbalance between positive and\nnegative samples, and at the same time ensure the long-term stability of the\nalgorithm. This paper compares and summarizes some commonly used skills and the\nimprovement they can bring. The single model we proposed won the top 14th in\nthe 2nd Alibaba Cloud AIOps Competition belonging to the 25th PAKDD conference.\nIt takes only 30 minutes to pass the online test, while most of the other\ncontestants' solution need more than 3 hours. Codes has been open source to\nhttps://www.github.com/ycd2016/acaioc2.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 11:38:05 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 05:38:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yao", "Chengdong", ""]]}, {"id": "2105.04720", "submitter": "Renan Souza", "authors": "Renan Souza, V\\'itor Silva, Alexandre A. B. Lima, Daniel de Oliveira,\n  Patrick Valduriez, Marta Mattoso", "title": "Distributed In-memory Data Management for Workflow Executions", "comments": "26 pages, 14 figures, PeerJ Computer Science (2021)", "journal-ref": null, "doi": "10.7717/peerj-cs.527", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex scientific experiments from various domains are typically modeled as\nworkflows and executed on large-scale machines using a Parallel Workflow\nManagement System (WMS). Since such executions usually last for hours or days,\nsome WMSs provide user steering support, i.e., they allow users to run data\nanalyses and, depending on the results, adapt the workflows at runtime. A\nchallenge in the parallel execution control design is to manage workflow data\nfor efficient executions while enabling user steering support. Data access for\nhigh scalability is typically transaction-oriented, while for data analysis, it\nis online analytical-oriented so that managing such hybrid workloads makes the\nchallenge even harder. In this work, we present SchalaDB, an architecture with\na set of design principles and techniques based on distributed in-memory data\nmanagement for efficient workflow execution control and user steering. We\npropose a distributed data design for scalable workflow task scheduling and\nhigh availability driven by a parallel and distributed in-memory DBMS. To\nevaluate our proposal, we develop d-Chiron, a WMS designed according to\nSchalaDB's principles. We carry out an extensive experimental evaluation on an\nHPC cluster with up to 960 computing cores. Among other analyses, we show that\neven when running data analyses for user steering, SchalaDB's overhead is\nnegligible for workloads composed of hundreds of concurrent tasks on shared\ndata. Our results encourage workflow engine developers to follow a parallel and\ndistributed data-oriented approach not only for scheduling and monitoring but\nalso for user steering.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 00:16:47 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 01:30:04 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Souza", "Renan", ""], ["Silva", "V\u00edtor", ""], ["Lima", "Alexandre A. B.", ""], ["de Oliveira", "Daniel", ""], ["Valduriez", "Patrick", ""], ["Mattoso", "Marta", ""]]}, {"id": "2105.05130", "submitter": "Li Wang", "authors": "Li Wang", "title": "Towards a Model for LSH", "comments": "arXiv admin note: text overlap with arXiv:2103.01888", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As data volumes continue to grow, clustering and outlier detection algorithms\nare becoming increasingly time-consuming. Classical index structures for\nneighbor search are no longer sustainable due to the \"curse of dimensionality\".\nInstead, approximated index structures offer a good opportunity to\nsignificantly accelerate the neighbor search for clustering and outlier\ndetection and to have the lowest possible error rate in the results of the\nalgorithms. Locality-sensitive hashing is one of those. We indicate directions\nto model the properties of LSH.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 15:39:55 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Li", ""]]}, {"id": "2105.05443", "submitter": "Wei Dong", "authors": "Wei Dong and Ke Yi", "title": "A Nearly Instance-optimal Differentially Private Mechanism for\n  Conjunctive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Releasing the result size of conjunctive queries and graph pattern queries\nunder differential privacy (DP) has received considerable attention in the\nliterature, but existing solutions do not offer any optimality guarantees. We\nprovide the first DP mechanism for this problem with a fairly strong notion of\noptimality, which can be considered as a natural relaxation of\ninstance-optimality.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 05:47:07 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Dong", "Wei", ""], ["Yi", "Ke", ""]]}, {"id": "2105.05699", "submitter": "Chris Williams", "authors": "Tijl De Bie, Luc De Raedt, Jos\\'e Hern\\'andez-Orallo, Holger H. Hoos,\n  Padhraic Smyth, Christopher K. I. Williams", "title": "Automating Data Science: Prospects and Challenges", "comments": "19 pages, 3 figures. Accepted for publication (April 2021) in\n  Communications of the ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the complexity of typical data science projects and the associated\ndemand for human expertise, automation has the potential to transform the data\nscience process.\n  Key insights:\n  * Automation in data science aims to facilitate and transform the work of\ndata scientists, not to replace them.\n  * Important parts of data science are already being automated, especially in\nthe modeling stages, where techniques such as automated machine learning\n(AutoML) are gaining traction.\n  * Other aspects are harder to automate, not only because of technological\nchallenges, but because open-ended and context-dependent tasks require human\ninteraction.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 14:34:35 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["De Bie", "Tijl", ""], ["De Raedt", "Luc", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""], ["Hoos", "Holger H.", ""], ["Smyth", "Padhraic", ""], ["Williams", "Christopher K. I.", ""]]}, {"id": "2105.05782", "submitter": "Sainyam Galhotra", "authors": "Raghavendra Addanki, Sainyam Galhotra, Barna Saha", "title": "How to Design Robust Algorithms using Noisy Comparison Oracle", "comments": "PVLDB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Metric based comparison operations such as finding maximum, nearest and\nfarthest neighbor are fundamental to studying various clustering techniques\nsuch as $k$-center clustering and agglomerative hierarchical clustering. These\ntechniques crucially rely on accurate estimation of pairwise distance between\nrecords. However, computing exact features of the records, and their pairwise\ndistances is often challenging, and sometimes not possible. We circumvent this\nchallenge by leveraging weak supervision in the form of a comparison oracle\nthat compares the relative distance between the queried points such as `Is\npoint u closer to v or w closer to x?'.\n  However, it is possible that some queries are easier to answer than others\nusing a comparison oracle. We capture this by introducing two different noise\nmodels called adversarial and probabilistic noise. In this paper, we study\nvarious problems that include finding maximum, nearest/farthest neighbor search\nunder these noise models. Building upon the techniques we develop for these\ncomparison operations, we give robust algorithms for k-center clustering and\nagglomerative hierarchical clustering. We prove that our algorithms achieve\ngood approximation guarantees with a high probability and analyze their query\ncomplexity. We evaluate the effectiveness and efficiency of our techniques\nempirically on various real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 16:58:09 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Addanki", "Raghavendra", ""], ["Galhotra", "Sainyam", ""], ["Saha", "Barna", ""]]}, {"id": "2105.05807", "submitter": "Zhusheng Wang", "authors": "Zhusheng Wang and Sennur Ulukus", "title": "Symmetric Private Information Retrieval with User-Side Common Randomness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of symmetric private information retrieval (SPIR)\nwith user-side common randomness. In SPIR, a user retrieves a message out of\n$K$ messages from $N$ non-colluding and replicated databases in such a way that\nno single database knows the retrieved message index (user privacy), and the\nuser gets to know nothing further than the retrieved message (database\nprivacy). SPIR has a capacity smaller than the PIR capacity which requires only\nuser privacy, is infeasible in the case of a single database, and requires\nshared common randomness among the databases. We introduce a new variant of\nSPIR where the user is provided with a random subset of the shared database\ncommon randomness, which is unknown to the databases. We determine the exact\ncapacity region of the triple $(d, \\rho_S, \\rho_U)$, where $d$ is the download\ncost, $\\rho_S$ is the amount of shared database (server) common randomness, and\n$\\rho_U$ is the amount of available user-side common randomness. We show that\nwith a suitable amount of $\\rho_U$, this new SPIR achieves the capacity of\nconventional PIR. As a corollary, single-database SPIR becomes feasible.\nFurther, the presence of user-side $\\rho_U$ reduces the amount of required\nserver-side $\\rho_S$.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:11:35 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wang", "Zhusheng", ""], ["Ulukus", "Sennur", ""]]}, {"id": "2105.06058", "submitter": "Anna Fariha", "authors": "Sainyam Galhotra, Anna Fariha, Raoni Louren\\c{c}o, Juliana Freire,\n  Alexandra Meliou, Divesh Srivastava", "title": "DataExposer: Exposing Disconnect between Data and Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data is a central component of many modern systems, the cause of a system\nmalfunction may reside in the data, and, specifically, particular properties of\nthe data. For example, a health-monitoring system that is designed under the\nassumption that weight is reported in imperial units (lbs) will malfunction\nwhen encountering weight reported in metric units (kilograms). Similar to\nsoftware debugging, which aims to find bugs in the mechanism (source code or\nruntime conditions), our goal is to debug the data to identify potential\nsources of disconnect between the assumptions about the data and the systems\nthat operate on that data. Specifically, we seek which properties of the data\ncause a data-driven system to malfunction. We propose DataExposer, a framework\nto identify data properties, called profiles, that are the root causes of\nperformance degradation or failure of a system that operates on the data. Such\nidentification is necessary to repair the system and resolve the disconnect\nbetween data and system. Our technique is based on causal reasoning through\ninterventions: when a system malfunctions for a dataset, DataExposer alters the\ndata profiles and observes changes in the system's behavior due to the\nalteration. Unlike statistical observational analysis that reports mere\ncorrelations, DataExposer reports causally verified root causes, in terms of\ndata profiles, of the system malfunction. We empirically evaluate DataExposer\non three real-world and several synthetic data-driven systems that fail on\ndatasets due to a diverse set of reasons. In all cases, DataExposer identifies\nthe root causes precisely while requiring orders of magnitude fewer\ninterventions than prior techniques.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 03:18:46 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Fariha", "Anna", ""], ["Louren\u00e7o", "Raoni", ""], ["Freire", "Juliana", ""], ["Meliou", "Alexandra", ""], ["Srivastava", "Divesh", ""]]}, {"id": "2105.06399", "submitter": "Ali Jazayeri", "authors": "Ali Jazayeri and Christopher C. Yang", "title": "Frequent Pattern Mining in Continuous-time Temporal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are used as highly expressive tools in different disciplines. In\nrecent years, the analysis and mining of temporal networks have attracted\nsubstantial attention. Frequent pattern mining is considered an essential task\nin the network science literature. In addition to the numerous applications,\nthe investigation of frequent pattern mining in networks directly impacts other\nanalytical approaches, such as clustering, quasi-clique and clique mining, and\nlink prediction. In nearly all the algorithms proposed for frequent pattern\nmining in temporal networks, the networks are represented as sequences of\nstatic networks. Then, the inter- or intra-network patterns are mined. This\ntype of representation imposes a computation-expressiveness trade-off to the\nmining problem. In this paper, we propose a novel representation that can\npreserve the temporal aspects of the network losslessly. Then, we introduce the\nconcept of constrained interval graphs (CIGs). Next, we develop a series of\nalgorithms for mining the complete set of frequent temporal patterns in a\ntemporal network data set. We also consider four different definitions of\nisomorphism to allow noise tolerance in temporal data collection. Implementing\nthe algorithm for three real-world data sets proves the practicality of the\nproposed algorithm and its capability to discover unknown patterns in various\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 02:47:24 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Jazayeri", "Ali", ""], ["Yang", "Christopher C.", ""]]}, {"id": "2105.06494", "submitter": "Carlos Javier Fern\\'andez Candel", "authors": "Carlos J. Fern\\'andez Candel, Diego Sevilla Ruiz and Jes\\'us J.\n  Garc\\'ia-Molina", "title": "A Unified Metamodel for NoSQL and Relational Databases", "comments": "31 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Database field is undergoing significant changes. Although relational\nsystems are still predominant, the interest in NoSQL systems is continuously\nincreasing. In this scenario, polyglot persistence is envisioned as the\ndatabase architecture to be prevalent in the future.\n  Multi-model database tools normally use a generic or unified metamodel to\nrepresent schemas of the data model that they support. Such metamodels\nfacilitate developing utilities, as they can be built on a common\nrepresentation. Also, the number of mappings required to migrate databases from\na data model to another is reduced, and integrability is favored.\n  In this paper, we present the U-Schema unified metamodel able to represent\nlogical schemas for the four most popular NoSQL paradigms (columnar, document,\nkey-value, and graph) as well as relational schemas. We will formally define\nthe mappings between U-Schema and the data model defined for each paradigm. How\nthese mappings have been implemented and validated will be discussed, and some\napplications of U-Schema will be shown.\n  To achieve flexibility to respond to data changes, most of NoSQL systems are\n\"schema-on-write,\" and the declaration of schemas is not required. Such an\nabsence of schema declaration makes structural variability possible, i.e.,\nstored data of the same entity type can have different structure. Moreover,\ndata relationships supported by each data model are different. We will show how\nall these issues have been tackled in our approach.\n  Our metamodel goes beyond the existing proposals by distinguishing entity\ntypes and relationship types, representing aggregation and reference\nrelationships, and including the notion of structural variability. Our\ncontributions also include developing schema extraction strategies for\nschemaless systems of each NoSQL data model, and tackling performance and\nscalability in the implementation for each store.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 18:13:42 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 08:42:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Candel", "Carlos J. Fern\u00e1ndez", ""], ["Ruiz", "Diego Sevilla", ""], ["Garc\u00eda-Molina", "Jes\u00fas J.", ""]]}, {"id": "2105.07396", "submitter": "Henderik Alex Proper", "authors": "R.D.T. Janssen, H.A. Proper, H. Bosma, D. Verhoef, S.J.B.A.\n  Hoppenbrouwers", "title": "Developing an Architecture Method Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, there are millions of professionals worldwide acting as a designer,\narchitect or engineer in the design, realization, and implementation of\ninformation systems. At this moment there is no well established and clearly\nidentified body of knowledge that defines their profession in a \"standard\" way.\nIn this article, we present the idea of developing an architecture method\nlibrary. Such a library could play a pivotal role to further professionalize\nthe field. The library contains project experiences, reference architectures,\nliterature, proven methods, tools, etc. Access mechanisms allow the\nprofessional to use this body of knowledge. By giving it an open nature, it can\nbe filled by professionals from different fields. Feedback mechanisms are\npossible to improve the contents of the library, for example by giving feedback\non the method components in the library.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 10:18:26 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 09:45:47 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Janssen", "R. D. T.", ""], ["Proper", "H. A.", ""], ["Bosma", "H.", ""], ["Verhoef", "D.", ""], ["Hoppenbrouwers", "S. J. B. A.", ""]]}, {"id": "2105.07597", "submitter": "Zhenzhong Chen", "authors": "Yaochen Zhu and Zhenzhong Chen", "title": "Collaborative Variational Bandwidth Auto-encoder for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collaborative filtering has been widely adopted by modern recommender systems\nto discover user preferences based on their past behaviors. However, the\nobserved interactions for different users are usually unbalanced, which leads\nto high uncertainty in the collaborative embeddings of users with sparse\nratings, thereby severely degenerating the recommendation performance.\nConsequently, more efforts have been dedicated to the hybrid recommendation\nstrategy where user/item features are utilized as auxiliary information to\naddress the sparsity problem. However, since these features contain rich\nmultimodal patterns and most of them are irrelevant to the recommendation\npurpose, excessive reliance on these features will make the model difficult to\ngeneralize. To address the above two challenges, we propose a VBAE for\nrecommendation. VBAE models both the collaborative and the user feature\nembeddings as Gaussian random variables inferred via deep neural networks to\ncapture non-linear similarities between users based on their ratings and\nfeatures. Furthermore, VBAE establishes an information regulation mechanism by\nintroducing a user-dependent channel variable where the bandwidth is determined\nby the information already contained in the observed ratings to dynamically\ncontrol the amount of information allowed to be accessed from the corresponding\nuser features. The user-dependent channel variable alleviates the uncertainty\nproblem when the ratings are sparse while avoids unnecessary dependence of the\nmodel on noisy user features simultaneously. Codes and datasets are released at\nhttps://github.com/yaochenzhu/vbae.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 04:00:33 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhu", "Yaochen", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2105.07666", "submitter": "Daniel Schuster", "authors": "Daniel Schuster, Sebastiaan J. van Zelst, Wil M. P. van der Aalst", "title": "Cortado---An Interactive Tool for Data-Driven Process Discovery and\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining aims to diagnose and improve operational processes. Process\nmining techniques allow analyzing the event data generated and recorded during\nthe execution of (business) processes to gain valuable insights. Process\ndiscovery is a key discipline in process mining that comprises the discovery of\nprocess models on the basis of the recorded event data. Most process discovery\nalgorithms work in a fully automated fashion. Apart from adjusting their\nconfiguration parameters, conventional process discovery algorithms offer\nlimited to no user interaction, i.e., we either edit the discovered process\nmodel by hand or change the algorithm's input by, for instance, filtering the\nevent data. However, recent work indicates that the integration of domain\nknowledge in (semi-)automated process discovery algorithms often enhances the\nquality of the process models discovered. Therefore, this paper introduces\nCortado, a novel process discovery tool that leverages domain knowledge while\nincrementally discovering a process model from given event data. Starting from\nan initial process model, Cortado enables the user to incrementally add new\nprocess behavior to the process model under construction in a visual and\nintuitive manner. As such, Cortado unifies the world of manual process modeling\nwith that of automated process discovery.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:29:43 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Schuster", "Daniel", ""], ["van Zelst", "Sebastiaan J.", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2105.07753", "submitter": "Kimiaki Shirahama", "authors": "Kazuma Fujioka and Kimiaki Shirahama", "title": "Generic Itemset Mining Based on Reinforcement Learning", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest problems in itemset mining is the requirement of\ndeveloping a data structure or algorithm, every time a user wants to extract a\ndifferent type of itemsets. To overcome this, we propose a method, called\nGeneric Itemset Mining based on Reinforcement Learning (GIM-RL), that offers a\nunified framework to train an agent for extracting any type of itemsets. In\nGIM-RL, the environment formulates iterative steps of extracting a target type\nof itemsets from a dataset. At each step, an agent performs an action to add or\nremove an item to or from the current itemset, and then obtains from the\nenvironment a reward that represents how relevant the itemset resulting from\nthe action is to the target type. Through numerous trial-and-error steps where\nvarious rewards are obtained by diverse actions, the agent is trained to\nmaximise cumulative rewards so that it acquires the optimal action policy for\nforming as many itemsets of the target type as possible. In this framework, an\nagent for extracting any type of itemsets can be trained as long as a reward\nsuitable for the type can be defined. The extensive experiments on mining high\nutility itemsets, frequent itemsets and association rules show the general\neffectiveness and one remarkable potential (agent transfer) of GIM-RL. We hope\nthat GIM-RL opens a new research direction towards learning-based itemset\nmining.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:57:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Fujioka", "Kazuma", ""], ["Shirahama", "Kimiaki", ""]]}, {"id": "2105.08105", "submitter": "Fei Chiang", "authors": "Zheng Zheng, Longtao Zheng, Morteza Alipour Langouri, Fei Chiang,\n  Lukasz Golab, Jaroslaw Szlichta", "title": "Discovery and Contextual Data Cleaning with Ontology Functional\n  Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional Dependencies (FDs) define attribute relationships based on\nsyntactic equality, and, when usedin data cleaning, they erroneously label\nsyntactically different but semantically equivalent values as errors. We\nexplore dependency-based data cleaning with Ontology Functional\nDependencies(OFDs), which express semantic attribute relationships such as\nsynonyms and is-a hierarchies defined by an ontology. We study the theoretical\nfoundations for OFDs, including sound and complete axioms and a linear-time\ninference procedure. We then propose an algorithm for discovering OFDs (exact\nones and ones that hold with some exceptions) from data that uses the axioms to\nprune the search space. Towards enabling OFDs as data quality rules in\npractice, we study the problem of finding minimal repairs to a relation and\nontology with respect to a set of OFDs. We demonstrate the effectiveness of our\ntechniques on real datasets, and show that OFDs can significantly reduce the\nnumber of false positive errors in data cleaning techniques that rely on\ntraditional FDs.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:33:48 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 02:21:46 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 22:49:59 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zheng", "Zheng", ""], ["Zheng", "Longtao", ""], ["Langouri", "Morteza Alipour", ""], ["Chiang", "Fei", ""], ["Golab", "Lukasz", ""], ["Szlichta", "Jaroslaw", ""]]}, {"id": "2105.08131", "submitter": "Bryar Hassan Dr.", "authors": "Bryar A. Hassan, Shko M. Qader", "title": "A New Framework to Adopt Multidimensional Databases for Organizational\n  Information System Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As information becomes increasingly sizable for organizations to maintain the\nchallenge of organizing data still remains. More importantly, the on-going\nprocess of analysing incoming data occurs on a continual basis and\norganizations should employ existing procedures that may not be adequate or\nefficient when attempting to access specific information to analyse. In these\nlatter days of technological advancement, organizations can offer their\ncustomers extensive data resources to utilize and thus accomplish individual\nobjectives and maintain competitiveness; however, it remains a challenge in\nproviding data in a format that serves each clients suited needs. For some, the\ncomplexity of a data model can be overwhelming to utilize. Furthermore,\ncompanies should secure an understanding of the purchasing power used by\nspecific consumer groups to remain competitive and ease the operation of data\nanalysis. This research paper is to examine the use of multi-dimensional models\nwithin a business environment and how it may provide customers and managers\nwith generating queries that will provide accurate and relevant data for\neffective analysis. It also provides a new framework that can aid various types\nof organisations using sizable database systems to create their own\nmultidimensional model from relational databases and present the data in\nmultidimensional views. It also defines the requirements. Despite the\navailability of set tools, the complexity of utilizing the conceptions\ndiscourages customers as they may become apprehensive about exploring these\noptions for analytical purposes. This could be done by conducting a query.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 19:49:45 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Hassan", "Bryar A.", ""], ["Qader", "Shko M.", ""]]}, {"id": "2105.08312", "submitter": "Elena Strzheletska", "authors": "Elena V. Strzheletska and Vassilis J. Tsotras", "title": "Reachability and Top-k Reachability Queries with Transfer Decay", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prevalence of location tracking systems has resulted in large volumes of\nspatiotemporal data generated every day. Addressing reachability queries on\nsuch datasets is important for a wide range of applications (surveillance,\npublic health, social networks, etc.) A spatiotemporal reachability query\nidentifies whether a physical item (or information etc.) could have been\ntransferred from the source object $O_S$ to the target object $O_T$ during a\ntime interval $I$ (either directly, or through a chain of intermediate\ntransfers). In previous research on spatiotemporal reachability queries, the\nnumber of such transfers is not limited, and the weight of a piece of\ntransferred information remains the same. This paper introduces novel\nreachability queries, which assume a scenario of information decay. Such\nqueries arise when the value of information that travels through the chain of\nintermediate objects decreases with each transfer. To address such queries\nefficiently over large spatiotemporal datasets, we introduce the RICCdecay\nalgorithm. Further, the decay scenario leads to an important extension: if\nthere are many different sources of information, the aggregate value of\ninformation an object can obtain varies. As a result, we introduce a top-k\nreachability problem, identifying the k objects with the highest accumulated\ninformation. We also present the RICCtopK algorithm that can efficiently\ncompute top-k reachability with transfer decay queries. An experimental\nevaluation shows the efficiency of the proposed algorithms over previous\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 07:00:32 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Strzheletska", "Elena V.", ""], ["Tsotras", "Vassilis J.", ""]]}, {"id": "2105.08628", "submitter": "Zheng Dong", "authors": "Zheng Dong, Xin Huang, Guorui Yuan, Hengshu Zhu, Hui Xiong", "title": "Butterfly-Core Community Search over Labeled Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Community search aims at finding densely connected subgraphs for query\nvertices in a graph. While this task has been studied widely in the literature,\nmost of the existing works only focus on finding homogeneous communities rather\nthan heterogeneous communities with different labels. In this paper, we\nmotivate a new problem of cross-group community search, namely Butterfly-Core\nCommunity (BCC), over a labeled graph, where each vertex has a label indicating\nits properties and an edge between two vertices indicates their cross\nrelationship. Specifically, for two query vertices with different labels, we\naim to find a densely connected cross community that contains two query\nvertices and consists of butterfly networks, where each wing of the butterflies\nis induced by a k-core search based on one query vertex and two wings are\nconnected by these butterflies. Indeed, the BCC structure admits the structure\ncohesiveness and minimum diameter, and thus can effectively capture the\nheterogeneous and concise collaborative team. Moreover, we theoretically prove\nthis problem is NP-hard and analyze its non-approximability. To efficiently\ntackle the problem, we develop a heuristic algorithm, which first finds a BCC\ncontaining the query vertices, then iteratively removes the farthest vertices\nto the query vertices from the graph. The algorithm can achieve a\n2-approximation to the optimal solution. To further improve the efficiency, we\ndesign a butterfly-core index and develop a suite of efficient algorithms for\nbutterfly-core identification and maintenance as vertices are eliminated.\nExtensive experiments on eight real-world networks and four novel case studies\nvalidate the effectiveness and efficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 09:21:38 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 11:07:13 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Dong", "Zheng", ""], ["Huang", "Xin", ""], ["Yuan", "Guorui", ""], ["Zhu", "Hengshu", ""], ["Xiong", "Hui", ""]]}, {"id": "2105.08716", "submitter": "Henderik Alex Proper", "authors": "P. D. Bruza and H. A. Proper", "title": "Discovering the Information that is lost in our Databases -- Why bother\n  storing data if you can't find the information?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are surrounded by an ever increasing amount of data that is stored in a\nvariety of databases. In this article we will use a very liberal definition of\n\\EM{database}. Basically any collection of data can be regarded as a database,\nranging from the files in a directory on a disk, to ftp and web servers,\nthrough to relational or object-oriented databases. The sole reason for storing\ndata in databases is that there is an anticipated need for the stored data at\nsome time in the future. This means that providing smooth access paths by which\nstored information can be retrieved is at least as important as ensuring\nintegrity of the stored information. In practice, however, providing users with\nadequate avenues by which to access stored information has received far less\nattention.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:56:32 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bruza", "P. D.", ""], ["Proper", "H. A.", ""]]}, {"id": "2105.08770", "submitter": "Roy Friedman", "authors": "Gil Einziger and Ohad Eytan and Roy Friedman and Benjamin Manes", "title": "Lightweight Robust Size Aware Cache Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern key-value stores, object stores, Internet proxy caches, as well as\nContent Delivery Networks (CDN) often manage objects of diverse sizes, e.g.,\nblobs, video files of different lengths, images with varying resolution, and\nsmall documents. In such workloads, size-aware cache policies outperform\nsize-oblivious algorithms. Unfortunately, existing size-aware algorithms tend\nto be overly complicated and computationally~expensive.\n  Our work follows a more approachable pattern; we extend the prevalent\n(size-oblivious) TinyLFU cache admission policy to handle variable sized items.\nImplementing our approach inside two popular caching libraries only requires\nminor changes. We show that our algorithms yield competitive or better\nhit-ratios and byte hit-ratios compared to the state of the art size-aware\nalgorithms such as AdaptSize, LHD, LRB, and GDSF. Further, a runtime comparison\nindicates that our implementation is faster by up to x3 compared to the best\nalternative, i.e., it imposes much lower CPU overhead.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:35:40 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 18:41:00 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Einziger", "Gil", ""], ["Eytan", "Ohad", ""], ["Friedman", "Roy", ""], ["Manes", "Benjamin", ""]]}, {"id": "2105.08830", "submitter": "Lujing Cen", "authors": "Lujing Cen, Andreas Kipf, Ryan Marcus, Tim Kraska", "title": "LEA: A Learned Encoding Advisor for Column Stores", "comments": null, "journal-ref": null, "doi": "10.1145/3464509.3464885", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data warehouses organize data in a columnar format to enable faster scans and\nbetter compression. Modern systems offer a variety of column encodings that can\nreduce storage footprint and improve query performance. Selecting a good\nencoding scheme for a particular column is an optimization problem that depends\non the data, the query workload, and the underlying hardware. We introduce\nLearned Encoding Advisor (LEA), a learned approach to column encoding\nselection. LEA is trained on synthetic datasets with various distributions on\nthe target system. Once trained, LEA uses sample data and statistics (such as\ncardinality) from the user's database to predict the optimal column encodings.\nLEA can optimize for encoded size, query performance, or a combination of the\ntwo. Compared to the heuristic-based encoding advisor of a commercial column\nstore on TPC-H, LEA achieves 19% lower query latency while using 26% less\nspace.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:06:42 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Cen", "Lujing", ""], ["Kipf", "Andreas", ""], ["Marcus", "Ryan", ""], ["Kraska", "Tim", ""]]}, {"id": "2105.08842", "submitter": "Ansgar Scherp", "authors": "Fabian Singhofer, Aygul Garifullina, Mathias Kern, Ansgar Scherp", "title": "rx-anon -- A Novel Approach on the De-Identification of Heterogeneous\n  Data based on a Modified Mondrian Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches for data anonymization consider relational data and\ntextual data independently. We propose rx-anon, an anonymization approach for\nheterogeneous semi-structured documents composed of relational and textual\nattributes. We map sensitive terms extracted from the text to the structured\ndata. This allows us to use concepts like k-anonymity to generate a joined,\nprivacy-preserved version of the heterogeneous data input. We introduce the\nconcept of redundant sensitive information to consistently anonymize the\nheterogeneous data. To control the influence of anonymization over unstructured\ntextual data versus structured data attributes, we introduce a modified,\nparameterized Mondrian algorithm. The parameter $\\lambda$ allows to give\ndifferent weight on the relational and textual attributes during the\nanonymization process. We evaluate our approach with two real-world datasets\nusing a Normalized Certainty Penalty score, adapted to the problem of jointly\nanonymizing relational and textual data. The results show that our approach is\ncapable of reducing information loss by using the tuning parameter to control\nthe Mondrian partitioning while guaranteeing k-anonymity for relational\nattributes as well as for sensitive terms. As rx-anon is a framework approach,\nit can be reused and extended by other anonymization algorithms, privacy\nmodels, and textual similarity metrics.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:50:12 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Singhofer", "Fabian", ""], ["Garifullina", "Aygul", ""], ["Kern", "Mathias", ""], ["Scherp", "Ansgar", ""]]}, {"id": "2105.08878", "submitter": "Jeremy Chen", "authors": "Jeremy Chen, Yuqing Huang, Mushi Wang, Semih Salihoglu, Ken Salem", "title": "Accurate Summary-based Cardinality Estimation Through the Lens of\n  Cardinality Estimation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two classes of summary-based cardinality estimators that use\nstatistics about input relations and small-size joins in the context of graph\ndatabase management systems: (i) optimistic estimators that make uniformity and\nconditional independence assumptions; and (ii) the recent pessimistic\nestimators that use information theoretic linear programs. We begin by\naddressing the problem of how to make accurate estimates for optimistic\nestimators. We model these estimators as picking bottom-to-top paths in a\ncardinality estimation graph (CEG), which contains sub-queries as nodes and\nweighted edges between sub-queries that represent average degrees. We outline a\nspace of heuristics to make an optimistic estimate in this framework and show\nthat effective heuristics depend on the structure of the input queries. We\nobserve that on acyclic queries and queries with small-size cycles, using the\nmaximum-weight path is an effective technique to address the well known\nunderestimation problem for optimistic estimators. We show that on a large\nsuite of datasets and workloads, the accuracy of such estimates is up to three\norders of magnitude more accurate in mean q-error than some prior heuristics\nthat have been proposed in prior work. In contrast, we show that on queries\nwith larger cycles these estimators tend to overestimate, which can partially\nbe addressed by using minimum weight paths and more effectively by using an\nalternative CEG. We then show that CEGs can also model the recent pessimistic\nestimators. This surprising result allows us to connect two disparate lines of\nwork on optimistic and pessimistic estimators, adopt an optimization from\npessimistic estimators to optimistic ones, and provide insights into the\npessimistic estimators, such as showing that there are alternative\ncombinatorial solutions to the linear programs that define them.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 01:52:38 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Chen", "Jeremy", ""], ["Huang", "Yuqing", ""], ["Wang", "Mushi", ""], ["Salihoglu", "Semih", ""], ["Salem", "Ken", ""]]}, {"id": "2105.09058", "submitter": "George Chernishev", "authors": "Alexander Slesarev, Evgeniy Klyuchikov, Kirill Smirnov, George\n  Chernishev", "title": "Revisiting Data Compression in Column-Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data compression is widely used in contemporary column-oriented DBMSes to\nlower space usage and to speed up query processing. Pioneering systems have\nintroduced compression to tackle the disk bandwidth bottleneck by trading CPU\nprocessing power for it. The main issue of this is a trade-off between the\ncompression ratio and the decompression CPU cost. Existing results state that\nlight-weight compression with small decompression costs outperforms\nheavy-weight compression schemes in column-stores. However, since the time\nthese results were obtained, CPU, RAM, and disk performance have advanced\nconsiderably. Moreover, novel compression algorithms have emerged.\n  In this paper, we revisit the problem of compression in disk-based\ncolumn-stores. More precisely, we study the I/O-RAM compression scheme which\nimplies that there are two types of pages of different size: disk pages\n(compressed) and in-memory pages (uncompressed). In this scheme, the buffer\nmanager is responsible for decompressing pages as soon as they arrive from\ndisk. This scheme is rather popular as it is easy to implement: several modern\ncolumn and row-stores use it.\n  We pose and address the following research questions: 1) Are heavy-weight\ncompression schemes still inappropriate for disk-based column-stores?, 2) Are\nnew light-weight compression algorithms better than the old ones?, 3) Is there\na need for SIMD-employing decompression algorithms in case of a disk-based\nsystem? We study these questions experimentally using a columnar query engine\nand Star Schema Benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:53:41 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Slesarev", "Alexander", ""], ["Klyuchikov", "Evgeniy", ""], ["Smirnov", "Kirill", ""], ["Chernishev", "George", ""]]}, {"id": "2105.09160", "submitter": "Chao Zhang", "authors": "Chao Zhang, Jiaheng Lu, Qingsong Guo, Xinyong Zhang, Xiaochun Han,\n  Minqi Zhou", "title": "Automatic View Selection in Graph Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several works have studied the problem of view selection in graph\ndatabases. However, existing methods cannot fully exploit the graph properties\nof views, e.g., supergraph views and common subgraph views, which leads to a\nlow view utility and duplicate view content. To address the problem, we propose\nan end-to-end graph view selection tool, G-View, which can judiciously generate\na view set from a query workload by exploring the graph properties of candidate\nviews and considering their efficacy. Specifically, given a graph query set and\na space budget, G-View translates each query to a candidate view pattern and\nchecks the query containment via a filtering-and-verification framework. G-View\nthen selects the views using a graph gene algorithm (GGA), which relies on a\nthree-phase framework that explores graph view transformations to reduce the\nview space and optimize the view benefit. Finally, G-View generates the\nextended graph views that persist all the edge-induced subgraphs to answer the\nsubgraph and supergraph queries simultaneously. Extensive experiments on\nreal-life and synthetic datasets demonstrated G-View achieved averagely 21x and\n2x query performance speedup over two view-based methods while having 2x and 5x\nsmaller space overhead, respectively. Moreover, the proposed selection\nalgorithm, GGA, outperformed other selection methods in both effectiveness and\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 14:21:53 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhang", "Chao", ""], ["Lu", "Jiaheng", ""], ["Guo", "Qingsong", ""], ["Zhang", "Xinyong", ""], ["Han", "Xiaochun", ""], ["Zhou", "Minqi", ""]]}, {"id": "2105.09312", "submitter": "Sandra Geisler", "authors": "Sandra Geisler, Maria-Esther Vidal, Cinzia Cappiello, Bernadette\n  Farias L\\'oscio, Avigdor Gal, Matthias Jarke, Maurizio Lenzerini, Paolo\n  Missier, Boris Otto, Elda Paja, Barbara Pernici, Jakob Rehof", "title": "Knowledge-driven Data Ecosystems Towards Data Transparency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Data Ecosystem offers a keystone-player or alliance-driven infrastructure\nthat enables the interaction of different stakeholders and the resolution of\ninteroperability issues among shared data. However, despite years of research\nin data governance and management, trustability is still affected by the\nabsence of transparent and traceable data-driven pipelines. In this work, we\nfocus on requirements and challenges that data ecosystems face when ensuring\ndata transparency. Requirements are derived from the data and organizational\nmanagement, as well as from broader legal and ethical considerations. We\npropose a novel knowledge-driven data ecosystem architecture, providing the\npillars for satisfying the analyzed requirements. We illustrate the potential\nof our proposal in a real-world scenario. Lastly, we discuss and rate the\npotential of the proposed architecture in the fulfillment of these\nrequirements.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:08:43 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 10:47:59 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Geisler", "Sandra", ""], ["Vidal", "Maria-Esther", ""], ["Cappiello", "Cinzia", ""], ["L\u00f3scio", "Bernadette Farias", ""], ["Gal", "Avigdor", ""], ["Jarke", "Matthias", ""], ["Lenzerini", "Maurizio", ""], ["Missier", "Paolo", ""], ["Otto", "Boris", ""], ["Paja", "Elda", ""], ["Pernici", "Barbara", ""], ["Rehof", "Jakob", ""]]}, {"id": "2105.09418", "submitter": "Mayukh Bagchi", "authors": "Fausto Giunchiglia, Simone Bocca, Mattia Fumagalli, Mayukh Bagchi and\n  Alessio Zamboni", "title": "iTelos- Building reusable knowledge graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is a fact that, when developing a new application, it is virtually\nimpossible to reuse, as-is, existing datasets. This difficulty is the cause of\nadditional costs, with the further drawback that the resulting application will\nagain be hardly reusable. It is a negative loop which consistently reinforces\nitself and for which there seems to be no way out. iTelos is a general purpose\nmethodology designed to break this loop. Its main goal is to generate reusable\nKnowledge Graphs (KGs), built reusing, as much as possible, already existing\ndata. The key assumption is that the design of a KG should be done middle-out\nmeaning by this that the design should take into consideration, in all phases\nof the development: (i) the purpose to be served, that we formalize as a set of\ncompetency queries, (ii) a set of pre-existing datasets, possibly extracted\nfrom existing KGs, and (iii) a set of pre-existing reference schemas, whose\ngoal is to facilitate sharability. We call these reference schemas,\nteleologies, as distinct from ontologies, meaning by this that, while having a\nsimilar purpose, they are designed to be easily adapted, thus becoming a key\nenabler of itelos.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 22:07:46 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Giunchiglia", "Fausto", ""], ["Bocca", "Simone", ""], ["Fumagalli", "Mattia", ""], ["Bagchi", "Mayukh", ""], ["Zamboni", "Alessio", ""]]}, {"id": "2105.09432", "submitter": "Mayukh Bagchi", "authors": "Fausto Giunchiglia, Alessio Zamboni, Mayukh Bagchi and Simone Bocca", "title": "Stratified Data Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach to the problem of semantic heterogeneity where\ndata are organized into a set of stratified and independent representation\nlayers, namely: conceptual(where a set of unique alinguistic identifiers are\nconnected inside a graph codifying their meaning), language(where sets of\nsynonyms, possibly from multiple languages, annotate concepts), knowledge(in\nthe form of a graph where nodes are entity types and links are properties), and\ndata(in the form of a graph of entities populating the previous knowledge\ngraph). This allows us to state the problem of semantic heterogeneity as a\nproblem of Representation Diversity where the different types of heterogeneity,\nviz. Conceptual, Language, Knowledge, and Data, are uniformly dealt within each\nsingle layer, independently from the others. In this paper we describe the\nproposed stratified representation of data and the process by which data are\nfirst transformed into the target representation, then suitably integrated and\nthen, finally, presented to the user in her preferred format. The proposed\nframework has been evaluated in various pilot case studies and in a number of\nindustrial data integration problems.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 23:14:41 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Giunchiglia", "Fausto", ""], ["Zamboni", "Alessio", ""], ["Bagchi", "Mayukh", ""], ["Bocca", "Simone", ""]]}, {"id": "2105.09561", "submitter": "Henderik Alex Proper", "authors": "H. A. Proper", "title": "Generating Significant Examples for Conceptual Schema Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report bases itself on the idea of using concrete examples to verify\nconceptual schemas, and in particular cardinality constraints. When novice ORM\nmodellers model domains, the selection of proper cardinality constraints for\nrelationship types is quite often prone to errors. In this report we propose a\nmechanism for the generation of significant examples for selected subschemas.\nThe generated examples are significant in the sense that they illustrate the\npossible combinations of instances that are allowed with respect to the\ncardinality constraints on the involved relationship types. In this report we\nfirstly provide a brief informal discussion of the basic idea. Then we present\na syntactic mechanism to select the subschema for which example instances are\nto be generated. This is followed by the actual example generation algorithm\nitself. We will also present, as a {\\em spin-off}, an algorithm that allows us\nto detect possible flaws in the conceptual schema by calculating the number of\ninstances that can be used to populate the types in the schema.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 07:27:52 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Proper", "H. A.", ""]]}, {"id": "2105.10016", "submitter": "Xinyu Liu", "authors": "Xinyu Liu, Qi Zhou, Joy Arulraj, and Alessandro Orso", "title": "Automated Performance Bug Detection in Database Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because database systems are the critical component of modern data-intensive\napplications, it is important to ensure that they operate correctly. To this\nend, developers extensively test these systems to eliminate bugs that\nnegatively affect functionality. In addition to functional bugs, however, there\nis another important class of bugs: performance bugs. These bugs negatively\naffect the response time of a database system and can therefore affect the\noverall performance of the system. Despite their impact on end-user experience,\nperformance bugs have received considerably less attention than functional\nbugs.\n  In this paper, we present AMOEBA, a system for automatically detecting\nperformance bugs in database systems. The core idea behind AMOEBA is to\nconstruct query pairs that are semantically equivalent to each other and then\ncompare their response time on the same database system. If the queries exhibit\na significant difference in their runtime performance, then the root cause is\nlikely a performance bug in the system. We propose a novel set of structure and\npredicate mutation rules for constructing query pairs that are likely to\nuncover performance bugs. We introduce feedback mechanisms for improving the\nefficacy and computational efficiency of the tool. We evaluate AMOEBA on two\nwidely-used DBMSs, namely PostgreSQL and CockroachDB. AMOEBA has discovered 20\npreviously-unknown performance bugs, among which developers have already\nconfirmed 14 and fixed 4.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:18:43 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Liu", "Xinyu", ""], ["Zhou", "Qi", ""], ["Arulraj", "Joy", ""], ["Orso", "Alessandro", ""]]}, {"id": "2105.10329", "submitter": "Ding Ding", "authors": "Jiachen Wang, Ding Ding, Huan Wang, Conrad Christensen, Zhaoguo Wang,\n  Haibo Chen, Jinyang Li", "title": "Polyjuice: High-Performance Transactions via Learned Concurrency Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrency control algorithms are key determinants of the performance of\nin-memory databases. Existing algorithms are designed to work well for certain\nworkloads. For example, optimistic concurrency control (OCC) is better than\ntwo-phase-locking (2PL) under low contention, while the converse is true under\nhigh contention.\n  To adapt to different workloads, prior works mix or switch between a few\nknown algorithms using manual insights or simple heuristics. We propose a\nlearning-based framework that instead explicitly optimizes concurrency control\nvia offline training to maximize performance. Instead of choosing among a small\nnumber of known algorithms, our approach searches in a \"policy space\" of\nfine-grained actions, resulting in novel algorithms that can outperform\nexisting algorithms by specializing to a given workload.\n  We build Polyjuice based on our learning framework and evaluate it against\nseveral existing algorithms. Under different configurations of TPC-C and TPC-E,\nPolyjuice can achieve throughput numbers higher than the best of existing\nalgorithms by 15% to 56%.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 13:01:19 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 15:04:50 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 13:34:53 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wang", "Jiachen", ""], ["Ding", "Ding", ""], ["Wang", "Huan", ""], ["Christensen", "Conrad", ""], ["Wang", "Zhaoguo", ""], ["Chen", "Haibo", ""], ["Li", "Jinyang", ""]]}, {"id": "2105.10349", "submitter": "Henderik Alex Proper", "authors": "H. A. Proper", "title": "Interactive Query Formulation using Spider Queries", "comments": "arXiv admin note: substantial text overlap with arXiv:2102.01411,\n  arXiv:2105.09009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective information disclosure in the context of databases with a large\nconceptual schema is known to be a non-trivial problem. In particular the\nformulation of ad-hoc queries is a major problem in such contexts. Existing\napproaches for tackling this problem include graphical query interfaces, query\nby navigation, query by construction, and point to point queries. In this\narticle we propose the spider query mechanism as a final corner stone for an\neasy to use computer supported query formulation mechanism for InfoAssisant.\nThe basic idea behind a spider query is to build a (partial) query of all\ninformation considered to be relevant with respect to a given object type. The\nresult of this process is always a tree that fans out over existing conceptual\nschema (a spider). We also provide a brief discussion on the integration of the\nspider quer mechanism with the existing query by navigation, query by\nconstruction, and point to point query\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 07:50:47 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Proper", "H. A.", ""]]}, {"id": "2105.10520", "submitter": "Andreas Sendros", "authors": "P. Kostamis, A. Sendros, P.S. Efraimidis", "title": "Exploring Ethereum's Data Stores: A Cost and Performance Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of using a blockchain infrastructure as well as the time required to\nsearch and retrieve information from it must be considered when designing a\ndecentralized application. In this work, we examine a comprehensive set of data\nmanagement approaches for Ethereum applications and assess the associated cost\nin gas as well as the retrieval performance. More precisely, we analyze the\nstorage and retrieval of various-sized data, utilizing smart contract storage.\nIn addition, we study hybrid approaches by using IPFS and Swarm as storage\nplatforms along with Ethereum as a timestamping proof mechanism. Such schemes\nare especially effective when large chunks of data have to be managed.\nMoreover, we present methods for low-cost data handling in Ethereum, namely the\nevent-logs, the transaction payload, and the almost surprising exploitation of\nunused function arguments. Finally, we evaluate these methods on a\ncomprehensive set of experiments.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 18:06:29 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kostamis", "P.", ""], ["Sendros", "A.", ""], ["Efraimidis", "P. S.", ""]]}, {"id": "2105.10911", "submitter": "Amin Beheshti", "authors": "Amin Beheshti, Boualem Benatallah, Hamid Reza Motahari-Nezhad, Samira\n  Ghodratnama, Farhad Amouzgar", "title": "A Query Language for Summarizing and Analyzing Business Process Data", "comments": "arXiv admin note: text overlap with arXiv:1908.09232 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern enterprises, Business Processes (BPs) are realized over a mix of\nworkflows, IT systems, Web services and direct collaborations of people.\nAccordingly, process data (i.e., BP execution data such as logs containing\nevents, interaction messages and other process artifacts) is scattered across\nseveral systems and data sources, and increasingly show all typical properties\nof the Big Data. Understanding the execution of process data is challenging as\nkey business insights remain hidden in the interactions among process entities:\nmost objects are interconnected, forming complex, heterogeneous but often\nsemi-structured networks. In the context of business processes, we consider the\nBig Data problem as a massive number of interconnected data islands from\npersonal, shared and business data. We present a framework to model process\ndata as graphs, i.e., Process Graph, and present abstractions to summarize the\nprocess graph and to discover concept hierarchies for entities based on both\ndata objects and their interactions in process graphs. We present a language,\nnamely BP-SPARQL, for the explorative querying and understanding of process\ngraphs from various user perspectives. We have implemented a scalable\narchitecture for querying, exploration and analysis of process graphs. We\nreport on experiments performed on both synthetic and real-world datasets that\nshow the viability and efficiency of the approach.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 11:07:53 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Beheshti", "Amin", ""], ["Benatallah", "Boualem", ""], ["Motahari-Nezhad", "Hamid Reza", ""], ["Ghodratnama", "Samira", ""], ["Amouzgar", "Farhad", ""]]}, {"id": "2105.11075", "submitter": "Chen Luo", "authors": "Chen Luo and Michael J. Carey", "title": "DynaHash: Efficient Data Rebalancing in Apache AsterixDB (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parallel shared-nothing data management systems have been widely used to\nexploit a cluster of machines for efficient and scalable data processing. When\na cluster needs to be dynamically scaled in or out, data must be efficiently\nrebalanced. Ideally, data rebalancing should have a low data movement cost,\nincur a small overhead on data ingestion and query processing, and be performed\nonline without blocking reads or writes. However, existing parallel data\nmanagement systems often exhibit certain limitations and drawbacks in terms of\nefficient data rebalancing.\n  In this paper, we introduce DynaHash, an efficient data rebalancing approach\nthat combines dynamic bucketing with extendible hashing for shared-nothing\nOLAP-style parallel data management systems. DynaHash dynamically partitions\nthe records into a number of buckets using extendible hashing to achieve good a\nload balance with small rebalancing costs. We further describe an end-to-end\nimplementation of the proposed approach inside an open-source Big Data\nManagement System (BDMS), Apache AsterixDB. Our implementation exploits the\nout-of-place update design of LSM-trees to efficiently rebalance data without\nblocking concurrent reads and writes. Finally, we have conducted performance\nexperiments using the TPC-H benchmark and we present the results here.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 03:06:15 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Luo", "Chen", ""], ["Carey", "Michael J.", ""]]}, {"id": "2105.11147", "submitter": "Luigi Bellomarini", "authors": "Luigi Bellomarini, Emanuel Sallinger", "title": "Harmless but Useful: Beyond Separable Equality Constraints in Datalog+/-", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontological query answering is the problem of answering queries in the\npresence of schema constraints representing the domain of interest. Datalog+/-\nis a common family of languages for schema constraints, including\ntuple-generating dependencies (TGDs) and equality-generating dependencies\n(EGDs). The interplay of TGDs and EGDs leads to undecidability or\nintractability of query answering when adding EGDs to tractable Datalog+/-\nfragments, like Warded Datalog+/-, for which, in the sole presence of TGDs,\nquery answering is PTIME in data complexity. There have been attempts to limit\nthe interaction of TGDs and EGDs and guarantee tractability, in particular with\nthe introduction of separable EGDs, to make EGDs irrelevant for query answering\nas long as the set of constraints is satisfied. While being tractable,\nseparable EGDs have limited expressive power.\n  We propose a more general class of EGDs, which we call \"harmless\", that\nsubsume separable EGDs and allow to model a much broader class of problems.\nUnlike separable EGDs, harmless EGDs, besides enforcing ground equality\nconstraints, specialize the query answer by grounding or renaming the labelled\nnulls introduced by existential quantification in the TGDs. Harmless EGDs\ncapture the cases when the answer obtained in the presence of EGDs is less\ngeneral than the one obtained with TGDs only. We conclude that the theoretical\nproblem of deciding whether a set of constraints contains harmless EGDs is\nundecidable. We contribute a sufficient syntactic condition characterizing\nharmless EGDs, broad and useful in practice. We focus on Warded Datalog+/- with\nharmless EGDs and argue that, in such fragment, query answering is decidable\nand PTIME in data complexity. We study chase-based techniques for query\nanswering in Warded Datalog+/- with harmless EGDs, conducive to an efficient\nalgorithm to be implemented in state-of-the-art reasoners.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 08:22:16 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 05:52:54 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bellomarini", "Luigi", ""], ["Sallinger", "Emanuel", ""]]}, {"id": "2105.11410", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Kanthi Komar, Sanjukta Bhowmick and Sharma\n  Chakravarthy", "title": "From Base Data To Knowledge Discovery -- A Life Cycle Approach -- Using\n  Multilayer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Any large complex data analysis to infer or discover meaningful\ninformation/knowledge involves the following steps (in addition to data\ncollection, cleaning, preparing the data for analysis such as attribute\nelimination): i) Modeling the data -- an approach for modeling and deriving a\ndata representation for analysis using that approach, ii) translating analysis\nobjectives into computations on the model generated; this can be as simple as a\nsingle computation (e.g., community detection) or may involve a sequence of\noperations (e.g., pair-wise community detection over multiple networks) using\nexpressions based on the model, iii) computation of the expressions generated\n-- efficiency and scalability come into picture here, and iv) drill-down of\nresults to interpret or understand them clearly. Beyond this, it is also\nmeaningful to visualize results for easier understanding. Covid-19\nvisualization dashboard presented in this paper is an example of this.\n  This paper covers all of the above steps of data analysis life cycle using a\ndata representation that is gaining importance for multi-entity, multi-feature\ndata sets - Multilayer Networks. We use several data sets to establish the\neffectiveness of modeling using MLNs and analyze them using the proposed\ndecoupling approach. For coverage, we use different types of MLNs for modeling,\nand community and centrality computations for analysis. The data sets used - US\ncommercial airlines, IMDb, DBLP, and Covid-19 data set. Our experimental\nanalyses using the identified steps validate modeling, breadth of objectives\nthat can be computed, and overall versatility of the life cycle approach.\nCorrectness of results is verified, where possible, using independently\navailable ground truth. We demonstrate drill-down that is afforded by this\napproach (due to structure and semantics preservation) for a better\nunderstanding and visualization of results.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:00:30 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Santra", "Abhishek", ""], ["Komar", "Kanthi", ""], ["Bhowmick", "Sanjukta", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "2105.11926", "submitter": "Henderik Alex Proper", "authors": "H. A. Proper", "title": "ConQuer-92 -- The revised report on the conceptual query language LISA-D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report the conceptual query language ConQuer-92 is introduced. This\nquery language serves as the backbone of InfoAssistant's query facilities.\nFurthermore, this language can also be used for the specification of derivation\nrules (e.g. subtype defining rules) and textual constraints in InfoModeler.\nThis report is solely concerned with a formal definition, and the explanation\nthereof, of ConQuer-92. The implementation of ConQuer-92 in SQL-92 will be\ntreated in a separate report.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:09:35 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Proper", "H. A.", ""]]}, {"id": "2105.11983", "submitter": "Majid Rafiei", "authors": "Majid Rafiei and Wil M.P. van der Aalst", "title": "Group-Based Privacy Preservation Techniques for Process Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Process mining techniques help to improve processes using event data. Such\ndata are widely available in information systems. However, they often contain\nhighly sensitive information. For example, healthcare information systems\nrecord event data that can be utilized by process mining techniques to improve\nthe treatment process, reduce patient's waiting times, improve resource\nproductivity, etc. However, the recorded event data include highly sensitive\ninformation related to treatment activities. Responsible process mining should\nprovide insights about the underlying processes, yet, at the same time, it\nshould not reveal sensitive information. In this paper, we discuss the\nchallenges regarding directly applying existing well-known group-based privacy\npreservation techniques, e.g., k-anonymity, l-diversity, etc, to event data. We\nprovide formal definitions of attack models and introduce an effective\ngroup-based privacy preservation technique for process mining. Our technique\ncovers the main perspectives of process mining including control-flow, time,\ncase, and organizational perspectives. The proposed technique provides\ninterpretable and adjustable parameters to handle different privacy aspects. We\nemploy real-life event data and evaluate both data utility and result utility\nto show the effectiveness of the privacy preservation technique. We also\ncompare this approach with other group-based approaches for privacy-preserving\nevent data publishing.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:37:20 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Rafiei", "Majid", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2105.12228", "submitter": "Marco Minghini PhD", "authors": "Marco Minghini, Vlado Cetl, Alexander Kotsev, Robert Tomas, Michael\n  Lutz", "title": "INSPIRE: The Entry Point to Europe's Big Geospatial Data Infrastructure", "comments": "21 pages, 6 figures", "journal-ref": "In: Werner M., Chiang YY. (eds) Handbook of Big Geospatial Data.\n  Springer, Cham (2021)", "doi": "10.1007/978-3-030-55462-0_24", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initiated in 2007, the INSPIRE Directive has set a legal framework to create\na European-wide Spatial Data Infrastructure (SDI) to support the European Union\n(EU) environmental policies. This chapter analyses the INSPIRE infrastructure\nfrom a Big Geospatial Data perspective, describing how data is shared in an\ninteroperable way by public sector organisations in the EU Member States and\nhow it is made available in and accessible within the infrastructure. The\nINSPIRE Geoportal, which is the entry point to the whole infrastructure, is\npresented in detail. To justify its nature of a Big Geospatial Data\ninfrastructure, the characteristics of INSPIRE data are mapped to those of Big\nData's six 'Vs'. Despite many good results achieved in terms of data sharing,\nsome challenges still remain related to data consumption from the user side.\nThe chapter concludes with a dedicated discussion on how INSPIRE, and\ntraditional SDIs in general, should evolve into modern data ecosystems to\naddress these challenges while also embracing the modern practices of data\nsharing through the web.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 21:37:58 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 09:38:40 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Minghini", "Marco", ""], ["Cetl", "Vlado", ""], ["Kotsev", "Alexander", ""], ["Tomas", "Robert", ""], ["Lutz", "Michael", ""]]}, {"id": "2105.12287", "submitter": "Debjyoti Paul", "authors": "Debjyoti Paul, Jie Cao, Feifei Li, Vivek Srikumar", "title": "Database Workload Characterization with Query Plan Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart databases are adopting artificial intelligence (AI) technologies to\nachieve {\\em instance optimality}, and in the future, databases will come with\nprepackaged AI models within their core components. The reason is that every\ndatabase runs on different workloads, demands specific resources, and settings\nto achieve optimal performance. It prompts the necessity to understand\nworkloads running in the system along with their features comprehensively,\nwhich we dub as workload characterization.\n  To address this workload characterization problem, we propose our query plan\nencoders that learn essential features and their correlations from query plans.\nOur pretrained encoders capture the {\\em structural} and the {\\em computational\nperformance} of queries independently. We show that our pretrained encoders are\nadaptable to workloads that expedite the transfer learning process. We\nperformed independent assessments of structural encoder and performance\nencoders with multiple downstream tasks. For the overall evaluation of our\nquery plan encoders, we architect two downstream tasks (i) query latency\nprediction and (ii) query classification. These tasks show the importance of\nfeature-based workload characterization. We also performed extensive\nexperiments on individual encoders to verify the effectiveness of\nrepresentation learning and domain adaptability.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 01:17:27 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Paul", "Debjyoti", ""], ["Cao", "Jie", ""], ["Li", "Feifei", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2105.12457", "submitter": "Benjamin Hilprecht", "authors": "Benjamin Hilprecht and Carsten Binnig", "title": "ReStore -- Neural Data Completion for Relational Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical approaches for OLAP assume that the data of all tables is complete.\nHowever, in case of incomplete tables with missing tuples, classical approaches\nfail since the result of a SQL aggregate query might significantly differ from\nthe results computed on the full dataset. Today, the only way to deal with\nmissing data is to manually complete the dataset which causes not only high\nefforts but also requires good statistical skills to determine when a dataset\nis actually complete. In this paper, we propose an automated approach for\nrelational data completion called ReStore using a new class of (neural)\nschema-structured completion models that are able to synthesize data which\nresembles the missing tuples. As we show in our evaluation, this efficiently\nhelps to reduce the relative error of aggregate queries by up to 390% on\nreal-world data compared to using the incomplete data directly for query\nanswering.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 10:35:58 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hilprecht", "Benjamin", ""], ["Binnig", "Carsten", ""]]}, {"id": "2105.12507", "submitter": "Anastasios Gounaris", "authors": "Anna-Valentini Michailidou, Anastasios Gounaris and Konstantinos\n  Tsichlas", "title": "Cost models for geo-distributed massively parallel streaming analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report is part of the DataflowOpt project on optimization of modern\ndataflows and aims to introduce a data quality-aware cost model that covers the\nfollowing aspects in combination: (1) heterogeneity in compute nodes, (2)\ngeo-distribution, (3) massive parallelism, (4) complex DAGs and (5) streaming\napplications. Such a cost model can be then leveraged to devise cost-based\noptimization solutions that deal with task placement and operator\nconfiguration.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 12:18:32 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Michailidou", "Anna-Valentini", ""], ["Gounaris", "Anastasios", ""], ["Tsichlas", "Konstantinos", ""]]}, {"id": "2105.12647", "submitter": "Henderik Alex Proper", "authors": "Henderik A. Proper and Terry A. Halpin", "title": "Conceptual Schema Optimisation -- Database Optimisation before sliding\n  down the Waterfall", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we discuss an approach to database optimisation in which a\nconceptual schema is optimised by applying a sequence of transformations. By\nperforming these optimisations on the conceptual schema, a large part of the\ndatabase optimisation can be done before actually sliding down the software\ndevelopment waterfall. When optimising schemas, one would like to preserve some\nlevel of equivalence between the schemas before and after a transformation. We\ndistinguish between two classes of equivalence, one based on the mathematical\nsemantics of the conceptual schemas, and one on conceptual preference by\nhumans. As a medium for the schema transformations we use the universe of all\n(correct) conceptual schemas. A schema transformation process can then be seen\nas a journey (a schema-time worm) within this universe. The underlying theory\nis conveyed intuitively with sample transformations, and formalised within the\nframework of Object-Role Modelling. A metalanguage is introduced for the\nspecification of transformations, and more importantly their semantics. While\nthe discussion focusses on the data perspective, the approach has a high level\nof generality and is extensible to process and behaviour perspectives.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:06:22 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Proper", "Henderik A.", ""], ["Halpin", "Terry A.", ""]]}, {"id": "2105.13336", "submitter": "Kaixin Zhang", "authors": "Kaixin Zhang, Hongzhi Wang, Tongxin Li, Han Hu, Jiye Qiu, Songling Zou", "title": "TENSILE: A Tensor granularity dynamic GPU memory scheduler method\n  towards multiple dynamic workloads system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has been an area of intense researching. However, as\na kind of computing intensive task, deep learning highly relies on the the\nscale of the GPU memory, which is usually expensive and scarce. Although there\nare some extensive works have been proposed for dynamic GPU memory management,\nthey are hard to be applied to systems with multitasking dynamic workloads,\nsuch as in-database machine learning system.\n  In this paper, we demonstrated TENSILE, a method of managing GPU memory in\ntensor granularity to reduce the GPU memory peak, with taking the multitasking\ndynamic workloads into consideration. As far as we know, TENSILE is the first\nmethod which is designed to manage multiple workloads' GPU memory using. We\nimplement TENSILE on our own deep learning framework, and evaluated its\nperformance. The experiment results shows that our method can achieve less time\noverhead than prior works with more GPU memory saved.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:46:16 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:31:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Kaixin", ""], ["Wang", "Hongzhi", ""], ["Li", "Tongxin", ""], ["Hu", "Han", ""], ["Qiu", "Jiye", ""], ["Zou", "Songling", ""]]}, {"id": "2105.13733", "submitter": "Pavlos Fafalios", "authors": "Pavlos Fafalios, Kostas Petrakis, Georgios Samaritakis, Korina Doerr,\n  Athina Kritsotaki, Yannis Tzitzikas, Martin Doerr", "title": "FAST CAT: Collaborative Data Entry and Curation for Semantic\n  Interoperability in Digital Humanities", "comments": "This is a preprint of an article accepted for publication at the ACM\n  Journal on Computing and Cultural Heritage (JOCCH)", "journal-ref": null, "doi": "10.1145/3461460", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Descriptive and empirical sciences, such as History, are the sciences that\ncollect, observe and describe phenomena in order to explain them and draw\ninterpretative conclusions about influences, driving forces and impacts under\ngiven circumstances. Spreadsheet software and relational database management\nsystems are still the dominant tools for quantitative analysis and overall data\nmanagement in these these sciences, allowing researchers to directly analyse\nthe gathered data and perform scholarly interpretation. However, this current\npractice has a set of limitations, including the high dependency of the\ncollected data on the initial research hypothesis, usually useless for other\nresearch, the lack of representation of the details from which the registered\nrelations are inferred, and the difficulty to revisit the original data sources\nfor verification, corrections or improvements. To cope with these problems, in\nthis paper we present FAST CAT, a collaborative system for assistive data entry\nand curation in Digital Humanities and similar forms of empirical research. We\ndescribe the related challenges, the overall methodology we follow for\nsupporting semantic interoperability, and discuss the use of FAST CAT in the\ncontext of a European (ERC) project of Maritime History, called SeaLiT, which\nexamines economic, social and demographic impacts of the introduction of\nsteamboats in the Mediterranean area between the 1850s and the 1920s.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 10:58:07 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fafalios", "Pavlos", ""], ["Petrakis", "Kostas", ""], ["Samaritakis", "Georgios", ""], ["Doerr", "Korina", ""], ["Kritsotaki", "Athina", ""], ["Tzitzikas", "Yannis", ""], ["Doerr", "Martin", ""]]}, {"id": "2105.14107", "submitter": "Yifan Li", "authors": "Yifan Li, Xiaohui Yu, Nick Koudas", "title": "Data Acquisition for Improving Machine Learning Models", "comments": "Accepted by VLDB 2021", "journal-ref": null, "doi": "10.14778/3467861.3467872", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast advances in Machine Learning over the last ten years have been\npowered by the availability of suitably prepared data for training purposes.\nThe future of ML-enabled enterprise hinges on data. As such, there is already a\nvibrant market offering data annotation services to tailor sophisticated ML\nmodels. In this paper, we present research on the practical problem of\nobtaining data in order to improve the accuracy of ML models. We consider an\nenvironment in which consumers query for data to enhance the accuracy of their\nmodels and data providers who possess data make them available for training\npurposes. We first formalize this interaction process laying out the suitable\nframework and associated parameters for data exchange. We then propose two data\nacquisition strategies that consider a trade-off between exploration during\nwhich we obtain data to learn about the distribution of a provider's data and\nexploitation during which we optimize our data inquiries utilizing the gained\nknowledge. In the first strategy, Estimation and Allocation, we utilize queries\nto estimate the utilities of various predicates while learning about the\ndistribution of the provider's data; then we proceed to the allocation stage in\nwhich we utilize those learned utility estimates to inform our data acquisition\ndecisions. The second algorithmic proposal, named Sequential Predicate\nSelection, utilizes a sampling strategy to explore the distribution of the\nprovider's data, adaptively investing more resources to parts of the data space\nthat are statistically more promising to improve overall model accuracy. We\npresent a detailed experimental evaluation of our proposals utilizing a variety\nof ML models and associated real data sets exploring all applicable parameters\nof interest. We identify trade-offs and highlight the relative benefits of each\nalgorithm to further optimize model accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:06:05 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 13:50:52 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Li", "Yifan", ""], ["Yu", "Xiaohui", ""], ["Koudas", "Nick", ""]]}, {"id": "2105.14307", "submitter": "Neha Makhija", "authors": "Neha Makhija, Wolfgang Gatterbauer", "title": "Towards a Dichotomy for Minimally Factorizing the Provenance of\n  Self-Join Free Conjunctive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the minimal-size factorization of the\nprovenance of self-join-free conjunctive queries, i.e., we want to find an\nequivalent propositional formula that minimizes the number of variable\noccurrences. Our work is partly motivated from probabilistic inference where\nread-once formulas are known to allow exact PTIME solutions and non-read-once\nformulas allow approximate solutions with an error that depends on the number\nof repetitions of variables. We embark on the challenge of characterizing the\ndata complexity of this problem and show its connection to the query resilience\nproblem. While the problem is NP-complete in general, we develop an encoding as\nmax-flow problem that is guaranteed to give the exact solution for several\nqueries (and otherwise approximate minimizations). We show that our encoding is\nguaranteed to return a read-once factorization if it exists. Our problem and\napproach is a complete solution that naturally recovers exact solutions for all\nknown PTIME cases, as well as identifying additional queries for which the\nproblem can be solved in PTIME.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 14:21:38 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Makhija", "Neha", ""], ["Gatterbauer", "Wolfgang", ""]]}, {"id": "2105.14369", "submitter": "Walter Forkel", "authors": "Stefan Borgwardt, Walter Forkel, Alisa Kovtunova", "title": "Temporal Minimal-World Semantics for Sparse ABoxes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology-mediated query answering is a popular paradigm for enriching answers\nto user queries with background knowledge. For querying the absence of\ninformation, however, there exist only few ontology-based approaches. Moreover,\nthese proposals conflate the closed-domain and closed-world assumption, and\ntherefore are not suited to deal with the anonymous objects that are common in\nontological reasoning. Many real-world applications, like processing electronic\nhealth records (EHRs), also contain a temporal dimension, and require efficient\nreasoning algorithms. Moreover, since medical data is not recorded on a regular\nbasis, reasoners must deal with sparse data with potentially large temporal\ngaps. Our contribution consists of two main parts: In the first part we\nintroduce a new closed-world semantics for answering conjunctive queries with\nnegation over ontologies formulated in the description logic ELHb, which is\nbased on the minimal canonical model. We propose a rewriting strategy for\ndealing with negated query atoms, which shows that query answering is possible\nin polynomial time in data complexity. In the second part, we extend this\nminimal-world semantics for answering metric temporal conjunctive queries with\nnegation over the lightweight temporal logic TELHb and obtain similar\nrewritability and complexity results. This paper is under consideration in\nTheory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 20:20:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Borgwardt", "Stefan", ""], ["Forkel", "Walter", ""], ["Kovtunova", "Alisa", ""]]}, {"id": "2105.14435", "submitter": "Remy Wang", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Reinhard Pichler, Dan Suciu, Yisu\n  Remy Wang", "title": "Convergence of Datalog over (Pre-) Semirings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recursive queries have been traditionally studied in the framework of\ndatalog, a language that restricts recursion to monotone queries over sets,\nwhich is guaranteed to converge in polynomial time in the size of the input.\nBut modern big data systems require recursive computations beyond the Boolean\nspace. In this paper we study the convergence of datalog when it is interpreted\nover an arbitrary semiring. We consider an ordered semiring, define the\nsemantics of a datalog program as a least fixpoint in this semiring, and study\nthe number of steps required to reach that fixpoint, if ever. We identify\nalgebraic properties of the semiring that correspond to certain convergence\nproperties of datalog programs. Finally, we describe a class of ordered\nsemirings on which one can use the semi-naive evaluation algorithm on any\ndatalog program.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 06:03:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Pichler", "Reinhard", ""], ["Suciu", "Dan", ""], ["Wang", "Yisu Remy", ""]]}, {"id": "2105.14867", "submitter": "Claudio Hartmann", "authors": "Lars Kegel (1), Claudio Hartmann (1), Maik Thiele (1), Wolfgang Lehner\n  (1) ((1) TU Dresden)", "title": "Accurate and Efficient Time Series Matching by Season- and Trend-aware\n  Symbolic Approximation -- Extended Version Including Additional Evaluation\n  and Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing and analyzing time series data\\-sets have become a central issue\nin many domains requiring data management systems to support time series as a\nnative data type. A crucial prerequisite of these systems is time series\nmatching, which still is a challenging problem. A time series is a\nhigh-dimensional data type, its representation is storage-, and its comparison\nis time-consuming. Among the representation techniques that tackle these\nchallenges, the symbolic aggregate approximation (SAX) is the current state of\nthe art. This technique reduces a time series to a low-dimensional space by\nsegmenting it and discretizing each segment into a small symbolic alphabet.\nHowever, SAX ignores the deterministic behavior of time series such as cyclical\nrepeating patterns or trend component affecting all segments and leading to a\ndistortion of the symbolic distribution. In this paper, we present a season-\nand a trend-aware symbolic approximation. We show that this improves the\nsymbolic distribution and increase the representation accuracy without\nincreasing its memory footprint. Most importantly, this enables a more\nefficient time series matching by providing a match up to three orders of\nmagnitude faster than SAX.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:44:05 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 08:46:20 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kegel", "Lars", "", "TU Dresden"], ["Hartmann", "Claudio", "", "TU Dresden"], ["Thiele", "Maik", "", "TU Dresden"], ["Lehner", "Wolfgang", "", "TU Dresden"]]}]