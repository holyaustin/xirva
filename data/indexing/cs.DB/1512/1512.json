[{"id": "1512.00126", "submitter": "Alex Zhavoronkov", "authors": "Yuri Nikolsky, Roman Gurinovich, Oleg Kuryan, Aleksandr Pashuk, Alexej\n  Scherbakov, Konstantin Romantsov, Leslie C. Jellen, Alex Zhavoronkov", "title": "GrantMed: a new, international system for tracking grants and funding\n  trends in the life sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of PubMed and other search engines in managing the\nmassive volume of biomedical literature and the retrieval of individual\npublications, grant-related data remains scattered and relatively inaccessible.\nThis is problematic, as project and funding data has significant analytical\nvalue and could be integral to publication retrieval. Here, we introduce\nGrantMed, a searchable international database of biomedical grants that\nintegrates some 20 million publications with the nearly 1.4 million research\nprojects and 650 billion dollars of funding that made them possible. For any\ngiven topic in the life sciences, Grantmed provides instantaneous visualization\nof the past 30 years of dollars spent and projects awarded, along with detailed\nindividual project descriptions, funding amounts, and links to investigators,\nresearch organizations, and resulting publications. It summarizes trends in\nfunding and publication rates for areas of interest and merges data from\nvarious national grant databases to create one international grant tracking\nsystem. This information will benefit the research community and funding\nentities alike. Users can view trends over time or current projects underway\nand use this information to navigate the decision-making process in moving\nforward. They can view projects prior to publication and records of previous\nprojects. Convenient access to this data for analytical purposes will be\nbeneficial in many ways, helping to prevent project overlap, reduce funding\nredundancy, identify areas of success, accelerate dissemination of ideas, and\nexpose knowledge gaps in moving forward. It is our hope that this will be a\ncentral resource for international life sciences research communities and the\nfunding organizations that support them, ultimately streamlining progress.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 03:06:47 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Nikolsky", "Yuri", ""], ["Gurinovich", "Roman", ""], ["Kuryan", "Oleg", ""], ["Pashuk", "Aleksandr", ""], ["Scherbakov", "Alexej", ""], ["Romantsov", "Konstantin", ""], ["Jellen", "Leslie C.", ""], ["Zhavoronkov", "Alex", ""]]}, {"id": "1512.00196", "submitter": "Stefan Sch\\\"onig", "authors": "Stefan Sch\\\"onig", "title": "SQL Queries for Declarative Process Mining on Event Logs of Relational\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible business processes can often be modelled more easily using a\ndeclarative rather than a procedural modelling approach. Process mining aims at\nautomating the discovery of business process models. Existing declarative\nprocess mining approaches either suffer performance issues with real-life event\nlogs or limit their expressiveness to a specific set of constaint types.\nLately, with RelationalXES a relational database architecture for storing event\nlog data has been introduced. In this technical report, we introduce a mining\napproach that directly works on relational event data by querying the log with\nconventional SQL. We provide a list of SQL queries for discovering a set of\ncommonly used and mined process constraints.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 09:44:05 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 12:06:50 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Sch\u00f6nig", "Stefan", ""]]}, {"id": "1512.00537", "submitter": "Anja Gruenheid", "authors": "Anja Gruenheid and Besmira Nushi and Tim Kraska and Wolfgang\n  Gatterbauer and Donald Kossmann", "title": "Fault-Tolerant Entity Resolution with the Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, crowdsourcing is increasingly applied as a means to enhance\ndata quality. Although the crowd generates insightful information especially\nfor complex problems such as entity resolution (ER), the output quality of\ncrowd workers is often noisy. That is, workers may unintentionally generate\nfalse or contradicting data even for simple tasks. The challenge that we\naddress in this paper is how to minimize the cost for task requesters while\nmaximizing ER result quality under the assumption of unreliable input from the\ncrowd. For that purpose, we first establish how to deduce a consistent ER\nsolution from noisy worker answers as part of the data interpretation problem.\nWe then focus on the next-crowdsource problem which is to find the next task\nthat maximizes the information gain of the ER result for the minimal additional\ncost. We compare our robust data interpretation strategies to alternative\nstate-of-the-art approaches that do not incorporate the notion of\nfault-tolerance, i.e., the robustness to noise. In our experimental evaluation\nwe show that our approaches yield a quality improvement of at least 20% for two\nreal-world datasets. Furthermore, we examine task-to-worker assignment\nstrategies as well as task parallelization techniques in terms of their cost\nand quality trade-offs in this paper. Based on both synthetic and crowdsourced\ndatasets, we then draw conclusions on how to minimize cost while maintaining\nhigh quality ER results.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 01:03:47 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Gruenheid", "Anja", ""], ["Nushi", "Besmira", ""], ["Kraska", "Tim", ""], ["Gatterbauer", "Wolfgang", ""], ["Kossmann", "Donald", ""]]}, {"id": "1512.00757", "submitter": "Zilong Tan", "authors": "Zilong Tan, Shivnath Babu", "title": "Tempo: Robust and Self-Tuning Resource Management in Multi-tenant\n  Parallel Databases", "comments": "14 pages, 12 figures, 2 tables", "journal-ref": "Proceedings of the VLDB Endowment, Volume 9, Issue 10, June 2016", "doi": "10.14778/2977797.2977799", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-tenant database systems have a component called the Resource Manager,\nor RM that is responsible for allocating resources to tenants. RMs today do not\nprovide direct support for performance objectives such as: \"Average job\nresponse time of tenant A must be less than two minutes\", or \"No more than 5%\nof tenant B's jobs can miss the deadline of 1 hour.\" Thus, DBAs have to tinker\nwith the RM's low-level configuration settings to meet such objectives. We\npropose a framework called Tempo that brings simplicity, self-tuning, and\nrobustness to existing RMs. Tempo provides a simple interface for DBAs to\nspecify performance objectives declaratively, and optimizes the RM\nconfiguration settings to meet these objectives. Tempo has a solid theoretical\nfoundation which gives key robustness guarantees. We report experiments done on\nTempo using production traces of data-processing workloads from companies such\nas Facebook and Cloudera. These experiments demonstrate significant\nimprovements in meeting desired performance objectives over RM configuration\nsettings specified by human experts.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 16:25:35 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Tan", "Zilong", ""], ["Babu", "Shivnath", ""]]}, {"id": "1512.01041", "submitter": "Pietro Codara", "authors": "Stefano Aguzzoli, Pietro Codara, Tommaso Flaminio, Brunella Gerla,\n  Diego Valota", "title": "Querying with {\\L}ukasiewicz logic", "comments": null, "journal-ref": "2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),\n  pp.1-8", "doi": "10.1109/FUZZ-IEEE.2015.7338061", "report-no": null, "categories": "cs.LO cs.AI cs.DB math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present, by way of case studies, a proof of concept, based\non a prototype working on a automotive data set, aimed at showing the potential\nusefulness of using formulas of {\\L}ukasiewicz propositional logic to query\ndatabases in a fuzzy way. Our approach distinguishes itself for its stress on\nthe purely linguistic, contraposed with numeric, formulations of queries. Our\nqueries are expressed in the pure language of logic, and when we use (integer)\nnumbers, these stand for shortenings of formulas on the syntactic level, and\nserve as linguistic hedges on the semantic one. Our case-study queries aim\nfirst at showing that each numeric-threshold fuzzy query is simulated by a\n{\\L}ukasiewicz formula. Then they focus on the expressing power of\n{\\L}ukasiewicz logic which easily allows for updating queries by clauses and\nfor modifying them through a potentially infinite variety of linguistic hedges\nimplemented with a uniform syntactic mechanism. Finally we shall hint how,\nalready at propositional level, {\\L}ukasiewicz natural semantics enjoys a\ndegree of reflection, allowing to write syntactically simple queries that\nsemantically work as meta-queries weighing the contribution of simpler ones.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 11:26:40 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Aguzzoli", "Stefano", ""], ["Codara", "Pietro", ""], ["Flaminio", "Tommaso", ""], ["Gerla", "Brunella", ""], ["Valota", "Diego", ""]]}, {"id": "1512.01479", "submitter": "Marco Guarnieri", "authors": "Marco Guarnieri, Srdjan Marinovic, David Basin", "title": "Strong and Provably Secure Database Access Control", "comments": "A short version of this paper has been published in the proceedings\n  of the 1st IEEE European Symposium on Security and Privacy (EuroS&P 2016)", "journal-ref": null, "doi": "10.1109/EuroSP.2016.23", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing SQL access control mechanisms are extremely limited. Attackers can\nleak information and escalate their privileges using advanced database features\nsuch as views, triggers, and integrity constraints. This is not merely a\nproblem of vendors lagging behind the state-of-the-art. The theoretical\nfoundations for database security lack adequate security definitions and a\nrealistic attacker model, both of which are needed to evaluate the security of\nmodern databases. We address these issues and present a provably secure access\ncontrol mechanism that prevents attacks that defeat popular SQL database\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 16:59:51 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2016 15:36:58 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Guarnieri", "Marco", ""], ["Marinovic", "Srdjan", ""], ["Basin", "David", ""]]}, {"id": "1512.01681", "submitter": "Tomasz Gogacz", "authors": "Tomasz Gogacz, Jerzy Marcinkowski", "title": "Red Spider Meets a Rainworm: Conjunctive Query Finite Determinacy Is\n  Undecidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We solve a well known and long-standing open problem in database theory,\nproving that Conjunctive Query Finite Determinacy Problem is undecidable. The\ntechnique we use builds on the top of our Red Spider method which we developed\nin our paper [GM15] to show undecidability of the same problem in the\n\"unrestricted case\" -- when database instances are allowed to be infinite. We\nalso show a specific instance $Q_0$, ${\\cal Q}= \\{Q_1, Q_2, \\ldots Q_k\\}$ such\nthat the set $\\cal Q$ of CQs does not determine CQ $Q_0$ but finitely\ndetermines it. Finally, we claim that while $Q_0$ is finitely determined by\n$\\cal Q$, there is no FO-rewriting of $Q_0$, with respect to $\\cal Q$, and we\noutline a proof of this claim\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 15:54:19 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Gogacz", "Tomasz", ""], ["Marcinkowski", "Jerzy", ""]]}, {"id": "1512.01808", "submitter": "Tomasz Gogacz", "authors": "Tomasz Gogacz, Szymon Toru\\'nczyk", "title": "Entropy bounds for conjunctive queries with functional dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of finding the worst-case bound for the size of the\nresult $Q(\\mathbb{ D})$ of a fixed conjunctive query $Q$ applied to a database\n$\\mathbb{ D}$ satisfying given functional dependencies. We provide a precise\ncharacterization of this bound in terms of entropy vectors, and in terms of\nfinite groups. In particular, we show that an upper bound provided by Gottlob,\nLee, Valiant and Valiant is tight, answering a question from their paper. Our\nresult generalizes the bound due to Atserias, Grohe and Marx, who consider the\ncase without functional dependencies. Our result shows that the problem of\ncomputing the worst-case size bound, in the general case, is closely related to\ndifficult problems from information theory.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 17:15:38 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Gogacz", "Tomasz", ""], ["Toru\u0144czyk", "Szymon", ""]]}, {"id": "1512.02138", "submitter": "Abolfazl Asudeh", "authors": "Abolfazl Asudeh, Saravanan Thirumuruganathan, Nan Zhang, Gautam Das", "title": "Discovering the Skyline of Web Databases", "comments": null, "journal-ref": "The Proceedings of the VLDB Endowment (PVLDB) 2016, Vol 9, No. 7,\n  pages 600 - 611", "doi": "10.14778/2983200.2983205", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many web databases are \"hidden\" behind proprietary search interfaces that\nenforce the top-$k$ output constraint, i.e., each query returns at most $k$ of\nall matching tuples, preferentially selected and returned according to a\nproprietary ranking function. In this paper, we initiate research into the\nnovel problem of skyline discovery over top-$k$ hidden web databases. Since\nskyline tuples provide critical insights into the database and include the\ntop-ranked tuple for every possible ranking function following the monotonic\norder of attribute values, skyline discovery from a hidden web database can\nenable a wide variety of innovative third-party applications over one or\nmultiple web databases. Our research in the paper shows that the critical\nfactor affecting the cost of skyline discovery is the type of search interface\ncontrols provided by the website. As such, we develop efficient algorithms for\nthree most popular types, i.e., one-ended range, free range and point\npredicates, and then combine them to support web databases that feature a\nmixture of these types. Rigorous theoretical analysis and extensive real-world\nonline and offline experiments demonstrate the effectiveness of our proposed\ntechniques and their superiority over baseline solutions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 17:41:20 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 19:25:40 GMT"}, {"version": "v3", "created": "Sun, 20 Mar 2016 20:06:21 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Asudeh", "Abolfazl", ""], ["Thirumuruganathan", "Saravanan", ""], ["Zhang", "Nan", ""], ["Das", "Gautam", ""]]}, {"id": "1512.02568", "submitter": "Tarun Kathuria", "authors": "Tarun Kathuria, S. Sudarshan", "title": "Efficient and Provable Multi-Query Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex queries for massive data analysis jobs have become increasingly\ncommonplace. Many such queries contain com- mon subexpressions, either within a\nsingle query or among multiple queries submitted as a batch. Conventional query\noptimizers do not exploit these subexpressions and produce sub-optimal plans.\nThe problem of multi-query optimization (MQO) is to generate an optimal\ncombined evaluation plan by computing common subexpressions once and reusing\nthem. Exhaustive algorithms for MQO explore an O(n^n) search space. Thus, this\nproblem has primarily been tackled using various heuristic algorithms, without\nproviding any theoretical guarantees on the quality of their solution. In this\npaper, instead of the conventional cost minimization problem, we treat the\nproblem as maximizing a linear transformation of the cost function. We propose\na greedy algorithm for this transformed formulation of the problem, which under\nweak, intuitive assumptions, provides an approximation factor guarantee for\nthis formulation. We go on to show that this factor is optimal, unless P = NP.\nAnother noteworthy point about our algorithm is that it can be easily\nincorporated into existing transformation-based optimizers. We finally propose\noptimizations which can be used to improve the efficiency of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 17:56:26 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 18:18:08 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Kathuria", "Tarun", ""], ["Sudarshan", "S.", ""]]}, {"id": "1512.02714", "submitter": "Rong Zhu", "authors": "Rong Zhu, Zhaonian Zou, Jianzhong Li", "title": "SimRank Computation on Uncertain Graphs", "comments": "14 pages, under review by ICDE'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SimRank is a similarity measure between vertices in a graph, which has become\na fundamental technique in graph analytics. Recently, many algorithms have been\nproposed for efficient evaluation of SimRank similarities. However, the\nexisting SimRank computation algorithms either overlook uncertainty in graph\nstructures or is based on an unreasonable assumption (Du et al). In this paper,\nwe study SimRank similarities on uncertain graphs based on the possible world\nmodel of uncertain graphs. Following the random-walk-based formulation of\nSimRank on deterministic graphs and the possible worlds model of uncertain\ngraphs, we define random walks on uncertain graphs for the first time and show\nthat our definition of random walks satisfies Markov's property. We formulate\nthe SimRank measure based on random walks on uncertain graphs. We discover a\ncritical difference between random walks on uncertain graphs and random walks\non deterministic graphs, which makes all existing SimRank computation\nalgorithms on deterministic graphs inapplicable to uncertain graphs. To\nefficiently compute SimRank similarities, we propose three algorithms, namely\nthe baseline algorithm with high accuracy, the sampling algorithm with high\nefficiency, and the two-phase algorithm with comparable efficiency as the\nsampling algorithm and about an order of magnitude smaller relative error than\nthe sampling algorithm. The extensive experiments and case studies verify the\neffectiveness of our SimRank measure and the efficiency of our SimRank\ncomputation algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 01:32:04 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Zhu", "Rong", ""], ["Zou", "Zhaonian", ""], ["Li", "Jianzhong", ""]]}, {"id": "1512.03207", "submitter": "Carl Friedrich Bolz", "authors": "Carl Friedrich Bolz, Darya Kurilova, Laurence Tratt", "title": "Making an Embedded DBMS JIT-friendly", "comments": "24 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While database management systems (DBMSs) are highly optimized, interactions\nacross the boundary between the programming language (PL) and the DBMS are\ncostly, even for in-process embedded DBMSs. In this paper, we show that\nprograms that interact with the popular embedded DBMS SQLite can be\nsignificantly optimized - by a factor of 3.4 in our benchmarks - by inlining\nacross the PL / DBMS boundary. We achieved this speed-up by replacing parts of\nSQLite's C interpreter with RPython code and composing the resulting\nmeta-tracing virtual machine (VM) - called SQPyte - with the PyPy VM. SQPyte\ndoes not compromise stand-alone SQL performance and is 2.2% faster than SQLite\non the widely used TPC-H benchmark suite.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 10:57:56 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 15:35:58 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 09:03:22 GMT"}, {"version": "v4", "created": "Mon, 20 Jun 2016 05:35:28 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Bolz", "Carl Friedrich", ""], ["Kurilova", "Darya", ""], ["Tratt", "Laurence", ""]]}, {"id": "1512.03501", "submitter": "Marian-Andrei Rizoiu", "authors": "Marian-Andrei Rizoiu, Julien Velcin, St\\'ephane Bonnevay, St\\'ephane\n  Lallich", "title": "ClusPath: A Temporal-driven Clustering to Infer Typical Evolution Paths", "comments": null, "journal-ref": null, "doi": "10.1007/s10618-015-0445-7", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ClusPath, a novel algorithm for detecting general evolution\ntendencies in a population of entities. We show how abstract notions, such as\nthe Swedish socio-economical model (in a political dataset) or the companies\nfiscal optimization (in an economical dataset) can be inferred from low-level\ndescriptive features. Such high-level regularities in the evolution of entities\nare detected by combining spatial and temporal features into a spatio-temporal\ndissimilarity measure and using semi-supervised clustering techniques. The\nrelations between the evolution phases are modeled using a graph structure,\ninferred simultaneously with the partition, by using a \"slow changing world\"\nassumption. The idea is to ensure a smooth passage for entities along their\nevolution paths, which catches the long-term trends in the dataset.\nAdditionally, we also provide a method, based on an evolutionary algorithm, to\ntune the parameters of ClusPath to new, unseen datasets. This method assesses\nthe fitness of a solution using four opposed quality measures and proposes a\nbalanced compromise.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 01:32:20 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Rizoiu", "Marian-Andrei", ""], ["Velcin", "Julien", ""], ["Bonnevay", "St\u00e9phane", ""], ["Lallich", "St\u00e9phane", ""]]}, {"id": "1512.03564", "submitter": "Matteo Brucato", "authors": "Matteo Brucato, Juan Felipe Beltran, Azza Abouzied, Alexandra Meliou", "title": "Scalable Package Queries in Relational Database Systems", "comments": "Extended version of PVLDB 2016 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional database queries follow a simple model: they define constraints\nthat each tuple in the result must satisfy. This model is computationally\nefficient, as the database system can evaluate the query conditions on each\ntuple individually. However, many practical, real-world problems require a\ncollection of result tuples to satisfy constraints collectively, rather than\nindividually. In this paper, we present package queries, a new query model that\nextends traditional database queries to handle complex constraints and\npreferences over answer sets. We develop a full-fledged package query system,\nimplemented on top of a traditional database engine. Our work makes several\ncontributions. First, we design PaQL, a SQL-based query language that supports\nthe declarative specification of package queries. We prove that PaQL is as\nleast as expressive as integer linear programming, and therefore, evaluation of\npackage queries is in general NP-hard. Second, we present a fundamental\nevaluation strategy that combines the capabilities of databases and constraint\noptimization solvers to derive solutions to package queries. The core of our\napproach is a set of translation rules that transform a package query to an\ninteger linear program. Third, we introduce an offline data partitioning\nstrategy allowing query evaluation to scale to large data sizes. Fourth, we\nintroduce SketchRefine, a scalable algorithm for package evaluation, with\nstrong approximation guarantees ($(1 \\pm\\epsilon)^6$-factor approximation).\nFinally, we present extensive experiments over real-world and benchmark data.\nThe results demonstrate that SketchRefine is effective at deriving high-quality\npackage results, and achieves runtime performance that is an order of magnitude\nfaster than directly using ILP solvers over large datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 09:47:43 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 00:53:52 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Brucato", "Matteo", ""], ["Beltran", "Juan Felipe", ""], ["Abouzied", "Azza", ""], ["Meliou", "Alexandra", ""]]}, {"id": "1512.03880", "submitter": "Jinyang  Gao", "authors": "Jinyang Gao, H.V.Jagadish, Beng Chin Ooi", "title": "Active Sampler: Light-weight Accelerator for Complex Data Analytics at\n  Scale", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed amazing outcomes from \"Big Models\" trained by\n\"Big Data\". Most popular algorithms for model training are iterative. Due to\nthe surging volumes of data, we can usually afford to process only a fraction\nof the training data in each iteration. Typically, the data are either\nuniformly sampled or sequentially accessed.\n  In this paper, we study how the data access pattern can affect model\ntraining. We propose an Active Sampler algorithm, where training data with more\n\"learning value\" to the model are sampled more frequently. The goal is to focus\ntraining effort on valuable instances near the classification boundaries,\nrather than evident cases, noisy data or outliers. We show the correctness and\noptimality of Active Sampler in theory, and then develop a light-weight\nvectorized implementation. Active Sampler is orthogonal to most approaches\noptimizing the efficiency of large-scale data analytics, and can be applied to\nmost analytics models trained by stochastic gradient descent (SGD) algorithm.\nExtensive experimental evaluations demonstrate that Active Sampler can speed up\nthe training procedure of SVM, feature selection and deep learning, for\ncomparable training quality by 1.6-2.2x.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 06:32:33 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Gao", "Jinyang", ""], ["Jagadish", "H. V.", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1512.03899", "submitter": "Mathew Joseph", "authors": "Mathew Joseph, Gabriel Kuper, Till Mossakowski, Luciano Serafini", "title": "Query Answering over Contextualized RDF/OWL Knowledge with\n  Forall-Existential Bridge Rules: Decidable Finite Extension Classes (Post\n  Print)", "comments": null, "journal-ref": "Semantic Web (IOS Press) Vol 7:1 Pages 25-61. 2016", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of contextualized knowledge in the Semantic Web (SW) has\nled to the popularity of knowledge formats such as \\emph{quads} in the SW\ncommunity. A quad is an extension of an RDF triple with contextual information\nof the triple. In this paper, we study the problem of query answering over\nquads augmented with forall-existential bridge rules that enable\ninteroperability of reasoning between triples in various contexts. We call a\nset of quads together with such expressive bridge rules, a quad-system. Query\nanswering over quad-systems is undecidable, in general. We derive decidable\nclasses of quad-systems, for which query answering can be done using forward\nchaining. Sound, complete and terminating procedures, which are adaptations of\nthe well known chase algorithm, are provided for these classes for deciding\nquery entailment. Safe, msafe, and csafe class of quad-systems restrict the\nstructure of blank nodes generated during the chase computation process to be\ndirected acyclic graphs (DAGs) of bounded depth. RR and restricted RR classes\ndo not allow the generation of blank nodes during the chase computation\nprocess. Both data and combined complexity of query entailment has been\nestablished for the classes derived. We further show that quad-systems are\nequivalent to forall-existential rules whose predicates are restricted to\nternary arity, modulo polynomial time translations. We subsequently show that\nthe technique of safety, strictly subsumes in expressivity, some of the well\nknown and expressive techniques, such as joint acyclicity and model faithful\nacyclicity, used for decidability guarantees in the realm of forall-existential\nrules.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 09:56:38 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Joseph", "Mathew", ""], ["Kuper", "Gabriel", ""], ["Mossakowski", "Till", ""], ["Serafini", "Luciano", ""]]}, {"id": "1512.03921", "submitter": "Nikolaos Stasinopoulos", "authors": "Foto Afrati, Nikos Stasinopoulos, Jeffrey D. Ullman, Angelos\n  Vassilakopoulos", "title": "SharesSkew: An Algorithm to Handle Skew for Joins in MapReduce", "comments": null, "journal-ref": null, "doi": "10.1016/j.is.2018.06.005", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of computing a multiway join in one\nround of MapReduce when the data may be skewed. We optimize on communication\ncost, i.e., the amount of data that is transferred from the mappers to the\nreducers. We identify join attributes values that appear very frequently, Heavy\nHitters (HH). We distribute HH valued records to reducers avoiding skew by\nusing an adaptation of the Shares~\\cite{AfUl} algorithm to achieve minimum\ncommunication cost. Our algorithm is implemented for experimentation and is\noffered as open source software. Furthermore, we investigate a class of\nmultiway joins for which a simpler variant of the algorithm can handle skew. We\noffer closed forms for computing the parameters of the algorithm for chain and\nsymmetric joins.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 15:31:37 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Afrati", "Foto", ""], ["Stasinopoulos", "Nikos", ""], ["Ullman", "Jeffrey D.", ""], ["Vassilakopoulos", "Angelos", ""]]}, {"id": "1512.04009", "submitter": "Shenggang Ying", "authors": "Shenggang Ying, Mingsheng Ying and Yuan Feng", "title": "Quantum Privacy-Preserving Data Mining", "comments": "5 pages. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is a key technology in big data analytics and it can discover\nunderstandable knowledge (patterns) hidden in large data sets. Association rule\nis one of the most useful knowledge patterns, and a large number of algorithms\nhave been developed in the data mining literature to generate association rules\ncorresponding to different problems and situations. Privacy becomes a vital\nissue when data mining is used to sensitive data sets like medical records,\ncommercial data sets and national security. In this Letter, we present a\nquantum protocol for mining association rules on vertically partitioned\ndatabases. The quantum protocol can improve the privacy level preserved by\nknown classical protocols and at the same time it can exponentially reduce the\ncomputational complexity and communication cost.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 06:26:56 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 09:13:35 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Ying", "Shenggang", ""], ["Ying", "Mingsheng", ""], ["Feng", "Yuan", ""]]}, {"id": "1512.04817", "submitter": "Michael Hay", "authors": "Michael Hay, Ashwin Machanavajjhala, Gerome Miklau, Yan Chen, and Dan\n  Zhang", "title": "Principled Evaluation of Differentially Private Algorithms using DPBench", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy has become the dominant standard in the research\ncommunity for strong privacy protection. There has been a flood of research\ninto query answering algorithms that meet this standard. Algorithms are\nbecoming increasingly complex, and in particular, the performance of many\nemerging algorithms is {\\em data dependent}, meaning the distribution of the\nnoise added to query answers may change depending on the input data.\nTheoretical analysis typically only considers the worst case, making empirical\nstudy of average case performance increasingly important.\n  In this paper we propose a set of evaluation principles which we argue are\nessential for sound evaluation. Based on these principles we propose DPBench, a\nnovel evaluation framework for standardized evaluation of privacy algorithms.\nWe then apply our benchmark to evaluate algorithms for answering 1- and\n2-dimensional range queries. The result is a thorough empirical study of 15\npublished algorithms on a total of 27 datasets that offers new insights into\nalgorithm behavior---in particular the influence of dataset scale and\nshape---and a more complete characterization of the state of the art. Our\nmethodology is able to resolve inconsistencies in prior empirical studies and\nplace algorithm performance in context through comparison to simple baselines.\nFinally, we pose open research questions which we hope will guide future\nalgorithm design.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 15:29:36 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Hay", "Michael", ""], ["Machanavajjhala", "Ashwin", ""], ["Miklau", "Gerome", ""], ["Chen", "Yan", ""], ["Zhang", "Dan", ""]]}, {"id": "1512.04973", "submitter": "Ndapandula Nakashole", "authors": "Ndapandula Nakashole", "title": "An Operator for Entity Extraction in MapReduce", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary-based entity extraction involves finding mentions of dictionary\nentities in text. Text mentions are often noisy, containing spurious or missing\nwords. Efficient algorithms for detecting approximate entity mentions follow\none of two general techniques. The first approach is to build an index on the\nentities and perform index lookups of document substrings. The second approach\nrecognizes that the number of substrings generated from documents can explode\nto large numbers, to get around this, they use a filter to prune many such\nsubstrings which do not match any dictionary entity and then only verify the\nremaining substrings if they are entity mentions of dictionary entities, by\nmeans of a text join. The choice between the index-based approach and the\nfilter & verification-based approach is a case-to-case decision as the best\napproach depends on the characteristics of the input entity dictionary, for\nexample frequency of entity mentions. Choosing the right approach for the\nsetting can make a substantial difference in execution time. Making this choice\nis however non-trivial as there are parameters within each of the approaches\nthat make the space of possible approaches very large. In this paper, we\npresent a cost-based operator for making the choice among execution plans for\nentity extraction. Since we need to deal with large dictionaries and even\nlarger large datasets, our operator is developed for implementations of\nMapReduce distributed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 21:23:20 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Nakashole", "Ndapandula", ""]]}, {"id": "1512.05511", "submitter": "Nils Vortmeier", "authors": "Pablo Mu\\~noz and Nils Vortmeier and Thomas Zeume", "title": "Dynamic Graph Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph databases in many applications---semantic web, transport or biological\nnetworks among others---are not only large, but also frequently modified.\nEvaluating graph queries in this dynamic context is a challenging task, as\nthose queries often combine first-order and navigational features.\n  Motivated by recent results on maintaining dynamic reachability, we study the\ndynamic evaluation of traditional query languages for graphs in the descriptive\ncomplexity framework. Our focus is on maintaining regular path queries, and\nextensions thereof, by first-order formulas. In particular we are interested in\npath queries defined by non-regular languages and in extended conjunctive\nregular path queries (which allow to compare labels of paths based on word\nrelations). Further we study the closely related problems of maintaining\ndistances in graphs and reachability in product graphs.\n  In this preliminary study we obtain upper bounds for those problems in\nrestricted settings, such as undirected and acyclic graphs, or under insertions\nonly, and negative results regarding quantifier-free update formulas. In\naddition we point out interesting directions for further research.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 09:46:42 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Mu\u00f1oz", "Pablo", ""], ["Vortmeier", "Nils", ""], ["Zeume", "Thomas", ""]]}, {"id": "1512.05685", "submitter": "Johann Schaible", "authors": "Johann Schaible and Thomas Gottron and Ansgar Scherp", "title": "TermPicker: Enabling the Reuse of Vocabulary Terms by Exploiting Data\n  from the Linked Open Data Cloud - An Extended Technical Report", "comments": "17 pages, 3 figures, extended technical report for a Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding which vocabulary terms to use when modeling data as Linked Open Data\n(LOD) is far from trivial. Choosing too general vocabulary terms, or terms from\nvocabularies that are not used by other LOD datasets, is likely to lead to a\ndata representation, which will be harder to understand by humans and to be\nconsumed by Linked data applications. In this technical report, we propose\nTermPicker: a novel approach for vocabulary reuse by recommending RDF types and\nproperties based on exploiting the information on how other data providers on\nthe LOD cloud use RDF types and properties to describe their data. To this end,\nwe introduce the notion of so-called schema-level patterns (SLPs). They capture\nhow sets of RDF types are connected via sets of properties within some data\ncollection, e.g., within a dataset on the LOD cloud. TermPicker uses such SLPs\nand generates a ranked list of vocabulary terms for reuse. The lists of\nrecommended terms are ordered by a ranking model which is computed using the\nmachine learning approach Learning To Rank (L2R). TermPicker is evaluated based\non the recommendation quality that is measured using the Mean Average Precision\n(MAP) and the Mean Reciprocal Rank at the first five positions (MRR@5). Our\nresults illustrate an improvement of the recommendation quality by 29% - 36%\nwhen using SLPs compared to the beforehand investigated baselines of\nrecommending solely popular vocabulary terms or terms from the same vocabulary.\nThe overall best results are achieved using SLPs in conjunction with the\nLearning To Rank algorithm Random Forests.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 17:37:56 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 22:00:10 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Schaible", "Johann", ""], ["Gottron", "Thomas", ""], ["Scherp", "Ansgar", ""]]}, {"id": "1512.06080", "submitter": "Lorena Etcheverry", "authors": "Lorena Etcheverry and Silvia Silvia Gomez and Alejandro Vaisman", "title": "Modeling and Querying Data Cubes on the Semantic Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web is changing the way in which data warehouses are designed, used, and\nqueried. With the advent of initiatives such as Open Data and Open Government,\norganizations want to share their multidimensional data cubes and make them\navailable to be queried online. The RDF data cube vocabulary (QB), the W3C\nstandard to publish statistical data in RDF, presents several limitations to\nfully support the multidimensional model. The QB4OLAP vocabulary extends QB to\novercome these limitations, allowing to im- plement the typical OLAP\noperations, such as rollup, slice, dice, and drill-across using standard SPARQL\nqueries. In this paper we introduce a formal data model where the main object\nis the data cube, and define OLAP operations using this model, independent of\nthe underlying representation of the cube. We show then that a cube expressed\nusing our model can be represented using the QB4OLAP vocabulary, and finally we\nprovide a SPARQL implementation of OLAP operations over data cubes in QB4OLAP.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 19:01:36 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Etcheverry", "Lorena", ""], ["Gomez", "Silvia Silvia", ""], ["Vaisman", "Alejandro", ""]]}, {"id": "1512.06143", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Sanjeev Khanna, Yang Li, Val Tannen", "title": "Algorithms for Provisioning Queries and Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provisioning is a technique for avoiding repeated expensive computations in\nwhat-if analysis. Given a query, an analyst formulates $k$ hypotheticals, each\nretaining some of the tuples of a database instance, possibly overlapping, and\nshe wishes to answer the query under scenarios, where a scenario is defined by\na subset of the hypotheticals that are \"turned on\". We say that a query admits\ncompact provisioning if given any database instance and any $k$ hypotheticals,\none can create a poly-size (in $k$) sketch that can then be used to answer the\nquery under any of the $2^{k}$ possible scenarios without accessing the\noriginal instance.\n  In this paper, we focus on provisioning complex queries that combine\nrelational algebra (the logical component), grouping, and statistics/analytics\n(the numerical component). We first show that queries that compute quantiles or\nlinear regression (as well as simpler queries that compute count and\nsum/average of positive values) can be compactly provisioned to provide\n(multiplicative) approximate answers to an arbitrary precision. In contrast,\nexact provisioning for each of these statistics requires the sketch size to be\nexponential in $k$. We then establish that for any complex query whose logical\ncomponent is a positive relational algebra query, as long as the numerical\ncomponent can be compactly provisioned, the complex query itself can be\ncompactly provisioned. On the other hand, introducing negation or recursion in\nthe logical component again requires the sketch size to be exponential in $k$.\nWhile our positive results use algorithms that do not access the original\ninstance after a scenario is known, we prove our lower bounds even for the case\nwhen, knowing the scenario, limited access to the instance is allowed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 21:29:58 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Assadi", "Sepehr", ""], ["Khanna", "Sanjeev", ""], ["Li", "Yang", ""], ["Tannen", "Val", ""]]}, {"id": "1512.06168", "submitter": "Jose Faleiro", "authors": "Kun Ren, Jose M. Faleiro, Daniel J. Abadi", "title": "Design Principles for Scaling Multi-core OLTP Under High Contention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although significant recent progress has been made in improving the\nmulti-core scalability of high throughput transactional database systems,\nmodern systems still fail to achieve scalable throughput for workloads\ninvolving frequent access to highly contended data. Most of this inability to\nachieve high throughput is explained by the fundamental constraints involved in\nguaranteeing ACID --- the addition of cores results in more concurrent\ntransactions accessing the same contended data for which access must be\nserialized in order to guarantee isolation. Thus, linear scalability for\ncontended workloads is impossible. However, there exist flaws in many modern\narchitectures that exacerbate their poor scalability, and result in throughput\nthat is much worse than fundamentally required by the workload.\n  In this paper we identify two prevalent design principles that limit the\nmulti-core scalability of many (but not all) transactional database systems on\ncontended workloads: the multi-purpose nature of execution threads in these\nsystems, and the lack of advanced planning of data access. We demonstrate the\ndeleterious results of these design principles by implementing a prototype\nsystem, ORTHRUS, that is motivated by the principles of separation of database\ncomponent functionality and advanced planning of transactions. We find that\nthese two principles alone result in significantly improved scalability on\nhigh-contention workloads, and an order of magnitude increase in throughput for\na non-trivial subset of these contended workloads.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 00:24:59 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2016 19:00:15 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2016 16:41:08 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Ren", "Kun", ""], ["Faleiro", "Jose M.", ""], ["Abadi", "Daniel J.", ""]]}, {"id": "1512.06246", "submitter": "Gaetano Geck", "authors": "Gaetano Geck, Bas Ketsman, Frank Neven, Thomas Schwentick", "title": "Parallel-Correctness and Containment for Conjunctive Queries with Union\n  and Negation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-round multiway join algorithms first reshuffle data over many servers\nand then evaluate the query at hand in a parallel and communication-free way. A\nkey question is whether a given distribution policy for the reshuffle is\nadequate for computing a given query, also referred to as parallel-correctness.\nThis paper extends the study of the complexity of parallel-correctness and its\nconstituents, parallel-soundness and parallel-completeness, to unions of\nconjunctive queries with and without negation. As a by-product it is shown that\nthe containment problem for conjunctive queries with negation is\ncoNEXPTIME-complete.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 13:18:24 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Geck", "Gaetano", ""], ["Ketsman", "Bas", ""], ["Neven", "Frank", ""], ["Schwentick", "Thomas", ""]]}, {"id": "1512.06388", "submitter": "Xi Wu", "authors": "Xi Wu, Matthew Fredrikson, Wentao Wu, Somesh Jha, Jeffrey F. Naughton", "title": "Revisiting Differentially Private Regression: Lessons From Learning\n  Theory and their Consequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private regression has received attention from both database and security\ncommunities. Recent work by Fredrikson et al. (USENIX Security 2014) analyzed\nthe functional mechanism (Zhang et al. VLDB 2012) for training linear\nregression models over medical data. Unfortunately, they found that model\naccuracy is already unacceptable with differential privacy when $\\varepsilon =\n5$. We address this issue, presenting an explicit connection between\ndifferential privacy and stable learning theory through which a substantially\nbetter privacy/utility tradeoff can be obtained. Perhaps more importantly, our\ntheory reveals that the most basic mechanism in differential privacy, output\nperturbation, can be used to obtain a better tradeoff for all\nconvex-Lipschitz-bounded learning tasks. Since output perturbation is simple to\nimplement, it means that our approach is potentially widely applicable in\npractice. We go on to apply it on the same medical data as used by Fredrikson\net al. Encouragingly, we achieve accurate models even for $\\varepsilon = 0.1$.\nIn the last part of this paper, we study the impact of our improved\ndifferentially private mechanisms on model inversion attacks, a privacy attack\nintroduced by Fredrikson et al. We observe that the improved tradeoff makes the\nresulting differentially private model more susceptible to inversion attacks.\nWe analyze this phenomenon formally.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 15:53:29 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Wu", "Xi", ""], ["Fredrikson", "Matthew", ""], ["Wu", "Wentao", ""], ["Jha", "Somesh", ""], ["Naughton", "Jeffrey F.", ""]]}, {"id": "1512.06395", "submitter": "Jaroslaw Szlichta", "authors": "Mehdi Kargar, Lukasz Golab, Jaroslaw Szlichta", "title": "Effective Keyword Search in Graphs", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a node-labeled graph, keyword search finds subtrees of the graph whose\nnodes contain all of the query keywords. This provides a way to query graph\ndatabases that neither requires mastery of a query language such as SPARQL, nor\na deep knowledge of the database schema. Previous work ranks answer trees using\ncombinations of structural and content-based metrics, such as path lengths\nbetween keywords or relevance of the labels in the answer tree to the query\nkeywords. We propose two new ways to rank keyword search results over graphs.\nThe first takes node importance into account while the second is a bi-objective\noptimization of edge weights and node importance. Since both of these problems\nare NP-hard, we propose greedy algorithms to solve them, and experimentally\nverify their effectiveness and efficiency on a real dataset.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 16:20:17 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 16:59:38 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2016 19:14:03 GMT"}, {"version": "v4", "created": "Wed, 23 Mar 2016 19:54:12 GMT"}, {"version": "v5", "created": "Tue, 29 Mar 2016 15:43:11 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Kargar", "Mehdi", ""], ["Golab", "Lukasz", ""], ["Szlichta", "Jaroslaw", ""]]}, {"id": "1512.06423", "submitter": "Witold Litwin", "authors": "Sushil Jajodia, Witold Litwin and Thomas Schwarz", "title": "On-the fly AES Decryption/Encryption for Cloud SQL Databases", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": "Lamsade Res. Report 06-15-2015", "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the client-side AES256 encryption for a cloud SQL DB. A column\nciphertext is deterministic or probabilistic. We trust the cloud DBMS for\nsecurity of its run-time values, e.g., through a moving target defense. The\nclient may send AES key(s) with the query. These serve the on-the-fly\ndecryption of selected ciphertext into plaintext for query evaluation. The DBMS\nclears the key(s) and the plaintext at the query end at latest. It may deliver\nciphertext to decryption enabled clients or plaintext otherwise, e.g., to\nbrowsers/navigators. The scheme functionally offers to a cloud DBMS\ncapabilities of a plaintext SQL DBMS. AES processing overhead appears\nnegligible for a modern CPU, e.g., a popular Intel I5. The determin-istic\nencryption may have no storage overhead. The probabilistic one doubles the DB\nstorage. The scheme seems the first generally practical for an outsourced\nencrypted SQL DB. An implementation sufficient to practice with appears easy.\nAn existing cloud SQL DBMS with UDF support should do.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 19:49:50 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Jajodia", "Sushil", ""], ["Litwin", "Witold", ""], ["Schwarz", "Thomas", ""]]}, {"id": "1512.06474", "submitter": "Theodoros Rekatsinas", "authors": "Manas Joglekar and Theodoros Rekatsinas and Hector Garcia-Molina and\n  Aditya Parameswaran and Christopher R\\'e", "title": "SLiMFast: Guaranteed Results for Data Fusion and Source Reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on data fusion, i.e., the problem of unifying conflicting data from\ndata sources into a single representation by estimating the source accuracies.\nWe propose SLiMFast, a framework that expresses data fusion as a statistical\nlearning problem over discriminative probabilistic models, which in many cases\ncorrespond to logistic regression. In contrast to previous approaches that use\ncomplex generative models, discriminative models make fewer distributional\nassumptions over data sources and allow us to obtain rigorous theoretical\nguarantees. Furthermore, we show how SLiMFast enables incorporating domain\nknowledge into data fusion, yielding accuracy improvements of up to 50\\% over\nstate-of-the-art baselines. Building upon our theoretical results, we design an\noptimizer that obviates the need for users to manually select an algorithm for\nlearning SLiMFast's parameters. We validate our optimizer on multiple\nreal-world datasets and show that it can accurately predict the learning\nalgorithm that yields the best data fusion results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 02:28:17 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 22:55:37 GMT"}, {"version": "v3", "created": "Sat, 12 Nov 2016 17:33:47 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Joglekar", "Manas", ""], ["Rekatsinas", "Theodoros", ""], ["Garcia-Molina", "Hector", ""], ["Parameswaran", "Aditya", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1512.06635", "submitter": "Konstantin Golenberg", "authors": "Konstantin Golenberg and Yehoshua Sagiv", "title": "A Practically Efficient Algorithm for Generating Answers to Keyword\n  Search over Data Graphs", "comments": "Full version of ICDT'16 paper", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2016.23", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In keyword search over a data graph, an answer is a non-redundant subtree\nthat contains all the keywords of the query. A naive approach to producing all\nthe answers by increasing height is to generalize Dijkstra's algorithm to\nenumerating all acyclic paths by increasing weight. The idea of freezing is\nintroduced so that (most) non-shortest paths are generated only if they are\nactually needed for producing answers. The resulting algorithm for generating\nsubtrees, called GTF, is subtle and its proof of correctness is intricate.\nExtensive experiments show that GTF outperforms existing systems, even ones\nthat for efficiency's sake are incomplete (i.e., cannot produce all the\nanswers). In particular, GTF is scalable and performs well even on large data\ngraphs and when many answers are needed.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 14:14:54 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Golenberg", "Konstantin", ""], ["Sagiv", "Yehoshua", ""]]}, {"id": "1512.06912", "submitter": "Ognjen Savkovic", "authors": "Ognjen Savkovic, Elisa Marengo, Werner Nutt", "title": "Query Stability in Monotonic Data-Aware Business Processes [Extended\n  Version]", "comments": "This report is the extended version of a paper accepted at the 19th\n  International Conference on Database Theory (ICDT 2016), March 15-18, 2016 -\n  Bordeaux, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organizations continuously accumulate data, often according to some business\nprocesses. If one poses a query over such data for decision support, it is\nimportant to know whether the query is stable, that is, whether the answers\nwill stay the same or may change in the future because business processes may\nadd further data. We investigate query stability for conjunctive queries. To\nthis end, we define a formalism that combines an explicit representation of the\ncontrol flow of a process with a specification of how data is read and inserted\ninto the database. We consider different restrictions of the process model and\nthe state of the system, such as negation in conditions, cyclic executions,\nread access to written data, presence of pending process instances, and the\npossibility to start fresh process instances. We identify for which facet\ncombinations stability of conjunctive queries is decidable and provide\nencodings into variants of Datalog that are optimal with respect to the\nworst-case complexity of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 23:45:41 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Savkovic", "Ognjen", ""], ["Marengo", "Elisa", ""], ["Nutt", "Werner", ""]]}, {"id": "1512.06945", "submitter": "EPTCS", "authors": "Fernando S\\'aenz-P\\'erez (Universidad Complutense de Madrid)", "title": "Restricted Predicates for Hypothetical Datalog", "comments": "In Proceedings PROLE 2015, arXiv:1512.06178", "journal-ref": "EPTCS 200, 2015, pp. 64-79", "doi": "10.4204/EPTCS.200.5", "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothetical Datalog is based on an intuitionistic semantics rather than on a\nclassical logic semantics, and embedded implications are allowed in rule\nbodies. While the usual implication (i.e., the neck of a Horn clause) stands\nfor inferring facts, an embedded implication plays the role of assuming its\npremise for deriving its consequence. A former work introduced both a formal\nframework and a goal-oriented tabled implementation, allowing negation in rule\nbodies. While in that work positive assumptions for both facts and rules can\noccur in the premise, negative assumptions are not allowed. In this work, we\ncover this subject by introducing a new concept: a restricted predicate, which\nallows negative assumptions by pruning the usual semantics of a predicate. This\nnew setting has been implemented in the deductive system DES.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 03:16:55 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["S\u00e1enz-P\u00e9rez", "Fernando", "", "Universidad Complutense de Madrid"]]}, {"id": "1512.07021", "submitter": "Alexander Sch\\\"atzle", "authors": "Alexander Sch\\\"atzle, Martin Przyjaciel-Zablocki, Simon Skilevic,\n  Georg Lausen", "title": "S2RDF: RDF Querying with SPARQL on Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDF has become very popular for semantic data publishing due to its flexible\nand universal graph-like data model. Yet, the ever-increasing size of RDF data\ncollections makes it more and more infeasible to store and process them on a\nsingle machine, raising the need for distributed approaches. Instead of\nbuilding a standalone but closed distributed RDF store, we endorse the usage of\nexisting infrastructures for Big Data processing, e.g. Hadoop. However, SPARQL\nquery performance is a major challenge as these platforms are not designed for\nRDF processing from ground. Thus, existing Hadoop-based approaches often favor\ncertain query pattern shape while performance drops significantly for other\nshapes. In this paper, we describe a novel relational partitioning schema for\nRDF data called ExtVP that uses a semi-join based preprocessing, akin to the\nconcept of Join Indices in relational databases, to efficiently minimize query\ninput size regardless of its pattern shape and diameter. Our prototype system\nS2RDF is built on top of Spark and uses its relational interface to execute\nSPARQL queries over ExtVP. We demonstrate its superior performance in\ncomparison to state of the art SPARQL-on-Hadoop approaches using the recent\nWatDiv test suite. S2RDF achieves sub-second runtimes for majority of queries\non a billion triples RDF graph.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 10:42:31 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 08:20:04 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 14:58:43 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Sch\u00e4tzle", "Alexander", ""], ["Przyjaciel-Zablocki", "Martin", ""], ["Skilevic", "Simon", ""], ["Lausen", "Georg", ""]]}, {"id": "1512.07430", "submitter": "Robert Kent", "authors": "Robert E. Kent", "title": "The ERA of FOLE: Foundation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the representation of ontologies in the first-order\nlogical environment FOLE (Kent 2013). An ontology defines the primitives with\nwhich to model the knowledge resources for a community of discourse (Gruber\n2009). These primitives, consisting of classes, relationships and properties,\nare represented by the entity-relationship-attribute ERA data model (Chen\n1976). An ontology uses formal axioms to constrain the interpretation of these\nprimitives. In short, an ontology specifies a logical theory. This paper is the\nfirst in a series of three papers that provide a rigorous mathematical\nrepresentation for the ERA data model in particular, and ontologies in general,\nwithin the first-order logical environment FOLE. The first two papers show how\nFOLE represents the formalism and semantics of (many-sorted) first-order logic\nin a classification form corresponding to ideas discussed in the Information\nFlow Framework (IFF). In particular, this first paper provides a foundation\nthat connects elements of the ERA data model with components of the first-order\nlogical environment FOLE, and the second paper provides a superstructure that\nextends FOLE to the formalisms of first-order logic. The third paper defines an\ninterpretation of FOLE in terms of the transformational passage, first\ndescribed in (Kent 2013), from the classification form of first-order logic to\nan equivalent interpretation form, thereby defining the formalism and semantics\nof first-order logical/relational database systems (Kent 2011). The FOLE\nrepresentation follows a conceptual structures approach, that is completely\ncompatible with formal concept analysis (Ganter and Wille 1999) and information\nflow (Barwise and Seligman 1997).\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 11:00:15 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Kent", "Robert E.", ""]]}, {"id": "1512.07734", "submitter": "Zhichun Wang", "authors": "Zhichun Wang and Juanzi Li", "title": "RDF2Rules: Learning Rules from RDF Knowledge Bases by Mining Frequent\n  Predicate Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several large-scale RDF knowledge bases have been built and applied\nin many knowledge-based applications. To further increase the number of facts\nin RDF knowledge bases, logic rules can be used to predict new facts based on\nthe existing ones. Therefore, how to automatically learn reliable rules from\nlarge-scale knowledge bases becomes increasingly important. In this paper, we\npropose a novel rule learning approach named RDF2Rules for RDF knowledge bases.\nRDF2Rules first mines frequent predicate cycles (FPCs), a kind of interesting\nfrequent patterns in knowledge bases, and then generates rules from the mined\nFPCs. Because each FPC can produce multiple rules, and effective pruning\nstrategy is used in the process of mining FPCs, RDF2Rules works very\nefficiently. Another advantage of RDF2Rules is that it uses the entity type\ninformation when generates and evaluates rules, which makes the learned rules\nmore accurate. Experiments show that our approach outperforms the compared\napproach in terms of both efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 07:19:01 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Wang", "Zhichun", ""], ["Li", "Juanzi", ""]]}, {"id": "1512.07806", "submitter": "Zhi-Hong Deng", "authors": "Zhi-Hong Deng", "title": "Mining Top-K Co-Occurrence Items", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent itemset mining has emerged as a fundamental problem in data mining\nand plays an important role in many data mining tasks, such as association\nanalysis, classification, etc. In the framework of frequent itemset mining, the\nresults are itemsets that are frequent in the whole database. However, in some\napplications, such recommendation systems and social networks, people are more\ninterested in finding out the items that occur with some user-specified\nitemsets (query itemsets) most frequently in a database. In this paper, we\naddress the problem by proposing a new mining task named top-k co-occurrence\nitem mining, where k is the desired number of items to be found. Four baseline\nalgorithms are presented first. Then, we introduce a special data structure\nnamed Pi-Tree (Prefix itemset Tree) to maintain the information of itemsets.\nBased on Pi-Tree, we propose two algorithms, namely PT (Pi-Tree-based\nalgorithm) and PT-TA (Pi-Tree-based algorithm with TA pruning), for mining\ntop-k co-occurrence items by incorporating several novel strategies for pruning\nthe search space to achieve high efficiency. The performance of PT and PT-TA\nwas evaluated against the four proposed baseline algorithms on both synthetic\nand real databases. Extensive experiments show that PT not only outperforms\nother algorithms substantially in terms execution time but also has excellent\nscalability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 12:49:46 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Deng", "Zhi-Hong", ""]]}, {"id": "1512.08417", "submitter": "Todor Ivanov", "authors": "Todor Ivanov and Max-Georg Beer", "title": "Evaluating Hive and Spark SQL with BigBench", "comments": "50 pages, 20 Tables", "journal-ref": null, "doi": null, "report-no": "Technical Report No. 2015-2", "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work was to utilize BigBench [1] as a Big Data\nbenchmark and evaluate and compare two processing engines: MapReduce [2] and\nSpark [3]. MapReduce is the established engine for processing data on Hadoop.\nSpark is a popular alternative engine that promises faster processing times\nthan the established MapReduce engine. BigBench was chosen for this comparison\nbecause it is the first end-to-end analytics Big Data benchmark and it is\ncurrently under public review as TPCx-BB [4]. One of our goals was to evaluate\nthe benchmark by performing various scalability tests and validate that it is\nable to stress test the processing engines. First, we analyzed the steps\nnecessary to execute the available MapReduce implementation of BigBench [1] on\nSpark. Then, all the 30 BigBench queries were executed on MapReduce/Hive with\ndifferent scale factors in order to see how the performance changes with the\nincrease of the data size. Next, the group of HiveQL queries were executed on\nSpark SQL and compared with their respective Hive runtimes. This report gives a\ndetailed overview on how to setup an experimental Hadoop cluster and execute\nBigBench on both Hive and Spark SQL. It provides the absolute times for all\nexperiments preformed for different scale factors as well as query results\nwhich can be used to validate correct benchmark execution. Additionally,\nmultiple issues and workarounds were encountered and solved during our work. An\nevaluation of the resource utilization (CPU, memory, disk and network usage) of\na subset of representative BigBench queries is presented to illustrate the\nbehavior of the different query groups on both processing engines. Last but not\nleast it is important to mention that larger parts of this report are taken\nfrom the master thesis of Max-Georg Beer, entitled \"Evaluation of BigBench on\nApache Spark Compared to MapReduce\" [5].\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 14:12:16 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 17:49:24 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Ivanov", "Todor", ""], ["Beer", "Max-Georg", ""]]}, {"id": "1512.08493", "submitter": "Lina Yao", "authors": "Lina Yao, Quan Z. Sheng, Anne H.H. Ngu, Xue Li, Boualem Benatallah", "title": "Unveiling Contextual Similarity of Things via Mining Human-Thing\n  Interactions in the Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances in radio-frequency identification (RFID), wireless\nsensor networks, and Web services, physical things are becoming an integral\npart of the emerging ubiquitous Web. Finding correlations of ubiquitous things\nis a crucial prerequisite for many important applications such as things\nsearch, discovery, classification, recommendation, and composition. This\narticle presents DisCor-T, a novel graph-based method for discovering\nunderlying connections of things via mining the rich content embodied in\nhuman-thing interactions in terms of user, temporal and spatial information. We\nmodel these various information using two graphs, namely spatio-temporal graph\nand social graph. Then, random walk with restart (RWR) is applied to find\nproximities among things, and a relational graph of things (RGT) indicating\nimplicit correlations of things is learned. The correlation analysis lays a\nsolid foundation contributing to improved effectiveness in things management.\nTo demonstrate the utility, we develop a flexible feature-based classification\nframework on top of RGT and perform a systematic case study. Our evaluation\nexhibits the strength and feasibility of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 13:47:27 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2015 10:36:47 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 03:10:57 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Yao", "Lina", ""], ["Sheng", "Quan Z.", ""], ["Ngu", "Anne H. H.", ""], ["Li", "Xue", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1512.08518", "submitter": "Peng Cheng", "authors": "Peng Cheng, Xiang Lian, Lei Chen, Cyrus Shahabi", "title": "Prediction-Based Task Assignment in Spatial Crowdsourcing (Technical\n  Report)", "comments": "15 pages", "journal-ref": null, "doi": "10.1109/ICDE.2017.146", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial crowdsourcing refers to a system that periodically assigns a number\nof location-based workers with spatial tasks nearby (e.g., taking photos or\nvideos at some spatial locations). Previous works on the spatial crowdsourcing\nusually designed task assignment strategies that maximize some assignment\nscores, which are however only based on available workers/tasks in the system\nat the time point of assigning workers/tasks. These strategies may achieve\nlocal optimality, due to the neglect of future workers/tasks that may join the\nsystem. In contrast, in this paper, we aim to achieve \"globally\" optimal task\nassignments, by considering not only those present, but also future (via\npredictions), workers/tasks. Specifically, we formalize an important problem,\nnamely prediction-based spatial crowdsourcing (PB-SC), which expects to obtain\na \"globally\" optimal strategy for worker-and-task assignments, over both\npresent and predicted task/worker locations, such that the total assignment\nquality score is maximized under the constraint of the traveling budget. In\nthis paper, we design an effective grid-based prediction method to estimate\nspatial distributions of workers/tasks in the future, and then utilize the\npredicted ones in our procedure of task assignments. We prove that the PB-SC\nproblem is NP-hard, and thus intractable. Therefore, we propose efficient\napproximate algorithms to tackle the PB-SC problem, including greedy and\ndivide-and-conquer (D&C) approaches, which can efficiently assign workers to\nspatial tasks with high quality scores and low budget consumptions, by\nconsidering both current and future task/worker distributions. Through\nextensive experiments, we demonstrate the efficiency and effectiveness of our\nPB-SC processing approaches on real/synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 15:17:00 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2016 03:27:50 GMT"}, {"version": "v3", "created": "Fri, 21 Oct 2016 05:37:34 GMT"}, {"version": "v4", "created": "Tue, 20 Dec 2016 09:59:17 GMT"}, {"version": "v5", "created": "Mon, 20 Feb 2017 05:50:06 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Cheng", "Peng", ""], ["Lian", "Xiang", ""], ["Chen", "Lei", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1512.08799", "submitter": "Hao Wu", "authors": "Hao Wu, Maoyuan Sun, Peng Mi, Nikolaj Tatti, Chris North, Naren\n  Ramakrishnan", "title": "Interactive Discovery of Coordinated Relationship Chains with Maximum\n  Entropy Models", "comments": "The journal version of paper is submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern visual analytic tools promote human-in-the-loop analysis but are\nlimited in their ability to direct the user toward interesting and promising\ndirections of study. This problem is especially acute when the analysis task is\nexploratory in nature, e.g., the discovery of potentially coordinated\nrelationships in massive text datasets. Such tasks are very common in domains\nlike intelligence analysis and security forensics where the goal is to uncover\nsurprising coalitions bridging multiple types of relations. We introduce new\nmaximum entropy models to discover surprising chains of relationships\nleveraging count data about entity occurrences in documents. These models are\nembedded in a visual analytic system called MERCER that treats relationship\nbundles as first class objects and directs the user toward promising lines of\ninquiry. We demonstrate how user input can judiciously direct analysis toward\nvalid conclusions whereas a purely algorithmic approach could be led astray.\nExperimental results on both synthetic and real datasets from the intelligence\ncommunity are presented.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 21:27:05 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Wu", "Hao", ""], ["Sun", "Maoyuan", ""], ["Mi", "Peng", ""], ["Tatti", "Nikolaj", ""], ["North", "Chris", ""], ["Ramakrishnan", "Naren", ""]]}]