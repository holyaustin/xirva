[{"id": "1304.0567", "submitter": "Antonis Loizou Ph.D.", "authors": "Antonis Loizou and Paul Groth", "title": "On the Formulation of Performant SPARQL Queries", "comments": null, "journal-ref": null, "doi": "10.1016/j.websem.2014.11.003", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of the flexibility of RDF and the expressiveness of SPARQL\nprovides a powerful mechanism to model, integrate and query data. However,\nthese properties also mean that it is nontrivial to write performant SPARQL\nqueries. Indeed, it is quite easy to create queries that tax even the most\noptimised triple stores. Currently, application developers have little concrete\nguidance on how to write \"good\" queries. The goal of this paper is to begin to\nbridge this gap. It describes 5 heuristics that can be applied to create\noptimised queries. The heuristics are informed by formal results in the\nliterature on the semantics and complexity of evaluating SPARQL queries, which\nensures that queries following these rules can be optimised effectively by an\nunderlying RDF store. Moreover, we empirically verify the efficacy of the\nheuristics using a set of openly available datasets and corresponding SPARQL\nqueries developed by a large pharmacology data integration project. The\nexperimental results show improvements in performance across 6 state-of-the-art\nRDF stores.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 09:02:48 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Loizou", "Antonis", ""], ["Groth", "Paul", ""]]}, {"id": "1304.0959", "submitter": "Nihat Tartal", "authors": "Gosta Grahne, Adrian Onet, Nihat Tartal", "title": "Conditional Tables in practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the ever increasing importance of the internet, interoperability of\nheterogeneous data sources is as well of ever increasing importance.\nInteroperability can be achieved e.g. through data integration and data\nexchange. Common to both approaches is the need for the DBMS to be able to\nstore and query incomplete databases. In this report we present PossDB, a DBMS\ncapable of storing and querying incomplete databases. The system is wrapper\nover PostgreSQL, and the query language is an extension of a subset of standard\nSQL. Our experimental results show that our system scales well, actually better\nthan comparable systems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 14:11:04 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Grahne", "Gosta", ""], ["Onet", "Adrian", ""], ["Tartal", "Nihat", ""]]}, {"id": "1304.1411", "submitter": "Quoc Trung Tran", "authors": "Quoc Trung Tran, Ivo Jimenez, Rui Wang, Neoklis Polyzotis, Anastasia\n  Ailamaki", "title": "RITA: An Index-Tuning Advisor for Replicated Databases", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Given a replicated database, a divergent design tunes the indexes in each\nreplica differently in order to specialize it for a specific subset of the\nworkload. This specialization brings significant performance gains compared to\nthe common practice of having the same indexes in all replicas, but requires\nthe development of new tuning tools for database administrators. In this paper\nwe introduce RITA (Replication-aware Index Tuning Advisor), a novel\ndivergent-tuning advisor that offers several essential features not found in\nexisting tools: it generates robust divergent designs that allow the system to\nadapt gracefully to replica failures; it computes designs that spread the load\nevenly among specialized replicas, both during normal operation and when\nreplicas fail; it monitors the workload online in order to detect changes that\nrequire a recomputation of the divergent design; and, it offers suggestions to\nelastically reconfigure the system (by adding/removing replicas or\nadding/dropping indexes) to respond to workload changes. The key technical\ninnovation behind RITA is showing that the problem of selecting an optimal\ndesign can be formulated as a Binary Integer Program (BIP). The BIP has a\nrelatively small number of variables, which makes it feasible to solve it\nefficiently using any off-the-shelf linear-optimization software. Experimental\nresults demonstrate that RITA computes better divergent designs compared to\nexisting tools, offers more features, and has fast execution times.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 15:54:48 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 09:43:59 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["Tran", "Quoc Trung", ""], ["Jimenez", "Ivo", ""], ["Wang", "Rui", ""], ["Polyzotis", "Neoklis", ""], ["Ailamaki", "Anastasia", ""]]}, {"id": "1304.1838", "submitter": "Jagan Sankaranarayanan", "authors": "Jeff LeFevre, Jagan Sankaranarayanan, Hakan Hacigumus, Junichi\n  Tatemura, Neoklis Polyzotis", "title": "Towards a Workload for Evolutionary Analytics", "comments": "10 pages", "journal-ref": "DanaC: Workshop on Data analytics in the Cloud, June 2013, New\n  York, NY", "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Emerging data analysis involves the ingestion and exploration of new data\nsets, application of complex functions, and frequent query revisions based on\nobserving prior query answers. We call this new type of analysis evolutionary\nanalytics and identify its properties. This type of analysis is not well\nrepresented by current benchmark workloads. In this paper, we present a\nworkload and identify several metrics to test system support for evolutionary\nanalytics. Along with our metrics, we present methodologies for running the\nworkload that capture this analytical scenario.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 00:26:41 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2013 07:00:10 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2013 19:19:22 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["LeFevre", "Jeff", ""], ["Sankaranarayanan", "Jagan", ""], ["Hacigumus", "Hakan", ""], ["Tatemura", "Junichi", ""], ["Polyzotis", "Neoklis", ""]]}, {"id": "1304.1877", "submitter": "Tomasz Kajdanowicz", "authors": "Katarzyna Pasierb, Tomasz Kajdanowicz, Przemyslaw Kazienko", "title": "Privacy-preserving Data Mining, Sharing and Publishing", "comments": null, "journal-ref": "Journal of Medical Informatics & Technologies, Vol. 18, pp. 69-76,\n  2011", "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The goal of the paper is to present different approaches to\nprivacy-preserving data sharing and publishing in the context of e-health care\nsystems. In particular, the literature review on technical issues in privacy\nassurance and current real-life high complexity implementation of medical\nsystem that assumes proper data sharing mechanisms are presented in the paper.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 11:20:24 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Pasierb", "Katarzyna", ""], ["Kajdanowicz", "Tomasz", ""], ["Kazienko", "Przemyslaw", ""]]}, {"id": "1304.1995", "submitter": "Liang Liu", "authors": "Liu Liang", "title": "Image Retrieval using Histogram Factorization and Contextual Similarity\n  Learning", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval has been a top topic in the field of both computer vision and\nmachine learning for a long time. Content based image retrieval, which tries to\nretrieve images from a database visually similar to a query image, has\nattracted much attention. Two most important issues of image retrieval are the\nrepresentation and ranking of the images. Recently, bag-of-words based method\nhas shown its power as a representation method. Moreover, nonnegative matrix\nfactorization is also a popular way to represent the data samples. In addition,\ncontextual similarity learning has also been studied and proven to be an\neffective method for the ranking problem. However, these technologies have\nnever been used together. In this paper, we developed an effective image\nretrieval system by representing each image using the bag-of-words method as\nhistograms, and then apply the nonnegative matrix factorization to factorize\nthe histograms, and finally learn the ranking score using the contextual\nsimilarity learning method. The proposed novel system is evaluated on a large\nscale image database and the effectiveness is shown.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 13:15:17 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2013 17:59:33 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Liang", "Liu", ""]]}, {"id": "1304.2184", "submitter": "Evgeniy Grigoriev A.", "authors": "Evgeniy Grigoriev", "title": "Object-Oriented Translation for Programmable Relational System (DRAFT)", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces the principles of object-oriented translation for target\nmachine which provides executing the sequences of elementary operations on\npersistent data presented as a set of relations (programmable relational\nsystem). The language of this target machine bases on formal operations of\nrelational data model. An approach is given to convert both the description of\ncomplex object-oriented data structures and operations on these data, into a\ndescription of relational structures and operations on them. The proposed\napproach makes possible to extend the target relational language with commands\nallowing data be described as a set of complex persistent objects of different\nclasses. Object views are introduced which allow relational operations be\napplied to the data of complex objects. It is shown that any operation and\nmethod can be executed on any group of the objects without explicit and\nimplicit iterators. Binding of both attributes and methods with their\npolymorphic implementations are discussed. Classes can be co-used with\nrelations as scalar domains, in referential integrity constraints and in data\nquery operations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 12:29:31 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Grigoriev", "Evgeniy", ""]]}, {"id": "1304.2313", "submitter": "Jerome Le Ny", "authors": "Jerome Le Ny", "title": "On Differentially Private Filtering for Event Streams", "comments": "arXiv admin note: text overlap with arXiv:1207.4305", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rigorous privacy mechanisms that can cope with dynamic data are required to\nencourage a wider adoption of large-scale monitoring and decision systems\nrelying on end-user information. A promising approach to develop these\nmechanisms is to specify quantitative privacy requirements at design time\nrather than as an afterthought, and to rely on signal processing techniques to\nachieve satisfying trade-offs between privacy and performance specifications.\nThis paper discusses, from the signal processing point of view, an event stream\nanalysis problem introduced in the database and cryptography literature. A\ndiscrete-valued input signal describes the occurrence of events contributed by\nend-users, and a system is supposed to provide some output signal based on this\ninformation, while preserving the privacy of the participants. The notion of\nprivacy adopted here is that of event-level differential privacy, which\nprovides strong privacy guarantees and has important operational advantages.\nSeveral mechanisms are described to provide differentially private output\nsignals while minimizing the impact on performance. These mechanisms\ndemonstrate the benefits of leveraging system theoretic techniques to provide\nprivacy guarantees for dynamic systems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 14:04:16 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Ny", "Jerome Le", ""]]}, {"id": "1304.2576", "submitter": "Xiaokui Xiao", "authors": "Andy Diwen Zhu, Hui Ma, Xiaokui Xiao, Siqiang Luo, Youze Tang,\n  Shuigeng Zhou", "title": "Shortest Path and Distance Queries on Road Networks: Towards Bridging\n  Theory and Practice", "comments": "to appear in SIGMOD 2013. Table 1 updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two locations $s$ and $t$ in a road network, a distance query returns\nthe minimum network distance from $s$ to $t$, while a shortest path query\ncomputes the actual route that achieves the minimum distance. These two types\nof queries find important applications in practice, and a plethora of solutions\nhave been proposed in past few decades. The existing solutions, however, are\noptimized for either practical or asymptotic performance, but not both. In\nparticular, the techniques with enhanced practical efficiency are mostly\nheuristic-based, and they offer unattractive worst-case guarantees in terms of\nspace and time. On the other hand, the methods that are worst-case efficient\noften entail prohibitive preprocessing or space overheads, which render them\ninapplicable for the large road networks (with millions of nodes) commonly used\nin modern map applications.\n  This paper presents {\\em Arterial Hierarchy (AH)}, an index structure that\nnarrows the gap between theory and practice in answering shortest path and\ndistance queries on road networks. On the theoretical side, we show that, under\na realistic assumption, AH answers any distance query in $\\tilde{O}(\\log \\r)$\ntime, where $\\r = d_{max}/d_{min}$, and $d_{max}$ (resp.\\ $d_{min}$) is the\nlargest (resp.\\ smallest) $L_\\infty$ distance between any two nodes in the road\nnetwork. In addition, any shortest path query can be answered in $\\tilde{O}(k +\n\\log \\r)$ time, where $k$ is the number of nodes on the shortest path. On the\npractical side, we experimentally evaluate AH on a large set of real road\nnetworks with up to twenty million nodes, and we demonstrate that (i) AH\noutperforms the state of the art in terms of query time, and (ii) its space and\npre-computation overheads are moderate.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 13:14:38 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2013 13:38:37 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Zhu", "Andy Diwen", ""], ["Ma", "Hui", ""], ["Xiao", "Xiaokui", ""], ["Luo", "Siqiang", ""], ["Tang", "Youze", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "1304.2637", "submitter": "Juan Reutter L", "authors": "Juan L. Reutter", "title": "Containment of Nested Regular Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested regular expressions (NREs) have been proposed as a powerful formalism\nfor querying RDFS graphs, but research in a more general graph database context\nhas been scarce, and static analysis results are currently lacking. In this\npaper we investigate the problem of containment of NREs, and show that it can\nbe solved in PSPACE, i.e., the same complexity as the problem of containment of\nregular expressions or regular path queries (RPQs).\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 15:40:21 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 18:47:43 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Reutter", "Juan L.", ""]]}, {"id": "1304.2867", "submitter": "M.M.A. Hashem", "authors": "Md. Mamun Ali Sarker, Md. Ashraf Hossain Khan and M.M.A. Hashem", "title": "Guidelines to the Problem of Location Management and Database\n  Architecture for the Next Generation Mobile Networks", "comments": null, "journal-ref": "Procs. of the International Conference on Computer and\n  Communication Engineering (ICCCE 06), Vol. II, pp. 761-766, Kuala Lumpur,\n  Malaysia, May 9-11, (2006)", "doi": null, "report-no": null, "categories": "cs.NI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In near future, anticipated large number of mobile users may introduce very\nlarge centralized databases and increase end-to-end delays in location\nregistration and call delivery on HLR-VLR database and will become infeasible.\nAfter observing several problems we propose some guidelines. Multitree\ndistributed database, high throughput index structure, memory oriented database\norganization are used. Location management guidelines for moving user in\noverlapping network, neighbor discovery protocol (NDP), and global roaming rule\nare adopted. Analytic model and examples are presented to evaluate the\nefficiency of proposed guidelines.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 07:36:55 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Sarker", "Md. Mamun Ali", ""], ["Khan", "Md. Ashraf Hossain", ""], ["Hashem", "M. M. A.", ""]]}, {"id": "1304.3120", "submitter": "Jonathan Arthur Quaye-Ballard", "authors": "J. A. Quaye-Ballard, R. An, A. B. Agyemang, N. Y. Oppong-Quayson, and\n  J. E. N. Ablade", "title": "GUI Database for the Equipment Store of the Department of Geomatic\n  Engineering, KNUST", "comments": "6 pages, 9 figures, (IJACSA) International Journal of Advanced\n  Computer Science and Applications, Vol. 3, No. 7, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geospatial analyst is required to apply art, science, and technology to\nmeasure relative positions of natural and man-made features above or beneath\nthe earths surface, and to present this information either graphically or\nnumerically. The reference positions for these measurements need to be well\narchived and managed to effectively sustain the activities in the spatial\nindustry. The research herein described highlights the need for an information\nsystem for the Land Surveyors Equipment Store. Such a system is a database\nmanagement system with a user friendly graphical interface. This paper\ndescribes one such system that has been developed for the Equipment Store of\nthe Department of Geomatic Engineering, Kwame Nkrumah University of Science and\nTechnology, Ghana. The system facilitates efficient management and location of\ninstruments, as well as easy location of beacons together with their attribute\ninformation, it provides multimedia information about instruments in an\nEquipment Store. Digital camera was used capture the pictorial descriptions of\nthe beacons. Geographic Information System software was employed to visualize\nthe spatial location of beacons and to publish the various layers for the\nGraphical User Interface. The aesthetics of the interface was developed with\nuser interface design tools and coded by programming. The developed Suite,\npowered by a reliable and fully scalable database, provides an efficient way of\nbooking and analyzing transactions in an Equipment Store.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 00:25:01 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Quaye-Ballard", "J. A.", ""], ["An", "R.", ""], ["Agyemang", "A. B.", ""], ["Oppong-Quayson", "N. Y.", ""], ["Ablade", "J. E. N.", ""]]}, {"id": "1304.3603", "submitter": "Sunita Jahirabadkar", "authors": "Sunita Jahirabadkar and Parag Kulkarni", "title": "SCAF An effective approach to Classify Subspace Clustering algorithms", "comments": null, "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.2, March 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering discovers the clusters embedded in multiple, overlapping\nsubspaces of high dimensional data. Many significant subspace clustering\nalgorithms exist, each having different characteristics caused by the use of\ndifferent techniques, assumptions, heuristics used etc. A comprehensive\nclassification scheme is essential which will consider all such characteristics\nto divide subspace clustering approaches in various families. The algorithms\nbelonging to same family will satisfy common characteristics. Such a\ncategorization will help future developers to better understand the quality\ncriteria to be used and similar algorithms to be used to compare results with\ntheir proposed clustering algorithms. In this paper, we first proposed the\nconcept of SCAF (Subspace Clustering Algorithms Family). Characteristics of\nSCAF will be based on the classes such as cluster orientation, overlap of\ndimensions etc. As an illustration, we further provided a comprehensive,\nsystematic description and comparison of few significant algorithms belonging\nto 'Axis parallel, overlapping, density based' SCAF.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 11:02:56 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Jahirabadkar", "Sunita", ""], ["Kulkarni", "Parag", ""]]}, {"id": "1304.4184", "submitter": "Srikantaiah K C", "authors": "K. C. Srikantaiah, N. Krishna Kumar, K. R. Venugopal, L. M. Patnaik", "title": "Bidirectional Growth based Mining and Cyclic Behaviour Analysis of Web\n  Sequential Patterns", "comments": "19 pages", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.2, March 2013", "doi": "10.5121/ijdkp.2013.3204", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web sequential patterns are important for analyzing and understanding users\nbehaviour to improve the quality of service offered by the World Wide Web. Web\nPrefetching is one such technique that utilizes prefetching rules derived\nthrough Cyclic Model Analysis of the mined Web sequential patterns. The more\naccurate the prediction and more satisfying the results of prefetching if we\nuse a highly efficient and scalable mining technique such as the Bidirectional\nGrowth based Directed Acyclic Graph. In this paper, we propose a novel\nalgorithm called Bidirectional Growth based mining Cyclic behavior Analysis of\nweb sequential Patterns (BGCAP) that effectively combines these strategies to\ngenerate prefetching rules in the form of 2-sequence patterns with Periodicity\nand threshold of Cyclic Behaviour that can be utilized to effectively prefetch\nWeb pages, thus reducing the users perceived latency. As BGCAP is based on\nBidirectional pattern growth, it performs only (log n+1) levels of recursion\nfor mining n Web sequential patterns. Our experimental results show that\nprefetching rules generated using BGCAP is 5-10 percent faster for different\ndata sizes and 10-15% faster for a fixed data size than TD-Mine. In addition,\nBGCAP generates about 5-15 percent more prefetching rules than TD-Mine.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 17:36:57 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Srikantaiah", "K. C.", ""], ["Kumar", "N. Krishna", ""], ["Venugopal", "K. R.", ""], ["Patnaik", "L. M.", ""]]}, {"id": "1304.4187", "submitter": "Emilien Antoine", "authors": "Serge Abiteboul (LSV), \\'Emilien Antoine (LSV), Julia Stoyanovich", "title": "The Webdamlog System Managing Distributed Knowledge on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the use of WebdamLog, a declarative high-level lan- guage in the\nstyle of datalog, to support the distribution of both data and knowledge (i.e.,\nprograms) over a network of au- tonomous peers. The main novelty of WebdamLog\ncompared to datalog is its use of delegation, that is, the ability for a peer\nto communicate a program to another peer. We present results of a user study,\nshowing that users can write WebdamLog programs quickly and correctly, and with\na minimal amount of training. We present an implementation of the WebdamLog\ninference engine relying on the Bud dat- alog engine. We describe an\nexperimental evaluation of the WebdamLog engine, demonstrating that WebdamLog\ncan be im- plemented efficiently. We conclude with a discussion of ongoing and\nfuture work.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 18:00:40 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Abiteboul", "Serge", "", "LSV"], ["Antoine", "\u00c9milien", "", "LSV"], ["Stoyanovich", "Julia", ""]]}, {"id": "1304.4303", "submitter": "Azza Abouzied", "authors": "Azza Abouzied and Dana Angluin and Christos Papadimitriou and Joseph\n  M. Hellerstein and Avi Silberschatz", "title": "Learning and Verifying Quantified Boolean Queries by Example", "comments": "Extended Version of PODS 2013 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help a user specify and verify quantified queries --- a class of database\nqueries known to be very challenging for all but the most expert users --- one\ncan question the user on whether certain data objects are answers or\nnon-answers to her intended query. In this paper, we analyze the number of\nquestions needed to learn or verify qhorn queries, a special class of Boolean\nquantified queries whose underlying form is conjunctions of quantified Horn\nexpressions. We provide optimal polynomial-question and polynomial-time\nlearning and verification algorithms for two subclasses of the class qhorn with\nupper constant limits on a query's causal density.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 00:21:25 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Abouzied", "Azza", ""], ["Angluin", "Dana", ""], ["Papadimitriou", "Christos", ""], ["Hellerstein", "Joseph M.", ""], ["Silberschatz", "Avi", ""]]}, {"id": "1304.4329", "submitter": "Rajesh Pasupuleti", "authors": "Pasupuleti Rajesh, Gugulothu Narsimha", "title": "Privacy Preserving Data Mining by Using Implicit Function Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining has made broad significant multidisciplinary field used in vast\napplication domains and extracts knowledge by identifying structural\nrelationship among the objects in large data bases. Privacy preserving data\nmining is a new area of data mining research for providing privacy of sensitive\nknowledge of information extracted from data mining system to be shared by the\nintended persons not to everyone to access. In this paper, we proposed a new\napproach of privacy preserving data mining by using implicit function theorem\nfor secure transformation of sensitive data obtained from data mining system.\nwe proposed two way enhanced security approach. First transforming original\nvalues of sensitive data into different partial derivatives of functional\nvalues for perturbation of data. secondly generating symmetric key value by\nEigen values of jacobian matrix for secure computation. we given an example of\nacademic sensitive data converting into vector valued functions to explain\nabout our proposed concept and presented implementation based results of new\nproposed of approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 04:30:38 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Rajesh", "Pasupuleti", ""], ["Narsimha", "Gugulothu", ""]]}, {"id": "1304.4613", "submitter": "Ye Wang", "authors": "Bing-Rong Lin, Ye Wang, Shantanu Rane", "title": "On the Benefits of Sampling in Privacy Preserving Statistical Analysis\n  on Distributed Databases", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem where mutually untrusting curators possess portions of\na vertically partitioned database containing information about a set of\nindividuals. The goal is to enable an authorized party to obtain aggregate\n(statistical) information from the database while protecting the privacy of the\nindividuals, which we formalize using Differential Privacy. This process can be\nfacilitated by an untrusted server that provides storage and processing\nservices but should not learn anything about the database. This work describes\na data release mechanism that employs Post Randomization (PRAM), encryption and\nrandom sampling to maintain privacy, while allowing the authorized party to\nconduct an accurate statistical analysis of the data. Encryption ensures that\nthe storage server obtains no information about the database, while PRAM and\nsampling ensures individual privacy is maintained against the authorized party.\nWe characterize how much the composition of random sampling with PRAM increases\nthe differential privacy of system compared to using PRAM alone. We also\nanalyze the statistical utility of our system, by bounding the estimation error\n- the expected l2-norm error between the true empirical distribution and the\nestimated distribution - as a function of the number of samples, PRAM noise,\nand other system parameters. Our analysis shows a tradeoff between increasing\nPRAM noise versus decreasing the number of samples to maintain a desired level\nof privacy, and we determine the optimal number of samples that balances this\ntradeoff and maximizes the utility. In experimental simulations with the UCI\n\"Adult Data Set\" and with synthetically generated data, we confirm that the\ntheoretically predicted optimal number of samples indeed achieves close to the\nminimal empirical error, and that our analytical error bounds match well with\nthe empirical results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 20:28:32 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Lin", "Bing-Rong", ""], ["Wang", "Ye", ""], ["Rane", "Shantanu", ""]]}, {"id": "1304.4661", "submitter": "Takuya Akiba", "authors": "Takuya Akiba, Yoichi Iwata, and Yuichi Yoshida", "title": "Fast Exact Shortest-Path Distance Queries on Large Networks by Pruned\n  Landmark Labeling", "comments": "To appear in SIGMOD 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new exact method for shortest-path distance queries on\nlarge-scale networks. Our method precomputes distance labels for vertices by\nperforming a breadth-first search from every vertex. Seemingly too obvious and\ntoo inefficient at first glance, the key ingredient introduced here is pruning\nduring breadth-first searches. While we can still answer the correct distance\nfor any pair of vertices from the labels, it surprisingly reduces the search\nspace and sizes of labels. Moreover, we show that we can perform 32 or 64\nbreadth-first searches simultaneously exploiting bitwise operations. We\nexperimentally demonstrate that the combination of these two techniques is\nefficient and robust on various kinds of large-scale real-world networks. In\nparticular, our method can handle social networks and web graphs with hundreds\nof millions of edges, which are two orders of magnitude larger than the limits\nof previous exact methods, with comparable query time to those of previous\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 01:11:12 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Akiba", "Takuya", ""], ["Iwata", "Yoichi", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1304.4795", "submitter": "Shixi Chen", "authors": "Shixi Chen, Shuigeng Zhou", "title": "Recursive Mechanism: Towards Node Differential Privacy and Unrestricted\n  Joins [Full Version, Draft 0.1]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing studies on differential privacy mainly consider aggregation on data\nsets where each entry corresponds to a particular participant to be protected.\nIn many situations, a user may pose a relational algebra query on a sensitive\ndatabase, and desires differentially private aggregation on the result of the\nquery. However, no known work is capable to release this kind of aggregation\nwhen the query contains unrestricted join operations. This severely limits the\napplications of existing differential privacy techniques because many data\nanalysis tasks require unrestricted joins. One example is subgraph counting on\na graph. Existing methods for differentially private subgraph counting address\nonly edge differential privacy and are subject to very simple subgraphs. Before\nthis work, whether any nontrivial graph statistics can be released with\nreasonable accuracy under node differential privacy is still an open problem.\n  In this paper, we propose a novel differentially private mechanism to release\nan approximation to a linear statistic of the result of some positive\nrelational algebra calculation over a sensitive database. Unrestricted joins\nare supported in our mechanism. The error bound of the approximate answer is\nroughly proportional to the \\emph{empirical sensitivity} of the query --- a new\nnotion that measures the maximum possible change to the query answer when a\nparticipant withdraws its data from the sensitive database. For subgraph\ncounting, our mechanism provides the first solution to achieve node\ndifferential privacy, for any kind of subgraphs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 12:40:15 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Chen", "Shixi", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "1304.5409", "submitter": "Carsten Gottschlich", "authors": "Carsten Gottschlich and Stephan Huckemann", "title": "Separating the Real from the Synthetic: Minutiae Histograms as\n  Fingerprints of Fingerprints", "comments": null, "journal-ref": "IET Biometrics, vol. 3, no. 4, pp. 291-301, Dec. 2014", "doi": "10.1049/iet-bmt.2013.0065", "report-no": null, "categories": "cs.CV cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we show that by the current state-of-the-art synthetically\ngenerated fingerprints can easily be discriminated from real fingerprints. We\npropose a method based on second order extended minutiae histograms (MHs) which\ncan distinguish between real and synthetic prints with very high accuracy. MHs\nprovide a fixed-length feature vector for a fingerprint which are invariant\nunder rotation and translation. This 'test of realness' can be applied to\nsynthetic fingerprints produced by any method. In this work, tests are\nconducted on the 12 publicly available databases of FVC2000, FVC2002 and\nFVC2004 which are well established benchmarks for evaluating the performance of\nfingerprint recognition algorithms; 3 of these 12 databases consist of\nartificial fingerprints generated by the SFinGe software. Additionally, we\nevaluate the discriminative performance on a database of synthetic fingerprints\ngenerated by the software of Bicz versus real fingerprint images. We conclude\nwith suggestions for the improvement of synthetic fingerprint generation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 13:21:13 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2014 11:28:08 GMT"}, {"version": "v3", "created": "Wed, 15 Oct 2014 14:04:46 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Gottschlich", "Carsten", ""], ["Huckemann", "Stephan", ""]]}, {"id": "1304.5566", "submitter": "Michael Cotterell", "authors": "Michael E. Cotterell, Terrance Medina", "title": "A Markov Model for Ontology Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion of available data along with the need to integrate and utilize\nthat data has led to a pressing interest in data integration techniques. In\nterms of Semantic Web technologies, Ontology Alignment is a key step in the\nprocess of integrating heterogeneous knowledge bases. In this paper, we present\nthe Edge Confidence technique, a modification and improvement over the popular\nSimilarity Flooding technique for Ontology Alignment.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 00:25:50 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Cotterell", "Michael E.", ""], ["Medina", "Terrance", ""]]}, {"id": "1304.6473", "submitter": "Vit Novacek", "authors": "Vit Novacek, Aisha Naseer", "title": "Technical report: Linking the scientific and clinical data with\n  KI2NA-LHC", "comments": "A longer version of a paper originally published at the IEEE\n  conference on Computer-Based Medical Systems (CBMS'13), under the name:\n  Linking the Scientific and Clinical Data with KI2NA-LHC - An Outline (authors\n  are the same)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a use case and propose a system for data and knowledge\nintegration in life sciences. In particular, we focus on linking clinical\nresources (electronic patient records) with scientific documents and data\n(research articles, biomedical ontologies and databases). Our motivation is\ntwo-fold. Firstly, we aim to instantly provide scientific context of particular\npatient cases for clinicians in order for them to propose treatments in a more\ninformed way. Secondly, we want to build a technical infrastructure for\nresearchers that will allow them to semi-automatically formulate and evaluate\ntheir hypothesis against longitudinal patient data. This paper describes the\nproposed system and its typical usage in a broader context of KI2NA, an ongoing\ncollaboration between the DERI research institute and Fujitsu Laboratories. We\nintroduce an architecture of the proposed framework called KI2NA-LHC (for\nLinked Health Care) and outline the details of its implementation. We also\ndescribe typical usage scenarios and propose a methodology for evaluation of\nthe whole framework. The main goal of this paper is to introduce our ongoing\nwork to a broader expert audience. By doing so, we aim to establish an\nearly-adopter community for our work and elicit feedback we could reflect in\nthe development of the prototype so that it is better tailored to the\nrequirements of target users.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 18:13:04 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Novacek", "Vit", ""], ["Naseer", "Aisha", ""]]}, {"id": "1304.6575", "submitter": "Togerchety Hitendra sarma", "authors": "B.Hanmanthu, B.Raghu Ram, P.Niranjan", "title": "Third Party Privacy Preserving Protocol for Perturbation Based\n  Classification of Vertically Fragmented Data Bases", "comments": "Appeared in ICECIT-2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy is become major issue in distributed data mining. In the literature\nwe can found many proposals of privacy preserving which can be divided into two\nmajor categories that is trusted third party and multiparty based privacy\nprotocols. In case of trusted third party models the conventional asymmetric\ncryptographic based techniques will be used and in case of multi party based\nprotocols data perturbed to make sure no other party to understand original\ndata. In order to enhance security features by combining strengths of both\nmodels in this paper, we propose to use data perturbed techniques in third\nparty privacy preserving protocol to conduct the classification on vertically\nfragmented data bases. Specially, we present a method to build Naive Bayes\nclassification from the disguised and decentralized databases. In order to\nperform classification we propose third party protocol for secure computations.\nWe conduct experiments to compare the accuracy of our Naive Bayes with the one\nbuilt from the original undisguised data. Our results show that although the\ndata are disguised and decentralized, our method can still achieve fairly high\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 13:21:35 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Hanmanthu", "B.", ""], ["Ram", "B. Raghu", ""], ["Niranjan", "P.", ""]]}, {"id": "1304.7094", "submitter": "Dr. Rajesh Kumar  Tiwari", "authors": "Jun Ziang Pinn, A. Fr. Zung", "title": "A new Watermarking Technique for Secure Database", "comments": "Database", "journal-ref": "International Journal of Computer Engineering & Applications, Vol.\n  I, No. I, 2013", "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital multimedia watermarking technology was suggested in the last decade\nto embed copyright information in digital objects such images, audio and video.\nHowever, the increasing use of relational database systems in many real-life\napplications created an ever increasing need for watermarking database systems.\nAs a result, watermarking relational database systems is now merging as a\nresearch area that deals with the legal issue of copyright protection of\ndatabase systems. Approach: In this study, we proposed an efficient database\nwatermarking algorithm based on inserting binary image watermarks in\nnon-numeric mutli-word attributes of selected database tuples. Results: The\nalgorithm is robust as it resists attempts to remove or degrade the embedded\nwatermark and it is blind as it does not require the original database in order\nto extract the embedded watermark. Conclusion: Experimental results\ndemonstrated blindness and the robustness of the algorithm against common\ndatabase attacks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 08:46:42 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Pinn", "Jun Ziang", ""], ["Zung", "A. Fr.", ""]]}, {"id": "1304.7096", "submitter": "Dr. Rajesh Kumar  Tiwari", "authors": "Rajesh Kumar Tiwari", "title": "A Novel approach for Hybrid Database", "comments": null, "journal-ref": "International Journal of Computer Engineering & Applications, Vol\n  1, Iss 1, 2013", "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current world of economic crises, the cost control is one of the chief\nconcerns for all types of industries, especially for the small venders. The\nsmall vendors are suppose to minimize their budget on Information Technology by\nreducing the initial investment in hardware and costly database servers like\nORACLE, SQL Server, SYBASE, etc. for the purpose of data processing and\nstoring. In other divisions, the electronic devices manufacturing companies\nwant to increase the demand and reduce the manufacturing cost by introducing\nthe low cost technologies. The new small devices like ipods, iphones, palm top\netc. are now-a-days used as data computation and storing tools. For both the\ncases mentioned above, instead of going for the costly database servers which\nadditionally requires extra hardware as well as the extra expenses in training\nand handling, the flat file may be considered as a candidate due to its easy\nhandling nature, fast accessing, and of course free of cost. But the main\nhurdle is the security aspects which are not up to the optimum level. In this\npaper, we propose a methodology that combines all the merit of the flat file\nand with the help of a novel steganographic technique we can maintain the\nutmost security fence. The new proposed methodology will undoubtedly be highly\nbeneficial for small vendors as well as for the above said electronic devices\nmanufacturer\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 08:50:54 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Tiwari", "Rajesh Kumar", ""]]}, {"id": "1304.7285", "submitter": "Minyar Sassi", "authors": "Minyar Sassi-Hidri and Soukaina Ben Bdira", "title": "Traitement approximatif des requ\\^etes flexibles avec groupement\n  d'attributs et jointure", "comments": "in French. The 13\\`eme Conf\\'erence Francophone sur l'Extraction et\n  la Gestion des Connaissances (EGC), pp. 29-30, 2013", "journal-ref": "The 3rd International Conference on Advances in Databases,\n  Knowledge, and Data Applications (DBKDA), pp. 128-135, 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of approximate processing for flexible\nqueries in the form SELECT-FROM-WHERE-GROUP BY with join condition. It offers a\nflexible framework for online aggregation while promoting response time at the\nexpense of result accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 09:27:18 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Sassi-Hidri", "Minyar", ""], ["Bdira", "Soukaina Ben", ""]]}, {"id": "1304.7544", "submitter": "Jimmy Lin", "authors": "Jimmy Lin", "title": "Monoidify! Monoids as a Design Principle for Efficient MapReduce\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that since the sort/shuffle stage in MapReduce is costly,\nlocal aggregation is one important principle to designing efficient algorithms.\nThis short paper represents an attempt to more clearly articulate this design\nprinciple in terms of monoids, which generalizes the use of combiners and the\nin-mapper combining pattern.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 00:30:36 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Lin", "Jimmy", ""]]}, {"id": "1304.7799", "submitter": "Medha Atre", "authors": "Medha Atre", "title": "Left Bit Right: For SPARQL Join Queries with OPTIONAL Patterns\n  (Left-outer-joins)", "comments": "SIGMOD 2015", "journal-ref": null, "doi": "10.1145/2723372.2746483", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SPARQL basic graph pattern (BGP) (a.k.a. SQL inner-join) query optimization\nis a well researched area. However, optimization of OPTIONAL pattern queries\n(a.k.a. SQL left-outer-joins) poses additional challenges, due to the\nrestrictions on the \\textit{reordering} of left-outer-joins. The occurrence of\nsuch queries tends to be as high as 50% of the total queries (e.g., DBPedia\nquery logs).\n  In this paper, we present \\textit{Left Bit Right} (LBR), a technique for\n\\textit{well-designed} nested BGP and OPTIONAL pattern queries. Through LBR, we\npropose a novel method to represent such queries using a graph of\n\\textit{supernodes}, which is used to aggressively prune the RDF triples, with\nthe help of compressed indexes. We also propose novel optimization strategies\n-- first of a kind, to the best of our knowledge -- that combine together the\ncharacteristics of \\textit{acyclicity} of queries, \\textit{minimality}, and\n\\textit{nullification}, \\textit{best-match} operators. In this paper, we focus\non OPTIONAL patterns without UNIONs or FILTERs, but we also show how UNIONs and\nFILTERs can be handled with our technique using a \\textit{query rewrite}. Our\nevaluation on RDF graphs of up to and over one billion triples, on a commodity\nlaptop with 8 GB memory, shows that LBR can process \\textit{well-designed}\nlow-selectivity complex queries up to 11 times faster compared to the\nstate-of-the-art RDF column-stores as Virtuoso and MonetDB, and for highly\nselective queries, LBR is at par with them.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 20:49:49 GMT"}, {"version": "v2", "created": "Sun, 12 May 2013 14:43:44 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2015 12:05:42 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Atre", "Medha", ""]]}, {"id": "1304.7854", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Jaffer Gardezi", "title": "On the Complexity of Query Answering under Matching Dependencies for\n  Entity Resolution", "comments": "To appear in Proc. of the Alberto Mendelzon International Workshop on\n  Foundations of Data Management (AMW 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching Dependencies (MDs) are a relatively recent proposal for declarative\nentity resolution. They are rules that specify, given the similarities\nsatisfied by values in a database, what values should be considered duplicates,\nand have to be matched. On the basis of a chase-like procedure for MD\nenforcement, we can obtain clean (duplicate-free) instances; actually possibly\nseveral of them. The resolved answers to queries are those that are invariant\nunder the resulting class of resolved instances. In previous work we identified\nsome tractable cases (i.e. for certain classes of queries and MDs) of resolved\nquery answering. In this paper we further investigate the complexity of this\nproblem, identifying some intractable cases. For a special case we obtain a\ndichotomy complexity result.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 04:05:44 GMT"}, {"version": "v2", "created": "Sun, 26 May 2013 21:34:35 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Gardezi", "Jaffer", ""]]}]