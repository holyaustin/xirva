[{"id": "1903.01159", "submitter": "Nameer Al Khafaf", "authors": "Nameer Al Khafaf, Mahdi Jalili and Peter Sokolowski", "title": "Optimal Clustering of Energy Consumers based on Entropy of the\n  Correlation Matrix between Clusters", "comments": "8 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased deployment of residential smart meters has made it possible to\nrecord energy consumption data on short intervals. These data, if used\nefficiently, carry valuable information for managing power demand and\nincreasing energy consumption efficiency. However, analyzing smart meter data\nof millions of customers in a timely manner is quite challenging. An efficient\nway to analyze these data is to first identify clusters of customers, and then\nfocus on analyzing these clusters. Deciding on the optimal number of clusters\nis a challenging task. In this manuscript, we propose a metric to efficiently\nfind the optimal number of clusters. A genetic algorithm based feature\nselection is used to reduce the number of features, which are then fed into\nself-organizing maps for clustering. We apply the proposed clustering technique\non two electricity consumption datasets from Victoria, Australia and Ireland.\nThe numerical simulations reveal effectiveness of the proposed method in\nfinding the optimal clusters.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 10:21:45 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Khafaf", "Nameer Al", ""], ["Jalili", "Mahdi", ""], ["Sokolowski", "Peter", ""]]}, {"id": "1903.03412", "submitter": "Dong Zhang", "authors": "Dong-dong Zhang, Lei Zhang, Vladimir Zaborovsky, Feng Xie, Yan-wen Wu,\n  Ting-ting Lu", "title": "Research on the pixel-based and object-oriented methods of urban feature\n  extraction with GF-2 remote-sensing images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.CV cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the rapid urbanization construction of China, acquisition of urban\ngeographic information and timely data updating are important and fundamental\ntasks for the refined management of cities. With the development of domestic\nremote sensing technology, the application of Gaofen-2 (GF-2) high-resolution\nremote sensing images can greatly improve the accuracy of information\nextraction. This paper introduces an approach using object-oriented\nclassification methods for urban feature extraction based on GF-2 satellite\ndata. A combination of spectral, spatial attributes and membership functions\nwas employed for mapping the urban features of Qinhuai District, Nanjing. The\ndata preprocessing is carried out by ENVI software, and the subsequent data is\nexported into the eCognition software for object-oriented classification and\nextraction of urban feature information. Finally, the obtained raster image\nclassification results are vectorized using the ARCGIS software, and the vector\ngraphics are stored in the library, which can be used for further analysis and\nmodeling. Accuracy assessment was performed using ground truth data acquired by\nvisual interpretation and from other reliable secondary data sources. Compared\nwith the result of pixel-based supervised (neural net) classification, the\ndeveloped object-oriented method can significantly improve extraction accuracy,\nand after manual interpretation, an overall accuracy of 95.44% can be achieved,\nwith a Kappa coefficient of 0.9405, which objectively confirmed the superiority\nof the object-oriented method and the feasibility of the utilization of GF-2\nsatellite data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:19:36 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Zhang", "Dong-dong", ""], ["Zhang", "Lei", ""], ["Zaborovsky", "Vladimir", ""], ["Xie", "Feng", ""], ["Wu", "Yan-wen", ""], ["Lu", "Ting-ting", ""]]}, {"id": "1903.04448", "submitter": "Judith Fan", "authors": "Judith Fan, Robert Hawkins, Mike Wu, Noah Goodman", "title": "Pragmatic inference and visual abstraction enable contextual flexibility\n  during visual communication", "comments": "29 pages; 5 figures; submitted draft of manuscript", "journal-ref": null, "doi": "10.1007/s42113-019-00058-7", "report-no": null, "categories": "cs.OH cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual modes of communication are ubiquitous in modern life --- from maps to\ndata plots to political cartoons. Here we investigate drawing, the most basic\nform of visual communication. Participants were paired in an online environment\nto play a drawing-based reference game. On each trial, both participants were\nshown the same four objects, but in different locations. The sketcher's goal\nwas to draw one of these objects so that the viewer could select it from the\narray. On `close' trials, objects belonged to the same basic-level category,\nwhereas on `far' trials objects belonged to different categories. We found that\npeople exploited shared information to efficiently communicate about the target\nobject: on far trials, sketchers achieved high recognition accuracy while\napplying fewer strokes, using less ink, and spending less time on their\ndrawings than on close trials. We hypothesized that humans succeed in this task\nby recruiting two core faculties: visual abstraction, the ability to perceive\nthe correspondence between an object and a drawing of it; and pragmatic\ninference, the ability to judge what information would help a viewer\ndistinguish the target from distractors. To evaluate this hypothesis, we\ndeveloped a computational model of the sketcher that embodied both faculties,\ninstantiated as a deep convolutional neural network nested within a\nprobabilistic program. We found that this model fit human data well and\noutperformed lesioned variants. Together, this work provides the first\nalgorithmically explicit theory of how visual perception and social cognition\njointly support contextual flexibility in visual communication.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 17:18:16 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 01:06:20 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Fan", "Judith", ""], ["Hawkins", "Robert", ""], ["Wu", "Mike", ""], ["Goodman", "Noah", ""]]}, {"id": "1903.04651", "submitter": "Ruven Pillay", "authors": "Ruven Pillay, Jon Y Hardeberg, Sony George", "title": "Hyperspectral Calibration of Art: Acquisition and Calibration Workflows", "comments": null, "journal-ref": null, "doi": "10.1080/01971360.2018.1549919", "report-no": null, "categories": "eess.IV cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging has become an increasingly used tool in the analysis of\nworks of art. However, the quality of the acquired data and the processing of\nthat data to produce accurate and reproducible spectral image cubes can be a\nchallenge to many cultural heritage users. The calibration of data that is both\nspectrally and spatially accurate is an essential step in order to obtain\nuseful and relevant results from hyperspectral imaging. Data that is too noisy\nor inaccurate will produce sub-optimal results when used for pigment mapping,\nthe detection of hidden features, change detection or for quantitative spectral\ndocumentation. To help address this, therefore, we will examine the specific\nacquisition and calibration workflows necessary for works of art. These\nworkflows includes the key parameters that must be addressed during acquisition\nand the essential steps and issues at each of the stages required during\npost-processing in order to fully calibrate hyperspectral data. In addition we\nwill look in detail at the key issues that affect data quality and propose\npractical solutions that can make significant differences to overall\nhyperspectral image quality.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 23:23:45 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Pillay", "Ruven", ""], ["Hardeberg", "Jon Y", ""], ["George", "Sony", ""]]}, {"id": "1903.06630", "submitter": "Guy Lemieux", "authors": "Guy G.F. Lemieux, Joe Edwards, Joel Vandergriendt, Aaron Severance,\n  Ryan De Iaco, Abdullah Raouf, Hussein Osman, Tom Watzka, Satwant Singh", "title": "TinBiNN: Tiny Binarized Neural Network Overlay in about 5,000 4-LUTs and\n  5mW", "comments": "Presented at 3rd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2017) arXiv:1704.08802", "journal-ref": null, "doi": null, "report-no": "OLAF/2017/06", "categories": "cs.DC cs.CV cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduced-precision arithmetic improves the size, cost, power and performance\nof neural networks in digital logic. In convolutional neural networks, the use\nof 1b weights can achieve state-of-the-art error rates while eliminating\nmultiplication, reducing storage and improving power efficiency. The\nBinaryConnect binary-weighted system, for example, achieves 9.9% error using\nfloating-point activations on the CIFAR-10 dataset. In this paper, we introduce\nTinBiNN, a lightweight vector processor overlay for accelerating inference\ncomputations with 1b weights and 8b activations. The overlay is very small --\nit uses about 5,000 4-input LUTs and fits into a low cost iCE40 UltraPlus FPGA\nfrom Lattice Semiconductor. To show this can be useful, we build two embedded\n'person detector' systems by shrinking the original BinaryConnect network. The\nfirst is a 10-category classifier with a 89% smaller network that runs in\n1,315ms and achieves 13.6% error. The other is a 1-category classifier that is\neven smaller, runs in 195ms, and has only 0.4% error. In both classifiers, the\nerror can be attributed entirely to training and not reduced precision.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 14:51:36 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Lemieux", "Guy G. F.", ""], ["Edwards", "Joe", ""], ["Vandergriendt", "Joel", ""], ["Severance", "Aaron", ""], ["De Iaco", "Ryan", ""], ["Raouf", "Abdullah", ""], ["Osman", "Hussein", ""], ["Watzka", "Tom", ""], ["Singh", "Satwant", ""]]}, {"id": "1903.07134", "submitter": "Daryl DeFord", "authors": "Daryl R. DeFord and Daniel N. Rockmore", "title": "On the Spectrum of Finite, Rooted Homogeneous Trees", "comments": "18 pages, 3 figures. Updated version contains additional results and\n  expanded references. Accepted for publication in Linear Algebra and its\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RT cs.OH math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the adjacency spectrum of families of finite rooted\ntrees with regular branching properties. In particular, we show that in the\ncase of constant branching, the eigenvalues are realized as the roots of a\nfamily of generalized Fibonacci polynomials and produce a limiting distribution\nfor the eigenvalues as the tree depth goes to infinity. We indicate how these\nresults can be extended to periodic branching patterns and also provide a\ngeneralization to higher order simplicial complexes.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 17:19:41 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 17:33:32 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["DeFord", "Daryl R.", ""], ["Rockmore", "Daniel N.", ""]]}, {"id": "1903.08647", "submitter": "John Lawrence Nazareth", "authors": "John Lawrence Nazareth", "title": "Numerical Algorithmic Science and Engineering within Computer Science:\n  Rationale, Foundations and Organization", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A re-calibration is proposed for \"numerical analysis\" as it arises\nspecifically within the broader, embracing field of modern computer science\n(CS). This would facilitate research into theoretical and practicable models of\nreal-number computation at the foundations of CS, and it would also advance the\ninstructional objectives of the CS field. Our approach is premised on the key\nobservation that the great \"watershed\" in numerical computation is much more\nbetween finite- and infinite-dimensional numerical problems than it is between\ndiscrete and continuous numerical problems. A revitalized discipline for\nnumerical computation within modern CS can more accurately be defined as\n\"numerical algorithmic science & engineering (NAS&E), or more compactly, as\n\"numerical algorithmics,\" its focus being the algorithmic solution of numerical\nproblems that are either discrete, or continuous over a space of finite\ndimension, or a combination of the two. It is the counterpart within modern CS\nof the numerical analysis discipline, whose primary focus is the algorithmic\nsolution of continuous, infinite-dimensional numerical problems and their\nfinite-dimensional approximates, and whose specialists today have largely been\nrepatriated to departments of mathematics. Our detailed overview of NAS&E from\nthe viewpoints of rationale, foundations, and organization is preceded by a\nrecounting of the role played by numerical analysts in the evolution of\nacademic departments of computer science, in order to provide background for\nNAS&E and place the newly-emerging discipline within its larger historical\ncontext.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 00:44:25 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Nazareth", "John Lawrence", ""]]}, {"id": "1903.09495", "submitter": "Chen Yuan", "authors": "Jing Hong, Yue Li, Yiran Xu, Chen Yuan, Hong Fan, Guangyi Liu, and\n  Renchang Dai", "title": "Substation One-Line Diagram Automatic Generation and Visualization", "comments": "6 pages, 6 figures, 1 table, accepted by 2019 IEEE PES ISGT ASIA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Energy Management System (EMS) applications and many other off-line\nplanning and study tools, one-line diagram (OLND) of the whole system and\nstations is a straightforward view for planners and operators to design,\nmonitor, analyze, and control the power system. Large-scale power system OLND\nis usually manually developed and maintained. The work is tedious,\ntime-consuming and ease to make mistake. Meanwhile, the manually created\ndiagrams are hard to be shared among the on-line and off-line systems. To save\nthe time and efforts to draw and maintain OLNDs, and provide the capability to\nshare the OLNDs, a tool to automatically develop substation based upon Common\nInformation Model (CIM) standard is needed. Currently, there is no standard\nrule to draw the substation OLND. Besides, the substation layouts can be\naltered from the typical formats in textbooks based on factors of economy,\nefficiency, engineering practice, etc. This paper presents a tool on substation\nOLND automatic generation and visualization. This tool takes the substation\nCIM/E model as input, then automatically computes the coordinates of all\ncomponents and generates the substation OLND based on its components attributes\nand connectivity relations. Evaluation of the proposed approach is presented\nusing a real provincial power system. Over 95\\% of substation OLNDs are\ndecently presented and the rest are corner cases, needing extra effort to do\nspecific reconfiguration.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 20:41:50 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Hong", "Jing", ""], ["Li", "Yue", ""], ["Xu", "Yiran", ""], ["Yuan", "Chen", ""], ["Fan", "Hong", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""]]}, {"id": "1903.09516", "submitter": "Kristian Kersting", "authors": "Kristian Kersting and Jan Peters and Constantin Rothkopf", "title": "Was ist eine Professur fuer Kuenstliche Intelligenz?", "comments": "in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Federal Government of Germany aims to boost the research in the field of\nArtificial Intelligence (AI). For instance, 100 new professorships are said to\nbe established. However, the white paper of the government does not answer what\nan AI professorship is at all. In order to give colleagues, politicians, and\ncitizens an idea, we present a view that is often followed when appointing\nprofessors for AI at German and international universities. We hope that it\nwill help to establish a guideline with internationally accepted measures and\nthus make the public debate more informed.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 14:35:11 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Kersting", "Kristian", ""], ["Peters", "Jan", ""], ["Rothkopf", "Constantin", ""]]}, {"id": "1903.09518", "submitter": "Arthur Gaudron", "authors": "Gaudron Arthur (CAOR)", "title": "Trial of an AI: Empowering people to explore law and science challenges", "comments": null, "journal-ref": "IFIM's International Journal on Law & Regulation of Artificial\n  Intelligence & Robotics, 2019, 1 (1)", "doi": null, "report-no": null, "categories": "cs.OH cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence represents many things: a new market to conquer or a\nquality label for tech companies, a threat for traditional industries, a menace\nfor democracy, or a blessing for our busy everyday life. The press abounds in\nexamples illustrating these aspects, but one should draw not hasty and\npremature conclusions. The first successes in AI have been a surprise for\nsociety at large-including researchers in the field. Today, after the initial\nstupefaction, we have examples of the system reactions: traditional companies\nare heavily investing in AI, social platforms are monitored during elections,\ndata collection is more and more regulated, etc. The resilience of an\norganization (i.e. its capacity to resist to a shock) relies deeply on the\nperception of its environment. Future problems have to be anticipated, while\nunforeseen events occurring have to be quickly identified in order to be\nmitigated as fast as possible. The author states that this clear perception\nstarts with a common definition of AI in terms of capacities and limits. AI\npractitioners should make notions and concepts accessible to the general public\nand the impacted fields (e.g. industries, law, education). It is a truism that\nonly law experts would have the potential to estimate IA impacts on judicial\nsystem. However, questions remain on how to connect different kind of expertise\nand what is the appropriate level of detail required for the knowledge\nexchanges. And the same consideration is true for dissemination towards\nsociety. Ultimately, society will live with decisions made by the \"experts\". It\nsounds wise to involve society in the decision process rather than risking to\npay consequences later. Therefore, society also needs the key concepts to\nunderstand AI impact on their life. This was the purpose of the trial of an IA\nthat took place in October 2018 at the Court of Appeal of Paris: gathering\nexperts from various fields to expose challenges in law and science towards a\ngeneral public.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 07:22:29 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Arthur", "Gaudron", "", "CAOR"]]}, {"id": "1903.09653", "submitter": "Viacheslav Dubeyko", "authors": "Viacheslav Dubeyko", "title": "Anti-Turing Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The invention of CPU-centric computing paradigm was incredible breakthrough\nof computer science that revolutionized our everyday life dramatically.\nHowever, the CPU- centric paradigm is based on the Turing machine concept and,\nas a result, expensive and power-hungry data transferring between the memory\nand CPU core is inevitable operation. Anti-Turing machine paradigm can be based\non two fundamental principles: (1) data-centric computing, and (2)\ndecentralized computing. Anti-Turing machine is able to execute a special type\nof programs. The commands of such program have to be addressed to the 2D or 3D\npersistent memory space is able to process data in-place. This program should\nnot define the position or structure of data but it has to define the goal of\ndata processing activity. Generally speaking, it needs to consider the whole\nmemory space like the data transformation space. But the data placement,\nparticular algorithm implementation, and strategy of algorithm execution are\nout of scope of the program.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 18:03:05 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Dubeyko", "Viacheslav", ""]]}, {"id": "1903.10565", "submitter": "Wenying Ji", "authors": "Wenying Ji", "title": "Simulation-Based Analytics for Fabrication Quality-Associated Decision\n  Support", "comments": null, "journal-ref": null, "doi": "10.7939/R3HX16598", "report-no": null, "categories": "cs.OH stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated, data-driven quality management systems, which facilitate the\ntransformation of data into useable information, are desired to enhance\ndecision-making processes. Integration of accurate, reliable, and\nstraightforward approaches that measure uncertainty of inspection processes are\ninstrumental for the successful implementation of automated, data-driven\nquality management systems. This research has addressed these needs by\nexploring and adapting Bayesian statistics-based approaches for fraction\nnonconforming posterior distribution derivation purposes. Using these accurate\nand reliable inputs, this research further develops novel, analytically-based\napproaches to improve the practical function of traditional construction\nfabrication quality management systems. Multiple descriptive and predictive\nanalytical functionalities are developed to support and augment\nquality-associated decision-making processes. Multi-relational databases (e.g.,\nquality management system, engineering design system, and cost management\nsystem) from an industrial company in Edmonton, Canada, are investigated and\nmapped to implement the novel system proposed. This research has contributed to\nacademic literature and practice by: (1) advancing decision-support systems for\nconstruction management by developing a dynamic simulation environment that\nuses real-time data to enhance simulation predictability; (2) developing\nintegrated analytical methods for improved modeling in fabrication\nquality-associated decision making; and (3) creating reliable and interpretable\ndecision-support metrics for quality performance measurement, complexity\nanalysis, and rework cost management to reduce the data interpretation load of\npractitioners and to uncover valuable knowledge and information from available\ndata sources.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 16:09:41 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Ji", "Wenying", ""]]}, {"id": "1903.12470", "submitter": "Jayant Gupchup A", "authors": "Jayant Gupchup, Yasaman Hosseinkashi, Pavel Dmitriev, Daniel\n  Schneider, Ross Cutler, Andrei Jefremov, Martin Ellis", "title": "Trustworthy Experimentation Under Telemetry Loss", "comments": "Proceedings of the 27th ACM International Conference on Information\n  and Knowledge Management, October 2018", "journal-ref": null, "doi": "10.1145/3269206.3271747", "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failure to accurately measure the outcomes of an experiment can lead to bias\nand incorrect conclusions. Online controlled experiments (aka AB tests) are\nincreasingly being used to make decisions to improve websites as well as mobile\nand desktop applications. We argue that loss of telemetry data (during upload\nor post-processing) can skew the results of experiments, leading to loss of\nstatistical power and inaccurate or erroneous conclusions. By systematically\ninvestigating the causes of telemetry loss, we argue that it is not practical\nto entirely eliminate it. Consequently, experimentation systems need to be\nrobust to its effects. Furthermore, we note that it is nontrivial to measure\nthe absolute level of telemetry loss in an experimentation system. In this\npaper, we take a top-down approach towards solving this problem. We motivate\nthe impact of loss qualitatively using experiments in real applications\ndeployed at scale, and formalize the problem by presenting a theoretical\nbreakdown of the bias introduced by loss. Based on this foundation, we present\na general framework for quantitatively evaluating the impact of telemetry loss,\nand present two solutions to measure the absolute levels of loss. This\nframework is used by well-known applications at Microsoft, with millions of\nusers and billions of sessions. These general principles can be adopted by any\napplication to improve the overall trustworthiness of experimentation and\ndata-driven decision making.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 01:36:01 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Gupchup", "Jayant", ""], ["Hosseinkashi", "Yasaman", ""], ["Dmitriev", "Pavel", ""], ["Schneider", "Daniel", ""], ["Cutler", "Ross", ""], ["Jefremov", "Andrei", ""], ["Ellis", "Martin", ""]]}, {"id": "1903.12582", "submitter": "Petar Radanliev", "authors": "Petar Radanliev, David Charles De Roure, Jason R.C. Nurse, Peter\n  Burnap, Rafael Mantilla Montalvo", "title": "Methodology for Designing Decision Support Systems for Visualising and\n  Mitigating Supply Chain Cyber Risk from IoT Technologies", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.32975.53921", "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a methodology for designing decision support systems for\nvisualising and mitigating the Internet of Things cyber risks. Digital\ntechnologies present new cyber risk in the supply chain which are often not\nvisible to companies participating in the supply chains. This study\ninvestigates how the Internet of Things cyber risks can be visualised and\nmitigated in the process of designing business and supply chain strategies. The\nemerging DSS methodology present new findings on how digital technologies\naffect business and supply chain systems. Through epistemological analysis, the\narticle derives with a decision support system for visualising supply chain\ncyber risk from Internet of Things digital technologies. Such methods do not\nexist at present and this represents the first attempt to devise a decision\nsupport system that would enable practitioners to develop a step by step\nprocess for visualising, assessing and mitigating the emerging cyber risk from\nIoT technologies on shared infrastructure in legacy supply chain systems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 13:42:53 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Radanliev", "Petar", ""], ["De Roure", "David Charles", ""], ["Nurse", "Jason R. C.", ""], ["Burnap", "Peter", ""], ["Montalvo", "Rafael Mantilla", ""]]}]