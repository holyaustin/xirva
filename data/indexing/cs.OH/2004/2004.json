[{"id": "2004.05087", "submitter": "Robert Ehrensperger", "authors": "Robert Ehrensperger, Clemens Sauerwein and Ruth Breu", "title": "Current Practices in the Information Collection for Enterprise\n  Architecture Management", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digital transformation influences business models, processes, and\nenterprise IT landscape as a whole. Therefore, business-IT alignment is\nbecoming more important than ever before. Enterprise architecture management\n(EAM) is designed to support and improve this business-IT alignment. The\nsuccess of EAM crucially depends on the information available about a company's\nenterprise architecture, such as infrastructure components, applications, and\nbusiness processes. This paper discusses the results of a qualitative expert\nsurvey with 26 experts in the field of EAM. The goal of this survey was to\nhighlight current practices in the information collection for EAM and identify\nrelevant information from enterprise-external data sources. The results provide\na comprehensive overview of collected and utilized information in the industry,\nincluding an assessment of the relevance of such information. Furthermore, the\nresults highlight challenges in practice and point out investments that\norganizations plan in the field of EAM.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:38:35 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Ehrensperger", "Robert", ""], ["Sauerwein", "Clemens", ""], ["Breu", "Ruth", ""]]}, {"id": "2004.05193", "submitter": "Johannes Vrana", "authors": "Johannes Vrana", "title": "NDE 4.0: Digital Twin, Semantics, Interfaces, Networking, Feedback, New\n  Markets and Integration into the Industrial Internet of Things", "comments": "24 pages, 12 figures", "journal-ref": "Mater. Eval. 78 (2020) 835-851", "doi": "10.32548/2020.me-04131", "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The industrial revolution is divided into three phases by historians: The\ninvention of the steam engine (mechanization), electricity (mass production)\nand the microelectric revolution (automation). There was a similar development\nin non-destructive evaluation: tools such as lenses or stethoscopes allowed the\nhuman senses to be sharpened, the conversion of waves makes the invisible\nvisible and thus offers a \"look\" into the components and finally automation,\ndigitization and reconstruction. During the entire industrial development NDE\nwas decisively responsible for the quality and thus for the success of the\nmanufactured goods. Industry is now talking about a fourth revolution: The\ninformatization, digitization and networking of industrial production. As\nalways, NDE will be critical to the success of this fourth revolution by\nproviding the database needed for feedback in a networked production\nenvironment. For NDE, this will lead to change. The test results must be made\navailable to a networked production environment in such a way that they can be\nevaluated for feedback loops, the testability must be considered in the design\nand the reliability of the test statements will become increasingly important.\nThis publication presents first an orientation to NDE 4.0, including the\ndevelopment of Industry and NDE, a definition of its revolutions, a collection\nof several current-day challenges of NDE, and a discussion whether and how\nthose can be solved with NDE 4.0. Second this publication presents concepts on\nhow NDE can be integrated into Industry 4.0 landscapes: The Reference\nArchitecture Model Industry 4.0 (RAMI 4.0) shows the complete Industry 4.0\nspace and allows every Industry 4.0 standard and interface to be located. The\nIndustry 4.0 Asset Administration Shell (AAS) implements the digital twin and\nis the interface between Industry 4.0 communication and the physical device.\nThe ...\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:40:55 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Vrana", "Johannes", ""]]}, {"id": "2004.06647", "submitter": "Natasha Jarus", "authors": "Natasha Jarus, Antonio Sabatini, Pratik Maheshwari, Sahra Sedigh\n  Sarvestani", "title": "Software-Based Monitoring and Analysis of a USB Host Controller Subject\n  to Electrostatic Discharge", "comments": "To appear in proceedings of RTEST2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing, understanding, and mitigating the effects of failure in embedded\nsystems is essential for building dependable control systems. We develop a\nsoftware-based monitoring methodology to further this goal. This methodology\ncan be applied to any embedded system peripheral and allows the system to\noperate normally while the monitoring software is running. We use software to\ninstrument the operating system kernel and record indicators of system\nbehavior. By comparing those indicators against baseline indicators of normal\nsystem operation, faults can be detected and appropriate action can be taken.\n  We implement this methodology to detect faults caused by electrostatic\ndischarge in a USB host controller. As indicators, we select specific control\nregisters that provide a manifestation of the internal execution of the host\ncontroller. Analysis of the recorded register values reveals differences in\nsystem execution when the system is subject to interference. %We also develop a\nclassifier capable of predicting whether or not the system's behavior is being\naffected by such shocks. This improved understanding of system behavior may\nlead to better hardware and software mitigation of electrostatic discharge and\nassist in root-cause analysis and repair of failures.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 16:08:52 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Jarus", "Natasha", ""], ["Sabatini", "Antonio", ""], ["Maheshwari", "Pratik", ""], ["Sarvestani", "Sahra Sedigh", ""]]}, {"id": "2004.08817", "submitter": "Tomasz Szydlo", "authors": "Joanna Sendorek, Tomasz Szydlo, Mateusz Windak, Robert Brzoza-Woch", "title": "Dataset for anomalies detection in 3D printing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, Internet of Things plays a significant role in many domains.\nEspecially, Industry 4.0 is making a great usage of concepts like smart sensors\nand big data analysis. IoT devices are commonly used to monitor industry\nmachines and detect anomalies in their work. In this paper we present and\ndescribe a set of data streams coming from working 3D printer. Among others, it\ncontains accelerometer data of printer head, intrusion power and temperatures\nof the printer elements. In order to gain data we lead to several printing\nmalfunctions applied to the 3D model. Resulting dataset can therefore be used\nfor anomalies detection research.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 11:20:21 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Sendorek", "Joanna", ""], ["Szydlo", "Tomasz", ""], ["Windak", "Mateusz", ""], ["Brzoza-Woch", "Robert", ""]]}, {"id": "2004.08961", "submitter": "Omar Al-Kadi", "authors": "Omar S. Al-Kadi", "title": "Knowledge Management Systems Requirements Specifications", "comments": "Online report - University of Canberra (May 2003)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Knowledge Management Systems (KMS) have drawn remarkable\nattention. However, there is no common understanding of how a knowledge\nmanagement system should look like or where the corresponding research should\nbe directed at. Based on a number of essential requirements that a KMS should\nsatisfy, this report introduces some possible requirements for the\ncommonwealth's KMS components forming the KMS architecture. Also, these\nrequirements will be analysed through evaluating and measuring there\nfunctionality to produce a tangible outcome.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 21:08:58 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Al-Kadi", "Omar S.", ""]]}, {"id": "2004.09599", "submitter": "Valerio Pascucci", "authors": "Luca Cinquini, Steve Petruzza, Jason Jerome Boutte, Sasha Ames, Ghaleb\n  Abdulla, Venkatramani Balaji, Robert Ferraro, Aparna Radhakrishnan, Laura\n  Carriere, Thomas Maxwell, Giorgio Scorzelli and Valerio Pascucci", "title": "Distributed Resources for the Earth System Grid Advanced Management\n  (DREAM)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DREAM project was funded more than 3 years ago to design and implement a\nnext-generation ESGF (Earth System Grid Federation [1]) architecture which\nwould be suitable for managing and accessing data and services resources on a\ndistributed and scalable environment. In particular, the project intended to\nfocus on the computing and visualization capabilities of the stack, which at\nthe time were rather primitive. At the beginning, the team had the general\nnotion that a better ESGF architecture could be built by modularizing each\ncomponent, and redefining its interaction with other components by defining and\nexposing a well defined API. Although this was still the high level principle\nthat guided the work, the DREAM project was able to accomplish its goals by\nleveraging new practices in IT that started just about 3 or 4 years ago: the\nadvent of containerization technologies (specifically, Docker), the development\nof frameworks to manage containers at scale (Docker Swarm and Kubernetes), and\ntheir application to the commercial Cloud. Thanks to these new technologies,\nDREAM was able to improve the ESGF architecture (including its computing and\nvisualization services) to a level of deployability and scalability beyond the\noriginal expectations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:13:33 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Cinquini", "Luca", ""], ["Petruzza", "Steve", ""], ["Boutte", "Jason Jerome", ""], ["Ames", "Sasha", ""], ["Abdulla", "Ghaleb", ""], ["Balaji", "Venkatramani", ""], ["Ferraro", "Robert", ""], ["Radhakrishnan", "Aparna", ""], ["Carriere", "Laura", ""], ["Maxwell", "Thomas", ""], ["Scorzelli", "Giorgio", ""], ["Pascucci", "Valerio", ""]]}, {"id": "2004.09796", "submitter": "Mohammed Belkhatir", "authors": "Mohammed Belkhatir, Shalini Bala, Noureddine Belkhatir", "title": "Business Process Re-engineering in Supply Chains Examining the case of\n  the expanding Halal industry", "comments": "International Conference Enterprise Information Systems 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to several issues arising in the rapidly-expanding Halal industry, among\nthem the production of non-genuine or contaminated products and meats, there is\na need to develop effective solutions for ensuring authenticity and quality.\nThis paper proposes the specification of a formalized supply chain framework\nfor the production and monitoring of food and products. The latter enforces\nhigh-level quality of automated monitoring as well as shorter production cycles\nthrough enhanced coordination between the actors and organizations involved.\nOur proposal is guided by business process support to ensure quality and\nefficiency of product development and delivery. It moreover meets the\nrequirements of industrial standards by adopting the Capability Maturity Model\nIntegration highest process maturity level through establishing quantitative\nprocess-improvement objectives, proposing the integrated support of engineering\nprocesses, enforcing synchronization and coordination, drastic monitoring and\nexception handling. We then delve into some of the important technologies from\nthe implementation point-of-view and align it with the formalized Halal\nframework. An Information Technology support instantiation is proposed leading\nto a use case scenario with technology identification.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 18:25:24 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Belkhatir", "Mohammed", ""], ["Bala", "Shalini", ""], ["Belkhatir", "Noureddine", ""]]}, {"id": "2004.09971", "submitter": "Iman Helal", "authors": "Iman M. A. Helal and Ahmed Awad", "title": "Correlating Unlabeled Events at Runtime", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is of great importance for both data-centric and\nprocess-centric systems. Process mining receives so-called process logs which\nare collections of partially-ordered events. An event has to possess at least\nthree attributes, case ID, task ID and a timestamp for mining approaches to\nwork. When a case ID is unknown, the event is called unlabeled. Traditionally,\nprocess mining is an offline task, where events are collected from different\nsources are usually manually correlated. That is, events belonging to the same\ninstance are assigned the same case ID. With today's high-volume/high-speed\nnature of, e.g., IoT applications, process mining shifts to be an online task.\nFor this, event correlation has to be automated and has to occur as the data is\ngenerated. In this paper, we introduce an approach that correlates unlabeled\nevents at runtime. Given a process model, a stream of unlabeled events and\nother information about task duration, our approach can induce a case\nidentifier to a set of unlabeled events with a trust percentage. It can also\ncheck the conformance of the identified cases with the process model. A\nprototype of the proposed approach was implemented and evaluated against\nreal-life and synthetic logs.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 05:02:32 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Helal", "Iman M. A.", ""], ["Awad", "Ahmed", ""]]}, {"id": "2004.10531", "submitter": "Oksana Shadura", "authors": "Oksana Shadura (1), Brian Paul Bockelman (2), Philippe Canal (3),\n  Danilo Piparo (4) and Zhe Zhang (1) ((1) University of Nebraska-Lincoln, (2)\n  Morgridge Institute for Research, (3) Fermilab, (4) CERN)", "title": "ROOT I/O compression improvements for HEP analysis", "comments": "Submitted as a proceeding for CHEP 2019", "journal-ref": null, "doi": "10.1051/epjconf/202024502017", "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We overview recent changes in the ROOT I/O system, increasing performance and\nenhancing it and improving its interaction with other data analysis ecosystems.\nBoth the newly introduced compression algorithms, the much faster bulk I/O data\npath, and a few additional techniques have the potential to significantly to\nimprove experiment's software performance. The need for efficient lossless data\ncompression has grown significantly as the amount of HEP data collected,\ntransmitted, and stored has dramatically increased during the LHC era. While\ncompression reduces storage space and, potentially, I/O bandwidth usage, it\nshould not be applied blindly: there are significant trade-offs between the\nincreased CPU cost for reading and writing files and the reduce storage space.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:35:43 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Shadura", "Oksana", ""], ["Bockelman", "Brian Paul", ""], ["Canal", "Philippe", ""], ["Piparo", "Danilo", ""], ["Zhang", "Zhe", ""]]}, {"id": "2004.11229", "submitter": "Jaeho Choi", "authors": "Jaeho Choi, Seunghyeok Oh, Joongheon Kim", "title": "Quantum Approximation for Wireless Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a quantum approximate optimization algorithm (QAOA)\nmethod for wireless scheduling problems. The QAOA is one of the promising\nhybrid quantum-classical algorithms for many applications and it provides\nhighly accurate optimization solutions in NP-hard problems. QAOA maps the given\nproblems into Hilbert spaces, and then it generates Hamiltonian for the given\nobjectives and constraints. Then, QAOA finds proper parameters from classical\noptimization approaches in order to optimize the expectation value of generated\nHamiltonian. Based on the parameters, the optimal solution to the given problem\ncan be obtained from the optimum of the expectation value of Hamiltonian.\nInspired by QAOA, a quantum approximate optimization for scheduling (QAOS)\nalgorithm is proposed. First of all, this paper formulates a wireless\nscheduling problem using maximum weight independent set (MWIS). Then, for the\ngiven MWIS, the proposed QAOS designs the Hamiltonian of the problem. After\nthat, the iterative QAOS sequence solves the wireless scheduling problem. This\npaper verifies the novelty of the proposed QAOS via simulations implemented by\nCirq and TensorFlow-Quantum.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 13:29:22 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 05:00:57 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Choi", "Jaeho", ""], ["Oh", "Seunghyeok", ""], ["Kim", "Joongheon", ""]]}, {"id": "2004.13784", "submitter": "Ahmadreza Baghaie", "authors": "Dilip Kumar Verma, Ahmadreza Baghaie", "title": "Convolutional Neural Networks vs. Deformable Image Registration For\n  Medical Slice Interpolation", "comments": "Submitted to LISAT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image slice interpolation is an active field of research. The methods\nfor this task can be categorized into two broad groups: intensity-based and\nobject-based interpolation methods. While intensity-based methods are generally\neasier to perform and less computationally expensive, object-based methods are\ncapable of producing more accurate results and account for deformable changes\nin the objects within the slices. In this paper, performance of two well-known\nobject-based interpolation methods is analyzed and compared. Here, a deformable\nregistration-based method specifically designed for medical applications and a\nlearning-based method, trained for video frame interpolation, are considered.\nWhile the deformable registration-based technique is capable of accurate\nmodeling of the changes in the shapes of the objects within slices, the\nlearning-based method is able to produce results with similar accuracy, but\nwith a much sharper appearance in a fraction of the time. This is despite the\nfact that the learning-based approach is not trained on medical images and\nrather is trained using regular video footage. However, experiments show that\nthe method is capable of accurate slice interpolation results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 19:28:26 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Verma", "Dilip Kumar", ""], ["Baghaie", "Ahmadreza", ""]]}]