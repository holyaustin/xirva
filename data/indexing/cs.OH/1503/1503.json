[{"id": "1503.00094", "submitter": "Jingwei Liu", "authors": "Jingwei Liu, Yi Liu, Meizhi Xu", "title": "Parameter Estimation of Jelinski-Moranda Model Based on Weighted\n  Nonlinear Least Squares and Heteroscedasticity", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation method of Jelinski-Moranda (JM) model based on weighted\nnonlinear least squares (WNLS) is proposed. The formulae of resolving the\nparameter WNLS estimation (WNLSE) are derived, and the empirical weight\nfunction and heteroscedasticity problem are discussed. The effects of\noptimization parameter estimation selection based on maximum likelihood\nestimation (MLE) method, least squares estimation (LSE) method and weighted\nnonlinear least squares estimation (WNLSE) method are also investigated. Two\nstrategies of heteroscedasticity decision and weighting methods embedded in JM\nmodel prediction process are also investigated. The experimental results on\nstandard software reliability analysis database-Naval Tactical Data System\n(NTDS) and three datasets used by J.D. Musa demonstrate that WNLSE method can\nbe superior to LSE and MLE under the relative error (RE) criterion.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 07:55:30 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Liu", "Jingwei", ""], ["Liu", "Yi", ""], ["Xu", "Meizhi", ""]]}, {"id": "1503.00208", "submitter": "Zohreh  Ghasemi", "authors": "Zohreh Ghasemi", "title": "Google-based Mode Choice Modeling Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microsimulation based frameworks have become very popular in many research\nareas including travel demand modeling where activity-based models have been in\nthe center of attention for the past decade. Advanced activity-based models\nsynthesize the entire population of the study region and simulate their\nactivities in a way that they can keep track of agents resources as well as\ntheir spatial location. However, the models that are built for these frameworks\ndo not take into account this information mainly because they do not have them\nat the modeling stage. This paper tries to describe the importance of this\ninformation by analyzing a travel survey and generate the actual alternatives\nthat individuals had when making their trips. With a focus on transit, the\nstudy reveals how transit alternatives are limited\\unavailable in certain areas\nwhich must be taken in to account in our mode choice models. Some statistics\nregarding available alternatives and the constraints people encounter when\nmaking a choice are presented with a comprehensive choice set formation. A mode\nchoice model is then developed based on this approach to represent the\nimportance of such information.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 03:16:34 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Ghasemi", "Zohreh", ""]]}, {"id": "1503.00673", "submitter": "Daniele Rotolo", "authors": "Daniele Rotolo, Diana Hicks, Ben R. Martin", "title": "What Is an Emerging Technology?", "comments": "Research Policy (in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is considerable and growing interest in the emergence of novel\ntechnologies, especially from the policy-making perspective. Yet as an area of\nstudy, emerging technologies lacks key foundational elements, namely a\nconsensus on what classifies a technology as 'emergent' and strong research\ndesigns that operationalize central theoretical concepts. The present paper\naims to fill this gap by developing a definition of 'emerging technologies' and\nlinking this conceptual effort with the development of a framework for the\noperationalisation of technological emergence. The definition is developed by\ncombining a basic understanding of the term and in particular the concept of\n'emergence' with a review of key innovation studies dealing with definitional\nissues of technological emergence. The resulting definition identifies five\nattributes that feature in the emergence of novel technologies. These are: (i)\nradical novelty, (ii) relatively fast growth, (iii) coherence, (iv) prominent\nimpact, and (v) uncertainty and ambiguity. The framework for operationalising\nemerging technologies is then elaborated on the basis of the proposed\nattributes. To do so, we identify and review major empirical approaches (mainly\nin, although not limited to, the scientometric domain) for the detection and\nstudy of emerging technologies (these include indicators and trend analysis,\ncitation analysis, co-word analysis, overlay mapping, and combinations thereof)\nand elaborate on how these can be used to operationalise the different\nattributes of emergence.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 17:28:50 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2015 13:21:38 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2015 09:33:47 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2016 13:16:10 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Rotolo", "Daniele", ""], ["Hicks", "Diana", ""], ["Martin", "Ben R.", ""]]}, {"id": "1503.00688", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang", "title": "Photoplethysmography-Based Heart Rate Monitoring in Physical Activities\n  via Joint Sparse Spectrum Reconstruction", "comments": "Published in IEEE Transactions on Biomedical Engineering, Vol. 62,\n  No. 8, PP. 1902-1910, August 2015", "journal-ref": "IEEE Transactions on Biomedical Engineering, Vol. 62, No. 8, PP.\n  1902-1910, August 2015", "doi": "10.1109/TBME.2015.2406332", "report-no": null, "categories": "cs.OH cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal: A new method for heart rate monitoring using photoplethysmography (PPG)\nduring physical activities is proposed. Methods: It jointly estimates spectra\nof PPG signals and simultaneous acceleration signals, utilizing the multiple\nmeasurement vector model in sparse signal recovery. Due to a common sparsity\nconstraint on spectral coefficients, the method can easily identify and remove\nspectral peaks of motion artifact (MA) in PPG spectra. Thus, it does not need\nany extra signal processing modular to remove MA as in some other algorithms.\nFurthermore, seeking spectral peaks associated with heart rate is simplified.\nResults: Experimental results on 12 PPG datasets sampled at 25 Hz and recorded\nduring subjects' fast running showed that it had high performance. The average\nabsolute estimation error was 1.28 beat per minute and the standard deviation\nwas 2.61 beat per minute. Conclusion and Significance: These results show that\nthe method has great potential to be used for PPG-based heart rate monitoring\nin wearable devices for fitness tracking and health monitoring.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 01:48:20 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 06:21:23 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Zhang", "Zhilin", ""]]}, {"id": "1503.00698", "submitter": "Helio M. de Oliveira", "authors": "L.R. Soares and H.M. de Oliveira", "title": "Fault Analysis Using Gegenbauer Multiresolution Analysis", "comments": "6 pages, 12 figures. In: Transmission and Distribution IEEE/PES/T&D\n  Latin America, Sao Paulo, Brazil, 2004", "journal-ref": null, "doi": "10.1109/TDC.2004.1432473", "report-no": null, "categories": "math.CA cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper exploits the multiresolution analysis in the fault analysis on\ntransmission lines. Faults were simulated using the ATP (Alternative Transient\nProgram), considering signals at 128/cycle. A nonorthogonal multiresolution\nanalysis was provided by Gegenbauer scaling and wavelet filters. In the cases\nwhere the signal reconstruction is not required, orthogonality may be\nimmaterial. Gegenbauer filter banks are thereby offered in this paper as a tool\nfor analyzing fault signals on transmission lines. Results are compared to\nthose ones derived from a 4-coefficient Daubechies filter. The main advantages\nin favor of Gegenbauer filters are their smaller computational effort and their\nconstant group delay, as they are symmetric filters.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 14:09:35 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Soares", "L. R.", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1503.02009", "submitter": "Sergio Consoli", "authors": "Sergio Consoli, Jos\\`e Andr\\`es Moreno P\\`erez, and Nenad Mladenovic", "title": "Towards an intelligent VNS heuristic for the k-labelled spanning forest\n  problem", "comments": "2 pages, Fifteenth International Conference on Computer Aided Systems\n  Theory (EUROCAST 2015), Las Palmas de Gran Canaria, Spain", "journal-ref": "Computer Aided Systems Theory, pages 79-80 (2015)", "doi": null, "report-no": null, "categories": "cs.OH cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a currently ongoing project, we investigate a new possibility for solving\nthe k-labelled spanning forest (kLSF) problem by an intelligent Variable\nNeighbourhood Search (Int-VNS) metaheuristic. In the kLSF problem we are given\nan undirected input graph G and an integer positive value k, and the aim is to\nfind a spanning forest of G having the minimum number of connected components\nand the upper bound k on the number of labels to use. The problem is related to\nthe minimum labelling spanning tree (MLST) problem, whose goal is to get the\nspanning tree of the input graph with the minimum number of labels, and has\nseveral applications in the real world, where one aims to ensure connectivity\nby means of homogeneous connections. The Int-VNS metaheuristic that we propose\nfor the kLSF problem is derived from the promising intelligent VNS strategy\nrecently proposed for the MLST problem, and integrates the basic VNS for the\nkLSF problem with other complementary approaches from machine learning,\nstatistics and experimental algorithmics, in order to produce high-quality\nperformance and to completely automate the resulting strategy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 14:10:19 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Consoli", "Sergio", ""], ["P\u00e8rez", "Jos\u00e8 Andr\u00e8s Moreno", ""], ["Mladenovic", "Nenad", ""]]}, {"id": "1503.02656", "submitter": "Kongyang Chen", "authors": "Kongyang Chen and Guang Tan", "title": "Modeling and Improving the Energy Performance of GPS Receivers for\n  Mobile Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated GPS receivers have become a basic module in today's mobile\ndevices. While serving as the cornerstone for location based services, GPS\nmodules have a serious battery drain problem due to high computation load. This\npaper aims to reveal the impact of key software parameters on hardware energy\nconsumption, by establishing an energy model for a standard GPS receiver\narchitecture as found in both academic and industrial designs. In particular,\nour measurements show that the receiver's energy consumption is in large part\nlinear with the number of tracked satellites. This leads to a design of\nselective tracking algorithm that provides similar positioning accuracy (around\n12m) with a subset of selected satellites, which translates to an energy saving\nof 20.9-23.1\\% on the Namuru board.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 06:43:37 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 03:16:23 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Chen", "Kongyang", ""], ["Tan", "Guang", ""]]}, {"id": "1503.02678", "submitter": "Kamran Karimi", "authors": "Kamran Karimi, Diwakar Krishnamurthy, Parissa Mirjafari", "title": "When In-Memory Computing is Slower than Heavy Disk Usage", "comments": "This paper has been withdrawn by the authors for personal reasons. No\n  further revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disk access latency and transfer times are often considered to have a major\nand detrimental impact on the running time of software. Developers are often\nadvised to favour in-memory operations and minimise disk access. Furthermore,\ndiskless computer architectures are being studied and designed to remove this\nbottleneck all together, to improve application performance in areas such as\nHigh Performance Computing, Big Data, and Business Intelligence. In this paper\nwe use code inspired by real, production software, to show that in-memory\noperations are not always a guarantee for high performance, and may actually\ncause a considerable slow-down. We also show how small code changes can have\ndramatic effects on running times. We argue that a combination of system-level\nimprovements and better developer awareness and coding practices are necessary\nto ensure in-memory computing can achieve its full potential.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 05:29:01 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 18:05:24 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Karimi", "Kamran", ""], ["Krishnamurthy", "Diwakar", ""], ["Mirjafari", "Parissa", ""]]}, {"id": "1503.03113", "submitter": "Alexander Dorman", "authors": "A. M. Dorman", "title": "Hardware Probing Interface and Test Robustness", "comments": "7 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computerized integrity test of an electronic product hardware interface and\nproduct probing validation are considered. Integrity testing is based on a\ncurrent voltage characteristic measurement, when a small voltage and/or current\nstimuli are applied to the product pads including power supply circuitry pads,\nso that the product is not normally powered on. Test fixture needles validation\nis a part of a self test maintenance scenario designed to predict deterioration\nof product probing.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 22:06:51 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Dorman", "A. M.", ""]]}, {"id": "1503.03769", "submitter": "Wang Junjie", "authors": "Wang Junjie and Zhao Lingling and Su Xiaohong and Ma Peijun", "title": "Distributed Computation Particle PHD filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle probability hypothesis density filtering has become a promising\nmeans for multi-target tracking due to its capability of handling an unknown\nand time-varying number of targets in non-linear non-Gaussian system. However,\nits computational complexity grows linearly with the number of measurements and\nparticles assigned to each target, and this can be very time consuming\nespecially when numerous targets and clutter exist in the surveillance region.\nAddressing this issue, we present a distributed computation particle PHD filter\nfor target tracking. Its framework consists of several local particle PHD\nfilters at each processing element and a central unit. Each processing element\ntakes responsibility for part particles but full measurements and provides\nlocal estimates; central unit controls particle exchange between processing\nelements and specifies a fusion rule to match and fuse the estimates from\ndifferent local filters. The proposed framework is suitable for parallel\nimplementation and maintains the tracking accuracy. Simulations verify the\nproposed method can provide comparative accuracy as well as a significant\nspeedup with the standard particle PHD filter.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 14:55:05 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Junjie", "Wang", ""], ["Lingling", "Zhao", ""], ["Xiaohong", "Su", ""], ["Peijun", "Ma", ""]]}, {"id": "1503.04006", "submitter": "Jaydeb Bhaumik", "authors": "Jaydeb Bhaumik", "title": "Synthesis of all Maximum Length Cellular Automata of Cell Size up to 12", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum length CA has wide range of applications in design of linear block\ncode, cryptographic primitives and VLSI testing particularly in\nBuilt-In-Self-Test. In this paper, an algorithm to compute all $n$-cell maximum\nlength CA-rule vectors is proposed. Also rule vectors for each primitive\npolynomial in GF(2^2) to GF(2^{12} have been computed by simulation and they\nhave been listed.Programmable rule vectors based maximum length CA can be used\nto design cryptographic primitives.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 12:09:37 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Bhaumik", "Jaydeb", ""]]}, {"id": "1503.04315", "submitter": "Marios Papachristou", "authors": "Marios Papachristou", "title": "Designing and Building a Three-dimensional Projective Scanner for\n  Smartphones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the frustrating things in the digital fabrication era is that its\nmedia are neither affordable nor easily accessible and usable.\nThree-dimensional (3D) fabrication media (DFM) such as 3D Printers and 3D\nScanners have experienced an upsurge in popularity, while the latter remain\nexpensive and hard to function. With this paper, we aim to present you the\nRhoScanner Project - a an affordable and efficient Three-dimensional Projective\nScanner for Smart-phones, hence shedding light on the extended capabilities of\ndigital fabrication media on popular use.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 15:36:10 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Papachristou", "Marios", ""]]}, {"id": "1503.05273", "submitter": "Abhisek Ukil", "authors": "A. Ukil, W. Siti", "title": "Feeder Load Balancing using Fuzzy Logic and Combinatorial\n  Optimization-based Implementation", "comments": "11 pages", "journal-ref": "Electric Power Systems Research, Elsevier, vol. 78, issue 11, pp.\n  1922-1932, 2008", "doi": "10.1016/j.epsr.2008.03.020", "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution system problems, such as planning, loss minimization, and\nenergy restoration, usually involve the phase balancing or network\nreconfiguration procedures. The determination of an optimal phase balance is,\nin general, a combinatorial optimization problem. This paper proposes a novel\nreconfiguration of the phase balancing using the fuzzy logic and the\ncombinatorial optimization-based implementation step back to back. Input to the\nfuzzy step is the total load per phase of the feeders. Output of the fuzzy step\nis the load change values, negative value for load releasing and positive value\nfor load receiving. The output of the fuzzy step is the input to the load\nchanging system. The load changing system uses combinatorial optimization\ntechniques to translate the change values (kW) into number of load points and\nthen selects the specific load points. It also performs the inter-changing of\nthe load points between the releasing and the receiving phases in an optimal\nfashion. Application results using the distribution feeder network of South\nAfrica are presented in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 03:09:30 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ukil", "A.", ""], ["Siti", "W.", ""]]}, {"id": "1503.05287", "submitter": "Abhisek Ukil", "authors": "A. Ukil, R. Zivanovic", "title": "Adjusted Haar Wavelet for Application in the Power Systems Disturbance\n  Analysis", "comments": "13 pages in final printed version", "journal-ref": "Digital Signal Processing, Elsevier, vol. 18, issue 2, pp.\n  103-115, 2008", "doi": "10.1016/j.dsp.2007.04.001", "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abrupt change detection based on the wavelet transform and threshold method\nis very effective in detecting the abrupt changes and hence segmenting the\nsignals recorded during disturbances in the electrical power network. The\nwavelet method estimates the time-instants of the changes in the signal model\nparameters during the pre-fault condition, after initiation of fault, after\ncircuit-breaker opening and auto-reclosure. Certain kinds of disturbance\nsignals do not show distinct abrupt changes in the signal parameters. In those\ncases, the standard mother wavelets fail to achieve correct event-specific\nsegmentations. A new adjustment technique to the standard Haar wavelet is\nproposed in this paper, by introducing 2n adjusting zeros in the Haar wavelet\nscaling filter, n being a positive integer. This technique is quite effective\nin segmenting those fault signals into pre- and post-fault segments, and it is\nan improvement over the standard mother wavelets for this application. This\npaper presents many practical examples where recorded signals from the power\nnetwork in South Africa have been used.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 05:50:39 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Ukil", "A.", ""], ["Zivanovic", "R.", ""]]}, {"id": "1503.05733", "submitter": "Steve Kerrison", "authors": "Steve Kerrison and Kerstin Eder", "title": "A software controlled voltage tuning system using multi-purpose ring\n  oscillators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel software driven voltage tuning method that\nutilises multi-purpose Ring Oscillators (ROs) to provide process variation and\nenvironment sensitive energy reductions. The proposed technique enables voltage\ntuning based on the observed frequency of the ROs, taken as a representation of\nthe device speed and used to estimate a safe minimum operating voltage at a\ngiven core frequency. A conservative linear relationship between RO frequency\nand silicon speed is used to approximate the critical path of the processor.\n  Using a multi-purpose RO not specifically implemented for critical path\ncharacterisation is a unique approach to voltage tuning. The parameters\ngoverning the relationship between RO and silicon speed are obtained through\nthe testing of a sample of processors from different wafer regions. These\nparameters can then be used on all devices of that model. The tuning method and\nsoftware control framework is demonstrated on a sample of XMOS XS1-U8A-64\nembedded microprocessors, yielding a dynamic power saving of up to 25% with no\nperformance reduction and no negative impact on the real-time constraints of\nthe embedded software running on the processor.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 12:16:33 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Kerrison", "Steve", ""], ["Eder", "Kerstin", ""]]}, {"id": "1503.06462", "submitter": "Sgk Patro", "authors": "S. Gopal Krishna Patro and Kishore Kumar Sahu", "title": "Normalization: A Preprocessing Stage", "comments": "4 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we know that the normalization is a pre-processing stage of any type\nproblem statement. Especially normalization takes important role in the field\nof soft computing, cloud computing etc. for manipulation of data like scale\ndown or scale up the range of data before it becomes used for further stage.\nThere are so many normalization techniques are there namely Min-Max\nnormalization, Z-score normalization and Decimal scaling normalization. So by\nreferring these normalization techniques we are going to propose one new\nnormalization technique namely, Integer Scaling Normalization. And we are going\nto show our proposed normalization technique using various data sets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 06:54:53 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Patro", "S. Gopal Krishna", ""], ["Sahu", "Kishore Kumar", ""]]}, {"id": "1503.06618", "submitter": "Abhisek Ukil", "authors": "A. Ukil", "title": "Practical Denoising of MEG Data using Wavelet Transform", "comments": "8 pages. arXiv admin note: text overlap with arXiv:1503.05821", "journal-ref": "Lecture Notes in Computer Science, Springer, vol. 4233, pp.\n  578-585, 2006", "doi": "10.1007/11893257_65", "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetoencephalography (MEG) is an important noninvasive, nonhazardous\ntechnology for functional brain mapping, measuring the magnetic fields due to\nthe intracellular neuronal current flow in the brain. However, the inherent\nlevel of noise in the data collection process is large enough to obscure the\nsignal(s) of interest most often. In this paper, a practical denoising\ntechnique based on the wavelet transform and the multiresolution signal\ndecomposition technique is presented. The proposed technique is substantiated\nby the application results using three different mother wavelets on the\nrecorded MEG signal.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 12:22:39 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Ukil", "A.", ""]]}, {"id": "1503.06729", "submitter": "Jean-Francois Caron", "authors": "Lina Bouhaya (NAVIER UMR 8205), Olivier Baverel, Jean-Fran\\c{c}ois\n  Caron (NAVIER UMR 8205)", "title": "Optimization of gridshell bar orientation using a simplified genetic\n  approach", "comments": null, "journal-ref": "Structural and Multidisciplinary Optimization, Springer Verlag\n  (Germany), 2014, 50 (5), pp. 839-848", "doi": "10.1007/s00158-014-1088-9", "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gridshells are defined as structures that have the shape and rigidity of a\ndouble curvature shell but consist of a grid instead of a continuous surface.\nThis study concerns those obtained by elastic deformation of an initially flat\ntwo-way grid. This paper presents a novel approach to generate gridshells on an\nimposed shape under imposed boundary conditions. A numerical tool based on a\ngeometrical method, the compass method, is developed. It is coupled with\ngenetic algorithms to optimize the orientation of gridshell bars in order to\nminimize the stresses and therefore to avoid bar breakage during the\nconstruction phase. Examples of application are shown.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 19:09:20 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Bouhaya", "Lina", "", "NAVIER UMR 8205"], ["Baverel", "Olivier", "", "NAVIER UMR 8205"], ["Caron", "Jean-Fran\u00e7ois", "", "NAVIER UMR 8205"]]}, {"id": "1503.07016", "submitter": "Eug\\'enio Rodrigues", "authors": "Ana Rita Amaral, Eug\\'enio Rodrigues, Ad\\'elio Rodrigues Gaspar,\n  \\'Alvaro Gomes", "title": "A parametric study of window-to-floor ratio of three window types using\n  dynamic simulation", "comments": "7 pages, 2 figures, Proceedings of Energy for Sustainability 2015\n  Conference: Sustainable Cities: Designing for People and the Planet, Coimbra,\n  14-15 May, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The windows can be responsible for unnecessary energy consumption in a\nbuilding, if incorrectly designed, shadowed or oriented. Considering an annual\nthermal comfort assessment of a space, if windows are over-dimensioned, they\ncan contribute to the increase of the heating needs due to heat losses, and\nalso to the increase of cooling needs due to over-exposure to solar radiation.\nWhen under-dimensioned, the same space may benefit from reduced heat losses\nthrough the glazing surface but does not benefit from solar radiation gains.\nTherefore, it is important to find the optimum design that minimizes both the\nheating and cooling needs. This paper presents a parametric study of window\ntype (single, double and triple glazing), orientation and opening size, located\nin the city of Coimbra, Portugal. An annual and a seasonal assessment were\ndone, in order to obtain the set of optimum values around 360 degree\norientation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 12:41:14 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Amaral", "Ana Rita", ""], ["Rodrigues", "Eug\u00e9nio", ""], ["Gaspar", "Ad\u00e9lio Rodrigues", ""], ["Gomes", "\u00c1lvaro", ""]]}]