[{"id": "1705.00045", "submitter": "Xinyu Hua", "authors": "Xinyu Hua and Lu Wang", "title": "Understanding and Detecting Supporting Arguments of Diverse Types", "comments": "This paper is accepted as a short paper in ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of sentence-level supporting argument detection\nfrom relevant documents for user-specified claims. A dataset containing claims\nand associated citation articles is collected from online debate website\nidebate.org. We then manually label sentence-level supporting arguments from\nthe documents along with their types as study, factual, opinion, or reasoning.\nWe further characterize arguments of different types, and explore whether\nleveraging type information can facilitate the supporting arguments detection\ntask. Experimental results show that LambdaMART (Burges, 2010) ranker that uses\nfeatures informed by argument types yields better performance than the same\nranker trained without type information.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 19:29:54 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 22:00:13 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Hua", "Xinyu", ""], ["Wang", "Lu", ""]]}, {"id": "1705.00105", "submitter": "Yury Maximov", "authors": "Sumit Sidana, Mikhail Trofimov, Oleg Horodnitskii, Charlotte Laclau,\n  Yury Maximov, Massih-Reza Amini", "title": "Representation Learning and Pairwise Ranking for Implicit Feedback in\n  Recommendation Systems", "comments": "12 pages, 4 figures, 5 tables, Modified version contains updated\n  results of all the models and rectifies some of the earlier issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel ranking framework for collaborative\nfiltering with the overall aim of learning user preferences over items by\nminimizing a pairwise ranking loss. We show the minimization problem involves\ndependent random variables and provide a theoretical analysis by proving the\nconsistency of the empirical risk minimization in the worst case where all\nusers choose a minimal number of positive and negative items. We further derive\na Neural-Network model that jointly learns a new representation of users and\nitems in an embedded space as well as the preference relation of users over the\npairs of items. The learning objective is based on three scenarios of ranking\nlosses that control the ability of the model to maintain the ordering over the\nitems induced from the users' preferences, as well as, the capacity of the\ndot-product defined in the learned embedded space to produce the ordering. The\nproposed model is by nature suitable for implicit feedback and involves the\nestimation of only very few parameters. Through extensive experiments on\nseveral real-world benchmarks on implicit data, we show the interest of\nlearning the preference and the embedding simultaneously when compared to\nlearning those separately. We also demonstrate that our approach is very\ncompetitive with the best state-of-the-art collaborative filtering techniques\nproposed for implicit feedback.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 01:03:40 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 09:47:02 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 14:35:37 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 09:31:35 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Sidana", "Sumit", ""], ["Trofimov", "Mikhail", ""], ["Horodnitskii", "Oleg", ""], ["Laclau", "Charlotte", ""], ["Maximov", "Yury", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1705.00106", "submitter": "Xinya Du", "authors": "Xinya Du, Junru Shao and Claire Cardie", "title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "comments": "Accepted to ACL 2017, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study automatic question generation for sentences from text passages in\nreading comprehension. We introduce an attention-based sequence learning model\nfor the task and investigate the effect of encoding sentence- vs.\nparagraph-level information. In contrast to all previous work, our model does\nnot rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead\ntrainable end-to-end via sequence-to-sequence learning. Automatic evaluation\nresults show that our system significantly outperforms the state-of-the-art\nrule-based system. In human evaluations, questions generated by our system are\nalso rated as being more natural (i.e., grammaticality, fluency) and as more\ndifficult to answer (in terms of syntactic and lexical divergence from the\noriginal text and reasoning needed to answer).\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 01:08:48 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Du", "Xinya", ""], ["Shao", "Junru", ""], ["Cardie", "Claire", ""]]}, {"id": "1705.00108", "submitter": "Matthew Peters", "authors": "Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power", "title": "Semi-supervised sequence tagging with bidirectional language models", "comments": "To appear in ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained word embeddings learned from unlabeled text have become a\nstandard component of neural network architectures for NLP tasks. However, in\nmost cases, the recurrent network that operates on word-level representations\nto produce context sensitive representations is trained on relatively little\nlabeled data. In this paper, we demonstrate a general semi-supervised approach\nfor adding pre- trained context embeddings from bidirectional language models\nto NLP systems and apply it to sequence labeling tasks. We evaluate our model\non two standard datasets for named entity recognition (NER) and chunking, and\nin both cases achieve state of the art results, surpassing previous systems\nthat use other forms of transfer or joint learning with additional labeled data\nand task specific gazetteers.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 01:13:04 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Peters", "Matthew E.", ""], ["Ammar", "Waleed", ""], ["Bhagavatula", "Chandra", ""], ["Power", "Russell", ""]]}, {"id": "1705.00217", "submitter": "Mikhail Khodak", "authors": "Mikhail Khodak, Andrej Risteski, Christiane Fellbaum, Sanjeev Arora", "title": "Extending and Improving Wordnet via Unsupervised Word Embeddings", "comments": "17 pages, 3 figures, In Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an unsupervised approach for improving WordNet that builds\nupon recent advances in document and sense representation via distributional\nsemantics. We apply our methods to construct Wordnets in French and Russian,\nlanguages which both lack good manual constructions.1 These are evaluated on\ntwo new 600-word test sets for word-to-synset matching and found to improve\ngreatly upon synset recall, outperforming the best automated Wordnets in\nF-score. Our methods require very few linguistic resources, thus being\napplicable for Wordnet construction in low-resources languages, and may further\nbe applied to sense clustering and other Wordnet improvements.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 17:50:02 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Khodak", "Mikhail", ""], ["Risteski", "Andrej", ""], ["Fellbaum", "Christiane", ""], ["Arora", "Sanjeev", ""]]}, {"id": "1705.00251", "submitter": "Lei Shu", "authors": "Lei Shu, Hu Xu, Bing Liu", "title": "Lifelong Learning CRF for Supervised Aspect Extraction", "comments": "Accepted at ACL 2017. arXiv admin note: text overlap with\n  arXiv:1612.07940", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes a focused contribution to supervised aspect extraction. It\nshows that if the system has performed aspect extraction from many past domains\nand retained their results as knowledge, Conditional Random Fields (CRF) can\nleverage this knowledge in a lifelong learning manner to extract in a new\ndomain markedly better than the traditional CRF without using this prior\nknowledge. The key innovation is that even after CRF training, the model can\nstill improve its extraction with experiences in its applications.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 23:33:13 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Shu", "Lei", ""], ["Xu", "Hu", ""], ["Liu", "Bing", ""]]}, {"id": "1705.00316", "submitter": "Hui Su", "authors": "Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi Niu, Yang Zhao, Akiko\n  Aizawa and Guoping Long", "title": "A Conditional Variational Framework for Dialog Generation", "comments": "Accepted by ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep latent variable models have been shown to facilitate the response\ngeneration for open-domain dialog systems. However, these latent variables are\nhighly randomized, leading to uncontrollable generated responses. In this\npaper, we propose a framework allowing conditional response generation based on\nspecific attributes. These attributes can be either manually assigned or\nautomatically detected. Moreover, the dialog states for both speakers are\nmodeled separately in order to reflect personal features. We validate this\nframework on two different scenarios, where the attribute refers to genericness\nand sentiment states respectively. The experiment result testified the\npotential of our model, where meaningful responses can be generated in\naccordance with the specified attributes.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 13:52:49 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 08:08:53 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 19:58:49 GMT"}, {"version": "v4", "created": "Thu, 6 Jul 2017 09:03:45 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Shen", "Xiaoyu", ""], ["Su", "Hui", ""], ["Li", "Yanran", ""], ["Li", "Wenjie", ""], ["Niu", "Shuzi", ""], ["Zhao", "Yang", ""], ["Aizawa", "Akiko", ""], ["Long", "Guoping", ""]]}, {"id": "1705.00321", "submitter": "Ganbin Zhou", "authors": "Ganbin Zhou, Ping Luo, Rongyu Cao, Yijun Xiao, Fen Lin, Bo Chen, Qing\n  He", "title": "Tree-Structured Neural Machine for Linguistics-Aware Sentence Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from other sequential data, sentences in natural language are\nstructured by linguistic grammars. Previous generative conversational models\nwith chain-structured decoder ignore this structure in human language and might\ngenerate plausible responses with less satisfactory relevance and fluency. In\nthis study, we aim to incorporate the results from linguistic analysis into the\nprocess of sentence generation for high-quality conversation generation.\nSpecifically, we use a dependency parser to transform each response sentence\ninto a dependency tree and construct a training corpus of sentence-tree pairs.\nA tree-structured decoder is developed to learn the mapping from a sentence to\nits tree, where different types of hidden states are used to depict the local\ndependencies from an internal tree node to its children. For training\nacceleration, we propose a tree canonicalization method, which transforms trees\ninto equivalent ternary trees. Then, with a proposed tree-structured search\nmethod, the model is able to generate the most probable responses in the form\nof dependency trees, which are finally flattened into sequences as the system\noutput. Experimental results demonstrate that the proposed X2Tree framework\noutperforms baseline methods over 11.15% increase of acceptance ratio.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 15:09:10 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 14:38:10 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 05:31:23 GMT"}, {"version": "v4", "created": "Wed, 3 Jan 2018 07:35:19 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Zhou", "Ganbin", ""], ["Luo", "Ping", ""], ["Cao", "Rongyu", ""], ["Xiao", "Yijun", ""], ["Lin", "Fen", ""], ["Chen", "Bo", ""], ["He", "Qing", ""]]}, {"id": "1705.00335", "submitter": "Silvio Amir", "authors": "Silvio Amir, Glen Coppersmith, Paula Carvalho, M\\'ario J. Silva, Byron\n  C. Wallace", "title": "Quantifying Mental Health from Social Media with Neural User Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental illnesses adversely affect a significant proportion of the population\nworldwide. However, the methods traditionally used for estimating and\ncharacterizing the prevalence of mental health conditions are time-consuming\nand expensive. Consequently, best-available estimates concerning the prevalence\nof mental health conditions are often years out of date. Automated approaches\nto supplement these survey methods with broad, aggregated information derived\nfrom social media content provides a potential means for near real-time\nestimates at scale. These may, in turn, provide grist for supporting,\nevaluating and iteratively improving upon public health programs and\ninterventions.\n  We propose a novel model for automated mental health status quantification\nthat incorporates user embeddings. This builds upon recent work exploring\nrepresentation learning methods that induce embeddings by leveraging social\nmedia post histories. Such embeddings capture latent characteristics of\nindividuals (e.g., political leanings) and encode a soft notion of homophily.\nIn this paper, we investigate whether user embeddings learned from twitter post\nhistories encode information that correlates with mental health statuses. To\nthis end, we estimated user embeddings for a set of users known to be affected\nby depression and post-traumatic stress disorder (PTSD), and for a set of\ndemographically matched `control' users. We then evaluated these embeddings\nwith respect to: (i) their ability to capture homophilic relations with respect\nto mental health status; and (ii) the performance of downstream mental health\nprediction models based on these features. Our experimental results demonstrate\nthat the user embeddings capture similarities between users with respect to\nmental conditions, and are predictive of mental health.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 16:12:28 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Amir", "Silvio", ""], ["Coppersmith", "Glen", ""], ["Carvalho", "Paula", ""], ["Silva", "M\u00e1rio J.", ""], ["Wallace", "Byron C.", ""]]}, {"id": "1705.00364", "submitter": "John Wieting", "authors": "John Wieting and Kevin Gimpel", "title": "Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings", "comments": "Published as a long paper at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning general-purpose, paraphrastic sentence\nembeddings, revisiting the setting of Wieting et al. (2016b). While they found\nLSTM recurrent networks to underperform word averaging, we present several\ndevelopments that together produce the opposite conclusion. These include\ntraining on sentence pairs rather than phrase pairs, averaging states to\nrepresent sequences, and regularizing aggressively. These improve LSTMs in both\ntransfer learning and supervised settings. We also introduce a new recurrent\narchitecture, the Gated Recurrent Averaging Network, that is inspired by\naveraging and LSTMs while outperforming them both. We analyze our learned\nmodels, finding evidence of preferences for particular parts of speech and\ndependency relations.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 19:18:22 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Wieting", "John", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1705.00390", "submitter": "Ted Pedersen", "authors": "Ted Pedersen", "title": "Duluth at SemEval--2016 Task 14 : Extending Gloss Overlaps to Enrich\n  Semantic Taxonomies", "comments": "4 pages, Appears in the Proceedings of the 10th International\n  Workshop on Semantic Evaluation (SemEval 2016), June 2016, pp. 1328-1331, San\n  Diego, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Duluth systems that participated in Task 14 of\nSemEval 2016, Semantic Taxonomy Enrichment. There were three related systems in\nthe formal evaluation which are discussed here, along with numerous\npost--evaluation runs. All of these systems identified synonyms between WordNet\nand other dictionaries by measuring the gloss overlaps between them. These\nsystems perform better than the random baseline and one post--evaluation\nvariation was within a respectable margin of the median result attained by all\nparticipating systems.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 00:36:59 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Pedersen", "Ted", ""]]}, {"id": "1705.00403", "submitter": "Emma Strubell", "authors": "Emma Strubell and Andrew McCallum", "title": "Dependency Parsing with Dilated Iterated Graph CNNs", "comments": "2nd Workshop on Structured Prediction for Natural Language Processing\n  (at EMNLP '17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependency parses are an effective way to inject linguistic knowledge into\nmany downstream tasks, and many practitioners wish to efficiently parse\nsentences at scale. Recent advances in GPU hardware have enabled neural\nnetworks to achieve significant gains over the previous best models, these\nmodels still fail to leverage GPUs' capability for massive parallelism due to\ntheir requirement of sequential processing of the sentence. In response, we\npropose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for\ngraph-based dependency parsing, a graph convolutional architecture that allows\nfor efficient end-to-end GPU parsing. In experiments on the English Penn\nTreeBank benchmark, we show that DIG-CNNs perform on par with some of the best\nneural network parsers.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 02:39:00 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 21:55:04 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Strubell", "Emma", ""], ["McCallum", "Andrew", ""]]}, {"id": "1705.00424", "submitter": "Meng Fang", "authors": "Meng Fang and Trevor Cohn", "title": "Model Transfer for Tagging Low-resource Languages using a Bilingual\n  Dictionary", "comments": "5 pages with 2 pages reference. Accepted to appear in ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual model transfer is a compelling and popular method for\npredicting annotations in a low-resource language, whereby parallel corpora\nprovide a bridge to a high-resource language and its associated annotated\ncorpora. However, parallel data is not readily available for many languages,\nlimiting the applicability of these approaches. We address these drawbacks in\nour framework which takes advantage of cross-lingual word embeddings trained\nsolely on a high coverage bilingual dictionary. We propose a novel neural\nnetwork model for joint training from both sources of data based on\ncross-lingual word embeddings, and show substantial empirical improvements over\nbaseline techniques. We also propose several active learning heuristics, which\nresult in improvements over competitive benchmark methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 05:58:56 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Fang", "Meng", ""], ["Cohn", "Trevor", ""]]}, {"id": "1705.00440", "submitter": "Marzieh Fadaee", "authors": "Marzieh Fadaee, Arianna Bisazza, Christof Monz", "title": "Data Augmentation for Low-Resource Neural Machine Translation", "comments": "5 pages, 2 figures, Accepted at ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-2090", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of a Neural Machine Translation system depends substantially on\nthe availability of sizable parallel corpora. For low-resource language pairs\nthis is not the case, resulting in poor translation quality. Inspired by work\nin computer vision, we propose a novel data augmentation approach that targets\nlow-frequency words by generating new sentence pairs containing rare words in\nnew, synthetically created contexts. Experimental results on simulated\nlow-resource settings show that our method improves translation quality by up\nto 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 08:12:15 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Fadaee", "Marzieh", ""], ["Bisazza", "Arianna", ""], ["Monz", "Christof", ""]]}, {"id": "1705.00441", "submitter": "Marzieh Fadaee", "authors": "Marzieh Fadaee, Arianna Bisazza, Christof Monz", "title": "Learning Topic-Sensitive Word Representations", "comments": "5 pages, 1 figure, Accepted at ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-2070", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed word representations are widely used for modeling words in NLP\ntasks. Most of the existing models generate one representation per word and do\nnot consider different meanings of a word. We present two approaches to learn\nmultiple topic-sensitive representations per word by using Hierarchical\nDirichlet Process. We observe that by modeling topics and integrating topic\ndistributions for each document we obtain representations that are able to\ndistinguish between different meanings of a given word. Our models yield\nstatistically significant improvements for the lexical substitution task\nindicating that commonly used single word representations, even when combined\nwith contextual information, are insufficient for this task.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 08:16:56 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Fadaee", "Marzieh", ""], ["Bisazza", "Arianna", ""], ["Monz", "Christof", ""]]}, {"id": "1705.00464", "submitter": "Ted Zhang", "authors": "Ted Zhang, Dengxin Dai, Tinne Tuytelaars, Marie-Francine Moens, Luc\n  Van Gool", "title": "Speech-Based Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces speech-based visual question answering (VQA), the task\nof generating an answer given an image and a spoken question. Two methods are\nstudied: an end-to-end, deep neural network that directly uses audio waveforms\nas input versus a pipelined approach that performs ASR (Automatic Speech\nRecognition) on the question, followed by text-based visual question answering.\nFurthermore, we investigate the robustness of both methods by injecting various\nlevels of noise into the spoken question and find both methods to be tolerate\nnoise at similar levels.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 10:43:28 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 03:43:20 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Zhang", "Ted", ""], ["Dai", "Dengxin", ""], ["Tuytelaars", "Tinne", ""], ["Moens", "Marie-Francine", ""], ["Van Gool", "Luc", ""]]}, {"id": "1705.00545", "submitter": "Diego Amancio Dr.", "authors": "Vanessa Q. Marinho, Graeme Hirst, Diego R. Amancio", "title": "Labelled network subgraphs reveal stylistic subtleties in written texts", "comments": "To appear in Journal of Complex Networks (JCN cnx047). The paper is\n  available at https://tinyurl.com/y922r6pw", "journal-ref": null, "doi": "10.1093/comnet/cnx047", "report-no": null, "categories": "cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast amount of data and increase of computational capacity have allowed\nthe analysis of texts from several perspectives, including the representation\nof texts as complex networks. Nodes of the network represent the words, and\nedges represent some relationship, usually word co-occurrence. Even though\nnetworked representations have been applied to study some tasks, such\napproaches are not usually combined with traditional models relying upon\nstatistical paradigms. Because networked models are able to grasp textual\npatterns, we devised a hybrid classifier, called labelled subgraphs, that\ncombines the frequency of common words with small structures found in the\ntopology of the network, known as motifs. Our approach is illustrated in two\ncontexts, authorship attribution and translationese identification. In the\nformer, a set of novels written by different authors is analyzed. To identify\ntranslationese, texts from the Canadian Hansard and the European parliament\nwere classified as to original and translated instances. Our results suggest\nthat labelled subgraphs are able to represent texts and it should be further\nexplored in other tasks, such as the analysis of text complexity, language\nproficiency, and machine translation.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 14:36:21 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 17:09:50 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 02:16:49 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Marinho", "Vanessa Q.", ""], ["Hirst", "Graeme", ""], ["Amancio", "Diego R.", ""]]}, {"id": "1705.00557", "submitter": "Yacine Jernite", "authors": "Yacine Jernite, Samuel R. Bowman and David Sontag", "title": "Discourse-Based Objectives for Fast Unsupervised Sentence Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel objective function for the unsupervised training\nof neural network sentence encoders. It exploits signals from paragraph-level\ndiscourse coherence to train these models to understand text. Our objective is\npurely discriminative, allowing us to train models many times faster than was\npossible under prior methods, and it yields models which perform well in\nextrinsic evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 09:15:35 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Jernite", "Yacine", ""], ["Bowman", "Samuel R.", ""], ["Sontag", "David", ""]]}, {"id": "1705.00571", "submitter": "Andrew Moore", "authors": "Andrew Moore, Paul Rayson", "title": "Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter:\n  predicting sentiment from financial news headlines", "comments": "5 pages, to Appear in the Proceedings of the 11th International\n  Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC", "journal-ref": null, "doi": "10.18653/v1/S17-2095", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes our participation in Task 5 track 2 of SemEval 2017 to\npredict the sentiment of financial news headlines for a specific company on a\ncontinuous scale between -1 and 1. We tackled the problem using a number of\napproaches, utilising a Support Vector Regression (SVR) and a Bidirectional\nLong Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM\nmodel over the SVR and came fourth in the track. We report a number of\ndifferent evaluations using a finance specific word embedding model and reflect\non the effects of using different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 15:57:41 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Moore", "Andrew", ""], ["Rayson", "Paul", ""]]}, {"id": "1705.00581", "submitter": "Arun Balajee Vasudevan", "authors": "Arun Balajee Vasudevan, Michael Gygli, Anna Volokitin and Luc Van Gool", "title": "Query-adaptive Video Summarization via Quality-aware Relevance\n  Estimation", "comments": "ACM Multimedia 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the problem of automatic video summarization has recently received a\nlot of attention, the problem of creating a video summary that also highlights\nelements relevant to a search query has been less studied. We address this\nproblem by posing query-relevant summarization as a video frame subset\nselection problem, which lets us optimise for summaries which are\nsimultaneously diverse, representative of the entire video, and relevant to a\ntext query. We quantify relevance by measuring the distance between frames and\nqueries in a common textual-visual semantic embedding space induced by a neural\nnetwork. In addition, we extend the model to capture query-independent\nproperties, such as frame quality. We compare our method against previous state\nof the art on textual-visual embeddings for thumbnail selection and show that\nour model outperforms them on relevance prediction. Furthermore, we introduce a\nnew dataset, annotated with diversity and query-specific relevance labels. On\nthis dataset, we train and test our complete model for video summarization and\nshow that it outperforms standard baselines such as Maximal Marginal Relevance.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 16:28:18 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 13:18:56 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Vasudevan", "Arun Balajee", ""], ["Gygli", "Michael", ""], ["Volokitin", "Anna", ""], ["Van Gool", "Luc", ""]]}, {"id": "1705.00601", "submitter": "Akrit Mohapatra", "authors": "Aroma Mahendru, Viraj Prabhu, Akrit Mohapatra, Dhruv Batra, Stefan Lee", "title": "The Promise of Premise: Harnessing Question Premises in Visual Question\n  Answering", "comments": "Published at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we make a simple observation that questions about images often\ncontain premises - objects and relationships implied by the question - and that\nreasoning about premises can help Visual Question Answering (VQA) models\nrespond more intelligently to irrelevant or previously unseen questions. When\npresented with a question that is irrelevant to an image, state-of-the-art VQA\nmodels will still answer purely based on learned language biases, resulting in\nnon-sensical or even misleading answers. We note that a visual question is\nirrelevant to an image if at least one of its premises is false (i.e. not\ndepicted in the image). We leverage this observation to construct a dataset for\nQuestion Relevance Prediction and Explanation (QRPE) by searching for false\npremises. We train novel question relevance detection models and show that\nmodels that reason about premises consistently outperform models that do not.\nWe also find that forcing standard VQA models to reason about premises during\ntraining can lead to improvements on tasks requiring compositional reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:41:37 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 18:12:18 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Mahendru", "Aroma", ""], ["Prabhu", "Viraj", ""], ["Mohapatra", "Akrit", ""], ["Batra", "Dhruv", ""], ["Lee", "Stefan", ""]]}, {"id": "1705.00648", "submitter": "William Yang Wang", "authors": "William Yang Wang", "title": "\"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News\n  Detection", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic fake news detection is a challenging problem in deception\ndetection, and it has tremendous real-world political and social impacts.\nHowever, statistical approaches to combating fake news has been dramatically\nlimited by the lack of labeled benchmark datasets. In this paper, we present\nliar: a new, publicly available dataset for fake news detection. We collected a\ndecade-long, 12.8K manually labeled short statements in various contexts from\nPolitiFact.com, which provides detailed analysis report and links to source\ndocuments for each case. This dataset can be used for fact-checking research as\nwell. Notably, this new dataset is an order of magnitude larger than previously\nlargest public fake news datasets of similar type. Empirically, we investigate\nautomatic fake news detection based on surface-level linguistic patterns. We\nhave designed a novel, hybrid convolutional neural network to integrate\nmeta-data with text. We show that this hybrid approach can improve a text-only\ndeep learning model.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 18:20:47 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Wang", "William Yang", ""]]}, {"id": "1705.00652", "submitter": "Matthew Henderson", "authors": "Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-hsuan Sung, Laszlo\n  Lukacs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, Ray Kurzweil", "title": "Efficient Natural Language Response Suggestion for Smart Reply", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a computationally efficient machine-learned method for\nnatural language response suggestion. Feed-forward neural networks using n-gram\nembedding features encode messages into vectors which are optimized to give\nmessage-response pairs a high dot-product value. An optimized search finds\nresponse suggestions. The method is evaluated in a large-scale commercial\ne-mail application, Inbox by Gmail. Compared to a sequence-to-sequence\napproach, the new system achieves the same quality at a small fraction of the\ncomputational requirements and latency.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 18:24:15 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Henderson", "Matthew", ""], ["Al-Rfou", "Rami", ""], ["Strope", "Brian", ""], ["Sung", "Yun-hsuan", ""], ["Lukacs", "Laszlo", ""], ["Guo", "Ruiqi", ""], ["Kumar", "Sanjiv", ""], ["Miklos", "Balint", ""], ["Kurzweil", "Ray", ""]]}, {"id": "1705.00694", "submitter": "Stepan Kuznetsov", "authors": "Max Kanovich, Stepan Kuznetsov, Glyn Morrill, Andre Scedrov", "title": "A polynomial time algorithm for the Lambek calculus with brackets of\n  bounded order", "comments": null, "journal-ref": "Proc. FSCD 2017, LIPIcs vol. 84, 22:1-22:17", "doi": "10.4230/LIPIcs.FSCD.2017.22", "report-no": null, "categories": "cs.LO cs.CL cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lambek calculus is a logical foundation of categorial grammar, a linguistic\nparadigm of grammar as logic and parsing as deduction. Pentus (2010) gave a\npolynomial-time algorithm for determ- ining provability of bounded depth\nformulas in the Lambek calculus with empty antecedents allowed. Pentus'\nalgorithm is based on tabularisation of proof nets. Lambek calculus with\nbrackets is a conservative extension of Lambek calculus with bracket\nmodalities, suitable for the modeling of syntactical domains. In this paper we\ngive an algorithm for provability the Lambek calculus with brackets allowing\nempty antecedents. Our algorithm runs in polynomial time when both the formula\ndepth and the bracket nesting depth are bounded. It combines a Pentus-style\ntabularisation of proof nets with an automata-theoretic treatment of\nbracketing.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 20:12:11 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 10:39:22 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Kanovich", "Max", ""], ["Kuznetsov", "Stepan", ""], ["Morrill", "Glyn", ""], ["Scedrov", "Andre", ""]]}, {"id": "1705.00697", "submitter": "Luis Argerich", "authors": "Juan Andr\\'es Laura, Gabriel Masi, Luis Argerich", "title": "From Imitation to Prediction, Data Compression vs Recurrent Neural\n  Networks for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent studies [1][13][12] Recurrent Neural Networks were used for\ngenerative processes and their surprising performance can be explained by their\nability to create good predictions. In addition, data compression is also based\non predictions. What the problem comes down to is whether a data compressor\ncould be used to perform as well as recurrent neural networks in natural\nlanguage processing tasks. If this is possible,then the problem comes down to\ndetermining if a compression algorithm is even more intelligent than a neural\nnetwork in specific tasks related to human language. In our journey we\ndiscovered what we think is the fundamental difference between a Data\nCompression Algorithm and a Recurrent Neural Network.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 20:23:13 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Laura", "Juan Andr\u00e9s", ""], ["Masi", "Gabriel", ""], ["Argerich", "Luis", ""]]}, {"id": "1705.00746", "submitter": "Nobuhiro Kaji", "authors": "Satoshi Akasaki and Nobuhiro Kaji", "title": "Chat Detection in an Intelligent Assistant: Combining Task-oriented and\n  Non-task-oriented Spoken Dialogue Systems", "comments": "Accepted by ACL2017. The dataset is available at\n  https://research-lab.yahoo.co.jp/en/software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently emerged intelligent assistants on smartphones and home electronics\n(e.g., Siri and Alexa) can be seen as novel hybrids of domain-specific\ntask-oriented spoken dialogue systems and open-domain non-task-oriented ones.\nTo realize such hybrid dialogue systems, this paper investigates determining\nwhether or not a user is going to have a chat with the system. To address the\nlack of benchmark datasets for this task, we construct a new dataset consisting\nof 15; 160 utterances collected from the real log data of a commercial\nintelligent assistant (and will release the dataset to facilitate future\nresearch activity). In addition, we investigate using tweets and Web search\nqueries for handling open-domain user utterances, which characterize the task\nof chat detection. Experiments demonstrated that, while simple supervised\nmethods are effective, the use of the tweets and search queries further\nimproves the F1-score from 86.21 to 87.53.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 00:23:43 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 02:04:15 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Akasaki", "Satoshi", ""], ["Kaji", "Nobuhiro", ""]]}, {"id": "1705.00753", "submitter": "Yun Chen", "authors": "Yun Chen, Yang Liu, Yong Cheng, Victor O.K. Li", "title": "A Teacher-Student Framework for Zero-Resource Neural Machine Translation", "comments": "Accepted as a long paper by ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While end-to-end neural machine translation (NMT) has made remarkable\nprogress recently, it still suffers from the data scarcity problem for\nlow-resource language pairs and domains. In this paper, we propose a method for\nzero-resource NMT by assuming that parallel sentences have close probabilities\nof generating a sentence in a third language. Based on this assumption, our\nmethod is able to train a source-to-target NMT model (\"student\") without\nparallel corpora available, guided by an existing pivot-to-target NMT model\n(\"teacher\") on a source-pivot parallel corpus. Experimental results show that\nthe proposed method significantly improves over a baseline pivot-based model by\n+3.0 BLEU points across various language pairs.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 01:14:06 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Chen", "Yun", ""], ["Liu", "Yang", ""], ["Cheng", "Yong", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1705.00823", "submitter": "Yuya Yoshikawa", "authors": "Yuya Yoshikawa, Yutaro Shigeto, Akikazu Takeuchi", "title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption\n  Dataset", "comments": "Accepted as ACL2017 short paper. 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, automatic generation of image descriptions (captions), that\nis, image captioning, has attracted a great deal of attention. In this paper,\nwe particularly consider generating Japanese captions for images. Since most\navailable caption datasets have been constructed for English language, there\nare few datasets for Japanese. To tackle this problem, we construct a\nlarge-scale Japanese image caption dataset based on images from MS-COCO, which\nis called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions\nfor 164,062 images. In the experiment, we show that a neural network trained\nusing STAIR Captions can generate more natural and better Japanese captions,\ncompared to those generated using English-Japanese machine translation after\ngenerating English captions.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 07:07:55 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Yoshikawa", "Yuya", ""], ["Shigeto", "Yutaro", ""], ["Takeuchi", "Akikazu", ""]]}, {"id": "1705.00861", "submitter": "Mingxuan Wang", "authors": "Mingxuan Wang, Zhengdong Lu, Jie Zhou, Qun Liu", "title": "Deep Neural Machine Translation with Linear Associative Unit", "comments": "10 pages, ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art\nNeural Machine Translation (NMT) with their capability in modeling complex\nfunctions and capturing complex linguistic structures. However NMT systems with\ndeep architecture in their encoder or decoder RNNs often suffer from severe\ngradient diffusion due to the non-linear recurrent activations, which often\nmake the optimization much more difficult. To address this problem we propose\nnovel linear associative units (LAU) to reduce the gradient propagation length\ninside the recurrent unit. Different from conventional approaches (LSTM unit\nand GRU), LAUs utilizes linear associative connections between input and output\nof the recurrent unit, which allows unimpeded information flow through both\nspace and time direction. The model is quite simple, but it is surprisingly\neffective. Our empirical study on Chinese-English translation shows that our\nmodel with proper configuration can improve by 11.7 BLEU upon Groundhog and the\nbest reported results in the same setting. On WMT14 English-German task and a\nlarger WMT14 English-French task, our model achieves comparable results with\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 08:58:17 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Wang", "Mingxuan", ""], ["Lu", "Zhengdong", ""], ["Zhou", "Jie", ""], ["Liu", "Qun", ""]]}, {"id": "1705.00995", "submitter": "Amir Karami", "authors": "Amir Karami and Aryya Gangopadhyay and Bin Zhou and Hadi Kharrazi", "title": "Fuzzy Approach Topic Discovery in Health and Medical Corpora", "comments": "12 Pages, International Journal of Fuzzy Systems, 2017", "journal-ref": null, "doi": "10.1007/s40815-017-0327-9", "report-no": null, "categories": "stat.ML cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of medical documents and electronic health records (EHRs) are in\ntext format that poses a challenge for data processing and finding relevant\ndocuments. Looking for ways to automatically retrieve the enormous amount of\nhealth and medical knowledge has always been an intriguing topic. Powerful\nmethods have been developed in recent years to make the text processing\nautomatic. One of the popular approaches to retrieve information based on\ndiscovering the themes in health & medical corpora is topic modeling, however,\nthis approach still needs new perspectives. In this research we describe fuzzy\nlatent semantic analysis (FLSA), a novel approach in topic modeling using fuzzy\nperspective. FLSA can handle health & medical corpora redundancy issue and\nprovides a new method to estimate the number of topics. The quantitative\nevaluations show that FLSA produces superior performance and features to latent\nDirichlet allocation (LDA), the most popular topic model.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 14:29:14 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 02:41:00 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Karami", "Amir", ""], ["Gangopadhyay", "Aryya", ""], ["Zhou", "Bin", ""], ["Kharrazi", "Hadi", ""]]}, {"id": "1705.01020", "submitter": "Junhui Li", "authors": "Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min Zhang, Guodong Zhou", "title": "Modeling Source Syntax for Neural Machine Translation", "comments": "Accepted by ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though a linguistics-free sequence to sequence model in neural machine\ntranslation (NMT) has certain capability of implicitly learning syntactic\ninformation of source sentences, this paper shows that source syntax can be\nexplicitly incorporated into NMT effectively to provide further improvements.\nSpecifically, we linearize parse trees of source sentences to obtain structural\nlabel sequences. On the basis, we propose three different sorts of encoders to\nincorporate source syntax into NMT: 1) Parallel RNN encoder that learns word\nand label annotation vectors parallelly; 2) Hierarchical RNN encoder that\nlearns word and label annotation vectors in a two-level hierarchy; and 3) Mixed\nRNN encoder that stitchingly learns word and label annotation vectors over\nsequences where words and labels are mixed. Experimentation on\nChinese-to-English translation demonstrates that all the three proposed\nsyntactic encoders are able to improve translation accuracy. It is interesting\nto note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best\nperformance with an significant improvement of 1.4 BLEU points. Moreover, an\nin-depth analysis from several perspectives is provided to reveal how source\nsyntax benefits NMT.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 15:21:46 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Li", "Junhui", ""], ["Xiong", "Deyi", ""], ["Tu", "Zhaopeng", ""], ["Zhu", "Muhua", ""], ["Zhang", "Min", ""], ["Zhou", "Guodong", ""]]}, {"id": "1705.01042", "submitter": "Weiqian Yan", "authors": "Weiqian Yan, Kanchan Khurad", "title": "Entity Linking with people entity on Wikipedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new model that uses named entity recognition,\ncoreference resolution, and entity linking techniques, to approach the task of\nlinking people entities on Wikipedia people pages to their corresponding\nWikipedia pages if applicable. Our task is different from general and\ntraditional entity linking because we are working in a limited domain, namely,\npeople entities, and we are including pronouns as entities, whereas in the\npast, pronouns were never considered as entities in entity linking. We have\nbuilt 2 models, both outperforms our baseline model significantly. The purpose\nof our project is to build a model that could be use to generate cleaner data\nfor future entity linking tasks. Our contribution include a clean data set\nconsisting of 50Wikipedia people pages, and 2 entity linking models,\nspecifically tuned for this domain.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 16:06:03 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Yan", "Weiqian", ""], ["Khurad", "Kanchan", ""]]}, {"id": "1705.01214", "submitter": "Maira Gatti de Bayser", "authors": "Maira Gatti de Bayser, Paulo Cavalin, Renan Souza, Alan Braz, Heloisa\n  Candello, Claudio Pinhanez, Jean-Pierre Briot", "title": "A Hybrid Architecture for Multi-Party Conversational Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-party Conversational Systems are systems with natural language\ninteraction between one or more people or systems. From the moment that an\nutterance is sent to a group, to the moment that it is replied in the group by\na member, several activities must be done by the system: utterance\nunderstanding, information search, reasoning, among others. In this paper we\npresent the challenges of designing and building multi-party conversational\nsystems, the state of the art, our proposed hybrid architecture using both\nrules and machine learning and some insights after implementing and evaluating\none on the finance domain.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 01:05:14 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 18:28:13 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["de Bayser", "Maira Gatti", ""], ["Cavalin", "Paulo", ""], ["Souza", "Renan", ""], ["Braz", "Alan", ""], ["Candello", "Heloisa", ""], ["Pinhanez", "Claudio", ""], ["Briot", "Jean-Pierre", ""]]}, {"id": "1705.01253", "submitter": "Hongyang Xue", "authors": "Hongyang Xue, Zhou Zhao, Deng Cai", "title": "The Forgettable-Watcher Model for Video Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of visual question answering approaches have been proposed recently,\naiming at understanding the visual scenes by answering the natural language\nquestions. While the image question answering has drawn significant attention,\nvideo question answering is largely unexplored.\n  Video-QA is different from Image-QA since the information and the events are\nscattered among multiple frames. In order to better utilize the temporal\nstructure of the videos and the phrasal structures of the answers, we propose\ntwo mechanisms: the re-watching and the re-reading mechanisms and combine them\ninto the forgettable-watcher model. Then we propose a TGIF-QA dataset for video\nquestion answering with the help of automatic question generation. Finally, we\nevaluate the models on our dataset. The experimental results show the\neffectiveness of our proposed models.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 04:46:33 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Xue", "Hongyang", ""], ["Zhao", "Zhou", ""], ["Cai", "Deng", ""]]}, {"id": "1705.01265", "submitter": "Georgios Balikas", "authors": "Georgios Balikas, Ioannis Partalas", "title": "On the effectiveness of feature set augmentation using clusters of word\n  embeddings", "comments": "SwissText 2018; oral presentations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word clusters have been empirically shown to offer important performance\nimprovements on various tasks. Despite their importance, their incorporation in\nthe standard pipeline of feature engineering relies more on a trial-and-error\nprocedure where one evaluates several hyper-parameters, like the number of\nclusters to be used. In order to better understand the role of such features we\nsystematically evaluate their effect on four tasks, those of named entity\nsegmentation and classification as well as, those of five-point sentiment\nclassification and quantification. Our results strongly suggest that cluster\nmembership features improve the performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 06:33:37 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 10:38:19 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Balikas", "Georgios", ""], ["Partalas", "Ioannis", ""]]}, {"id": "1705.01306", "submitter": "Daniel Fleischer", "authors": "Alon Rozental, Daniel Fleischer", "title": "Amobee at SemEval-2017 Task 4: Deep Learning System for Sentiment\n  Detection on Twitter", "comments": "6 pages, accepted to the 11th International Workshop on Semantic\n  Evaluation (SemEval-2017)", "journal-ref": null, "doi": "10.18653/v1/S17-2108", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Amobee sentiment analysis system, adapted to compete\nin SemEval 2017 task 4. The system consists of two parts: a supervised training\nof RNN models based on a Twitter sentiment treebank, and the use of feedforward\nNN, Naive Bayes and logistic regression classifiers to produce predictions for\nthe different sub-tasks. The algorithm reached the 3rd place on the 5-label\nclassification task (sub-task C).\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 08:50:56 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Rozental", "Alon", ""], ["Fleischer", "Daniel", ""]]}, {"id": "1705.01346", "submitter": "Danhao Zhu", "authors": "Danhao Zhu, Si Shen, Xin-Yu Dai and Jiajun Chen", "title": "Going Wider: Recurrent Neural Network With Parallel Cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) has been widely applied for sequence modeling.\nIn RNN, the hidden states at current step are full connected to those at\nprevious step, thus the influence from less related features at previous step\nmay potentially decrease model's learning ability. We propose a simple\ntechnique called parallel cells (PCs) to enhance the learning ability of\nRecurrent Neural Network (RNN). In each layer, we run multiple small RNN cells\nrather than one single large cell. In this paper, we evaluate PCs on 2 tasks.\nOn language modeling task on PTB (Penn Tree Bank), our model outperforms state\nof art models by decreasing perplexity from 78.6 to 75.3. On Chinese-English\ntranslation task, our model increases BLEU score for 0.39 points than baseline\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 10:22:22 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Zhu", "Danhao", ""], ["Shen", "Si", ""], ["Dai", "Xin-Yu", ""], ["Chen", "Jiajun", ""]]}, {"id": "1705.01359", "submitter": "Moin Nabi", "authors": "Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurelie Herbelot,\n  Moin Nabi, Enver Sangineto, Raffaella Bernardi", "title": "FOIL it! Find One mismatch between Image and Language caption", "comments": "To appear at ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-1024", "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to understand whether current language and vision\n(LaVi) models truly grasp the interaction between the two modalities. To this\nend, we propose an extension of the MSCOCO dataset, FOIL-COCO, which associates\nimages with both correct and \"foil\" captions, that is, descriptions of the\nimage that are highly similar to the original ones, but contain one single\nmistake (\"foil word\"). We show that current LaVi models fall into the traps of\nthis data and perform badly on three tasks: a) caption classification (correct\nvs. foil); b) foil word detection; c) foil word correction. Humans, in\ncontrast, have near-perfect performance on those tasks. We demonstrate that\nmerely utilising language cues is not enough to model FOIL-COCO and that it\nchallenges the state-of-the-art by requiring a fine-grained understanding of\nthe relation between text and image.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 11:07:13 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Shekhar", "Ravi", ""], ["Pezzelle", "Sandro", ""], ["Klimovich", "Yauhen", ""], ["Herbelot", "Aurelie", ""], ["Nabi", "Moin", ""], ["Sangineto", "Enver", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1705.01452", "submitter": "Zhaopeng Tu", "authors": "Hao Zhou, Zhaopeng Tu, Shujian Huang, Xiaohua Liu, Hang Li, Jiajun\n  Chen", "title": "Chunk-Based Bi-Scale Decoder for Neural Machine Translation", "comments": "Accepted as a short paper by ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In typical neural machine translation~(NMT), the decoder generates a sentence\nword by word, packing all linguistic granularities in the same time-scale of\nRNN. In this paper, we propose a new type of decoder for NMT, which splits the\ndecode state into two parts and updates them in two different time-scales.\nSpecifically, we first predict a chunk time-scale state for phrasal modeling,\non top of which multiple word time-scale states are generated. In this way, the\ntarget sentence is translated hierarchically from chunks to words, with\ninformation in different granularities being leveraged. Experiments show that\nour proposed model significantly improves the translation performance over the\nstate-of-the-art NMT model.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 14:39:56 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Zhou", "Hao", ""], ["Tu", "Zhaopeng", ""], ["Huang", "Shujian", ""], ["Liu", "Xiaohua", ""], ["Li", "Hang", ""], ["Chen", "Jiajun", ""]]}, {"id": "1705.01684", "submitter": "Ryan Cotterell Ryan D Cotterell", "authors": "Ryan Cotterell and Jason Eisner", "title": "Probabilistic Typology: Deep Generative Models of Vowel Inventories", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic typology studies the range of structures present in human\nlanguage. The main goal of the field is to discover which sets of possible\nphenomena are universal, and which are merely frequent. For example, all\nlanguages have vowels, while most---but not all---languages have an /u/ sound.\nIn this paper we present the first probabilistic treatment of a basic question\nin phonological typology: What makes a natural vowel inventory? We introduce a\nseries of deep stochastic point processes, and contrast them with previous\ncomputational, simulation-based approaches. We provide a comprehensive suite of\nexperiments on over 200 distinct languages.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 03:13:03 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Cotterell", "Ryan", ""], ["Eisner", "Jason", ""]]}, {"id": "1705.01833", "submitter": "Somnath Roy", "authors": "Somnath Roy", "title": "A Finite State and Rule-based Akshara to Prosodeme (A2P) Converter in\n  Hindi", "comments": "If you need software (A2P Converter), you have to write for the same\n  at \"somnathroy86@gmail.com\" or \"somnat75_llh@jnu.ac.in\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a software module called Akshara to Prosodeme (A2P)\nconverter in Hindi. It converts an input grapheme into prosedeme (sequence of\nphonemes with the specification of syllable boundaries and prosodic labels).\nThe software is based on two proposed finite state machines\\textemdash one for\nthe syllabification and another for the syllable labeling. In addition to that,\nit also uses a set of nonlinear phonological rules proposed for foot formation\nin Hindi, which encompass solutions to schwa-deletion in simple, compound,\nderived and inflected words. The nonlinear phonological rules are based on\nmetrical phonology with the provision of recursive foot structure. A software\nmodule is implemented in Python. The testing of the software for\nsyllabification, syllable labeling, schwa deletion and prosodic labeling yield\nan accuracy of more than 99% on a lexicon of size 28664 words.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 13:33:00 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Roy", "Somnath", ""]]}, {"id": "1705.01991", "submitter": "Jacob Devlin", "authors": "Jacob Devlin", "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine\n  Translation Decoding on the CPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attentional sequence-to-sequence models have become the new standard for\nmachine translation, but one challenge of such models is a significant increase\nin training and decoding cost compared to phrase-based systems. Here, we focus\non efficient decoding, with a goal of achieving accuracy close the\nstate-of-the-art in neural machine translation (NMT), while achieving CPU\ndecoding speed/throughput close to that of a phrasal decoder.\n  We approach this problem from two angles: First, we describe several\ntechniques for speeding up an NMT beam search decoder, which obtain a 4.4x\nspeedup over a very efficient baseline decoder without changing the decoder\noutput. Second, we propose a simple but powerful network architecture which\nuses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked\nfully-connected layers applied at every timestep. This architecture achieves\nsimilar accuracy to a deep recurrent model, at a small fraction of the training\nand decoding cost. By combining these techniques, our best system achieves a\nvery competitive accuracy of 38.3 BLEU on WMT English-French NewsTest2014,\nwhile decoding at 100 words/sec on single-threaded CPU. We believe this is the\nbest published accuracy/speed trade-off of an NMT system.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 19:50:35 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Devlin", "Jacob", ""]]}, {"id": "1705.02012", "submitter": "Tong Wang", "authors": "Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessandro Sordoni, Philip\n  Bachman, Sandeep Subramanian, Saizheng Zhang, Adam Trischler", "title": "Machine Comprehension by Text-to-Text Neural Question Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a recurrent neural model that generates natural-language questions\nfrom documents, conditioned on answers. We show how to train the model using a\ncombination of supervised and reinforcement learning. After teacher forcing for\nstandard maximum likelihood training, we fine-tune the model using policy\ngradient techniques to maximize several rewards that measure question quality.\nMost notably, one of these rewards is the performance of a question-answering\nsystem. We motivate question generation as a means to improve the performance\nof question answering systems. Our model is trained and evaluated on the recent\nquestion-answering dataset SQuAD.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 20:58:06 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 14:47:05 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Yuan", "Xingdi", ""], ["Wang", "Tong", ""], ["Gulcehre", "Caglar", ""], ["Sordoni", "Alessandro", ""], ["Bachman", "Philip", ""], ["Subramanian", "Sandeep", ""], ["Zhang", "Saizheng", ""], ["Trischler", "Adam", ""]]}, {"id": "1705.02023", "submitter": "Hussam Hamdan", "authors": "Hussam Hamdan", "title": "Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters\n  for Tweet Polarity Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Senti17 system which uses ten convolutional neural\nnetworks (ConvNet) to assign a sentiment label to a tweet. The network consists\nof a convolutional layer followed by a fully-connected layer and a Softmax on\ntop. Ten instances of this network are initialized with the same word\nembeddings as inputs but with different initializations for the network\nweights. We combine the results of all instances by selecting the sentiment\nlabel given by the majority of the ten voters. This system is ranked fourth in\nSemEval-2017 Task4 over 38 systems with 67.4%\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 21:13:24 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Hamdan", "Hussam", ""]]}, {"id": "1705.02073", "submitter": "Ruochen Xu", "authors": "Ruochen Xu and Yiming Yang", "title": "Cross-lingual Distillation for Text Classification", "comments": "Accepted at ACL 2017; Code available at\n  https://github.com/xrc10/cross-distill", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual text classification(CLTC) is the task of classifying documents\nwritten in different languages into the same taxonomy of categories. This paper\npresents a novel approach to CLTC that builds on model distillation, which\nadapts and extends a framework originally proposed for model compression. Using\nsoft probabilistic predictions for the documents in a label-rich language as\nthe (induced) supervisory labels in a parallel corpus of documents, we train\nclassifiers successfully for new languages in which labeled training data are\nnot available. An adversarial feature adaptation technique is also applied\nduring the model training to reduce distribution mismatch. We conducted\nexperiments on two benchmark CLTC datasets, treating English as the source\nlanguage and German, French, Japan and Chinese as the unlabeled target\nlanguages. The proposed approach had the advantageous or comparable performance\nof the other state-of-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 03:36:11 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 01:14:28 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Xu", "Ruochen", ""], ["Yang", "Yiming", ""]]}, {"id": "1705.02077", "submitter": "Mengxue Li", "authors": "Mengxue Li, Shiqiang Geng, Yang Gao, Haijing Liu, Hao Wang", "title": "Crowdsourcing Argumentation Structures in Chinese Hotel Reviews", "comments": "6 pages,3 figures,This article has been submitted to \"The 2017 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC2017)\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argumentation mining aims at automatically extracting the premises-claim\ndiscourse structures in natural language texts. There is a great demand for\nargumentation corpora for customer reviews. However, due to the controversial\nnature of the argumentation annotation task, there exist very few large-scale\nargumentation corpora for customer reviews. In this work, we novelly use the\ncrowdsourcing technique to collect argumentation annotations in Chinese hotel\nreviews. As the first Chinese argumentation dataset, our corpus includes 4814\nargument component annotations and 411 argument relation annotations, and its\nannotations qualities are comparable to some widely used argumentation corpora\nin other languages.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 03:43:35 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Li", "Mengxue", ""], ["Geng", "Shiqiang", ""], ["Gao", "Yang", ""], ["Liu", "Haijing", ""], ["Wang", "Hao", ""]]}, {"id": "1705.02131", "submitter": "Minglan Li", "authors": "Minglan Li, Yang Gao, Hui Wen, Yang Du, Haijing Liu and Hao Wang", "title": "Joint RNN Model for Argument Component Boundary Detection", "comments": "6 pages, 3 figures, submitted to IEEE SMC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argument Component Boundary Detection (ACBD) is an important sub-task in\nargumentation mining; it aims at identifying the word sequences that constitute\nargument components, and is usually considered as the first sub-task in the\nargumentation mining pipeline. Existing ACBD methods heavily depend on\ntask-specific knowledge, and require considerable human efforts on\nfeature-engineering. To tackle these problems, in this work, we formulate ACBD\nas a sequence labeling problem and propose a variety of Recurrent Neural\nNetwork (RNN) based methods, which do not use domain specific or handcrafted\nfeatures beyond the relative position of the sentence in the document. In\nparticular, we propose a novel joint RNN model that can predict whether\nsentences are argumentative or not, and use the predicted results to more\nprecisely detect the argument component boundaries. We evaluate our techniques\non two corpora from two different genres; results suggest that our joint RNN\nmodel obtain the state-of-the-art performance on both datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 08:49:14 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Li", "Minglan", ""], ["Gao", "Yang", ""], ["Wen", "Hui", ""], ["Du", "Yang", ""], ["Liu", "Haijing", ""], ["Wang", "Hao", ""]]}, {"id": "1705.02203", "submitter": "Tesfamariam Mulugeta Abuhay", "authors": "Tesfamariam M. Abuhay, Sergey V. Kovalchuk, Klavdiya O. Bochenina,\n  George Kampis, Valeria V. Krzhizhanovskaya, Michael H. Lees", "title": "Analysis of Computational Science Papers from ICCS 2001-2016 using Topic\n  Modeling and Graph Theory", "comments": "Accepted by International Conference on Computational Science (ICCS)\n  2017 which will be held in Zurich, Switzerland from June 11-June 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents results of topic modeling and network models of topics\nusing the International Conference on Computational Science corpus, which\ncontains domain-specific (computational science) papers over sixteen years (a\ntotal of 5695 papers). We discuss topical structures of International\nConference on Computational Science, how these topics evolve over time in\nresponse to the topicality of various problems, technologies and methods, and\nhow all these topics relate to one another. This analysis illustrates\nmultidisciplinary research and collaborations among scientific communities, by\nconstructing static and dynamic networks from the topic modeling results and\nthe keywords of authors. The results of this study give insights about the past\nand future trends of core discussion topics in computational science. We used\nthe Non-negative Matrix Factorization topic modeling algorithm to discover\ntopics and labeled and grouped results hierarchically.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 13:24:41 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Abuhay", "Tesfamariam M.", ""], ["Kovalchuk", "Sergey V.", ""], ["Bochenina", "Klavdiya O.", ""], ["Kampis", "George", ""], ["Krzhizhanovskaya", "Valeria V.", ""], ["Lees", "Michael H.", ""]]}, {"id": "1705.02269", "submitter": "Sebastian Brarda", "authors": "Sebastian Brarda, Philip Yeres, Samuel R. Bowman", "title": "Sequential Attention: A Context-Aware Alignment Function for Machine\n  Reading", "comments": "To appear in ACL 2017 2nd Workshop on Representation Learning for\n  NLP. Contains additional experiments in section 4 and a revised Figure 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a neural network model with a novel Sequential\nAttention layer that extends soft attention by assigning weights to words in an\ninput sequence in a way that takes into account not just how well that word\nmatches a query, but how well surrounding words match. We evaluate this\napproach on the task of reading comprehension (on the Who did What and CNN\ndatasets) and show that it dramatically improves a strong baseline--the\nStanford Reader--and is competitive with the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 15:37:11 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 22:25:55 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Brarda", "Sebastian", ""], ["Yeres", "Philip", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1705.02304", "submitter": "Chao Li", "authors": "Chao Li, Xiaokong Ma, Bing Jiang, Xiangang Li, Xuewei Zhang, Xiao Liu,\n  Ying Cao, Ajay Kannan, Zhenyao Zhu", "title": "Deep Speaker: an End-to-End Neural Speaker Embedding System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deep Speaker, a neural speaker embedding system that maps\nutterances to a hypersphere where speaker similarity is measured by cosine\nsimilarity. The embeddings generated by Deep Speaker can be used for many\ntasks, including speaker identification, verification, and clustering. We\nexperiment with ResCNN and GRU architectures to extract the acoustic features,\nthen mean pool to produce utterance-level speaker embeddings, and train using\ntriplet loss based on cosine similarity. Experiments on three distinct datasets\nsuggest that Deep Speaker outperforms a DNN-based i-vector baseline. For\nexample, Deep Speaker reduces the verification equal error rate by 50%\n(relatively) and improves the identification accuracy by 60% (relatively) on a\ntext-independent dataset. We also present results that suggest adapting from a\nmodel trained with Mandarin can improve accuracy for English speaker\nrecognition.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 17:10:16 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Li", "Chao", ""], ["Ma", "Xiaokong", ""], ["Jiang", "Bing", ""], ["Li", "Xiangang", ""], ["Zhang", "Xuewei", ""], ["Liu", "Xiao", ""], ["Cao", "Ying", ""], ["Kannan", "Ajay", ""], ["Zhu", "Zhenyao", ""]]}, {"id": "1705.02314", "submitter": "Burcu Can", "authors": "Serkan Ozen, Burcu Can", "title": "Building Morphological Chains for Agglutinative Languages", "comments": "10 pages, accepted and presented at the CICLing 2017 (18th\n  International Conference on Intelligent Text Processing and Computational\n  Linguistics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build morphological chains for agglutinative languages by\nusing a log-linear model for the morphological segmentation task. The model is\nbased on the unsupervised morphological segmentation system called\nMorphoChains. We extend MorphoChains log linear model by expanding the\ncandidate space recursively to cover more split points for agglutinative\nlanguages such as Turkish, whereas in the original model candidates are\ngenerated by considering only binary segmentation of each word. The results\nshow that we improve the state-of-art Turkish scores by 12% having a F-measure\nof 72% and we improve the English scores by 3% having a F-measure of 74%.\nEventually, the system outperforms both MorphoChains and other well-known\nunsupervised morphological segmentation systems. The results indicate that\ncandidate generation plays an important role in such an unsupervised log-linear\nmodel that is learned using contrastive estimation with negative samples.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 17:30:50 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Ozen", "Serkan", ""], ["Can", "Burcu", ""]]}, {"id": "1705.02315", "submitter": "Xiaosong Wang", "authors": "Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri and\n  Ronald M. Summers", "title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on\n  Weakly-Supervised Classification and Localization of Common Thorax Diseases", "comments": "CVPR 2017 spotlight;V1: CVPR submission+supplementary; V2: Statistics\n  and benchmark results on published ChestX-ray14 dataset are updated in\n  Appendix B V3: Minor correction V4: new data download link upated:\n  https://nihcc.app.box.com/v/ChestXray-NIHCC V5: Update benchmark results on\n  the published data split in the appendix", "journal-ref": "IEEE CVPR 2017, pp. 2097-2106 (2017)", "doi": "10.1109/CVPR.2017.369", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chest X-ray is one of the most commonly accessible radiological\nexaminations for screening and diagnosis of many lung diseases. A tremendous\nnumber of X-ray imaging studies accompanied by radiological reports are\naccumulated and stored in many modern hospitals' Picture Archiving and\nCommunication Systems (PACS). On the other side, it is still an open question\nhow this type of hospital-size knowledge database containing invaluable imaging\ninformatics (i.e., loosely labeled) can be used to facilitate the data-hungry\ndeep learning paradigms in building truly large-scale high precision\ncomputer-aided diagnosis (CAD) systems.\n  In this paper, we present a new chest X-ray database, namely \"ChestX-ray8\",\nwhich comprises 108,948 frontal-view X-ray images of 32,717 unique patients\nwith the text-mined eight disease image labels (where each image can have\nmulti-labels), from the associated radiological reports using natural language\nprocessing. Importantly, we demonstrate that these commonly occurring thoracic\ndiseases can be detected and even spatially-located via a unified\nweakly-supervised multi-label image classification and disease localization\nframework, which is validated using our proposed dataset. Although the initial\nquantitative results are promising as reported, deep convolutional neural\nnetwork based \"reading chest X-rays\" (i.e., recognizing and locating the common\ndisease patterns trained with only image-level labels) remains a strenuous task\nfor fully-automated high precision CAD systems. Data download link:\nhttps://nihcc.app.box.com/v/ChestXray-NIHCC\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 17:31:12 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 17:45:07 GMT"}, {"version": "v3", "created": "Wed, 19 Jul 2017 19:12:50 GMT"}, {"version": "v4", "created": "Wed, 27 Sep 2017 14:33:36 GMT"}, {"version": "v5", "created": "Thu, 14 Dec 2017 19:35:31 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Wang", "Xiaosong", ""], ["Peng", "Yifan", ""], ["Lu", "Le", ""], ["Lu", "Zhiyong", ""], ["Bagheri", "Mohammadhadi", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1705.02364", "submitter": "Alexis Conneau", "authors": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine\n  Bordes", "title": "Supervised Learning of Universal Sentence Representations from Natural\n  Language Inference Data", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern NLP systems rely on word embeddings, previously trained in an\nunsupervised manner on large corpora, as base features. Efforts to obtain\nembeddings for larger chunks of text, such as sentences, have however not been\nso successful. Several attempts at learning unsupervised representations of\nsentences have not reached satisfactory enough performance to be widely\nadopted. In this paper, we show how universal sentence representations trained\nusing the supervised data of the Stanford Natural Language Inference datasets\ncan consistently outperform unsupervised methods like SkipThought vectors on a\nwide range of transfer tasks. Much like how computer vision uses ImageNet to\nobtain features, which can then be transferred to other tasks, our work tends\nto indicate the suitability of natural language inference for transfer learning\nto other NLP tasks. Our encoder is publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 18:54:39 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 13:16:21 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 13:15:01 GMT"}, {"version": "v4", "created": "Fri, 21 Jul 2017 09:50:36 GMT"}, {"version": "v5", "created": "Sun, 8 Jul 2018 21:22:11 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Conneau", "Alexis", ""], ["Kiela", "Douwe", ""], ["Schwenk", "Holger", ""], ["Barrault", "Loic", ""], ["Bordes", "Antoine", ""]]}, {"id": "1705.02394", "submitter": "Stefan Scherer", "authors": "Jonathan Chang, Stefan Scherer", "title": "Learning Representations of Emotional Speech with Deep Convolutional\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically assessing emotional valence in human speech has historically\nbeen a difficult task for machine learning algorithms. The subtle changes in\nthe voice of the speaker that are indicative of positive or negative emotional\nstates are often \"overshadowed\" by voice characteristics relating to emotional\nintensity or emotional activation. In this work we explore a representation\nlearning approach that automatically derives discriminative representations of\nemotional speech. In particular, we investigate two machine learning strategies\nto improve classifier performance: (1) utilization of unlabeled data using a\ndeep convolutional generative adversarial network (DCGAN), and (2) multitask\nlearning. Within our extensive experiments we leverage a multitask annotated\nemotional corpus as well as a large unlabeled meeting corpus (around 100\nhours). Our speaker-independent classification experiments show that in\nparticular the use of unlabeled data in our investigations improves performance\nof the classifiers and both fully supervised baseline approaches are\noutperformed considerably. We improve the classification of emotional valence\non a discrete 5-point scale to 43.88% and on a 3-point scale to 49.80%, which\nis competitive to state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 18:28:25 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Chang", "Jonathan", ""], ["Scherer", "Stefan", ""]]}, {"id": "1705.02395", "submitter": "Markus Borg", "authors": "Markus Borg, Iben Lennerstad, Rasmus Ros, Elizabeth Bjarnason", "title": "On Using Active Learning and Self-Training when Mining Performance\n  Discussions on Stack Overflow", "comments": "Preprint of paper accepted for the Proc. of the 21st International\n  Conference on Evaluation and Assessment in Software Engineering, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abundant data is the key to successful machine learning. However, supervised\nlearning requires annotated data that are often hard to obtain. In a\nclassification task with limited resources, Active Learning (AL) promises to\nguide annotators to examples that bring the most value for a classifier. AL can\nbe successfully combined with self-training, i.e., extending a training set\nwith the unlabelled examples for which a classifier is the most certain. We\nreport our experiences on using AL in a systematic manner to train an SVM\nclassifier for Stack Overflow posts discussing performance of software\ncomponents. We show that the training examples deemed as the most valuable to\nthe classifier are also the most difficult for humans to annotate. Despite\ncarefully evolved annotation criteria, we report low inter-rater agreement, but\nwe also propose mitigation strategies. Finally, based on one annotator's work,\nwe show that self-training can improve the classification accuracy. We conclude\nthe paper by discussing implication for future text miners aspiring to use AL\nand self-training.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 20:47:36 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Borg", "Markus", ""], ["Lennerstad", "Iben", ""], ["Ros", "Rasmus", ""], ["Bjarnason", "Elizabeth", ""]]}, {"id": "1705.02411", "submitter": "Anirudh Raju", "authors": "Ming Sun, Anirudh Raju, George Tucker, Sankaran Panchapagesan,\n  Gengshen Fu, Arindam Mandal, Spyros Matsoukas, Nikko Strom, Shiv Vitaladevuni", "title": "Max-Pooling Loss Training of Long Short-Term Memory Networks for\n  Small-Footprint Keyword Spotting", "comments": null, "journal-ref": "Spoken Language Technology Workshop (SLT), 2016 IEEE (pp.\n  474-480). IEEE", "doi": "10.1109/SLT.2016.7846306", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a max-pooling based loss function for training Long Short-Term\nMemory (LSTM) networks for small-footprint keyword spotting (KWS), with low\nCPU, memory, and latency requirements. The max-pooling loss training can be\nfurther guided by initializing with a cross-entropy loss trained network. A\nposterior smoothing based evaluation approach is employed to measure keyword\nspotting performance. Our experimental results show that LSTM models trained\nusing cross-entropy loss or max-pooling loss outperform a cross-entropy loss\ntrained baseline feed-forward Deep Neural Network (DNN). In addition,\nmax-pooling loss trained LSTM with randomly initialized network performs better\ncompared to cross-entropy loss trained LSTM. Finally, the max-pooling loss\ntrained LSTM initialized with a cross-entropy pre-trained network shows the\nbest performance, which yields $67.6\\%$ relative reduction compared to baseline\nfeed-forward DNN in Area Under the Curve (AUC) measure.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 22:36:04 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sun", "Ming", ""], ["Raju", "Anirudh", ""], ["Tucker", "George", ""], ["Panchapagesan", "Sankaran", ""], ["Fu", "Gengshen", ""], ["Mandal", "Arindam", ""], ["Matsoukas", "Spyros", ""], ["Strom", "Nikko", ""], ["Vitaladevuni", "Shiv", ""]]}, {"id": "1705.02426", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Yuexin Wu, Yiming Yang", "title": "Analogical Inference for Multi-Relational Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale multi-relational embedding refers to the task of learning the\nlatent representations for entities and relations in large knowledge graphs. An\neffective and scalable solution for this problem is crucial for the true\nsuccess of knowledge-based inference in a broad range of applications. This\npaper proposes a novel framework for optimizing the latent representations with\nrespect to the \\textit{analogical} properties of the embedded entities and\nrelations. By formulating the learning objective in a differentiable fashion,\nour model enjoys both theoretical power and computational scalability, and\nsignificantly outperformed a large number of representative baseline methods on\nbenchmark datasets. Furthermore, the model offers an elegant unification of\nseveral well-known methods in multi-relational embedding, which can be proven\nto be special instantiations of our framework.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 01:40:28 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 16:58:24 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Liu", "Hanxiao", ""], ["Wu", "Yuexin", ""], ["Yang", "Yiming", ""]]}, {"id": "1705.02452", "submitter": "Somnath Roy", "authors": "Pramod Pandey, Somnath Roy", "title": "A Generative Model of a Pronunciation Lexicon for Hindi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice browser applications in Text-to- Speech (TTS) and Automatic Speech\nRecognition (ASR) systems crucially depend on a pronunciation lexicon. The\npresent paper describes the model of pronunciation lexicon of Hindi developed\nto automatically generate the output forms of Hindi at two levels, the\n<phoneme> and the <PS> (PS, in short for Prosodic Structure). The latter level\ninvolves both syllable-division and stress placement. The paper describes the\ntool developed for generating the two-level outputs of lexica in Hindi.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 06:35:58 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Pandey", "Pramod", ""], ["Roy", "Somnath", ""]]}, {"id": "1705.02494", "submitter": "Ikuya Yamada", "authors": "Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji", "title": "Learning Distributed Representations of Texts and Entities from\n  Knowledge Base", "comments": null, "journal-ref": "Transactions of the Association for Computational Linguistics, 5\n  (2017), 397-411", "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a neural network model that jointly learns distributed\nrepresentations of texts and knowledge base (KB) entities. Given a text in the\nKB, we train our proposed model to predict entities that are relevant to the\ntext. Our model is designed to be generic with the ability to address various\nNLP tasks with ease. We train the model using a large corpus of texts and their\nentity annotations extracted from Wikipedia. We evaluated the model on three\nimportant NLP tasks (i.e., sentence textual similarity, entity linking, and\nfactoid question answering) involving both unsupervised and supervised\nsettings. As a result, we achieved state-of-the-art results on all three of\nthese tasks. Our code and trained models are publicly available for further\nacademic research.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 15:11:30 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 16:59:08 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 15:27:55 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Yamada", "Ikuya", ""], ["Shindo", "Hiroyuki", ""], ["Takeda", "Hideaki", ""], ["Takefuji", "Yoshiyasu", ""]]}, {"id": "1705.02518", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Kashyap Popat, Gerhard Weikum", "title": "Exploring Latent Semantic Factors to Find Useful Product Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews provided by consumers are a valuable asset for e-Commerce\nplatforms, influencing potential consumers in making purchasing decisions.\nHowever, these reviews are of varying quality, with the useful ones buried deep\nwithin a heap of non-informative reviews. In this work, we attempt to\nautomatically identify review quality in terms of its helpfulness to the end\nconsumers. In contrast to previous works in this domain exploiting a variety of\nsyntactic and community-level features, we delve deep into the semantics of\nreviews as to what makes them useful, providing interpretable explanation for\nthe same. We identify a set of consistency and semantic factors, all from the\ntext, ratings, and timestamps of user-generated reviews, making our approach\ngeneralizable across all communities and domains. We explore review semantics\nin terms of several latent factors like the expertise of its author, his\njudgment about the fine-grained facets of the underlying product, and his\nwriting style. These are cast into a Hidden Markov Model -- Latent Dirichlet\nAllocation (HMM-LDA) based model to jointly infer: (i) reviewer expertise, (ii)\nitem facets, and (iii) review helpfulness. Large-scale experiments on five\nreal-world datasets from Amazon show significant improvement over\nstate-of-the-art baselines in predicting and ranking useful reviews.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:21:48 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Popat", "Kashyap", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02519", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Hemank Lamba, Gerhard Weikum", "title": "Item Recommendation with Evolving User Preferences and Experience", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2015.111", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current recommender systems exploit user and item similarities by\ncollaborative filtering. Some advanced methods also consider the temporal\nevolution of item ratings as a global background process. However, all prior\nmethods disregard the individual evolution of a user's experience level and how\nthis is expressed in the user's writing in a review community. In this paper,\nwe model the joint evolution of user experience, interest in specific item\nfacets, writing style, and rating behavior. This way we can generate individual\nrecommendations that take into account the user's maturity level (e.g.,\nrecommending art movies rather than blockbusters for a cinematography expert).\nAs only item ratings and review texts are observables, we capture the user's\nexperience and interests in a latent model learned from her reviews, vocabulary\nand writing style. We develop a generative HMM-LDA model to trace user\nevolution, where the Hidden Markov Model (HMM) traces her latent experience\nprogressing over time -- with solely user reviews and ratings as observables\nover time. The facets of a user's interest are drawn from a Latent Dirichlet\nAllocation (LDA) model derived from her reviews, as a function of her (again\nlatent) experience level. In experiments with five real-world datasets, we show\nthat our model improves the rating prediction over state-of-the-art baselines,\nby a substantial margin. We also show, in a use-case study, that our model\nperforms well in the assessment of user experience levels.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:22:41 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Lamba", "Hemank", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02522", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Gerhard Weikum, Cristian Danescu-Niculescu-Mizil", "title": "People on Drugs: Credibility of User Statements in Health Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online health communities are a valuable source of information for patients\nand physicians. However, such user-generated resources are often plagued by\ninaccuracies and misinformation. In this work we propose a method for\nautomatically establishing the credibility of user-generated medical statements\nand the trustworthiness of their authors by exploiting linguistic cues and\ndistant supervision from expert sources. To this end we introduce a\nprobabilistic graphical model that jointly learns user trustworthiness,\nstatement credibility, and language objectivity. We apply this methodology to\nthe task of extracting rare or unknown side-effects of medical drugs --- this\nbeing one of the problems where large scale non-expert data has the potential\nto complement expert medical knowledge. We show that our method can reliably\nextract side-effects and filter out false statements, while identifying\ntrustworthy users that are likely to contribute valuable medical information.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:38:33 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Weikum", "Gerhard", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1705.02667", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Gerhard Weikum", "title": "People on Media: Jointly Identifying Credible News and Trustworthy\n  Citizen Journalists in Online Communities", "comments": null, "journal-ref": null, "doi": "10.1145/2806416.2806537", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media seems to have become more partisan, often providing a biased coverage\nof news catering to the interest of specific groups. It is therefore essential\nto identify credible information content that provides an objective narrative\nof an event. News communities such as digg, reddit, or newstrust offer\nrecommendations, reviews, quality ratings, and further insights on journalistic\nworks. However, there is a complex interaction between different factors in\nsuch online communities: fairness and style of reporting, language clarity and\nobjectivity, topical perspectives (like political viewpoint), expertise and\nbias of community members, and more. This paper presents a model to\nsystematically analyze the different interactions in a news community between\nusers, news, and sources. We develop a probabilistic graphical model that\nleverages this joint interaction to identify 1) highly credible news articles,\n2) trustworthy news sources, and 3) expert users who perform the role of\n\"citizen journalists\" in the community. Our method extends CRF models to\nincorporate real-valued ratings, as some communities have very fine-grained\nscales that cannot be easily discretized without losing information. To the\nbest of our knowledge, this paper is the first full-fledged analysis of\ncredibility, trust, and expertise in news communities.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:41:31 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 16:40:16 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02668", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Sourav Dutta, Gerhard Weikum", "title": "Credible Review Detection with Limited Information using Consistency\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews provide viewpoints on the strengths and shortcomings of\nproducts/services, influencing potential customers' purchasing decisions.\nHowever, the proliferation of non-credible reviews -- either fake (promoting/\ndemoting an item), incompetent (involving irrelevant aspects), or biased --\nentails the problem of identifying credible reviews. Prior works involve\nclassifiers harnessing rich information about items/users -- which might not be\nreadily available in several domains -- that provide only limited\ninterpretability as to why a review is deemed non-credible. This paper presents\na novel approach to address the above issues. We utilize latent topic models\nleveraging review texts, item ratings, and timestamps to derive consistency\nfeatures without relying on item/user histories, unavailable for \"long-tail\"\nitems/users. We develop models, for computing review credibility scores to\nprovide interpretable evidence for non-credible reviews, that are also\ntransferable to other domains -- addressing the scarcity of labeled data.\nExperiments on real-world datasets demonstrate improvements over\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:43:01 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Dutta", "Sourav", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02669", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Stephan Guennemann, Gerhard Weikum", "title": "Item Recommendation with Continuous Experience Evolution of Users using\n  Brownian Motion", "comments": null, "journal-ref": null, "doi": "10.1145/2939672.2939780", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online review communities are dynamic as users join and leave, adopt new\nvocabulary, and adapt to evolving trends. Recent work has shown that\nrecommender systems benefit from explicit consideration of user experience.\nHowever, prior work assumes a fixed number of discrete experience levels,\nwhereas in reality users gain experience and mature continuously over time.\nThis paper presents a new model that captures the continuous evolution of user\nexperience, and the resulting language model in reviews and other posts. Our\nmodel is unsupervised and combines principles of Geometric Brownian Motion,\nBrownian Motion, and Latent Dirichlet Allocation to trace a smooth temporal\nprogression of user experience and language model respectively. We develop\npractical algorithms for estimating the model parameters from data and for\ninference with our model (e.g., to recommend items). Extensive experiments with\nfive real-world datasets show that our model not only fits data better than\ndiscrete-model baselines, but also outperforms state-of-the-art methods for\npredicting item ratings.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:46:43 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 06:47:51 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 17:56:00 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Guennemann", "Stephan", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02700", "submitter": "Vincent Fiorentini", "authors": "Vincent Fiorentini, Megan Shao, Julie Medero", "title": "Generating Memorable Mnemonic Encodings of Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major system is a mnemonic system that can be used to memorize sequences\nof numbers. In this work, we present a method to automatically generate\nsentences that encode a given number. We propose several encoding models and\ncompare the most promising ones in a password memorability study. The results\nof the study show that a model combining part-of-speech sentence templates with\nan $n$-gram language model produces the most memorable password\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 21:16:35 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Fiorentini", "Vincent", ""], ["Shao", "Megan", ""], ["Medero", "Julie", ""]]}, {"id": "1705.02735", "submitter": "Amir Zadeh", "authors": "Edmund Tong, Amir Zadeh, Cara Jones, Louis-Philippe Morency", "title": "Combating Human Trafficking with Deep Multimodal Models", "comments": "ACL 2017 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human trafficking is a global epidemic affecting millions of people across\nthe planet. Sex trafficking, the dominant form of human trafficking, has seen a\nsignificant rise mostly due to the abundance of escort websites, where human\ntraffickers can openly advertise among at-will escort advertisements. In this\npaper, we take a major step in the automatic detection of advertisements\nsuspected to pertain to human trafficking. We present a novel dataset called\nTrafficking-10k, with more than 10,000 advertisements annotated for this task.\nThe dataset contains two sources of information per advertisement: text and\nimages. For the accurate detection of trafficking advertisements, we designed\nand trained a deep multimodal model called the Human Trafficking Deep Network\n(HTDN).\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 03:48:01 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Tong", "Edmund", ""], ["Zadeh", "Amir", ""], ["Jones", "Cara", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1705.02750", "submitter": "Hayate Iso Hayate ISO", "authors": "Hayate Iso, Shoko Wakamiya, Eiji Aramaki", "title": "Density Estimation for Geolocation via Convolutional Mixture Density\n  Network", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, geographic information related to Twitter is crucially important\nfor fine-grained applications. However, the amount of geographic information\navail- able on Twitter is low, which makes the pursuit of many applications\nchallenging. Under such circumstances, estimating the location of a tweet is an\nimportant goal of the study. Unlike most previous studies that estimate the\npre-defined district as the classification task, this study employs a\nprobability distribution to represent richer information of the tweet, not only\nthe location but also its ambiguity. To realize this modeling, we propose the\nconvolutional mixture density network (CMDN), which uses text data to estimate\nthe mixture model parameters. Experimentally obtained results reveal that CMDN\nachieved the highest prediction performance among the method for predicting the\nexact coordinates. It also provides a quantitative representation of the\nlocation ambiguity for each tweet that properly works for extracting the\nreliable location estimations.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 05:50:47 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Iso", "Hayate", ""], ["Wakamiya", "Shoko", ""], ["Aramaki", "Eiji", ""]]}, {"id": "1705.02798", "submitter": "Minghao Hu", "authors": "Minghao Hu and Yuxing Peng and Zhen Huang and Xipeng Qiu and Furu Wei\n  and Ming Zhou", "title": "Reinforced Mnemonic Reader for Machine Reading Comprehension", "comments": "Published in 27th International Joint Conference on Artificial\n  Intelligence (IJCAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the Reinforced Mnemonic Reader for machine\nreading comprehension tasks, which enhances previous attentive readers in two\naspects. First, a reattention mechanism is proposed to refine current\nattentions by directly accessing to past attentions that are temporally\nmemorized in a multi-round alignment architecture, so as to avoid the problems\nof attention redundancy and attention deficiency. Second, a new optimization\napproach, called dynamic-critical reinforcement learning, is introduced to\nextend the standard supervised method. It always encourages to predict a more\nacceptable answer so as to address the convergence suppression problem occurred\nin traditional reinforcement learning algorithms. Extensive experiments on the\nStanford Question Answering Dataset (SQuAD) show that our model achieves\nstate-of-the-art results. Meanwhile, our model outperforms previous systems by\nover 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 09:43:05 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 08:38:05 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 14:17:27 GMT"}, {"version": "v4", "created": "Wed, 25 Apr 2018 07:13:22 GMT"}, {"version": "v5", "created": "Sun, 29 Apr 2018 03:09:36 GMT"}, {"version": "v6", "created": "Wed, 6 Jun 2018 02:16:53 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Hu", "Minghao", ""], ["Peng", "Yuxing", ""], ["Huang", "Zhen", ""], ["Qiu", "Xipeng", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "1705.02925", "submitter": "Pradeep Dasigi", "authors": "Pradeep Dasigi and Waleed Ammar and Chris Dyer and Eduard Hovy", "title": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Type-level word embeddings use the same set of parameters to represent all\ninstances of a word regardless of its context, ignoring the inherent lexical\nambiguity in language. Instead, we embed semantic concepts (or synsets) as\ndefined in WordNet and represent a word token in a particular context by\nestimating a distribution over relevant semantic concepts. We use the new,\ncontext-sensitive embeddings in a model for predicting prepositional phrase(PP)\nattachments and jointly learn the concept embeddings and model parameters. We\nshow that using context-sensitive embeddings improves the accuracy of the PP\nattachment model by 5.4% absolute points, which amounts to a 34.4% relative\nreduction in errors.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 15:40:51 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Dasigi", "Pradeep", ""], ["Ammar", "Waleed", ""], ["Dyer", "Chris", ""], ["Hovy", "Eduard", ""]]}, {"id": "1705.03122", "submitter": "Michael Auli", "authors": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N.\n  Dauphin", "title": "Convolutional Sequence to Sequence Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalent approach to sequence to sequence learning maps an input\nsequence to a variable length output sequence via recurrent neural networks. We\nintroduce an architecture based entirely on convolutional neural networks.\nCompared to recurrent models, computations over all elements can be fully\nparallelized during training and optimization is easier since the number of\nnon-linearities is fixed and independent of the input length. Our use of gated\nlinear units eases gradient propagation and we equip each decoder layer with a\nseparate attention module. We outperform the accuracy of the deep LSTM setup of\nWu et al. (2016) on both WMT'14 English-German and WMT'14 English-French\ntranslation at an order of magnitude faster speed, both on GPU and CPU.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 23:25:30 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 16:14:26 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 01:40:57 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Gehring", "Jonas", ""], ["Auli", "Michael", ""], ["Grangier", "David", ""], ["Yarats", "Denis", ""], ["Dauphin", "Yann N.", ""]]}, {"id": "1705.03127", "submitter": "Stefan Jansen", "authors": "Stefan Jansen", "title": "Word and Phrase Translation with word2vec", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word and phrase tables are key inputs to machine translations, but costly to\nproduce. New unsupervised learning methods represent words and phrases in a\nhigh-dimensional vector space, and these monolingual embeddings have been shown\nto encode syntactic and semantic relationships between language elements. The\ninformation captured by these embeddings can be exploited for bilingual\ntranslation by learning a transformation matrix that allows matching relative\npositions across two monolingual vector spaces. This method aims to identify\nhigh-quality candidates for word and phrase translation more cost-effectively\nfrom unlabeled data.\n  This paper expands the scope of previous attempts of bilingual translation to\nfour languages (English, German, Spanish, and French). It shows how to process\nthe source data, train a neural network to learn the high-dimensional\nembeddings for individual languages and expands the framework for testing their\nquality beyond the English language. Furthermore, it shows how to learn\nbilingual transformation matrices and obtain candidates for word and phrase\ntranslation, and assess their quality.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 00:09:38 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 06:04:24 GMT"}, {"version": "v3", "created": "Thu, 11 May 2017 02:18:47 GMT"}, {"version": "v4", "created": "Tue, 24 Apr 2018 15:39:41 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Jansen", "Stefan", ""]]}, {"id": "1705.03151", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang, Yixiang Chen, Lantian Li and Andrew Abel", "title": "Phonetic Temporal Neural Model for Language Identification", "comments": "Submitted to TASLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural models, particularly the LSTM-RNN model, have shown great\npotential for language identification (LID). However, the use of phonetic\ninformation has been largely overlooked by most existing neural LID methods,\nalthough this information has been used very successfully in conventional\nphonetic LID systems. We present a phonetic temporal neural model for LID,\nwhich is an LSTM-RNN LID system that accepts phonetic features produced by a\nphone-discriminative DNN as the input, rather than raw acoustic features. This\nnew model is similar to traditional phonetic LID methods, but the phonetic\nknowledge here is much richer: it is at the frame level and involves compacted\ninformation of all phones. Our experiments conducted on the Babel database and\nthe AP16-OLR database demonstrate that the temporal phonetic neural approach is\nvery effective, and significantly outperforms existing acoustic neural models.\nIt also outperforms the conventional i-vector approach on short utterances and\nin noisy conditions.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 02:46:21 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 11:23:34 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 05:23:26 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Chen", "Yixiang", ""], ["Li", "Lantian", ""], ["Abel", "Andrew", ""]]}, {"id": "1705.03152", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang, Yixiang Chen, Ying Shi, Lantian Li", "title": "Phone-aware Neural Language Identification", "comments": "arXiv admin note: text overlap with arXiv:1705.03151", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pure acoustic neural models, particularly the LSTM-RNN model, have shown\ngreat potential in language identification (LID). However, the phonetic\ninformation has been largely overlooked by most of existing neural LID models,\nalthough this information has been used in the conventional phonetic LID\nsystems with a great success. We present a phone-aware neural LID architecture,\nwhich is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR\nsystem. By utilizing the phonetic knowledge, the LID performance can be\nsignificantly improved. Interestingly, even if the test language is not\ninvolved in the ASR training, the phonetic knowledge still presents a large\ncontribution. Our experiments conducted on four languages within the Babel\ncorpus demonstrated that the phone-aware approach is highly effective.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 02:47:22 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 11:43:40 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Chen", "Yixiang", ""], ["Shi", "Ying", ""], ["Li", "Lantian", ""]]}, {"id": "1705.03202", "submitter": "Ruobing Xie", "authors": "Ruobing Xie, Zhiyuan Liu, Fen Lin, Leyu Lin", "title": "Does William Shakespeare REALLY Write Hamlet? Knowledge Representation\n  Learning with Confidence", "comments": "8 pages", "journal-ref": "AAAI-2018", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs), which could provide essential relational information\nbetween entities, have been widely utilized in various knowledge-driven\napplications. Since the overall human knowledge is innumerable that still grows\nexplosively and changes frequently, knowledge construction and update\ninevitably involve automatic mechanisms with less human supervision, which\nusually bring in plenty of noises and conflicts to KGs. However, most\nconventional knowledge representation learning methods assume that all triple\nfacts in existing KGs share the same significance without any noises. To\naddress this problem, we propose a novel confidence-aware knowledge\nrepresentation learning framework (CKRL), which detects possible noises in KGs\nwhile learning knowledge representations with confidence simultaneously.\nSpecifically, we introduce the triple confidence to conventional\ntranslation-based methods for knowledge representation learning. To make triple\nconfidence more flexible and universal, we only utilize the internal structural\ninformation in KGs, and propose three kinds of triple confidences considering\nboth local and global structural information. In experiments, We evaluate our\nmodels on knowledge graph noise detection, knowledge graph completion and\ntriple classification. Experimental results demonstrate that our\nconfidence-aware models achieve significant and consistent improvements on all\ntasks, which confirms the capability of CKRL modeling confidence with\nstructural information in both KG noise detection and knowledge representation\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 06:46:21 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 16:15:36 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Xie", "Ruobing", ""], ["Liu", "Zhiyuan", ""], ["Lin", "Fen", ""], ["Lin", "Leyu", ""]]}, {"id": "1705.03247", "submitter": "Somnath Roy", "authors": "Somnath Roy", "title": "A Systematic Review of Hindi Prosody", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prosody describes both form and function of a sentence using the\nsuprasegmental features of speech. Prosody phenomena are explored in the domain\nof higher phonological constituents such as word, phonological phrase and\nintonational phrase. The study of prosody at the word level is called word\nprosody and above word level is called sentence prosody. Word Prosody describes\nstress pattern by comparing the prosodic features of its constituent syllables.\nSentence Prosody involves the study on phrasing pattern and intonatonal pattern\nof a language. The aim of this study is to summarize the existing works on\nHindi prosody carried out in different domain of language and speech\nprocessing. The review is presented in a systematic fashion so that it could be\na useful resource for one who wants to build on the existing works.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 09:35:22 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Roy", "Somnath", ""]]}, {"id": "1705.03261", "submitter": "Zibo Yi", "authors": "Zibo Yi, Shasha Li, Jie Yu, Qingbo Wu", "title": "Drug-drug Interaction Extraction via Recurrent Neural Network with\n  Multiple Attention Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug-drug interaction (DDI) is a vital information when physicians and\npharmacists intend to co-administer two or more drugs. Thus, several DDI\ndatabases are constructed to avoid mistakenly combined use. In recent years,\nautomatically extracting DDIs from biomedical text has drawn researchers'\nattention. However, the existing work utilize either complex feature\nengineering or NLP tools, both of which are insufficient for sentence\ncomprehension. Inspired by the deep learning approaches in natural language\nprocessing, we propose a recur- rent neural network model with multiple\nattention layers for DDI classification. We evaluate our model on 2013 SemEval\nDDIExtraction dataset. The experiments show that our model classifies most of\nthe drug pairs into correct DDI categories, which outperforms the existing NLP\nor deep learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 10:22:48 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 15:54:36 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Yi", "Zibo", ""], ["Li", "Shasha", ""], ["Yu", "Jie", ""], ["Wu", "Qingbo", ""]]}, {"id": "1705.03389", "submitter": "Liang Li", "authors": "Liang Li, Pengyu Li, Yifan Liu, Tao Wan, Zengchang Qin", "title": "Logical Parsing from Natural Language Based on a Neural Translation\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Semantic parsing has emerged as a significant and powerful paradigm for\nnatural language interface and question answering systems. Traditional methods\nof building a semantic parser rely on high-quality lexicons, hand-crafted\ngrammars and linguistic features which are limited by applied domain or\nrepresentation. In this paper, we propose a general approach to learn from\ndenotations based on Seq2Seq model augmented with attention mechanism. We\nencode input sequence into vectors and use dynamic programming to infer\ncandidate logical forms. We utilize the fact that similar utterances should\nhave similar logical forms to help reduce the searching space. Under our\nlearning policy, the Seq2Seq model can learn mappings gradually with noises.\nCurriculum learning is adopted to make the learning smoother. We test our\nmethod on the arithmetic domain which shows our model can successfully infer\nthe correct logical forms and learn the word meanings, compositionality and\noperation orders simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 15:35:25 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Li", "Liang", ""], ["Li", "Pengyu", ""], ["Liu", "Yifan", ""], ["Wan", "Tao", ""], ["Qin", "Zengchang", ""]]}, {"id": "1705.03454", "submitter": "Mihail Eric", "authors": "Matthew Lamm and Mihail Eric", "title": "The Pragmatics of Indirect Commands in Collaborative Discourse", "comments": "International Conference on Computational Semantics 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today's artificial assistants are typically prompted to perform tasks through\ndirect, imperative commands such as \\emph{Set a timer} or \\emph{Pick up the\nbox}. However, to progress toward more natural exchanges between humans and\nthese assistants, it is important to understand the way non-imperative\nutterances can indirectly elicit action of an addressee. In this paper, we\ninvestigate command types in the setting of a grounded, collaborative game. We\nfocus on a less understood family of utterances for eliciting agent action,\nlocatives like \\emph{The chair is in the other room}, and demonstrate how these\nutterances indirectly command in specific game state contexts. Our work shows\nthat models with domain-specific grounding can effectively realize the\npragmatic reasoning that is necessary for more robust natural language\ninteraction.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 19:34:23 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 19:04:40 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Lamm", "Matthew", ""], ["Eric", "Mihail", ""]]}, {"id": "1705.03455", "submitter": "Ankur Bapna", "authors": "Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, Larry Heck", "title": "Sequential Dialogue Context Modeling for Spoken Language Understanding", "comments": "8 + 2 pages, Updated 10/17: Updated typos in abstract, Updated 07/07:\n  Updated Title, abstract and few minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken Language Understanding (SLU) is a key component of goal oriented\ndialogue systems that would parse user utterances into semantic frame\nrepresentations. Traditionally SLU does not utilize the dialogue history beyond\nthe previous system turn and contextual ambiguities are resolved by the\ndownstream components. In this paper, we explore novel approaches for modeling\ndialogue context in a recurrent neural network (RNN) based language\nunderstanding system. We propose the Sequential Dialogue Encoder Network, that\nallows encoding context from the dialogue history in chronological order. We\ncompare the performance of our proposed architecture with two context models,\none that uses just the previous turn context and another that encodes dialogue\ncontext in a memory network, but loses the order of utterances in the dialogue\nhistory. Experiments with a multi-domain dialogue dataset demonstrate that the\nproposed architecture results in reduced semantic frame error rates.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 20:57:30 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 00:21:13 GMT"}, {"version": "v3", "created": "Fri, 7 Jul 2017 21:35:02 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Bapna", "Ankur", ""], ["Tur", "Gokhan", ""], ["Hakkani-Tur", "Dilek", ""], ["Heck", "Larry", ""]]}, {"id": "1705.03487", "submitter": "Lav Varshney", "authors": "Masahiro Kazama, Minami Sugimoto, Chizuru Hosokawa, Keisuke\n  Matsushima, Lav R. Varshney, and Yoshiki Ishikawa", "title": "A neural network system for transformation of regional cuisine style", "comments": null, "journal-ref": null, "doi": "10.3389/fict.2018.00014", "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel system which can transform a recipe into any selected\nregional style (e.g., Japanese, Mediterranean, or Italian). This system has two\ncharacteristics. First the system can identify the degree of regional cuisine\nstyle mixture of any selected recipe and visualize such regional cuisine style\nmixtures using barycentric Newton diagrams. Second, the system can suggest\ningredient substitutions through an extended word2vec model, such that a recipe\nbecomes more authentic for any selected regional cuisine style. Drawing on a\nlarge number of recipes from Yummly, an example shows how the proposed system\ncan transform a traditional Japanese recipe, Sukiyaki, into French style.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 18:35:35 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 15:27:02 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Kazama", "Masahiro", ""], ["Sugimoto", "Minami", ""], ["Hosokawa", "Chizuru", ""], ["Matsushima", "Keisuke", ""], ["Varshney", "Lav R.", ""], ["Ishikawa", "Yoshiki", ""]]}, {"id": "1705.03508", "submitter": "Hamid Reza Hassanzadeh", "authors": "Hamid Reza Hassanzadeh, Ying Sha, May D. Wang", "title": "DeepDeath: Learning to Predict the Underlying Cause of Death with Big\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple cause-of-death data provides a valuable source of information that\ncan be used to enhance health standards by predicting health related\ntrajectories in societies with large populations. These data are often\navailable in large quantities across U.S. states and require Big Data\ntechniques to uncover complex hidden patterns. We design two different classes\nof models suitable for large-scale analysis of mortality data, a Hadoop-based\nensemble of random forests trained over N-grams, and the DeepDeath, a deep\nclassifier based on the recurrent neural network (RNN). We apply both classes\nto the mortality data provided by the National Center for Health Statistics and\nshow that while both perform significantly better than the random classifier,\nthe deep model that utilizes long short-term memory networks (LSTMs), surpasses\nthe N-gram based models and is capable of learning the temporal aspect of the\ndata without a need for building ad-hoc, expert-driven features.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 17:01:57 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Hassanzadeh", "Hamid Reza", ""], ["Sha", "Ying", ""], ["Wang", "May D.", ""]]}, {"id": "1705.03551", "submitter": "Mandar Joshi", "authors": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for\n  Reading Comprehension", "comments": "Added references, fixed typos, minor baseline update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TriviaQA, a challenging reading comprehension dataset containing\nover 650K question-answer-evidence triples. TriviaQA includes 95K\nquestion-answer pairs authored by trivia enthusiasts and independently gathered\nevidence documents, six per question on average, that provide high quality\ndistant supervision for answering the questions. We show that, in comparison to\nother recently introduced large-scale datasets, TriviaQA (1) has relatively\ncomplex, compositional questions, (2) has considerable syntactic and lexical\nvariability between questions and corresponding answer-evidence sentences, and\n(3) requires more cross sentence reasoning to find answers. We also present two\nbaseline algorithms: a feature-based classifier and a state-of-the-art neural\nnetwork, that performs well on SQuAD reading comprehension. Neither approach\ncomes close to human performance (23% and 40% vs. 80%), suggesting that\nTriviaQA is a challenging testbed that is worth significant future study. Data\nand code available at -- http://nlp.cs.washington.edu/triviaqa/\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 21:35:07 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 21:12:37 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Joshi", "Mandar", ""], ["Choi", "Eunsol", ""], ["Weld", "Daniel S.", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1705.03556", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, W. Bruce Croft", "title": "Relevance-based Word Embedding", "comments": "to appear in the proceedings of The 40th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR '17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a high-dimensional dense representation for vocabulary terms, also\nknown as a word embedding, has recently attracted much attention in natural\nlanguage processing and information retrieval tasks. The embedding vectors are\ntypically learned based on term proximity in a large corpus. This means that\nthe objective in well-known word embedding algorithms, e.g., word2vec, is to\naccurately predict adjacent word(s) for a given word or context. However, this\nobjective is not necessarily equivalent to the goal of many information\nretrieval (IR) tasks. The primary objective in various IR tasks is to capture\nrelevance instead of term proximity, syntactic, or even semantic similarity.\nThis is the motivation for developing unsupervised relevance-based word\nembedding models that learn word representations based on query-document\nrelevance information. In this paper, we propose two learning models with\ndifferent objective functions; one learns a relevance distribution over the\nvocabulary set for each query, and the other classifies each term as belonging\nto the relevant or non-relevant class for each query. To train our models, we\nused over six million unique queries and the top ranked documents retrieved in\nresponse to each query, which are assumed to be relevant to the query. We\nextrinsically evaluate our learned word representation models using two IR\ntasks: query expansion and query classification. Both query expansion\nexperiments on four TREC collections and query classification experiments on\nthe KDD Cup 2005 dataset suggest that the relevance-based word embedding models\nsignificantly outperform state-of-the-art proximity-based embedding models,\nsuch as word2vec and GloVe.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 22:09:01 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 22:11:57 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Zamani", "Hamed", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1705.03557", "submitter": "Ahmed Khalifa", "authors": "Ahmed Khalifa, Gabriella A. B. Barros, Julian Togelius", "title": "DeepTingle", "comments": "8 pages, 7 figures and 1 table. Published at ICCC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeepTingle is a text prediction and classification system trained on the\ncollected works of the renowned fantastic gay erotica author Chuck Tingle.\nWhereas the writing assistance tools you use everyday (in the form of\npredictive text, translation, grammar checking and so on) are trained on\ngeneric, purportedly \"neutral\" datasets, DeepTingle is trained on a very\nspecific, internally consistent but externally arguably eccentric dataset. This\nallows us to foreground and confront the norms embedded in data-driven\ncreativity and productivity assistance tools. As such tools effectively\nfunction as extensions of our cognition into technology, it is important to\nidentify the norms they embed within themselves and, by extension, us.\nDeepTingle is realized as a web application based on LSTM networks and the\nGloVe word embedding, implemented in JavaScript with Keras-JS.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 22:12:19 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 20:14:32 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Khalifa", "Ahmed", ""], ["Barros", "Gabriella A. B.", ""], ["Togelius", "Julian", ""]]}, {"id": "1705.03633", "submitter": "Justin Johnson", "authors": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy\n  Hoffman, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick", "title": "Inferring and Executing Programs for Visual Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for visual reasoning attempt to directly map inputs to\noutputs using black-box architectures without explicitly modeling the\nunderlying reasoning processes. As a result, these black-box models often learn\nto exploit biases in the data rather than learning to perform visual reasoning.\nInspired by module networks, this paper proposes a model for visual reasoning\nthat consists of a program generator that constructs an explicit representation\nof the reasoning process to be performed, and an execution engine that executes\nthe resulting program to produce an answer. Both the program generator and the\nexecution engine are implemented by neural networks, and are trained using a\ncombination of backpropagation and REINFORCE. Using the CLEVR benchmark for\nvisual reasoning, we show that our model significantly outperforms strong\nbaselines and generalizes better in a variety of settings.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 07:08:23 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Johnson", "Justin", ""], ["Hariharan", "Bharath", ""], ["van der Maaten", "Laurens", ""], ["Hoffman", "Judy", ""], ["Fei-Fei", "Li", ""], ["Zitnick", "C. Lawrence", ""], ["Girshick", "Ross", ""]]}, {"id": "1705.03645", "submitter": "Shantanu Kumar", "authors": "Shantanu Kumar", "title": "A Survey of Deep Learning Methods for Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation Extraction is an important sub-task of Information Extraction which\nhas the potential of employing deep learning (DL) models with the creation of\nlarge datasets using distant supervision. In this review, we compare the\ncontributions and pitfalls of the various DL models that have been used for the\ntask, to help guide the path ahead.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 08:05:44 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Kumar", "Shantanu", ""]]}, {"id": "1705.03670", "submitter": "Lantian Li Mr.", "authors": "Lantian Li, Yixiang Chen, Ying Shi, Zhiyuan Tang, Dong Wang", "title": "Deep Speaker Feature Learning for Text-independent Speaker Verification", "comments": "deep neural networks, speaker verification, speaker feature", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep neural networks (DNNs) have been used to learn speaker\nfeatures. However, the quality of the learned features is not sufficiently\ngood, so a complex back-end model, either neural or probabilistic, has to be\nused to address the residual uncertainty when applied to speaker verification,\njust as with raw features. This paper presents a convolutional time-delay deep\nneural network structure (CT-DNN) for speaker feature learning. Our\nexperimental results on the Fisher database demonstrated that this CT-DNN can\nproduce high-quality speaker features: even with a single feature (0.3 seconds\nincluding the context), the EER can be as low as 7.68%. This effectively\nconfirmed that the speaker trait is largely a deterministic short-time property\nrather than a long-time distributional pattern, and therefore can be extracted\nfrom just dozens of frames.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 09:30:42 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Li", "Lantian", ""], ["Chen", "Yixiang", ""], ["Shi", "Ying", ""], ["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""]]}, {"id": "1705.03751", "submitter": "Gagan Madan", "authors": "Gagan Madan", "title": "A Survey of Distant Supervision Methods using PGMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation Extraction refers to the task of populating a database with tuples\nof the form $r(e_1, e_2)$, where $r$ is a relation and $e_1$, $e_2$ are\nentities. Distant supervision is one such technique which tries to\nautomatically generate training examples based on an existing KB such as\nFreebase. This paper is a survey of some of the techniques in distant\nsupervision which primarily rely on Probabilistic Graphical Models (PGMs).\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 13:19:38 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Madan", "Gagan", ""]]}, {"id": "1705.03773", "submitter": "Jiyuan Zhang", "authors": "Jiyuan Zhang, Yang Feng, Dong Wang, Yang Wang, Andrew Abel, Shiyue\n  Zhang, Andi Zhang", "title": "Flexible and Creative Chinese Poetry Generation Using Neural Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that Chinese poems can be successfully generated by\nsequence-to-sequence neural models, particularly with the attention mechanism.\nA potential problem of this approach, however, is that neural models can only\nlearn abstract rules, while poem generation is a highly creative process that\ninvolves not only rules but also innovations for which pure statistical models\nare not appropriate in principle. This work proposes a memory-augmented neural\nmodel for Chinese poem generation, where the neural model and the augmented\nmemory work together to balance the requirements of linguistic accordance and\naesthetic innovation, leading to innovative generations that are still\nrule-compliant. In addition, it is found that the memory mechanism provides\ninteresting flexibility that can be used to generate poems with different\nstyles.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 13:55:53 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Zhang", "Jiyuan", ""], ["Feng", "Yang", ""], ["Wang", "Dong", ""], ["Wang", "Yang", ""], ["Abel", "Andrew", ""], ["Zhang", "Shiyue", ""], ["Zhang", "Andi", ""]]}, {"id": "1705.03802", "submitter": "Laura Perez-Beltrachini", "authors": "Laura Perez-Beltrachini and Claire Gardent", "title": "Analysing Data-To-Text Generation Benchmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several data-sets associating data to text have been created to\ntrain data-to-text surface realisers. It is unclear however to what extent the\nsurface realisation task exercised by these data-sets is linguistically\nchallenging. Do these data-sets provide enough variety to encourage the\ndevelopment of generic, high-quality data-to-text surface realisers ? In this\npaper, we argue that these data-sets have important drawbacks. We back up our\nclaim using statistics, metrics and manual evaluation. We conclude by eliciting\na set of criteria for the creation of a data-to-text benchmark which could help\nbetter support the development, evaluation and comparison of linguistically\nsophisticated data-to-text surface realisers.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 14:42:54 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Perez-Beltrachini", "Laura", ""], ["Gardent", "Claire", ""]]}, {"id": "1705.03865", "submitter": "Akshay Gupta", "authors": "Akshay Kumar Gupta", "title": "Survey of Visual Question Answering: Datasets and Techniques", "comments": "10 pages, 3 figures, 3 tables Added references, corrected typos, made\n  references less wordy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (or VQA) is a new and exciting problem that\ncombines natural language processing and computer vision techniques. We present\na survey of the various datasets and models that have been used to tackle this\ntask. The first part of the survey details the various datasets for VQA and\ncompares them along some common factors. The second part of this survey details\nthe different approaches for VQA, classified into four types: non-deep learning\nmodels, deep learning models without attention, deep learning models with\nattention, and other models which do not fit into the first three. Finally, we\ncompare the performances of these approaches and provide some directions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 17:30:17 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 06:46:52 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Gupta", "Akshay Kumar", ""]]}, {"id": "1705.03919", "submitter": "Mitchell Stern", "authors": "Mitchell Stern, Jacob Andreas, Dan Klein", "title": "A Minimal Span-Based Neural Constituency Parser", "comments": "To appear in ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a minimal neural model for constituency parsing\nbased on independent scoring of labels and spans. We show that this model is\nnot only compatible with classical dynamic programming techniques, but also\nadmits a novel greedy top-down inference algorithm based on recursive\npartitioning of the input. We demonstrate empirically that both prediction\nschemes are competitive with recent work, and when combined with basic\nextensions to the scoring model are capable of achieving state-of-the-art\nsingle-model performance on the Penn Treebank (91.79 F1) and strong performance\non the French Treebank (82.23 F1).\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 18:44:15 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Stern", "Mitchell", ""], ["Andreas", "Jacob", ""], ["Klein", "Dan", ""]]}, {"id": "1705.03995", "submitter": "Bingfeng Luo", "authors": "Bingfeng Luo, Yansong Feng, Zheng Wang, Zhanxing Zhu, Songfang Huang,\n  Rui Yan and Dongyan Zhao", "title": "Learning with Noise: Enhance Distantly Supervised Relation Extraction\n  with Dynamic Transition Matrix", "comments": "10 pages, accepted by ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-1040", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distant supervision significantly reduces human efforts in building training\ndata for many classification tasks. While promising, this technique often\nintroduces noise to the generated training data, which can severely affect the\nmodel performance. In this paper, we take a deep look at the application of\ndistant supervision in relation extraction. We show that the dynamic transition\nmatrix can effectively characterize the noise in the training data built by\ndistant supervision. The transition matrix can be effectively trained using a\nnovel curriculum learning based method without any direct supervision about the\nnoise. We thoroughly evaluate our approach under a wide range of extraction\nscenarios. Experimental results show that our approach consistently improves\nthe extraction results and outperforms the state-of-the-art in various\nevaluation scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 02:56:29 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Luo", "Bingfeng", ""], ["Feng", "Yansong", ""], ["Wang", "Zheng", ""], ["Zhu", "Zhanxing", ""], ["Huang", "Songfang", ""], ["Yan", "Rui", ""], ["Zhao", "Dongyan", ""]]}, {"id": "1705.04003", "submitter": "Hoang Pham", "authors": "Thai-Hoang Pham, Phuong Le-Hong", "title": "Content-based Approach for Vietnamese Spam SMS Filtering", "comments": "4 pages, IALP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short Message Service (SMS) spam is a serious problem in Vietnam because of\nthe availability of very cheap pre-paid SMS packages. There are some systems to\ndetect and filter spam messages for English, most of which use machine learning\ntechniques to analyze the content of messages and classify them. For\nVietnamese, there is some research on spam email filtering but none focused on\nSMS. In this work, we propose the first system for filtering Vietnamese spam\nSMS. We first propose an appropriate preprocessing method since existing tools\nfor Vietnamese preprocessing cannot give good accuracy on our dataset. We then\nexperiment with vector representations and classifiers to find the best model\nfor this problem. Our system achieves an accuracy of 94% when labelling spam\nmessages while the misclassification rate of legitimate messages is relatively\nsmall, about only 0.4%. This is an encouraging result compared to that of\nEnglish and can be served as a strong baseline for future development of\nVietnamese SMS spam prevention systems.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 04:04:33 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Pham", "Thai-Hoang", ""], ["Le-Hong", "Phuong", ""]]}, {"id": "1705.04038", "submitter": "Hoang Pham", "authors": "Thai-Hoang Pham, Xuan-Khoai Pham, Phuong Le-Hong", "title": "Building a Semantic Role Labelling System for Vietnamese", "comments": "8 pages, ICDIM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic role labelling (SRL) is a task in natural language processing which\ndetects and classifies the semantic arguments associated with the predicates of\na sentence. It is an important step towards understanding the meaning of a\nnatural language. There exists SRL systems for well-studied languages like\nEnglish, Chinese or Japanese but there is not any such system for the\nVietnamese language. In this paper, we present the first SRL system for\nVietnamese with encouraging accuracy. We first demonstrate that a simple\napplication of SRL techniques developed for English could not give a good\naccuracy for Vietnamese. We then introduce a new algorithm for extracting\ncandidate syntactic constituents, which is much more accurate than the common\nnode-mapping algorithm usually used in the identification step. Finally, in the\nclassification step, in addition to the common linguistic features, we propose\nnovel and useful features for use in SRL. Our SRL system achieves an $F_1$\nscore of 73.53\\% on the Vietnamese PropBank corpus. This system, including\nsoftware and corpus, is available as an open source project and we believe that\nit is a good baseline for the development of future Vietnamese SRL systems.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 07:08:30 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Pham", "Thai-Hoang", ""], ["Pham", "Xuan-Khoai", ""], ["Le-Hong", "Phuong", ""]]}, {"id": "1705.04044", "submitter": "Hoang Pham", "authors": "Thai-Hoang Pham, Phuong Le-Hong", "title": "End-to-end Recurrent Neural Network Models for Vietnamese Named Entity\n  Recognition: Word-level vs. Character-level", "comments": "14 pages, 5 figures, 7 tables, accepted to PACLING 2017, fix CRF\n  formular", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates end-to-end neural network architectures for\nVietnamese named entity recognition. Our best model is a combination of\nbidirectional Long Short-Term Memory (Bi-LSTM), Convolutional Neural Network\n(CNN), Conditional Random Field (CRF), using pre-trained word embeddings as\ninput, which achieves an F1 score of 88.59% on a standard test set. Our system\nis able to achieve a comparable performance to the first-rank system of the\nVLSP campaign without using any syntactic or hand-crafted features. We also\ngive an extensive empirical study on using common deep learning models for\nVietnamese NER, at both word and character level.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 07:31:39 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 16:49:03 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 00:04:32 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Pham", "Thai-Hoang", ""], ["Le-Hong", "Phuong", ""]]}, {"id": "1705.04146", "submitter": "Wang Ling", "authors": "Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom", "title": "Program Induction by Rationale Generation : Learning to Solve and\n  Explain Algebraic Word Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving algebraic word problems requires executing a series of arithmetic\noperations---a program---to obtain a final answer. However, since programs can\nbe arbitrarily complicated, inducing them directly from question-answer pairs\nis a formidable challenge. To make this task more feasible, we solve these\nproblems by generating answer rationales, sequences of natural language and\nhuman-readable mathematical expressions that derive the final answer through a\nseries of small steps. Although rationales do not explicitly specify programs,\nthey provide a scaffolding for their structure via intermediate milestones. To\nevaluate our approach, we have created a new 100,000-sample dataset of\nquestions, answers and rationales. Experimental results show that indirect\nsupervision of program learning via answer rationales is a promising strategy\nfor inducing arithmetic programs.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 13:04:47 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 14:57:26 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 16:45:03 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Ling", "Wang", ""], ["Yogatama", "Dani", ""], ["Dyer", "Chris", ""], ["Blunsom", "Phil", ""]]}, {"id": "1705.04153", "submitter": "Pengfei Liu", "authors": "Pengfei Liu, Xipeng Qiu, Xuanjing Huang", "title": "Dynamic Compositional Neural Networks over Tree Structure", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-structured neural networks have proven to be effective in learning\nsemantic representations by exploiting syntactic information. In spite of their\nsuccess, most existing models suffer from the underfitting problem: they\nrecursively use the same shared compositional function throughout the whole\ncompositional process and lack expressive power due to inability to capture the\nrichness of compositionality. In this paper, we address this issue by\nintroducing the dynamic compositional neural networks over tree structure\n(DC-TreeNN), in which the compositional function is dynamically generated by a\nmeta network. The role of meta-network is to capture the metaknowledge across\nthe different compositional rules and formulate them. Experimental results on\ntwo typical tasks show the effectiveness of the proposed models.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 13:11:23 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Liu", "Pengfei", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1705.04187", "submitter": "Diego Amancio Dr.", "authors": "Camilo Akimushkin and Diego R. Amancio and Osvaldo N. Oliveira Jr", "title": "On the role of words in the network structure of texts: application to\n  authorship attribution", "comments": null, "journal-ref": "Physica A v. 495, p. 49-58, 2018", "doi": "10.1016/j.physa.2017.12.054", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well-established automatic analyses of texts mainly consider frequencies of\nlinguistic units, e.g. letters, words and bigrams, while methods based on\nco-occurrence networks consider the structure of texts regardless of the nodes\nlabel (i.e. the words semantics). In this paper, we reconcile these distinct\nviewpoints by introducing a generalized similarity measure to compare texts\nwhich accounts for both the network structure of texts and the role of\nindividual words in the networks. We use the similarity measure for authorship\nattribution of three collections of books, each composed of 8 authors and 10\nbooks per author. High accuracy rates were obtained with typical values from\n90% to 98.75%, much higher than with the traditional the TF-IDF approach for\nthe same collections. These accuracies are also higher than taking only the\ntopology of networks into account. We conclude that the different properties of\nspecific words on the macroscopic scale structure of a whole text are as\nrelevant as their frequency of appearance; conversely, considering the identity\nof nodes brings further knowledge about a piece of text represented as a\nnetwork.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 14:00:10 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Akimushkin", "Camilo", ""], ["Amancio", "Diego R.", ""], ["Oliveira", "Osvaldo N.", "Jr"]]}, {"id": "1705.04253", "submitter": "Behrang QasemiZadeh", "authors": "Behrang QasemiZadeh, Laura Kallmeyer", "title": "Sketching Word Vectors Through Hashing", "comments": "Results regarding the neural network are updated/ part-of-speech\n  tagging is replaced by short text classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new fast word embedding technique using hash functions. The\nmethod is a derandomization of a new type of random projections: By\ndisregarding the classic constraint used in designing random projections (i.e.,\npreserving pairwise distances in a particular normed space), our solution\nexploits extremely sparse non-negative random projections. Our experiments show\nthat the proposed method can achieve competitive results, comparable to neural\nembedding learning techniques, however, with only a fraction of the\ncomputational complexity of these methods. While the proposed derandomization\nenhances the computational and space complexity of our method, the possibility\nof applying weighting methods such as positive pointwise mutual information\n(PPMI) to our models after their construction (and at a reduced dimensionality)\nimparts a high discriminatory power to the resulting embeddings. Obviously,\nthis method comes with other known benefits of random projection-based\ntechniques such as ease of update.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 15:53:00 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 10:27:55 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["QasemiZadeh", "Behrang", ""], ["Kallmeyer", "Laura", ""]]}, {"id": "1705.04304", "submitter": "Romain Paulus", "authors": "Romain Paulus, Caiming Xiong and Richard Socher", "title": "A Deep Reinforced Model for Abstractive Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attentional, RNN-based encoder-decoder models for abstractive summarization\nhave achieved good performance on short input and output sequences. For longer\ndocuments and summaries however these models often include repetitive and\nincoherent phrases. We introduce a neural network model with a novel\nintra-attention that attends over the input and continuously generated output\nseparately, and a new training method that combines standard supervised word\nprediction and reinforcement learning (RL). Models trained only with supervised\nlearning often exhibit \"exposure bias\" - they assume ground truth is provided\nat each step during training. However, when standard word prediction is\ncombined with the global sequence prediction training of RL the resulting\nsummaries become more readable. We evaluate this model on the CNN/Daily Mail\nand New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the\nCNN/Daily Mail dataset, an improvement over previous state-of-the-art models.\nHuman evaluation also shows that our model produces higher quality summaries.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 17:39:35 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 04:11:32 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 20:11:26 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Paulus", "Romain", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1705.04350", "submitter": "Desmond Elliott", "authors": "Desmond Elliott, \\'Akos K\\'ad\\'ar", "title": "Imagination improves Multimodal Translation", "comments": "Clarified main contributions, minor correction to Equation 8,\n  additional comparisons in Table 2, added more related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We decompose multimodal translation into two sub-tasks: learning to translate\nand learning visually grounded representations. In a multitask learning\nframework, translations are learned in an attention-based encoder-decoder, and\ngrounded representations are learned through image representation prediction.\nOur approach improves translation performance compared to the state of the art\non the Multi30K dataset. Furthermore, it is equally effective if we train the\nimage prediction task on the external MS COCO dataset, and we find improvements\nif we train the translation model on the external News Commentary parallel\ntext.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 18:49:17 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 09:18:55 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Elliott", "Desmond", ""], ["K\u00e1d\u00e1r", "\u00c1kos", ""]]}, {"id": "1705.04400", "submitter": "Sanjeev Satheesh", "authors": "Eric Battenberg, Rewon Child, Adam Coates, Christopher Fougner,\n  Yashesh Gaur, Jiaji Huang, Heewoo Jun, Ajay Kannan, Markus Kliegl, Atul\n  Kumar, Hairong Liu, Vinay Rao, Sanjeev Satheesh, David Seetapun, Anuroop\n  Sriram, Zhenyao Zhu", "title": "Reducing Bias in Production Speech Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replacing hand-engineered pipelines with end-to-end deep learning systems has\nenabled strong results in applications like speech and object recognition.\nHowever, the causality and latency constraints of production systems put\nend-to-end speech models back into the underfitting regime and expose biases in\nthe model that we show cannot be overcome by \"scaling up\", i.e., training\nbigger models on more data. In this work we systematically identify and address\nsources of bias, reducing error rates by up to 20% while remaining practical\nfor deployment. We achieve this by utilizing improved neural architectures for\nstreaming inference, solving optimization issues, and employing strategies that\nincrease audio and label modelling versatility.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 23:34:42 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Battenberg", "Eric", ""], ["Child", "Rewon", ""], ["Coates", "Adam", ""], ["Fougner", "Christopher", ""], ["Gaur", "Yashesh", ""], ["Huang", "Jiaji", ""], ["Jun", "Heewoo", ""], ["Kannan", "Ajay", ""], ["Kliegl", "Markus", ""], ["Kumar", "Atul", ""], ["Liu", "Hairong", ""], ["Rao", "Vinay", ""], ["Satheesh", "Sanjeev", ""], ["Seetapun", "David", ""], ["Sriram", "Anuroop", ""], ["Zhu", "Zhenyao", ""]]}, {"id": "1705.04416", "submitter": "Joshua Peterson", "authors": "Dawn Chen, Joshua C. Peterson, Thomas L. Griffiths", "title": "Evaluating vector-space models of analogy", "comments": "6 pages, 4 figures, In the Proceedings of the 39th Annual Conference\n  of the Cognitive Science Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector-space representations provide geometric tools for reasoning about the\nsimilarity of a set of objects and their relationships. Recent machine learning\nmethods for deriving vector-space embeddings of words (e.g., word2vec) have\nachieved considerable success in natural language processing. These vector\nspaces have also been shown to exhibit a surprising capacity to capture verbal\nanalogies, with similar results for natural images, giving new life to a\nclassic model of analogies as parallelograms that was first proposed by\ncognitive scientists. We evaluate the parallelogram model of analogy as applied\nto modern word embeddings, providing a detailed analysis of the extent to which\nthis approach captures human relational similarity judgments in a large\nbenchmark dataset. We find that that some semantic relationships are better\ncaptured than others. We then provide evidence for deeper limitations of the\nparallelogram model based on the intrinsic geometric constraints of vector\nspaces, paralleling classic results for first-order similarity.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 01:26:23 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 20:52:12 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Chen", "Dawn", ""], ["Peterson", "Joshua C.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1705.04434", "submitter": "Peng Qi", "authors": "Peng Qi and Christopher D. Manning", "title": "Arc-swift: A Novel Transition System for Dependency Parsing", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transition-based dependency parsers often need sequences of local shift and\nreduce operations to produce certain attachments. Correct individual decisions\nhence require global information about the sentence context and mistakes cause\nerror propagation. This paper proposes a novel transition system, arc-swift,\nthat enables direct attachments between tokens farther apart with a single\ntransition. This allows the parser to leverage lexical information more\ndirectly in transition decisions. Hence, arc-swift can achieve significantly\nbetter performance with a very small beam size. Our parsers reduce error by\n3.7--7.6% relative to those using existing transition systems on the Penn\nTreebank dependency parsing task and English Universal Dependencies.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 03:44:34 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Qi", "Peng", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1705.04815", "submitter": "Kyle Richardson", "authors": "Kyle Richardson and Jonas Kuhn", "title": "Learning Semantic Correspondences in Technical Documentation", "comments": "accepted to ACL-2017", "journal-ref": null, "doi": "10.18653/v1/P17-1148", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of translating high-level textual descriptions to\nformal representations in technical documentation as part of an effort to model\nthe meaning of such documentation. We focus specifically on the problem of\nlearning translational correspondences between text descriptions and grounded\nrepresentations in the target documentation, such as formal representation of\nfunctions or code templates. Our approach exploits the parallel nature of such\ndocumentation, or the tight coupling between high-level text and the low-level\nrepresentations we aim to learn. Data is collected by mining technical\ndocuments for such parallel text-representation pairs, which we use to train a\nsimple semantic parsing model. We report new baseline results on sixteen novel\ndatasets, including the standard library documentation for nine popular\nprogramming languages across seven natural languages, and a small collection of\nUnix utility manuals.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 12:29:39 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Richardson", "Kyle", ""], ["Kuhn", "Jonas", ""]]}, {"id": "1705.04839", "submitter": "Firoj Alam", "authors": "Firoj Alam, Morena Danieli and Giuseppe Riccardi", "title": "Annotating and Modeling Empathy in Spoken Conversations", "comments": "Journal of Computer Speech and Language", "journal-ref": null, "doi": "10.1016/j.csl.2017.12.003", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Empathy, as defined in behavioral sciences, expresses the ability of human\nbeings to recognize, understand and react to emotions, attitudes and beliefs of\nothers. The lack of an operational definition of empathy makes it difficult to\nmeasure it. In this paper, we address two related problems in automatic\naffective behavior analysis: the design of the annotation protocol and the\nautomatic recognition of empathy from spoken conversations. We propose and\nevaluate an annotation scheme for empathy inspired by the modal model of\nemotions. The annotation scheme was evaluated on a corpus of real-life, dyadic\nspoken conversations. In the context of behavioral analysis, we designed an\nautomatic segmentation and classification system for empathy. Given the\ndifferent speech and language levels of representation where empathy may be\ncommunicated, we investigated features derived from the lexical and acoustic\nspaces. The feature development process was designed to support both the fusion\nand automatic selection of relevant features from high dimensional space. The\nautomatic classification system was evaluated on call center conversations\nwhere it showed significantly better performance than the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 14:49:08 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 20:13:29 GMT"}, {"version": "v3", "created": "Fri, 29 Dec 2017 12:52:49 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Alam", "Firoj", ""], ["Danieli", "Morena", ""], ["Riccardi", "Giuseppe", ""]]}, {"id": "1705.05039", "submitter": "Kechen Qin", "authors": "Kechen Qin, Lu Wang, and Joseph Kim", "title": "Joint Modeling of Content and Discourse Relations in Dialogues", "comments": "Accepted by ACL 2017. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a joint modeling approach to identify salient discussion points in\nspoken meetings as well as to label the discourse relations between speaker\nturns. A variation of our model is also discussed when discourse relations are\ntreated as latent variables. Experimental results on two popular meeting\ncorpora show that our joint model can outperform state-of-the-art approaches\nfor both phrase-based content selection and discourse relation prediction\ntasks. We also evaluate our model on predicting the consistency among team\nmembers' understanding of their group decisions. Classifiers trained with\nfeatures constructed from our model achieve significant better predictive\nperformance than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 23:45:49 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Qin", "Kechen", ""], ["Wang", "Lu", ""], ["Kim", "Joseph", ""]]}, {"id": "1705.05040", "submitter": "Kechen Qin", "authors": "Lu Wang, Nick Beauchamp, Sarah Shugars, and Kechen Qin", "title": "Winning on the Merits: The Joint Effects of Content and Style on Debate\n  Outcomes", "comments": "Accepted by TACL, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Debate and deliberation play essential roles in politics and government, but\nmost models presume that debates are won mainly via superior style or agenda\ncontrol. Ideally, however, debates would be won on the merits, as a function of\nwhich side has the stronger arguments. We propose a predictive model of debate\nthat estimates the effects of linguistic features and the latent persuasive\nstrengths of different topics, as well as the interactions between the two.\nUsing a dataset of 118 Oxford-style debates, our model's combination of content\n(as latent topics) and style (as linguistic features) allows us to predict\naudience-adjudicated winners with 74% accuracy, significantly outperforming\nlinguistic features alone (66%). Our model finds that winning sides employ\nstronger arguments, and allows us to identify the linguistic features\nassociated with strong or weak arguments.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 00:21:03 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Wang", "Lu", ""], ["Beauchamp", "Nick", ""], ["Shugars", "Sarah", ""], ["Qin", "Kechen", ""]]}, {"id": "1705.05183", "submitter": "Sahil Manchanda", "authors": "Sahil Manchanda and Ashish Anand", "title": "Representation learning of drug and disease terms for drug repositioning", "comments": "Accepted to appear in 3rd IEEE International Conference on\n  Cybernetics (Spl Session: Deep Learning for Prediction and Estimation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug repositioning (DR) refers to identification of novel indications for the\napproved drugs. The requirement of huge investment of time as well as money and\nrisk of failure in clinical trials have led to surge in interest in drug\nrepositioning. DR exploits two major aspects associated with drugs and\ndiseases: existence of similarity among drugs and among diseases due to their\nshared involved genes or pathways or common biological effects. Existing\nmethods of identifying drug-disease association majorly rely on the information\navailable in the structured databases only. On the other hand, abundant\ninformation available in form of free texts in biomedical research articles are\nnot being fully exploited. Word-embedding or obtaining vector representation of\nwords from a large corpora of free texts using neural network methods have been\nshown to give significant performance for several natural language processing\ntasks. In this work we propose a novel way of representation learning to obtain\nfeatures of drugs and diseases by combining complementary information available\nin unstructured texts and structured datasets. Next we use matrix completion\napproach on these feature vectors to learn projection matrix between drug and\ndisease vector spaces. The proposed method has shown competitive performance\nwith state-of-the-art methods. Further, the case studies on Alzheimer's and\nHypertension diseases have shown that the predicted associations are matching\nwith the existing knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 12:29:52 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 12:29:56 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Manchanda", "Sahil", ""], ["Anand", "Ashish", ""]]}, {"id": "1705.05311", "submitter": "Lukas Galke", "authors": "Lukas Galke, Florian Mai, Alan Schelten, Dennis Brunsch, Ansgar Scherp", "title": "Using Titles vs. Full-text as Source for Automated Semantic Document\n  Annotation", "comments": "Accepted as SHORT PAPER by K-CAP 2017, 9 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant part of the largest Knowledge Graph today, the Linked Open Data\ncloud, consists of metadata about documents such as publications, news reports,\nand other media articles. While the widespread access to the document metadata\nis a tremendous advancement, it is yet not so easy to assign semantic\nannotations and organize the documents along semantic concepts. Providing\nsemantic annotations like concepts in SKOS thesauri is a classical research\ntopic, but typically it is conducted on the full-text of the documents. For the\nfirst time, we offer a systematic comparison of classification approaches to\ninvestigate how far semantic annotations can be conducted using just the\nmetadata of the documents such as titles published as labels on the Linked Open\nData cloud. We compare the classifications obtained from analyzing the\ndocuments' titles with semantic annotations obtained from analyzing the\nfull-text. Apart from the prominent text classification baselines kNN and SVM,\nwe also compare recent techniques of Learning to Rank and neural networks and\nrevisit the traditional methods logistic regression, Rocchio, and Naive Bayes.\nThe results show that across three of our four datasets, the performance of the\nclassifications using only titles reaches over 90% of the quality compared to\nthe classification performance when using the full-text. Thus, conducting\ndocument classification by just using the titles is a reasonable approach for\nautomated semantic annotation and opens up new possibilities for enriching\nKnowledge Graphs.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 16:07:35 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 10:05:49 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Galke", "Lukas", ""], ["Mai", "Florian", ""], ["Schelten", "Alan", ""], ["Brunsch", "Dennis", ""], ["Scherp", "Ansgar", ""]]}, {"id": "1705.05414", "submitter": "Mihail Eric", "authors": "Mihail Eric and Christopher D. Manning", "title": "Key-Value Retrieval Networks for Task-Oriented Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural task-oriented dialogue systems often struggle to smoothly interface\nwith a knowledge base. In this work, we seek to address this problem by\nproposing a new neural dialogue agent that is able to effectively sustain\ngrounded, multi-domain discourse through a novel key-value retrieval mechanism.\nThe model is end-to-end differentiable and does not need to explicitly model\ndialogue state or belief trackers. We also release a new dataset of 3,031\ndialogues that are grounded through underlying knowledge bases and span three\ndistinct tasks in the in-car personal assistant space: calendar scheduling,\nweather information retrieval, and point-of-interest navigation. Our\narchitecture is simultaneously trained on data from all domains and\nsignificantly outperforms a competitive rule-based system and other existing\nneural dialogue architectures on the provided domains according to both\nautomatic and human evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 19:11:12 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 00:45:59 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Eric", "Mihail", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1705.05437", "submitter": "Surag Nair", "authors": "Surag Nair", "title": "A Biomedical Information Extraction Primer for NLP Researchers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical Information Extraction is an exciting field at the crossroads of\nNatural Language Processing, Biology and Medicine. It encompasses a variety of\ndifferent tasks that require application of state-of-the-art NLP techniques,\nsuch as NER and Relation Extraction. This paper provides an overview of the\nproblems in the field and discusses some of the techniques used for solving\nthem.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 10:00:00 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Nair", "Surag", ""]]}, {"id": "1705.05487", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee, Peter Szolovits", "title": "NeuroNER: an easy-to-use program for named-entity recognition based on\n  neural networks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named-entity recognition (NER) aims at identifying entities of interest in a\ntext. Artificial neural networks (ANNs) have recently been shown to outperform\nexisting NER systems. However, ANNs remain challenging to use for non-expert\nusers. In this paper, we present NeuroNER, an easy-to-use named-entity\nrecognition tool based on ANNs. Users can annotate entities using a graphical\nweb-based user interface (BRAT): the annotations are then used to train an ANN,\nwhich in turn predict entities' locations and categories in new texts. NeuroNER\nmakes this annotation-training-prediction flow smooth and accessible to anyone.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 00:03:19 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""], ["Szolovits", "Peter", ""]]}, {"id": "1705.05633", "submitter": "Tao Ding", "authors": "Tao Ding, Warren K. Bickel, Shimei Pan", "title": "Social Media-based Substance Use Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how the state-of-the-art machine learning and\ntext mining techniques can be used to build effective social media-based\nsubstance use detection systems. Since a substance use ground truth is\ndifficult to obtain on a large scale, to maximize system performance, we\nexplore different feature learning methods to take advantage of a large amount\nof unsupervised social media data. We also demonstrate the benefit of using\nmulti-view unsupervised feature learning to combine heterogeneous user\ninformation such as Facebook `\"likes\" and \"status updates\" to enhance system\nperformance. Based on our evaluation, our best models achieved 86% AUC for\npredicting tobacco use, 81% for alcohol use and 84% for drug use, all of which\nsignificantly outperformed existing methods. Our investigation has also\nuncovered interesting relations between a user's social media behavior (e.g.,\nword usage) and substance use.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 10:37:52 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 19:38:14 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Ding", "Tao", ""], ["Bickel", "Warren K.", ""], ["Pan", "Shimei", ""]]}, {"id": "1705.05742", "submitter": "Rakshit Trivedi", "authors": "Rakshit Trivedi, Hanjun Dai, Yichen Wang, Le Song", "title": "Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large scale event data with time stamps has given rise to\ndynamically evolving knowledge graphs that contain temporal information for\neach edge. Reasoning over time in such dynamic knowledge graphs is not yet well\nunderstood. To this end, we present Know-Evolve, a novel deep evolutionary\nknowledge network that learns non-linearly evolving entity representations over\ntime. The occurrence of a fact (edge) is modeled as a multivariate point\nprocess whose intensity function is modulated by the score for that fact\ncomputed based on the learned entity embeddings. We demonstrate significantly\nimproved performance over various relational learning approaches on two large\nscale real-world datasets. Further, our method effectively predicts occurrence\nor recurrence time of a fact which is novel compared to prior reasoning\napproaches in multi-relational setting.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 14:53:02 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 04:54:07 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 05:21:46 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Trivedi", "Rakshit", ""], ["Dai", "Hanjun", ""], ["Wang", "Yichen", ""], ["Song", "Le", ""]]}, {"id": "1705.05762", "submitter": "Felipe  Urbina FelipeUrbina <", "authors": "Felipe Urbina and Javier Vera", "title": "A decentralized route to the origins of scaling in human language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Zipf's law establishes that if the words of a (large) text are ordered by\ndecreasing frequency, the frequency versus the rank decreases as a power law\nwith exponent close to $-1$. Previous work has stressed that this pattern\narises from a conflict of interests of the participants of communication. The\nchallenge here is to define a computational multi-agent language game, mainly\nbased on a parameter that measures the relative participant's interests.\nNumerical simulations suggest that at critical values of the parameter a\nhuman-like vocabulary, exhibiting scaling properties, seems to appear. The\nappearance of an intermediate distribution of frequencies at some critical\nvalues of the parameter suggests that on a population of artificial agents the\nemergence of scaling partly arises as a self-organized process only from local\ninteractions between agents.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 15:23:14 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 17:35:00 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Urbina", "Felipe", ""], ["Vera", "Javier", ""]]}, {"id": "1705.05940", "submitter": "Enes Avcu", "authors": "Enes Avcu, Chihiro Shibata and Jeffrey Heinz", "title": "Subregular Complexity and Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper argues that the judicial use of formal language theory and\ngrammatical inference are invaluable tools in understanding how deep neural\nnetworks can and cannot represent and learn long-term dependencies in temporal\nsequences. Learning experiments were conducted with two types of Recurrent\nNeural Networks (RNNs) on six formal languages drawn from the Strictly Local\n(SL) and Strictly Piecewise (SP) classes. The networks were Simple RNNs\n(s-RNNs) and Long Short-Term Memory RNNs (LSTMs) of varying sizes. The SL and\nSP classes are among the simplest in a mathematically well-understood hierarchy\nof subregular classes. They encode local and long-term dependencies,\nrespectively. The grammatical inference algorithm Regular Positive and Negative\nInference (RPNI) provided a baseline. According to earlier research, the LSTM\narchitecture should be capable of learning long-term dependencies and should\noutperform s-RNNs. The results of these experiments challenge this narrative.\nFirst, the LSTMs' performance was generally worse in the SP experiments than in\nthe SL ones. Second, the s-RNNs out-performed the LSTMs on the most complex SP\nexperiment and performed comparably to them on the others.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 22:13:45 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 02:18:14 GMT"}, {"version": "v3", "created": "Sat, 14 Oct 2017 18:24:08 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Avcu", "Enes", ""], ["Shibata", "Chihiro", ""], ["Heinz", "Jeffrey", ""]]}, {"id": "1705.05952", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Mark Dras, Mark Johnson", "title": "A Novel Neural Network Model for Joint POS Tagging and Graph-based\n  Dependency Parsing", "comments": "v2: also include universal POS tagging, UAS and LAS accuracies w.r.t\n  gold-standard segmentation on Universal Dependencies 2.0 - CoNLL 2017 shared\n  task test data; in CoNLL 2017", "journal-ref": null, "doi": "10.18653/v1/K17-3014", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural network model that learns POS tagging and\ngraph-based dependency parsing jointly. Our model uses bidirectional LSTMs to\nlearn feature representations shared for both POS tagging and dependency\nparsing tasks, thus handling the feature-engineering problem. Our extensive\nexperiments, on 19 languages from the Universal Dependencies project, show that\nour model outperforms the state-of-the-art neural network-based\nStack-propagation model for joint POS tagging and transition-based dependency\nparsing, resulting in a new state of the art. Our code is open-source and\navailable together with pre-trained models at:\nhttps://github.com/datquocnguyen/jPTDP\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 23:09:00 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 18:25:57 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Dras", "Mark", ""], ["Johnson", "Mark", ""]]}, {"id": "1705.05992", "submitter": "Jun Zhang", "authors": "Xu Tian, Jun Zhang, Zejun Ma, Yi He, Juan Wei", "title": "Frame Stacking and Retaining for Recurrent Neural Network Acoustic Model", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frame stacking is broadly applied in end-to-end neural network training like\nconnectionist temporal classification (CTC), and it leads to more accurate\nmodels and faster decoding. However, it is not well-suited to conventional\nneural network based on context-dependent state acoustic model, if the decoder\nis unchanged. In this paper, we propose a novel frame retaining method which is\napplied in decoding. The system which combined frame retaining with frame\nstacking could reduces the time consumption of both training and decoding. Long\nshort-term memory (LSTM) recurrent neural networks (RNNs) using it achieve\nalmost linear training speedup and reduces relative 41\\% real time factor\n(RTF). At the same time, recognition performance is no degradation or improves\nsightly on Shenma voice search dataset in Mandarin.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 02:34:27 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Tian", "Xu", ""], ["Zhang", "Jun", ""], ["Ma", "Zejun", ""], ["He", "Yi", ""], ["Wei", "Juan", ""]]}, {"id": "1705.06031", "submitter": "Xiaojun Wan", "authors": "Wei Wei and Xiaojun Wan", "title": "Learning to Identify Ambiguous and Misleading News Headlines", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy is one of the basic principles of journalism. However, it is\nincreasingly hard to manage due to the diversity of news media. Some editors of\nonline news tend to use catchy headlines which trick readers into clicking.\nThese headlines are either ambiguous or misleading, degrading the reading\nexperience of the audience. Thus, identifying inaccurate news headlines is a\ntask worth studying. Previous work names these headlines \"clickbaits\" and\nmainly focus on the features extracted from the headlines, which limits the\nperformance since the consistency between headlines and news bodies is\nunderappreciated. In this paper, we clearly redefine the problem and identify\nambiguous and misleading headlines separately. We utilize class sequential\nrules to exploit structure information when detecting ambiguous headlines. For\nthe identification of misleading headlines, we extract features based on the\ncongruence between headlines and bodies. To make use of the large unlabeled\ndata set, we apply a co-training method and gain an increase in performance.\nThe experiment results show the effectiveness of our methods. Then we use our\nclassifiers to detect inaccurate headlines crawled from different sources and\nconduct a data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 07:24:02 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 13:01:24 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Wei", "Wei", ""], ["Wan", "Xiaojun", ""]]}, {"id": "1705.06106", "submitter": "Katharina Kann", "authors": "Katharina Kann and Hinrich Sch\\\"utze", "title": "Unlabeled Data for Morphological Generation With Character-Based\n  Sequence-to-Sequence Models", "comments": "Accepted at SCLeM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-supervised way of training a character-based\nencoder-decoder recurrent neural network for morphological reinflection, the\ntask of generating one inflected word form from another. This is achieved by\nusing unlabeled tokens or random strings as training data for an autoencoding\ntask, adapting a network for morphological reinflection, and performing\nmulti-task training. We thus use limited labeled data more effectively,\nobtaining up to 9.9% improvement over state-of-the-art baselines for 8\ndifferent languages.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 11:48:15 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 13:02:20 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Kann", "Katharina", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1705.06262", "submitter": "Vincent Major", "authors": "Vincent Major, Alisa Surkis, and Yindalon Aphinyanaphongs", "title": "Utility of General and Specific Word Embeddings for Classifying\n  Translational Stages of Research", "comments": "10 pages. Accepted to AMIA 2018 Annual Symposium, San Francisco,\n  November 3-7, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional text classification models make a bag-of-words assumption\nreducing text into word occurrence counts per document. Recent algorithms such\nas word2vec are capable of learning semantic meaning and similarity between\nwords in an entirely unsupervised manner using a contextual window and doing so\nmuch faster than previous methods. Each word is projected into vector space\nsuch that similar meaning words such as \"strong\" and \"powerful\" are projected\ninto the same general Euclidean space. Open questions about these embeddings\ninclude their utility across classification tasks and the optimal properties\nand source of documents to construct broadly functional embeddings. In this\nwork, we demonstrate the usefulness of pre-trained embeddings for\nclassification in our task and demonstrate that custom word embeddings, built\nin the domain and for the tasks, can improve performance over word embeddings\nlearnt on more general data including news articles or Wikipedia.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:08:11 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 23:40:23 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Major", "Vincent", ""], ["Surkis", "Alisa", ""], ["Aphinyanaphongs", "Yindalon", ""]]}, {"id": "1705.06273", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt, Peter Szolovits", "title": "Transfer Learning for Named-Entity Recognition with Neural Networks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for named-entity recognition (NER). In order to achieve high\nperformances, ANNs need to be trained on a large labeled dataset. However,\nlabels might be difficult to obtain for the dataset on which the user wants to\nperform NER: label scarcity is particularly pronounced for patient note\nde-identification, which is an instance of NER. In this work, we analyze to\nwhat extent transfer learning may address this issue. In particular, we\ndemonstrate that transferring an ANN model trained on a large labeled dataset\nto another dataset with a limited number of labels improves upon the\nstate-of-the-art results on two different datasets for patient note\nde-identification.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:45:15 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""], ["Szolovits", "Peter", ""]]}, {"id": "1705.06353", "submitter": "Christophe Bruchansky", "authors": "Christophe Bruchansky", "title": "Political Footprints: Political Discourse Analysis using Pre-Trained\n  Word Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss how machine learning could be used to produce a\nsystematic and more objective political discourse analysis. Political\nfootprints are vector space models (VSMs) applied to political discourse. Each\nof their vectors represents a word, and is produced by training the English\nlexicon on large text corpora. This paper presents a simple implementation of\npolitical footprints, some heuristics on how to use them, and their application\nto four cases: the U.N. Kyoto Protocol and Paris Agreement, and two U.S.\npresidential elections. The reader will be offered a number of reasons to\nbelieve that political footprints produce meaningful results, along with some\nsuggestions on how to improve their implementation.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 21:29:08 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Bruchansky", "Christophe", ""]]}, {"id": "1705.06369", "submitter": "Edoardo Maria Ponti", "authors": "Edoardo Maria Ponti, Ivan Vuli\\'c, Anna Korhonen", "title": "Decoding Sentiment from Distributed Representations of Sentences", "comments": null, "journal-ref": null, "doi": "10.18653/v1/S17-1003", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of sentences have been developed recently to\nrepresent their meaning as real-valued vectors. However, it is not clear how\nmuch information such representations retain about the polarity of sentences.\nTo study this question, we decode sentiment from unsupervised sentence\nrepresentations learned with different architectures (sensitive to the order of\nwords, the order of sentences, or none) in 9 typologically diverse languages.\nSentiment results from the (recursive) composition of lexical items and\ngrammatical strategies such as negation and concession. The results are\nmanifold: we show that there is no `one-size-fits-all' representation\narchitecture outperforming the others across the board. Rather, the top-ranking\narchitectures depend on the language and data at hand. Moreover, we find that\nin several cases the additive composition model based on skip-gram word vectors\nmay surpass supervised state-of-art architectures such as bidirectional LSTMs.\nFinally, we provide a possible explanation of the observed variation based on\nthe type of negative constructions in each language.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 23:32:12 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 15:51:54 GMT"}, {"version": "v3", "created": "Fri, 16 Jun 2017 18:34:43 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ponti", "Edoardo Maria", ""], ["Vuli\u0107", "Ivan", ""], ["Korhonen", "Anna", ""]]}, {"id": "1705.06400", "submitter": "Matthias Plappert", "authors": "Matthias Plappert, Christian Mandery, Tamim Asfour", "title": "Learning a bidirectional mapping between human whole-body motion and\n  natural language using deep recurrent neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.robot.2018.07.006", "report-no": null, "categories": "cs.LG cs.CL cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking human whole-body motion and natural language is of great interest for\nthe generation of semantic representations of observed human behaviors as well\nas for the generation of robot behaviors based on natural language input. While\nthere has been a large body of research in this area, most approaches that\nexist today require a symbolic representation of motions (e.g. in the form of\nmotion primitives), which have to be defined a-priori or require complex\nsegmentation algorithms. In contrast, recent advances in the field of neural\nnetworks and especially deep learning have demonstrated that sub-symbolic\nrepresentations that can be learned end-to-end usually outperform more\ntraditional approaches, for applications such as machine translation. In this\npaper we propose a generative model that learns a bidirectional mapping between\nhuman whole-body motion and natural language using deep recurrent neural\nnetworks (RNNs) and sequence-to-sequence learning. Our approach does not\nrequire any segmentation or manual feature engineering and learns a distributed\nrepresentation, which is shared for all motions and descriptions. We evaluate\nour approach on 2,846 human whole-body motions and 6,187 natural language\ndescriptions thereof from the KIT Motion-Language Dataset. Our results clearly\ndemonstrate the effectiveness of the proposed model: We show that our model\ngenerates a wide variety of realistic motions only from descriptions thereof in\nform of a single sentence. Conversely, our model is also capable of generating\ncorrect and detailed natural language descriptions from human motions.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 02:50:40 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 10:07:57 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Plappert", "Matthias", ""], ["Mandery", "Christian", ""], ["Asfour", "Tamim", ""]]}, {"id": "1705.06457", "submitter": "Augustin Speyer", "authors": "Augustin Speyer, Robin Lemke", "title": "Information Density as a Factor for Variation in the Embedding of\n  Relative Clauses", "comments": "10 pages. To be submitted in a German version to 'Sprachwissenschaft'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In German, relative clauses can be positioned in-situ or extraposed. A\npotential factor for the variation might be information density. In this study,\nthis hypothesis is tested with a corpus of 17th century German funeral sermons.\nFor each referent in the relative clauses and their matrix clauses, the\nattention state was determined (first calculation). In a second calculation,\nfor each word the surprisal values were determined, using a bi-gram language\nmodel. In a third calculation, the surprisal values were accommodated as to\nwhether it is the first occurrence of the word in question or not. All three\ncalculations pointed in the same direction: With in-situ relative clauses, the\nrate of new referents was lower and the average surprisal values were lower,\nespecially the accommodated surprisal values, than with extraposed relative\nclauses. This indicated that in-formation density is a factor governing the\nchoice between in-situ and extraposed relative clauses. The study also sheds\nlight on the intrinsic relation-ship between the information theoretic concept\nof information density and in-formation structural concepts such as givenness\nwhich are used under a more linguistic perspective.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 08:16:20 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Speyer", "Augustin", ""], ["Lemke", "Robin", ""]]}, {"id": "1705.06463", "submitter": "Hongmin Wang", "authors": "Hongmin Wang, Yue Zhang, GuangYong Leonard Chan, Jie Yang, Hai Leong\n  Chieu", "title": "Universal Dependencies Parsing for Colloquial Singaporean English", "comments": "Accepted by ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singlish can be interesting to the ACL community both linguistically as a\nmajor creole based on English, and computationally for information extraction\nand sentiment analysis of regional social media. We investigate dependency\nparsing of Singlish by constructing a dependency treebank under the Universal\nDependencies scheme, and then training a neural network model by integrating\nEnglish syntactic knowledge into a state-of-the-art parser trained on the\nSinglish treebank. Results show that English knowledge can lead to 25% relative\nerror reduction, resulting in a parser of 84.47% accuracies. To the best of our\nknowledge, we are the first to use neural stacking to improve cross-lingual\ndependency parsing on low-resource languages. We make both our annotation and\nparser available for further research.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 08:27:42 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Wang", "Hongmin", ""], ["Zhang", "Yue", ""], ["Chan", "GuangYong Leonard", ""], ["Yang", "Jie", ""], ["Chieu", "Hai Leong", ""]]}, {"id": "1705.06476", "submitter": "Jason  Weston", "authors": "Alexander H. Miller, Will Feng, Adam Fisch, Jiasen Lu, Dhruv Batra,\n  Antoine Bordes, Devi Parikh, Jason Weston", "title": "ParlAI: A Dialog Research Software Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ParlAI (pronounced \"par-lay\"), an open-source software platform\nfor dialog research implemented in Python, available at http://parl.ai. Its\ngoal is to provide a unified framework for sharing, training and testing of\ndialog models, integration of Amazon Mechanical Turk for data collection, human\nevaluation, and online/reinforcement learning; and a repository of machine\nlearning models for comparing with others' models, and improving upon existing\narchitectures. Over 20 tasks are supported in the first release, including\npopular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail,\nCBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated,\nincluding neural models such as memory networks, seq2seq and attentive LSTMs.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 08:54:47 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 18:35:03 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 04:17:48 GMT"}, {"version": "v4", "created": "Thu, 8 Mar 2018 19:58:17 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Miller", "Alexander H.", ""], ["Feng", "Will", ""], ["Fisch", "Adam", ""], ["Lu", "Jiasen", ""], ["Batra", "Dhruv", ""], ["Bordes", "Antoine", ""], ["Parikh", "Devi", ""], ["Weston", "Jason", ""]]}, {"id": "1705.06510", "submitter": "Alessio Cardillo", "authors": "Andrea Martini and Alessio Cardillo and Paolo De Los Rios", "title": "Entropic selection of concepts unveils hidden topics in documents\n  corpora", "comments": "Main + SI. Major overhaul after first round of review", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The organization and evolution of science has recently become itself an\nobject of scientific quantitative investigation, thanks to the wealth of\ninformation that can be extracted from scientific documents, such as citations\nbetween papers and co-authorship between researchers. However, only few studies\nhave focused on the concepts that characterize full documents and that can be\nextracted and analyzed, revealing the deeper organization of scientific\nknowledge. Unfortunately, several concepts can be so common across documents\nthat they hinder the emergence of the underlying topical structure of the\ndocument corpus, because they give rise to a large amount of spurious and\ntrivial relations among documents. To identify and remove common concepts, we\nintroduce a method to gauge their relevance according to an objective\ninformation-theoretic measure related to the statistics of their occurrence\nacross the document corpus. After progressively removing concepts that,\naccording to this metric, can be considered as generic, we find that the topic\norganization displays a correspondingly more refined structure.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 10:24:03 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 07:17:34 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Martini", "Andrea", ""], ["Cardillo", "Alessio", ""], ["Rios", "Paolo De Los", ""]]}, {"id": "1705.06824", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Shuiwang Ji", "title": "Learning Convolutional Text Representations for Visual Question\n  Answering", "comments": "Conference paper at SDM 2018. https://github.com/divelab/svae", "journal-ref": "In proceedings of the 2018 SIAM International Conference on Data\n  Mining (pp. 594-602). 2018", "doi": "10.1137/1.9781611975321.67", "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is a recently proposed artificial intelligence task\nthat requires a deep understanding of both images and texts. In deep learning,\nimages are typically modeled through convolutional neural networks, and texts\nare typically modeled through recurrent neural networks. While the requirement\nfor modeling images is similar to traditional computer vision tasks, such as\nobject recognition and image classification, visual question answering raises a\ndifferent need for textual representation as compared to other natural language\nprocessing tasks. In this work, we perform a detailed analysis on natural\nlanguage questions in visual question answering. Based on the analysis, we\npropose to rely on convolutional neural networks for learning textual\nrepresentations. By exploring the various properties of convolutional neural\nnetworks specialized for text data, such as width and depth, we present our\n\"CNN Inception + Gate\" model. We show that our model improves question\nrepresentations and thus the overall accuracy of visual question answering\nmodels. We also show that the text representation requirement in visual\nquestion answering is more complicated and comprehensive than that in\nconventional natural language processing tasks, making it a better task to\nevaluate textual representation methods. Shallow models like fastText, which\ncan obtain comparable results with deep learning models in tasks like text\nclassification, are not suitable in visual question answering.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:51:44 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 17:38:50 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.07008", "submitter": "Leandro dos Santos", "authors": "Leandro B. dos Santos, Magali S. Duran, Nathan S. Hartmann, Arnaldo\n  Candido Jr., Gustavo H. Paetzold, Sandra M. Aluisio", "title": "A Lightweight Regression Method to Infer Psycholinguistic Properties for\n  Brazilian Portuguese", "comments": "Paper accepted for TSD2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psycholinguistic properties of words have been used in various approaches to\nNatural Language Processing tasks, such as text simplification and readability\nassessment. Most of these properties are subjective, involving costly and\ntime-consuming surveys to be gathered. Recent approaches use the limited\ndatasets of psycholinguistic properties to extend them automatically to large\nlexicons. However, some of the resources used by such approaches are not\navailable to most languages. This study presents a method to infer\npsycholinguistic properties for Brazilian Portuguese (BP) using regressors\nbuilt with a light set of features usually available for less resourced\nlanguages: word length, frequency lists, lexical databases composed of school\ndictionaries and word embedding models. The correlations between the properties\ninferred are close to those obtained by related works. The resulting resource\ncontains 26,874 words in BP annotated with concreteness, age of acquisition,\nimageability and subjective frequency.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 14:17:31 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Santos", "Leandro B. dos", ""], ["Duran", "Magali S.", ""], ["Hartmann", "Nathan S.", ""], ["Candido", "Arnaldo", "Jr."], ["Paetzold", "Gustavo H.", ""], ["Aluisio", "Sandra M.", ""]]}, {"id": "1705.07136", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham Neubig, Eduard Hovy", "title": "Softmax Q-Distribution Estimation for Structured Prediction: A\n  Theoretical Interpretation for RAML", "comments": "Under Review of ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reward augmented maximum likelihood (RAML), a simple and effective learning\nframework to directly optimize towards the reward function in structured\nprediction tasks, has led to a number of impressive empirical successes. RAML\nincorporates task-specific reward by performing maximum-likelihood updates on\ncandidate outputs sampled according to an exponentiated payoff distribution,\nwhich gives higher probabilities to candidates that are close to the reference\noutput. While RAML is notable for its simplicity, efficiency, and its\nimpressive empirical successes, the theoretical properties of RAML, especially\nthe behavior of the exponentiated payoff distribution, has not been examined\nthoroughly. In this work, we introduce softmax Q-distribution estimation, a\nnovel theoretical interpretation of RAML, which reveals the relation between\nRAML and Bayesian decision theory. The softmax Q-distribution can be regarded\nas a smooth approximation of the Bayes decision boundary, and the Bayes\ndecision rule is achieved by decoding with this Q-distribution. We further show\nthat RAML is equivalent to approximately estimating the softmax Q-distribution,\nwith the temperature $\\tau$ controlling approximation error. We perform two\nexperiments, one on synthetic data of multi-class classification and one on\nreal data of image captioning, to demonstrate the relationship between RAML and\nthe proposed softmax Q-distribution estimation method, verifying our\ntheoretical analysis. Additional experiments on three structured prediction\ntasks with rewards defined on sequential (named entity recognition), tree-based\n(dependency parsing) and irregular (machine translation) structures show\nnotable improvements over maximum likelihood baselines.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 18:17:00 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 04:17:49 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 19:50:50 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Ma", "Xuezhe", ""], ["Yin", "Pengcheng", ""], ["Liu", "Jingzhou", ""], ["Neubig", "Graham", ""], ["Hovy", "Eduard", ""]]}, {"id": "1705.07267", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Yong Wang, Kyunghyun Cho and Victor O.K. Li", "title": "Search Engine Guided Non-Parametric Neural Machine Translation", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend an attention-based neural machine translation (NMT)\nmodel by allowing it to access an entire training set of parallel sentence\npairs even after training. The proposed approach consists of two stages. In the\nfirst stage--retrieval stage--, an off-the-shelf, black-box search engine is\nused to retrieve a small subset of sentence pairs from a training set given a\nsource sentence. These pairs are further filtered based on a fuzzy matching\nscore based on edit distance. In the second stage--translation stage--, a novel\ntranslation model, called translation memory enhanced NMT (TM-NMT), seamlessly\nuses both the source sentence and a set of retrieved sentence pairs to perform\nthe translation. Empirical evaluation on three language pairs (En-Fr, En-De,\nand En-Es) shows that the proposed approach significantly outperforms the\nbaseline approach and the improvement is more significant when more relevant\nsentence pairs were retrieved.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 06:53:09 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 08:15:24 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Gu", "Jiatao", ""], ["Wang", "Yong", ""], ["Cho", "Kyunghyun", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1705.07318", "submitter": "Chun Tian", "authors": "Chun Tian", "title": "Formalized Lambek Calculus in Higher Order Logic (HOL4)", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, a rather complete proof-theoretical formalization of Lambek\nCalculus (non-associative with arbitrary extensions) has been ported from Coq\nproof assistent to HOL4 theorem prover, with some improvements and new\ntheorems.\n  Three deduction systems (Syntactic Calculus, Natural Deduction and Sequent\nCalculus) of Lambek Calculus are defined with many related theorems proved. The\nequivalance between these systems are formally proved. Finally, a formalization\nof Sequent Calculus proofs (where Coq has built-in supports) has been designed\nand implemented in HOL4. Some basic results including the sub-formula\nproperties of the so-called \"cut-free\" proofs are formally proved.\n  This work can be considered as the preliminary work towards a language parser\nbased on category grammars which is not multimodal but still has ability to\nsupport context-sensitive languages through customized extensions.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 15:04:34 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Tian", "Chun", ""]]}, {"id": "1705.07368", "submitter": "James Foulds", "authors": "James Foulds", "title": "Mixed Membership Word Embeddings for Computational Social Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings improve the performance of NLP systems by revealing the\nhidden structural relationships between words. Despite their success in many\napplications, word embeddings have seen very little use in computational social\nscience NLP tasks, presumably due to their reliance on big data, and to a lack\nof interpretability. I propose a probabilistic model-based word embedding\nmethod which can recover interpretable embeddings, without big data. The key\ninsight is to leverage mixed membership modeling, in which global\nrepresentations are shared, but individual entities (i.e. dictionary words) are\nfree to use these representations to uniquely differing degrees. I show how to\ntrain the model using a combination of state-of-the-art training techniques for\nword embeddings and topic models. The experimental results show an improvement\nin predictive language modeling of up to 63% in MRR over the skip-gram, and\ndemonstrate that the representations are beneficial for supervised learning. I\nillustrate the interpretability of the models with computational social science\ncase studies on State of the Union addresses and NIPS articles.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 23:45:54 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 03:12:35 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 00:34:49 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Foulds", "James", ""]]}, {"id": "1705.07371", "submitter": "Utkarsh Porwal", "authors": "Yingbo Zhou, Utkarsh Porwal, Roberto Konow", "title": "Spelling Correction as a Foreign Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we reformulated the spell correction problem as a machine\ntranslation task under the encoder-decoder framework. This reformulation\nenabled us to use a single model for solving the problem that is traditionally\nformulated as learning a language model and an error model. This model employs\nmulti-layer recurrent neural networks as an encoder and a decoder. We\ndemonstrate the effectiveness of this model using an internal dataset, where\nthe training data is automatically obtained from user logs. The model offers\ncompetitive performance as compared to the state of the art methods but does\nnot require any feature engineering nor hand tuning between models.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 00:14:07 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 22:47:15 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Zhou", "Yingbo", ""], ["Porwal", "Utkarsh", ""], ["Konow", "Roberto", ""]]}, {"id": "1705.07393", "submitter": "Kenton Lee", "authors": "Kenton Lee, Omer Levy, Luke Zettlemoyer", "title": "Recurrent Additive Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce recurrent additive networks (RANs), a new gated RNN which is\ndistinguished by the use of purely additive latent state updates. At every time\nstep, the new state is computed as a gated component-wise sum of the input and\nthe previous state, without any of the non-linearities commonly used in RNN\ntransition dynamics. We formally show that RAN states are weighted sums of the\ninput vectors, and that the gates only contribute to computing the weights of\nthese sums. Despite this relatively simple functional form, experiments\ndemonstrate that RANs perform on par with LSTMs on benchmark language modeling\nproblems. This result shows that many of the non-linear computations in LSTMs\nand related networks are not essential, at least for the problems we consider,\nand suggests that the gates are doing more of the computational work than\npreviously understood.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 04:42:11 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 14:37:32 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Lee", "Kenton", ""], ["Levy", "Omer", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1705.07425", "submitter": "Thomas Niebler", "authors": "Thomas Niebler, Martin Becker, Christian P\\\"olitz, Andreas Hotho", "title": "Learning Semantic Relatedness From Human Feedback Using Metric Learning", "comments": "Under review at ISWC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the degree of semantic relatedness between words is an important\ntask with a variety of semantic applications, such as ontology learning for the\nSemantic Web, semantic search or query expansion. To accomplish this in an\nautomated fashion, many relatedness measures have been proposed. However, most\nof these metrics only encode information contained in the underlying corpus and\nthus do not directly model human intuition. To solve this, we propose to\nutilize a metric learning approach to improve existing semantic relatedness\nmeasures by learning from additional information, such as explicit human\nfeedback. For this, we argue to use word embeddings instead of traditional\nhigh-dimensional vector representations in order to leverage their semantic\ndensity and to reduce computational cost. We rigorously test our approach on\nseveral domains including tagging data as well as publicly available embeddings\nbased on Wikipedia texts and navigation. Human feedback about semantic\nrelatedness for learning and evaluation is extracted from publicly available\ndatasets such as MEN or WS-353. We find that our method can significantly\nimprove semantic relatedness measures by learning from additional information,\nsuch as explicit human feedback. For tagging data, we are the first to generate\nand study embeddings. Our results are of special interest for ontology and\nrecommendation engineers, but also for any other researchers and practitioners\nof Semantic Web techniques.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 10:16:49 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 13:07:07 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Niebler", "Thomas", ""], ["Becker", "Martin", ""], ["P\u00f6litz", "Christian", ""], ["Hotho", "Andreas", ""]]}, {"id": "1705.07687", "submitter": "Aitor Garcia Pablos", "authors": "Aitor Garc\\'ia-Pablos, Montse Cuadros, German Rigau", "title": "W2VLDA: Almost Unsupervised System for Aspect Based Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase of online customer opinions in specialised websites and\nsocial networks, the necessity of automatic systems to help to organise and\nclassify customer reviews by domain-specific aspect/categories and sentiment\npolarity is more important than ever. Supervised approaches to Aspect Based\nSentiment Analysis obtain good results for the domain/language their are\ntrained on, but having manually labelled data for training supervised systems\nfor all domains and languages are usually very costly and time consuming. In\nthis work we describe W2VLDA, an almost unsupervised system based on topic\nmodelling, that combined with some other unsupervised methods and a minimal\nconfiguration, performs aspect/category classifiation,\naspect-terms/opinion-words separation and sentiment polarity classification for\nany given domain and language. We evaluate the performance of the aspect and\nsentiment classification in the multilingual SemEval 2016 task 5 (ABSA)\ndataset. We show competitive results for several languages (English, Spanish,\nFrench and Dutch) and domains (hotels, restaurants, electronic-devices).\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 12:01:10 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 07:36:48 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Garc\u00eda-Pablos", "Aitor", ""], ["Cuadros", "Montse", ""], ["Rigau", "German", ""]]}, {"id": "1705.07704", "submitter": "Mathieu Blondel", "authors": "Vlad Niculae and Mathieu Blondel", "title": "A Regularized Framework for Sparse and Structured Neural Attention", "comments": "In proceedings of NeurIPS 2017; added errata", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks are often augmented with an attention mechanism, which\ntells the network where to focus within the input. We propose in this paper a\nnew framework for sparse and structured attention, building upon a smoothed max\noperator. We show that the gradient of this operator defines a mapping from\nreal values to probabilities, suitable as an attention mechanism. Our framework\nincludes softmax and a slight generalization of the recently-proposed sparsemax\nas special cases. However, we also show how our framework can incorporate\nmodern structured penalties, resulting in more interpretable attention\nmechanisms, that focus on entire segments or groups of an input. We derive\nefficient algorithms to compute the forward and backward passes of our\nattention mechanisms, enabling their use in a neural network trained with\nbackpropagation. To showcase their potential as a drop-in replacement for\nexisting ones, we evaluate our attention mechanisms on three large-scale tasks:\ntextual entailment, machine translation, and sentence summarization. Our\nattention mechanisms improve interpretability without sacrificing performance;\nnotably, on textual entailment and summarization, we outperform the standard\nattention mechanisms based on softmax and sparsemax.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 13:11:24 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 23:58:20 GMT"}, {"version": "v3", "created": "Sat, 23 Feb 2019 01:00:47 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Niculae", "Vlad", ""], ["Blondel", "Mathieu", ""]]}, {"id": "1705.07830", "submitter": "Jannis Bulian", "authors": "Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech\n  Gajewski, Andrea Gesmundo, Neil Houlsby, Wei Wang", "title": "Ask the Right Questions: Active Question Reformulation with\n  Reinforcement Learning", "comments": null, "journal-ref": "Sixth International Conference on Learning Representations (ICLR),\n  2018", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We frame Question Answering (QA) as a Reinforcement Learning task, an\napproach that we call Active Question Answering. We propose an agent that sits\nbetween the user and a black box QA system and learns to reformulate questions\nto elicit the best possible answers. The agent probes the system with,\npotentially many, natural language reformulations of an initial question and\naggregates the returned evidence to yield the best answer. The reformulation\nsystem is trained end-to-end to maximize answer quality using policy gradient.\nWe evaluate on SearchQA, a dataset of complex questions extracted from\nJeopardy!. The agent outperforms a state-of-the-art base model, playing the\nrole of the environment, and other benchmarks. We also analyze the language\nthat the agent has learned while interacting with the question answering\nsystem. We find that successful question reformulations look quite different\nfrom natural language paraphrases. The agent is able to discover non-trivial\nreformulation strategies that resemble classic information retrieval techniques\nsuch as term re-weighting (tf-idf) and stemming.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 16:19:21 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 12:21:14 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 16:02:24 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Buck", "Christian", ""], ["Bulian", "Jannis", ""], ["Ciaramita", "Massimiliano", ""], ["Gajewski", "Wojciech", ""], ["Gesmundo", "Andrea", ""], ["Houlsby", "Neil", ""], ["Wang", "Wei", ""]]}, {"id": "1705.07860", "submitter": "Graham Neubig", "authors": "Graham Neubig and Yoav Goldberg and Chris Dyer", "title": "On-the-fly Operation Batching in Dynamic Computation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer\nmore flexibility for implementing models that cope with data of varying\ndimensions and structure, relative to toolkits that operate on statically\ndeclared computations (e.g., TensorFlow, CNTK, and Theano). However, existing\ntoolkits - both static and dynamic - require that the developer organize the\ncomputations into the batches necessary for exploiting high-performance\nalgorithms and hardware. This batching task is generally difficult, but it\nbecomes a major hurdle as architectures become complex. In this paper, we\npresent an algorithm, and its implementation in the DyNet toolkit, for\nautomatically batching operations. Developers simply write minibatch\ncomputations as aggregations of single instance computations, and the batching\nalgorithm seamlessly executes them, on the fly, using computationally efficient\nbatched operations. On a variety of tasks, we obtain throughput similar to that\nobtained with manual batches, as well as comparable speedups over\nsingle-instance learning on architectures that are impractical to batch\nmanually.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:04:56 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Neubig", "Graham", ""], ["Goldberg", "Yoav", ""], ["Dyer", "Chris", ""]]}, {"id": "1705.07962", "submitter": "Tony Beltramelli", "authors": "Tony Beltramelli", "title": "pix2code: Generating Code from a Graphical User Interface Screenshot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transforming a graphical user interface screenshot created by a designer into\ncomputer code is a typical task conducted by a developer in order to build\ncustomized software, websites, and mobile applications. In this paper, we show\nthat deep learning methods can be leveraged to train a model end-to-end to\nautomatically generate code from a single input image with over 77% of accuracy\nfor three different platforms (i.e. iOS, Android and web-based technologies).\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:32:20 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 11:27:47 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Beltramelli", "Tony", ""]]}, {"id": "1705.08018", "submitter": "Ashwini Jaya Kumar", "authors": "Ashwini Jaya Kumar, Camilo Morales, Maria-Esther Vidal, Christoph\n  Schmidt, S\\\"oren Auer", "title": "Use of Knowledge Graph in Rescoring the N-Best List in Automatic Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the evolution of neural network based methods, automatic speech\nrecognition (ASR) field has been advanced to a level where building an\napplication with speech interface is a reality. In spite of these advances,\nbuilding a real-time speech recogniser faces several problems such as low\nrecognition accuracy, domain constraint, and out-of-vocabulary words. The low\nrecognition accuracy problem is addressed by improving the acoustic model,\nlanguage model, decoder and by rescoring the N-best list at the output of the\ndecoder. We are considering the N-best list rescoring approach to improve the\nrecognition accuracy. Most of the methods in the literature use the\ngrammatical, lexical, syntactic and semantic connection between the words in a\nrecognised sentence as a feature to rescore. In this paper, we have tried to\nsee the semantic relatedness between the words in a sentence to rescore the\nN-best list. Semantic relatedness is computed using\nTransE~\\cite{bordes2013translating}, a method for low dimensional embedding of\na triple in a knowledge graph. The novelty of the paper is the application of\nsemantic web to automatic speech recognition.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:53:05 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Kumar", "Ashwini Jaya", ""], ["Morales", "Camilo", ""], ["Vidal", "Maria-Esther", ""], ["Schmidt", "Christoph", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1705.08038", "submitter": "Vivek Kulkarni", "authors": "Vivek Kulkarni, Margaret L. Kern, David Stillwell, Michal Kosinski,\n  Sandra Matz, Lyle Ungar, Steven Skiena, H. Andrew Schwartz", "title": "Latent Human Traits in the Language of Social Media: An Open-Vocabulary\n  Approach", "comments": "In submission to PLOS One", "journal-ref": null, "doi": "10.1371/journal.pone.0201703", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past century, personality theory and research has successfully\nidentified core sets of characteristics that consistently describe and explain\nfundamental differences in the way people think, feel and behave. Such\ncharacteristics were derived through theory, dictionary analyses, and survey\nresearch using explicit self-reports. The availability of social media data\nspanning millions of users now makes it possible to automatically derive\ncharacteristics from language use -- at large scale. Taking advantage of\nlinguistic information available through Facebook, we study the process of\ninferring a new set of potential human traits based on unprompted language use.\nWe subject these new traits to a comprehensive set of evaluations and compare\nthem with a popular five factor model of personality. We find that our\nlanguage-based trait construct is often more generalizable in that it often\npredicts non-questionnaire-based outcomes better than questionnaire-based\ntraits (e.g. entities someone likes, income and intelligence quotient), while\nthe factors remain nearly as stable as traditional factors. Our approach\nsuggests a value in new constructs of personality derived from everyday human\nlanguage use.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 23:13:02 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Kulkarni", "Vivek", ""], ["Kern", "Margaret L.", ""], ["Stillwell", "David", ""], ["Kosinski", "Michal", ""], ["Matz", "Sandra", ""], ["Ungar", "Lyle", ""], ["Skiena", "Steven", ""], ["Schwartz", "H. Andrew", ""]]}, {"id": "1705.08063", "submitter": "Arman Cohan", "authors": "Arman Cohan, Nazli Goharian", "title": "Contextualizing Citations for Scientific Summarization using Word\n  Embeddings and Domain Knowledge", "comments": "SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3080740", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation texts are sometimes not very informative or in some cases inaccurate\nby themselves; they need the appropriate context from the referenced paper to\nreflect its exact contributions. To address this problem, we propose an\nunsupervised model that uses distributed representation of words as well as\ndomain knowledge to extract the appropriate context from the reference paper.\nEvaluation results show the effectiveness of our model by significantly\noutperforming the state-of-the-art. We furthermore demonstrate how an effective\ncontextualization method results in improving citation-based summarization of\nthe scientific articles.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 02:55:56 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "1705.08091", "submitter": "Andros Tjandra", "authors": "Andros Tjandra, Sakriani Sakti, Satoshi Nakamura", "title": "Local Monotonic Attention Mechanism for End-to-End Speech and Language\n  Processing", "comments": "Accepted at IJCNLP 2017 --- (V2: added more experiments on G2P & MT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, encoder-decoder neural networks have shown impressive performance\non many sequence-related tasks. The architecture commonly uses an attentional\nmechanism which allows the model to learn alignments between the source and the\ntarget sequence. Most attentional mechanisms used today is based on a global\nattention property which requires a computation of a weighted summarization of\nthe whole input sequence generated by encoder states. However, it is\ncomputationally expensive and often produces misalignment on the longer input\nsequence. Furthermore, it does not fit with monotonous or left-to-right nature\nin several tasks, such as automatic speech recognition (ASR),\ngrapheme-to-phoneme (G2P), etc. In this paper, we propose a novel attention\nmechanism that has local and monotonic properties. Various ways to control\nthose properties are also explored. Experimental results on ASR, G2P and\nmachine translation between two languages with similar sentence structures,\ndemonstrate that the proposed encoder-decoder model with local monotonic\nattention could achieve significant performance improvements and reduce the\ncomputational complexity in comparison with the one that used the standard\nglobal attention architecture.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 06:32:36 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 15:34:00 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Tjandra", "Andros", ""], ["Sakti", "Sakriani", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1705.08094", "submitter": "Guangdong Bai", "authors": "Zhengkui Wang, Guangdong Bai, Soumyadeb Chowdhury, Quanqing Xu, Zhi\n  Lin Seow", "title": "TwiInsight: Discovering Topics and Sentiments from Social Media Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms contain a great wealth of information which provides\nopportunities for us to explore hidden patterns or unknown correlations, and\nunderstand people's satisfaction with what they are discussing. As one\nshowcase, in this paper, we present a system, TwiInsight which explores the\ninsight of Twitter data. Different from other Twitter analysis systems,\nTwiInsight automatically extracts the popular topics under different categories\n(e.g., healthcare, food, technology, sports and transport) discussed in Twitter\nvia topic modeling and also identifies the correlated topics across different\ncategories. Additionally, it also discovers the people's opinions on the tweets\nand topics via the sentiment analysis. The system also employs an intuitive and\ninformative visualization to show the uncovered insight. Furthermore, we also\ndevelop and compare six most popular algorithms - three for sentiment analysis\nand three for topic modeling.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 06:49:12 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Wang", "Zhengkui", ""], ["Bai", "Guangdong", ""], ["Chowdhury", "Soumyadeb", ""], ["Xu", "Quanqing", ""], ["Seow", "Zhi Lin", ""]]}, {"id": "1705.08142", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, Anders\n  S{\\o}gaard", "title": "Latent Multi-task Architecture Learning", "comments": "To appear in Proceedings of AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) allows deep neural networks to learn from related\ntasks by sharing parameters with other networks. In practice, however, MTL\ninvolves searching an enormous space of possible parameter sharing\narchitectures to find (a) the layers or subspaces that benefit from sharing,\n(b) the appropriate amount of sharing, and (c) the appropriate relative weights\nof the different task losses. Recent work has addressed each of the above\nproblems in isolation. In this work we present an approach that learns a latent\nmulti-task architecture that jointly addresses (a)--(c). We present experiments\non synthetic data and data from OntoNotes 5.0, including four different tasks\nand seven different domains. Our extension consistently outperforms previous\napproaches to learning latent architectures for multi-task problems and\nachieves up to 15% average error reductions over common approaches to MTL.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 08:58:09 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 14:05:37 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 10:30:52 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ruder", "Sebastian", ""], ["Bingel", "Joachim", ""], ["Augenstein", "Isabelle", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1705.08321", "submitter": "Roman Gurinovich", "authors": "Roman Gurinovich, Alexander Pashuk, Yuriy Petrovskiy, Alex\n  Dmitrievskij, Oleg Kuryan, Alexei Scerbacov, Antonia Tiggre, Elena Moroz,\n  Yuri Nikolsky", "title": "Increasing Papers' Discoverability with Precise Semantic Labeling: the\n  sci.AI Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of published findings in biomedicine increases continually. At the\nsame time, specifics of the domain's terminology complicates the task of\nrelevant publications retrieval. In the current research, we investigate\ninfluence of terms' variability and ambiguity on a paper's likelihood of being\nretrieved. We obtained statistics that demonstrate significance of the issue\nand its challenges, followed by presenting the sci.AI platform, which allows\nprecise terms labeling as a resolution.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 17:04:42 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gurinovich", "Roman", ""], ["Pashuk", "Alexander", ""], ["Petrovskiy", "Yuriy", ""], ["Dmitrievskij", "Alex", ""], ["Kuryan", "Oleg", ""], ["Scerbacov", "Alexei", ""], ["Tiggre", "Antonia", ""], ["Moroz", "Elena", ""], ["Nikolsky", "Yuri", ""]]}, {"id": "1705.08386", "submitter": "Karol Kurach", "authors": "Karol Kurach, Sylvain Gelly, Michal Jastrzebski, Philip Haeusser,\n  Olivier Teytaud, Damien Vincent, Olivier Bousquet", "title": "Better Text Understanding Through Image-To-Text Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic text embeddings are successfully used in a variety of tasks. However,\nthey are often learnt by capturing the co-occurrence structure from pure text\ncorpora, resulting in limitations of their ability to generalize. In this\npaper, we explore models that incorporate visual information into the text\nrepresentation. Based on comprehensive ablation studies, we propose a\nconceptually simple, yet well performing architecture. It outperforms previous\nmultimodal approaches on a set of well established benchmarks. We also improve\nthe state-of-the-art results for image-related text datasets, using orders of\nmagnitude less data.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:06:32 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 08:08:20 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Kurach", "Karol", ""], ["Gelly", "Sylvain", ""], ["Jastrzebski", "Michal", ""], ["Haeusser", "Philip", ""], ["Teytaud", "Olivier", ""], ["Vincent", "Damien", ""], ["Bousquet", "Olivier", ""]]}, {"id": "1705.08432", "submitter": "Hamid Palangi", "authors": "Hamid Palangi, Paul Smolensky, Xiaodong He, Li Deng", "title": "Question-Answering with Grammatically-Interpretable Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an architecture, the Tensor Product Recurrent Network (TPRN). In\nour application of TPRN, internal representations learned by end-to-end\noptimization in a deep neural network performing a textual question-answering\n(QA) task can be interpreted using basic concepts from linguistic theory. No\nperformance penalty need be paid for this increased interpretability: the\nproposed model performs comparably to a state-of-the-art system on the SQuAD QA\ntask. The internal representation which is interpreted is a Tensor Product\nRepresentation: for each input word, the model selects a symbol to encode the\nword, and a role in which to place the symbol, and binds the two together. The\nselection is via soft attention. The overall interpretation is built from\ninterpretations of the symbols, as recruited by the trained model, and\ninterpretations of the roles as used by the model. We find support for our\ninitial hypothesis that symbols can be interpreted as lexical-semantic word\nmeanings, while roles can be interpreted as approximations of grammatical roles\n(or categories) such as subject, wh-word, determiner, etc. Fine-grained\nanalysis reveals specific correspondences between the learned roles and parts\nof speech as assigned by a standard tagger (Toutanova et al. 2003), and finds\nseveral discrepancies in the model's favor. In this sense, the model learns\nsignificant aspects of grammar, after having been exposed solely to\nlinguistically unannotated text, questions, and answers: no prior linguistic\nknowledge is given to the model. What is given is the means to build\nrepresentations using symbols and roles, with an inductive bias favoring use of\nthese in an approximately discrete manner.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:40:14 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 23:49:18 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Palangi", "Hamid", ""], ["Smolensky", "Paul", ""], ["He", "Xiaodong", ""], ["Deng", "Li", ""]]}, {"id": "1705.08488", "submitter": "Denis Newman-Griffis", "authors": "Denis Newman-Griffis and Eric Fosler-Lussier", "title": "Second-Order Word Embeddings from Nearest Neighbor Topological Features", "comments": "Submitted to NIPS 2017. (8 pages + 4 reference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce second-order vector representations of words, induced from\nnearest neighborhood topological features in pre-trained contextual word\nembeddings. We then analyze the effects of using second-order embeddings as\ninput features in two deep natural language processing models, for named entity\nrecognition and recognizing textual entailment, as well as a linear model for\nparaphrase recognition. Surprisingly, we find that nearest neighbor information\nalone is sufficient to capture most of the performance benefits derived from\nusing pre-trained word embeddings. Furthermore, second-order embeddings are\nable to handle highly heterogeneous data better than first-order\nrepresentations, though at the cost of some specificity. Additionally,\naugmenting contextual embeddings with second-order information further improves\nmodel performance in some cases. Due to variance in the random initializations\nof word embeddings, utilizing nearest neighbor features from multiple\nfirst-order embedding samples can also contribute to downstream performance\ngains. Finally, we identify intriguing characteristics of second-order\nembedding spaces for further research, including much higher density and\ndifferent semantic interpretations of cosine similarity.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:12:05 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Newman-Griffis", "Denis", ""], ["Fosler-Lussier", "Eric", ""]]}, {"id": "1705.08557", "submitter": "Ankit Vani", "authors": "Ankit Vani, Yacine Jernite, David Sontag", "title": "Grounded Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the Grounded Recurrent Neural Network (GRNN), a\nrecurrent neural network architecture for multi-label prediction which\nexplicitly ties labels to specific dimensions of the recurrent hidden state (we\ncall this process \"grounding\"). The approach is particularly well-suited for\nextracting large numbers of concepts from text. We apply the new model to\naddress an important problem in healthcare of understanding what medical\nconcepts are discussed in clinical text. Using a publicly available dataset\nderived from Intensive Care Units, we learn to label a patient's diagnoses and\nprocedures from their discharge summary. Our evaluation shows a clear advantage\nto using our proposed architecture over a variety of strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 23:17:49 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Vani", "Ankit", ""], ["Jernite", "Yacine", ""], ["Sontag", "David", ""]]}, {"id": "1705.08828", "submitter": "Laurent Besacier", "authors": "Jeremy Ferrero, Laurent Besacier, Didier Schwab and Frederic Agnes", "title": "Deep Investigation of Cross-Language Plagiarism Detection Methods", "comments": "Accepted to BUCC (10th Workshop on Building and Using Comparable\n  Corpora) colocated with ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is a deep investigation of cross-language plagiarism detection\nmethods on a new recently introduced open dataset, which contains parallel and\ncomparable collections of documents with multiple characteristics (different\ngenres, languages and sizes of texts). We investigate cross-language plagiarism\ndetection methods for 6 language pairs on 2 granularities of text units in\norder to draw robust conclusions on the best methods while deeply analyzing\ncorrelations across document styles and languages.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:50:47 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Ferrero", "Jeremy", ""], ["Besacier", "Laurent", ""], ["Schwab", "Didier", ""], ["Agnes", "Frederic", ""]]}, {"id": "1705.08843", "submitter": "Fabio Massimo Zanzotto", "authors": "Fabio Massimo Zanzotto and Giordano Cristini and Giorgio Satta", "title": "Parsing with CYK over Distributed Representations", "comments": "The algorithm has been greatly improved. Experiments have been\n  redesigned", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic parsing is a key task in natural language processing. This task has\nbeen dominated by symbolic, grammar-based parsers. Neural networks, with their\ndistributed representations, are challenging these methods. In this article we\nshow that existing symbolic parsing algorithms can cross the border and be\nentirely formulated over distributed representations. To this end we introduce\na version of the traditional Cocke-Younger-Kasami (CYK) algorithm, called\nD-CYK, which is entirely defined over distributed representations. Our D-CYK\nuses matrix multiplication on real number matrices of size independent of the\nlength of the input string. These operations are compatible with traditional\nneural networks. Experiments show that our D-CYK approximates the original CYK\nalgorithm. By showing that CYK can be entirely performed on distributed\nrepresentations, we open the way to the definition of recurrent layers of\nCYK-informed neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:22:13 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 08:30:42 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Zanzotto", "Fabio Massimo", ""], ["Cristini", "Giordano", ""], ["Satta", "Giorgio", ""]]}, {"id": "1705.08942", "submitter": "Necva B\\\"ol\\\"uc\\\"u", "authors": "Necva B\\\"ol\\\"uc\\\"u and Burcu Can", "title": "Joint PoS Tagging and Stemming for Agglutinative Languages", "comments": "12 pages with 3 figures, accepted and presented at the CICLING 2017 -\n  18th International Conference on Intelligent Text Processing and\n  Computational Linguistics", "journal-ref": "CICLING 2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of word forms in agglutinative languages is theoretically infinite\nand this variety in word forms introduces sparsity in many natural language\nprocessing tasks. Part-of-speech tagging (PoS tagging) is one of these tasks\nthat often suffers from sparsity. In this paper, we present an unsupervised\nBayesian model using Hidden Markov Models (HMMs) for joint PoS tagging and\nstemming for agglutinative languages. We use stemming to reduce sparsity in PoS\ntagging. Two tasks are jointly performed to provide a mutual benefit in both\ntasks. Our results show that joint POS tagging and stemming improves PoS\ntagging scores. We present results for Turkish and Finnish as agglutinative\nlanguages and English as a morphologically poor language.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:44:35 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["B\u00f6l\u00fcc\u00fc", "Necva", ""], ["Can", "Burcu", ""]]}, {"id": "1705.08947", "submitter": "Andrew Gibiansky", "authors": "Sercan Arik, Gregory Diamos, Andrew Gibiansky, John Miller, Kainan\n  Peng, Wei Ping, Jonathan Raiman, Yanqi Zhou", "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech", "comments": "Accepted in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a technique for augmenting neural text-to-speech (TTS) with\nlowdimensional trainable speaker embeddings to generate different voices from a\nsingle model. As a starting point, we show improvements over the two\nstate-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and\nTacotron. We introduce Deep Voice 2, which is based on a similar pipeline with\nDeep Voice 1, but constructed with higher performance building blocks and\ndemonstrates a significant audio quality improvement over Deep Voice 1. We\nimprove Tacotron by introducing a post-processing neural vocoder, and\ndemonstrate a significant audio quality improvement. We then demonstrate our\ntechnique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron\non two multi-speaker TTS datasets. We show that a single neural TTS system can\nlearn hundreds of unique voices from less than half an hour of data per\nspeaker, while achieving high audio quality synthesis and preserving the\nspeaker identities almost perfectly.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:53:13 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 21:43:18 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Arik", "Sercan", ""], ["Diamos", "Gregory", ""], ["Gibiansky", "Andrew", ""], ["Miller", "John", ""], ["Peng", "Kainan", ""], ["Ping", "Wei", ""], ["Raiman", "Jonathan", ""], ["Zhou", "Yanqi", ""]]}, {"id": "1705.08992", "submitter": "Leonid Peshkin", "authors": "Nicholas Harvey, Vahab Mirrokni, David Karger, Virginia Savova, Leonid\n  Peshkin", "title": "Matroids Hitting Sets and Unsupervised Dependency Grammar Induction", "comments": "11 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formulates a novel problem on graphs: find the minimal subset of\nedges in a fully connected graph, such that the resulting graph contains all\nspanning trees for a set of specifed sub-graphs. This formulation is motivated\nby an un-supervised grammar induction problem from computational linguistics.\nWe present a reduction to some known problems and algorithms from graph theory,\nprovide computational complexity results, and describe an approximation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 22:53:56 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 20:24:11 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Harvey", "Nicholas", ""], ["Mirrokni", "Vahab", ""], ["Karger", "David", ""], ["Savova", "Virginia", ""], ["Peshkin", "Leonid", ""]]}, {"id": "1705.09037", "submitter": "Tao Lei", "authors": "Tao Lei, Wengong Jin, Regina Barzilay and Tommi Jaakkola", "title": "Deriving Neural Architectures from Sequence and Graph Kernels", "comments": "extended version of ICML 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of neural architectures for structured objects is typically guided\nby experimental insights rather than a formal process. In this work, we appeal\nto kernels over combinatorial structures, such as sequences and graphs, to\nderive appropriate neural operations. We introduce a class of deep recurrent\nneural operations and formally characterize their associated kernel spaces. Our\nrecurrent modules compare the input to virtual reference objects (cf. filters\nin CNN) via the kernels. Similar to traditional neural operations, these\nreference objects are parameterized and directly optimized in end-to-end\ntraining. We empirically evaluate the proposed class of neural architectures on\nstandard applications such as language modeling and molecular graph regression,\nachieving state-of-the-art results across these applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 03:58:10 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 14:34:24 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 13:56:23 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Lei", "Tao", ""], ["Jin", "Wengong", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1705.09054", "submitter": "Zhipeng Xie", "authors": "Zhipeng Xie and Junfeng Hu", "title": "Max-Cosine Matching Based Neural Models for Recognizing Textual\n  Entailment", "comments": null, "journal-ref": "DASFAA (1) 2017: 295-308", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing textual entailment is a fundamental task in a variety of text\nmining or natural language processing applications. This paper proposes a\nsimple neural model for RTE problem. It first matches each word in the\nhypothesis with its most-similar word in the premise, producing an augmented\nrepresentation of the hypothesis conditioned on the premise as a sequence of\nword pairs. The LSTM model is then used to model this augmented sequence, and\nthe final output from the LSTM is fed into a softmax layer to make the\nprediction. Besides the base model, in order to enhance its performance, we\nalso proposed three techniques: the integration of multiple word-embedding\nlibrary, bi-way integration, and ensemble based on model averaging.\nExperimental results on the SNLI dataset have shown that the three techniques\nare effective in boosting the predicative accuracy and that our method\noutperforms several state-of-the-state ones.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 05:45:42 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Xie", "Zhipeng", ""], ["Hu", "Junfeng", ""]]}, {"id": "1705.09189", "submitter": "Jean Maillard", "authors": "Jean Maillard, Stephen Clark, Dani Yogatama", "title": "Jointly Learning Sentence Embeddings and Syntax with Unsupervised\n  Tree-LSTMs", "comments": null, "journal-ref": "Natural Language Engineering 25, no. 4 (2019): 433-49", "doi": "10.1017/S1351324919000184", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural network that represents sentences by composing their\nwords according to induced binary parse trees. We use Tree-LSTM as our\ncomposition function, applied along a tree structure found by a fully\ndifferentiable natural language chart parser. Our model simultaneously\noptimises both the composition function and the parser, thus eliminating the\nneed for externally-provided parse trees which are normally required for\nTree-LSTM. It can therefore be seen as a tree-based RNN that is unsupervised\nwith respect to the parse trees. As it is fully differentiable, our model is\neasily trained with an off-the-shelf gradient descent method and\nbackpropagation. We demonstrate that it achieves better performance compared to\nvarious supervised Tree-LSTM architectures on a textual entailment task and a\nreverse dictionary task.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 14:09:48 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Maillard", "Jean", ""], ["Clark", "Stephen", ""], ["Yogatama", "Dani", ""]]}, {"id": "1705.09207", "submitter": "Yang Liu", "authors": "Yang Liu and Mirella Lapata", "title": "Learning Structured Text Representations", "comments": "change to one-based indexing, published in Transactions of the\n  Association for Computational Linguistics (TACL),\n  https://transacl.org/ojs/index.php/tacl/article/view/1185/280", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on learning structure-aware document representations\nfrom data without recourse to a discourse parser or additional annotations.\nDrawing inspiration from recent efforts to empower neural networks with a\nstructural bias, we propose a model that can encode a document while\nautomatically inducing rich structural dependencies. Specifically, we embed a\ndifferentiable non-projective parsing algorithm into a neural model and use\nattention mechanisms to incorporate the structural biases. Experimental\nevaluation across different tasks and datasets shows that the proposed model\nachieves state-of-the-art results on document modeling tasks while inducing\nintermediate structures which are both interpretable and meaningful.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 14:54:07 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 23:14:58 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 20:42:25 GMT"}, {"version": "v4", "created": "Sat, 3 Feb 2018 13:31:40 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Liu", "Yang", ""], ["Lapata", "Mirella", ""]]}, {"id": "1705.09222", "submitter": "Ashwini Jaya Kumar", "authors": "Ashwini Jaya Kumar, S\\\"oren Auer, Christoph Schmidt, Joachim k\\\"ohler", "title": "Towards a Knowledge Graph based Speech Interface", "comments": "Under Review in International Workshop on Grounding Language\n  Understanding, Satellite of Interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications which use human speech as an input require a speech interface\nwith high recognition accuracy. The words or phrases in the recognised text are\nannotated with a machine-understandable meaning and linked to knowledge graphs\nfor further processing by the target application. These semantic annotations of\nrecognised words can be represented as a subject-predicate-object triples which\ncollectively form a graph often referred to as a knowledge graph. This type of\nknowledge representation facilitates to use speech interfaces with any spoken\ninput application, since the information is represented in logical, semantic\nform, retrieving and storing can be followed using any web standard query\nlanguages. In this work, we develop a methodology for linking speech input to\nknowledge graphs and study the impact of recognition errors in the overall\nprocess. We show that for a corpus with lower WER, the annotation and linking\nof entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight,\na tool to interlink text documents with the linked open data is used to link\nthe speech recognition output to the DBpedia knowledge graph. Such a\nknowledge-based speech recognition interface is useful for applications such as\nquestion answering or spoken dialog systems.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:54:32 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Kumar", "Ashwini Jaya", ""], ["Auer", "S\u00f6ren", ""], ["Schmidt", "Christoph", ""], ["k\u00f6hler", "Joachim", ""]]}, {"id": "1705.09296", "submitter": "Dallas Card", "authors": "Dallas Card and Chenhao Tan and Noah A. Smith", "title": "Neural Models for Documents with Metadata", "comments": "13 pages, 3 figures, 6 tables; updating to version published at ACL\n  2018", "journal-ref": "Dallas Card, Chenhao Tan, and Noah A. Smith. (2018). Neural Models\n  for Documents with Metadata. In Proceedings of the 56th Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers)", "doi": "10.18653/v1/P18-1189", "report-no": null, "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most real-world document collections involve various types of metadata, such\nas author, source, and date, and yet the most commonly-used approaches to\nmodeling text corpora ignore this information. While specialized models have\nbeen developed for particular applications, few are widely used in practice, as\ncustomization typically requires derivation of a custom inference algorithm. In\nthis paper, we build on recent advances in variational inference methods and\npropose a general neural framework, based on topic models, to enable flexible\nincorporation of metadata and allow for rapid exploration of alternative\nmodels. Our approach achieves strong performance, with a manageable tradeoff\nbetween perplexity, coherence, and sparsity. Finally, we demonstrate the\npotential of our framework through an exploration of a corpus of articles about\nUS immigration.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 18:00:03 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 20:26:37 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Card", "Dallas", ""], ["Tan", "Chenhao", ""], ["Smith", "Noah A.", ""]]}, {"id": "1705.09515", "submitter": "Yannick Esteve", "authors": "Edwin Simonnet (LIUM), Sahar Ghannay (LIUM), Nathalie Camelin (LIUM),\n  Yannick Est\\`eve (LIUM), Renato De Mori (LIA)", "title": "ASR error management for improving spoken language understanding", "comments": "Interspeech 2017, Aug 2017, Stockholm, Sweden. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of automatic speech recognition (ASR) error\ndetection and their use for improving spoken language understanding (SLU)\nsystems. In this study, the SLU task consists in automatically extracting, from\nASR transcriptions , semantic concepts and concept/values pairs in a e.g\ntouristic information system. An approach is proposed for enriching the set of\nsemantic labels with error specific labels and by using a recently proposed\nneural approach based on word embeddings to compute well calibrated ASR\nconfidence measures. Experimental results are reported showing that it is\npossible to decrease significantly the Concept/Value Error Rate with a state of\nthe art system, outperforming previously published results performance on the\nsame experimental data. It also shown that combining an SLU approach based on\nconditional random fields with a neural encoder/decoder attention based\narchitecture , it is possible to effectively identifying confidence islands and\nuncertain semantic output segments useful for deciding appropriate error\nhandling actions by the dialogue manager strategy .\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 10:34:24 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Simonnet", "Edwin", "", "LIUM"], ["Ghannay", "Sahar", "", "LIUM"], ["Camelin", "Nathalie", "", "LIUM"], ["Est\u00e8ve", "Yannick", "", "LIUM"], ["De Mori", "Renato", "", "LIA"]]}, {"id": "1705.09516", "submitter": "Sunil Sahu", "authors": "Patchigolla V S S Rahul, Sunil Kumar Sahu, Ashish Anand", "title": "Biomedical Event Trigger Identification Using Bidirectional Recurrent\n  Neural Network Based Models", "comments": "The work has been accepted in BioNLP at ACL-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical events describe complex interactions between various biomedical\nentities. Event trigger is a word or a phrase which typically signifies the\noccurrence of an event. Event trigger identification is an important first step\nin all event extraction methods. However many of the current approaches either\nrely on complex hand-crafted features or consider features only within a\nwindow. In this paper we propose a method that takes the advantage of recurrent\nneural network (RNN) to extract higher level features present across the\nsentence. Thus hidden state representation of RNN along with word and entity\ntype embedding as features avoid relying on the complex hand-crafted features\ngenerated using various NLP toolkits. Our experiments have shown to achieve\nstate-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have\nalso performed category-wise analysis of the result and discussed the\nimportance of various features in trigger identification task.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 10:36:12 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Rahul", "Patchigolla V S S", ""], ["Sahu", "Sunil Kumar", ""], ["Anand", "Ashish", ""]]}, {"id": "1705.09585", "submitter": "Rohan Kshirsagar", "authors": "Rohan Kshirsagar, Robert Morris, Sam Bowman", "title": "Detecting and Explaining Crisis", "comments": "Accepted at CLPsych, ACL workshop. 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals on social media may reveal themselves to be in various states of\ncrisis (e.g. suicide, self-harm, abuse, or eating disorders). Detecting crisis\nfrom social media text automatically and accurately can have profound\nconsequences. However, detecting a general state of crisis without explaining\nwhy has limited applications. An explanation in this context is a coherent,\nconcise subset of the text that rationalizes the crisis detection. We explore\nseveral methods to detect and explain crisis using a combination of neural and\nnon-neural techniques. We evaluate these techniques on a unique data set\nobtained from Koko, an anonymous emotional support network available through\nvarious messaging applications. We annotate a small subset of the samples\nlabeled with crisis with corresponding explanations. Our best technique\nsignificantly outperforms the baseline for detection and explanation.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 13:44:54 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Kshirsagar", "Rohan", ""], ["Morris", "Robert", ""], ["Bowman", "Sam", ""]]}, {"id": "1705.09655", "submitter": "Tianxiao Shen", "authors": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola", "title": "Style Transfer from Non-Parallel Text by Cross-Alignment", "comments": "NIPS 2017 camera-ready. Added human evaluation on sentiment transfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on style transfer on the basis of non-parallel text. This\nis an instance of a broad family of problems including machine translation,\ndecipherment, and sentiment modification. The key challenge is to separate the\ncontent from other aspects such as style. We assume a shared latent content\ndistribution across different text corpora, and propose a method that leverages\nrefined alignment of latent representations to perform style transfer. The\ntransferred sentences from one style should match example sentences from the\nother style as a population. We demonstrate the effectiveness of this\ncross-alignment method on three tasks: sentiment modification, decipherment of\nword substitution ciphers, and recovery of word order.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 17:40:12 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 15:07:03 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Shen", "Tianxiao", ""], ["Lei", "Tao", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1705.09656", "submitter": "Claudia Orellana-Rodriguez", "authors": "Terrence Szymanski, Claudia Orellana-Rodriguez and Mark T. Keane", "title": "Helping News Editors Write Better Headlines: A Recommender to Improve\n  the Keyword Contents & Shareability of News Headlines", "comments": null, "journal-ref": "Natural Language Processing meets Journalism. IJCAI-16 workshop.\n  Pages 30-34. 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a software tool that employs state-of-the-art natural language\nprocessing (NLP) and machine learning techniques to help newspaper editors\ncompose effective headlines for online publication. The system identifies the\nmost salient keywords in a news article and ranks them based on both their\noverall popularity and their direct relevance to the article. The system also\nuses a supervised regression model to identify headlines that are likely to be\nwidely shared on social media. The user interface is designed to simplify and\nspeed the editor's decision process on the composition of the headline. As\nsuch, the tool provides an efficient way to combine the benefits of automated\npredictors of engagement and search-engine optimization (SEO) with human\njudgments of overall headline quality.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 17:40:58 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Szymanski", "Terrence", ""], ["Orellana-Rodriguez", "Claudia", ""], ["Keane", "Mark T.", ""]]}, {"id": "1705.09665", "submitter": "Cristian Danescu-Niculescu-Mizil", "authors": "Justine Zhang and William L. Hamilton and Cristian\n  Danescu-Niculescu-Mizil and Dan Jurafsky and Jure Leskovec", "title": "Community Identity and User Engagement in a Multi-Community Landscape", "comments": "10 page, 3 figures, To appear in the Proceedings of the 11th\n  International Conference On Web And Social Media, ICWSM 2017; this version\n  has subtle differences with the proceedings version, including an\n  introductory quote", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A community's identity defines and shapes its internal dynamics. Our current\nunderstanding of this interplay is mostly limited to glimpses gathered from\nisolated studies of individual communities. In this work we provide a\nsystematic exploration of the nature of this relation across a wide variety of\nonline communities. To this end we introduce a quantitative, language-based\ntypology reflecting two key aspects of a community's identity: how distinctive,\nand how temporally dynamic it is. By mapping almost 300 Reddit communities into\nthe landscape induced by this typology, we reveal regularities in how patterns\nof user engagement vary with the characteristics of a community.\n  Our results suggest that the way new and existing users engage with a\ncommunity depends strongly and systematically on the nature of the collective\nidentity it fosters, in ways that are highly consequential to community\nmaintainers. For example, communities with distinctive and highly dynamic\nidentities are more likely to retain their users. However, such niche\ncommunities also exhibit much larger acculturation gaps between existing users\nand newcomers, which potentially hinder the integration of the latter.\n  More generally, our methodology reveals differences in how various social\nphenomena manifest across communities, and shows that structuring the\nmulti-community landscape can lead to a better understanding of the systematic\nnature of this diversity.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 18:00:02 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zhang", "Justine", ""], ["Hamilton", "William L.", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Jurafsky", "Dan", ""], ["Leskovec", "Jure", ""]]}, {"id": "1705.09724", "submitter": "Iroro Orife", "authors": "Shane Walker, Morten Pedersen, Iroro Orife and Jason Flaks", "title": "Semi-Supervised Model Training for Unbounded Conversational Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For conversational large-vocabulary continuous speech recognition (LVCSR)\ntasks, up to about two thousand hours of audio is commonly used to train state\nof the art models. Collection of labeled conversational audio however, is\nprohibitively expensive, laborious and error-prone. Furthermore, academic\ncorpora like Fisher English (2004) or Switchboard (1992) are inadequate to\ntrain models with sufficient accuracy in the unbounded space of conversational\nspeech. These corpora are also timeworn due to dated acoustic telephony\nfeatures and the rapid advancement of colloquial vocabulary and idiomatic\nspeech over the last decades. Utilizing the colossal scale of our unlabeled\ntelephony dataset, we propose a technique to construct a modern, high quality\nconversational speech training corpus on the order of hundreds of millions of\nutterances (or tens of thousands of hours) for both acoustic and language model\ntraining. We describe the data collection, selection and training, evaluating\nthe results of our updated speech recognition system on a test corpus of 7K\nmanually transcribed utterances. We show relative word error rate (WER)\nreductions of {35%, 19%} on {agent, caller} utterances over our seed model and\n5% absolute WER improvements over IBM Watson STT on this conversational speech\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 21:10:15 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Walker", "Shane", ""], ["Pedersen", "Morten", ""], ["Orife", "Iroro", ""], ["Flaks", "Jason", ""]]}, {"id": "1705.09731", "submitter": "Massimo Stella", "authors": "Massimo Stella, Nicole M. Beckage, Markus Brede and Manlio De Domenico", "title": "Multiplex model of mental lexicon reveals explosive learning in humans", "comments": "13 pages, 4 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word similarities affect language acquisition and use in a multi-relational\nway barely accounted for in the literature. We propose a multiplex network\nrepresentation of this mental lexicon of word similarities as a natural\nframework for investigating large-scale cognitive patterns. Our representation\naccounts for semantic, taxonomic, and phonological interactions and it\nidentifies a cluster of words which are used with greater frequency, are\nidentified, memorised, and learned more easily, and have more meanings than\nexpected at random. This cluster emerges around age 7 through an explosive\ntransition not reproduced by null models. We relate this explosive emergence to\npolysemy -- redundancy in word meanings. Results indicate that the word cluster\nacts as a core for the lexicon, increasing both lexical navigability and\nrobustness to linguistic degradation. Our findings provide quantitative\nconfirmation of existing conjectures about core structure in the mental lexicon\nand the importance of integrating multi-relational word-word interactions in\npsycholinguistic frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 22:18:39 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 00:57:24 GMT"}, {"version": "v3", "created": "Mon, 22 Jan 2018 09:14:16 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Stella", "Massimo", ""], ["Beckage", "Nicole M.", ""], ["Brede", "Markus", ""], ["De Domenico", "Manlio", ""]]}, {"id": "1705.09755", "submitter": "Andrew Landgraf", "authors": "Andrew J. Landgraf, Jeremy Bellay", "title": "word2vec Skip-Gram with Negative Sampling is a Weighted Logistic PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the skip-gram formulation of word2vec trained with negative\nsampling is equivalent to a weighted logistic PCA. This connection allows us to\nbetter understand the objective, compare it to other word embedding methods,\nand extend it to higher dimensional models.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 02:23:18 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Landgraf", "Andrew J.", ""], ["Bellay", "Jeremy", ""]]}, {"id": "1705.09837", "submitter": "Carlos G\\'omez-Rodr\\'iguez", "authors": "Carlos G\\'omez-Rodr\\'iguez", "title": "On the relation between dependency distance, crossing dependencies, and\n  parsing. Comment on \"Dependency distance: a new perspective on syntactic\n  patterns in natural languages\" by Haitao Liu et al", "comments": "This is a comment on the article \"Dependency distance: a new\n  perspective on syntactic patterns in natural languages\", by Haitao Liu et al.\n  Accepted for publication in Physics of Life Reviews. This version is\n  identical to the previous one, but with the link to the accepted article and\n  license as required by Elsevier", "journal-ref": "Physics of Life Reviews, 21:200-203, 2017", "doi": "10.1016/j.plrev.2017.05.007", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liu et al. (2017) provide a comprehensive account of research on dependency\ndistance in human languages. While the article is a very rich and useful report\non this complex subject, here I will expand on a few specific issues where\nresearch in computational linguistics (specifically natural language\nprocessing) can inform DDM research, and vice versa. These aspects have not\nbeen explored much in the article by Liu et al. or elsewhere, probably due to\nthe little overlap between both research communities, but they may provide\ninteresting insights for improving our understanding of the evolution of human\nlanguages, the mechanisms by which the brain processes and understands\nlanguage, and the construction of effective computer systems to achieve this\ngoal.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 16:19:37 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 11:12:41 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "1705.09899", "submitter": "Zeerak Butt", "authors": "Zeerak Waseem, Thomas Davidson, Dana Warmsley, Ingmar Weber", "title": "Understanding Abuse: A Typology of Abusive Language Detection Subtasks", "comments": "To appear in the proceedings of the 1st Workshop on Abusive Language\n  Online. Please cite that version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the body of research on abusive language detection and analysis grows,\nthere is a need for critical consideration of the relationships between\ndifferent subtasks that have been grouped under this label. Based on work on\nhate speech, cyberbullying, and online abuse we propose a typology that\ncaptures central similarities and differences between subtasks and we discuss\nits implications for data annotation and feature construction. We emphasize the\npractical actions that can be taken by researchers to best approach their\nabusive language detection subtask of interest.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 06:59:07 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 11:07:51 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Waseem", "Zeerak", ""], ["Davidson", "Thomas", ""], ["Warmsley", "Dana", ""], ["Weber", "Ingmar", ""]]}, {"id": "1705.09906", "submitter": "Haichao Zhang", "authors": "Haichao Zhang, Haonan Yu, and Wei Xu", "title": "Listen, Interact and Talk: Learning to Speak via Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the long-term goals of artificial intelligence is to build an agent\nthat can communicate intelligently with human in natural language. Most\nexisting work on natural language learning relies heavily on training over a\npre-collected dataset with annotated labels, leading to an agent that\nessentially captures the statistics of the fixed external training data. As the\ntraining data is essentially a static snapshot representation of the knowledge\nfrom the annotator, the agent trained this way is limited in adaptiveness and\ngeneralization of its behavior. Moreover, this is very different from the\nlanguage learning process of humans, where language is acquired during\ncommunication by taking speaking action and learning from the consequences of\nspeaking action in an interactive manner. This paper presents an interactive\nsetting for grounded natural language learning, where an agent learns natural\nlanguage by interacting with a teacher and learning from feedback, thus\nlearning and improving language skills while taking part in the conversation.\nTo achieve this goal, we propose a model which incorporates both imitation and\nreinforcement by leveraging jointly sentence and reward feedbacks from the\nteacher. Experiments are conducted to validate the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 07:48:14 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zhang", "Haichao", ""], ["Yu", "Haonan", ""], ["Xu", "Wei", ""]]}, {"id": "1705.09932", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "The placement of the head that maximizes predictability. An information\n  theoretic approach", "comments": "in press in Glottometrics", "journal-ref": "Glottometrics 39, 38-71", "doi": null, "report-no": null, "categories": "cs.CL nlin.AO physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimization of the length of syntactic dependencies is a\nwell-established principle of word order and the basis of a mathematical theory\nof word order. Here we complete that theory from the perspective of information\ntheory, adding a competing word order principle: the maximization of\npredictability of a target element. These two principles are in conflict: to\nmaximize the predictability of the head, the head should appear last, which\nmaximizes the costs with respect to dependency length minimization. The\nimplications of such a broad theoretical framework to understand the\noptimality, diversity and evolution of the six possible orderings of subject,\nobject and verb are reviewed.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 12:19:03 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 05:31:21 GMT"}, {"version": "v3", "created": "Sun, 3 Sep 2017 12:25:35 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1705.09975", "submitter": "Nazli Farajidavar", "authors": "Nazli Farajidavar, Sefki Kolozali and Payam Barnaghi", "title": "A Deep Multi-View Learning Framework for City Event Extraction from\n  Twitter Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cities have been a thriving place for citizens over the centuries due to\ntheir complex infrastructure. The emergence of the Cyber-Physical-Social\nSystems (CPSS) and context-aware technologies boost a growing interest in\nanalysing, extracting and eventually understanding city events which\nsubsequently can be utilised to leverage the citizen observations of their\ncities. In this paper, we investigate the feasibility of using Twitter textual\nstreams for extracting city events. We propose a hierarchical multi-view deep\nlearning approach to contextualise citizen observations of various city systems\nand services. Our goal has been to build a flexible architecture that can learn\nrepresentations useful for tasks, thus avoiding excessive task-specific feature\nengineering. We apply our approach on a real-world dataset consisting of event\nreports and tweets of over four months from San Francisco Bay Area dataset and\nadditional datasets collected from London. The results of our evaluations show\nthat our proposed solution outperforms the existing models and can be used for\nextracting city related events with an averaged accuracy of 81% over all\nclasses. To further evaluate the impact of our Twitter event extraction model,\nwe have used two sources of authorised reports through collecting road traffic\ndisruptions data from Transport for London API, and parsing the Time Out London\nwebsite for sociocultural events. The analysis showed that 49.5% of the Twitter\ntraffic comments are reported approximately five hours prior to the authorities\nofficial records. Moreover, we discovered that amongst the scheduled\nsociocultural event topics; tweets reporting transportation, cultural and\nsocial events are 31.75% more likely to influence the distribution of the\nTwitter comments than sport, weather and crime topics.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 18:22:15 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Farajidavar", "Nazli", ""], ["Kolozali", "Sefki", ""], ["Barnaghi", "Payam", ""]]}, {"id": "1705.09980", "submitter": "Rik van Noord", "authors": "Rik van Noord and Johan Bos", "title": "Neural Semantic Parsing by Character-based Translation: Experiments with\n  Abstract Meaning Representations", "comments": "Camera ready for CLIN 2017 journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the character-level translation method for neural semantic\nparsing on a large corpus of sentences annotated with Abstract Meaning\nRepresentations (AMRs). Using a sequence-to-sequence model, and some trivial\npreprocessing and postprocessing of AMRs, we obtain a baseline accuracy of 53.1\n(F-score on AMR-triples). We examine five different approaches to improve this\nbaseline result: (i) reordering AMR branches to match the word order of the\ninput sentence increases performance to 58.3; (ii) adding part-of-speech tags\n(automatically produced) to the input shows improvement as well (57.2); (iii)\nSo does the introduction of super characters (conflating frequent sequences of\ncharacters to a single character), reaching 57.4; (iv) optimizing the training\nprocess by using pre-training and averaging a set of models increases\nperformance to 58.7; (v) adding silver-standard training data obtained by an\noff-the-shelf parser yields the biggest improvement, resulting in an F-score of\n64.0. Combining all five techniques leads to an F-score of 71.0 on holdout\ndata, which is state-of-the-art in AMR parsing. This is remarkable because of\nthe relative simplicity of the approach.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 19:41:09 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 08:30:33 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["van Noord", "Rik", ""], ["Bos", "Johan", ""]]}, {"id": "1705.09993", "submitter": "John Pavlopoulos", "authors": "John Pavlopoulos and Prodromos Malakasiotis and Ion Androutsopoulos", "title": "Deep Learning for User Comment Moderation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimenting with a new dataset of 1.6M user comments from a Greek news\nportal and existing datasets of English Wikipedia comments, we show that an RNN\noutperforms the previous state of the art in moderation. A deep,\nclassification-specific attention mechanism improves further the overall\nperformance of the RNN. We also compare against a CNN and a word-list baseline,\nconsidering both fully automatic and semi-automatic moderation.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 21:12:56 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 15:25:56 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Pavlopoulos", "John", ""], ["Malakasiotis", "Prodromos", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "1705.09995", "submitter": "Nisansa De Silva", "authors": "Nisansa de Silva, Danaja Maldeniya, Chamilka Wijeratne", "title": "Subject Specific Stream Classification Preprocessing Algorithm for\n  Twitter Data Stream", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-blogging service Twitter is a lucrative source for data mining\napplications on global sentiment. But due to the omnifariousness of the\nsubjects mentioned in each data item; it is inefficient to run a data mining\nalgorithm on the raw data. This paper discusses an algorithm to accurately\nclassify the entire stream in to a given number of mutually exclusive\ncollectively exhaustive streams upon each of which the data mining algorithm\ncan be run separately yielding more relevant results with a high efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 22:16:35 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["de Silva", "Nisansa", ""], ["Maldeniya", "Danaja", ""], ["Wijeratne", "Chamilka", ""]]}, {"id": "1705.10030", "submitter": "Hu Xu", "authors": "Hu Xu, Lei Shu, Philip S. Yu", "title": "Supervised Complementary Entity Recognition with Augmented Key-value\n  Pairs of Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting opinion targets is an important task in sentiment analysis on\nproduct reviews and complementary entities (products) are one important type of\nopinion targets that may work together with the reviewed product. In this\npaper, we address the problem of Complementary Entity Recognition (CER) as a\nsupervised sequence labeling with the capability of expanding domain knowledge\nas key-value pairs from unlabeled reviews, by automatically learning and\nenhancing knowledge-based features. We use Conditional Random Field (CRF) as\nthe base learner and augment CRF with knowledge-based features (called the\nKnowledge-based CRF or KCRF for short). We conduct experiments to show that\nKCRF effectively improves the performance of supervised CER task.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 03:45:25 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Xu", "Hu", ""], ["Shu", "Lei", ""], ["Yu", "Philip S.", ""]]}, {"id": "1705.10112", "submitter": "Vladimir Bochkarev", "authors": "Valery D. Solovyev, Vladimir V. Bochkarev, Anna V. Shevlyakova", "title": "Dynamics of core of language vocabulary", "comments": "This report was presented at the Workshop \"Computational linguistics\n  and language science\", Moscow, Russia on April 25, 2016", "journal-ref": "CEUR Workshop Proceedings 1886, 122-129 (2016)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies of the overall structure of vocabulary and its dynamics became\npossible due to creation of diachronic text corpora, especially Google Books\nNgram. This article discusses the question of core change rate and the degree\nto which the core words cover the texts. Different periods of the last three\ncenturies and six main European languages presented in Google Books Ngram are\ncompared. The main result is high stability of core change rate, which is\nanalogous to stability of the Swadesh list.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 10:51:20 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Solovyev", "Valery D.", ""], ["Bochkarev", "Vladimir V.", ""], ["Shevlyakova", "Anna V.", ""]]}, {"id": "1705.10130", "submitter": "Murtadha AL-Sharuee", "authors": "Murtadha Talib AL-Sharuee, Fei Liu, Mahardhika Pratama", "title": "An Automatic Contextual Analysis and Clustering Classifiers Ensemble\n  approach to Sentiment Analysis", "comments": "This article is submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Products reviews are one of the major resources to determine the public\nsentiment. The existing literature on reviews sentiment analysis mainly\nutilizes supervised paradigm, which needs labeled data to be trained on and\nsuffers from domain-dependency. This article addresses these issues by\ndescribes a completely automatic approach for sentiment analysis based on\nunsupervised ensemble learning. The method consists of two phases. The first\nphase is contextual analysis, which has five processes, namely (1) data\npreparation; (2) spelling correction; (3) intensifier handling; (4) negation\nhandling and (5) contrast handling. The second phase comprises the unsupervised\nlearning approach, which is an ensemble of clustering classifiers using a\nmajority voting mechanism with different weight schemes. The base classifier of\nthe ensemble method is a modified k-means algorithm. The base classifier is\nmodified by extracting initial centroids from the feature set via using\nSentWordNet (SWN). We also introduce new sentiment analysis problems of\nAustralian airlines and home builders which offer potential benchmark problems\nin the sentiment analysis field. Our experiments on datasets from different\ndomains show that contextual analysis and the ensemble phases improve the\nclustering performance in term of accuracy, stability and generalization\nability.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 11:37:58 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["AL-Sharuee", "Murtadha Talib", ""], ["Liu", "Fei", ""], ["Pratama", "Mahardhika", ""]]}, {"id": "1705.10209", "submitter": "Micha{\\l} Zapotoczny", "authors": "Micha{\\l} Zapotoczny, Pawe{\\l} Rychlikowski, and Jan Chorowski", "title": "On Multilingual Training of Neural Dependency Parsers", "comments": "preprint accepted into the TSD2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a recently proposed neural dependency parser can be improved by\njoint training on multiple languages from the same family. The parser is\nimplemented as a deep neural network whose only input is orthographic\nrepresentations of words. In order to successfully parse, the network has to\ndiscover how linguistically relevant concepts can be inferred from word\nspellings. We analyze the representations of characters and words that are\nlearned by the network to establish which properties of languages were\naccounted for. In particular we show that the parser has approximately learned\nto associate Latin characters with their Cyrillic counterparts and that it can\ngroup Polish and Russian words that have a similar grammatical function.\nFinally, we evaluate the parser on selected languages from the Universal\nDependencies dataset and show that it is competitive with other recently\nproposed state-of-the art methods, while having a simple structure.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:24:08 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zapotoczny", "Micha\u0142", ""], ["Rychlikowski", "Pawe\u0142", ""], ["Chorowski", "Jan", ""]]}, {"id": "1705.10229", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, Steve Young", "title": "Latent Intention Dialogue Models", "comments": "Accepted at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a dialogue agent that is capable of making autonomous decisions\nand communicating by natural language is one of the long-term goals of machine\nlearning research. Traditional approaches either rely on hand-crafting a small\nstate-action set for applying reinforcement learning that is not scalable or\nconstructing deterministic models for learning dialogue sentences that fail to\ncapture natural conversational variability. In this paper, we propose a Latent\nIntention Dialogue Model (LIDM) that employs a discrete latent variable to\nlearn underlying dialogue intentions in the framework of neural variational\ninference. In a goal-oriented dialogue scenario, these latent intentions can be\ninterpreted as actions guiding the generation of machine responses, which can\nbe further refined autonomously by reinforcement learning. The experimental\nevaluation of LIDM shows that the model out-performs published benchmarks for\nboth corpus-based and human evaluation, demonstrating the effectiveness of\ndiscrete latent variable models for learning goal-oriented dialogues.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:01:44 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Miao", "Yishu", ""], ["Blunsom", "Phil", ""], ["Young", "Steve", ""]]}, {"id": "1705.10272", "submitter": "Ted Pedersen", "authors": "Xinru Yan and Ted Pedersen", "title": "Who's to say what's funny? A computer using Language Models and Deep\n  Learning, That's Who!", "comments": "3 pages, Appears in the Proceedings of the Women and Underrepresented\n  Minorities in Natural Language Processing Workshop (WiNLP-2017), July 30,\n  2017, Vancouver, BC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humor is a defining characteristic of human beings. Our goal is to develop\nmethods that automatically detect humorous statements and rank them on a\ncontinuous scale. In this paper we report on results using a Language Model\napproach, and outline our plans for using methods from Deep Learning.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 16:20:21 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Yan", "Xinru", ""], ["Pedersen", "Ted", ""]]}, {"id": "1705.10369", "submitter": "Andrew Drozdov", "authors": "Katrina Evtimova, Andrew Drozdov, Douwe Kiela, Kyunghyun Cho", "title": "Emergent Communication in a Multi-Modal, Multi-Step Referential Game", "comments": "Published as a conference paper at ICLR 2018. 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.IT cs.MA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by previous work on emergent communication in referential games, we\npropose a novel multi-modal, multi-step referential game, where the sender and\nreceiver have access to distinct modalities of an object, and their information\nexchange is bidirectional and of arbitrary duration. The multi-modal multi-step\nsetting allows agents to develop an internal communication significantly closer\nto natural language, in that they share a single set of messages, and that the\nlength of the conversation may vary according to the difficulty of the task. We\nexamine these properties empirically using a dataset consisting of images and\ntextual descriptions of mammals, where the agents are tasked with identifying\nthe correct object. Our experiments indicate that a robust and efficient\ncommunication protocol emerges, where gradual information exchange informs\nbetter predictions and higher communication bandwidth improves generalization.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 19:25:49 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 17:09:14 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 04:07:30 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 19:22:22 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Evtimova", "Katrina", ""], ["Drozdov", "Andrew", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1705.10415", "submitter": "Diego Amancio Dr.", "authors": "Vanessa Q. Marinho, Henrique F. de Arruda, Thales S. Lima, Luciano F.\n  Costa and Diego R. Amancio", "title": "On the \"Calligraphy\" of Books", "comments": "TextGraphs ACL 2017 (to appear)", "journal-ref": null, "doi": "10.18653/v1/w17-2401", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship attribution is a natural language processing task that has been\nwidely studied, often by considering small order statistics. In this paper, we\nexplore a complex network approach to assign the authorship of texts based on\ntheir mesoscopic representation, in an attempt to capture the flow of the\nnarrative. Indeed, as reported in this work, such an approach allowed the\nidentification of the dominant narrative structure of the studied authors. This\nhas been achieved due to the ability of the mesoscopic approach to take into\naccount relationships between different, not necessarily adjacent, parts of the\ntext, which is able to capture the story flow. The potential of the proposed\napproach has been illustrated through principal component analysis, a\ncomparison with the chance baseline method, and network visualization. Such\nvisualizations reveal individual characteristics of the authors, which can be\nunderstood as a kind of calligraphy.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 23:34:03 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Marinho", "Vanessa Q.", ""], ["de Arruda", "Henrique F.", ""], ["Lima", "Thales S.", ""], ["Costa", "Luciano F.", ""], ["Amancio", "Diego R.", ""]]}, {"id": "1705.10586", "submitter": "Zhenzhou Wu", "authors": "Zhenzhou Wu and Xin Zheng and Daniel Dahlmeier", "title": "Character-Based Text Classification using Top Down Semantic Model for\n  Sentence Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the success of deep learning on many fronts especially image and\nspeech, its application in text classification often is still not as good as a\nsimple linear SVM on n-gram TF-IDF representation especially for smaller\ndatasets. Deep learning tends to emphasize on sentence level semantics when\nlearning a representation with models like recurrent neural network or\nrecursive neural network, however from the success of TF-IDF representation, it\nseems a bag-of-words type of representation has its strength. Taking advantage\nof both representions, we present a model known as TDSM (Top Down Semantic\nModel) for extracting a sentence representation that considers both the\nword-level semantics by linearly combining the words with attention weights and\nthe sentence-level semantics with BiLSTM and use it on text classification. We\napply the model on characters and our results show that our model is better\nthan all the other character-based and word-based convolutional neural network\nmodels by \\cite{zhang15} across seven different datasets with only 1\\% of their\nparameters. We also demonstrate that this model beats traditional linear models\non TF-IDF vectors on small and polished datasets like news article in which\ntypically deep learning models surrender.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:53:00 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Wu", "Zhenzhou", ""], ["Zheng", "Xin", ""], ["Dahlmeier", "Daniel", ""]]}, {"id": "1705.10610", "submitter": "Hoang Pham", "authors": "Thai-Hoang Pham, Phuong Le-Hong", "title": "The Importance of Automatic Syntactic Features in Vietnamese Named\n  Entity Recognition", "comments": "7 pages, 9 tables, 3 figures, accepted to PACLIC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a state-of-the-art system for Vietnamese Named Entity\nRecognition (NER). By incorporating automatic syntactic features with word\nembeddings as input for bidirectional Long Short-Term Memory (Bi-LSTM), our\nsystem, although simpler than some deep learning architectures, achieves a much\nbetter result for Vietnamese NER. The proposed method achieves an overall F1\nscore of 92.05% on the test set of an evaluation campaign, organized in late\n2016 by the Vietnamese Language and Speech Processing (VLSP) community. Our\nnamed entity recognition system outperforms the best previous systems for\nVietnamese NER by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 08:10:15 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 16:23:40 GMT"}, {"version": "v3", "created": "Thu, 8 Jun 2017 16:17:49 GMT"}, {"version": "v4", "created": "Mon, 28 Aug 2017 02:32:16 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Pham", "Thai-Hoang", ""], ["Le-Hong", "Phuong", ""]]}, {"id": "1705.10754", "submitter": "Francisco Rangel", "authors": "Francisco Rangel and Marc Franco-Salvador and Paolo Rosso", "title": "A Low Dimensionality Representation for Language Variety Identification", "comments": null, "journal-ref": "CICLing - Computational Linguistics and Intelligent Text\n  Processing, 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language variety identification aims at labelling texts in a native language\n(e.g. Spanish, Portuguese, English) with its specific variation (e.g.\nArgentina, Chile, Mexico, Peru, Spain; Brazil, Portugal; UK, US). In this work\nwe propose a low dimensionality representation (LDR) to address this task with\nfive different varieties of Spanish: Argentina, Chile, Mexico, Peru and Spain.\nWe compare our LDR method with common state-of-the-art representations and show\nan increase in accuracy of ~35%. Furthermore, we compare LDR with two reference\ndistributed representation models. Experimental results show competitive\nperformance while dramatically reducing the dimensionality --and increasing the\nbig data suitability-- to only 6 features per variety. Additionally, we analyse\nthe behaviour of the employed machine learning algorithms and the most\ndiscriminating features. Finally, we employ an alternative dataset to test the\nrobustness of our low dimensionality representation with another set of similar\nlanguages.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 17:07:45 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Rangel", "Francisco", ""], ["Franco-Salvador", "Marc", ""], ["Rosso", "Paolo", ""]]}, {"id": "1705.10814", "submitter": "Xiang Yu", "authors": "Xiang Yu and Ngoc Thang Vu", "title": "Character Composition Model with Convolutional Neural Networks for\n  Dependency Parsing on Morphologically Rich Languages", "comments": "Accepted in ACL 2017 (Short)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a transition-based dependency parser that uses a convolutional\nneural network to compose word representations from characters. The character\ncomposition model shows great improvement over the word-lookup model,\nespecially for parsing agglutinative languages. These improvements are even\nbetter than using pre-trained word embeddings from extra data. On the SPMRL\ndata sets, our system outperforms the previous best greedy parser (Ballesteros\net al., 2015) by a margin of 3% on average.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 18:23:18 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Yu", "Xiang", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "1705.10874", "submitter": "Zixing Zhang", "authors": "Zixing Zhang, J\\\"urgen Geiger, Jouni Pohjalainen, Amr El-Desoky Mousa,\n  Wenyu Jin, Bj\\\"orn Schuller", "title": "Deep Learning for Environmentally Robust Speech Recognition: An Overview\n  of Recent Developments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliminating the negative effect of non-stationary environmental noise is a\nlong-standing research topic for automatic speech recognition that stills\nremains an important challenge. Data-driven supervised approaches, including\nones based on deep neural networks, have recently emerged as potential\nalternatives to traditional unsupervised approaches and with sufficient\ntraining, can alleviate the shortcomings of the unsupervised methods in various\nreal-life acoustic environments. In this light, we review recently developed,\nrepresentative deep learning approaches for tackling non-stationary additive\nand convolutional degradation of speech with the aim of providing guidelines\nfor those involved in the development of environmentally robust speech\nrecognition systems. We separately discuss single- and multi-channel techniques\ndeveloped for the front-end and back-end of speech recognition systems, as well\nas joint front-end and back-end training frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 21:31:25 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 09:44:32 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 09:05:57 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Zhang", "Zixing", ""], ["Geiger", "J\u00fcrgen", ""], ["Pohjalainen", "Jouni", ""], ["Mousa", "Amr El-Desoky", ""], ["Jin", "Wenyu", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1705.10900", "submitter": "Paul Michel", "authors": "Paul Michel, Abhilasha Ravichander, Shruti Rijhwani", "title": "Does the Geometry of Word Embeddings Help Document Classification? A\n  Case Study on Persistent Homology Based Representations", "comments": "5 pages, 3 figures. Rep4NLP workshop at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the pertinence of methods from algebraic topology for text\ndata analysis. These methods enable the development of\nmathematically-principled isometric-invariant mappings from a set of vectors to\na document embedding, which is stable with respect to the geometry of the\ndocument in the selected metric space. In this work, we evaluate the utility of\nthese topology-based document representations in traditional NLP tasks,\nspecifically document clustering and sentiment classification. We find that the\nembeddings do not benefit text analysis. In fact, performance is worse than\nsimple techniques like $\\textit{tf-idf}$, indicating that the geometry of the\ndocument does not provide enough variability for classification on the basis of\ntopic or sentiment in the chosen datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 00:43:04 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Michel", "Paul", ""], ["Ravichander", "Abhilasha", ""], ["Rijhwani", "Shruti", ""]]}, {"id": "1705.10929", "submitter": "Sandeep Subramanian", "authors": "Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal,\n  Aaron Courville", "title": "Adversarial Generation of Natural Language", "comments": "11 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have gathered a lot of attention from\nthe computer vision community, yielding impressive results for image\ngeneration. Advances in the adversarial generation of natural language from\nnoise however are not commensurate with the progress made in generating images,\nand still lag far behind likelihood based methods. In this paper, we take a\nstep towards generating natural language with a GAN objective alone. We\nintroduce a simple baseline that addresses the discrete output space problem\nwithout relying on gradient estimators and show that it is able to achieve\nstate-of-the-art results on a Chinese poem generation dataset. We present\nquantitative results on generating sentences from context-free and\nprobabilistic context-free grammars, and qualitative language modeling results.\nA conditional version is also described that can generate sequences conditioned\non sentence characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 03:06:39 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Rajeswar", "Sai", ""], ["Subramanian", "Sandeep", ""], ["Dutil", "Francis", ""], ["Pal", "Christopher", ""], ["Courville", "Aaron", ""]]}, {"id": "1705.10962", "submitter": "Koichiro Yoshino Ph.D.", "authors": "Koichiro Yoshino, Shinsuke Mori, Satoshi Nakamura", "title": "Analysis of the Effect of Dependency Information on Predicate-Argument\n  Structure Analysis and Zero Anaphora Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates and analyzes the effect of dependency information on\npredicate-argument structure analysis (PASA) and zero anaphora resolution (ZAR)\nfor Japanese, and shows that a straightforward approach of PASA and ZAR works\neffectively even if dependency information was not available. We constructed an\nanalyzer that directly predicts relationships of predicates and arguments with\ntheir semantic roles from a POS-tagged corpus. The features of the system are\ndesigned to compensate for the absence of syntactic information by using\nfeatures used in dependency parsing as a reference. We also constructed\nanalyzers that use the oracle dependency and the real dependency parsing\nresults, and compared with the system that does not use any syntactic\ninformation to verify that the improvement provided by dependencies is not\ncrucial.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 07:32:29 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Yoshino", "Koichiro", ""], ["Mori", "Shinsuke", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1705.11001", "submitter": "Kevin Lin", "authors": "Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, Ming-Ting Sun", "title": "Adversarial Ranking for Language Generation", "comments": "NIPS2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have great successes on synthesizing\ndata. However, the existing GANs restrict the discriminator to be a binary\nclassifier, and thus limit their learning capacity for tasks that need to\nsynthesize output with rich structures such as natural language descriptions.\nIn this paper, we propose a novel generative adversarial network, RankGAN, for\ngenerating high-quality language descriptions. Rather than training the\ndiscriminator to learn and assign absolute binary predicate for individual data\nsample, the proposed RankGAN is able to analyze and rank a collection of\nhuman-written and machine-written sentences by giving a reference group. By\nviewing a set of data samples collectively and evaluating their quality through\nrelative ranking scores, the discriminator is able to make better assessment\nwhich in turn helps to learn a better generator. The proposed RankGAN is\noptimized through the policy gradient technique. Experimental results on\nmultiple public datasets clearly demonstrate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:21:04 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 01:58:50 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 05:43:33 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Lin", "Kevin", ""], ["Li", "Dianqi", ""], ["He", "Xiaodong", ""], ["Zhang", "Zhengyou", ""], ["Sun", "Ming-Ting", ""]]}, {"id": "1705.11122", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig", "title": "Controllable Invariance through Adversarial Feature Learning", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning meaningful representations that maintain the content necessary for a\nparticular task while filtering away detrimental variations is a problem of\ngreat interest in machine learning. In this paper, we tackle the problem of\nlearning representations invariant to a specific factor or trait of data. The\nrepresentation learning process is formulated as an adversarial minimax game.\nWe analyze the optimal equilibrium of such a game and find that it amounts to\nmaximizing the uncertainty of inferring the detrimental factor given the\nrepresentation while maximizing the certainty of making task-specific\npredictions. On three benchmark tasks, namely fair and bias-free\nclassification, language-independent generation, and lighting-independent image\nclassification, we show that the proposed framework induces an invariant\nrepresentation, and leads to better generalization evidenced by the improved\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 14:57:33 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 17:47:33 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 00:59:58 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Xie", "Qizhe", ""], ["Dai", "Zihang", ""], ["Du", "Yulun", ""], ["Hovy", "Eduard", ""], ["Neubig", "Graham", ""]]}, {"id": "1705.11160", "submitter": "Junhui Li", "authors": "Junhui Li and Muhua Zhu", "title": "Learning When to Attend for Neural Machine Translation", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, attention mechanisms have become an indispensable\ncomponent of end-to-end neural machine translation models. However, previous\nattention models always refer to some source words when predicting a target\nword, which contradicts with the fact that some target words have no\ncorresponding source words. Motivated by this observation, we propose a novel\nattention model that has the capability of determining when a decoder should\nattend to source words and when it should not. Experimental results on NIST\nChinese-English translation tasks show that the new model achieves an\nimprovement of 0.8 BLEU score over a state-of-the-art baseline.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 16:05:49 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Li", "Junhui", ""], ["Zhu", "Muhua", ""]]}, {"id": "1705.11168", "submitter": "Jon Gauthier", "authors": "Li Lucy, Jon Gauthier", "title": "Are distributional representations ready for the real world? Evaluating\n  word vectors for grounded perceptual meaning", "comments": "Accepted at RoboNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distributional word representation methods exploit word co-occurrences to\nbuild compact vector encodings of words. While these representations enjoy\nwidespread use in modern natural language processing, it is unclear whether\nthey accurately encode all necessary facets of conceptual meaning. In this\npaper, we evaluate how well these representations can predict perceptual and\nconceptual features of concrete concepts, drawing on two semantic norm datasets\nsourced from human participants. We find that several standard word\nrepresentations fail to encode many salient perceptual features of concepts,\nand show that these deficits correlate with word-word similarity prediction\nerrors. Our analyses provide motivation for grounded and embodied language\nlearning approaches, which may help to remedy these deficits.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 16:31:54 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Lucy", "Li", ""], ["Gauthier", "Jon", ""]]}, {"id": "1705.11192", "submitter": "Serhii Havrylov", "authors": "Serhii Havrylov, Ivan Titov", "title": "Emergence of Language with Multi-agent Games: Learning to Communicate\n  with Sequences of Symbols", "comments": "The paper was accepted at NIPS 2017. The extended abstract was\n  presented at ICLR 2017 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to communicate through interaction, rather than relying on explicit\nsupervision, is often considered a prerequisite for developing a general AI. We\nstudy a setting where two agents engage in playing a referential game and, from\nscratch, develop a communication protocol necessary to succeed in this game.\nUnlike previous work, we require that messages they exchange, both at train and\ntest time, are in the form of a language (i.e. sequences of discrete symbols).\nWe compare a reinforcement learning approach and one using a differentiable\nrelaxation (straight-through Gumbel-softmax estimator) and observe that the\nlatter is much faster to converge and it results in more effective protocols.\nInterestingly, we also observe that the protocol we induce by optimizing the\ncommunication success exhibits a degree of compositionality and variability\n(i.e. the same information can be phrased in different ways), both properties\ncharacteristic of natural languages. As the ultimate goal is to ensure that\ncommunication is accomplished in natural language, we also perform experiments\nwhere we inject prior information about natural language into our model and\nstudy properties of the resulting protocol.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 17:47:55 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 15:04:51 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Havrylov", "Serhii", ""], ["Titov", "Ivan", ""]]}]