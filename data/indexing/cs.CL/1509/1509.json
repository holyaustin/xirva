[{"id": "1509.00533", "submitter": "Scott Wisdom", "authors": "Scott Wisdom, Thomas Powers, Les Atlas, and James Pitton", "title": "Enhancement and Recognition of Reverberant and Noisy Speech by Extending\n  Its Coherence", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most speech enhancement algorithms make use of the short-time Fourier\ntransform (STFT), which is a simple and flexible time-frequency decomposition\nthat estimates the short-time spectrum of a signal. However, the duration of\nshort STFT frames are inherently limited by the nonstationarity of speech\nsignals. The main contribution of this paper is a demonstration of speech\nenhancement and automatic speech recognition in the presence of reverberation\nand noise by extending the length of analysis windows. We accomplish this\nextension by performing enhancement in the short-time fan-chirp transform\n(STFChT) domain, an overcomplete time-frequency representation that is coherent\nwith speech signals over longer analysis window durations than the STFT. This\nextended coherence is gained by using a linear model of fundamental frequency\nvariation of voiced speech signals. Our approach centers around using a\nsingle-channel minimum mean-square error log-spectral amplitude (MMSE-LSA)\nestimator proposed by Habets, which scales coefficients in a time-frequency\ndomain to suppress noise and reverberation. In the case of multiple\nmicrophones, we preprocess the data with either a minimum variance\ndistortionless response (MVDR) beamformer, or a delay-and-sum beamformer (DSB).\nWe evaluate our algorithm on both speech enhancement and recognition tasks for\nthe REVERB challenge dataset. Compared to the same processing done in the STFT\ndomain, our approach achieves significant improvement in terms of objective\nenhancement metrics (including PESQ---the ITU-T standard measurement for speech\nquality). In terms of automatic speech recognition (ASR) performance as\nmeasured by word error rate (WER), our experiments indicate that the STFT with\na long window is more effective for ASR.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 00:31:40 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Wisdom", "Scott", ""], ["Powers", "Thomas", ""], ["Atlas", "Les", ""], ["Pitton", "James", ""]]}, {"id": "1509.00685", "submitter": "Alexander M. Rush", "authors": "Alexander M. Rush, Sumit Chopra and Jason Weston", "title": "A Neural Attention Model for Abstractive Sentence Summarization", "comments": "Proceedings of EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarization based on text extraction is inherently limited, but\ngeneration-style abstractive methods have proven challenging to build. In this\nwork, we propose a fully data-driven approach to abstractive sentence\nsummarization. Our method utilizes a local attention-based model that generates\neach word of the summary conditioned on the input sentence. While the model is\nstructurally simple, it can easily be trained end-to-end and scales to a large\namount of training data. The model shows significant performance gains on the\nDUC-2004 shared task compared with several strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 13:20:40 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 19:55:45 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Rush", "Alexander M.", ""], ["Chopra", "Sumit", ""], ["Weston", "Jason", ""]]}, {"id": "1509.00705", "submitter": "Dinesh Balaji Sashikanth", "authors": "Dinesh Balaji Sashikanth", "title": "Analysis of Communication Pattern with Scammers in Enron Corpus", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an exploratory analysis into fraud detection taking Enron email\ncorpus as the case study. The paper posits conclusions like strict servitude\nand unquestionable faith among employees as breeding grounds for sham among\nhigher executives. We also try to infer on the nature of communication between\nfraudulent employees and between non- fraudulent-fraudulent employees\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 13:54:57 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Sashikanth", "Dinesh Balaji", ""]]}, {"id": "1509.00838", "submitter": "Hongyuan Mei", "authors": "Hongyuan Mei and Mohit Bansal and Matthew R. Walter", "title": "What to talk about and how? Selective Generation using LSTMs with\n  Coarse-to-Fine Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end, domain-independent neural encoder-aligner-decoder\nmodel for selective generation, i.e., the joint task of content selection and\nsurface realization. Our model first encodes a full set of over-determined\ndatabase event records via an LSTM-based recurrent neural network, then\nutilizes a novel coarse-to-fine aligner to identify the small subset of salient\nrecords to talk about, and finally employs a decoder to generate free-form\ndescriptions of the aligned, selected records. Our model achieves the best\nselection and generation results reported to-date (with 59% relative\nimprovement in generation) on the benchmark WeatherGov dataset, despite using\nno specialized features or linguistic resources. Using an improved k-nearest\nneighbor beam filter helps further. We also perform a series of ablations and\nvisualizations to elucidate the contributions of our key model components.\nLastly, we evaluate the generalizability of our model on the RoboCup dataset,\nand get results that are competitive with or better than the state-of-the-art,\ndespite being severely data-starved.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 19:52:56 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 23:07:32 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Mei", "Hongyuan", ""], ["Bansal", "Mohit", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1509.00963", "submitter": "Dilek K\\\"u\\c{c}\\\"uk", "authors": "Dilek K\\\"u\\c{c}\\\"uk and Do\\u{g}an K\\\"u\\c{c}\\\"uk", "title": "On TimeML-Compliant Temporal Expression Extraction in Turkish", "comments": "7 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly acknowledged that temporal expression extractors are important\ncomponents of larger natural language processing systems like information\nretrieval and question answering systems. Extraction and normalization of\ntemporal expressions in Turkish has not been given attention so far except the\nextraction of some date and time expressions within the course of named entity\nrecognition. As TimeML is the current standard of temporal expression and event\nannotation in natural language texts, in this paper, we present an analysis of\ntemporal expressions in Turkish based on the related TimeML classification\n(i.e., date, time, duration, and set expressions). We have created a lexicon\nfor Turkish temporal expressions and devised considerably wide-coverage\npatterns using the lexical classes as the building blocks. We believe that the\nproposed patterns, together with convenient normalization rules, can be readily\nused by prospective temporal expression extraction tools for Turkish.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 07:23:10 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["K\u00fc\u00e7\u00fck", "Dilek", ""], ["K\u00fc\u00e7\u00fck", "Do\u011fan", ""]]}, {"id": "1509.01007", "submitter": "Shay Cohen", "authors": "Dominique Osborne, Shashi Narayan and Shay B. Cohen", "title": "Encoding Prior Knowledge with Eigenword Embeddings", "comments": "in Transactions of the Association of Computational Linguistics\n  (TACL), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a method for reducing the dimension\nof data represented using two views. It has been previously used to derive word\nembeddings, where one view indicates a word, and the other view indicates its\ncontext. We describe a way to incorporate prior knowledge into CCA, give a\ntheoretical justification for it, and test it by deriving word embeddings and\nevaluating them on a myriad of datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 09:39:36 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 10:54:17 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 12:46:39 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Osborne", "Dominique", ""], ["Narayan", "Shashi", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1509.01023", "submitter": "Ibrahim Adeyanju", "authors": "Ibrahim Adeyanju", "title": "Generating Weather Forecast Texts with Case Based Reasoning", "comments": "6 pages", "journal-ref": "International Journal of Computer Applications 45(10) (2012) 35-40", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several techniques have been used to generate weather forecast texts. In this\npaper, case based reasoning (CBR) is proposed for weather forecast text\ngeneration because similar weather conditions occur over time and should have\nsimilar forecast texts. CBR-METEO, a system for generating weather forecast\ntexts was developed using a generic framework (jCOLIBRI) which provides modules\nfor the standard components of the CBR architecture. The advantage in a CBR\napproach is that systems can be built in minimal time with far less human\neffort after initial consultation with experts. The approach depends heavily on\nthe goodness of the retrieval and revision components of the CBR process. We\nevaluated CBRMETEO with NIST, an automated metric which has been shown to\ncorrelate well with human judgements for this domain. The system shows\ncomparable performance with other NLG systems that perform the same task.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 10:21:16 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Adeyanju", "Ibrahim", ""]]}, {"id": "1509.01288", "submitter": "Max Zimmermann", "authors": "Max Zimmermann, Eirini Ntoutsi, Myra Spiliopoulou", "title": "Incremental Active Opinion Learning Over a Stream of Opinionated\n  Documents", "comments": "10 pages, 14 figures, conference: WISDOM (KDD'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications that learn from opinionated documents, like tweets or product\nreviews, face two challenges. First, the opinionated documents constitute an\nevolving stream, where both the author's attitude and the vocabulary itself may\nchange. Second, labels of documents are scarce and labels of words are\nunreliable, because the sentiment of a word depends on the (unknown) context in\nthe author's mind. Most of the research on mining over opinionated streams\nfocuses on the first aspect of the problem, whereas for the second a continuous\nsupply of labels from the stream is assumed. Such an assumption though is\nutopian as the stream is infinite and the labeling cost is prohibitive. To this\nend, we investigate the potential of active stream learning algorithms that ask\nfor labels on demand. Our proposed ACOSTREAM 1 approach works with limited\nlabels: it uses an initial seed of labeled documents, occasionally requests\nadditional labels for documents from the human expert and incrementally adapts\nto the underlying stream while exploiting the available labeled documents. In\nits core, ACOSTREAM consists of a MNB classifier coupled with \"sampling\"\nstrategies for requesting class labels for new unlabeled documents. In the\nexperiments, we evaluate the classifier performance over time by varying: (a)\nthe class distribution of the opinionated stream, while assuming that the set\nof the words in the vocabulary is fixed but their polarities may change with\nthe class distribution; and (b) the number of unknown words arriving at each\nmoment, while the class polarity may also change. Our results show that active\nlearning on a stream of opinionated documents, delivers good performance while\nrequiring a small selection of labels\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 22:11:10 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Zimmermann", "Max", ""], ["Ntoutsi", "Eirini", ""], ["Spiliopoulou", "Myra", ""]]}, {"id": "1509.01310", "submitter": "Haitao Liu", "authors": "Qian Lu, Chunshan Xu and Haitao Liu", "title": "The influence of Chunking on Dependency Crossing and Distance", "comments": "6 figures", "journal-ref": null, "doi": "10.1002/cplx.21779.", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper hypothesizes that chunking plays important role in reducing\ndependency distance and dependency crossings. Computer simulations, when\ncompared with natural languages,show that chunking reduces mean dependency\ndistance (MDD) of a linear sequence of nodes (constrained by continuity or\nprojectivity) to that of natural languages. More interestingly, chunking alone\nbrings about less dependency crossings as well, though having failed to reduce\nthem, to such rarity as found in human languages. These results suggest that\nchunking may play a vital role in the minimization of dependency distance, and\na somewhat contributing role in the rarity of dependency crossing. In addition,\nthe results point to a possibility that the rarity of dependency crossings is\nnot a mere side-effect of minimization of dependency distance, but a linguistic\nphenomenon with its own motivations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 23:57:46 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Lu", "Qian", ""], ["Xu", "Chunshan", ""], ["Liu", "Haitao", ""]]}, {"id": "1509.01599", "submitter": "Jacob Eisenstein", "authors": "Parminder Bhatia and Yangfeng Ji and Jacob Eisenstein", "title": "Better Document-level Sentiment Analysis from RST Discourse Parsing", "comments": "Published at Empirical Methods in Natural Language Processing (EMNLP\n  2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse structure is the hidden link between surface features and\ndocument-level properties, such as sentiment polarity. We show that the\ndiscourse analyses produced by Rhetorical Structure Theory (RST) parsers can\nimprove document-level sentiment analysis, via composition of local information\nup the discourse tree. First, we show that reweighting discourse units\naccording to their position in a dependency representation of the rhetorical\nstructure can yield substantial improvements on lexicon-based sentiment\nanalysis. Next, we present a recursive neural network over the RST structure,\nwhich offers significant improvements over classification-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 20:28:12 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 15:41:53 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Bhatia", "Parminder", ""], ["Ji", "Yangfeng", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1509.01626", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Junbo Zhao, Yann LeCun", "title": "Character-level Convolutional Networks for Text Classification", "comments": "An early version of this work entitled \"Text Understanding from\n  Scratch\" was posted in Feb 2015 as arXiv:1502.01710. The present paper has\n  considerably more experimental results and a rewritten introduction, Advances\n  in Neural Information Processing Systems 28 (NIPS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article offers an empirical exploration on the use of character-level\nconvolutional networks (ConvNets) for text classification. We constructed\nseveral large-scale datasets to show that character-level convolutional\nnetworks could achieve state-of-the-art or competitive results. Comparisons are\noffered against traditional models such as bag of words, n-grams and their\nTFIDF variants, and deep learning models such as word-based ConvNets and\nrecurrent neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 22:31:53 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 17:12:43 GMT"}, {"version": "v3", "created": "Mon, 4 Apr 2016 02:34:30 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Zhang", "Xiang", ""], ["Zhao", "Junbo", ""], ["LeCun", "Yann", ""]]}, {"id": "1509.01692", "submitter": "Ekaterina Vylomova", "authors": "Ekaterina Vylomova, Laura Rimell, Trevor Cohn, Timothy Baldwin", "title": "Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility\n  of Vector Differences for Lexical Relation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on word embeddings has shown that simple vector subtraction over\npre-trained embeddings is surprisingly effective at capturing different lexical\nrelations, despite lacking explicit supervision. Prior work has evaluated this\nintriguing result using a word analogy prediction formulation and hand-selected\nrelations, but the generality of the finding over a broader range of lexical\nrelation types and different learning settings has not been evaluated. In this\npaper, we carry out such an evaluation in two learning settings: (1) spectral\nclustering to induce word relations, and (2) supervised learning to classify\nvector differences into relation types. We find that word embeddings capture a\nsurprising amount of information, and that, under suitable supervised training,\nvector subtraction generalises well to a broad range of relations, including\nover unseen lexical items.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 11:23:44 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 12:20:03 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 05:44:33 GMT"}, {"version": "v4", "created": "Sat, 13 Aug 2016 17:56:01 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Vylomova", "Ekaterina", ""], ["Rimell", "Laura", ""], ["Cohn", "Trevor", ""], ["Baldwin", "Timothy", ""]]}, {"id": "1509.01722", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "A commentary on \"The now-or-never bottleneck: a fundamental constraint\n  on language\", by Christiansen and Chater (2016)", "comments": null, "journal-ref": "Glottometrics 38, 107-111 (2017)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent article, Christiansen and Chater (2016) present a fundamental\nconstraint on language, i.e. a now-or-never bottleneck that arises from our\nfleeting memory, and explore its implications, e.g., chunk-and-pass processing,\noutlining a framework that promises to unify different areas of research. Here\nwe explore additional support for this constraint and suggest further\nconnections from quantitative linguistics and information theory.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 17:52:16 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 05:44:59 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1509.01771", "submitter": "Gibran Fuentes-Pineda", "authors": "Gibran Fuentes-Pineda and Ivan Vladimir Meza-Ruiz", "title": "Sampled Weighted Min-Hashing for Large-Scale Topic Mining", "comments": "10 pages, Proceedings of the Mexican Conference on Pattern\n  Recognition 2015", "journal-ref": null, "doi": "10.1007/978-3-319-19264-2_20", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to\nautomatically mine topics from large-scale corpora. SWMH generates multiple\nrandom partitions of the corpus vocabulary based on term co-occurrence and\nagglomerates highly overlapping inter-partition cells to produce the mined\ntopics. While other approaches define a topic as a probabilistic distribution\nover a vocabulary, SWMH topics are ordered subsets of such vocabulary.\nInterestingly, the topics mined by SWMH underlie themes from the corpus at\ndifferent levels of granularity. We extensively evaluate the meaningfulness of\nthe mined topics both qualitatively and quantitatively on the NIPS (1.7 K\ndocuments), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora.\nAdditionally, we compare the quality of SWMH with Online LDA topics for\ndocument representation in classification.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 06:09:28 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 03:14:12 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Fuentes-Pineda", "Gibran", ""], ["Meza-Ruiz", "Ivan Vladimir", ""]]}, {"id": "1509.01865", "submitter": "Alex Olieman", "authors": "Alex Olieman, Jaap Kamps, Maarten Marx, and Arjan Nusselder", "title": "A Hybrid Approach to Domain-Specific Entity Linking", "comments": "SEM'15", "journal-ref": "Proc. Posters and Demos track of 11th Int. Conf. on Semantic\n  Systems (2015) 55-58", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art Entity Linking (EL) systems are geared towards\ncorpora that are as heterogeneous as the Web, and therefore perform\nsub-optimally on domain-specific corpora. A key open problem is how to\nconstruct effective EL systems for specific domains, as knowledge of the local\ncontext should in principle increase, rather than decrease, effectiveness. In\nthis paper we propose the hybrid use of simple specialist linkers in\ncombination with an existing generalist system to address this problem. Our\nmain findings are the following. First, we construct a new reusable benchmark\nfor EL on a corpus of domain-specific conversations. Second, we test the\nperformance of a range of approaches under the same conditions, and show that\nspecialist linkers obtain high precision in isolation, and high recall when\ncombined with generalist linkers. Hence, we can effectively exploit local\ncontext and get the best of both worlds.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 23:16:45 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Olieman", "Alex", ""], ["Kamps", "Jaap", ""], ["Marx", "Maarten", ""], ["Nusselder", "Arjan", ""]]}, {"id": "1509.01899", "submitter": "Quan Liu", "authors": "Quan Liu, Wu Guo, Zhen-Hua Ling", "title": "Integrate Document Ranking Information into Confidence Measure\n  Calculation for Spoken Term Detection", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an algorithm to improve the calculation of confidence\nmeasure for spoken term detection (STD). Given an input query term, the\nalgorithm first calculates a measurement named document ranking weight for each\ndocument in the speech database to reflect its relevance with the query term by\nsumming all the confidence measures of the hypothesized term occurrences in\nthis document. The confidence measure of each term occurrence is then\nre-estimated through linear interpolation with the calculated document ranking\nweight to improve its reliability by integrating document-level information.\nExperiments are conducted on three standard STD tasks for Tamil, Vietnamese and\nEnglish respectively. The experimental results all demonstrate that the\nproposed algorithm achieves consistent improvements over the state-of-the-art\nmethod for confidence measure calculation. Furthermore, this algorithm is still\neffective even if a high accuracy speech recognizer is not available, which\nmakes it applicable for the languages with limited speech resources.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 04:40:14 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 09:01:35 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Liu", "Quan", ""], ["Guo", "Wu", ""], ["Ling", "Zhen-Hua", ""]]}, {"id": "1509.01938", "submitter": "Katrin Kirchhoff", "authors": "Katrin Kirchhoff, Bing Zhao, Wen Wang", "title": "Exploiting Out-of-Domain Data Sources for Dialectal Arabic Statistical\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine translation for dialectal Arabic is characterized by a\nlack of data since data acquisition involves the transcription and translation\nof spoken language. In this study we develop techniques for extracting parallel\ndata for one particular dialect of Arabic (Iraqi Arabic) from out-of-domain\ncorpora in different dialects of Arabic or in Modern Standard Arabic. We\ncompare two different data selection strategies (cross-entropy based and\nsubmodular selection) and demonstrate that a very small but highly targeted\namount of found data can improve the performance of a baseline machine\ntranslation system. We furthermore report on preliminary experiments on using\nautomatically translated speech data as additional training data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 07:54:17 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Kirchhoff", "Katrin", ""], ["Zhao", "Bing", ""], ["Wang", "Wen", ""]]}, {"id": "1509.01978", "submitter": "Alessia Amelio Dr.", "authors": "Darko Brodic, Alessia Amelio, Zoran N. Milivojevic", "title": "An Approach to the Analysis of the South Slavic Medieval Labels Using\n  Image Texture", "comments": "15 pages, 9 figures, 3rd Workshop on Recognition and Action for Scene\n  Understanding (REACTS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new script classification method for the discrimination\nof the South Slavic medieval labels. It consists in the textural analysis of\nthe script types. In the first step, each letter is coded by the equivalent\nscript type, which is defined by its typographical features. Obtained coded\ntext is subjected to the run-length statistical analysis and to the adjacent\nlocal binary pattern analysis in order to extract the features. The result\nshows a diversity between the extracted features of the scripts, which makes\nthe feature classification more effective. It is the basis for the\nclassification process of the script identification by using an extension of a\nstate-of-the-art approach for document clustering. The proposed method is\nevaluated on an example of hand-engraved in stone and hand-printed in paper\nlabels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate\nvery positive results, which prove the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 10:39:20 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Brodic", "Darko", ""], ["Amelio", "Alessia", ""], ["Milivojevic", "Zoran N.", ""]]}, {"id": "1509.02208", "submitter": "Cheng-Tao Chung", "authors": "Cheng-Tao Chung, Chun-an Chan, Lin-shan Lee", "title": "Unsupervised Discovery of Linguistic Structure Including Two-level\n  Acoustic Patterns Using Three Cascaded Stages of Iterative Optimization", "comments": "Accepted by ICASSP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques for unsupervised discovery of acoustic patterns are getting\nincreasingly attractive, because huge quantities of speech data are becoming\navailable but manual annotations remain hard to acquire. In this paper, we\npropose an approach for unsupervised discovery of linguistic structure for the\ntarget spoken language given raw speech data. This linguistic structure\nincludes two-level (subword-like and word-like) acoustic patterns, the lexicon\nof word-like patterns in terms of subword-like patterns and the N-gram language\nmodel based on word-like patterns. All patterns, models, and parameters can be\nautomatically learned from the unlabelled speech corpus. This is achieved by an\ninitialization step followed by three cascaded stages for acoustic, linguistic,\nand lexical iterative optimization. The lexicon of word-like patterns defines\nallowed consecutive sequence of HMMs for subword-like patterns. In each\niteration, model training and decoding produces updated labels from which the\nlexicon and HMMs can be further updated. In this way, model parameters and\ndecoded labels are respectively optimized in each iteration, and the knowledge\nabout the linguistic structure is learned gradually layer after layer. The\nproposed approach was tested in preliminary experiments on a corpus of Mandarin\nbroadcast news, including a task of spoken term detection with performance\ncompared to a parallel test using models trained in a supervised way. Results\nshow that the proposed system not only yields reasonable performance on its\nown, but is also complimentary to existing large vocabulary ASR systems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 22:23:01 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Chung", "Cheng-Tao", ""], ["Chan", "Chun-an", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1509.02213", "submitter": "Cheng-Tao Chung", "authors": "Cheng-Tao Chung, Chun-an Chan, Lin-shan Lee", "title": "Unsupervised Spoken Term Detection with Spoken Queries by Multi-level\n  Acoustic Patterns with Varying Model Granularity", "comments": "Accepted by ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for unsupervised Spoken Term Detection\nwith spoken queries using multiple sets of acoustic patterns automatically\ndiscovered from the target corpus. The different pattern HMM\nconfigurations(number of states per model, number of distinct models, number of\nGaussians per state)form a three-dimensional model granularity space. Different\nsets of acoustic patterns automatically discovered on different points properly\ndistributed over this three-dimensional space are complementary to one another,\nthus can jointly capture the characteristics of the spoken terms. By\nrepresenting the spoken content and spoken query as sequences of acoustic\npatterns, a series of approaches for matching the pattern index sequences while\nconsidering the signal variations are developed. In this way, not only the\non-line computation load can be reduced, but the signal distributions caused by\ndifferent speakers and acoustic conditions can be reasonably taken care of. The\nresults indicate that this approach significantly outperformed the unsupervised\nfeature-based DTW baseline by 16.16\\% in mean average precision on the TIMIT\ncorpus.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 22:40:31 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Chung", "Cheng-Tao", ""], ["Chan", "Chun-an", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1509.02217", "submitter": "Cheng-Tao Chung", "authors": "Cheng-Tao Chung, Wei-Ning Hsu, Cheng-Yi Lee, Lin-Shan Lee", "title": "Enhancing Automatically Discovered Multi-level Acoustic Patterns\n  Considering Context Consistency With Applications in Spoken Term Detection", "comments": "Accepted by ICASSP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for enhancing the multiple sets of\nacoustic patterns automatically discovered from a given corpus. In a previous\nwork it was proposed that different HMM configurations (number of states per\nmodel, number of distinct models) for the acoustic patterns form a\ntwo-dimensional space. Multiple sets of acoustic patterns automatically\ndiscovered with the HMM configurations properly located on different points\nover this two-dimensional space were shown to be complementary to one another,\njointly capturing the characteristics of the given corpus. By representing the\ngiven corpus as sequences of acoustic patterns on different HMM sets, the\npattern indices in these sequences can be relabeled considering the context\nconsistency across the different sequences. Good improvements were observed in\npreliminary experiments of pattern spoken term detection (STD) performed on\nboth TIMIT and Mandarin Broadcast News with such enhanced patterns.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 22:56:49 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Chung", "Cheng-Tao", ""], ["Hsu", "Wei-Ning", ""], ["Lee", "Cheng-Yi", ""], ["Lee", "Lin-Shan", ""]]}, {"id": "1509.02301", "submitter": "Octavian-Eugen Ganea", "authors": "Octavian-Eugen Ganea, Marina Ganea, Aurelien Lucchi, Carsten Eickhoff,\n  Thomas Hofmann", "title": "Probabilistic Bag-Of-Hyperlinks Model for Entity Linking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many fundamental problems in natural language processing rely on determining\nwhat entities appear in a given text. Commonly referenced as entity linking,\nthis step is a fundamental component of many NLP tasks such as text\nunderstanding, automatic summarization, semantic search or machine translation.\nName ambiguity, word polysemy, context dependencies and a heavy-tailed\ndistribution of entities contribute to the complexity of this problem.\n  We here propose a probabilistic approach that makes use of an effective\ngraphical model to perform collective entity disambiguation. Input mentions\n(i.e.,~linkable token spans) are disambiguated jointly across an entire\ndocument by combining a document-level prior of entity co-occurrences with\nlocal information captured from mentions and their surrounding context. The\nmodel is based on simple sufficient statistics extracted from data, thus\nrelying on few parameters to be learned.\n  Our method does not require extensive feature engineering, nor an expensive\ntraining procedure. We use loopy belief propagation to perform approximate\ninference. The low complexity of our model makes this step sufficiently fast\nfor real-time usage. We demonstrate the accuracy of our approach on a wide\nrange of benchmark datasets, showing that it matches, and in many cases\noutperforms, existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 09:43:13 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 13:40:31 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2016 19:22:44 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Ganea", "Octavian-Eugen", ""], ["Ganea", "Marina", ""], ["Lucchi", "Aurelien", ""], ["Eickhoff", "Carsten", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1509.02409", "submitter": "Mortaza Doulaty", "authors": "Mortaza Doulaty, Oscar Saz, Thomas Hain", "title": "Data-selective Transfer Learning for Multi-Domain Speech Recognition", "comments": null, "journal-ref": "16th Interspeech.Proc. (2015) 2897-2901", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative transfer in training of acoustic models for automatic speech\nrecognition has been reported in several contexts such as domain change or\nspeaker characteristics. This paper proposes a novel technique to overcome\nnegative transfer by efficient selection of speech data for acoustic model\ntraining. Here data is chosen on relevance for a specific target. A submodular\nfunction based on likelihood ratios is used to determine how acoustically\nsimilar each training utterance is to a target test set. The approach is\nevaluated on a wide-domain data set, covering speech from radio and TV\nbroadcasts, telephone conversations, meetings, lectures and read speech.\nExperiments demonstrate that the proposed technique both finds relevant data\nand limits negative transfer. Results on a 6--hour test set show a relative\nimprovement of 4% with data selection over using all data in PLP based models,\nand 2% with DNN features.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 15:20:12 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Doulaty", "Mortaza", ""], ["Saz", "Oscar", ""], ["Hain", "Thomas", ""]]}, {"id": "1509.02412", "submitter": "Mortaza Doulaty", "authors": "Mortaza Doulaty, Oscar Saz, Thomas Hain", "title": "Unsupervised Domain Discovery using Latent Dirichlet Allocation for\n  Acoustic Modelling in Speech Recognition", "comments": null, "journal-ref": "16th Interspeech.Proc. (2015) 3640-3644, Dresden, Germany", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech recognition systems are often highly domain dependent, a fact widely\nreported in the literature. However the concept of domain is complex and not\nbound to clear criteria. Hence it is often not evident if data should be\nconsidered to be out-of-domain. While both acoustic and language models can be\ndomain specific, work in this paper concentrates on acoustic modelling. We\npresent a novel method to perform unsupervised discovery of domains using\nLatent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is\nassumed to exist in the data, whereby each audio segment can be considered to\nbe a weighted mixture of domain properties. The classification of audio\nsegments into domains allows the creation of domain specific acoustic models\nfor automatic speech recognition. Experiments are conducted on a dataset of\ndiverse speech data covering speech from radio and TV broadcasts, telephone\nconversations, meetings, lectures and read speech, with a joint training set of\n60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to\nLDA based domains was shown to yield relative Word Error Rate (WER)\nimprovements of up to 16% relative, compared to pooled training, and up to 10%,\ncompared with models adapted with human-labelled prior domain knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 15:29:23 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Doulaty", "Mortaza", ""], ["Saz", "Oscar", ""], ["Hain", "Thomas", ""]]}, {"id": "1509.02437", "submitter": "Rishabh Soni", "authors": "Rishabh Soni, K. James Mathai", "title": "Improved Twitter Sentiment Prediction through Cluster-then-Predict Model", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade humans have experienced exponential growth in the use of\nonline resources, in particular social media and microblogging websites such as\nFacebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line,\netc. Many companies have identified these resources as a rich mine of marketing\nknowledge. This knowledge provides valuable feedback which allows them to\nfurther develop the next generation of their product. In this paper, sentiment\nanalysis of a product is performed by extracting tweets about that product and\nclassifying the tweets showing it as positive and negative sentiment. The\nauthors propose a hybrid approach which combines unsupervised learning in the\nform of K-means clustering to cluster the tweets and then performing supervised\nlearning methods such as Decision Trees and Support Vector Machines for\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 16:36:04 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Soni", "Rishabh", ""], ["Mathai", "K. James", ""]]}, {"id": "1509.03208", "submitter": "AbdelRahim Elmadany", "authors": "Abdelrahim A Elmadany, Sherif M Abdou and Mervat Gheith", "title": "Towards Understanding Egyptian Arabic Dialogues", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.03081", "journal-ref": "International Journal of Computer Applications 120(220, PP 7-12,\n  June 2015", "doi": "10.5120/21390-4427", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labelling of user's utterances to understanding his attends which called\nDialogue Act (DA) classification, it is considered the key player for dialogue\nlanguage understanding layer in automatic dialogue systems. In this paper, we\nproposed a novel approach to user's utterances labeling for Egyptian\nspontaneous dialogues and Instant Messages using Machine Learning (ML) approach\nwithout relying on any special lexicons, cues, or rules. Due to the lack of\nEgyptian dialect dialogue corpus, the system evaluated by multi-genre corpus\nincludes 4725 utterances for three domains, which are collected and annotated\nmanually from Egyptian call-centers. The system achieves F1 scores of 70. 36%\noverall domains.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 02:47:40 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Elmadany", "Abdelrahim A", ""], ["Abdou", "Sherif M", ""], ["Gheith", "Mervat", ""]]}, {"id": "1509.03295", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho and Carlos G\\'omez-Rodr\\'iguez", "title": "Liberating language research from dogmas of the 20th century", "comments": "Minor corrections", "journal-ref": "Liberating language research from dogmas of the 20th century.\n  Glottometrics 33, 33-34 (2016)", "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commentary on the article \"Large-scale evidence of dependency length\nminimization in 37 languages\" by Futrell, Mahowald & Gibson (PNAS 2015 112 (33)\n10336-10341).\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 11:27:03 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 17:44:43 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2016 07:58:19 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "1509.03488", "submitter": "Judith Eckle-Kohler", "authors": "Judith Eckle-Kohler", "title": "Verbs Taking Clausal and Non-Finite Arguments as Signals of Modality -\n  Revisiting the Issue of Meaning Grounded in Syntax", "comments": "Proceedings of the Association for Computational Linguistics (ACL)\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit Levin's theory about the correspondence of verb meaning and syntax\nand infer semantic classes from a large syntactic classification of more than\n600 German verbs taking clausal and non-finite arguments. Grasping the meaning\ncomponents of Levin-classes is known to be hard. We address this challenge by\nsetting up a multi-perspective semantic characterization of the inferred\nclasses. To this end, we link the inferred classes and their English\ntranslation to independently constructed semantic classes in three different\nlexicons - the German wordnet GermaNet, VerbNet and FrameNet - and perform a\ndetailed analysis and evaluation of the resulting German-English classification\n(available at www.ukp.tu-darmstadt.de/modality-verbclasses/).\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 13:05:15 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 10:37:48 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Eckle-Kohler", "Judith", ""]]}, {"id": "1509.03611", "submitter": "Ella Rabinovich", "authors": "Ella Rabinovich, Shuly Wintner, Ofek Luis Lewinsohn", "title": "A Parallel Corpus of Translationese", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a set of bilingual English--French and English--German parallel\ncorpora in which the direction of translation is accurately and reliably\nannotated. The corpora are diverse, consisting of parliamentary proceedings,\nliterary works, transcriptions of TED talks and political commentary. They will\nbe instrumental for research of translationese and its applications to (human\nand machine) translation; specifically, they can be used for the task of\ntranslationese identification, a research direction that enjoys a growing\ninterest in recent years. To validate the quality and reliability of the\ncorpora, we replicated previous results of supervised and unsupervised\nidentification of translationese, and further extended the experiments to\nadditional datasets and languages.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 19:07:49 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 13:41:11 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Rabinovich", "Ella", ""], ["Wintner", "Shuly", ""], ["Lewinsohn", "Ofek Luis", ""]]}, {"id": "1509.03739", "submitter": "Roland Roller", "authors": "Roland Roller, Eneko Agirre, Aitor Soroa, Mark Stevenson", "title": "Improving distant supervision using inference learning", "comments": "In Proceedings of the 53rd Annual Meeting of the Association for\n  Computational Linguistics and the 7th International Joint Conference on\n  Natural Language Processing (Volume 2: Short Papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision is a widely applied approach to automatic training of\nrelation extraction systems and has the advantage that it can generate large\namounts of labelled data with minimal effort. However, this data may contain\nerrors and consequently systems trained using distant supervision tend not to\nperform as well as those based on manually labelled data. This work proposes a\nnovel method for detecting potential false negative training examples using a\nknowledge inference method. Results show that our approach improves the\nperformance of relation extraction systems trained using distantly supervised\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 12:59:05 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Roller", "Roland", ""], ["Agirre", "Eneko", ""], ["Soroa", "Aitor", ""], ["Stevenson", "Mark", ""]]}, {"id": "1509.03870", "submitter": "Mortaza Doulaty", "authors": "Raymond W. M. Ng, Mortaza Doulaty, Rama Doddipatla, Wilker Aziz,\n  Kashif Shah, Oscar Saz, Madina Hasan, Ghada AlHarbi, Lucia Specia, Thomas\n  Hain", "title": "The USFD Spoken Language Translation System for IWSLT 2014", "comments": null, "journal-ref": "Proc. of 11th International Workshop on Spoken Language\n  Translation (SLT 2014) 86-91, Lake Tahoe, USA, December 4th and 5th, 2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The University of Sheffield (USFD) participated in the International Workshop\nfor Spoken Language Translation (IWSLT) in 2014. In this paper, we will\nintroduce the USFD SLT system for IWSLT. Automatic speech recognition (ASR) is\nachieved by two multi-pass deep neural network systems with adaptation and\nrescoring techniques. Machine translation (MT) is achieved by a phrase-based\nsystem. The USFD primary system incorporates state-of-the-art ASR and MT\ntechniques and gives a BLEU score of 23.45 and 14.75 on the English-to-French\nand English-to-German speech-to-text translation task with the IWSLT 2014 data.\nThe USFD contrastive systems explore the integration of ASR and MT by using a\nquality estimation system to rescore the ASR outputs, optimising towards better\ntranslation. This gives a further 0.54 and 0.26 BLEU improvement respectively\non the IWSLT 2012 and 2014 evaluation data.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 16:58:41 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Ng", "Raymond W. M.", ""], ["Doulaty", "Mortaza", ""], ["Doddipatla", "Rama", ""], ["Aziz", "Wilker", ""], ["Shah", "Kashif", ""], ["Saz", "Oscar", ""], ["Hasan", "Madina", ""], ["AlHarbi", "Ghada", ""], ["Specia", "Lucia", ""], ["Hain", "Thomas", ""]]}, {"id": "1509.04219", "submitter": "Afroze Ibrahim Baqapuri", "authors": "Afroze Ibrahim Baqapuri", "title": "Twitter Sentiment Analysis", "comments": "Bachelors Thesis Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project addresses the problem of sentiment analysis in twitter; that is\nclassifying tweets according to the sentiment expressed in them: positive,\nnegative or neutral. Twitter is an online micro-blogging and social-networking\nplatform which allows users to write short status updates of maximum length 140\ncharacters. It is a rapidly expanding service with over 200 million registered\nusers - out of which 100 million are active users and half of them log on\ntwitter on a daily basis - generating nearly 250 million tweets per day. Due to\nthis large amount of usage we hope to achieve a reflection of public sentiment\nby analysing the sentiments expressed in the tweets. Analysing the public\nsentiment is important for many applications such as firms trying to find out\nthe response of their products in the market, predicting political elections\nand predicting socioeconomic phenomena like stock exchange. The aim of this\nproject is to develop a functional classifier for accurate and automatic\nsentiment classification of an unknown tweet stream.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 17:39:37 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Baqapuri", "Afroze Ibrahim", ""]]}, {"id": "1509.04385", "submitter": "Amarappa S", "authors": "S. Amarappa and S. V. Sathyanarayana", "title": "Kannada named entity recognition and classification (nerc) based on\n  multinomial na\\\"ive bayes (mnb) classifier", "comments": "14 pages, 3 figures, International Journal on Natural Language\n  Computing (IJNLC) Vol. 4, No.4, August 2015", "journal-ref": null, "doi": "10.5121/ijnlc.2015.4404", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition and Classification (NERC) is a process of\nidentification of proper nouns in the text and classification of those nouns\ninto certain predefined categories like person name, location, organization,\ndate, and time etc. NERC in Kannada is an essential and challenging task. The\naim of this work is to develop a novel model for NERC, based on Multinomial\nNa\\\"ive Bayes (MNB) Classifier. The Methodology adopted in this paper is based\non feature extraction of training corpus, by using term frequency, inverse\ndocument frequency and fitting them to a tf-idf-vectorizer. The paper discusses\nthe various issues in developing the proposed model. The details of\nimplementation and performance evaluation are discussed. The experiments are\nconducted on a training corpus of size 95,170 tokens and test corpus of 5,000\ntokens. It is observed that the model works with Precision, Recall and\nF1-measure of 83%, 79% and 81% respectively.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 06:07:16 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Amarappa", "S.", ""], ["Sathyanarayana", "S. V.", ""]]}, {"id": "1509.04393", "submitter": "Haitao Liu", "authors": "Haitao Liu, Chunshan Xu and Junying Liang", "title": "Dependency length minimization: Puzzles and Promises", "comments": null, "journal-ref": "Glottometrics 2016, 33: 35 - 38", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent issue of PNAS, Futrell et al. claims that their study of 37\nlanguages gives the first large scale cross-language evidence for Dependency\nLength Minimization, which is an overstatement that ignores similar previous\nresearches. In addition,this study seems to pay no attention to factors like\nthe uniformity of genres,which weakens the validity of the argument that DLM is\nuniversal. Another problem is that this study sets the baseline random language\nas projective, which fails to truly uncover the difference between natural\nlanguage and random language, since projectivity is an important feature of\nmany natural languages. Finally, the paper contends an \"apparent relationship\nbetween head finality and dependency length\" despite the lack of an explicit\nstatistical comparison, which renders this conclusion rather hasty and\nimproper.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 04:29:50 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Liu", "Haitao", ""], ["Xu", "Chunshan", ""], ["Liang", "Junying", ""]]}, {"id": "1509.04473", "submitter": "Joachim Daiber", "authors": "Joachim Daiber, Lautaro Quiroz, Roger Wechsler, Stella Frank", "title": "Splitting Compounds by Semantic Analogy", "comments": null, "journal-ref": "Proceedings of the 1st Deep Machine Translation Workshop. Prague,\n  Czech Republic. 2015", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compounding is a highly productive word-formation process in some languages\nthat is often problematic for natural language processing applications. In this\npaper, we investigate whether distributional semantics in the form of word\nembeddings can enable a deeper, i.e., more knowledge-rich, processing of\ncompounds than the standard string-based methods. We present an unsupervised\napproach that exploits regularities in the semantic vector space (based on\nanalogies such as \"bookshop is to shop as bookshelf is to shelf\") to produce\ncompound analyses of high quality. A subsequent compound splitting algorithm\nbased on these analyses is highly effective, particularly for ambiguous\ncompounds. German to English machine translation experiments show that this\nsemantic analogy-based compound splitter leads to better translations than a\ncommonly used frequency-based method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 10:03:35 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Daiber", "Joachim", ""], ["Quiroz", "Lautaro", ""], ["Wechsler", "Roger", ""], ["Frank", "Stella", ""]]}, {"id": "1509.04556", "submitter": "Liang Liu", "authors": "Liang Liu and Lili Yu", "title": "On the evolution of word usage of classical Chinese poetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The hierarchy of classical Chinese poetry has been broadly acknowledged by a\nnumber of studies in Chinese literature. However, quantitative investigations\nabout the evolutionary linkages of classical Chinese poetry are limited. The\nprimary goal of this study is to provide quantitative evidence of the\nevolutionary linkages, with emphasis on character usage, among different period\ngenres of classical Chinese poetry. Specifically, various statistical analyses\nare performed to find and compare the patterns of character usage in the poems\nof nine period genres, including shi jing, chu ci, Han shi , Jin shi, Tang shi,\nSong shi, Yuan shi, Ming shi, and Qing shi. The result of analysis indicates\nthat each of nine period genres has unique patterns of character usage, with\nsome Chinese characters that are preferably used in the poems of a particular\nperiod genre. The analysis on the general pattern of character preference\nimplies a decreasing trend in the use of Chinese characters that rarely occur\nin modern Chinese literature along the timeline of dynastic types of classical\nChinese poetry. The phylogenetic analysis based on the distance matrix suggests\nthat the evolutionary linkages of different types of classical Chinese poetry\nare congruent with their chronological order, suggesting that character\nfrequencies contain phylogenetic information that is useful for inferring\nevolutionary linkages among various types of classical Chinese poetry. The\nestimated phylogenetic tree identifies four groups (shi jing, chu ci), (Han\nshi, Jin shi), (Tang shi, Song shi, Yuan shi), and (Ming shi, Qing shi). The\nstatistical analyses conducted in this study can be generalized to analyze the\ndata sets of general Chinese literature. Such analyses can provide quantitative\ninsights about the evolutionary linkages of general Chinese literature.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 18:08:13 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 00:40:16 GMT"}, {"version": "v3", "created": "Sun, 19 Jan 2020 14:58:54 GMT"}, {"version": "v4", "created": "Wed, 22 Jan 2020 03:32:39 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Liu", "Liang", ""], ["Yu", "Lili", ""]]}, {"id": "1509.04811", "submitter": "Tadele Damessie T", "authors": "Tadele Tedla", "title": "amLite: Amharic Transliteration Using Key Map Dictionary", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  amLite is a framework developed to map ASCII transliterated Amharic texts\nback to the original Amharic letter texts. The aim of such a framework is to\nmake existing Amharic linguistic data consistent and interoperable among\nresearchers. For achieving the objective, a key map dictionary is constructed\nusing the possible ASCII combinations actively in use for transliterating\nAmharic letters; and a mapping of the combinations to the corresponding Amharic\nletters is done. The mapping is then used to replace the Amharic linguistic\ntext back to form the original Amharic letters text. The framework indicated\n97.7, 99.7 and 98.4 percentage accuracy on converting the three sample random\ntest data. It is; however, possible to improve the accuracy of the framework by\nadding an exception to the implementation of the algorithm, or by preprocessing\nthe input text prior to conversion. This paper outlined the rationales behind\nthe need for developing the framework and the processes undertaken in the\ndevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 05:00:59 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Tedla", "Tadele", ""]]}, {"id": "1509.05209", "submitter": "Anthony Hunter", "authors": "Antonio Trenta and Anthony Hunter and Sebastian Riedel", "title": "Extraction of evidence tables from abstracts of randomized clinical\n  trials using a maximum entropy classifier and global constraints", "comments": "27 pages, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic use of the published results of randomized clinical trials is\nincreasingly important in evidence-based medicine. In order to collate and\nanalyze the results from potentially numerous trials, evidence tables are used\nto represent trials concerning a set of interventions of interest. An evidence\ntable has columns for the patient group, for each of the interventions being\ncompared, for the criterion for the comparison (e.g. proportion who survived\nafter 5 years from treatment), and for each of the results. Currently, it is a\nlabour-intensive activity to read each published paper and extract the\ninformation for each field in an evidence table. There have been some NLP\nstudies investigating how some of the features from papers can be extracted, or\nat least the relevant sentences identified. However, there is a lack of an NLP\nsystem for the systematic extraction of each item of information required for\nan evidence table. We address this need by a combination of a maximum entropy\nclassifier, and integer linear programming. We use the later to handle\nconstraints on what is an acceptable classification of the features to be\nextracted. With experimental results, we demonstrate substantial advantages in\nusing global constraints (such as the features describing the patient group,\nand the interventions, must occur before the features describing the results of\nthe comparison).\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 11:20:35 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Trenta", "Antonio", ""], ["Hunter", "Anthony", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1509.05281", "submitter": "Diego Amancio Dr.", "authors": "Diego R. Amancio", "title": "Network analysis of named entity co-occurrences in written texts", "comments": null, "journal-ref": "EPL 114 (2016) 58005", "doi": "10.1209/0295-5075/114/58005", "report-no": null, "categories": "cs.CL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of methods borrowed from statistics and physics to analyze written\ntexts has allowed the discovery of unprecedent patterns of human behavior and\ncognition by establishing links between models features and language structure.\nWhile current models have been useful to unveil patterns via analysis of\nsyntactical and semantical networks, only a few works have probed the relevance\nof investigating the structure arising from the relationship between relevant\nentities such as characters, locations and organizations. In this study, we\nrepresent entities appearing in the same context as a co-occurrence network,\nwhere links are established according to a null model based on random, shuffled\ntexts. Computational simulations performed in novels revealed that the proposed\nmodel displays interesting topological features, such as the small world\nfeature, characterized by high values of clustering coefficient. The\neffectiveness of our model was verified in a practical pattern recognition task\nin real networks. When compared with traditional word adjacency networks, our\nmodel displayed optimized results in identifying unknown references in texts.\nBecause the proposed representation plays a complementary role in\ncharacterizing unstructured documents via topological analysis of named\nentities, we believe that it could be useful to improve the characterization of\nwritten texts (and related systems), specially if combined with traditional\napproaches based on statistical and deeper paradigms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 15:08:36 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 21:43:20 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Amancio", "Diego R.", ""]]}, {"id": "1509.05488", "submitter": "Han Xiao Bookman", "authors": "Han Xiao, Minlie Huang, Yu Hao, Xiaoyan Zhu", "title": "TransG : A Generative Mixture Model for Knowledge Graph Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, knowledge graph embedding, which projects symbolic entities and\nrelations into continuous vector space, has become a new, hot topic in\nartificial intelligence. This paper addresses a new issue of multiple relation\nsemantics that a relation may have multiple meanings revealed by the entity\npairs associated with the corresponding triples, and proposes a novel Gaussian\nmixture model for embedding, TransG. The new model can discover latent\nsemantics for a relation and leverage a mixture of relation component vectors\nfor embedding a fact triple. To the best of our knowledge, this is the first\ngenerative model for knowledge graph embedding, which is able to deal with\nmultiple relation semantics. Extensive experiments show that the proposed model\nachieves substantial improvements against the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 02:30:17 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 02:17:33 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2015 01:32:17 GMT"}, {"version": "v4", "created": "Sun, 27 Dec 2015 05:46:51 GMT"}, {"version": "v5", "created": "Tue, 13 Jun 2017 06:03:20 GMT"}, {"version": "v6", "created": "Sat, 17 Jun 2017 03:54:46 GMT"}, {"version": "v7", "created": "Fri, 8 Sep 2017 12:55:14 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Xiao", "Han", ""], ["Huang", "Minlie", ""], ["Hao", "Yu", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1509.05490", "submitter": "Han Xiao Bookman", "authors": "Han Xiao, Minlie Huang, Yu Hao, Xiaoyan Zhu", "title": "TransA: An Adaptive Approach for Knowledge Graph Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation is a major topic in AI, and many studies attempt to\nrepresent entities and relations of knowledge base in a continuous vector\nspace. Among these attempts, translation-based methods build entity and\nrelation vectors by minimizing the translation loss from a head entity to a\ntail one. In spite of the success of these methods, translation-based methods\nalso suffer from the oversimplified loss metric, and are not competitive enough\nto model various and complex entities/relations in knowledge bases. To address\nthis issue, we propose \\textbf{TransA}, an adaptive metric approach for\nembedding, utilizing the metric learning ideas to provide a more flexible\nembedding method. Experiments are conducted on the benchmark datasets and our\nproposed method makes significant and consistent improvements over the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 02:40:07 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 02:21:20 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Xiao", "Han", ""], ["Huang", "Minlie", ""], ["Hao", "Yu", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1509.05517", "submitter": "Mikel Forcada Dr.", "authors": "Gang Chen and Mikel L. Forcada", "title": "A Light Sliding-Window Part-of-Speech Tagger for the Apertium\n  Free/Open-Source Machine Translation Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes a free/open-source implementation of the light\nsliding-window (LSW) part-of-speech tagger for the Apertium free/open-source\nmachine translation platform. Firstly, the mechanism and training process of\nthe tagger are reviewed, and a new method for incorporating linguistic rules is\nproposed. Secondly, experiments are conducted to compare the performances of\nthe tagger under different window settings, with or without Apertium-style\n\"forbid\" rules, with or without Constraint Grammar, and also with respect to\nthe traditional HMM tagger in Apertium.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 06:56:38 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Chen", "Gang", ""], ["Forcada", "Mikel L.", ""]]}, {"id": "1509.05736", "submitter": "Issa Atoum", "authors": "Issa Atoum, Chih How Bong, Narayanan Kulathuramaiyer", "title": "Building a Pilot Software Quality-in-Use Benchmark Dataset", "comments": "6 pages,3 figures, conference Proceedings of 9th International\n  Conference on IT in Asia CITA (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prepared domain specific datasets plays an important role to supervised\nlearning approaches. In this article a new sentence dataset for software\nquality-in-use is proposed. Three experts were chosen to annotate the data\nusing a proposed annotation scheme. Then the data were reconciled in a (no\nmatch eliminate) process to reduce bias. The Kappa, k statistics revealed an\nacceptable level of agreement; moderate to substantial agreement between the\nexperts. The built data can be used to evaluate software quality-in-use models\nin sentiment analysis models. Moreover, the annotation scheme can be used to\nextend the current dataset.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 18:19:48 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Atoum", "Issa", ""], ["Bong", "Chih How", ""], ["Kulathuramaiyer", "Narayanan", ""]]}, {"id": "1509.05808", "submitter": "Tatsunori Hashimoto", "authors": "Tatsunori B. Hashimoto, David Alvarez-Melis, Tommi S. Jaakkola", "title": "Word, graph and manifold embedding from Markov processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous vector representations of words and objects appear to carry\nsurprisingly rich semantic content. In this paper, we advance both the\nconceptual and theoretical understanding of word embeddings in three ways.\nFirst, we ground embeddings in semantic spaces studied in\ncognitive-psychometric literature and introduce new evaluation tasks. Second,\nin contrast to prior work, we take metric recovery as the key object of study,\nunify existing algorithms as consistent metric recovery methods based on\nco-occurrence counts from simple Markov random walks, and propose a new\nrecovery algorithm. Third, we generalize metric recovery to graphs and\nmanifolds, relating co-occurence counts on random walks in graphs and random\nprocesses on manifolds to the underlying metric to be recovered, thereby\nreconciling manifold estimation and embedding algorithms. We compare embedding\nalgorithms across a range of tasks, from nonlinear dimensionality reduction to\nthree semantic language tasks, including analogies, sequence completion, and\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 21:50:38 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Alvarez-Melis", "David", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1509.06053", "submitter": "Hugo Jair  Escalante", "authors": "Hugo Jair Escalante and Manuel Montes-y-G\\'omez, and Luis\n  Villase\\~nor-Pineda, and Marcelo Luis Errecalde", "title": "Early text classification: a Naive solution", "comments": "8 pages, preprint submitted to SDM'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is a widely studied problem, and it can be considered\nsolved for some domains and under certain circumstances. There are scenarios,\nhowever, that have received little or no attention at all, despite its\nrelevance and applicability. One of such scenarios is early text\nclassification, where one needs to know the category of a document by using\npartial information only. A document is processed as a sequence of terms, and\nthe goal is to devise a method that can make predictions as fast as possible.\nThe importance of this variant of the text classification problem is evident in\ndomains like sexual predator detection, where one wants to identify an offender\nas early as possible. This paper analyzes the suitability of the standard naive\nBayes classifier for approaching this problem. Specifically, we assess its\nperformance when classifying documents after seeing an increasingly number of\nterms. A simple modification to the standard naive Bayes implementation allows\nus to make predictions with partial information. To the best of our knowledge\nnaive Bayes has not been used for this purpose before. Throughout an extensive\nexperimental evaluation we show the effectiveness of the classifier for early\ntext classification. What is more, we show that this simple solution is very\ncompetitive when compared with state of the art methodologies that are more\nelaborated. We foresee our work will pave the way for the development of more\neffective early text classification techniques based in the naive Bayes\nformulation.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 21:01:51 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Escalante", "Hugo Jair", ""], ["Montes-y-G\u00f3mez", "Manuel", ""], ["Villase\u00f1or-Pineda", "Luis", ""], ["Errecalde", "Marcelo Luis", ""]]}, {"id": "1509.06103", "submitter": "Xiaofei Wang", "authors": "Xiaofei Wang, Chao Wu, Pengyuan Zhang, Ziteng Wang, Yong Liu, Xu Li,\n  Qiang Fu, Yonghong Yan", "title": "Noise Robust IOA/CAS Speech Separation and Recognition System For The\n  Third 'CHIME' Challenge", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the contribution to the third 'CHiME' speech separation\nand recognition challenge including both front-end signal processing and\nback-end speech recognition. In the front-end, Multi-channel Wiener filter\n(MWF) is designed to achieve background noise reduction. Different from\ntraditional MWF, optimized parameter for the tradeoff between noise reduction\nand target signal distortion is built according to the desired noise reduction\nlevel. In the back-end, several techniques are taken advantage to improve the\nnoisy Automatic Speech Recognition (ASR) performance including Deep Neural\nNetwork (DNN), Convolutional Neural Network (CNN) and Long short-term memory\n(LSTM) using medium vocabulary, Lattice rescoring with a big vocabulary\nlanguage model finite state transducer, and ROVER scheme. Experimental results\nshow the proposed system combining front-end and back-end is effective to\nimprove the ASR performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 03:37:11 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Wang", "Xiaofei", ""], ["Wu", "Chao", ""], ["Zhang", "Pengyuan", ""], ["Wang", "Ziteng", ""], ["Liu", "Yong", ""], ["Li", "Xu", ""], ["Fu", "Qiang", ""], ["Yan", "Yonghong", ""]]}, {"id": "1509.06585", "submitter": "Vincent Labatut", "authors": "Jean-Val\\`ere Cossu (LIA), Vincent Labatut (LIA), Nicolas Dugu\\'e (UO)", "title": "A Review of Features for the Discrimination of Twitter Users:\n  Application to the Prediction of Offline Influence", "comments": null, "journal-ref": "Social Network Analysis and Mining, Springer, 2016, 6 (1), pp.25", "doi": "10.1007/s13278-016-0329-x", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many works related to Twitter aim at characterizing its users in some way:\nrole on the service (spammers, bots, organizations, etc.), nature of the user\n(socio-professional category, age, etc.), topics of interest , and others.\nHowever, for a given user classification problem, it is very difficult to\nselect a set of appropriate features, because the many features described in\nthe literature are very heterogeneous, with name overlaps and collisions, and\nnumerous very close variants. In this article, we review a wide range of such\nfeatures. In order to present a clear state-of-the-art description, we unify\ntheir names, definitions and relationships, and we propose a new, neutral,\ntypology. We then illustrate the interest of our review by applying a selection\nof these features to the offline influence detection problem. This task\nconsists in identifying users which are influential in real-life, based on\ntheir Twitter account and related data. We show that most features deemed\nefficient to predict online influence, such as the numbers of retweets and\nfollowers, are not relevant to this problem. However, We propose several\ncontent-based approaches to label Twitter users as Influencers or not. We also\nrank them according to a predicted influence level. Our proposals are evaluated\nover the CLEF RepLab 2014 dataset, and outmatch state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 13:12:34 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 13:19:33 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 08:02:34 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Cossu", "Jean-Val\u00e8re", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Dugu\u00e9", "Nicolas", "", "UO"]]}, {"id": "1509.06594", "submitter": "Martha Lewis", "authors": "Bob Coecke and Martha Lewis", "title": "A Compositional Explanation of the Pet Fish Phenomenon", "comments": "QI2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The `pet fish' phenomenon is often cited as a paradigm example of the\n`non-compositionality' of human concept use. We show here how this phenomenon\nis naturally accommodated within a compositional distributional model of\nmeaning. This model describes the meaning of a composite concept by accounting\nfor interaction between its constituents via their grammatical roles. We give\ntwo illustrative examples to show how the qualitative phenomena are exhibited.\nWe go on to apply the model to experimental data, and finally discuss\nextensions of the formalism.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 13:33:34 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Coecke", "Bob", ""], ["Lewis", "Martha", ""]]}, {"id": "1509.06664", "submitter": "Tim Rockt\\\"aschel", "authors": "Tim Rockt\\\"aschel, Edward Grefenstette, Karl Moritz Hermann,\n  Tom\\'a\\v{s} Ko\\v{c}isk\\'y, Phil Blunsom", "title": "Reasoning about Entailment with Neural Attention", "comments": "ICLR 2016 camera-ready, 9 pages, 10 figures (incl. subfigures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most approaches to automatically recognizing entailment relations have\nused classifiers employing hand engineered features derived from complex\nnatural language processing pipelines, in practice their performance has been\nonly slightly better than bag-of-word pair classifiers using only lexical\nsimilarity. The only attempt so far to build an end-to-end differentiable\nneural network for entailment failed to outperform such a simple similarity\nclassifier. In this paper, we propose a neural model that reads two sentences\nto determine entailment using long short-term memory units. We extend this\nmodel with a word-by-word neural attention mechanism that encourages reasoning\nover entailments of pairs of words and phrases. Furthermore, we present a\nqualitative analysis of attention weights produced by this model, demonstrating\nsuch reasoning capabilities. On a large entailment dataset this model\noutperforms the previous best neural model and a classifier with engineered\nfeatures by a substantial margin. It is the first generic end-to-end\ndifferentiable system that achieves state-of-the-art accuracy on a textual\nentailment dataset.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 16:08:24 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 22:12:52 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2016 17:28:30 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 10:32:06 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Rockt\u00e4schel", "Tim", ""], ["Grefenstette", "Edward", ""], ["Hermann", "Karl Moritz", ""], ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Blunsom", "Phil", ""]]}, {"id": "1509.06928", "submitter": "Ahmed Ali", "authors": "Ahmed Ali, Najim Dehak, Patrick Cardinal, Sameer Khurana, Sree Harsha\n  Yella, James Glass, Peter Bell, Steve Renals", "title": "Automatic Dialect Detection in Arabic Broadcast Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We investigate different approaches for dialect identification in Arabic\nbroadcast speech, using phonetic, lexical features obtained from a speech\nrecognition system, and acoustic features using the i-vector framework. We\nstudied both generative and discriminate classifiers, and we combined these\nfeatures using a multi-class Support Vector Machine (SVM). We validated our\nresults on an Arabic/English language identification task, with an accuracy of\n100%. We used these features in a binary classifier to discriminate between\nModern Standard Arabic (MSA) and Dialectal Arabic, with an accuracy of 100%. We\nfurther report results using the proposed method to discriminate between the\nfive most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine,\nNorth African, and MSA, with an accuracy of 52%. We discuss dialect\nidentification errors in the context of dialect code-switching between\nDialectal Arabic and MSA, and compare the error pattern between manually\nlabeled data, and the output from our classifier. We also release the train and\ntest data as standard corpus for dialect identification.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 11:41:10 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 00:28:59 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Ali", "Ahmed", ""], ["Dehak", "Najim", ""], ["Cardinal", "Patrick", ""], ["Khurana", "Sameer", ""], ["Yella", "Sree Harsha", ""], ["Glass", "James", ""], ["Bell", "Peter", ""], ["Renals", "Steve", ""]]}, {"id": "1509.06937", "submitter": "Tobias Kuhn", "authors": "Kurt Winkler and Tobias Kuhn", "title": "Fully automatic multi-language translation with a catalogue of phrases -\n  successful employment for the Swiss avalanche bulletin", "comments": "Extended version of a previous workshop paper (arXiv:1405.6103),\n  accepted for the journal Language Resources and Evaluation, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Swiss avalanche bulletin is produced twice a day in four languages. Due\nto the lack of time available for manual translation, a fully automated\ntranslation system is employed, based on a catalogue of predefined phrases and\npredetermined rules of how these phrases can be combined to produce sentences.\nBecause this catalogue of phrases is limited to a small sublanguage, the system\nis able to automatically translate such sentences from German into the target\nlanguages French, Italian and English without subsequent proofreading or\ncorrection. Having been operational for two winter seasons, we assess here the\nquality of the produced texts based on two different surveys where participants\nrated texts from real avalanche bulletins from both origins, the catalogue of\nphrases versus manually written and translated texts. With a mean recognition\nrate of 55%, users can hardly distinguish between thetwo types of texts, and\ngive very similar ratings with respect to their language quality. Overall, the\noutput from the catalogue system can be considered virtually equivalent to a\ntext written by avalanche forecasters and then manually translated by\nprofessional translators. Furthermore, forecasters declared that all relevant\nsituations were captured by the system with sufficient accuracy. Forecaster's\nworking load did not change with the introduction of the catalogue: the extra\ntime to find matching sentences is compensated by the fact that they no longer\nneed to double-check manually translated texts. The reduction of daily\ntranslation costs is expected to offset the initial development costs within a\nfew years.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 12:09:07 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Winkler", "Kurt", ""], ["Kuhn", "Tobias", ""]]}, {"id": "1509.07175", "submitter": "Jaimie Murdock", "authors": "Jaimie Murdock and Colin Allen and Simon DeDeo", "title": "Exploration and Exploitation of Victorian Science in Darwin's Reading\n  Notebooks", "comments": "Cognition pre-print, published February 2017; 22 pages, plus 17 pages\n  supporting information, 7 pages references", "journal-ref": "Cognition 159 (2017) 117-126", "doi": "10.1016/j.cognition.2016.11.012", "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.DL physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Search in an environment with an uncertain distribution of resources involves\na trade-off between exploitation of past discoveries and further exploration.\nThis extends to information foraging, where a knowledge-seeker shifts between\nreading in depth and studying new domains. To study this decision-making\nprocess, we examine the reading choices made by one of the most celebrated\nscientists of the modern era: Charles Darwin. From the full-text of books\nlisted in his chronologically-organized reading journals, we generate topic\nmodels to quantify his local (text-to-text) and global (text-to-past) reading\ndecisions using Kullback-Liebler Divergence, a cognitively-validated,\ninformation-theoretic measure of relative surprise. Rather than a pattern of\nsurprise-minimization, corresponding to a pure exploitation strategy, Darwin's\nbehavior shifts from early exploitation to later exploration, seeking unusually\nhigh levels of cognitive surprise relative to previous eras. These shifts,\ndetected by an unsupervised Bayesian model, correlate with major intellectual\nepochs of his career as identified both by qualitative scholarship and Darwin's\nown self-commentary. Our methods allow us to compare his consumption of texts\nwith their publication order. We find Darwin's consumption more exploratory\nthan the culture's production, suggesting that underneath gradual societal\nchanges are the explorations of individual synthesis and discovery. Our\nquantitative methods advance the study of cognitive search through a framework\nfor testing interactions between individual and collective behavior and between\nshort- and long-term consumption choices. This novel application of topic\nmodeling to characterize individual reading complements widespread studies of\ncollective scientific behavior.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 22:41:46 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 03:43:07 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2016 21:47:35 GMT"}, {"version": "v4", "created": "Mon, 22 Aug 2016 21:52:56 GMT"}, {"version": "v5", "created": "Thu, 2 Feb 2017 15:51:17 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Murdock", "Jaimie", ""], ["Allen", "Colin", ""], ["DeDeo", "Simon", ""]]}, {"id": "1509.07179", "submitter": "Kai-Wei Chang", "authors": "Kai-Wei Chang and Shyam Upadhyay and Ming-Wei Chang and Vivek Srikumar\n  and Dan Roth", "title": "IllinoisSL: A JAVA Library for Structured Prediction", "comments": "http://cogcomp.cs.illinois.edu/software/illinois-sl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IllinoisSL is a Java library for learning structured prediction models. It\nsupports structured Support Vector Machines and structured Perceptron. The\nlibrary consists of a core learning module and several applications, which can\nbe executed from command-lines. Documentation is provided to guide users. In\nComparison to other structured learning libraries, IllinoisSL is efficient,\ngeneral, and easy to use.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 23:22:38 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Chang", "Kai-Wei", ""], ["Upadhyay", "Shyam", ""], ["Chang", "Ming-Wei", ""], ["Srikumar", "Vivek", ""], ["Roth", "Dan", ""]]}, {"id": "1509.07211", "submitter": "Fengyun Zhu", "authors": "Zaihu Pang, Fengyun Zhu", "title": "Noise-Robust ASR for the third 'CHiME' Challenge Exploiting\n  Time-Frequency Masking based Multi-Channel Speech Enhancement and Recurrent\n  Neural Network", "comments": "The 3rd 'CHiME' Speech Separation and Recognition Challenge, 5 pages,\n  1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the Lingban entry to the third 'CHiME' speech separation and\nrecognition challenge is presented. A time-frequency masking based speech\nenhancement front-end is proposed to suppress the environmental noise utilizing\nmulti-channel coherence and spatial cues. The state-of-the-art speech\nrecognition techniques, namely recurrent neural network based acoustic and\nlanguage modeling, state space minimum Bayes risk based discriminative acoustic\nmodeling, and i-vector based acoustic condition modeling, are carefully\nintegrated into the speech recognition back-end. To further improve the system\nperformance by fully exploiting the advantages of different technologies, the\nfinal recognition results are obtained by lattice combination and rescoring.\nEvaluations carried out on the official dataset prove the effectiveness of the\nproposed systems. Comparing with the best baseline result, the proposed system\nobtains consistent improvements with over 57% relative word error rate\nreduction on the real-data test set.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 02:16:11 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Pang", "Zaihu", ""], ["Zhu", "Fengyun", ""]]}, {"id": "1509.07308", "submitter": "Ivan Vuli\\'c", "authors": "Ivan Vuli\\'c and Marie-Francine Moens", "title": "Bilingual Distributed Word Representations from Document-Aligned\n  Comparable Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for learning bilingual word representations from\nnon-parallel document-aligned data. Following the recent advances in word\nrepresentation learning, our model learns dense real-valued word vectors, that\nis, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which\nheavily relied on parallel sentence-aligned corpora and/or readily available\ntranslation resources such as dictionaries, the article reveals that BWEs may\nbe learned solely on the basis of document-aligned comparable data without any\nadditional lexical resources nor syntactic information. We present a comparison\nof our approach with previous state-of-the-art models for learning bilingual\nword representations from comparable data that rely on the framework of\nmultilingual probabilistic topic modeling (MuPTM), as well as with\ndistributional local context-counting models. We demonstrate the utility of the\ninduced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2)\nsuggesting word translations in context for polysemous words. Our simple yet\neffective BWE-based models significantly outperform the MuPTM-based and\ncontext-counting representation models from comparable data as well as prior\nBWE-based models, and acquire the best reported results on both tasks for all\nthree tested language pairs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 11:00:04 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 12:47:15 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Vuli\u0107", "Ivan", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1509.07513", "submitter": "Marco Antonio Valenzuela Esc\\'arcega", "authors": "Marco A. Valenzuela-Esc\\'arcega, Gus Hahn-Powell, Mihai Surdeanu", "title": "Description of the Odin Event Extraction Framework and Rule Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the Odin framework, which is a domain-independent\nplatform for developing rule-based event extraction models. Odin aims to be\npowerful (the rule language allows the modeling of complex syntactic\nstructures) and robust (to recover from syntactic parsing errors, syntactic\npatterns can be freely mixed with surface, token-based patterns), while\nremaining simple (some domain grammars can be up and running in minutes), and\nfast (Odin processes over 100 sentences/second in a real-world domain with over\n200 rules). Here we include a thorough definition of the Odin rule language,\ntogether with a description of the Odin API in the Scala language, which allows\none to apply these rules to arbitrary texts.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 20:10:27 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Valenzuela-Esc\u00e1rcega", "Marco A.", ""], ["Hahn-Powell", "Gus", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "1509.07612", "submitter": "Nils Haldenwang", "authors": "Nils Haldenwang and Oliver Vornberger", "title": "Sentiment Uncertainty and Spam in Twitter Streams and Its Implications\n  for General Purpose Realtime Sentiment Analysis", "comments": "3 pages, 1 figure, accepted at GSCL '15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art benchmarks for Twitter Sentiment Analysis do not consider\nthe fact that for more than half of the tweets from the public stream a\ndistinct sentiment cannot be chosen. This paper provides a new perspective on\nTwitter Sentiment Analysis by highlighting the necessity of explicitly\nincorporating uncertainty. Moreover, a dataset of high quality to evaluate\nsolutions for this new problem is introduced and made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 07:55:26 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Haldenwang", "Nils", ""], ["Vornberger", "Oliver", ""]]}, {"id": "1509.07761", "submitter": "Igor Mozeti\\v{c}", "authors": "Petra Kralj Novak, Jasmina Smailovi\\'c, Borut Sluban, Igor Mozeti\\v{c}", "title": "Sentiment of Emojis", "comments": null, "journal-ref": "PLoS ONE 10(12): e0144296, 2015", "doi": "10.1371/journal.pone.0144296", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a new generation of emoticons, called emojis, that is increasingly\nbeing used in mobile communications and social media. In the past two years,\nover ten billion emojis were used on Twitter. Emojis are Unicode graphic\nsymbols, used as a shorthand to express concepts and ideas. In contrast to the\nsmall number of well-known emoticons that carry clear emotional contents, there\nare hundreds of emojis. But what are their emotional contents? We provide the\nfirst emoji sentiment lexicon, called the Emoji Sentiment Ranking, and draw a\nsentiment map of the 751 most frequently used emojis. The sentiment of the\nemojis is computed from the sentiment of the tweets in which they occur. We\nengaged 83 human annotators to label over 1.6 million tweets in 13 European\nlanguages by the sentiment polarity (negative, neutral, or positive). About 4%\nof the annotated tweets contain emojis. The sentiment analysis of the emojis\nallows us to draw several interesting conclusions. It turns out that most of\nthe emojis are positive, especially the most popular ones. The sentiment\ndistribution of the tweets with and without emojis is significantly different.\nThe inter-annotator agreement on the tweets with emojis is higher. Emojis tend\nto occur at the end of the tweets, and their sentiment polarity increases with\nthe distance. We observe no significant differences in the emoji rankings\nbetween the 13 languages and the Emoji Sentiment Ranking. Consequently, we\npropose our Emoji Sentiment Ranking as a European language-independent resource\nfor automated sentiment analysis. Finally, the paper provides a formalization\nof sentiment and a novel visualization in the form of a sentiment bar.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 15:41:13 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 10:02:47 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Novak", "Petra Kralj", ""], ["Smailovi\u0107", "Jasmina", ""], ["Sluban", "Borut", ""], ["Mozeti\u010d", "Igor", ""]]}, {"id": "1509.07845", "submitter": "Xintong Han", "authors": "Bharat Singh, Xintong Han, Zhe Wu, Vlad I. Morariu and Larry S. Davis", "title": "Selecting Relevant Web Trained Concepts for Automated Event Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex event retrieval is a challenging research problem, especially when no\ntraining videos are available. An alternative to collecting training videos is\nto train a large semantic concept bank a priori. Given a text description of an\nevent, event retrieval is performed by selecting concepts linguistically\nrelated to the event description and fusing the concept responses on unseen\nvideos. However, defining an exhaustive concept lexicon and pre-training it\nrequires vast computational resources. Therefore, recent approaches automate\nconcept discovery and training by leveraging large amounts of weakly annotated\nweb data. Compact visually salient concepts are automatically obtained by the\nuse of concept pairs or, more generally, n-grams. However, not all visually\nsalient n-grams are necessarily useful for an event query--some combinations of\nconcepts may be visually compact but irrelevant--and this drastically affects\nperformance. We propose an event retrieval algorithm that constructs pairs of\nautomatically discovered concepts and then prunes those concepts that are\nunlikely to be helpful for retrieval. Pruning depends both on the query and on\nthe specific video instance being evaluated. Our approach also addresses\ncalibration and domain adaptation issues that arise when applying concept\ndetectors to unseen videos. We demonstrate large improvements over other vision\nbased systems on the TRECVID MED 13 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 19:27:54 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Singh", "Bharat", ""], ["Han", "Xintong", ""], ["Wu", "Zhe", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1509.08639", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Tuned and GPU-accelerated parallel data mining from comparable corpora", "comments": "Machine translation, comparable corpora, Machine learning, NLP,\n  Knowledge-free learning, Unsupervised bi-lingual data mining", "journal-ref": "Lecture Notes in Artificial Intelligence, p. 32-40, ISBN:\n  978-3-319-24032-9, Springer, 2015", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multilingual nature of the world makes translation a crucial requirement\ntoday. Parallel dictionaries constructed by humans are a widely-available\nresource, but they are limited and do not provide enough coverage for good\nquality translation purposes, due to out-of-vocabulary words and neologisms.\nThis motivates the use of statistical translation systems, which are\nunfortunately dependent on the quantity and quality of training data. Such has\na very limited availability especially for some languages and very narrow text\ndomains. Is this research we present our improvements to Yalign mining\nmethodology by reimplementing the comparison algorithm, introducing a tuning\nscripts and by improving performance using GPU computing acceleration. The\nexperiments are conducted on various text domains and bi-data is extracted from\nthe Wikipedia dumps.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 08:44:14 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.08644", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Neural-based machine translation for medical text domain. Based on\n  European Medicines Agency leaflet texts", "comments": "machine translation, statistical machine translation, neural machine\n  trasnlation, nlp, text processing, medical communication", "journal-ref": "Procedia Computer Science, 2015, 64: 2-9", "doi": "10.1016/j.procs.2015.08.456", "report-no": null, "categories": "cs.CL cs.CY cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of machine translation is rapidly evolving. Today one can find\nseveral machine translation systems on the web that provide reasonable\ntranslations, although the systems are not perfect. In some specific domains,\nthe quality may decrease. A recently proposed approach to this domain is neural\nmachine translation. It aims at building a jointly-tuned single neural network\nthat maximizes translation performance, a very different approach from\ntraditional statistical machine translation. Recently proposed neural machine\ntranslation models often belong to the encoder-decoder family in which a source\nsentence is encoded into a fixed length vector that is, in turn, decoded to\ngenerate a translation. The present research examines the effects of different\ntraining methods on a Polish-English Machine Translation system used for\nmedical data. The European Medicines Agency parallel text corpus was used as\nthe basis for training of neural and statistical network-based translation\nsystems. The main machine translation evaluation metrics have also been used in\nanalysis of the systems. A comparison and implementation of a real-time medical\ntranslator is the main focus of our experiments.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 08:54:48 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.08842", "submitter": "Ryan Shaw", "authors": "Ryan Shaw", "title": "Automatically Segmenting Oral History Transcripts", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dividing oral histories into topically coherent segments can make them more\naccessible online. People regularly make judgments about where coherent\nsegments can be extracted from oral histories. But making these judgments can\nbe taxing, so automated assistance is potentially attractive to speed the task\nof extracting segments from open-ended interviews. When different people are\nasked to extract coherent segments from the same oral histories, they often do\nnot agree about precisely where such segments begin and end. This low agreement\nmakes the evaluation of algorithmic segmenters challenging, but there is reason\nto believe that for segmenting oral history transcripts, some approaches are\nmore promising than others. The BayesSeg algorithm performs slightly better\nthan TextTiling, while TextTiling does not perform significantly better than a\nuniform segmentation. BayesSeg might be used to suggest boundaries to someone\nsegmenting oral histories, but this segmentation task needs to be better\ndefined.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 16:46:52 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Shaw", "Ryan", ""]]}, {"id": "1509.08874", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Polish - English Speech Statistical Machine Translation Systems for the\n  IWSLT 2014", "comments": "Machine Translation, West slavic, Proceedings of the 11th\n  International Workshop on Spoken Language Translation, Tahoe Lake, USA, 2014.\n  arXiv admin note: text overlap with arXiv:1409.0473 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research explores effects of various training settings between Polish\nand English Statistical Machine Translation systems for spoken language.\nVarious elements of the TED parallel text corpora for the IWSLT 2014 evaluation\ncampaign were used as the basis for training of language models, and for\ndevelopment, tuning and testing of the translation system as well as Wikipedia\nbased comparable corpora prepared by us. The BLEU, NIST, METEOR and TER metrics\nwere used to evaluate the effects of data preparations on translation results.\nOur experiments included systems, which use lemma and morphological information\non Polish words. We also conducted a deep analysis of provided Polish data as\npreparatory work for the automatic data correction and cleaning phase.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 18:17:22 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.08881", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Building Subject-aligned Comparable Corpora and Mining it for Truly\n  Parallel Sentence Pairs", "comments": null, "journal-ref": "Procedia Technology, 18, Elsevier, p.126-132, 2014", "doi": "10.1016/j.protcy.2014.11.024", "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel sentences are a relatively scarce but extremely useful resource for\nmany applications including cross-lingual retrieval and statistical machine\ntranslation. This research explores our methodology for mining such data from\npreviously obtained comparable corpora. The task is highly practical since\nnon-parallel multilingual data exist in far greater quantities than parallel\ncorpora, but parallel sentences are a much more useful resource. Here we\npropose a web crawling method for building subject-aligned comparable corpora\nfrom Wikipedia articles. We also introduce a method for extracting truly\nparallel sentences that are filtered out from noisy or just comparable sentence\npairs. We describe our implementation of a specialized tool for this task as\nwell as training and adaption of a machine translation system that supplies our\nfilter with additional information about the similarity of comparable sentence\npairs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 18:35:49 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.08909", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Polish -English Statistical Machine Translation of Medical Texts", "comments": "New Research in Multimedia and Internet Systems, Springer. 09/2014,\n  ISSN: 1867-5662. arXiv admin note: text overlap with arXiv:1509.08874", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This new research explores the effects of various training methods on a\nPolish to English Statistical Machine Translation system for medical texts.\nVarious elements of the EMEA parallel text corpora from the OPUS project were\nused as the basis for training of phrase tables and language models and for\ndevelopment, tuning and testing of the translation system. The BLEU, NIST,\nMETEOR, RIBES and TER metrics have been used to evaluate the effects of various\nsystem and data preparations on translation results. Our experiments included\nsystems that used POS tagging, factored phrase models, hierarchical models,\nsyntactic taggers, and many different alignment methods. We also conducted a\ndeep analysis of Polish data as preparatory work for automatic data correction\nsuch as true casing and punctuation normalization phase.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 19:57:24 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.08967", "submitter": "Tom Sercu", "authors": "Tom Sercu, Christian Puhrsch, Brian Kingsbury, Yann LeCun", "title": "Very Deep Multilingual Convolutional Neural Networks for LVCSR", "comments": "Accepted for publication at ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are a standard component of many current\nstate-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR)\nsystems. However, CNNs in LVCSR have not kept pace with recent advances in\nother domains where deeper neural networks provide superior performance. In\nthis paper we propose a number of architectural advances in CNNs for LVCSR.\nFirst, we introduce a very deep convolutional network architecture with up to\n14 weight layers. There are multiple convolutional layers before each pooling\nlayer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture.\nThen, we introduce multilingual CNNs with multiple untied layers. Finally, we\nintroduce multi-scale input features aimed at exploiting more context at\nnegligible computational cost. We evaluate the improvements first on a Babel\ntask for low resource speech recognition, obtaining an absolute 5.77% WER\nimprovement over the baseline PLP DNN by training our CNN on the combined data\nof six different languages. We then evaluate the very deep CNNs on the Hub5'00\nbenchmark (using the 262 hours of SWB-1 training data) achieving a word error\nrate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6%\nrelative) over the best published CNN result so far.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 22:28:11 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2016 18:18:58 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Sercu", "Tom", ""], ["Puhrsch", "Christian", ""], ["Kingsbury", "Brian", ""], ["LeCun", "Yann", ""]]}, {"id": "1509.08973", "submitter": "Tadahiro Taniguchi", "authors": "Tadahiro Taniguchi, Takayuki Nagai, Tomoaki Nakamura, Naoto Iwahashi,\n  Tetsuya Ogata, and Hideki Asoh", "title": "Symbol Emergence in Robotics: A Survey", "comments": "submitted to Advanced Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can learn the use of language through physical interaction with their\nenvironment and semiotic communication with other people. It is very important\nto obtain a computational understanding of how humans can form a symbol system\nand obtain semiotic skills through their autonomous mental development.\nRecently, many studies have been conducted on the construction of robotic\nsystems and machine-learning methods that can learn the use of language through\nembodied multimodal interaction with their environment and other systems.\nUnderstanding human social interactions and developing a robot that can\nsmoothly communicate with human users in the long term, requires an\nunderstanding of the dynamics of symbol systems and is crucially important. The\nembodied cognition and social interaction of participants gradually change a\nsymbol system in a constructive manner. In this paper, we introduce a field of\nresearch called symbol emergence in robotics (SER). SER is a constructive\napproach towards an emergent symbol system. The emergent symbol system is\nsocially self-organized through both semiotic communications and physical\ninteractions with autonomous cognitive developmental agents, i.e., humans and\ndevelopmental robots. Specifically, we describe some state-of-art research\ntopics concerning SER, e.g., multimodal categorization, word discovery, and a\ndouble articulation analysis, that enable a robot to obtain words and their\nembodied meanings from raw sensory--motor information, including visual\ninformation, haptic information, auditory information, and acoustic speech\nsignals, in a totally unsupervised manner. Finally, we suggest future\ndirections of research in SER.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 23:16:48 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Taniguchi", "Tadahiro", ""], ["Nagai", "Takayuki", ""], ["Nakamura", "Tomoaki", ""], ["Iwahashi", "Naoto", ""], ["Ogata", "Tetsuya", ""], ["Asoh", "Hideki", ""]]}, {"id": "1509.09088", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Enhanced Bilingual Evaluation Understudy", "comments": "machine translation evaluation, enchanced bleu. in Lecture Notes on\n  Information Theory, ISSN: 2301-3788, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research extends the Bilingual Evaluation Understudy (BLEU) evaluation\ntechnique for statistical machine translation to make it more adjustable and\nrobust. We intend to adapt it to resemble human evaluation more. We perform\nexperiments to evaluate the performance of our technique against the primary\nexisting evaluation methods. We describe and show the improvements it makes\nover existing methods as well as correlation to them. When human translators\ntranslate a text, they often use synonyms, different word orders or style, and\nother similar variations. We propose an SMT evaluation technique that enhances\nthe BLEU metric to consider variations such as those.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:13:00 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.09090", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Real-Time Statistical Speech Translation", "comments": "machine translation, polish english", "journal-ref": "Advances in Intelligent Systems and Computing volume 275,\n  p.107-114, Publisher: Springer, ISSN 2194-5357, ISBN 978-3-319-05950-1, 2014", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research investigates the Statistical Machine Translation approaches to\ntranslate speech in real time automatically. Such systems can be used in a\npipeline with speech recognition and synthesis software in order to produce a\nreal-time voice communication system between foreigners. We obtained three main\ndata sets from spoken proceedings that represent three different types of human\nspeech. TED, Europarl, and OPUS parallel text corpora were used as the basis\nfor training of language models, for developmental tuning and testing of the\ntranslation system. We also conducted experiments involving part of speech\ntagging, compound splitting, linear language model interpolation, TrueCasing\nand morphosyntactic analysis. We evaluated the effects of variety of data\npreparations on the translation results using the BLEU, NIST, METEOR and TER\nmetrics and tried to give answer which metric is most suitable for PL-EN\nlanguage pair.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:20:27 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.09093", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "A Sentence Meaning Based Alignment Method for Parallel Text Corpora\n  Preparation", "comments": "corpora filtration, text alignement, corpora improvement. arXiv admin\n  note: text overlap with arXiv:1509.08881", "journal-ref": "Advances in Intelligent Systems and Computing volume 275,\n  p.107-114, Publisher: Springer, ISSN 2194-5357, ISBN 978-3-319-05950-1, 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text alignment is crucial to the accuracy of Machine Translation (MT)\nsystems, some NLP tools or any other text processing tasks requiring bilingual\ndata. This research proposes a language independent sentence alignment approach\nbased on Polish (not position-sensitive language) to English experiments. This\nalignment approach was developed on the TED Talks corpus, but can be used for\nany text domain or language pair. The proposed approach implements various\nheuristics for sentence recognition. Some of them value synonyms and semantic\ntext structure analysis as a part of additional information. Minimization of\ndata loss was ensured. The solution is compared to other sentence alignment\nimplementations. Also an improvement in MT system score with text processed\nwith described tool is shown.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:29:51 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.09097", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Polish - English Speech Statistical Machine Translation Systems for the\n  IWSLT 2013", "comments": "statistical machine translation. arXiv admin note: substantial text\n  overlap with arXiv:1509.08874, arXiv:1509.08909", "journal-ref": "Proceedings of the 10th International Workshop on Spoken Language\n  Translation, Heidelberg, Germany, p. 113-119, 2013", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research explores the effects of various training settings from Polish\nto English Statistical Machine Translation system for spoken language. Various\nelements of the TED parallel text corpora for the IWSLT 2013 evaluation\ncampaign were used as the basis for training of language models, and for\ndevelopment, tuning and testing of the translation system. The BLEU, NIST,\nMETEOR and TER metrics were used to evaluate the effects of data preparations\non translation results. Our experiments included systems, which use stems and\nmorphological information on Polish words. We also conducted a deep analysis of\nprovided Polish data as preparatory work for the automatic data correction and\ncleaning phase.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:35:38 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.09121", "submitter": "Sitabhra Sinha", "authors": "Md Izhar Ashraf and Sitabhra Sinha", "title": "The \"handedness\" of language: Directional symmetry breaking of sign\n  usage in words", "comments": "10 pages, 4 figures + Supplementary Information (15 pages, 8\n  figures), final corrected version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language, which allows complex ideas to be communicated through symbolic\nsequences, is a characteristic feature of our species and manifested in a\nmultitude of forms. Using large written corpora for many different languages\nand scripts, we show that the occurrence probability distributions of signs at\nthe left and right ends of words have a distinct heterogeneous nature.\nCharacterizing this asymmetry using quantitative inequality measures, viz.\ninformation entropy and the Gini index, we show that the beginning of a word is\nless restrictive in sign usage than the end. This property is not simply\nattributable to the use of common affixes as it is seen even when only word\nroots are considered. We use the existence of this asymmetry to infer the\ndirection of writing in undeciphered inscriptions that agrees with the\narchaeological evidence. Unlike traditional investigations of phonotactic\nconstraints which focus on language-specific patterns, our study reveals a\nproperty valid across languages and writing systems. As both language and\nwriting are unique aspects of our species, this universal signature may reflect\nan innate feature of the human cognitive phenomenon.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 10:59:24 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 03:05:57 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Ashraf", "Md Izhar", ""], ["Sinha", "Sitabhra", ""]]}]