[{"id": "1904.00110", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano and Ond\\v{r}ej Bojar", "title": "Keyphrase Generation: A Text Summarization Struggle", "comments": "7 pages, 3 tables. Published in proceedings of 2019 Annual Conference\n  of the North American Chapter of the Association for Computational\n  Linguistics. Identical to the previous version", "journal-ref": null, "doi": "10.18653/v1/N19-1070", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Authors' keyphrases assigned to scientific articles are essential for\nrecognizing content and topic aspects. Most of the proposed supervised and\nunsupervised methods for keyphrase generation are unable to produce terms that\nare valuable but do not appear in the text. In this paper, we explore the\npossibility of considering the keyphrase string as an abstractive summary of\nthe title and the abstract. First, we collect, process and release a large\ndataset of scientific paper metadata that contains 2.2 million records. Then we\nexperiment with popular text summarization neural architectures. Despite using\nadvanced deep learning models, large quantities of data and many days of\ncomputation, our systematic evaluation on four test datasets reveals that the\nexplored text summarization methods could not produce better keyphrases than\nthe simpler unsupervised methods, or the existing supervised ones.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 22:43:26 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 19:54:28 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "1904.00118", "submitter": "Fan Bai", "authors": "Fan Bai and Alan Ritter", "title": "Structured Minimally Supervised Learning for Neural Relation Extraction", "comments": "Accepted to NAACL 2019. This version improves the model\n  description(present original \"Bag-Size Adaptive Learning Rate\" as \"Bag-Size\n  Weighting Function\"). No result/conclusion change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to minimally supervised relation extraction that\ncombines the benefits of learned representations and structured learning, and\naccurately predicts sentence-level relation mentions given only\nproposition-level supervision from a KB. By explicitly reasoning about missing\ndata during learning, our approach enables large-scale training of 1D\nconvolutional neural networks while mitigating the issue of label noise\ninherent in distant supervision. Our approach achieves state-of-the-art results\non minimally supervised sentential relation extraction, outperforming a number\nof baselines, including a competitive approach that uses the attention layer of\na purely neural model.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 23:28:01 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 02:36:09 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2019 20:44:28 GMT"}, {"version": "v4", "created": "Sat, 4 May 2019 21:19:33 GMT"}, {"version": "v5", "created": "Mon, 18 Nov 2019 21:24:41 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Bai", "Fan", ""], ["Ritter", "Alan", ""]]}, {"id": "1904.00132", "submitter": "Chenyang Huang", "authors": "Chenyang Huang, Amine Trabelsi, Osmar R. Za\\\"iane", "title": "ANA at SemEval-2019 Task 3: Contextual Emotion detection in\n  Conversations through hierarchical LSTMs and BERT", "comments": "Accepted at the SemEval-2019 International Workshop on Semantic\n  Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the system submitted by ANA Team for the SemEval-2019\nTask 3: EmoContext. We propose a novel Hierarchical LSTMs for Contextual\nEmotion Detection (HRLCE) model. It classifies the emotion of an utterance\ngiven its conversational context. The results show that, in this task, our\nHRCLE outperforms the most recent state-of-the-art text classification\nframework: BERT. We combine the results generated by BERT and HRCLE to achieve\nan overall score of 0.7709 which ranked 5th on the final leader board of the\ncompetition among 165 Teams.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 01:51:24 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 20:43:22 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Huang", "Chenyang", ""], ["Trabelsi", "Amine", ""], ["Za\u00efane", "Osmar R.", ""]]}, {"id": "1904.00143", "submitter": "Zhi-Xiu Ye", "authors": "Zhi-Xiu Ye, Zhen-Hua Ling", "title": "Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag\n  Attentions", "comments": "accepted by NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a neural relation extraction method to deal with the\nnoisy training data generated by distant supervision. Previous studies mainly\nfocus on sentence-level de-noising by designing neural networks with intra-bag\nattentions. In this paper, both intra-bag and inter-bag attentions are\nconsidered in order to deal with the noise at sentence-level and bag-level\nrespectively. First, relation-aware bag representations are calculated by\nweighting sentence embeddings using intra-bag attentions. Here, each possible\nrelation is utilized as the query for attention calculation instead of only\nusing the target relation in conventional methods. Furthermore, the\nrepresentation of a group of bags in the training set which share the same\nrelation label is calculated by weighting bag representations using a\nsimilarity-based inter-bag attention module. Finally, a bag group is utilized\nas a training sample when building our relation extractor. Experimental results\non the New York Times dataset demonstrate the effectiveness of our proposed\nintra-bag and inter-bag attention modules. Our method also achieves better\nrelation extraction accuracy than state-of-the-art methods on this dataset.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 03:55:20 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Ye", "Zhi-Xiu", ""], ["Ling", "Zhen-Hua", ""]]}, {"id": "1904.00157", "submitter": "Marco Baroni", "authors": "Marco Baroni", "title": "Linguistic generalization and compositionality in modern artificial\n  neural networks", "comments": "Please cite as \"to appear in the Philosophical Transactions of the\n  Royal Society B\"", "journal-ref": null, "doi": "10.1098/rstb.2019.0307", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, deep artificial neural networks have achieved astounding\nperformance in many natural language processing tasks. Given the high\nproductivity of language, these models must possess effective generalization\nabilities. It is widely assumed that humans handle linguistic productivity by\nmeans of algebraic compositional rules: Are deep networks similarly\ncompositional? After reviewing the main innovations characterizing current deep\nlanguage processing networks, I discuss a set of studies suggesting that deep\nnetworks are capable of subtle grammar-dependent generalizations, but also that\nthey do not rely on systematic compositional rules. I argue that the intriguing\nbehaviour of these devices (still awaiting a full understanding) should be of\ninterest to linguists and cognitive scientists, as it offers a new perspective\non possible computational strategies to deal with linguistic productivity\nbeyond rule-based compositionality, and it might lead to new insights into the\nless systematic generalization patterns that also appear in natural language.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 06:48:32 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 10:32:00 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 13:42:35 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Baroni", "Marco", ""]]}, {"id": "1904.00160", "submitter": "Satoshi Yamane", "authors": "Tetsuto Takano, Satoshi Yamane", "title": "Machine translation considering context information using\n  Encoder-Decoder model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the task of machine translation, context information is one of the\nimportant factor. But considering the context information model dose not\nproposed. The paper propose a new model which can integrate context information\nand make translation. In this paper, we create a new model based Encoder\nDecoder model. When translating current sentence, the model integrates output\nfrom preceding encoder with current encoder. The model can consider context\ninformation and the result score is higher than existing model.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 07:13:38 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Takano", "Tetsuto", ""], ["Yamane", "Satoshi", ""]]}, {"id": "1904.00313", "submitter": "Christopher Potts", "authors": "Bruno Godefroy and Christopher Potts", "title": "Modeling Drug-Disease Relations with Linguistic and Knowledge Graph\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  FDA drug labels are rich sources of information about drugs and drug-disease\nrelations, but their complexity makes them challenging texts to analyze in\nisolation. To overcome this, we situate these labels in two health knowledge\ngraphs: one built from precise structured information about drugs and diseases,\nand another built entirely from a database of clinical narrative texts using\nsimple heuristic methods. We show that Probabilistic Soft Logic models defined\nover these graphs are superior to text-only and relation-only variants, and\nthat the clinical narratives graph delivers exceptional results with little\nmanual effort. Finally, we release a new dataset of drug labels with\nannotations for five distinct drug-disease relations.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 00:48:42 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Godefroy", "Bruno", ""], ["Potts", "Christopher", ""]]}, {"id": "1904.00350", "submitter": "Sungjoon Park", "authors": "Sungjoon Park, Donghyun Kim, Alice Oh", "title": "Conversation Model Fine-Tuning for Classifying Client Utterances in\n  Counseling Dialogues", "comments": "9 pages, 2 figures, NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent surge of text-based online counseling applications enables us to\ncollect and analyze interactions between counselors and clients. A dataset of\nthose interactions can be used to learn to automatically classify the client\nutterances into categories that help counselors in diagnosing client status and\npredicting counseling outcome. With proper anonymization, we collect\ncounselor-client dialogues, define meaningful categories of client utterances\nwith professional counselors, and develop a novel neural network model for\nclassifying the client utterances. The central idea of our model, ConvMFiT, is\na pre-trained conversation model which consists of a general language model\nbuilt from an out-of-domain corpus and two role-specific language models built\nfrom unlabeled in-domain dialogues. The classification result shows that\nConvMFiT outperforms state-of-the-art comparison models. Further, the attention\nweights in the learned model confirm that the model finds expected linguistic\npatterns for each category.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 07:30:47 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Park", "Sungjoon", ""], ["Kim", "Donghyun", ""], ["Oh", "Alice", ""]]}, {"id": "1904.00365", "submitter": "Albina Khusainova", "authors": "Albina Khusainova, Adil Khan, and Ad\\'in Ram\\'irez Rivera", "title": "SART - Similarity, Analogies, and Relatedness for Tatar Language: New\n  Benchmark Datasets for Word Embeddings Evaluation", "comments": "The datasets are available at https://github.com/tat-nlp/SART", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a huge imbalance between languages currently spoken and\ncorresponding resources to study them. Most of the attention naturally goes to\nthe \"big\" languages: those which have the largest presence in terms of media\nand number of speakers. Other less represented languages sometimes do not even\nhave a good quality corpus to study them. In this paper, we tackle this\nimbalance by presenting a new set of evaluation resources for Tatar, a language\nof the Turkic language family which is mainly spoken in Tatarstan Republic,\nRussia.\n  We present three datasets: Similarity and Relatedness datasets that consist\nof human scored word pairs and can be used to evaluate semantic models; and\nAnalogies dataset that comprises analogy questions and allows to explore\nsemantic, syntactic, and morphological aspects of language modeling. All three\ndatasets build upon existing datasets for the English language and follow the\nsame structure. However, they are not mere translations. They take into account\nspecifics of the Tatar language and expand beyond the original datasets. We\nevaluate state-of-the-art word embedding models for two languages using our\nproposed datasets for Tatar and the original datasets for English and report\nour findings on performance comparison.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 09:23:17 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Khusainova", "Albina", ""], ["Khan", "Adil", ""], ["Rivera", "Ad\u00edn Ram\u00edrez", ""]]}, {"id": "1904.00585", "submitter": "Xiang Dai", "authors": "Xiang Dai and Sarvnaz Karimi and Ben Hachey and Cecile Paris", "title": "Using Similarity Measures to Select Pretraining Data for NER", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word vectors and Language Models (LMs) pretrained on a large amount of\nunlabelled data can dramatically improve various Natural Language Processing\n(NLP) tasks. However, the measure and impact of similarity between pretraining\ndata and target task data are left to intuition. We propose three\ncost-effective measures to quantify different aspects of similarity between\nsource pretraining and target task data. We demonstrate that these measures are\ngood predictors of the usefulness of pretrained models for Named Entity\nRecognition (NER) over 30 data pairs. Results also suggest that pretrained LMs\nare more effective and more predictable than pretrained word vectors, but\npretrained word vectors are better when pretraining data is dissimilar.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 06:45:45 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 03:15:33 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Dai", "Xiang", ""], ["Karimi", "Sarvnaz", ""], ["Hachey", "Ben", ""], ["Paris", "Cecile", ""]]}, {"id": "1904.00615", "submitter": "Maximin Coavoux", "authors": "Maximin Coavoux, Shay B. Cohen", "title": "Discontinuous Constituency Parsing with a Stack-Free Transition System\n  and a Dynamic Oracle", "comments": "Accepted for publication at NAACL 2019; 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel transition system for discontinuous constituency\nparsing. Instead of storing subtrees in a stack --i.e. a data structure with\nlinear-time sequential access-- the proposed system uses a set of parsing\nitems, with constant-time random access. This change makes it possible to\nconstruct any discontinuous constituency tree in exactly $4n - 2$ transitions\nfor a sentence of length $n$. At each parsing step, the parser considers every\nitem in the set to be combined with a focus item and to construct a new\nconstituent in a bottom-up fashion. The parsing strategy is based on the\nassumption that most syntactic structures can be parsed incrementally and that\nthe set --the memory of the parser-- remains reasonably small on average.\nMoreover, we introduce a provably correct dynamic oracle for the new transition\nsystem, and present the first experiments in discontinuous constituency parsing\nusing a dynamic oracle. Our parser obtains state-of-the-art results on three\nEnglish and German discontinuous treebanks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 07:49:19 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Coavoux", "Maximin", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1904.00639", "submitter": "Mamoru Komachi", "authors": "Tosho Hirasawa and Hayahide Yamagishi and Yukio Matsumura and Mamoru\n  Komachi", "title": "Multimodal Machine Translation with Embedding Prediction", "comments": "6 pages; NAACL 2019 Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal machine translation is an attractive application of neural machine\ntranslation (NMT). It helps computers to deeply understand visual objects and\ntheir relations with natural languages. However, multimodal NMT systems suffer\nfrom a shortage of available training data, resulting in poor performance for\ntranslating rare words. In NMT, pretrained word embeddings have been shown to\nimprove NMT of low-resource domains, and a search-based approach is proposed to\naddress the rare word problem. In this study, we effectively combine these two\napproaches in the context of multimodal NMT and explore how we can take full\nadvantage of pretrained word embeddings to better translate rare words. We\nreport overall performance improvements of 1.24 METEOR and 2.49 BLEU and\nachieve an improvement of 7.67 F-score for rare word translation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:47:40 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Hirasawa", "Tosho", ""], ["Yamagishi", "Hayahide", ""], ["Matsumura", "Yukio", ""], ["Komachi", "Mamoru", ""]]}, {"id": "1904.00648", "submitter": "Lorenzo Porcaro", "authors": "Lorenzo Porcaro, Horacio Saggion", "title": "Recognizing Musical Entities in User-generated Content", "comments": "International Conference on Computational Linguistics and Intelligent\n  Text Processing (CICLing) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing Musical Entities is important for Music Information Retrieval\n(MIR) since it can improve the performance of several tasks such as music\nrecommendation, genre classification or artist similarity. However, most entity\nrecognition systems in the music domain have concentrated on formal texts (e.g.\nartists' biographies, encyclopedic articles, etc.), ignoring rich and noisy\nuser-generated content. In this work, we present a novel method to recognize\nmusical entities in Twitter content generated by users following a classical\nmusic radio channel. Our approach takes advantage of both formal radio schedule\nand users' tweets to improve entity recognition. We instantiate several machine\nlearning algorithms to perform entity recognition combining task-specific and\ncorpus-based features. We also show how to improve recognition results by\njointly considering formal and user-generated content\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:10:14 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Porcaro", "Lorenzo", ""], ["Saggion", "Horacio", ""]]}, {"id": "1904.00669", "submitter": "Daniel Hershcovich", "authors": "Daniel Hershcovich, Assaf Toledo, Alon Halfon and Noam Slonim", "title": "Syntactic Interchangeability in Word Embedding Models", "comments": "Accepted to RepEval 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbors in word embedding models are commonly observed to be\nsemantically similar, but the relations between them can vary greatly. We\ninvestigate the extent to which word embedding models preserve syntactic\ninterchangeability, as reflected by distances between word vectors, and the\neffect of hyper-parameters---context window size in particular. We use part of\nspeech (POS) as a proxy for syntactic interchangeability, as generally\nspeaking, words with the same POS are syntactically valid in the same contexts.\nWe also investigate the relationship between interchangeability and similarity\nas judged by commonly-used word similarity benchmarks, and correlate the result\nwith the performance of word embedding models on these benchmarks. Our results\nwill inform future research and applications in the selection of word embedding\nmodel, suggesting a principle for an appropriate selection of the context\nwindow size parameter depending on the use-case.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:49:16 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 08:31:22 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Hershcovich", "Daniel", ""], ["Toledo", "Assaf", ""], ["Halfon", "Alon", ""], ["Slonim", "Noam", ""]]}, {"id": "1904.00676", "submitter": "Debjit Paul", "authors": "Debjit Paul, Anette Frank", "title": "Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human\n  Needs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make machines better understand sentiments, research needs to move from\npolarity identification to understanding the reasons that underlie the\nexpression of sentiment. Categorizing the goals or needs of humans is one way\nto explain the expression of sentiment in text. Humans are good at\nunderstanding situations described in natural language and can easily connect\nthem to the character's psychological needs using commonsense knowledge. We\npresent a novel method to extract, rank, filter and select multi-hop relation\npaths from a commonsense knowledge resource to interpret the expression of\nsentiment in terms of their underlying human needs. We efficiently integrate\nthe acquired knowledge paths in a neural model that interfaces context\nrepresentations with knowledge using a gated attention mechanism. We assess the\nmodel's performance on a recently published dataset for categorizing human\nneeds. Selectively integrating knowledge paths boosts performance and\nestablishes a new state-of-the-art. Our model offers interpretability through\nthe learned attention map over commonsense knowledge paths. Human evaluation\nhighlights the relevance of the encoded knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:57:54 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Paul", "Debjit", ""], ["Frank", "Anette", ""]]}, {"id": "1904.00688", "submitter": "Abdelkrime Aries", "authors": "Abdelkrime Aries, Djamel eddine Zegour, Walid Khaled Hidouci", "title": "Automatic text summarization: What has been done and what has to be done", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Summaries are important when it comes to process huge amounts of information.\nTheir most important benefit is saving time, which we do not have much\nnowadays. Therefore, a summary must be short, representative and readable.\nGenerating summaries automatically can be beneficial for humans, since it can\nsave time and help selecting relevant documents. Automatic summarization and,\nin particular, Automatic text summarization (ATS) is not a new research field;\nIt was known since the 50s. Since then, researchers have been active to find\nthe perfect summarization method. In this article, we will discuss different\nworks in automatic summarization, especially the recent ones. We will present\nsome problems and limits which prevent works to move forward. Most of these\nchallenges are much more related to the nature of processed languages. These\nchallenges are interesting for academics and developers, as a path to follow in\nthis field.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 10:22:42 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Aries", "Abdelkrime", ""], ["Zegour", "Djamel eddine", ""], ["Hidouci", "Walid Khaled", ""]]}, {"id": "1904.00720", "submitter": "Ziyu Yao", "authors": "Ziyu Yao, Jayavardhan Reddy Peddamail, Huan Sun", "title": "CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning", "comments": "10 pages, 2 figures. Accepted by The Web Conference (WWW) 2019", "journal-ref": null, "doi": "10.1145/3308558.3313632", "report-no": null, "categories": "cs.SE cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accelerate software development, much research has been performed to help\npeople understand and reuse the huge amount of available code resources. Two\nimportant tasks have been widely studied: code retrieval, which aims to\nretrieve code snippets relevant to a given natural language query from a code\nbase, and code annotation, where the goal is to annotate a code snippet with a\nnatural language description. Despite their advancement in recent years, the\ntwo tasks are mostly explored separately. In this work, we investigate a novel\nperspective of Code annotation for Code retrieval (hence called `CoaCor'),\nwhere a code annotation model is trained to generate a natural language\nannotation that can represent the semantic meaning of a given code snippet and\ncan be leveraged by a code retrieval model to better distinguish relevant code\nsnippets from others. To this end, we propose an effective framework based on\nreinforcement learning, which explicitly encourages the code annotation model\nto generate annotations that can be used for the retrieval task. Through\nextensive experiments, we show that code annotations generated by our framework\nare much more detailed and more useful for code retrieval, and they can further\nimprove the performance of existing code retrieval models significantly.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 19:22:22 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Yao", "Ziyu", ""], ["Peddamail", "Jayavardhan Reddy", ""], ["Sun", "Huan", ""]]}, {"id": "1904.00761", "submitter": "Casper Hansen", "authors": "Christian Hansen, Casper Hansen, Stephen Alstrup, Jakob Grue Simonsen,\n  Christina Lioma", "title": "Neural Speed Reading with Structural-Jump-LSTM", "comments": "10 pages", "journal-ref": "7th International Conference on Learning Representations (ICLR)\n  2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) can model natural language by sequentially\n'reading' input tokens and outputting a distributed representation of each\ntoken. Due to the sequential nature of RNNs, inference time is linearly\ndependent on the input length, and all inputs are read regardless of their\nimportance. Efforts to speed up this inference, known as 'neural speed\nreading', either ignore or skim over part of the input. We present\nStructural-Jump-LSTM: the first neural speed reading model to both skip and\njump text during inference. The model consists of a standard LSTM and two\nagents: one capable of skipping single words when reading, and one capable of\nexploiting punctuation structure (sub-sentence separators (,:), sentence end\nsymbols (.!?), or end of text markers) to jump ahead after reading a word. A\ncomprehensive experimental evaluation of our model against all five\nstate-of-the-art neural reading models shows that Structural-Jump-LSTM achieves\nthe best overall floating point operations (FLOP) reduction (hence is faster),\nwhile keeping the same accuracy or even improving it compared to a vanilla LSTM\nthat reads the whole text.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 12:01:46 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 08:59:34 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Hansen", "Christian", ""], ["Hansen", "Casper", ""], ["Alstrup", "Stephen", ""], ["Simonsen", "Jakob Grue", ""], ["Lioma", "Christina", ""]]}, {"id": "1904.00762", "submitter": "Subba Reddy Oota", "authors": "Subba Reddy Oota, Adithya Avvaru, Mounika Marreddy, Radhika Mamidi", "title": "Affect in Tweets Using Experts Model", "comments": "10 pages, 6 figures, The 32nd Pacific Asia Conference on Language,\n  Information and Computation (PACLIC 32)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the intensity of emotion has gained significance as modern textual\ninputs in potential applications like social media, e-retail markets,\npsychology, advertisements etc., carry a lot of emotions, feelings, expressions\nalong with its meaning. However, the approaches of traditional sentiment\nanalysis primarily focuses on classifying the sentiment in general (positive or\nnegative) or at an aspect level(very positive, low negative, etc.) and cannot\nexploit the intensity information. Moreover, automatically identifying emotions\nlike anger, fear, joy, sadness, disgust etc., from text introduces challenging\nscenarios where single tweet may contain multiple emotions with different\nintensities and some emotions may even co-occur in some of the tweets. In this\npaper, we propose an architecture, Experts Model, inspired from the standard\nMixture of Experts (MoE) model. The key idea here is each expert learns\ndifferent sets of features from the feature vector which helps in better\nemotion detection from the tweet. We compared the results of our Experts Model\nwith both baseline results and top five performers of SemEval-2018 Task-1,\nAffect in Tweets (AIT). The experimental results show that our proposed\napproach deals with the emotion detection problem and stands at top-5 results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 11:10:29 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Oota", "Subba Reddy", ""], ["Avvaru", "Adithya", ""], ["Marreddy", "Mounika", ""], ["Mamidi", "Radhika", ""]]}, {"id": "1904.00771", "submitter": "Junichi Yamagishi", "authors": "Hieu-Thi Luong, Xin Wang, Junichi Yamagishi, Nobuyuki Nishizawa", "title": "Training Multi-Speaker Neural Text-to-Speech Systems using\n  Speaker-Imbalanced Speech Corpora", "comments": "Submitted to Interspeech 2019, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the available data of a target speaker is insufficient to train a high\nquality speaker-dependent neural text-to-speech (TTS) system, we can combine\ndata from multiple speakers and train a multi-speaker TTS model instead. Many\nstudies have shown that neural multi-speaker TTS model trained with a small\namount data from multiple speakers combined can generate synthetic speech with\nbetter quality and stability than a speaker-dependent one. However when the\namount of data from each speaker is highly unbalanced, the best approach to\nmake use of the excessive data remains unknown. Our experiments showed that\nsimply combining all available data from every speaker to train a multi-speaker\nmodel produces better than or at least similar performance to its\nspeaker-dependent counterpart. Moreover by using an ensemble multi-speaker\nmodel, in which each subsystem is trained on a subset of available data, we can\nfurther improve the quality of the synthetic speech especially for\nunderrepresented speakers whose training data is limited.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 12:39:05 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 23:35:06 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luong", "Hieu-Thi", ""], ["Wang", "Xin", ""], ["Yamagishi", "Junichi", ""], ["Nishizawa", "Nobuyuki", ""]]}, {"id": "1904.00784", "submitter": "Sai Krishna Rallabandi", "authors": "Sunayana Sitaram, Khyathi Raghavi Chandu, Sai Krishna Rallabandi and\n  Alan W Black", "title": "A Survey of Code-switched Speech and Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code-switching, the alternation of languages within a conversation or\nutterance, is a common communicative phenomenon that occurs in multilingual\ncommunities across the world. This survey reviews computational approaches for\ncode-switched Speech and Natural Language Processing. We motivate why\nprocessing code-switched text and speech is essential for building intelligent\nagents and systems that interact with users in multilingual communities. As\ncode-switching data and resources are scarce, we list what is available in\nvarious code-switched language pairs with the language processing tasks they\ncan be used for. We review code-switching research in various Speech and NLP\napplications, including language processing tools and end-to-end systems. We\nconclude with future directions and open problems in the field.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 14:36:50 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 14:18:31 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 23:55:01 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Sitaram", "Sunayana", ""], ["Chandu", "Khyathi Raghavi", ""], ["Rallabandi", "Sai Krishna", ""], ["Black", "Alan W", ""]]}, {"id": "1904.00785", "submitter": "Aleksandr Perevalov", "authors": "Aleksandr Perevalov, Daniil Kurushin, Rustam Faizrakhmanov and Farida\n  Khabibrakhmanova", "title": "Question Embeddings Based on Shannon Entropy: Solving intent\n  classification task in goal-oriented dialogue system", "comments": "Proceedings of International Conference on Applied Innovation in IT", "journal-ref": "2019/03/06, Volume 7, Issue 1, Koethen Germany, ISBN:\n  978-3-96057-086-8 (Online)", "doi": "10.25673/13485", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Question-answering systems and voice assistants are becoming major part of\nclient service departments of many organizations, helping them to reduce the\nlabor costs of staff. In many such systems, there is always natural language\nunderstanding module that solves intent classification task. This task is\ncomplicated because of its case-dependency - every subject area has its own\nsemantic kernel. The state of art approaches for intent classification are\ndifferent machine learning and deep learning methods that use text vector\nrepresentations as input. The basic vector representation models such as Bag of\nwords and TF-IDF generate sparse matrixes, which are becoming very big as the\namount of input data grows. Modern methods such as word2vec and FastText use\nneural networks to evaluate word embeddings with fixed dimension size. As we\nare developing a question-answering system for students and enrollees of the\nPerm National Research Polytechnic University, we have faced the problem of\nuser's intent detection. The subject area of our system is very specific, that\nis why there is a lack of training data. This aspect makes intent\nclassification task more challenging for using state of the art deep learning\nmethods. In this paper, we propose an approach of the questions embeddings\nrepresentation based on calculation of Shannon entropy.The goal of the approach\nis to produce low dimensional question vectors as neural approaches do and to\noutperform related methods, described above in condition of small dataset. We\nevaluate and compare our model with existing ones using logistic regression and\ndataset that contains questions asked by students and enrollees. The data is\nlabeled into six classes. Experimental comparison of proposed approach and\nother models revealed that proposed model performed better in the given task.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:59:32 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Perevalov", "Aleksandr", ""], ["Kurushin", "Daniil", ""], ["Faizrakhmanov", "Rustam", ""], ["Khabibrakhmanova", "Farida", ""]]}, {"id": "1904.00788", "submitter": "Soheil Esmaeilzadeh", "authors": "Soheil Esmaeilzadeh, Gao Xian Peh, Angela Xu", "title": "Neural Abstractive Text Summarization and Fake News Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study abstractive text summarization by exploring different\nmodels such as LSTM-encoder-decoder with attention, pointer-generator networks,\ncoverage mechanisms, and transformers. Upon extensive and careful\nhyperparameter tuning we compare the proposed architectures against each other\nfor the abstractive text summarization task. Finally, as an extension of our\nwork, we apply our text summarization model as a feature extractor for a fake\nnews detection task where the news articles prior to classification will be\nsummarized and the results are compared against the classification using only\nthe original news text.\n  keywords: LSTM, encoder-deconder, abstractive text summarization,\npointer-generator, coverage mechanism, transformers, fake news detection\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 07:27:51 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 07:46:43 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Esmaeilzadeh", "Soheil", ""], ["Peh", "Gao Xian", ""], ["Xu", "Angela", ""]]}, {"id": "1904.00796", "submitter": "Debajyoti Chatterjee", "authors": "Debajyoti Chatterjee", "title": "Making Neural Machine Reading Comprehension Faster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims at solving the Machine Reading Comprehension problem where\nquestions have to be answered given a context passage. The challenge is to\ndevelop a computationally faster model which will have improved inference time.\nState of the art in many natural language understanding tasks, BERT model, has\nbeen used and knowledge distillation method has been applied to train two\nsmaller models. The developed models are compared with other models which have\nbeen developed with the same intention.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 05:03:15 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chatterjee", "Debajyoti", ""]]}, {"id": "1904.00805", "submitter": "Ben Gelman", "authors": "Jessica Moore, Ben Gelman, David Slater", "title": "A Convolutional Neural Network for Language-Agnostic Source Code\n  Summarization", "comments": "ENASE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Descriptive comments play a crucial role in the software engineering process.\nThey decrease development time, enable better bug detection, and facilitate the\nreuse of previously written code. However, comments are commonly the last of a\nsoftware developer's priorities and are thus either insufficient or missing\nentirely. Automatic source code summarization may therefore have the ability to\nsignificantly improve the software development process. We introduce a novel\nencoder-decoder model that summarizes source code, effectively writing a\ncomment to describe the code's functionality. We make two primary innovations\nbeyond current source code summarization models. First, our encoder is fully\nlanguage-agnostic and requires no complex input preprocessing. Second, our\ndecoder has an open vocabulary, enabling it to predict any word, even ones not\nseen in training. We demonstrate results comparable to state-of-the-art methods\non a single-language data set and provide the first results on a data set\nconsisting of multiple programming languages.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 15:53:28 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Moore", "Jessica", ""], ["Gelman", "Ben", ""], ["Slater", "David", ""]]}, {"id": "1904.00812", "submitter": "Ramon Ferrer i Cancho", "authors": "Bernardino Casas, Antoni Hern\\'andez-Fern\\'andez, Neus Catal\\`a, Ramon\n  Ferrer-i-Cancho and Jaume Baixeries", "title": "Polysemy and brevity versus frequency in language", "comments": null, "journal-ref": "Computer Speech and Language 58, 19-50 (2019)", "doi": "10.1016/j.csl.2019.03.007", "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pioneering research of G. K. Zipf on the relationship between word\nfrequency and other word features led to the formulation of various linguistic\nlaws. The most popular is Zipf's law for word frequencies. Here we focus on two\nlaws that have been studied less intensively: the meaning-frequency law, i.e.\nthe tendency of more frequent words to be more polysemous, and the law of\nabbreviation, i.e. the tendency of more frequent words to be shorter. In a\nprevious work, we tested the robustness of these Zipfian laws for English,\nroughly measuring word length in number of characters and distinguishing adult\nfrom child speech. In the present article, we extend our study to other\nlanguages (Dutch and Spanish) and introduce two additional measures of length:\nsyllabic length and phonemic length. Our correlation analysis indicates that\nboth the meaning-frequency law and the law of abbreviation hold overall in all\nthe analyzed languages.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:21:57 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Casas", "Bernardino", ""], ["Hern\u00e1ndez-Fern\u00e1ndez", "Antoni", ""], ["Catal\u00e0", "Neus", ""], ["Ferrer-i-Cancho", "Ramon", ""], ["Baixeries", "Jaume", ""]]}, {"id": "1904.00929", "submitter": "Manuel Ciosici", "authors": "Manuel Ciosici and Tobias Sommer and Ira Assent", "title": "Unsupervised Abbreviation Disambiguation Contextual disambiguation using\n  word embeddings", "comments": "Fixed author names; Revised text and experimental section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abbreviations often have several distinct meanings, often making their use in\ntext ambiguous. Expanding them to their intended meaning in context is\nimportant for Machine Reading tasks such as document search, recommendation and\nquestion answering. Existing approaches mostly rely on manually labeled\nexamples of abbreviations and their correct long-forms. Such data sets are\ncostly to create and result in trained models with limited applicability and\nflexibility. Importantly, most current methods must be subjected to a full\nempirical evaluation in order to understand their limitations, which is\ncumbersome in practice.\n  In this paper, we present an entirely unsupervised abbreviation\ndisambiguation method (called UAD) that picks up abbreviation definitions from\nunstructured text. Creating distinct tokens per meaning, we learn context\nrepresentations as word vectors. We demonstrate how to further boost\nabbreviation disambiguation performance by obtaining better context\nrepresentations using additional unstructured text. Our method is the first\nabbreviation disambiguation approach with a transparent model that allows\nperformance analysis without requiring full-scale evaluation, making it highly\nrelevant for real-world deployments.\n  In our thorough empirical evaluation, UAD achieves high performance on large\nreal-world data sets from different domains and outperforms both baseline and\nstate-of-the-art methods. UAD scales well and supports thousands of\nabbreviations with multiple different meanings within a single model.\n  In order to spur more research into abbreviation disambiguation, we publish a\nnew data set, that we also use in our experiments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 15:59:18 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 07:26:11 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Ciosici", "Manuel", ""], ["Sommer", "Tobias", ""], ["Assent", "Ira", ""]]}, {"id": "1904.00930", "submitter": "Nikolai Vogler", "authors": "Nikolai Vogler, Craig Stewart, Graham Neubig", "title": "Lost in Interpretation: Predicting Untranslated Terminology in\n  Simultaneous Interpretation", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous interpretation, the translation of speech from one language to\nanother in real-time, is an inherently difficult and strenuous task. One of the\ngreatest challenges faced by interpreters is the accurate translation of\ndifficult terminology like proper names, numbers, or other entities.\nIntelligent computer-assisted interpreting (CAI) tools that could analyze the\nspoken word and detect terms likely to be untranslated by an interpreter could\nreduce translation error and improve interpreter performance. In this paper, we\npropose a task of predicting which terminology simultaneous interpreters will\nleave untranslated, and examine methods that perform this task using supervised\nsequence taggers. We describe a number of task-specific features explicitly\ndesigned to indicate when an interpreter may struggle with translating a word.\nExperimental results on a newly-annotated version of the NAIST Simultaneous\nTranslation Corpus (Shimizu et al., 2014) indicate the promise of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:03:59 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Vogler", "Nikolai", ""], ["Stewart", "Craig", ""], ["Neubig", "Graham", ""]]}, {"id": "1904.00962", "submitter": "Yang You", "authors": "Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv\n  Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt\n  Keutzer and Cho-Jui Hsieh", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large deep neural networks on massive datasets is computationally\nvery challenging. There has been recent surge in interest in using large batch\nstochastic optimization methods to tackle this issue. The most prominent\nalgorithm in this line of research is LARS, which by employing layerwise\nadaptive learning rates trains ResNet on ImageNet in a few minutes. However,\nLARS performs poorly for attention models like BERT, indicating that its\nperformance gains are not consistent across tasks. In this paper, we first\nstudy a principled layerwise adaptation strategy to accelerate training of deep\nneural networks using large mini-batches. Using this strategy, we develop a new\nlayerwise adaptive large batch optimization technique called LAMB; we then\nprovide convergence analysis of LAMB as well as LARS, showing convergence to a\nstationary point in general nonconvex settings. Our empirical results\ndemonstrate the superior performance of LAMB across various tasks such as BERT\nand ResNet-50 training with very little hyperparameter tuning. In particular,\nfor BERT training, our optimizer enables use of very large batch sizes of 32868\nwithout any degradation of performance. By increasing the batch size to the\nmemory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to\njust 76 minutes (Table 1). The LAMB implementation is available at\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:53:35 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 06:20:00 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 17:09:47 GMT"}, {"version": "v4", "created": "Wed, 25 Sep 2019 16:07:11 GMT"}, {"version": "v5", "created": "Fri, 3 Jan 2020 06:53:00 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["You", "Yang", ""], ["Li", "Jing", ""], ["Reddi", "Sashank", ""], ["Hseu", "Jonathan", ""], ["Kumar", "Sanjiv", ""], ["Bhojanapalli", "Srinadh", ""], ["Song", "Xiaodan", ""], ["Demmel", "James", ""], ["Keutzer", "Kurt", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1904.00977", "submitter": "Ibai Roman", "authors": "Ibai Roman, Alexander Mendiburu, Roberto Santana and Jose A. Lozano", "title": "Sentiment analysis with genetically evolved Gaussian kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sentiment analysis consists of evaluating opinions or statements from the\nanalysis of text. Among the methods used to estimate the degree in which a text\nexpresses a given sentiment, are those based on Gaussian Processes. However,\ntraditional Gaussian Processes methods use a predefined kernel with\nhyperparameters that can be tuned but whose structure can not be adapted. In\nthis paper, we propose the application of Genetic Programming for evolving\nGaussian Process kernels that are more precise for sentiment analysis. We use\nuse a very flexible representation of kernels combined with a multi-objective\napproach that simultaneously considers two quality metrics and the\ncomputational time spent by the kernels. Our results show that the algorithm\ncan outperform Gaussian Processes with traditional kernels for some of the\nsentiment analysis tasks considered.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 17:28:35 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 08:48:49 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Roman", "Ibai", ""], ["Mendiburu", "Alexander", ""], ["Santana", "Roberto", ""], ["Lozano", "Jose A.", ""]]}, {"id": "1904.01032", "submitter": "Mingbo Ma", "authors": "Mingbo Ma, Renjie Zheng and Liang Huang", "title": "Learning to Stop in Structured Prediction for Neural Machine Translation", "comments": "5 pages", "journal-ref": "NAACL 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beam search optimization resolves many issues in neural machine translation.\nHowever, this method lacks principled stopping criteria and does not learn how\nto stop during training, and the model naturally prefers the longer hypotheses\nduring the testing time in practice since they use the raw score instead of the\nprobability-based score. We propose a novel ranking method which enables an\noptimal beam search stopping criteria. We further introduce a structured\nprediction loss function which penalizes suboptimal finished candidates\nproduced by beam search during training. Experiments of neural machine\ntranslation on both synthetic data and real languages (German-to-English and\nChinese-to-English) demonstrate our proposed methods lead to better length and\nBLEU score.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 18:01:08 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 17:29:38 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 21:37:54 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ma", "Mingbo", ""], ["Zheng", "Renjie", ""], ["Huang", "Liang", ""]]}, {"id": "1904.01038", "submitter": "Michael Auli", "authors": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan\n  Ng, David Grangier, Michael Auli", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "comments": "NAACL 2019 Demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  fairseq is an open-source sequence modeling toolkit that allows researchers\nand developers to train custom models for translation, summarization, language\nmodeling, and other text generation tasks. The toolkit is based on PyTorch and\nsupports distributed training across multiple GPUs and machines. We also\nsupport fast mixed-precision training and inference on modern GPUs. A demo\nvideo can be found at https://www.youtube.com/watch?v=OtgDdWtHvto\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 18:05:02 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Ott", "Myle", ""], ["Edunov", "Sergey", ""], ["Baevski", "Alexei", ""], ["Fan", "Angela", ""], ["Gross", "Sam", ""], ["Ng", "Nathan", ""], ["Grangier", "David", ""], ["Auli", "Michael", ""]]}, {"id": "1904.01120", "submitter": "Cheng-I Lai", "authors": "Cheng-I Lai, Nanxin Chen, Jes\\'us Villalba, Najim Dehak", "title": "ASSERT: Anti-Spoofing with Squeeze-Excitation and Residual neTworks", "comments": "Submitted to Interspeech 2019, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present JHU's system submission to the ASVspoof 2019 Challenge:\nAnti-Spoofing with Squeeze-Excitation and Residual neTworks (ASSERT).\nAnti-spoofing has gathered more and more attention since the inauguration of\nthe ASVspoof Challenges, and ASVspoof 2019 dedicates to address attacks from\nall three major types: text-to-speech, voice conversion, and replay. Built upon\nprevious research work on Deep Neural Network (DNN), ASSERT is a pipeline for\nDNN-based approach to anti-spoofing. ASSERT has four components: feature\nengineering, DNN models, network optimization and system combination, where the\nDNN models are variants of squeeze-excitation and residual networks. We\nconducted an ablation study of the effectiveness of each component on the\nASVspoof 2019 corpus, and experimental results showed that ASSERT obtained more\nthan 93% and 17% relative improvements over the baseline systems in the two\nsub-challenges in ASVspooof 2019, ranking ASSERT one of the top performing\nsystems. Code and pretrained models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 21:47:00 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Lai", "Cheng-I", ""], ["Chen", "Nanxin", ""], ["Villalba", "Jes\u00fas", ""], ["Dehak", "Najim", ""]]}, {"id": "1904.01130", "submitter": "Yuan Zhang", "authors": "Yuan Zhang, Jason Baldridge, Luheng He", "title": "PAWS: Paraphrase Adversaries from Word Scrambling", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing paraphrase identification datasets lack sentence pairs that have\nhigh lexical overlap without being paraphrases. Models trained on such data\nfail to distinguish pairs like flights from New York to Florida and flights\nfrom Florida to New York. This paper introduces PAWS (Paraphrase Adversaries\nfrom Word Scrambling), a new dataset with 108,463 well-formed paraphrase and\nnon-paraphrase pairs with high lexical overlap. Challenging pairs are generated\nby controlled word swapping and back translation, followed by fluency and\nparaphrase judgments by human raters. State-of-the-art models trained on\nexisting datasets have dismal performance on PAWS (<40% accuracy); however,\nincluding PAWS training data for these models improves their accuracy to 85%\nwhile maintaining performance on existing tasks. In contrast, models that do\nnot capture non-local contextual information fail even with PAWS training\nexamples. As such, PAWS provides an effective instrument for driving further\nprogress on models that better exploit structure, context, and pairwise\ncomparisons.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 22:21:14 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhang", "Yuan", ""], ["Baldridge", "Jason", ""], ["He", "Luheng", ""]]}, {"id": "1904.01138", "submitter": "Lifu Tu", "authors": "Lifu Tu, Kevin Gimpel", "title": "Benchmarking Approximate Inference Methods for Neural Structured\n  Prediction", "comments": "NAACL2019 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exact structured inference with neural network scoring functions is\ncomputationally challenging but several methods have been proposed for\napproximating inference. One approach is to perform gradient descent with\nrespect to the output structure directly (Belanger and McCallum, 2016). Another\napproach, proposed recently, is to train a neural network (an \"inference\nnetwork\") to perform inference (Tu and Gimpel, 2018). In this paper, we compare\nthese two families of inference methods on three sequence labeling datasets. We\nchoose sequence labeling because it permits us to use exact inference as a\nbenchmark in terms of speed, accuracy, and search error. Across datasets, we\ndemonstrate that inference networks achieve a better speed/accuracy/search\nerror trade-off than gradient descent, while also being faster than exact\ninference at similar accuracy levels. We find further benefit by combining\ninference networks and gradient descent, using the former to provide a warm\nstart for the latter.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 23:08:05 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 23:05:09 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Tu", "Lifu", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1904.01172", "submitter": "Shane Storks", "authors": "Shane Storks, Qiaozi Gao, Joyce Y. Chai", "title": "Recent Advances in Natural Language Inference: A Survey of Benchmarks,\n  Resources, and Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the NLP community, recent years have seen a surge of research activities\nthat address machines' ability to perform deep language understanding which\ngoes beyond what is explicitly stated in text, rather relying on reasoning and\nknowledge of the world. Many benchmark tasks and datasets have been created to\nsupport the development and evaluation of such natural language inference\nability. As these benchmarks become instrumental and a driving force for the\nNLP research community, this paper aims to provide an overview of recent\nbenchmarks, relevant knowledge resources, and state-of-the-art learning and\ninference approaches in order to support a better understanding of this growing\nfield.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 02:09:01 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 22:34:49 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 14:28:42 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Storks", "Shane", ""], ["Gao", "Qiaozi", ""], ["Chai", "Joyce Y.", ""]]}, {"id": "1904.01173", "submitter": "Mingda Chen", "authors": "Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel", "title": "A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence\n  Representations", "comments": "NAACL 2019 Long paper", "journal-ref": "NAACL 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative model for a sentence that uses two latent variables,\nwith one intended to represent the syntax of the sentence and the other to\nrepresent its semantics. We show we can achieve better disentanglement between\nsemantic and syntactic representations by training with multiple losses,\nincluding losses that exploit aligned paraphrastic sentences and word-order\ninformation. We also investigate the effect of moving from bag-of-words to\nrecurrent neural network modules. We evaluate our models as well as several\npopular pretrained embeddings on standard semantic similarity tasks and novel\nsyntactic similarity tasks. Empirically, we find that the model with the best\nperforming syntactic and semantic representations also gives rise to the most\ndisentangled representations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 02:09:05 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Chen", "Mingda", ""], ["Tang", "Qingming", ""], ["Wiseman", "Sam", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1904.01201", "submitter": "Manolis Savva", "authors": "Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik\n  Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra\n  Malik, Devi Parikh, Dhruv Batra", "title": "Habitat: A Platform for Embodied AI Research", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Habitat, a platform for research in embodied artificial\nintelligence (AI). Habitat enables training embodied agents (virtual robots) in\nhighly efficient photorealistic 3D simulation. Specifically, Habitat consists\nof: (i) Habitat-Sim: a flexible, high-performance 3D simulator with\nconfigurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is\nfast -- when rendering a scene from Matterport3D, it achieves several thousand\nframes per second (fps) running single-threaded, and can reach over 10,000 fps\nmulti-process on a single GPU. (ii) Habitat-API: a modular high-level library\nfor end-to-end development of embodied AI algorithms -- defining tasks (e.g.,\nnavigation, instruction following, question answering), configuring, training,\nand benchmarking embodied agents.\n  These large-scale engineering contributions enable us to answer scientific\nquestions requiring experiments that were till now impracticable or 'merely'\nimpractical. Specifically, in the context of point-goal navigation: (1) we\nrevisit the comparison between learning and SLAM approaches from two recent\nworks and find evidence for the opposite conclusion -- that learning\noutperforms SLAM if scaled to an order of magnitude more experience than\nprevious investigations, and (2) we conduct the first cross-dataset\ngeneralization experiments {train, test} x {Matterport3D, Gibson} for multiple\nsensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors\ngeneralize across datasets. We hope that our open-source platform and these\nfindings will advance research in embodied AI.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:52:27 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 01:39:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Savva", "Manolis", ""], ["Kadian", "Abhishek", ""], ["Maksymets", "Oleksandr", ""], ["Zhao", "Yili", ""], ["Wijmans", "Erik", ""], ["Jain", "Bhavana", ""], ["Straub", "Julian", ""], ["Liu", "Jia", ""], ["Koltun", "Vladlen", ""], ["Malik", "Jitendra", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.01246", "submitter": "Zi-Yuan Chen", "authors": "Zi-Yuan Chen, Chih-Hung Chang, Yi-Pei Chen, Jijnasa Nayak, Lun-Wei Ku", "title": "UHop: An Unrestricted-Hop Relation Extraction Framework for\n  Knowledge-Based Question Answering", "comments": "To appear in NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In relation extraction for knowledge-based question answering, searching from\none entity to another entity via a single relation is called \"one hop\". In\nrelated work, an exhaustive search from all one-hop relations, two-hop\nrelations, and so on to the max-hop relations in the knowledge graph is\nnecessary but expensive. Therefore, the number of hops is generally restricted\nto two or three. In this paper, we propose UHop, an unrestricted-hop framework\nwhich relaxes this restriction by use of a transition-based search framework to\nreplace the relation-chain-based search one. We conduct experiments on\nconventional 1- and 2-hop questions as well as lengthy questions, including\ndatasets such as WebQSP, PathQuestion, and Grid World. Results show that the\nproposed framework enables the ability to halt, works well with\nstate-of-the-art models, achieves competitive performance without exhaustive\nsearches, and opens the performance gap for long relation paths.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 07:08:25 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Chen", "Zi-Yuan", ""], ["Chang", "Chih-Hung", ""], ["Chen", "Yi-Pei", ""], ["Nayak", "Jijnasa", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "1904.01297", "submitter": "Thomas Kober", "authors": "Thomas Kober and Sander Bijl de Vroe and Mark Steedman", "title": "Temporal and Aspectual Entailment", "comments": "accepted at IWCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferences regarding \"Jane's arrival in London\" from predications such as\n\"Jane is going to London\" or \"Jane has gone to London\" depend on tense and\naspect of the predications. Tense determines the temporal location of the\npredication in the past, present or future of the time of utterance. The\naspectual auxiliaries on the other hand specify the internal constituency of\nthe event, i.e. whether the event of \"going to London\" is completed and whether\nits consequences hold at that time or not. While tense and aspect are among the\nmost important factors for determining natural language inference, there has\nbeen very little work to show whether modern NLP models capture these semantic\nconcepts. In this paper we propose a novel entailment dataset and analyse the\nability of a range of recently proposed NLP models to perform inference on\ntemporal predications. We show that the models encode a substantial amount of\nmorphosyntactic information relating to tense and aspect, but fail to model\ninferences that require reasoning with these semantic properties.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:57:55 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Kober", "Thomas", ""], ["de Vroe", "Sander Bijl", ""], ["Steedman", "Mark", ""]]}, {"id": "1904.01301", "submitter": "Sheng Shen", "authors": "Sheng Shen, Daniel Fried, Jacob Andreas, Dan Klein", "title": "Pragmatically Informative Text Generation", "comments": "8 pages. accepted as a conference paper at NAACL2019 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the informativeness of models for conditional text generation\nusing techniques from computational pragmatics. These techniques formulate\nlanguage production as a game between speakers and listeners, in which a\nspeaker should generate output text that a listener can use to correctly\nidentify the original input that the text describes. While such approaches are\nwidely used in cognitive science and grounded language learning, they have\nreceived less attention for more standard language generation tasks. We\nconsider two pragmatic modeling methods for text generation: one where\npragmatics is imposed by information preservation, and another where pragmatics\nis imposed by explicit modeling of distractors. We find that these methods\nimprove the performance of strong existing systems for abstractive\nsummarization and generation from structured meaning representations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 09:04:57 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 05:56:18 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Shen", "Sheng", ""], ["Fried", "Daniel", ""], ["Andreas", "Jacob", ""], ["Klein", "Dan", ""]]}, {"id": "1904.01313", "submitter": "Yanxuan Li", "authors": "Yanxuan Li", "title": "Short Text Classification Improved by Feature Space Extension", "comments": "8 pages,2 figures and 7 tables.to be published in", "journal-ref": null, "doi": "10.1088/1757-899X/533/1/012046", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosive development of mobile Internet, short text has been\napplied extensively. The difference between classifying short text and long\ndocuments is that short text is of shortness and sparsity. Thus, it is\nchallenging to deal with short text classification owing to its less semantic\ninformation. In this paper, we propose a novel topic-based convolutional neural\nnetwork (TB-CNN) based on Latent Dirichlet Allocation (LDA) model and\nconvolutional neural network. Comparing to traditional CNN methods, TB-CNN\ngenerates topic words with LDA model to reduce the sparseness and combines the\nembedding vectors of topic words and input words to extend feature space of\nshort text. The validation results on IMDB movie review dataset show the\nimprovement and effectiveness of TB-CNN.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 10:00:58 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Li", "Yanxuan", ""]]}, {"id": "1904.01356", "submitter": "Omer Arshad", "authors": "Omer Arshad, Ignazio Gallo, Shah Nawaz, Alessandro Calefati", "title": "Aiding Intra-Text Representations with Visual Context for Multimodal\n  Named Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With massive explosion of social media such as Twitter and Instagram, people\ndaily share billions of multimedia posts, containing images and text.\nTypically, text in these posts is short, informal and noisy, leading to\nambiguities which can be resolved using images. In this paper we explore\ntext-centric Named Entity Recognition task on these multimedia posts. We\npropose an end to end model which learns a joint representation of a text and\nan image. Our model extends multi-dimensional self attention technique, where\nnow image helps to enhance relationship between words. Experiments show that\nour model is capable of capturing both textual and visual contexts with greater\naccuracy, achieving state-of-the-art results on Twitter multimodal Named Entity\nRecognition dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 11:57:40 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Arshad", "Omer", ""], ["Gallo", "Ignazio", ""], ["Nawaz", "Shah", ""], ["Calefati", "Alessandro", ""]]}, {"id": "1904.01451", "submitter": "Michael A. Hedderich", "authors": "Michael A. Hedderich, Andrew Yates, Dietrich Klakow and Gerard de Melo", "title": "Using Multi-Sense Vector Embeddings for Reverse Dictionaries", "comments": "Accepted as long paper at the 13th International Conference on\n  Computational Semantics (IWCS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular word embedding methods such as word2vec and GloVe assign a single\nvector representation to each word, even if a word has multiple distinct\nmeanings. Multi-sense embeddings instead provide different vectors for each\nsense of a word. However, they typically cannot serve as a drop-in replacement\nfor conventional single-sense embeddings, because the correct sense vector\nneeds to be selected for each word. In this work, we study the effect of\nmulti-sense embeddings on the task of reverse dictionaries. We propose a\ntechnique to easily integrate them into an existing neural network architecture\nusing an attention mechanism. Our experiments demonstrate that large\nimprovements can be obtained when employing multi-sense embeddings both in the\ninput sequence as well as for the target representation. An analysis of the\nsense distributions and of the learned attention is provided as well.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 14:17:19 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Hedderich", "Michael A.", ""], ["Yates", "Andrew", ""], ["Klakow", "Dietrich", ""], ["de Melo", "Gerard", ""]]}, {"id": "1904.01464", "submitter": "Toms Bergmanis", "authors": "Toms Bergmanis and Sharon Goldwater", "title": "Training Data Augmentation for Context-Sensitive Neural Lemmatization\n  Using Inflection Tables and Raw Text", "comments": "Published in NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lemmatization aims to reduce the sparse data problem by relating the\ninflected forms of a word to its dictionary form. Using context can help, both\nfor unseen and ambiguous words. Yet most context-sensitive approaches require\nfull lemma-annotated sentences for training, which may be scarce or unavailable\nin low-resource languages. In addition (as shown here), in a low-resource\nsetting, a lemmatizer can learn more from $n$ labeled examples of distinct\nwords (types) than from $n$ (contiguous) labeled tokens, since the latter\ncontain far fewer distinct types. To combine the efficiency of type-based\nlearning with the benefits of context, we propose a way to train a\ncontext-sensitive lemmatizer with little or no labeled corpus data, using\ninflection tables from the UniMorph project and raw text examples from\nWikipedia that provide sentence contexts for the unambiguous UniMorph examples.\nDespite these being unambiguous examples, the model successfully generalizes\nfrom them, leading to improved results (both overall, and especially on unseen\nwords) in comparison to a baseline that does not use context.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 14:38:37 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 16:43:35 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 12:50:21 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Bergmanis", "Toms", ""], ["Goldwater", "Sharon", ""]]}, {"id": "1904.01500", "submitter": "Robert Schwarzenberg", "authors": "Robert Schwarzenberg, Lisa Raithel, David Harbecke", "title": "Neural Vector Conceptualization for Word Vector Space Interpretation", "comments": "NAACL-HLT 2019 Workshop on Evaluating Vector Space Representations\n  for NLP (RepEval)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed word vector spaces are considered hard to interpret which hinders\nthe understanding of natural language processing (NLP) models. In this work, we\nintroduce a new method to interpret arbitrary samples from a word vector space.\nTo this end, we train a neural model to conceptualize word vectors, which means\nthat it activates higher order concepts it recognizes in a given vector.\nContrary to prior approaches, our model operates in the original vector space\nand is capable of learning non-linear relations between word vectors and\nconcepts. Furthermore, we show that it produces considerably less entropic\nconcept activation profiles than the popular cosine similarity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:39:27 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Schwarzenberg", "Robert", ""], ["Raithel", "Lisa", ""], ["Harbecke", "David", ""]]}, {"id": "1904.01548", "submitter": "Dan Schwartz", "authors": "Dan Schwartz and Tom Mitchell", "title": "Understanding language-elicited EEG data by predicting it from a\n  fine-tuned language model", "comments": "To appear in Proceedings of the 2019 Conference of the North American\n  Chapter of the Association for Computational Linguistics", "journal-ref": null, "doi": "10.18653/v1/N19-1005", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) recordings of brain activity taken while\nparticipants read or listen to language are widely used within the cognitive\nneuroscience and psycholinguistics communities as a tool to study language\ncomprehension. Several time-locked stereotyped EEG responses to\nword-presentations -- known collectively as event-related potentials (ERPs) --\nare thought to be markers for semantic or syntactic processes that take place\nduring comprehension. However, the characterization of each individual ERP in\nterms of what features of a stream of language trigger the response remains\ncontroversial. Improving this characterization would make ERPs a more useful\ntool for studying language comprehension. We take a step towards better\nunderstanding the ERPs by fine-tuning a language model to predict them. This\nnew approach to analysis shows for the first time that all of the ERPs are\npredictable from embeddings of a stream of language. Prior work has only found\ntwo of the ERPs to be predictable. In addition to this analysis, we examine\nwhich ERPs benefit from sharing parameters during joint training. We find that\ntwo pairs of ERPs previously identified in the literature as being related to\neach other benefit from joint training, while several other pairs of ERPs that\nbenefit from joint training are suggestive of potential relationships.\nExtensions of this analysis that further examine what kinds of information in\nthe model embeddings relate to each ERP have the potential to elucidate the\nprocesses involved in human language comprehension.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 17:02:19 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Schwartz", "Dan", ""], ["Mitchell", "Tom", ""]]}, {"id": "1904.01575", "submitter": "Cheng-I Lai", "authors": "Cheng-I Lai", "title": "Contrastive Predictive Coding Based Feature for Automatic Speaker\n  Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis describes our ongoing work on Contrastive Predictive Coding (CPC)\nfeatures for speaker verification. CPC is a recently proposed representation\nlearning framework based on predictive coding and noise contrastive estimation.\nWe focus on incorporating CPC features into the standard automatic speaker\nverification systems, and we present our methods, experiments, and analysis.\nThis thesis also details necessary background knowledge in past and recent work\non automatic speaker verification systems, conventional speech features, and\nthe motivation and techniques behind CPC.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 23:54:08 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Lai", "Cheng-I", ""]]}, {"id": "1904.01587", "submitter": "Liye Fu", "authors": "Liye Fu, Jonathan P. Chang, Cristian Danescu-Niculescu-Mizil", "title": "Asking the Right Question: Inferring Advice-Seeking Intentions from\n  Personal Narratives", "comments": "To appear in the Proceedings of NAACL 2019, 14 pages. Data, code and\n  additional information at https://github.com/CornellNLP/ASQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People often share personal narratives in order to seek advice from others.\nTo properly infer the narrator's intention, one needs to apply a certain degree\nof common sense and social intuition. To test the capabilities of NLP systems\nto recover such intuition, we introduce the new task of inferring what is the\nadvice-seeking goal behind a personal narrative. We formulate this as a cloze\ntest, where the goal is to identify which of two advice-seeking questions was\nremoved from a given narrative.\n  The main challenge in constructing this task is finding pairs of semantically\nplausible advice-seeking questions for given narratives. To address this\nchallenge, we devise a method that exploits commonalities in experiences people\nshare online to automatically extract pairs of questions that are appropriate\ncandidates for the cloze task. This results in a dataset of over 20,000\npersonal narratives, each matched with a pair of related advice-seeking\nquestions: one actually intended by the narrator, and the other one not. The\ndataset covers a very broad array of human experiences, from dating, to career\noptions, to stolen iPads. We use human annotation to determine the degree to\nwhich the task relies on common sense and social intuition in addition to a\nsemantic understanding of the narrative. By introducing several baselines for\nthis new task we demonstrate its feasibility and identify avenues for better\nmodeling the intention of the narrator.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 18:00:02 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Fu", "Liye", ""], ["Chang", "Jonathan P.", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1904.01596", "submitter": "Dorottya Demszky", "authors": "Dorottya Demszky, Nikhil Garg, Rob Voigt, James Zou, Matthew Gentzkow,\n  Jesse Shapiro, Dan Jurafsky", "title": "Analyzing Polarization in Social Media: Method and Application to Tweets\n  on 21 Mass Shootings", "comments": "NAACL 2019; code and data available at\n  https://github.com/ddemszky/framing-twitter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an NLP framework to uncover four linguistic dimensions of\npolitical polarization in social media: topic choice, framing, affect and\nillocutionary force. We quantify these aspects with existing lexical methods,\nand propose clustering of tweet embeddings as a means to identify salient\ntopics for analysis across events; human evaluations show that our approach\ngenerates more cohesive topics than traditional LDA-based models. We apply our\nmethods to study 4.4M tweets on 21 mass shootings. We provide evidence that the\ndiscussion of these events is highly polarized politically and that this\npolarization is primarily driven by partisan differences in framing rather than\ntopic choice. We identify framing devices, such as grounding and the\ncontrasting use of the terms \"terrorist\" and \"crazy\", that contribute to\npolarization. Results pertaining to topic choice, affect and illocutionary\nforce suggest that Republicans focus more on the shooter and event-specific\nfacts (news) while Democrats focus more on the victims and call for policy\nchanges. Our work contributes to a deeper understanding of the way group\ndivisions manifest in language and to computational methods for studying them.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 18:00:09 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 02:59:06 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Demszky", "Dorottya", ""], ["Garg", "Nikhil", ""], ["Voigt", "Rob", ""], ["Zou", "James", ""], ["Gentzkow", "Matthew", ""], ["Shapiro", "Jesse", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1904.01606", "submitter": "Eric Lehman", "authors": "Eric Lehman, Jay DeYoung, Regina Barzilay, Byron C. Wallace", "title": "Inferring Which Medical Treatments Work from Reports of Clinical Trials", "comments": "Accepted to NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do we know if a particular medical treatment actually works? Ideally one\nwould consult all available evidence from relevant clinical trials.\nUnfortunately, such results are primarily disseminated in natural language\nscientific articles, imposing substantial burden on those trying to make sense\nof them. In this paper, we present a new task and corpus for making this\nunstructured evidence actionable. The task entails inferring reported findings\nfrom a full-text article describing a randomized controlled trial (RCT) with\nrespect to a given intervention, comparator, and outcome of interest, e.g.,\ninferring if an article provides evidence supporting the use of aspirin to\nreduce risk of stroke, as compared to placebo.\n  We present a new corpus for this task comprising 10,000+ prompts coupled with\nfull-text articles describing RCTs. Results using a suite of models --- ranging\nfrom heuristic (rule-based) approaches to attentive neural architectures ---\ndemonstrate the difficulty of the task, which we believe largely owes to the\nlengthy, technical input texts. To facilitate further work on this important,\nchallenging problem we make the corpus, documentation, a website and\nleaderboard, and code for baselines and evaluation available at\nhttp://evidence-inference.ebm-nlp.com/.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 18:17:49 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 18:31:06 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Lehman", "Eric", ""], ["DeYoung", "Jay", ""], ["Barzilay", "Regina", ""], ["Wallace", "Byron C.", ""]]}, {"id": "1904.01608", "submitter": "Arman Cohan", "authors": "Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady", "title": "Structural Scaffolds for Citation Intent Classification in Scientific\n  Publications", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the intent of a citation in scientific papers (e.g., background\ninformation, use of methods, comparing results) is critical for machine reading\nof individual publications and automated analysis of the scientific literature.\nWe propose structural scaffolds, a multitask model to incorporate structural\ninformation of scientific papers into citations for effective classification of\ncitation intents. Our model achieves a new state-of-the-art on an existing ACL\nanthology dataset (ACL-ARC) with a 13.3% absolute increase in F1 score, without\nrelying on external linguistic resources or hand-engineered features as done in\nexisting methods. In addition, we introduce a new dataset of citation intents\n(SciCite) which is more than five times larger and covers multiple scientific\ndomains compared with existing datasets. Our code and data are available at:\nhttps://github.com/allenai/scicite.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 18:22:09 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 16:37:20 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Cohan", "Arman", ""], ["Ammar", "Waleed", ""], ["van Zuylen", "Madeleine", ""], ["Cady", "Field", ""]]}, {"id": "1904.01617", "submitter": "Timo Schick", "authors": "Timo Schick, Hinrich Sch\\\"utze", "title": "Attentive Mimicking: Better Word Embeddings by Attending to Informative\n  Contexts", "comments": "Accepted at NAACL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning high-quality embeddings for rare words is a hard problem because of\nsparse context information. Mimicking (Pinter et al., 2017) has been proposed\nas a solution: given embeddings learned by a standard algorithm, a model is\nfirst trained to reproduce embeddings of frequent words from their surface form\nand then used to compute embeddings for rare words. In this paper, we introduce\nattentive mimicking: the mimicking model is given access not only to a word's\nsurface form, but also to all available contexts and learns to attend to the\nmost informative and reliable contexts for computing an embedding. In an\nevaluation on four tasks, we show that attentive mimicking outperforms previous\nwork for both rare and medium-frequency words. Thus, compared to previous work,\nattentive mimicking improves embeddings for a much larger part of the\nvocabulary, including the medium-frequency range.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 18:44:04 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 10:59:05 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Schick", "Timo", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1904.01628", "submitter": "Adam Lauretig", "authors": "Adam M. Lauretig", "title": "Identification, Interpretability, and Bayesian Word Embeddings", "comments": "Accepted to the Third Workshop on Natural Language Processing and\n  Computational Social Science at NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social scientists have recently turned to analyzing text using tools from\nnatural language processing like word embeddings to measure concepts like\nideology, bias, and affinity. However, word embeddings are difficult to use in\nthe regression framework familiar to social scientists: embeddings are are\nneither identified, nor directly interpretable. I offer two advances on\nstandard embedding models to remedy these problems. First, I develop Bayesian\nWord Embeddings with Automatic Relevance Determination priors, relaxing the\nassumption that all embedding dimensions have equal weight. Second, I apply\nwork identifying latent variable models to anchor the dimensions of the\nresulting embeddings, identifying them, and making them interpretable and\nusable in a regression. I then apply this model and anchoring approach to two\ncases, the shift in internationalist rhetoric in the American presidents'\ninaugural addresses, and the relationship between bellicosity in American\nforeign policy decision-makers' deliberations. I find that inaugural addresses\nbecame less internationalist after 1945, which goes against the conventional\nwisdom, and that an increase in bellicosity is associated with an increase in\nhostile actions by the United States, showing that elite deliberations are not\ncheap talk, and helping confirm the validity of the model.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 19:12:35 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Lauretig", "Adam M.", ""]]}, {"id": "1904.01650", "submitter": "Rosario Scalise", "authors": "Rosario Scalise, Jesse Thomason, Yonatan Bisk and Siddhartha Srinivasa", "title": "Improving Robot Success Detection using Static Object Data", "comments": "IROS 2019 + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use static object data to improve success detection for stacking objects\non and nesting objects in one another. Such actions are necessary for certain\nrobotics tasks, e.g., clearing a dining table or packing a warehouse bin.\nHowever, using an RGB-D camera to detect success can be insufficient:\nsame-colored objects can be difficult to differentiate, and reflective\nsilverware cause noisy depth camera perception. We show that adding static data\nabout the objects themselves improves the performance of an end-to-end pipeline\nfor classifying action outcomes. Images of the objects, and language\nexpressions describing them, encode prior geometry, shape, and size information\nthat refine classification accuracy. We collect over 13 hours of egocentric\nmanipulation data for training a model to reason about whether a robot\nsuccessfully placed unseen objects in or on one another. The model achieves up\nto a 57% absolute gain over the task baseline on pairs of previously unseen\nobjects.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:18:52 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 18:18:01 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Scalise", "Rosario", ""], ["Thomason", "Jesse", ""], ["Bisk", "Yonatan", ""], ["Srinivasa", "Siddhartha", ""]]}, {"id": "1904.01655", "submitter": "Surabhi Datta", "authors": "Surabhi Datta, Elmer V Bernstam, Kirk Roberts", "title": "A frame semantic overview of NLP-based information extraction for\n  cancer-related EHR notes", "comments": "2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: There is a lot of information about cancer in Electronic Health\nRecord (EHR) notes that can be useful for biomedical research provided natural\nlanguage processing (NLP) methods are available to extract and structure this\ninformation. In this paper, we present a scoping review of existing clinical\nNLP literature for cancer. Methods: We identified studies describing an NLP\nmethod to extract specific cancer-related information from EHR sources from\nPubMed, Google Scholar, ACL Anthology, and existing reviews. Two exclusion\ncriteria were used in this study. We excluded articles where the extraction\ntechniques used were too broad to be represented as frames and also where very\nlow-level extraction methods were used. 79 articles were included in the final\nreview. We organized this information according to frame semantic principles to\nhelp identify common areas of overlap and potential gaps. Results: Frames were\ncreated from the reviewed articles pertaining to cancer information such as\ncancer diagnosis, tumor description, cancer procedure, breast cancer diagnosis,\nprostate cancer diagnosis and pain in prostate cancer patients. These frames\nincluded both a definition as well as specific frame elements (i.e. extractable\nattributes). We found that cancer diagnosis was the most common frame among the\nreviewed papers (36 out of 79), with recent work focusing on extracting\ninformation related to treatment and breast cancer diagnosis. Conclusion: The\nlist of common frames described in this paper identifies important\ncancer-related information extracted by existing NLP techniques and serves as a\nuseful resource for future researchers requiring cancer information extracted\nfrom EHR notes. We also argue, due to the heavy duplication of cancer NLP\nsystems, that a general purpose resource of annotated cancer frames and\ncorresponding NLP tools would be valuable.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:27:42 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Datta", "Surabhi", ""], ["Bernstam", "Elmer V", ""], ["Roberts", "Kirk", ""]]}, {"id": "1904.01664", "submitter": "Katherine Metcalf", "authors": "Katherine Metcalf, Barry-John Theobald, Garrett Weinberg, Robert Lee,\n  Ing-Marie Jonsson, Russ Webb, Nicholas Apostoloff", "title": "Mirroring to Build Trust in Digital Assistants", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe experiments towards building a conversational digital assistant\nthat considers the preferred conversational style of the user. In particular,\nthese experiments are designed to measure whether users prefer and trust an\nassistant whose conversational style matches their own. To this end we\nconducted a user study where subjects interacted with a digital assistant that\nresponded in a way that either matched their conversational style, or did not.\nUsing self-reported personality attributes and subjects' feedback on the\ninteractions, we built models that can reliably predict a user's preferred\nconversational style.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:51:27 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Metcalf", "Katherine", ""], ["Theobald", "Barry-John", ""], ["Weinberg", "Garrett", ""], ["Lee", "Robert", ""], ["Jonsson", "Ing-Marie", ""], ["Webb", "Russ", ""], ["Apostoloff", "Nicholas", ""]]}, {"id": "1904.01684", "submitter": "Jekaterina Novikova Dr.", "authors": "Aparna Balagopalan, Ksenia Shkaruta, Jekaterina Novikova", "title": "Impact of ASR on Alzheimer's Disease Detection: All Errors are Equal,\n  but Deletions are More Equal than Others", "comments": "EMNLP Workshop on Noisy User-generated Text (W-NUT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Speech Recognition (ASR) is a critical component of any\nfully-automated speech-based dementia detection model. However, despite years\nof speech recognition research, little is known about the impact of ASR\naccuracy on dementia detection. In this paper, we experiment with controlled\namounts of artificially generated ASR errors and investigate their influence on\ndementia detection. We find that deletion errors affect detection performance\nthe most, due to their impact on the features of syntactic complexity and\ndiscourse representation in speech. We show the trend to be generalisable\nacross two different datasets for cognitive impairment detection. As a\nconclusion, we propose optimising the ASR to reflect a higher penalty for\ndeletion errors in order to improve dementia detection performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 21:59:35 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 00:44:41 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 18:08:35 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Balagopalan", "Aparna", ""], ["Shkaruta", "Ksenia", ""], ["Novikova", "Jekaterina", ""]]}, {"id": "1904.01689", "submitter": "Brent Hecht", "authors": "B. Hecht and D. Gergle", "title": "The Tower of Babel Meets Web 2.0: User-Generated Content and its\n  Applications in a Multilingual Context", "comments": "CHI 2010 Proceedings of the SIGCHI Conference on Human Factors in\n  Computing Systems", "journal-ref": null, "doi": "10.1145/1753326.1753370", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explores language's fragmenting effect on user-generated content\nby examining the diversity of knowledge representations across 25 different\nWikipedia language editions. This diversity is measured at two levels: the\nconcepts that are included in each edition and the ways in which these concepts\nare described. We demonstrate that the diversity present is greater than has\nbeen presumed in the literature and has a significant influence on applications\nthat use Wikipedia as a source of world knowledge. We close by explicating how\nknowledge diversity can be beneficially leveraged to create \"culturally-aware\napplications\" and \"hyperlingual applications\".\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:23:05 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Hecht", "B.", ""], ["Gergle", "D.", ""]]}, {"id": "1904.01735", "submitter": "Jianguo Zhang", "authors": "Jian-Guo Zhang, Pengcheng Zou, Zhao Li, Yao Wan, Xiuming Pan, Yu Gong,\n  Philip S. Yu", "title": "Multi-Modal Generative Adversarial Network for Short Product Title\n  Generation in Mobile E-Commerce", "comments": "Accepted by NAACL-HLT 2019. arXiv admin note: substantial text\n  overlap with arXiv:1811.04498", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, more and more customers browse and purchase products in favor of\nusing mobile E-Commerce Apps such as Taobao and Amazon. Since merchants are\nusually inclined to describe redundant and over-informative product titles to\nattract attentions from customers, it is important to concisely display short\nproduct titles on limited screen of mobile phones. To address this discrepancy,\nprevious studies mainly consider textual information of long product titles and\nlacks of human-like view during training and evaluation process. In this paper,\nwe propose a Multi-Modal Generative Adversarial Network (MM-GAN) for short\nproduct title generation in E-Commerce, which innovatively incorporates image\ninformation and attribute tags from product, as well as textual information\nfrom original long titles. MM-GAN poses short title generation as a\nreinforcement learning process, where the generated titles are evaluated by the\ndiscriminator in a human-like view. Extensive experiments on a large-scale\nE-Commerce dataset demonstrate that our algorithm outperforms other\nstate-of-the-art methods. Moreover, we deploy our model into a real-world\nonline E-Commerce environment and effectively boost the performance of click\nthrough rate and click conversion rate by 1.66% and 1.87%, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 01:29:48 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Zhang", "Jian-Guo", ""], ["Zou", "Pengcheng", ""], ["Li", "Zhao", ""], ["Wan", "Yao", ""], ["Pan", "Xiuming", ""], ["Gong", "Yu", ""], ["Yu", "Philip S.", ""]]}, {"id": "1904.01783", "submitter": "Heng Wang", "authors": "Jinbin Zhang, Heng Wang", "title": "Multi-task Learning for Chinese Word Usage Errors Detection", "comments": "4 pages, 2 figures, 1 table, has been accepted as a conference paper\n  of the 3rd IEEE International Conference on Computational Intelligence and\n  Applications (ICCIA 2018)", "journal-ref": "Proceedings of the 3rd IEEE International Conference on\n  Computational Intelligence and Applications (ICCIA 2018)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese word usage errors often occur in non-native Chinese learners'\nwriting. It is very helpful for non-native Chinese learners to detect them\nautomatically when learning writing. In this paper, we propose a novel\napproach, which takes advantages of different auxiliary tasks, such as\nPOS-tagging prediction and word log frequency prediction, to help the task of\nChinese word usage error detection. With the help of these auxiliary tasks, we\nachieve the state-of-the-art results on the performances on the HSK corpus\ndata, without any other extra data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 05:58:07 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Zhang", "Jinbin", ""], ["Wang", "Heng", ""]]}, {"id": "1904.01825", "submitter": "Quynh Ngoc Thi Do", "authors": "Quynh Ngoc Thi Do, Judith Gaspers", "title": "Cross-lingual transfer learning for spoken language understanding", "comments": "accepted at ICASSP, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, spoken language understanding (SLU) models are trained on\nannotated data which are costly to gather. Aiming to reduce data needs for\nbootstrapping a SLU system for a new language, we present a simple but\neffective weight transfer approach using data from another language. The\napproach is evaluated with our promising multi-task SLU framework developed\ntowards different languages. We evaluate our approach on the ATIS and a\nreal-world SLU dataset, showing that i) our monolingual models outperform the\nstate-of-the-art, ii) we can reduce data amounts needed for bootstrapping a SLU\nsystem for a new language greatly, and iii) while multitask training improves\nover separate training, different weight transfer settings may work best for\ndifferent SLU modules.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 08:11:57 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Do", "Quynh Ngoc Thi", ""], ["Gaspers", "Judith", ""]]}, {"id": "1904.01873", "submitter": "Romain Robbes", "authors": "Hlib Babii, Andrea Janes, Romain Robbes", "title": "Modeling Vocabulary for Big Code Machine Learning", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When building machine learning models that operate on source code, several\ndecisions have to be made to model source-code vocabulary. These decisions can\nhave a large impact: some can lead to not being able to train models at all,\nothers significantly affect performance, particularly for Neural Language\nModels. Yet, these decisions are not often fully described. This paper lists\nimportant modeling choices for source code vocabulary, and explores their\nimpact on the resulting vocabulary on a large-scale corpus of 14,436 projects.\nWe show that a subset of decisions have decisive characteristics, allowing to\ntrain accurate Neural Language Models quickly on a large corpus of 10,106\nprojects.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 09:27:57 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Babii", "Hlib", ""], ["Janes", "Andrea", ""], ["Robbes", "Romain", ""]]}, {"id": "1904.01938", "submitter": "Shuohang Wang", "authors": "Shuohang Wang, Sheng Zhang, Yelong Shen, Xiaodong Liu, Jingjing Liu,\n  Jianfeng Gao, Jing Jiang", "title": "Unsupervised Deep Structured Semantic Models for Commonsense Reasoning", "comments": "To appear in NAACL 2019, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense reasoning is fundamental to natural language understanding. While\ntraditional methods rely heavily on human-crafted features and knowledge bases,\nwe explore learning commonsense knowledge from a large amount of raw text via\nunsupervised learning. We propose two neural network models based on the Deep\nStructured Semantic Models (DSSM) framework to tackle two classic commonsense\nreasoning tasks, Winograd Schema challenges (WSC) and Pronoun Disambiguation\n(PDP). Evaluation shows that the proposed models effectively capture contextual\ninformation in the sentence and co-reference information between pronouns and\nnouns, and achieve significant improvement over previous state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 11:57:25 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wang", "Shuohang", ""], ["Zhang", "Sheng", ""], ["Shen", "Yelong", ""], ["Liu", "Xiaodong", ""], ["Liu", "Jingjing", ""], ["Gao", "Jianfeng", ""], ["Jiang", "Jing", ""]]}, {"id": "1904.01989", "submitter": "Manuel Mager", "authors": "Manuel Mager, \\\"Ozlem \\c{C}etino\\u{g}lu, Katharina Kann", "title": "Subword-Level Language Identification for Intra-Word Code-Switching", "comments": "NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Language identification for code-switching (CS), the phenomenon of\nalternating between two or more languages in conversations, has traditionally\nbeen approached under the assumption of a single language per token. However,\nif at least one language is morphologically rich, a large number of words can\nbe composed of morphemes from more than one language (intra-word CS). In this\npaper, we extend the language identification task to the subword-level, such\nthat it includes splitting mixed words while tagging each part with a language\nID. We further propose a model for this task, which is based on a segmental\nrecurrent neural network. In experiments on a new Spanish--Wixarika dataset and\non an adapted German--Turkish dataset, our proposed model performs slightly\nbetter than or roughly on par with our best baseline, respectively. Considering\nonly mixed words, however, it strongly outperforms all baselines.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 13:08:12 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Mager", "Manuel", ""], ["\u00c7etino\u011flu", "\u00d6zlem", ""], ["Kann", "Katharina", ""]]}, {"id": "1904.02020", "submitter": "Zita Marinho", "authors": "Afonso Mendes and Shashi Narayan and Sebasti\\~ao Miranda and Zita\n  Marinho and Andr\\'e F. T. Martins and Shay B. Cohen", "title": "Jointly Extracting and Compressing Documents with Summary State\n  Representations", "comments": null, "journal-ref": "NAACL 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new neural model for text summarization that first extracts\nsentences from a document and then compresses them. The proposed model offers a\nbalance that sidesteps the difficulties in abstractive methods while generating\nmore concise summaries than extractive methods. In addition, our model\ndynamically determines the length of the output summary based on the gold\nsummaries it observes during training and does not require length constraints\ntypical to extractive summarization. The model achieves state-of-the-art\nresults on the CNN/DailyMail and Newsroom datasets, improving over current\nextractive and abstractive methods. Human evaluations demonstrate that our\nmodel generates concise and informative summaries. We also make available a new\ndataset of oracle compressive summaries derived automatically from the\nCNN/DailyMail reference summaries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:24:04 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 16:09:19 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Mendes", "Afonso", ""], ["Narayan", "Shashi", ""], ["Miranda", "Sebasti\u00e3o", ""], ["Marinho", "Zita", ""], ["Martins", "Andr\u00e9 F. T.", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1904.02032", "submitter": "Justin Wood", "authors": "Justin Wood, Nicholas J. Matiasz, Alcino J. Silva, William Hsu, Alexej\n  Abyzov, Wei Wang", "title": "OpBerg: Discovering causal sentences using optimal alignments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The biological literature is rich with sentences that describe causal\nrelations. Methods that automatically extract such sentences can help\nbiologists to synthesize the literature and even discover latent relations that\nhad not been articulated explicitly. Current methods for extracting causal\nsentences are based on either machine learning or a predefined database of\ncausal terms. Machine learning approaches require a large set of labeled\ntraining data and can be susceptible to noise. Methods based on predefined\ndatabases are limited by the quality of their curation and are unable to\ncapture new concepts or mistakes in the input. We address these challenges by\nadapting and improving a method designed for a seemingly unrelated problem:\nfinding alignments between genomic sequences. This paper presents a novel and\noutperforming method for extracting causal relations from text by aligning the\npart-of-speech representations of an input set with that of known causal\nsentences. Our experiments show that when applied to the task of finding causal\nsentences in biological literature, our method improves on the accuracy of\nother methods in a computationally efficient manner.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:36:49 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wood", "Justin", ""], ["Matiasz", "Nicholas J.", ""], ["Silva", "Alcino J.", ""], ["Hsu", "William", ""], ["Abyzov", "Alexej", ""], ["Wang", "Wei", ""]]}, {"id": "1904.02036", "submitter": "Marcel Bollmann", "authors": "Marcel Bollmann", "title": "A Large-Scale Comparison of Historical Text Normalization Systems", "comments": "Accepted at NAACL 2019", "journal-ref": null, "doi": "10.18653/v1/N19-1389", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is no consensus on the state-of-the-art approach to historical text\nnormalization. Many techniques have been proposed, including rule-based\nmethods, distance metrics, character-based statistical machine translation, and\nneural encoder--decoder models, but studies have used different datasets,\ndifferent evaluation methods, and have come to different conclusions. This\npaper presents the largest study of historical text normalization done so far.\nWe critically survey the existing literature and report experiments on eight\nlanguages, comparing systems spanning all categories of proposed normalization\ntechniques, analysing the effect of training data quantity, and using different\nevaluation methods. The datasets and scripts are made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:46:38 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Bollmann", "Marcel", ""]]}, {"id": "1904.02037", "submitter": "Zita Marinho", "authors": "Sebasti\\~ao Miranda and David Nogueira and Afonso Mendes and Andreas\n  Vlachos and Andrew Secker and Rebecca Garrett and Jeff Mitchel and Zita\n  Marinho", "title": "Automated Fact Checking in the News Room", "comments": null, "journal-ref": "WEBCONF 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact checking is an essential task in journalism; its importance has been\nhighlighted due to recently increased concerns and efforts in combating\nmisinformation. In this paper, we present an automated fact-checking platform\nwhich given a claim, it retrieves relevant textual evidence from a document\ncollection, predicts whether each piece of evidence supports or refutes the\nclaim, and returns a final verdict. We describe the architecture of the system\nand the user interface, focusing on the choices made to improve its\nuser-friendliness and transparency. We conduct a user study of the\nfact-checking platform in a journalistic setting: we integrated it with a\ncollection of news articles and provide an evaluation of the platform using\nfeedback from journalists in their workflow. We found that the predictions of\nour platform were correct 58\\% of the time, and 59\\% of the returned evidence\nwas relevant.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:46:44 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Miranda", "Sebasti\u00e3o", ""], ["Nogueira", "David", ""], ["Mendes", "Afonso", ""], ["Vlachos", "Andreas", ""], ["Secker", "Andrew", ""], ["Garrett", "Rebecca", ""], ["Mitchel", "Jeff", ""], ["Marinho", "Zita", ""]]}, {"id": "1904.02099", "submitter": "Daniel Kondratyuk", "authors": "Dan Kondratyuk and Milan Straka", "title": "75 Languages, 1 Model: Parsing Universal Dependencies Universally", "comments": "Accepted for publication at EMNLP 2019. 17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present UDify, a multilingual multi-task model capable of accurately\npredicting universal part-of-speech, morphological features, lemmas, and\ndependency trees simultaneously for all 124 Universal Dependencies treebanks\nacross 75 languages. By leveraging a multilingual BERT self-attention model\npretrained on 104 languages, we found that fine-tuning it on all datasets\nconcatenated together with simple softmax classifiers for each UD task can\nresult in state-of-the-art UPOS, UFeats, Lemmas, UAS, and LAS scores, without\nrequiring any recurrent or language-specific components. We evaluate UDify for\nmultilingual learning, showing that low-resource languages benefit the most\nfrom cross-linguistic annotations. We also evaluate for zero-shot learning,\nwith results suggesting that multilingual training provides strong UD\npredictions even for languages that neither UDify nor BERT have ever been\ntrained on. Code for UDify is available at\nhttps://github.com/hyperparticle/udify.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:52:55 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 04:14:25 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2019 23:19:00 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kondratyuk", "Dan", ""], ["Straka", "Milan", ""]]}, {"id": "1904.02141", "submitter": "Yuying Zhu", "authors": "Yuying Zhu, Guoxin Wang, B\\\"orje F. Karlsson", "title": "CAN-NER: Convolutional Attention Network for Chinese Named Entity\n  Recognition", "comments": "This paper is accepted by NAACL-HLT 2019. The code is available at\n  https://github.com/microsoft/vert-papers/tree/master/papers/CAN-NER", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) in Chinese is essential but difficult because\nof the lack of natural delimiters. Therefore, Chinese Word Segmentation (CWS)\nis usually considered as the first step for Chinese NER. However, models based\non word-level embeddings and lexicon features often suffer from segmentation\nerrors and out-of-vocabulary (OOV) words. In this paper, we investigate a\nConvolutional Attention Network called CAN for Chinese NER, which consists of a\ncharacter-based convolutional neural network (CNN) with local-attention layer\nand a gated recurrent unit (GRU) with global self-attention layer to capture\nthe information from adjacent characters and sentence contexts. Also, compared\nto other models, not depending on any external resources like lexicons and\nemploying small size of char embeddings make our model more practical.\nExtensive experimental results show that our approach outperforms\nstate-of-the-art methods without word embedding and external lexicon resources\non different domain datasets including Weibo, MSRA and Chinese Resume NER\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:56:38 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 08:10:55 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 14:10:33 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhu", "Yuying", ""], ["Wang", "Guoxin", ""], ["Karlsson", "B\u00f6rje F.", ""]]}, {"id": "1904.02142", "submitter": "Andrew Drozdov", "authors": "Andrew Drozdov, Pat Verga, Mohit Yadav, Mohit Iyyer, Andrew McCallum", "title": "Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive\n  Autoencoders", "comments": "14 pages, 8 figures, 8 tables. NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce deep inside-outside recursive autoencoders (DIORA), a\nfully-unsupervised method for discovering syntax that simultaneously learns\nrepresentations for constituents within the induced tree. Our approach predicts\neach word in an input sentence conditioned on the rest of the sentence and uses\ninside-outside dynamic programming to consider all possible binary trees over\nthe sentence. At test time the CKY algorithm extracts the highest scoring\nparse. DIORA achieves a new state-of-the-art F1 in unsupervised binary\nconstituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:56:48 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 22:35:24 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Drozdov", "Andrew", ""], ["Verga", "Pat", ""], ["Yadav", "Mohit", ""], ["Iyyer", "Mohit", ""], ["McCallum", "Andrew", ""]]}, {"id": "1904.02181", "submitter": "Qiao Jin", "authors": "Qiao Jin, Bhuwan Dhingra, William W. Cohen, Xinghua Lu", "title": "Probing Biomedical Embeddings from Language Models", "comments": "NAACL-HLT 2019 Workshop on Evaluating Vector Space Representations\n  for NLP (RepEval)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualized word embeddings derived from pre-trained language models (LMs)\nshow significant improvements on downstream NLP tasks. Pre-training on\ndomain-specific corpora, such as biomedical articles, further improves their\nperformance. In this paper, we conduct probing experiments to determine what\nadditional information is carried intrinsically by the in-domain trained\ncontextualized embeddings. For this we use the pre-trained LMs as fixed feature\nextractors and restrict the downstream task models to not have additional\nsequence modeling layers. We compare BERT, ELMo, BioBERT and BioELMo, a\nbiomedical version of ELMo trained on 10M PubMed abstracts. Surprisingly, while\nfine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a\nfixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We\nuse visualization and nearest neighbor analysis to show that better encoding of\nentity-type and relational information leads to this superiority.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 18:05:02 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Jin", "Qiao", ""], ["Dhingra", "Bhuwan", ""], ["Cohen", "William W.", ""], ["Lu", "Xinghua", ""]]}, {"id": "1904.02210", "submitter": "Oliver Adams", "authors": "Oliver Adams, Matthew Wiesner, Shinji Watanabe, David Yarowsky", "title": "Massively Multilingual Adversarial Speech Recognition", "comments": "Accepted at NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on adaptation of multilingual end-to-end speech recognition models\ntrained on as many as 100 languages. Our findings shed light on the relative\nimportance of similarity between the target and pretraining languages along the\ndimensions of phonetics, phonology, language family, geographical location, and\northography. In this context, experiments demonstrate the effectiveness of two\nadditional pretraining objectives in encouraging language-independent encoder\nrepresentations: a context-independent phoneme objective paired with a\nlanguage-adversarial classification objective.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:28:53 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Adams", "Oliver", ""], ["Wiesner", "Matthew", ""], ["Watanabe", "Shinji", ""], ["Yarowsky", "David", ""]]}, {"id": "1904.02228", "submitter": "Peter Potash", "authors": "Peter Potash", "title": "The Effect of Downstream Classification Tasks for Evaluating Sentence\n  Embeddings", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One popular method for quantitatively evaluating the utility of sentence\nembeddings involves using them in downstream language processing tasks that\nrequire sentence representations as input. One simple such task is\nclassification, where the sentence representations are used to train and test\nmodels on several classification datasets. We argue that by evaluating sentence\nrepresentations in such a manner, the goal of the representations becomes\nlearning a low-dimensional factorization of a sentence-task label matrix. We\nshow how characteristics of this matrix can affect the ability for a\nlow-dimensional factorization to perform as sentence representations in a suite\nof classification tasks. Primarily, sentences that have more labels across all\npossible classification tasks have a higher reconstruction loss, however the\ngeneral nature of this effect is ultimately dependent on the overall\ndistribution of labels across all possible sentences.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 20:12:10 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 14:10:45 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Potash", "Peter", ""]]}, {"id": "1904.02232", "submitter": "Hu Xu", "authors": "Hu Xu, Bing Liu, Lei Shu, Philip S. Yu", "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based\n  Sentiment Analysis", "comments": "accepted by NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question-answering plays an important role in e-commerce as it allows\npotential customers to actively seek crucial information about products or\nservices to help their purchase decision making. Inspired by the recent success\nof machine reading comprehension (MRC) on formal documents, this paper explores\nthe potential of turning customer reviews into a large source of knowledge that\ncan be exploited to answer user questions.~We call this problem Review Reading\nComprehension (RRC). To the best of our knowledge, no existing work has been\ndone on RRC. In this work, we first build an RRC dataset called ReviewRC based\non a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has\nlimited training examples for RRC (and also for aspect-based sentiment\nanalysis), we then explore a novel post-training approach on the popular\nlanguage model BERT to enhance the performance of fine-tuning of BERT for RRC.\nTo show the generality of the approach, the proposed post-training is also\napplied to some other review-based tasks such as aspect extraction and aspect\nsentiment classification in aspect-based sentiment analysis. Experimental\nresults demonstrate that the proposed post-training is highly effective. The\ndatasets and code are available at https://www.cs.uic.edu/~hxu/.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 20:29:10 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 02:44:35 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Xu", "Hu", ""], ["Liu", "Bing", ""], ["Shu", "Lei", ""], ["Yu", "Philip S.", ""]]}, {"id": "1904.02244", "submitter": "Mamoru Komachi", "authors": "Hikaru Omori and Mamoru Komachi", "title": "Multi-task Learning for Japanese Predicate Argument Structure Analysis", "comments": "10 pages; NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An event-noun is a noun that has an argument structure similar to a\npredicate. Recent works, including those considered state-of-the-art, ignore\nevent-nouns or build a single model for solving both Japanese predicate\nargument structure analysis (PASA) and event-noun argument structure analysis\n(ENASA). However, because there are interactions between predicates and\nevent-nouns, it is not sufficient to target only predicates. To address this\nproblem, we present a multi-task learning method for PASA and ENASA. Our\nmulti-task models improved the performance of both tasks compared to a\nsingle-task model by sharing knowledge from each task. Moreover, in PASA, our\nmodels achieved state-of-the-art results in overall F1 scores on the NAIST Text\nCorpus. In addition, this is the first work to employ neural networks in ENASA.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 21:29:14 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Omori", "Hikaru", ""], ["Komachi", "Mamoru", ""]]}, {"id": "1904.02246", "submitter": "Jesse Mu", "authors": "Jesse Mu, Helen Yannakoudakis, Ekaterina Shutova", "title": "Learning Outside the Box: Discourse-level Features Improve Metaphor\n  Identification", "comments": "NAACL 2019; 6 pages; code available at\n  https://github.com/jayelm/broader-metaphor; v2 updates affiliations and\n  acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current approaches to metaphor identification use restricted linguistic\ncontexts, e.g. by considering only a verb's arguments or the sentence\ncontaining a phrase. Inspired by pragmatic accounts of metaphor, we argue that\nbroader discourse features are crucial for better metaphor identification. We\ntrain simple gradient boosting classifiers on representations of an utterance\nand its surrounding discourse learned with a variety of document embedding\nmethods, obtaining near state-of-the-art results on the 2018 VU Amsterdam\nmetaphor identification task without the complex metaphor-specific features or\ndeep neural architectures employed by other systems. A qualitative analysis\nfurther confirms the need for broader context in metaphor processing.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 21:38:25 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 22:28:01 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Mu", "Jesse", ""], ["Yannakoudakis", "Helen", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "1904.02281", "submitter": "Sudha Rao", "authors": "Sudha Rao and Hal Daum\\'e III", "title": "Answer-based Adversarial Training for Generating Clarification Questions", "comments": "Accepted at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for generating clarification questions with the goal\nof eliciting new information that would make the given textual context more\ncomplete. We propose that modeling hypothetical answers (to clarification\nquestions) as latent variables can guide our approach into generating more\nuseful clarification questions. We develop a Generative Adversarial Network\n(GAN) where the generator is a sequence-to-sequence model and the discriminator\nis a utility function that models the value of updating the context with the\nanswer to the clarification question. We evaluate on two datasets, using both\nautomatic metrics and human judgments of usefulness, specificity and relevance,\nshowing that our approach outperforms both a retrieval-based model and\nablations that exclude the utility model and the adversarial training.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 00:30:20 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Rao", "Sudha", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "1904.02293", "submitter": "Akshay Budhkar", "authors": "Akshay Budhkar, Krishnapriya Vishnubhotla, Safwan Hossain, Frank\n  Rudzicz", "title": "Generative Adversarial Networks for text using word2vec intermediaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have shown considerable success,\nespecially in the realistic generation of images. In this work, we apply\nsimilar techniques for the generation of text. We propose a novel approach to\nhandle the discrete nature of text, during training, using word embeddings. Our\nmethod is agnostic to vocabulary size and achieves competitive results relative\nto methods with various discrete gradient estimators.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 01:17:29 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Budhkar", "Akshay", ""], ["Vishnubhotla", "Krishnapriya", ""], ["Hossain", "Safwan", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1904.02295", "submitter": "Remi Mir", "authors": "Remi Mir, Bjarke Felbo, Nick Obradovich, Iyad Rahwan", "title": "Evaluating Style Transfer for Text", "comments": "To appear in Proceedings of the 2019 Conference of the North American\n  Chapter of the Association for Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in the area of style transfer for text is currently bottlenecked by\na lack of standard evaluation practices. This paper aims to alleviate this\nissue by experimentally identifying best practices with a Yelp sentiment\ndataset. We specify three aspects of interest (style transfer intensity,\ncontent preservation, and naturalness) and show how to obtain more reliable\nmeasures of them from human evaluation than in previous work. We propose a set\nof metrics for automated evaluation and demonstrate that they are more strongly\ncorrelated and in agreement with human judgment: direction-corrected Earth\nMover's Distance, Word Mover's Distance on style-masked texts, and adversarial\nclassification for the respective aspects. We also show that the three examined\nmodels exhibit tradeoffs between aspects of interest, demonstrating the\nimportance of evaluating style transfer models at specific points of their\ntradeoff plots. We release software with our evaluation metrics to facilitate\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 01:18:56 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Mir", "Remi", ""], ["Felbo", "Bjarke", ""], ["Obradovich", "Nick", ""], ["Rahwan", "Iyad", ""]]}, {"id": "1904.02306", "submitter": "Chaitanya Malaviya", "authors": "Chaitanya Malaviya, Shijie Wu, Ryan Cotterell", "title": "A Simple Joint Model for Improved Contextual Neural Lemmatization", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  English verbs have multiple forms. For instance, talk may also appear as\ntalks, talked or talking, depending on the context. The NLP task of\nlemmatization seeks to map these diverse forms back to a canonical one, known\nas the lemma. We present a simple joint neural model for lemmatization and\nmorphological tagging that achieves state-of-the-art results on 20 languages\nfrom the Universal Dependencies corpora. Our paper describes the model in\naddition to training and decoding procedures. Error analysis indicates that\njoint morphological tagging and lemmatization is especially helpful in\nlow-resource lemmatization and languages that display a larger degree of\nmorphological complexity. Code and pre-trained models are available at\nhttps://sigmorphon.github.io/sharedtasks/2019/task2/.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 02:03:19 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 02:57:35 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 20:52:39 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 07:23:27 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Malaviya", "Chaitanya", ""], ["Wu", "Shijie", ""], ["Cotterell", "Ryan", ""]]}, {"id": "1904.02321", "submitter": "Fei Liu", "authors": "Kristjan Arumae and Fei Liu", "title": "Guiding Extractive Summarization with Question-Answering Rewards", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highlighting while reading is a natural behavior for people to track salient\ncontent of a document. It would be desirable to teach an extractive summarizer\nto do the same. However, a major obstacle to the development of a supervised\nsummarizer is the lack of ground-truth. Manual annotation of extraction units\nis cost-prohibitive, whereas acquiring labels by automatically aligning human\nabstracts and source documents can yield inferior results. In this paper we\ndescribe a novel framework to guide a supervised, extractive summarization\nsystem with question-answering rewards. We argue that quality summaries should\nserve as a document surrogate to answer important questions, and such\nquestion-answer pairs can be conveniently obtained from human abstracts. The\nsystem learns to promote summaries that are informative, fluent, and perform\ncompetitively on question-answering. Our results compare favorably with those\nreported by strong summarization baselines as evaluated by automatic metrics\nand human assessors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 02:57:59 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Arumae", "Kristjan", ""], ["Liu", "Fei", ""]]}, {"id": "1904.02331", "submitter": "Jiawei Wu", "authors": "Jiawei Wu, Xin Wang, William Yang Wang", "title": "Extract and Edit: An Alternative to Back-Translation for Unsupervised\n  Neural Machine Translation", "comments": "11 pages, 3 figures. Accepted to NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overreliance on large parallel corpora significantly limits the\napplicability of machine translation systems to the majority of language pairs.\nBack-translation has been dominantly used in previous approaches for\nunsupervised neural machine translation, where pseudo sentence pairs are\ngenerated to train the models with a reconstruction loss. However, the pseudo\nsentences are usually of low quality as translation errors accumulate during\ntraining. To avoid this fundamental issue, we propose an alternative but more\neffective approach, extract-edit, to extract and then edit real sentences from\nthe target monolingual corpora. Furthermore, we introduce a comparative\ntranslation loss to evaluate the translated target sentences and thus train the\nunsupervised translation systems. Experiments show that the proposed approach\nconsistently outperforms the previous state-of-the-art unsupervised machine\ntranslation systems across two benchmarks (English-French and English-German)\nand two low-resource language pairs (English-Romanian and English-Russian) by\nmore than 2 (up to 3.63) BLEU points.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 03:22:40 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Wu", "Jiawei", ""], ["Wang", "Xin", ""], ["Wang", "William Yang", ""]]}, {"id": "1904.02338", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat and Ankur P. Parikh", "title": "Consistency by Agreement in Zero-shot Neural Machine Translation", "comments": "NAACL 2019 (14 pages, 5 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization and reliability of multilingual translation often highly\ndepend on the amount of available parallel data for each language pair of\ninterest. In this paper, we focus on zero-shot generalization---a challenging\nsetup that tests models on translation directions they have not been optimized\nfor at training time. To solve the problem, we (i) reformulate multilingual\ntranslation as probabilistic inference, (ii) define the notion of zero-shot\nconsistency and show why standard training often results in models unsuitable\nfor zero-shot tasks, and (iii) introduce a consistent agreement-based training\nmethod that encourages the model to produce equivalent translations of parallel\nsentences in auxiliary languages. We test our multilingual NMT models on\nmultiple public zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl)\nand show that agreement-based learning often results in 2-3 BLEU zero-shot\nimprovement over strong baselines without any loss in performance on supervised\ntranslation directions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 03:49:05 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 04:00:03 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Parikh", "Ankur P.", ""]]}, {"id": "1904.02342", "submitter": "Rik Koncel-Kedziorski", "authors": "Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and\n  Hannaneh Hajishirzi", "title": "Text Generation from Knowledge Graphs with Graph Transformers", "comments": "Accepted as a long paper in NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating texts which express complex ideas spanning multiple sentences\nrequires a structured representation of their content (document plan), but\nthese representations are prohibitively expensive to manually produce. In this\nwork, we address the problem of generating coherent multi-sentence texts from\nthe output of an information extraction system, and in particular a knowledge\ngraph. Graphical knowledge representations are ubiquitous in computing, but\npose a significant challenge for text generation techniques due to their\nnon-hierarchical nature, collapsing of long-distance dependencies, and\nstructural variety. We introduce a novel graph transforming encoder which can\nleverage the relational structure of such knowledge graphs without imposing\nlinearization or hierarchical constraints. Incorporated into an encoder-decoder\nsetup, we provide an end-to-end trainable system for graph-to-text generation\nthat we apply to the domain of scientific text. Automatic and human evaluations\nshow that our technique produces more informative texts which exhibit better\ndocument structure than competitive encoder-decoder methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 04:33:15 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 01:07:52 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Koncel-Kedziorski", "Rik", ""], ["Bekal", "Dhanush", ""], ["Luan", "Yi", ""], ["Lapata", "Mirella", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "1904.02343", "submitter": "Chunting Zhou", "authors": "Chunting Zhou, Xuezhe Ma, Di Wang, Graham Neubig", "title": "Density Matching for Bilingual Word Embedding", "comments": "Accepted by NAACL-HLT 2019", "journal-ref": "NAACL 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches to cross-lingual word embedding have generally been based\non linear transformations between the sets of embedding vectors in the two\nlanguages. In this paper, we propose an approach that instead expresses the two\nmonolingual embedding spaces as probability densities defined by a Gaussian\nmixture model, and matches the two densities using a method called normalizing\nflow. The method requires no explicit supervision, and can be learned with only\na seed dictionary of words that have identical strings. We argue that this\nformulation has several intuitively attractive properties, particularly with\nthe respect to improving robustness and generalization to mappings between\ndifficult language pairs or word pairs. On a benchmark data set of bilingual\nlexicon induction and cross-lingual word similarity, our approach can achieve\ncompetitive or superior performance compared to state-of-the-art published\nresults, with particularly strong results being found on etymologically distant\nand/or morphologically rich languages.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 04:36:11 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 03:47:04 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 21:08:13 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Zhou", "Chunting", ""], ["Ma", "Xuezhe", ""], ["Wang", "Di", ""], ["Neubig", "Graham", ""]]}, {"id": "1904.02347", "submitter": "Robin Jia", "authors": "Robin Jia and Cliff Wong and Hoifung Poon", "title": "Document-Level $N$-ary Relation Extraction with Multiscale\n  Representation Learning", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most information extraction methods focus on binary relations expressed\nwithin single sentences. In high-value domains, however, $n$-ary relations are\nof great demand (e.g., drug-gene-mutation interactions in precision oncology).\nSuch relations often involve entity mentions that are far apart in the\ndocument, yet existing work on cross-sentence relation extraction is generally\nconfined to small text spans (e.g., three consecutive sentences), which\nseverely limits recall. In this paper, we propose a novel multiscale neural\narchitecture for document-level $n$-ary relation extraction. Our system\ncombines representations learned over various text spans throughout the\ndocument and across the subrelation hierarchy. Widening the system's purview to\nthe entire document maximizes potential recall. Moreover, by integrating weak\nsignals across the document, multiscale modeling increases precision, even in\nthe presence of noisy labels from distant supervision. Experiments on\nbiomedical machine reading show that our approach substantially outperforms\nprevious $n$-ary relation extraction methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:02:38 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 07:31:05 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 20:54:36 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Jia", "Robin", ""], ["Wong", "Cliff", ""], ["Poon", "Hoifung", ""]]}, {"id": "1904.02357", "submitter": "Seraphina Goldfarb-Tarrant", "authors": "Seraphina Goldfarb-Tarrant, Haining Feng, Nanyun Peng", "title": "Plan, Write, and Revise: an Interactive System for Open-Domain Story\n  Generation", "comments": "Accepted to NAACL 2019 Demo Track, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Story composition is a challenging problem for machines and even for humans.\nWe present a neural narrative generation system that interacts with humans to\ngenerate stories. Our system has different levels of human interaction, which\nenables us to understand at what stage of story-writing human collaboration is\nmost productive, both to improving story quality and human engagement in the\nwriting process. We compare different varieties of interaction in\nstory-writing, story-planning, and diversity controls under time constraints,\nand show that increased types of human collaboration at both planning and\nwriting stages results in a 10-50% improvement in story quality as compared to\nless interactive baselines. We also show an accompanying increase in user\nengagement and satisfaction with stories as compared to our own less\ninteractive systems and to previous turn-taking approaches to interaction.\nFinally, we find that humans tasked with collaboratively improving a particular\ncharacteristic of a story are in fact able to do so, which has implications for\nfuture uses of human-in-the-loop systems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:42:53 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 10:26:10 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 21:13:32 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Goldfarb-Tarrant", "Seraphina", ""], ["Feng", "Haining", ""], ["Peng", "Nanyun", ""]]}, {"id": "1904.02373", "submitter": "Yongguo Kang", "authors": "Yanyao Bian, Changbin Chen, Yongguo Kang, Zhenglin Pan", "title": "Multi-reference Tacotron by Intercross Training for Style\n  Disentangling,Transfer and Control in Speech Synthesis", "comments": "Submitted for Interspeech 2019, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech style control and transfer techniques aim to enrich the diversity and\nexpressiveness of synthesized speech. Existing approaches model all speech\nstyles into one representation, lacking the ability to control a specific\nspeech feature independently. To address this issue, we introduce a novel\nmulti-reference structure to Tacotron and propose intercross training approach,\nwhich together ensure that each sub-encoder of the multi-reference encoder\nindependently disentangles and controls a specific style. Experimental results\nshow that our model is able to control and transfer desired speech styles\nindividually.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 06:37:19 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Bian", "Yanyao", ""], ["Chen", "Changbin", ""], ["Kang", "Yongguo", ""], ["Pan", "Zhenglin", ""]]}, {"id": "1904.02399", "submitter": "Prince Zizhuang Wang", "authors": "Prince Zizhuang Wang, William Yang Wang", "title": "Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for\n  Text Modeling", "comments": "NAACL 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Variational Autoencoder has been widely used for language modeling\nand text generation tasks. These models often face a difficult optimization\nproblem, also known as the Kullback-Leibler (KL) term vanishing issue, where\nthe posterior easily collapses to the prior, and the model will ignore latent\ncodes in generative tasks. To address this problem, we introduce an improved\nWasserstein Variational Autoencoder (WAE) with Riemannian Normalizing Flow\n(RNF) for text modeling. The RNF transforms a latent variable into a space that\nrespects the geometric characteristics of input space, which makes posterior\nimpossible to collapse to the non-informative prior. The Wasserstein objective\nminimizes the distance between the marginal distribution and the prior directly\nand therefore does not force the posterior to match the prior. Empirical\nexperiments show that our model avoids KL vanishing over a range of datasets\nand has better performances in tasks such as language modeling, likelihood\napproximation, and text generation. Through a series of experiments and\nanalysis over latent space, we show that our model learns latent distributions\nthat respect latent space geometry and is able to generate sentences that are\nmore diverse.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 08:13:42 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 00:36:11 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 03:06:29 GMT"}, {"version": "v4", "created": "Mon, 22 Apr 2019 21:19:28 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Wang", "Prince Zizhuang", ""], ["Wang", "William Yang", ""]]}, {"id": "1904.02418", "submitter": "Jing Qian", "authors": "Jing Qian, Mai ElSherief, Elizabeth Belding, William Yang Wang", "title": "Learning to Decipher Hate Symbols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing computational models to understand hate speech typically frame the\nproblem as a simple classification task, bypassing the understanding of hate\nsymbols (e.g., 14 words, kigy) and their secret connotations. In this paper, we\npropose a novel task of deciphering hate symbols. To do this, we leverage the\nUrban Dictionary and collected a new, symbol-rich Twitter corpus of hate\nspeech. We investigate neural network latent context models for deciphering\nhate symbols. More specifically, we study Sequence-to-Sequence models and show\nhow they are able to crack the ciphers based on context. Furthermore, we\npropose a novel Variational Decipher and show how it can generalize better to\nunseen hate symbols in a more challenging testing setting.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:11:24 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Qian", "Jing", ""], ["ElSherief", "Mai", ""], ["Belding", "Elizabeth", ""], ["Wang", "William Yang", ""]]}, {"id": "1904.02461", "submitter": "Inigo Jauregi Unanue", "authors": "Inigo Jauregi Unanue, Ehsan Zare Borzeshi, Nazanin Esmaili, Massimo\n  Piccardi", "title": "ReWE: Regressing Word Embeddings for Regularization of Neural Machine\n  Translation Systems", "comments": "Accepted at NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization of neural machine translation is still a significant problem,\nespecially in low-resource settings. To mollify this problem, we propose\nregressing word embeddings (ReWE) as a new regularization technique in a system\nthat is jointly trained to predict the next word in the translation\n(categorical value) and its word embedding (continuous value). Such a joint\ntraining allows the proposed system to learn the distributional properties\nrepresented by the word embeddings, empirically improving the generalization to\nunseen sentences. Experiments over three translation datasets have showed a\nconsistent improvement over a strong baseline, ranging between 0.91 and 2.54\nBLEU points, and also a marked improvement over a state-of-the-art system.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 10:30:52 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Unanue", "Inigo Jauregi", ""], ["Borzeshi", "Ehsan Zare", ""], ["Esmaili", "Nazanin", ""], ["Piccardi", "Massimo", ""]]}, {"id": "1904.02464", "submitter": "Damien Sileo", "authors": "Damien Sileo, Tim Van-De-Cruys, Camille Pradel, Philippe Muller", "title": "Composition of Sentence Embeddings:Lessons from Statistical Relational\n  Learning", "comments": "Camera-ready for *SEM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Various NLP problems -- such as the prediction of sentence similarity,\nentailment, and discourse relations -- are all instances of the same general\ntask: the modeling of semantic relations between a pair of textual elements. A\npopular model for such problems is to embed sentences into fixed size vectors,\nand use composition functions (e.g. concatenation or sum) of those vectors as\nfeatures for the prediction. At the same time, composition of embeddings has\nbeen a main focus within the field of Statistical Relational Learning (SRL)\nwhose goal is to predict relations between entities (typically from knowledge\nbase triples). In this article, we show that previous work on relation\nprediction between texts implicitly uses compositions from baseline SRL models.\nWe show that such compositions are not expressive enough for several tasks\n(e.g. natural language inference). We build on recent SRL models to address\ntextual relational problems, showing that they are more expressive, and can\nalleviate issues from simpler compositions. The resulting models significantly\nimprove the state of the art in both transferable sentence representation\nlearning and relation prediction.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 10:38:33 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Sileo", "Damien", ""], ["Van-De-Cruys", "Tim", ""], ["Pradel", "Camille", ""], ["Muller", "Philippe", ""]]}, {"id": "1904.02496", "submitter": "Jonathan Mamou", "authors": "Jonathan Mamou, Oren Pereg, Moshe Wasserblat, Ido Dagan", "title": "Multi-Context Term Embeddings: the Use Case of Corpus-based Term Set\n  Expansion", "comments": "6 pages, RepEval 2019 (NAACL-HLT workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel algorithm that combines multi-context term\nembeddings using a neural classifier and we test this approach on the use case\nof corpus-based term set expansion. In addition, we present a novel and unique\ndataset for intrinsic evaluation of corpus-based term set expansion algorithms.\nWe show that, over this dataset, our algorithm provides up to 5 mean average\nprecision points over the best baseline.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 11:45:52 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 08:51:49 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Mamou", "Jonathan", ""], ["Pereg", "Oren", ""], ["Wasserblat", "Moshe", ""], ["Dagan", "Ido", ""]]}, {"id": "1904.02547", "submitter": "Lisa Beinborn", "authors": "Lisa Beinborn, Samira Abnar, Rochelle Choenni", "title": "Robust Evaluation of Language-Brain Encoding Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Language-brain encoding experiments evaluate the ability of language models\nto predict brain responses elicited by language stimuli. The evaluation\nscenarios for this task have not yet been standardized which makes it difficult\nto compare and interpret results. We perform a series of evaluation experiments\nwith a consistent encoding setup and compute the results for multiple fMRI\ndatasets. In addition, we test the sensitivity of the evaluation measures to\nrandomized data and analyze the effect of voxel selection methods. Our\nexperimental framework is publicly available to make modelling decisions more\ntransparent and support reproducibility for future comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 13:34:18 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Beinborn", "Lisa", ""], ["Abnar", "Samira", ""], ["Choenni", "Rochelle", ""]]}, {"id": "1904.02594", "submitter": "Vipul Raheja", "authors": "Vipul Raheja, Joel Tetreault", "title": "Dialogue Act Classification with Context-Aware Self-Attention", "comments": "NAACL-HLT 2019. 7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in Dialogue Act classification has treated the task as a sequence\nlabeling problem using hierarchical deep neural networks. We build on this\nprior work by leveraging the effectiveness of a context-aware self-attention\nmechanism coupled with a hierarchical recurrent neural network. We conduct\nextensive evaluations on standard Dialogue Act classification datasets and show\nsignificant improvement over state-of-the-art results on the Switchboard\nDialogue Act (SwDA) Corpus. We also investigate the impact of different\nutterance-level representation learning methods and show that our method is\neffective at capturing utterance-level semantic text representations while\nmaintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:02:20 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 15:12:03 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Raheja", "Vipul", ""], ["Tetreault", "Joel", ""]]}, {"id": "1904.02619", "submitter": "Awni Hannun", "authors": "Awni Hannun, Ann Lee, Qiantong Xu, Ronan Collobert", "title": "Sequence-to-Sequence Speech Recognition with Time-Depth Separable\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully convolutional sequence-to-sequence encoder architecture\nwith a simple and efficient decoder. Our model improves WER on LibriSpeech\nwhile being an order of magnitude more efficient than a strong RNN baseline.\nKey to our approach is a time-depth separable convolution block which\ndramatically reduces the number of parameters in the model while keeping the\nreceptive field large. We also give a stable and efficient beam search\ninference procedure which allows us to effectively integrate a language model.\nCoupled with a convolutional language model, our time-depth separable\nconvolution architecture improves by more than 22% relative WER over the best\npreviously reported sequence-to-sequence results on the noisy LibriSpeech test\nset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:44:39 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Hannun", "Awni", ""], ["Lee", "Ann", ""], ["Xu", "Qiantong", ""], ["Collobert", "Ronan", ""]]}, {"id": "1904.02633", "submitter": "Tzu-Ming Harry Hsu", "authors": "Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie Boag,\n  Wei-Hung Weng, Peter Szolovits, Marzyeh Ghassemi", "title": "Clinically Accurate Chest X-Ray Report Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic generation of radiology reports given medical radiographs has\nsignificant potential to operationally and improve clinical patient care. A\nnumber of prior works have focused on this problem, employing advanced methods\nfrom computer vision and natural language generation to produce readable\nreports. However, these works often fail to account for the particular nuances\nof the radiology domain, and, in particular, the critical importance of\nclinical accuracy in the resulting generated reports. In this work, we present\na domain-aware automatic chest X-ray radiology report generation system which\nfirst predicts what topics will be discussed in the report, then conditionally\ngenerates sentences corresponding to these topics. The resulting system is\nfine-tuned using reinforcement learning, considering both readability and\nclinical accuracy, as assessed by the proposed Clinically Coherent Reward. We\nverify this system on two datasets, Open-I and MIMIC-CXR, and demonstrate that\nour model offers marked improvements on both language generation metrics and\nCheXpert assessed accuracy over a variety of competitive baselines.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:04:30 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 04:15:47 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Liu", "Guanxiong", ""], ["Hsu", "Tzu-Ming Harry", ""], ["McDermott", "Matthew", ""], ["Boag", "Willie", ""], ["Weng", "Wei-Hung", ""], ["Szolovits", "Peter", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1904.02651", "submitter": "Soham Parikh", "authors": "Soham Parikh, Ananya B. Sai, Preksha Nema, Mitesh M. Khapra", "title": "ElimiNet: A Model for Eliminating Options for Reading Comprehension with\n  Multiple Choice Questions", "comments": "IJCAI-18", "journal-ref": "Proceedings of the Twenty-Seventh International Joint Conference\n  on Artificial Intelligence (2018) Main track. Pages 4272-4278", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of Reading Comprehension with Multiple Choice Questions, requires a\nhuman (or machine) to read a given passage, question pair and select one of the\nn given options. The current state of the art model for this task first\ncomputes a question-aware representation for the passage and then selects the\noption which has the maximum similarity with this representation. However, when\nhumans perform this task they do not just focus on option selection but use a\ncombination of elimination and selection. Specifically, a human would first try\nto eliminate the most irrelevant option and then read the passage again in the\nlight of this new information (and perhaps ignore portions corresponding to the\neliminated option). This process could be repeated multiple times till the\nreader is finally ready to select the correct option. We propose ElimiNet, a\nneural network-based model which tries to mimic this process. Specifically, it\nhas gates which decide whether an option can be eliminated given the passage,\nquestion pair and if so it tries to make the passage representation orthogonal\nto this eliminated option (akin to ignoring portions of the passage\ncorresponding to the eliminated option). The model makes multiple rounds of\npartial elimination to refine the passage representation and finally uses a\nselection module to pick the best option. We evaluate our model on the recently\nreleased large scale RACE dataset and show that it outperforms the current\nstate of the art model on 7 out of the $13$ question types in this dataset.\nFurther, we show that taking an ensemble of our elimination-selection based\nmethod with a selection based method gives us an improvement of 3.1% over the\nbest-reported performance on this dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:44:44 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Parikh", "Soham", ""], ["Sai", "Ananya B.", ""], ["Nema", "Preksha", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "1904.02660", "submitter": "Alexander LeClair", "authors": "Alexander LeClair, Collin McMillan", "title": "Recommendations for Datasets for Source Code Summarization", "comments": "Accepted to NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source Code Summarization is the task of writing short, natural language\ndescriptions of source code. The main use for these descriptions is in software\ndocumentation e.g. the one-sentence Java method descriptions in JavaDocs. Code\nsummarization is rapidly becoming a popular research problem, but progress is\nrestrained due to a lack of suitable datasets. In addition, a lack of community\nstandards for creating datasets leads to confusing and unreproducible research\nresults -- we observe swings in performance of more than 33% due only to\nchanges in dataset design. In this paper, we make recommendations for these\nstandards from experimental results. We release a dataset based on prior work\nof over 2.1m pairs of Java methods and one sentence method descriptions from\nover 28k Java projects. We describe the dataset and point out key differences\nfrom natural language data, to guide and support future researchers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:56:01 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["LeClair", "Alexander", ""], ["McMillan", "Collin", ""]]}, {"id": "1904.02665", "submitter": "Soham Parikh", "authors": "Soham Parikh, Ananya B. Sai, Preksha Nema, Mitesh M. Khapra", "title": "Frustratingly Poor Performance of Reading Comprehension Models on\n  Non-adversarial Examples", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans learn to perform a difficult task (say, reading comprehension\n(RC) over longer passages), it is typically the case that their performance\nimproves significantly on an easier version of this task (say, RC over shorter\npassages). Ideally, we would want an intelligent agent to also exhibit such a\nbehavior. However, on experimenting with state of the art RC models using the\nstandard RACE dataset, we observe that this is not true. Specifically, we see\ncounter-intuitive results wherein even when we show frustratingly easy examples\nto the model at test time, there is hardly any improvement in its performance.\nWe refer to this as non-adversarial evaluation as opposed to adversarial\nevaluation. Such non-adversarial examples allow us to assess the utility of\nspecialized neural components. For example, we show that even for easy examples\nwhere the answer is clearly embedded in the passage, the neural components\ndesigned for paying attention to relevant portions of the passage fail to serve\ntheir intended purpose. We believe that the non-adversarial dataset created as\na part of this work would complement the research on adversarial evaluation and\ngive a more realistic assessment of the ability of RC models. All the datasets\nand codes developed as a part of this work will be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:00:48 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Parikh", "Soham", ""], ["Sai", "Ananya B.", ""], ["Nema", "Preksha", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "1904.02668", "submitter": "Nelson F. Liu", "authors": "Nelson F. Liu and Roy Schwartz and Noah A. Smith", "title": "Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets", "comments": "9 pages, 4 figures; to appear at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several datasets have recently been constructed to expose brittleness in\nmodels trained on existing benchmarks. While model performance on these\nchallenge datasets is significantly lower compared to the original benchmark,\nit is unclear what particular weaknesses they reveal. For example, a challenge\ndataset may be difficult because it targets phenomena that current models\ncannot capture, or because it simply exploits blind spots in a model's specific\ntraining set. We introduce inoculation by fine-tuning, a new analysis method\nfor studying challenge datasets by exposing models (the metaphorical patient)\nto a small amount of data from the challenge dataset (a metaphorical pathogen)\nand assessing how well they can adapt. We apply our method to analyze the NLI\n\"stress tests\" (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and\nLiang, 2017). We show that after slight exposure, some of these datasets are no\nlonger challenging, while others remain difficult. Our results indicate that\nfailures on challenge datasets may lead to very different conclusions about\nmodels, training datasets, and the challenge datasets themselves.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:04:30 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 18:39:19 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 15:42:01 GMT"}, {"version": "v4", "created": "Fri, 26 Apr 2019 23:35:38 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Liu", "Nelson F.", ""], ["Schwartz", "Roy", ""], ["Smith", "Noah A.", ""]]}, {"id": "1904.02671", "submitter": "Sharath Chandra Guntuku", "authors": "Sharath Chandra Guntuku, Mingyang Li, Louis Tay, Lyle H. Ungar", "title": "Studying Cultural Differences in Emoji Usage across the East and the\n  West", "comments": "ICWSM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global acceptance of Emojis suggests a cross-cultural, normative use of\nEmojis. Meanwhile, nuances in Emoji use across cultures may also exist due to\nlinguistic differences in expressing emotions and diversity in conceptualizing\ntopics. Indeed, literature in cross-cultural psychology has found both\nnormative and culture-specific ways in which emotions are expressed. In this\npaper, using social media, we compare the Emoji usage based on frequency,\ncontext, and topic associations across countries in the East (China and Japan)\nand the West (United States, United Kingdom, and Canada). Across the East and\nthe West, our study examines a) similarities and differences on the usage of\ndifferent categories of Emojis such as People, Food \\& Drink, Travel \\& Places\netc., b) potential mapping of Emoji use differences with previously identified\ncultural differences in users' expression about diverse concepts such as death,\nmoney emotions and family, and c) relative correspondence of validated\npsycho-linguistic categories with Ekman's emotions. The analysis of Emoji use\nin the East and the West reveals recognizable normative and culture specific\npatterns. This research reveals the ways in which Emojis can be used for\ncross-cultural communication.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:12:25 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Guntuku", "Sharath Chandra", ""], ["Li", "Mingyang", ""], ["Tay", "Louis", ""], ["Ungar", "Lyle H.", ""]]}, {"id": "1904.02682", "submitter": "Nora Hollenstein", "authors": "Nora Hollenstein, Maria Barrett, Marius Troendle, Francesco Bigiolli,\n  Nicolas Langer, Ce Zhang", "title": "Advancing NLP with Cognitive Language Processing Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we read, our brain processes language and generates cognitive processing\ndata such as gaze patterns and brain activity. These signals can be recorded\nwhile reading. Cognitive language processing data such as eye-tracking features\nhave shown improvements on single NLP tasks. We analyze whether using such\nhuman features can show consistent improvement across tasks and data sources.\nWe present an extensive investigation of the benefits and limitations of using\ncognitive processing data for NLP. Specifically, we use gaze and EEG features\nto augment models of named entity recognition, relation classification, and\nsentiment analysis. These methods significantly outperform the baselines and\nshow the potential and current limitations of employing human language\nprocessing data for NLP.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:38:16 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Hollenstein", "Nora", ""], ["Barrett", "Maria", ""], ["Troendle", "Marius", ""], ["Bigiolli", "Francesco", ""], ["Langer", "Nicolas", ""], ["Zhang", "Ce", ""]]}, {"id": "1904.02734", "submitter": "Shane Steinert-Threlkeld", "authors": "Lewis O'Sullivan and Shane Steinert-Threlkeld", "title": "Neural Models of the Psychosemantics of `Most'", "comments": "to appear at 9th Workshop on Cognitive Modeling and Computational\n  Linguistics (CMCL2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How are the meanings of linguistic expressions related to their use in\nconcrete cognitive tasks? Visual identification tasks show human speakers can\nexhibit considerable variation in their understanding, representation and\nverification of certain quantifiers. This paper initiates an investigation into\nneural models of these psycho-semantic tasks. We trained two types of network\n-- a convolutional neural network (CNN) model and a recurrent model of visual\nattention (RAM) -- on the \"most\" verification task from \\citet{Pietroski2009},\nmanipulating the visual scene and novel notions of task duration. Our results\nqualitatively mirror certain features of human performance (such as sensitivity\nto the ratio of set sizes, indicating a reliance on approximate number) while\ndiffering in interesting ways (such as exhibiting a subtly different pattern\nfor the effect of image type). We conclude by discussing the prospects for\nusing neural models as cognitive models of this and other psychosemantic tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 18:14:23 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["O'Sullivan", "Lewis", ""], ["Steinert-Threlkeld", "Shane", ""]]}, {"id": "1904.02755", "submitter": "Soham Ghosh", "authors": "Soham Ghosh, Anuva Agarwal, Zarana Parekh, Alexander Hauptmann", "title": "ExCL: Extractive Clip Localization Using Natural Language Descriptions", "comments": "Accepted at NAACL 2019, Short Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of retrieving clips within videos based on a given natural language\nquery requires cross-modal reasoning over multiple frames. Prior approaches\nsuch as sliding window classifiers are inefficient, while text-clip similarity\ndriven ranking-based approaches such as segment proposal networks are far more\ncomplicated. In order to select the most relevant video clip corresponding to\nthe given text description, we propose a novel extractive approach that\npredicts the start and end frames by leveraging cross-modal interactions\nbetween the text and video - this removes the need to retrieve and re-rank\nmultiple proposal segments. Using recurrent networks we encode the two\nmodalities into a joint representation which is then used in different variants\nof start-end frame predictor networks. Through extensive experimentation and\nablative analysis, we demonstrate that our simple and elegant approach\nsignificantly outperforms state of the art on two datasets and has comparable\nperformance on a third.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 19:17:04 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ghosh", "Soham", ""], ["Agarwal", "Anuva", ""], ["Parekh", "Zarana", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1904.02767", "submitter": "Reno Kriz", "authors": "Reno Kriz, Jo\\~ao Sedoc, Marianna Apidianaki, Carolina Zheng, Gaurav\n  Kumar, Eleni Miltsakaki and Chris Callison-Burch", "title": "Complexity-Weighted Loss and Diverse Reranking for Sentence\n  Simplification", "comments": "11 pages, North American Association of Computational Linguistics\n  (NAACL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence simplification is the task of rewriting texts so they are easier to\nunderstand. Recent research has applied sequence-to-sequence (Seq2Seq) models\nto this task, focusing largely on training-time improvements via reinforcement\nlearning and memory augmentation. One of the main problems with applying\ngeneric Seq2Seq models for simplification is that these models tend to copy\ndirectly from the original sentence, resulting in outputs that are relatively\nlong and complex. We aim to alleviate this issue through the use of two main\ntechniques. First, we incorporate content word complexities, as predicted with\na leveled word complexity model, into our loss function during training.\nSecond, we generate a large set of diverse candidate simplifications at test\ntime, and rerank these to promote fluency, adequacy, and simplicity. Here, we\nmeasure simplicity through a novel sentence complexity model. These extensions\nallow our models to perform competitively with state-of-the-art systems while\ngenerating simpler sentences. We report standard automatic and human evaluation\nmetrics.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 19:47:17 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Kriz", "Reno", ""], ["Sedoc", "Jo\u00e3o", ""], ["Apidianaki", "Marianna", ""], ["Zheng", "Carolina", ""], ["Kumar", "Gaurav", ""], ["Miltsakaki", "Eleni", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "1904.02790", "submitter": "Nishant Prateek", "authors": "Nishant Prateek, Mateusz {\\L}ajszczak, Roberto Barra-Chicote, Thomas\n  Drugman, Jaime Lorenzo-Trueba, Thomas Merritt, Srikanth Ronanki, Trevor Wood", "title": "In Other News: A Bi-style Text-to-speech Model for Synthesizing\n  Newscaster Voice with Limited Data", "comments": "Accepted at NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural text-to-speech synthesis (NTTS) models have shown significant progress\nin generating high-quality speech, however they require a large quantity of\ntraining data. This makes creating models for multiple styles expensive and\ntime-consuming. In this paper different styles of speech are analysed based on\nprosodic variations, from this a model is proposed to synthesise speech in the\nstyle of a newscaster, with just a few hours of supplementary data. We pose the\nproblem of synthesising in a target style using limited data as that of\ncreating a bi-style model that can synthesise both neutral-style and\nnewscaster-style speech via a one-hot vector which factorises the two styles.\nWe also propose conditioning the model on contextual word embeddings, and\nextensively evaluate it against neutral NTTS, and neutral concatenative-based\nsynthesis. This model closes the gap in perceived style-appropriateness between\nnatural recordings for newscaster-style of speech, and neutral speech synthesis\nby approximately two-thirds.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 20:59:20 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Prateek", "Nishant", ""], ["\u0141ajszczak", "Mateusz", ""], ["Barra-Chicote", "Roberto", ""], ["Drugman", "Thomas", ""], ["Lorenzo-Trueba", "Jaime", ""], ["Merritt", "Thomas", ""], ["Ronanki", "Srikanth", ""], ["Wood", "Trevor", ""]]}, {"id": "1904.02792", "submitter": "Hugh Zhang", "authors": "Tatsunori B. Hashimoto, Hugh Zhang, Percy Liang", "title": "Unifying Human and Statistical Evaluation for Natural Language\n  Generation", "comments": "NAACL Camera Ready Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we measure whether a natural language generation system produces both\nhigh quality and diverse outputs? Human evaluation captures quality but not\ndiversity, as it does not catch models that simply plagiarize from the training\nset. On the other hand, statistical evaluation (i.e., perplexity) captures\ndiversity but not quality, as models that occasionally emit low quality samples\nwould be insufficiently penalized. In this paper, we propose a unified\nframework which evaluates both diversity and quality, based on the optimal\nerror rate of predicting whether a sentence is human- or machine-generated. We\ndemonstrate that this error rate can be efficiently estimated by combining\nhuman and statistical evaluation, using an evaluation metric which we call\nHUSE. On summarization and chit-chat dialogue, we show that (i) HUSE detects\ndiversity defects which fool pure human evaluation and that (ii) techniques\nsuch as annealing for improving quality actually decrease HUSE due to decreased\ndiversity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 21:03:34 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Zhang", "Hugh", ""], ["Liang", "Percy", ""]]}, {"id": "1904.02793", "submitter": "Ashutosh Modi", "authors": "Pierre Colombo and Wojciech Witon and Ashutosh Modi and James Kennedy\n  and Mubbasir Kapadia", "title": "Affect-Driven Dialog Generation", "comments": "8+2 Pages, Accepted at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of current systems for end-to-end dialog generation focus on\nresponse quality without an explicit control over the affective content of the\nresponses. In this paper, we present an affect-driven dialog system, which\ngenerates emotional responses in a controlled manner using a continuous\nrepresentation of emotions. The system achieves this by modeling emotions at a\nword and sequence level using: (1) a vector representation of the desired\nemotion, (2) an affect regularizer, which penalizes neutral words, and (3) an\naffect sampling method, which forces the neural network to generate diverse\nwords that are emotionally relevant. During inference, we use a reranking\nprocedure that aims to extract the most emotionally relevant responses using a\nhuman-in-the-loop optimization process. We study the performance of our system\nin terms of both quantitative (BLEU score and response diversity), and\nqualitative (emotional appropriateness) measures.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 21:05:13 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Colombo", "Pierre", ""], ["Witon", "Wojciech", ""], ["Modi", "Ashutosh", ""], ["Kennedy", "James", ""], ["Kapadia", "Mubbasir", ""]]}, {"id": "1904.02800", "submitter": "Sanuj Sharma", "authors": "Sanuj Sharma, Prafulla Kumar Choubey, Ruihong Huang", "title": "Improving Dialogue State Tracking by Discerning the Relevant Context", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical conversation comprises of multiple turns between participants where\nthey go back-and-forth between different topics. At each user turn, dialogue\nstate tracking (DST) aims to estimate user's goal by processing the current\nutterance. However, in many turns, users implicitly refer to the previous goal,\nnecessitating the use of relevant dialogue history. Nonetheless, distinguishing\nrelevant history is challenging and a popular method of using dialogue recency\nfor that is inefficient. We, therefore, propose a novel framework for DST that\nidentifies relevant historical context by referring to the past utterances\nwhere a particular slot-value changes and uses that together with weighted\nsystem utterance to identify the relevant context. Specifically, we use the\ncurrent user utterance and the most recent system utterance to determine the\nrelevance of a system utterance. Empirical analyses show that our method\nimproves joint goal accuracy by 2.75% and 2.36% on WoZ 2.0 and MultiWoZ 2.0\nrestaurant domain datasets respectively over the previous state-of-the-art GLAD\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 21:53:41 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Sharma", "Sanuj", ""], ["Choubey", "Prafulla Kumar", ""], ["Huang", "Ruihong", ""]]}, {"id": "1904.02815", "submitter": "Ashutosh Modi", "authors": "Pooja Chitkara, Ashutosh Modi, Pravalika Avvaru, Sepehr Janghorbani,\n  Mubbasir Kapadia", "title": "Topic Spotting using Hierarchical Networks with Self Attention", "comments": "5+2 Pages, Accepted at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Success of deep learning techniques have renewed the interest in development\nof dialogue systems. However, current systems struggle to have consistent long\nterm conversations with the users and fail to build rapport. Topic spotting,\nthe task of automatically inferring the topic of a conversation, has been shown\nto be helpful in making a dialog system more engaging and efficient. We propose\na hierarchical model with self attention for topic spotting. Experiments on the\nSwitchboard corpus show the superior performance of our model over previously\nproposed techniques for topic spotting and deep models for text classification.\nAdditionally, in contrast to offline processing of dialog, we also analyze the\nperformance of our model in a more realistic setting i.e. in an online setting\nwhere the topic is identified in real time as the dialog progresses. Results\nshow that our model is able to generalize even with limited information in the\nonline setting.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 22:54:57 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Chitkara", "Pooja", ""], ["Modi", "Ashutosh", ""], ["Avvaru", "Pravalika", ""], ["Janghorbani", "Sepehr", ""], ["Kapadia", "Mubbasir", ""]]}, {"id": "1904.02817", "submitter": "Xiaochuang Han", "authors": "Xiaochuang Han and Jacob Eisenstein", "title": "Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence\n  Labeling", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualized word embeddings such as ELMo and BERT provide a foundation for\nstrong performance across a wide range of natural language processing tasks by\npretraining on large corpora of unlabeled text. However, the applicability of\nthis approach is unknown when the target domain varies substantially from the\npretraining corpus. We are specifically interested in the scenario in which\nlabeled data is available in only a canonical source domain such as newstext,\nand the target domain is distinct from both the labeled and pretraining texts.\nTo address this scenario, we propose domain-adaptive fine-tuning, in which the\ncontextualized embeddings are adapted by masked language modeling on text from\nthe target domain. We test this approach on sequence labeling in two\nchallenging domains: Early Modern English and Twitter. Both domains differ\nsubstantially from existing pretraining corpora, and domain-adaptive\nfine-tuning yields substantial improvements over strong BERT baselines, with\nparticularly impressive results on out-of-vocabulary words. We conclude that\ndomain-adaptive fine-tuning offers a simple and effective approach for the\nunsupervised adaptation of sequence labeling to difficult new domains.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 23:05:45 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 00:18:25 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Han", "Xiaochuang", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1904.02818", "submitter": "David Bieber", "authors": "Rui Zhao, David Bieber, Kevin Swersky, Daniel Tarlow", "title": "Neural Networks for Modeling Source Code Edits", "comments": "Deanonymized version of ICLR 2019 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Programming languages are emerging as a challenging and interesting domain\nfor machine learning. A core task, which has received significant attention in\nrecent years, is building generative models of source code. However, to our\nknowledge, previous generative models have always been framed in terms of\ngenerating static snapshots of code. In this work, we instead treat source code\nas a dynamic object and tackle the problem of modeling the edits that software\ndevelopers make to source code files. This requires extracting intent from\nprevious edits and leveraging it to generate subsequent edits. We develop\nseveral neural networks and use synthetic data to test their ability to learn\nchallenging edit patterns that require strong generalization. We then collect\nand train our models on a large-scale dataset of Google source code, consisting\nof millions of fine-grained edits from thousands of Python developers. From the\nmodeling perspective, our main conclusion is that a new composition of\nattentional and pointer network components provides the best overall\nperformance and scalability. From the application perspective, our results\nprovide preliminary evidence of the feasibility of developing tools that learn\nto predict future edits.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 23:06:09 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Zhao", "Rui", ""], ["Bieber", "David", ""], ["Swersky", "Kevin", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1904.02839", "submitter": "Alexander Hoyle", "authors": "Alexander Hoyle, Lawrence Wolf-Sonkin, Hanna Wallach, Ryan Cotterell\n  and Isabelle Augenstein", "title": "Combining Sentiment Lexica with a Multi-View Variational Autoencoder", "comments": "To appear in NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When assigning quantitative labels to a dataset, different methodologies may\nrely on different scales. In particular, when assigning polarities to words in\na sentiment lexicon, annotators may use binary, categorical, or continuous\nlabels. Naturally, it is of interest to unify these labels from disparate\nscales to both achieve maximal coverage over words and to create a single, more\nrobust sentiment lexicon while retaining scale coherence. We introduce a\ngenerative model of sentiment lexica to combine disparate scales into a common\nlatent representation. We realize this model with a novel multi-view\nvariational autoencoder (VAE), called SentiVAE. We evaluate our approach via a\ndownstream text classification task involving nine English-Language sentiment\nanalysis datasets; our representation outperforms six individual sentiment\nlexica, as well as a straightforward combination thereof.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 01:03:31 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Hoyle", "Alexander", ""], ["Wolf-Sonkin", "Lawrence", ""], ["Wallach", "Hanna", ""], ["Cotterell", "Ryan", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "1904.02856", "submitter": "Takuma Ebisu", "authors": "Takuma Ebisu and Ryutaro Ichise", "title": "Graph Pattern Entity Ranking Model for Knowledge Graph Completion", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs have evolved rapidly in recent years and their usefulness\nhas been demonstrated in many artificial intelligence tasks. However, knowledge\ngraphs often have lots of missing facts. To solve this problem, many knowledge\ngraph embedding models have been developed to populate knowledge graphs and\nthese have shown outstanding performance. However, knowledge graph embedding\nmodels are so-called black boxes, and the user does not know how the\ninformation in a knowledge graph is processed and the models can be difficult\nto interpret. In this paper, we utilize graph patterns in a knowledge graph to\novercome such problems. Our proposed model, the {\\it graph pattern entity\nranking model} (GRank), constructs an entity ranking system for each graph\npattern and evaluates them using a ranking measure. By doing so, we can find\ngraph patterns which are useful for predicting facts. Then, we perform link\nprediction tasks on standard datasets to evaluate our GRank method. We show\nthat our approach outperforms other state-of-the-art approaches such as ComplEx\nand TorusE for standard metrics such as HITS@{\\it n} and MRR. Moreover, our\nmodel is easily interpretable because the output facts are described by graph\npatterns.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 03:17:00 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ebisu", "Takuma", ""], ["Ichise", "Ryutaro", ""]]}, {"id": "1904.02927", "submitter": "Masato Mita", "authors": "Masato Mita, Tomoya Mizumoto, Masahiro Kaneko, Ryo Nagata and Kentaro\n  Inui", "title": "Cross-Corpora Evaluation and Analysis of Grammatical Error Correction\n  Models --- Is Single-Corpus Evaluation Enough?", "comments": "accepted by NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explores the necessity of performing cross-corpora evaluation for\ngrammatical error correction (GEC) models. GEC models have been previously\nevaluated based on a single commonly applied corpus: the CoNLL-2014 benchmark.\nHowever, the evaluation remains incomplete because the task difficulty varies\ndepending on the test corpus and conditions such as the proficiency levels of\nthe writers and essay topics. To overcome this limitation, we evaluate the\nperformance of several GEC models, including NMT-based (LSTM, CNN, and\ntransformer) and an SMT-based model, against various learner corpora\n(CoNLL-2013, CoNLL-2014, FCE, JFLEG, ICNALE, and KJ). Evaluation results reveal\nthat the models' rankings considerably vary depending on the corpus, indicating\nthat single-corpus evaluation is insufficient for GEC models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 08:14:08 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Mita", "Masato", ""], ["Mizumoto", "Tomoya", ""], ["Kaneko", "Masahiro", ""], ["Nagata", "Ryo", ""], ["Inui", "Kentaro", ""]]}, {"id": "1904.02954", "submitter": "Nils Reimers", "authors": "Nils Reimers, Iryna Gurevych", "title": "Alternative Weighting Schemes for ELMo Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  ELMo embeddings (Peters et. al, 2018) had a huge impact on the NLP community\nand may recent publications use these embeddings to boost the performance for\ndownstream NLP tasks. However, integration of ELMo embeddings in existent NLP\narchitectures is not straightforward. In contrast to traditional word\nembeddings, like GloVe or word2vec embeddings, the bi-directional language\nmodel of ELMo produces three 1024 dimensional vectors per token in a sentence.\nPeters et al. proposed to learn a task-specific weighting of these three\nvectors for downstream tasks. However, this proposed weighting scheme is not\nfeasible for certain tasks, and, as we will show, it does not necessarily yield\noptimal performance. We evaluate different methods that combine the three\nvectors from the language model in order to achieve the best possible\nperformance in downstream NLP tasks. We notice that the third layer of the\npublished language model often decreases the performance. By learning a\nweighted average of only the first two layers, we are able to improve the\nperformance for many datasets. Due to the reduced complexity of the language\nmodel, we have a training speed-up of 19-44% for the downstream task.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 09:24:36 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Reimers", "Nils", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1904.02981", "submitter": "Samuel Pecar", "authors": "Samuel Pecar, Marian Simko, Maria Bielikova", "title": "NL-FIIT at SemEval-2019 Task 9: Neural Model Ensemble for Suggestion\n  Mining", "comments": "Accepted at the SemEval-2019 International Workshop on Semantic\n  Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present neural model architecture submitted to the\nSemEval-2019 Task 9 competition: \"Suggestion Mining from Online Reviews and\nForums\". We participated in both subtasks for domain specific and also\ncross-domain suggestion mining. We proposed a recurrent neural network\narchitecture that employs Bi-LSTM layers and also self-attention mechanism. Our\narchitecture tries to encode words via word representations using ELMo and\nensembles multiple models to achieve better results. We performed experiments\nwith different setups of our proposed model involving weighting of prediction\nclasses for loss function. Our best model achieved in official test evaluation\nscore of 0.6816 for subtask A and 0.6850 for subtask B. In official results, we\nachieved 12th and 10th place in subtasks A and B, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 10:19:56 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Pecar", "Samuel", ""], ["Simko", "Marian", ""], ["Bielikova", "Maria", ""]]}, {"id": "1904.02996", "submitter": "Victor Prokhorov", "authors": "Victor Prokhorov, Mohammad Taher Pilehvar and Nigel Collier", "title": "Generating Knowledge Graph Paths from Textual Definitions using\n  Sequence-to-Sequence Models", "comments": "accepted at naacl 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for mapping unrestricted text to knowledge graph\nentities by framing the task as a sequence-to-sequence problem. Specifically,\ngiven the encoded state of an input text, our decoder directly predicts paths\nin the knowledge graph, starting from the root and ending at the target node\nfollowing hypernym-hyponym relationships. In this way, and in contrast to other\ntext-to-entity mapping systems, our model outputs hierarchically structured\npredictions that are fully interpretable in the context of the underlying\nontology, in an end-to-end manner. We present a proof-of-concept experiment\nwith encouraging results, comparable to those of state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 11:29:57 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Prokhorov", "Victor", ""], ["Pilehvar", "Mohammad Taher", ""], ["Collier", "Nigel", ""]]}, {"id": "1904.03035", "submitter": "Shikha Bordia Ms", "authors": "Shikha Bordia and Samuel R. Bowman", "title": "Identifying and Reducing Gender Bias in Word-Level Language Models", "comments": "12 pages with 8 tables and 1 figure; Published at NAACL SRW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many text corpora exhibit socially problematic biases, which can be\npropagated or amplified in the models trained on such data. For example, doctor\ncooccurs more frequently with male pronouns than female pronouns. In this study\nwe (i) propose a metric to measure gender bias; (ii) measure bias in a text\ncorpus and the text generated from a recurrent neural network language model\ntrained on the text corpus; (iii) propose a regularization loss term for the\nlanguage model that minimizes the projection of encoder-trained embeddings onto\nan embedding subspace that encodes gender; (iv) finally, evaluate efficacy of\nour proposed method on reducing gender bias. We find this regularization method\nto be effective in reducing gender bias up to an optimal weight assigned to the\nloss term, beyond which the model becomes unstable as the perplexity increases.\nWe replicate this study on three training corpora---Penn Treebank, WikiText-2,\nand CNN/Daily Mail---resulting in similar conclusions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 12:40:28 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Bordia", "Shikha", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1904.03084", "submitter": "Lukas Schmelzeisen", "authors": "Ipek Baris and Lukas Schmelzeisen and Steffen Staab", "title": "CLEARumor at SemEval-2019 Task 7: ConvoLving ELMo Against Rumors", "comments": "5 pages, 2 figures, 3 tables. Accepted for publication at\n  SemEval@NAACL-HLT 2019", "journal-ref": "SemEval@NAACL-HLT (2019) 1105-1109", "doi": "10.18653/v1/S19-2193", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission to SemEval-2019 Task 7: RumourEval:\nDetermining Rumor Veracity and Support for Rumors. We participated in both\nsubtasks. The goal of subtask A is to classify the type of interaction between\na rumorous social media post and a reply post as support, query, deny, or\ncomment. The goal of subtask B is to predict the veracity of a given rumor. For\nsubtask A, we implement a CNN-based neural architecture using ELMo embeddings\nof post text combined with auxiliary features and achieve a F1-score of 44.6%.\nFor subtask B, we employ a MLP neural network leveraging our estimates for\nsubtask A and achieve a F1-score of 30.1% (second place in the competition). We\nprovide results and analysis of our system performance and present ablation\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:25:25 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Baris", "Ipek", ""], ["Schmelzeisen", "Lukas", ""], ["Staab", "Steffen", ""]]}, {"id": "1904.03092", "submitter": "Zhaopeng Tu", "authors": "Jie Hao and Xing Wang and Baosong Yang and Longyue Wang and Jinfeng\n  Zhang and Zhaopeng Tu", "title": "Modeling Recurrence for Transformer", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Transformer model that is based solely on attention mechanisms,\nhas advanced the state-of-the-art on various machine translation tasks.\nHowever, recent studies reveal that the lack of recurrence hinders its further\nimprovement of translation capacity. In response to this problem, we propose to\ndirectly model recurrence for Transformer with an additional recurrence\nencoder. In addition to the standard recurrent neural network, we introduce a\nnovel attentive recurrent network to leverage the strengths of both attention\nand recurrent networks. Experimental results on the widely-used WMT14\nEnglish-German and WMT17 Chinese-English translation tasks demonstrate the\neffectiveness of the proposed approach. Our studies also reveal that the\nproposed model benefits from a short-cut that bridges the source and target\nsequences with a single recurrent layer, which outperforms its deep\ncounterpart.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:40:22 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Hao", "Jie", ""], ["Wang", "Xing", ""], ["Yang", "Baosong", ""], ["Wang", "Longyue", ""], ["Zhang", "Jinfeng", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "1904.03100", "submitter": "Zhaopeng Tu", "authors": "Jian Li and Baosong Yang and Zi-Yi Dou and Xing Wang and Michael R.\n  Lyu and Zhaopeng Tu", "title": "Information Aggregation for Multi-Head Attention with\n  Routing-by-Agreement", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-head attention is appealing for its ability to jointly extract\ndifferent types of information from multiple representation subspaces.\nConcerning the information aggregation, a common practice is to use a\nconcatenation followed by a linear transformation, which may not fully exploit\nthe expressiveness of multi-head attention. In this work, we propose to improve\nthe information aggregation for multi-head attention with a more powerful\nrouting-by-agreement algorithm. Specifically, the routing algorithm iteratively\nupdates the proportion of how much a part (i.e. the distinct information\nlearned from a specific subspace) should be assigned to a whole (i.e. the final\noutput representation), based on the agreement between parts and wholes.\nExperimental results on linguistic probing tasks and machine translation tasks\nprove the superiority of the advanced information aggregation over the standard\nlinear transformation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:52:28 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Li", "Jian", ""], ["Yang", "Baosong", ""], ["Dou", "Zi-Yi", ""], ["Wang", "Xing", ""], ["Lyu", "Michael R.", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "1904.03107", "submitter": "Zhaopeng Tu", "authors": "Baosong Yang and Longyue Wang and Derek Wong and Lidia S. Chao and\n  Zhaopeng Tu", "title": "Convolutional Self-Attention Networks", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention networks (SANs) have drawn increasing interest due to their\nhigh parallelization in computation and flexibility in modeling dependencies.\nSANs can be further enhanced with multi-head attention by allowing the model to\nattend to information from different representation subspaces. In this work, we\npropose novel convolutional self-attention networks, which offer SANs the\nabilities to 1) strengthen dependencies among neighboring elements, and 2)\nmodel the interaction between features extracted by multiple attention heads.\nExperimental results of machine translation on different language pairs and\nmodel settings show that our approach outperforms both the strong Transformer\nbaseline and other existing models on enhancing the locality of SANs. Comparing\nwith prior studies, the proposed model is parameter free in terms of\nintroducing no more parameters.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:02:26 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Yang", "Baosong", ""], ["Wang", "Longyue", ""], ["Wong", "Derek", ""], ["Chao", "Lidia S.", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "1904.03111", "submitter": "Jun Seok Kang", "authors": "Jun Seok Kang, Robert L. Logan IV, Zewei Chu, Yang Chen, Dheeru Dua,\n  Kevin Gimpel, Sameer Singh, Niranjan Balasubramanian", "title": "PoMo: Generating Entity-Specific Post-Modifiers in Context", "comments": "NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce entity post-modifier generation as an instance of a\ncollaborative writing task. Given a sentence about a target entity, the task is\nto automatically generate a post-modifier phrase that provides contextually\nrelevant information about the entity. For example, for the sentence, \"Barack\nObama, _______, supported the #MeToo movement.\", the phrase \"a father of two\ngirls\" is a contextually relevant post-modifier. To this end, we build PoMo, a\npost-modifier dataset created automatically from news articles reflecting a\njournalistic need for incorporating entity information that is relevant to a\nparticular news event. PoMo consists of more than 231K sentences with\npost-modifiers and associated facts extracted from Wikidata for around 57K\nunique entities. We use crowdsourcing to show that modeling contextual\nrelevance is necessary for accurate post-modifier generation. We adapt a number\nof existing generation approaches as baselines for this dataset. Our results\nshow there is large room for improvement in terms of both identifying relevant\nfacts to include (knowing which claims are relevant gives a >20% improvement in\nBLEU score), and generating appropriate post-modifier text for the context\n(providing relevant claims is not sufficient for accurate generation). We\nconduct an error analysis that suggests promising directions for future\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:10:09 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 19:30:16 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Kang", "Jun Seok", ""], ["Logan", "Robert L.", "IV"], ["Chu", "Zewei", ""], ["Chen", "Yang", ""], ["Dua", "Dheeru", ""], ["Gimpel", "Kevin", ""], ["Singh", "Sameer", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "1904.03122", "submitter": "Stefan Larson", "authors": "Stefan Larson, Anish Mahendran, Andrew Lee, Jonathan K. Kummerfeld,\n  Parker Hill, Michael A. Laurenzano, Johann Hauswald, Lingjia Tang, Jason Mars", "title": "Outlier Detection for Improved Data Quality and Diversity in Dialog\n  Systems", "comments": "Accepted as long paper to NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a corpus of data, outliers are either errors: mistakes in the data that\nare counterproductive, or are unique: informative samples that improve model\nrobustness. Identifying outliers can lead to better datasets by (1) removing\nnoise in datasets and (2) guiding collection of additional data to fill gaps.\nHowever, the problem of detecting both outlier types has received relatively\nlittle attention in NLP, particularly for dialog systems. We introduce a simple\nand effective technique for detecting both erroneous and unique samples in a\ncorpus of short texts using neural sentence embeddings combined with\ndistance-based outlier detection. We also present a novel data collection\npipeline built atop our detection technique to automatically and iteratively\nmine unique data samples while discarding erroneous samples. Experiments show\nthat our outlier detection technique is effective at finding errors while our\ndata collection pipeline yields highly diverse corpora that in turn produce\nmore robust intent classification and slot-filling models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:31:28 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Larson", "Stefan", ""], ["Mahendran", "Anish", ""], ["Lee", "Andrew", ""], ["Kummerfeld", "Jonathan K.", ""], ["Hill", "Parker", ""], ["Laurenzano", "Michael A.", ""], ["Hauswald", "Johann", ""], ["Tang", "Lingjia", ""], ["Mars", "Jason", ""]]}, {"id": "1904.03152", "submitter": "Dhruv Khandelwal", "authors": "Dhruv Khandelwal, Maarten Schoukens, Roland T\\'oth", "title": "Data-driven Modelling of Dynamical Systems Using Tree Adjoining Grammar\n  and Genetic Programming", "comments": "Paper accepted at IEEE CEC 2019", "journal-ref": null, "doi": "10.1109/CEC.2019.8790250", "report-no": null, "categories": "cs.SY cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for data-driven modelling of non-linear dynamical\nsystems typically involve interactions with an expert user. In order to\npartially automate the process of modelling physical systems from data, many\nEA-based approaches have been proposed for model-structure selection, with\nspecial focus on non-linear systems. Recently, an approach for data-driven\nmodelling of non-linear dynamical systems using Genetic Programming (GP) was\nproposed. The novelty of the method was the modelling of noise and the use of\nTree Adjoining Grammar to shape the search-space explored by GP. In this paper,\nwe report results achieved by the proposed method on three case studies. Each\nof the case studies considered here is based on real physical systems. The case\nstudies pose a variety of challenges. In particular, these challenges range\nover varying amounts of prior knowledge of the true system, amount of data\navailable, the complexity of the dynamics of the system, and the nature of\nnon-linearities in the system. Based on the results achieved for the case\nstudies, we critically analyse the performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 16:42:44 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Khandelwal", "Dhruv", ""], ["Schoukens", "Maarten", ""], ["T\u00f3th", "Roland", ""]]}, {"id": "1904.03164", "submitter": "Roman Klinger", "authors": "Laura Bostan and Roman Klinger", "title": "Exploring Fine-Tuned Embeddings that Model Intensifiers for Emotion\n  Analysis", "comments": "Accepted at WASSA at NAACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adjective phrases like \"a little bit surprised\", \"completely shocked\", or\n\"not stunned at all\" are not handled properly by currently published\nstate-of-the-art emotion classification and intensity prediction systems which\nuse pre-dominantly non-contextualized word embeddings as input. Based on this\nfinding, we analyze differences between embeddings used by these systems in\nregard to their capability of handling such cases. Furthermore, we argue that\nintensifiers in context of emotion words need special treatment, as is\nestablished for sentiment polarity classification, but not for more\nfine-grained emotion prediction. To resolve this issue, we analyze different\naspects of a post-processing pipeline which enriches the word representations\nof such phrases. This includes expansion of semantic spaces at the phrase level\nand sub-word level followed by retrofitting to emotion lexica. We evaluate the\nimpact of these steps with A La Carte and Bag-of-Substrings extensions based on\npretrained GloVe, Word2vec, and fastText embeddings against a crowd-sourced\ncorpus of intensity annotations for tweets containing our focus phrases. We\nshow that the fastText-based models do not gain from handling these specific\nphrases under inspection. For Word2vec embeddings, we show that our\npost-processing pipeline improves the results by up to 8% on a novel dataset\ndensely populated with intensifiers.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:13:01 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Bostan", "Laura", ""], ["Klinger", "Roman", ""]]}, {"id": "1904.03172", "submitter": "Alexander Shvets", "authors": "Alexander Shvets", "title": "Improving Scientific Article Visibility by Neural Title Simplification", "comments": "Contribution to the Proceedings of the 8th International Workshop on\n  Bibliometric-enhanced Information Retrieval (BIR 2019) as part of the 41th\n  European Conference on Information Retrieval (ECIR 2019), Cologne, Germany,\n  April 14, 2019. CEUR Workshop Proceedings, CEUR-WS.org 2019. Keywords:\n  Scientific Text Summarization, Machine Translation, Recommender Systems,\n  Personalized Simplification", "journal-ref": "Proceedings of the 8th International Workshop on\n  Bibliometric-enhanced Information Retrieval (BIR) co-located with ECIR 2019,\n  Cologne, Germany (pp. 140-147)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapidly growing amount of data that scientific content providers should\ndeliver to a user makes them create effective recommendation tools. A title of\nan article is often the only shown element to attract people's attention. We\noffer an approach to automatic generating titles with various levels of\ninformativeness to benefit from different categories of users. Statistics from\nResearchGate used to bias train datasets and specially designed post-processing\nstep applied to neural sequence-to-sequence models allow reaching the desired\nvariety of simplified titles to gain a trade-off between the attractiveness and\ntransparency of recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:44:12 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Shvets", "Alexander", ""]]}, {"id": "1904.03223", "submitter": "Anshuman Suri", "authors": "Parag Agrawal and Anshuman Suri", "title": "NELEC at SemEval-2019 Task 3: Think Twice Before Going Deep", "comments": "International Workshop on Semantic Evaluation (SemEval), NAACL-HLT\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Machine Learning techniques yield close to human performance on\ntext-based classification tasks. However, the presence of multi-modal noise in\nchat data such as emoticons, slang, spelling mistakes, code-mixed data, etc.\nmakes existing deep-learning solutions perform poorly. The inability of\ndeep-learning systems to robustly capture these covariates puts a cap on their\nperformance. We propose NELEC: Neural and Lexical Combiner, a system which\nelegantly combines textual and deep-learning based methods for sentiment\nclassification. We evaluate our system as part of the third task of 'Contextual\nEmotion Detection in Text' as part of SemEval-2019. Our system performs\nsignificantly better than the baseline, as well as our deep-learning model\nbenchmarks. It achieved a micro-averaged F1 score of 0.7765, ranking 3rd on the\ntest-set leader-board. Our code is available at\nhttps://github.com/iamgroot42/nelec\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 18:31:06 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Agrawal", "Parag", ""], ["Suri", "Anshuman", ""]]}, {"id": "1904.03225", "submitter": "Eben Holderness", "authors": "Eben Holderness, Philip Cawkwell, Kirsten Bolton, James Pustejovsky,\n  Mei-Hua Hall", "title": "Distinguishing Clinical Sentiment: The Importance of Domain Adaptation\n  in Psychiatric Patient Health Records", "comments": "Accepted at Clinical NLP @ NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently natural language processing (NLP) tools have been developed to\nidentify and extract salient risk indicators in electronic health records\n(EHRs). Sentiment analysis, although widely used in non-medical areas for\nimproving decision making, has been studied minimally in the clinical setting.\nIn this study, we undertook, to our knowledge, the first domain adaptation of\nsentiment analysis to psychiatric EHRs by defining psychiatric clinical\nsentiment, performing an annotation project, and evaluating multiple\nsentence-level sentiment machine learning (ML) models. Results indicate that\noff-the-shelf sentiment analysis tools fail in identifying clinically positive\nor negative polarity, and that the definition of clinical sentiment that we\nprovide is learnable with relatively small amounts of training data. This\nproject is an initial step towards further refining sentiment analysis methods\nfor clinical use. Our long-term objective is to incorporate the results of this\nproject as part of a machine learning model that predicts inpatient readmission\nrisk. We hope that this work will initiate a discussion concerning domain\nadaptation of sentiment analysis to the clinical setting.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 18:33:22 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Holderness", "Eben", ""], ["Cawkwell", "Philip", ""], ["Bolton", "Kirsten", ""], ["Pustejovsky", "James", ""], ["Hall", "Mei-Hua", ""]]}, {"id": "1904.03240", "submitter": "Yu-An Chung", "authors": "Yu-An Chung and Wei-Ning Hsu and Hao Tang and James Glass", "title": "An Unsupervised Autoregressive Model for Speech Representation Learning", "comments": "Accepted to Interspeech 2019. Code available at:\n  https://github.com/iamyuanchung/Autoregressive-Predictive-Coding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel unsupervised autoregressive neural model for\nlearning generic speech representations. In contrast to other speech\nrepresentation learning methods that aim to remove noise or speaker\nvariabilities, ours is designed to preserve information for a wide range of\ndownstream tasks. In addition, the proposed model does not require any phonetic\nor word boundary labels, allowing the model to benefit from large quantities of\nunlabeled data. Speech representations learned by our model significantly\nimprove performance on both phone classification and speaker verification over\nthe surface features and other supervised and unsupervised approaches. Further\nanalysis shows that different levels of speech information are captured by our\nmodel at different layers. In particular, the lower layers tend to be more\ndiscriminative for speakers, while the upper layers provide more phonetic\ncontent.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 19:04:19 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 03:27:39 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Chung", "Yu-An", ""], ["Hsu", "Wei-Ning", ""], ["Tang", "Hao", ""], ["Glass", "James", ""]]}, {"id": "1904.03244", "submitter": "Sarthak Jain", "authors": "Sarthak Jain, Ramin Mohammadi, Byron C. Wallace", "title": "An Analysis of Attention over Clinical Notes for Predictive Tasks", "comments": "Accepted at The 2nd Clinical Natural Language Processing Workshop (At\n  NAACL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shift to electronic medical records (EMRs) has engendered research into\nmachine learning and natural language technologies to analyze patient records,\nand to predict from these clinical outcomes of interest. Two observations\nmotivate our aims here. First, unstructured notes contained within EMR often\ncontain key information, and hence should be exploited by models. Second, while\nstrong predictive performance is important, interpretability of models is\nperhaps equally so for applications in this domain. Together, these points\nsuggest that neural models for EMR may benefit from incorporation of attention\nover notes, which one may hope will both yield performance gains and afford\ntransparency in predictions. In this work we perform experiments to explore\nthis question using two EMR corpora and four different predictive tasks, that:\n(i) inclusion of attention mechanisms is critical for neural encoder modules\nthat operate over notes fields in order to yield competitive performance, but,\n(ii) unfortunately, while these boost predictive performance, it is decidedly\nless clear whether they provide meaningful support for predictions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 19:22:47 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Jain", "Sarthak", ""], ["Mohammadi", "Ramin", ""], ["Wallace", "Byron C.", ""]]}, {"id": "1904.03256", "submitter": "Mohammad Sadegh Rasooli", "authors": "Maryam Aminian, Mohammad Sadegh Rasooli, Mona Diab", "title": "Cross-Lingual Transfer of Semantic Roles: From Raw Text to Semantic\n  Roles", "comments": "Accepted at the 13th International Conference on Computational\n  Semantics (IWCS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a transfer method based on annotation projection to develop a\ndependency-based semantic role labeling system for languages for which no\nsupervised linguistic information other than parallel data is available. Unlike\nprevious work that presumes the availability of supervised features such as\nlemmas, part-of-speech tags, and dependency parse trees, we only make use of\nword and character features. Our deep model considers using character-based\nrepresentations as well as unsupervised stem embeddings to alleviate the need\nfor supervised features. Our experiments outperform a state-of-the-art method\nthat uses supervised lexico-syntactic features on 6 out of 7 languages in the\nUniversal Proposition Bank.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:04:04 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Aminian", "Maryam", ""], ["Rasooli", "Mohammad Sadegh", ""], ["Diab", "Mona", ""]]}, {"id": "1904.03262", "submitter": "Yufang Hou", "authors": "Yufang Hou, Debasis Ganguly, Lea A. Deleris, Francesca Bonin", "title": "Extracting Factual Min/Max Age Information from Clinical Trial Studies", "comments": "10 pages, accepted in ClinicalNLP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population age information is an essential characteristic of clinical trials.\nIn this paper, we focus on extracting minimum and maximum (min/max) age values\nfor the study samples from clinical research articles. Specifically, we\ninvestigate the use of a neural network model for question answering to address\nthis information extraction task. The min/max age QA model is trained on the\nmassive structured clinical study records from ClinicalTrials.gov. For each\narticle, based on multiple min and max age values extracted from the QA model,\nwe predict both actual min/max age values for the study samples and filter out\nnon-factual age expressions. Our system improves the results over (i) a passage\nretrieval based IE system and (ii) a CRF-based system by a large margin when\nevaluated on an annotated dataset consisting of 50 research papers on smoking\ncessation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:18:51 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Hou", "Yufang", ""], ["Ganguly", "Debasis", ""], ["Deleris", "Lea A.", ""], ["Bonin", "Francesca", ""]]}, {"id": "1904.03266", "submitter": "Ashutosh Modi", "authors": "Sepehr Janghorbani and Ashutosh Modi and Jakob Buhmann and Mubbasir\n  Kapadia", "title": "Domain Authoring Assistant for Intelligent Virtual Agents", "comments": "8+1 pages, Accepted at 18th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing intelligent virtual characters has attracted a lot of attention in\nthe recent years. The process of creating such characters often involves a team\nof creative authors who describe different aspects of the characters in natural\nlanguage, and planning experts that translate this description into a planning\ndomain. This can be quite challenging as the team of creative authors should\ndiligently define every aspect of the character especially if it contains\ncomplex human-like behavior. Also a team of engineers has to manually translate\nthe natural language description of a character's personality into the planning\ndomain knowledge. This can be extremely time and resource demanding and can be\nan obstacle to author's creativity. The goal of this paper is to introduce an\nauthoring assistant tool to automate the process of domain generation from\nnatural language description of virtual characters, thus bridging between the\ncreative authoring team and the planning domain experts. Moreover, the proposed\ntool also identifies possible missing information in the domain description and\niteratively makes suggestions to the author.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:27:26 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Janghorbani", "Sepehr", ""], ["Modi", "Ashutosh", ""], ["Buhmann", "Jakob", ""], ["Kapadia", "Mubbasir", ""]]}, {"id": "1904.03279", "submitter": "Kartikeya Upasani", "authors": "Ashwini Challa, Kartikeya Upasani, Anusha Balakrishnan, Rajen Subba", "title": "Generate, Filter, and Rank: Grammaticality Classification for\n  Production-Ready NLG Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural approaches to Natural Language Generation (NLG) have been promising\nfor goal-oriented dialogue. One of the challenges of productionizing these\napproaches, however, is the ability to control response quality, and ensure\nthat generated responses are acceptable. We propose the use of a generate,\nfilter, and rank framework, in which candidate responses are first filtered to\neliminate unacceptable responses, and then ranked to select the best response.\nWhile acceptability includes grammatical correctness and semantic correctness,\nwe focus only on grammaticality classification in this paper, and show that\nexisting datasets for grammatical error correction don't correctly capture the\ndistribution of errors that data-driven generators are likely to make. We\nrelease a grammatical classification and semantic correctness classification\ndataset for the weather domain that consists of responses generated by 3\ndata-driven NLG systems. We then explore two supervised learning approaches\n(CNNs and GBDTs) for classifying grammaticality. Our experiments show that\ngrammaticality classification is very sensitive to the distribution of errors\nin the data, and that these distributions vary significantly with both the\nsource of the response as well as the domain. We show that it's possible to\nachieve high precision with reasonable recall on our dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:02:12 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 01:23:03 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Challa", "Ashwini", ""], ["Upasani", "Kartikeya", ""], ["Balakrishnan", "Anusha", ""], ["Subba", "Rajen", ""]]}, {"id": "1904.03288", "submitter": "Jason Li", "authors": "Jason Li, Vitaly Lavrukhin, Boris Ginsburg, Ryan Leary, Oleksii\n  Kuchaiev, Jonathan M. Cohen, Huyen Nguyen, Ravi Teja Gadde", "title": "Jasper: An End-to-End Convolutional Neural Acoustic Model", "comments": "Accepted to INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report state-of-the-art results on LibriSpeech among\nend-to-end speech recognition models without any external training data. Our\nmodel, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout,\nand residual connections. To improve training, we further introduce a new\nlayer-wise optimizer called NovoGrad. Through experiments, we demonstrate that\nthe proposed deep architecture performs as well or better than more complex\nchoices. Our deepest Jasper variant uses 54 convolutional layers. With this\narchitecture, we achieve 2.95% WER using a beam-search decoder with an external\nneural language model and 3.86% WER with a greedy decoder on LibriSpeech\ntest-clean. We also report competitive results on the Wall Street Journal and\nthe Hub5'00 conversational evaluation datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:35:44 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 20:13:21 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 00:02:28 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Li", "Jason", ""], ["Lavrukhin", "Vitaly", ""], ["Ginsburg", "Boris", ""], ["Leary", "Ryan", ""], ["Kuchaiev", "Oleksii", ""], ["Cohen", "Jonathan M.", ""], ["Nguyen", "Huyen", ""], ["Gadde", "Ravi Teja", ""]]}, {"id": "1904.03296", "submitter": "Yi Luan", "authors": "Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, Hannaneh\n  Hajishirzi", "title": "A General Framework for Information Extraction using Dynamic Span Graphs", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for several information extraction tasks\nthat share span representations using dynamically constructed span graphs. The\ngraphs are constructed by selecting the most confident entity spans and linking\nthese nodes with confidence-weighted relation types and coreferences. The\ndynamic span graph allows coreference and relation type confidences to\npropagate through the graph to iteratively refine the span representations.\nThis is unlike previous multi-task frameworks for information extraction in\nwhich the only interaction between tasks is in the shared first-layer LSTM. Our\nframework significantly outperforms the state-of-the-art on multiple\ninformation extraction tasks across multiple datasets reflecting different\ndomains. We further observe that the span enumeration approach is good at\ndetecting nested span entities, with significant F1 score improvement on the\nACE dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:52:18 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luan", "Yi", ""], ["Wadden", "Dave", ""], ["He", "Luheng", ""], ["Shah", "Amy", ""], ["Ostendorf", "Mari", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "1904.03300", "submitter": "Nana (Nargiza) Nosirova", "authors": "Nargiza Nosirova, Mingbin Xu, Hui Jiang", "title": "A Multi-task Learning Approach for Named Entity Recognition using Local\n  Detection", "comments": "8 pages, 1 figure, 5 tables (Rejected by ACL2018 with score 3-4-4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Named entity recognition (NER) systems that perform well require task-related\nand manually annotated datasets. However, they are expensive to develop, and\nare thus limited in size. As there already exists a large number of NER\ndatasets that share a certain degree of relationship but differ in content, it\nis important to explore the question of whether such datasets can be combined\nas a simple method for improving NER performance. To investigate this, we\ndeveloped a novel locally detecting multitask model using FFNNs. The model\nrelies on encoding variable-length sequences of words into theoretically\nlossless and unique fixed-size representations. We applied this method to\nseveral well-known NER tasks and compared the results of our model to baseline\nmodels as well as other published results. As a result, we observed competitive\nperformance in nearly all of the tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:58:27 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 13:05:52 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Nosirova", "Nargiza", ""], ["Xu", "Mingbin", ""], ["Jiang", "Hui", ""]]}, {"id": "1904.03305", "submitter": "Nana (Nargiza) Nosirova", "authors": "Nargiza Nosirova, Mingbin Xu, Hui Jiang", "title": "Effective Context and Fragment Feature Usage for Named Entity\n  Recognition", "comments": "7 pages, 1 figure, 7 tables (Rejected by EMNLP 2018 with score\n  3-4-4). arXiv admin note: text overlap with arXiv:1904.03300", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we explore a new approach to named entity recognition (NER)\nwith the goal of learning from context and fragment features more effectively,\ncontributing to the improvement of overall recognition performance. We use the\nrecent fixed-size ordinally forgetting encoding (FOFE) method to fully encode\neach sentence fragment and its left-right contexts into a fixed-size\nrepresentation. Next, we organize the context and fragment features into\ngroups, and feed each feature group to dedicated fully-connected layers.\nFinally, we merge each group's final dedicated layers and add a shared layer\nleading to a single output. The outcome of our experiments show that, given\nonly tokenized text and trained word embeddings, our system outperforms our\nbaseline models, and is competitive to the state-of-the-arts of various\nwell-known NER tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 22:10:15 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 13:02:15 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Nosirova", "Nargiza", ""], ["Xu", "Mingbin", ""], ["Jiang", "Hui", ""]]}, {"id": "1904.03310", "submitter": "Jieyu Zhao", "authors": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente\n  Ordonez, Kai-Wei Chang", "title": "Gender Bias in Contextualized Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we quantify, analyze and mitigate gender bias exhibited in\nELMo's contextualized word vectors. First, we conduct several intrinsic\nanalyses and find that (1) training data for ELMo contains significantly more\nmale than female entities, (2) the trained ELMo embeddings systematically\nencode gender information and (3) ELMo unequally encodes gender information\nabout male and female entities. Then, we show that a state-of-the-art\ncoreference system that depends on ELMo inherits its bias and demonstrates\nsignificant bias on the WinoBias probing corpus. Finally, we explore two\nmethods to mitigate such gender bias and show that the bias demonstrated on\nWinoBias can be eliminated.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 22:36:12 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhao", "Jieyu", ""], ["Wang", "Tianlu", ""], ["Yatskar", "Mark", ""], ["Cotterell", "Ryan", ""], ["Ordonez", "Vicente", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "1904.03323", "submitter": "Emily Alsentzer", "authors": "Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung Weng, Di Jin,\n  Tristan Naumann, Matthew B. A. McDermott", "title": "Publicly Available Clinical BERT Embeddings", "comments": "Clinical Natural Language Processing (ClinicalNLP) Workshop at NAACL\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT\n(Devlin et al., 2018) have dramatically improved performance for many natural\nlanguage processing (NLP) tasks in recent months. However, these models have\nbeen minimally explored on specialty corpora, such as clinical text; moreover,\nin the clinical domain, no publicly-available pre-trained BERT models yet\nexist. In this work, we address this need by exploring and releasing BERT\nmodels for clinical text: one for generic clinical text and another for\ndischarge summaries specifically. We demonstrate that using a domain-specific\nmodel yields performance improvements on three common clinical NLP tasks as\ncompared to nonspecific embeddings. These domain-specific models are not as\nperformant on two clinical de-identification tasks, and argue that this is a\nnatural consequence of the differences between de-identified source text and\nsynthetically non de-identified task text.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 00:34:39 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 01:12:57 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 20:41:58 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Alsentzer", "Emily", ""], ["Murphy", "John R.", ""], ["Boag", "Willie", ""], ["Weng", "Wei-Hung", ""], ["Jin", "Di", ""], ["Naumann", "Tristan", ""], ["McDermott", "Matthew B. A.", ""]]}, {"id": "1904.03339", "submitter": "Reinald Kim Amplayo", "authors": "Cheoneum Park and Juae Kim and Hyeon-gu Lee and Reinald Kim Amplayo\n  and Harksoo Kim and Jungyun Seo and Changki Lee", "title": "ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for\n  out-of-domain samples", "comments": "SemEval 2019 Task 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our system, Joint Encoders for Stable Suggestion\nInference (JESSI), for the SemEval 2019 Task 9: Suggestion Mining from Online\nReviews and Forums. JESSI is a combination of two sentence encoders: (a) one\nusing multiple pre-trained word embeddings learned from log-bilinear regression\n(GloVe) and translation (CoVe) models, and (b) one on top of word encodings\nfrom a pre-trained deep bidirectional transformer (BERT). We include a domain\nadversarial training module when training for out-of-domain samples. Our\nexperiments show that while BERT performs exceptionally well for in-domain\nsamples, several runs of the model show that it is unstable for out-of-domain\nsamples. The problem is mitigated tremendously by (1) combining BERT with a\nnon-BERT encoder, and (2) using an RNN-based classifier on top of BERT. Our\nfinal models obtained second place with 77.78\\% F-Score on Subtask A (i.e.\nin-domain) and achieved an F-Score of 79.59\\% on Subtask B (i.e.\nout-of-domain), even without using any additional external data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 02:24:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Park", "Cheoneum", ""], ["Kim", "Juae", ""], ["Lee", "Hyeon-gu", ""], ["Amplayo", "Reinald Kim", ""], ["Kim", "Harksoo", ""], ["Seo", "Jungyun", ""], ["Lee", "Changki", ""]]}, {"id": "1904.03366", "submitter": "Natalie Parde", "authors": "Yatri Modi and Natalie Parde", "title": "The Steep Road to Happily Ever After: An Analysis of Current Visual\n  Storytelling Models", "comments": "Accepted to the NAACL 2019 Workshop on Shortcomings in Vision and\n  Language (SiVL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual storytelling is an intriguing and complex task that only recently\nentered the research arena. In this work, we survey relevant work to date, and\nconduct a thorough error analysis of three very recent approaches to visual\nstorytelling. We categorize and provide examples of common types of errors, and\nidentify key shortcomings in current work. Finally, we make recommendations for\naddressing these limitations in the future.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 05:42:19 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Modi", "Yatri", ""], ["Parde", "Natalie", ""]]}, {"id": "1904.03371", "submitter": "Ehsan Kamalloo", "authors": "Nouha Dziri, Ehsan Kamalloo, Kory W. Mathewson and Osmar Zaiane", "title": "Evaluating Coherence in Dialogue Systems using Entailment", "comments": "5 pages, 2 figures; NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating open-domain dialogue systems is difficult due to the diversity of\npossible correct answers. Automatic metrics such as BLEU correlate weakly with\nhuman annotations, resulting in a significant bias across different models and\ndatasets. Some researchers resort to human judgment experimentation for\nassessing response quality, which is expensive, time consuming, and not\nscalable. Moreover, judges tend to evaluate a small number of dialogues,\nmeaning that minor differences in evaluation configuration may lead to\ndissimilar results. In this paper, we present interpretable metrics for\nevaluating topic coherence by making use of distributed sentence\nrepresentations. Furthermore, we introduce calculable approximations of human\njudgment based on conversational coherence by adopting state-of-the-art\nentailment techniques. Results show that our metrics can be used as a surrogate\nfor human judgment, making it easy to evaluate dialogue systems on large-scale\ndatasets and allowing an unbiased estimate for the quality of the responses.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 06:06:11 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 01:19:52 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Dziri", "Nouha", ""], ["Kamalloo", "Ehsan", ""], ["Mathewson", "Kory W.", ""], ["Zaiane", "Osmar", ""]]}, {"id": "1904.03396", "submitter": "Amit Moryossef", "authors": "Amit Moryossef, Yoav Goldberg and Ido Dagan", "title": "Step-by-Step: Separating Planning from Realization in Neural\n  Data-to-Text Generation", "comments": "9 main pages, 10 appendix pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-to-text generation can be conceptually divided into two parts: ordering\nand structuring the information (planning), and generating fluent language\ndescribing the information (realization). Modern neural generation systems\nconflate these two steps into a single end-to-end differentiable system. We\npropose to split the generation process into a symbolic text-planning stage\nthat is faithful to the input, followed by a neural generation stage that\nfocuses only on realization. For training a plan-to-text generator, we present\na method for matching reference texts to their corresponding text plans. For\ninference time, we describe a method for selecting high-quality text plans for\nnew inputs. We implement and evaluate our approach on the WebNLG benchmark. Our\nresults demonstrate that decoupling text planning from neural realization\nindeed improves the system's reliability and adequacy while maintaining fluent\noutput. We observe improvements both in BLEU scores and in manual evaluations.\nAnother benefit of our approach is the ability to output diverse realizations\nof the same input, paving the way to explicit control over the generated text\nstructure.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:25:32 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 20:58:28 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Moryossef", "Amit", ""], ["Goldberg", "Yoav", ""], ["Dagan", "Ido", ""]]}, {"id": "1904.03409", "submitter": "Shuoyang Ding", "authors": "Shuoyang Ding, Philipp Koehn", "title": "Parallelizable Stack Long Short-Term Memory", "comments": "Accepted to NAACL 2019 Workshop on Structured Prediction for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stack Long Short-Term Memory (StackLSTM) is useful for various applications\nsuch as parsing and string-to-tree neural machine translation, but it is also\nknown to be notoriously difficult to parallelize for GPU training due to the\nfact that the computations are dependent on discrete operations. In this paper,\nwe tackle this problem by utilizing state access patterns of StackLSTM to\nhomogenize computations with regard to different discrete operations. Our\nparsing experiments show that the method scales up almost linearly with\nincreasing batch size, and our parallelized PyTorch implementation trains\nsignificantly faster compared to the Dynet C++ implementation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 10:12:27 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ding", "Shuoyang", ""], ["Koehn", "Philipp", ""]]}, {"id": "1904.03417", "submitter": "Carlos G\\'omez-Rodr\\'iguez", "authors": "Michalina Strzyz, Carlos G\\'omez-Rodr\\'iguez", "title": "Speeding Up Natural Language Parsing by Reusing Partial Results", "comments": "Accepted manuscript for CICLing 2019. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel technique that applies case-based reasoning in\norder to generate templates for reusable parse tree fragments, based on PoS\ntags of bigrams and trigrams that demonstrate low variability in their\nsyntactic analyses from prior data. The aim of this approach is to improve the\nspeed of dependency parsers by avoiding redundant calculations. This can be\nresolved by applying the predefined templates that capture results of previous\nsyntactic analyses and directly assigning the stored structure to a new n-gram\nthat matches one of the templates, instead of parsing a similar text fragment\nagain. The study shows that using a heuristic approach to select and reuse the\npartial results increases parsing speed by reducing the input length to be\nprocessed by a parser. The increase in parsing speed comes at some expense of\naccuracy. Experiments on English show promising results: the input dimension\ncan be reduced by more than 20% at the cost of less than 3 points of Unlabeled\nAttachment Score.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 10:55:11 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Strzyz", "Michalina", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "1904.03446", "submitter": "Hao Sun", "authors": "Hao Sun, Xu Tan, Jun-Wei Gan, Hongzhi Liu, Sheng Zhao, Tao Qin and\n  Tie-Yan Liu", "title": "Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion", "comments": "5 pages, accepted by interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grapheme-to-phoneme (G2P) conversion is an important task in automatic speech\nrecognition and text-to-speech systems. Recently, G2P conversion is viewed as a\nsequence to sequence task and modeled by RNN or CNN based encoder-decoder\nframework. However, previous works do not consider the practical issues when\ndeploying G2P model in the production system, such as how to leverage\nadditional unlabeled data to boost the accuracy, as well as reduce model size\nfor online deployment. In this work, we propose token-level ensemble\ndistillation for G2P conversion, which can (1) boost the accuracy by distilling\nthe knowledge from additional unlabeled data, and (2) reduce the model size but\nmaintain the high accuracy, both of which are very practical and helpful in the\nonline production system. We use token-level knowledge distillation, which\nresults in better accuracy than the sequence-level counterpart. What is more,\nwe adopt the Transformer instead of RNN or CNN based models to further boost\nthe accuracy of G2P conversion. Experiments on the publicly available CMUDict\ndataset and an internal English dataset demonstrate the effectiveness of our\nproposed method. Particularly, our method achieves 19.88% WER on CMUDict\ndataset, outperforming the previous works by more than 4.22% WER, and setting\nthe new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 13:49:16 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 08:14:58 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 02:59:46 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Sun", "Hao", ""], ["Tan", "Xu", ""], ["Gan", "Jun-Wei", ""], ["Liu", "Hongzhi", ""], ["Zhao", "Sheng", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1904.03450", "submitter": "Jian Zhu", "authors": "Jian Zhu and Zuoyu Tian and Sandra K\\\"ubler", "title": "UM-IU@LING at SemEval-2019 Task 6: Identifying Offensive Tweets Using\n  BERT and SVMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the UM-IU@LING's system for the SemEval 2019 Task 6:\nOffensEval. We take a mixed approach to identify and categorize hate speech in\nsocial media. In subtask A, we fine-tuned a BERT based classifier to detect\nabusive content in tweets, achieving a macro F1 score of 0.8136 on the test\ndata, thus reaching the 3rd rank out of 103 submissions. In subtasks B and C,\nwe used a linear SVM with selected character n-gram features. For subtask C,\nour system could identify the target of abuse with a macro F1 score of 0.5243,\nranking it 27th out of 65 submissions.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 14:02:13 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhu", "Jian", ""], ["Tian", "Zuoyu", ""], ["K\u00fcbler", "Sandra", ""]]}, {"id": "1904.03454", "submitter": "Wang Chen", "authors": "Wang Chen, Hou Pong Chan, Piji Li, Lidong Bing, Irwin King", "title": "An Integrated Approach for Keyphrase Generation via Exploring the Power\n  of Retrieval and Extraction", "comments": "NAACL 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel integrated approach for keyphrase\ngeneration (KG). Unlike previous works which are purely extractive or\ngenerative, we first propose a new multi-task learning framework that jointly\nlearns an extractive model and a generative model. Besides extracting\nkeyphrases, the output of the extractive model is also employed to rectify the\ncopy probability distribution of the generative model, such that the generative\nmodel can better identify important contents from the given document. Moreover,\nwe retrieve similar documents with the given document from training data and\nuse their associated keyphrases as external knowledge for the generative model\nto produce more accurate keyphrases. For further exploiting the power of\nextraction and retrieval, we propose a neural-based merging module to combine\nand re-rank the predicted keyphrases from the enhanced generative model, the\nextractive model, and the retrieved keyphrases. Experiments on the five KG\nbenchmarks demonstrate that our integrated approach outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 14:18:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Chen", "Wang", ""], ["Chan", "Hou Pong", ""], ["Li", "Piji", ""], ["Bing", "Lidong", ""], ["King", "Irwin", ""]]}, {"id": "1904.03461", "submitter": "Erik Wijmans", "authors": "Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia\n  Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra", "title": "Embodied Question Answering in Photorealistic Environments with Point\n  Cloud Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help bridge the gap between internet vision-style problems and the goal of\nvision for embodied perception we instantiate a large-scale navigation task --\nEmbodied Question Answering [1] in photo-realistic environments (Matterport\n3D). We thoroughly study navigation policies that utilize 3D point clouds, RGB\nimages, or their combination. Our analysis of these models reveals several key\nfindings. We find that two seemingly naive navigation baselines, forward-only\nand random, are strong navigators and challenging to outperform, due to the\nspecific choice of the evaluation setting presented by [1]. We find a novel\nloss-weighting scheme we call Inflection Weighting to be important when\ntraining recurrent models for navigation with behavior cloning and are able to\nout perform the baselines with this technique. We find that point clouds\nprovide a richer signal than RGB images for learning obstacle avoidance,\nmotivating the use (and continued study) of 3D deep learning models for\nembodied navigation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 14:50:11 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wijmans", "Erik", ""], ["Datta", "Samyak", ""], ["Maksymets", "Oleksandr", ""], ["Das", "Abhishek", ""], ["Gkioxari", "Georgia", ""], ["Lee", "Stefan", ""], ["Essa", "Irfan", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.03478", "submitter": "Bob Coecke", "authors": "Bob Coecke", "title": "The Mathematics of Text Structure", "comments": "37 pages, many pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work we gave a mathematical foundation, referred to as DisCoCat,\nfor how words interact in a sentence in order to produce the meaning of that\nsentence. To do so, we exploited the perfect structural match of grammar and\ncategories of meaning spaces. Here, we give a mathematical foundation, referred\nto as DisCoCirc, for how sentences interact in texts in order to produce the\nmeaning of that text. First we revisit DisCoCat. While in DisCoCat all meanings\nare fixed as states (i.e. have no input), in DisCoCirc word meanings correspond\nto a type, or system, and the states of this system can evolve. Sentences are\ngates within a circuit which update the variable meanings of those words. Like\nin DisCoCat, word meanings can live in a variety of spaces e.g. propositional,\nvectorial, or cognitive. The compositional structure are string diagrams\nrepresenting information flows, and an entire text yields a single string\ndiagram in which word meanings lift to the meaning of an entire text. While the\ndevelopments in this paper are independent of a physical embodiment (cf.\nclassical vs. quantum computing), both the compositional formalism and\nsuggested meaning model are highly quantum-inspired, and implementation on a\nquantum computer would come with a range of benefits. We also praise Jim Lambek\nfor his role in mathematical linguistics in general, and the development of the\nDisCo program more specifically.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 15:47:13 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 16:55:03 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Coecke", "Bob", ""]]}, {"id": "1904.03493", "submitter": "Xin Wang", "authors": "Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang\n  Wang", "title": "VATEX: A Large-Scale, High-Quality Multilingual Dataset for\n  Video-and-Language Research", "comments": "ICCV 2019 Oral. 17 pages, 14 figures, 6 tables (updated the VATEX\n  website link: vatex-challenge.org)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new large-scale multilingual video description dataset, VATEX,\nwhich contains over 41,250 videos and 825,000 captions in both English and\nChinese. Among the captions, there are over 206,000 English-Chinese parallel\ntranslation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is\nmultilingual, larger, linguistically complex, and more diverse in terms of both\nvideo and natural language descriptions. We also introduce two tasks for\nvideo-and-language research based on VATEX: (1) Multilingual Video Captioning,\naimed at describing a video in various languages with a compact unified\ncaptioning model, and (2) Video-guided Machine Translation, to translate a\nsource language description into the target language using the video\ninformation as additional spatiotemporal context. Extensive experiments on the\nVATEX dataset show that, first, the unified multilingual model can not only\nproduce both English and Chinese descriptions for a video more efficiently, but\nalso offer improved performance over the monolingual models. Furthermore, we\ndemonstrate that the spatiotemporal video context can be effectively utilized\nto align source and target languages and thus assist machine translation. In\nthe end, we discuss the potentials of using VATEX for other video-and-language\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 16:50:31 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 06:29:53 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 16:47:55 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Wang", "Xin", ""], ["Wu", "Jiawei", ""], ["Chen", "Junkun", ""], ["Li", "Lei", ""], ["Wang", "Yuan-Fang", ""], ["Wang", "William Yang", ""]]}, {"id": "1904.03513", "submitter": "Ramy Baly", "authors": "Abdelrhman Saleh (1), Ramy Baly (2), Alberto Barr\\'on-Cede\\~no (3),\n  Giovanni Da San Martino (3), Mitra Mohtarami (2), Preslav Nakov (3) and James\n  Glass (2) ((1) Harvard University, MA, USA, (2) MIT Computer Science and\n  Artificial Intelligence Laboratory, MA, USA, (3) Qatar Computing Research\n  Institute, HBKU, Qatar)", "title": "Team QCRI-MIT at SemEval-2019 Task 4: Propaganda Analysis Meets\n  Hyperpartisan News Detection", "comments": "Hyperpartisanship, propaganda, news media, fake news, SemEval-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our submission to SemEval-2019 Task 4 on\nHyperpartisan News Detection. Our system relies on a variety of engineered\nfeatures originally used to detect propaganda. This is based on the assumption\nthat biased messages are propagandistic in the sense that they promote a\nparticular political cause or viewpoint. We trained a logistic regression model\nwith features ranging from simple bag-of-words to vocabulary richness and text\nreadability features. Our system achieved 72.9% accuracy on the test data that\nis annotated manually and 60.8% on the test data that is annotated with distant\nsupervision. Additional experiments showed that significant performance\nimprovements can be achieved with better feature pre-processing.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 19:04:29 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Saleh", "Abdelrhman", ""], ["Baly", "Ramy", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Martino", "Giovanni Da San", ""], ["Mohtarami", "Mitra", ""], ["Nakov", "Preslav", ""], ["Glass", "James", ""]]}, {"id": "1904.03518", "submitter": "Aditya Gupta", "authors": "Aditya Gupta and Greg Durrett", "title": "Tracking Discrete and Continuous Entity State for Process Understanding", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural text, which describes entities and their interactions as they\nundergo some process, depicts entities in a uniquely nuanced way. First, each\nentity may have some observable discrete attributes, such as its state or\nlocation; modeling these involves imposing global structure and enforcing\nconsistency. Second, an entity may have properties which are not made explicit\nbut can be effectively induced and tracked by neural networks. In this paper,\nwe propose a structured neural architecture that reflects this dual nature of\nentity evolution. The model tracks each entity recurrently, updating its hidden\ncontinuous representation at each step to contain relevant state information.\nThe global discrete state structure is explicitly modeled with a neural CRF\nover the changing hidden representation of the entity. This CRF can explicitly\ncapture constraints on entity states over time, enforcing that, for example, an\nentity cannot move to a location after it is destroyed. We evaluate the\nperformance of our proposed model on QA tasks over process paragraphs in the\nProPara dataset and find that our model achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 19:56:14 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Gupta", "Aditya", ""], ["Durrett", "Greg", ""]]}, {"id": "1904.03576", "submitter": "Panayiotis Georgiou", "authors": "Prashanth Gurunath Shivakumar, Mu Yang, Panayiotis Georgiou", "title": "Spoken Language Intent Detection using Confusion2Vec", "comments": null, "journal-ref": "Proceedings of Interspeech 2019", "doi": "10.21437/Interspeech.2019-2226", "report-no": "2226", "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding speaker's intent is a crucial part of spoken language understanding\n(SLU). The presence of noise or errors in the text transcriptions, in real life\nscenarios make the task more challenging. In this paper, we address the spoken\nlanguage intent detection under noisy conditions imposed by automatic speech\nrecognition (ASR) systems. We propose to employ confusion2vec word feature\nrepresentation to compensate for the errors made by ASR and to increase the\nrobustness of the SLU system. The confusion2vec, motivated from human speech\nproduction and perception, models acoustic relationships between words in\naddition to the semantic and syntactic relations of words in human language. We\nhypothesize that ASR often makes errors relating to acoustically similar words,\nand the confusion2vec with inherent model of acoustic relationships between\nwords is able to compensate for the errors. We demonstrate through experiments\non the ATIS benchmark dataset, the robustness of the proposed model to achieve\nstate-of-the-art results under noisy ASR conditions. Our system reduces\nclassification error rate (CER) by 20.84% and improves robustness by 37.48%\n(lower CER degradation) relative to the previous state-of-the-art going from\nclean to noisy transcripts. Improvements are also demonstrated when training\nthe intent detection models on noisy transcripts.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 03:18:44 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 23:14:58 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 00:31:05 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Shivakumar", "Prashanth Gurunath", ""], ["Yang", "Mu", ""], ["Georgiou", "Panayiotis", ""]]}, {"id": "1904.03595", "submitter": "Youssef Tamaazousti", "authors": "Sara Meftah, Youssef Tamaazousti, Nasredine Semmar, Hassane Essafi,\n  Fatiha Sadat", "title": "Joint Learning of Pre-Trained and Random Units for Domain Adaptation in\n  Part-of-Speech Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning neural networks is widely used to transfer valuable knowledge\nfrom high-resource to low-resource domains. In a standard fine-tuning scheme,\nsource and target problems are trained using the same architecture. Although\ncapable of adapting to new domains, pre-trained units struggle with learning\nuncommon target-specific patterns. In this paper, we propose to augment the\ntarget-network with normalised, weighted and randomly initialised units that\nbeget a better adaptation while maintaining the valuable source knowledge. Our\nexperiments on POS tagging of social media texts (Tweets domain) demonstrate\nthat our method achieves state-of-the-art performances on 3 commonly used\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 06:46:36 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Meftah", "Sara", ""], ["Tamaazousti", "Youssef", ""], ["Semmar", "Nasredine", ""], ["Essafi", "Hassane", ""], ["Sadat", "Fatiha", ""]]}, {"id": "1904.03651", "submitter": "Christos Baziotis", "authors": "Christos Baziotis, Ion Androutsopoulos, Ioannis Konstas, Alexandros\n  Potamianos", "title": "SEQ^3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for\n  Unsupervised Abstractive Sentence Compression", "comments": "Accepted to NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence-to-sequence models are currently the dominant approach in\nseveral natural language processing tasks, but require large parallel corpora.\nWe present a sequence-to-sequence-to-sequence autoencoder (SEQ^3), consisting\nof two chained encoder-decoder pairs, with words used as a sequence of discrete\nlatent variables. We apply the proposed model to unsupervised abstractive\nsentence compression, where the first and last sequences are the input and\nreconstructed sentences, respectively, while the middle sequence is the\ncompressed sentence. Constraining the length of the latent word sequences\nforces the model to distill important information from the input. A pretrained\nlanguage model, acting as a prior over the latent sequences, encourages the\ncompressed sentences to be human-readable. Continuous relaxations enable us to\nsample from categorical distributions, allowing gradient-based optimization,\nunlike alternatives that rely on reinforcement learning. The proposed model\ndoes not require parallel text-summary pairs, achieving promising results in\nunsupervised sentence compression on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 13:47:28 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 16:20:31 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Baziotis", "Christos", ""], ["Androutsopoulos", "Ion", ""], ["Konstas", "Ioannis", ""], ["Potamianos", "Alexandros", ""]]}, {"id": "1904.03670", "submitter": "Loren Lugosch", "authors": "Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar,\n  Yoshua Bengio", "title": "Speech Model Pre-training for End-to-End Spoken Language Understanding", "comments": "Accepted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas conventional spoken language understanding (SLU) systems map speech\nto text, and then text to intent, end-to-end SLU systems map speech directly to\nintent through a single trainable model. Achieving high accuracy with these\nend-to-end models without a large amount of training data is difficult. We\npropose a method to reduce the data requirements of end-to-end SLU in which the\nmodel is first pre-trained to predict words and phonemes, thus learning good\nfeatures for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and\nshow that our method improves performance both when the full dataset is used\nfor training and when only a small subset is used. We also describe preliminary\nexperiments to gauge the model's ability to generalize to new phrases not heard\nduring training.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 15:24:32 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 17:56:23 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Lugosch", "Loren", ""], ["Ravanelli", "Mirco", ""], ["Ignoto", "Patrick", ""], ["Tomar", "Vikrant Singh", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1904.03713", "submitter": "Natalie Parde", "authors": "Natalie Parde and Rodney D. Nielsen", "title": "AI Meets Austen: Towards Human-Robot Discussions of Literary Metaphor", "comments": "Accepted to the 20th International Conference on Artificial\n  Intelligence in Education (AIED 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence is revolutionizing formal education, fueled by\ninnovations in learning assessment, content generation, and instructional\ndelivery. Informal, lifelong learning settings have been the subject of less\nattention. We provide a proof-of-concept for an embodied book discussion\ncompanion, designed to stimulate conversations with readers about particularly\ncreative metaphors in fiction literature. We collect ratings from 26\nparticipants, each of whom discuss Jane Austen's \"Pride and Prejudice\" with the\nrobot across one or more sessions, and find that participants rate their\ninteractions highly. This suggests that companion robots could be an\ninteresting entryway for the promotion of lifelong learning and cognitive\nexercise in future applications.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 19:01:32 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Parde", "Natalie", ""], ["Nielsen", "Rodney D.", ""]]}, {"id": "1904.03736", "submitter": "Weiyan Shi", "authors": "Weiyan Shi, Tiancheng Zhao, Zhou Yu", "title": "Unsupervised Dialog Structure Learning", "comments": "Long paper accepted by NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a shared dialog structure from a set of task-oriented dialogs is an\nimportant challenge in computational linguistics. The learned dialog structure\ncan shed light on how to analyze human dialogs, and more importantly contribute\nto the design and evaluation of dialog systems. We propose to extract dialog\nstructures using a modified VRNN model with discrete latent vectors. Different\nfrom existing HMM-based models, our model is based on variational-autoencoder\n(VAE). Such model is able to capture more dynamics in dialogs beyond the\nsurface forms of the language. We find that qualitatively, our method extracts\nmeaningful dialog structure, and quantitatively, outperforms previous models on\nthe ability to predict unseen data. We further evaluate the model's\neffectiveness in a downstream task, the dialog system building task.\nExperiments show that, by integrating the learned dialog structure into the\nreward function design, the model converges faster and to a better outcome in a\nreinforcement learning setting.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 20:28:47 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 01:54:46 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Shi", "Weiyan", ""], ["Zhao", "Tiancheng", ""], ["Yu", "Zhou", ""]]}, {"id": "1904.03746", "submitter": "Yoon Kim", "authors": "Yoon Kim, Alexander M. Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer,\n  G\\'abor Melis", "title": "Unsupervised Recurrent Neural Network Grammars", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network grammars (RNNG) are generative models of language\nwhich jointly model syntax and surface structure by incrementally generating a\nsyntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs\nachieve strong language modeling and parsing performance, but require an\nannotated corpus of parse trees. In this work, we experiment with unsupervised\nlearning of RNNGs. Since directly marginalizing over the space of latent trees\nis intractable, we instead apply amortized variational inference. To maximize\nthe evidence lower bound, we develop an inference network parameterized as a\nneural CRF constituency parser. On language modeling, unsupervised RNNGs\nperform as well their supervised counterparts on benchmarks in English and\nChinese. On constituency grammar induction, they are competitive with recent\nneural language models that induce tree structures from words through attention\nmechanisms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:14:43 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:00:47 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 17:56:54 GMT"}, {"version": "v4", "created": "Wed, 12 Jun 2019 04:48:38 GMT"}, {"version": "v5", "created": "Fri, 14 Jun 2019 02:58:11 GMT"}, {"version": "v6", "created": "Mon, 5 Aug 2019 01:21:15 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Kim", "Yoon", ""], ["Rush", "Alexander M.", ""], ["Yu", "Lei", ""], ["Kuncoro", "Adhiguna", ""], ["Dyer", "Chris", ""], ["Melis", "G\u00e1bor", ""]]}, {"id": "1904.03799", "submitter": "Yerbolat Khassanov", "authors": "Yerbolat Khassanov, Zhiping Zeng, Van Tung Pham, Haihua Xu, Eng Siong\n  Chng", "title": "Enriching Rare Word Representations in Neural Language Models by\n  Embedding Matrix Augmentation", "comments": "5 pages, 2 figures, accepted to INTERSPEECH 2019", "journal-ref": null, "doi": "10.21437/Interspeech.2019-1858", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neural language models (NLM) achieve strong generalization capability by\nlearning the dense representation of words and using them to estimate\nprobability distribution function. However, learning the representation of rare\nwords is a challenging problem causing the NLM to produce unreliable\nprobability estimates. To address this problem, we propose a method to enrich\nrepresentations of rare words in pre-trained NLM and consequently improve its\nprobability estimation performance. The proposed method augments the word\nembedding matrices of pre-trained NLM while keeping other parameters unchanged.\nSpecifically, our method updates the embedding vectors of rare words using\nembedding vectors of other semantically and syntactically similar words. To\nevaluate the proposed method, we enrich the rare street names in the\npre-trained NLM and use it to rescore 100-best hypotheses output from the\nSingapore English speech recognition system. The enriched NLM reduces the word\nerror rate by 6% relative and improves the recognition accuracy of the rare\nwords by 16% absolute as compared to the baseline NLM.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 01:50:45 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 09:58:43 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Khassanov", "Yerbolat", ""], ["Zeng", "Zhiping", ""], ["Pham", "Van Tung", ""], ["Xu", "Haihua", ""], ["Chng", "Eng Siong", ""]]}, {"id": "1904.03802", "submitter": "Yerbolat Khassanov", "authors": "Yerbolat Khassanov, Haihua Xu, Van Tung Pham, Zhiping Zeng, Eng Siong\n  Chng, Chongjia Ni, Bin Ma", "title": "Constrained Output Embeddings for End-to-End Code-Switching Speech\n  Recognition with Only Monolingual Data", "comments": "5 pages, 3 figures, accepted to INTERSPEECH 2019", "journal-ref": null, "doi": "10.21437/Interspeech.2019-1867", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of code-switch training data is one of the major concerns in the\ndevelopment of end-to-end code-switching automatic speech recognition (ASR)\nmodels. In this work, we propose a method to train an improved end-to-end\ncode-switching ASR using only monolingual data. Our method encourages the\ndistributions of output token embeddings of monolingual languages to be\nsimilar, and hence, promotes the ASR model to easily code-switch between\nlanguages. Specifically, we propose to use Jensen-Shannon divergence and cosine\ndistance based constraints. The former will enforce output embeddings of\nmonolingual languages to possess similar distributions, while the later simply\nbrings the centroids of two distributions to be close to each other.\nExperimental results demonstrate high effectiveness of the proposed method,\nyielding up to 4.5% absolute mixed error rate improvement on Mandarin-English\ncode-switching ASR task.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 02:16:44 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 10:02:27 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Khassanov", "Yerbolat", ""], ["Xu", "Haihua", ""], ["Pham", "Van Tung", ""], ["Zeng", "Zhiping", ""], ["Chng", "Eng Siong", ""], ["Ni", "Chongjia", ""], ["Ma", "Bin", ""]]}, {"id": "1904.03879", "submitter": "Shuhao Gu", "authors": "Shuhao Gu, Yang Feng, Qun Liu", "title": "Improving Domain Adaptation Translation with Domain Invariant and\n  Specific Information", "comments": "11 pages, accepted by NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In domain adaptation for neural machine translation, translation performance\ncan benefit from separating features into domain-specific features and common\nfeatures. In this paper, we propose a method to explicitly model the two kinds\nof information in the encoder-decoder framework so as to exploit out-of-domain\ndata in in-domain training. In our method, we maintain a private encoder and a\nprivate decoder for each domain which are used to model domain-specific\ninformation. In the meantime, we introduce a common encoder and a common\ndecoder shared by all the domains which can only have domain-independent\ninformation flow through. Besides, we add a discriminator to the shared encoder\nand employ adversarial training for the whole model to reinforce the\nperformance of information separation and machine translation simultaneously.\nExperiment results show that our method can outperform competitive baselines\ngreatly on multiple data sets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 08:00:25 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 12:32:26 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Gu", "Shuhao", ""], ["Feng", "Yang", ""], ["Liu", "Qun", ""]]}, {"id": "1904.03885", "submitter": "Peratham Wiriyathammabhum Mr.", "authors": "Peratham Wiriyathammabhum, Abhinav Shrivastava, Vlad I. Morariu, Larry\n  S. Davis", "title": "Referring to Objects in Videos using Spatio-Temporal Identifying\n  Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new task, the grounding of spatio-temporal identifying\ndescriptions in videos. Previous work suggests potential bias in existing\ndatasets and emphasizes the need for a new data creation schema to better model\nlinguistic structure. We introduce a new data collection scheme based on\ngrammatical constraints for surface realization to enable us to investigate the\nproblem of grounding spatio-temporal identifying descriptions in videos. We\nthen propose a two-stream modular attention network that learns and grounds\nspatio-temporal identifying descriptions based on appearance and motion. We\nshow that motion modules help to ground motion-related words and also help to\nlearn in appearance modules because modular neural networks resolve task\ninterference between modules. Finally, we propose a future challenge and a need\nfor a robust system arising from replacing ground truth visual annotations with\nautomatic video object detector and temporal event localization.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 08:28:54 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wiriyathammabhum", "Peratham", ""], ["Shrivastava", "Abhinav", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1904.03898", "submitter": "Jue Wang", "authors": "Jue Wang, Ke Chen, Lidan Shou, Sai Wu and Sharad Mehrotra", "title": "Semi-Supervised Few-Shot Learning for Dual Question-Answer Extraction", "comments": "7 pages, 5 figures, submission to IJCAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of key phrase extraction from sentences.\nExisting state-of-the-art supervised methods require large amounts of annotated\ndata to achieve good performance and generalization. Collecting labeled data\nis, however, often expensive. In this paper, we redefine the problem as\nquestion-answer extraction, and present SAMIE: Self-Asking Model for\nInformation Ixtraction, a semi-supervised model which dually learns to ask and\nto answer questions by itself. Briefly, given a sentence $s$ and an answer $a$,\nthe model needs to choose the most appropriate question $\\hat q$; meanwhile,\nfor the given sentence $s$ and same question $\\hat q$ selected in the previous\nstep, the model will predict an answer $\\hat a$. The model can support few-shot\nlearning with very limited supervision. It can also be used to perform\nclustering analysis when no supervision is provided. Experimental results show\nthat the proposed method outperforms typical supervised methods especially when\ngiven little labeled data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:07:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Jue", ""], ["Chen", "Ke", ""], ["Shou", "Lidan", ""], ["Wu", "Sai", ""], ["Mehrotra", "Sharad", ""]]}, {"id": "1904.03922", "submitter": "Martin Josifoski", "authors": "Martin Josifoski, Ivan S. Paskov, Hristo S. Paskov, Martin Jaggi,\n  Robert West", "title": "Crosslingual Document Embedding as Reduced-Rank Ridge Regression", "comments": "In The Twelfth ACM International Conference on Web Search and Data\n  Mining (WSDM '19)", "journal-ref": null, "doi": "10.1145/3289600.3291023", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There has recently been much interest in extending vector-based word\nrepresentations to multiple languages, such that words can be compared across\nlanguages. In this paper, we shift the focus from words to documents and\nintroduce a method for embedding documents written in any language into a\nsingle, language-independent vector space. For training, our approach leverages\na multilingual corpus where the same concept is covered in multiple languages\n(but not necessarily via exact translations), such as Wikipedia. Our method,\nCr5 (Crosslingual reduced-rank ridge regression), starts by training a\nridge-regression-based classifier that uses language-specific bag-of-word\nfeatures in order to predict the concept that a given document is about. We\nshow that, when constraining the learned weight matrix to be of low rank, it\ncan be factored to obtain the desired mappings from language-specific\nbags-of-words to language-independent embeddings. As opposed to most prior\nmethods, which use pretrained monolingual word vectors, postprocess them to\nmake them crosslingual, and finally average word vectors to obtain document\nvectors, Cr5 is trained end-to-end and is thus natively crosslingual as well as\ndocument-level. Moreover, since our algorithm uses the singular value\ndecomposition as its core operation, it is highly scalable. Experiments show\nthat our method achieves state-of-the-art performance on a crosslingual\ndocument retrieval task. Finally, although not trained for embedding sentences\nand words, it also achieves competitive performance on crosslingual sentence\nand word retrieval tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 10:02:05 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Josifoski", "Martin", ""], ["Paskov", "Ivan S.", ""], ["Paskov", "Hristo S.", ""], ["Jaggi", "Martin", ""], ["West", "Robert", ""]]}, {"id": "1904.03969", "submitter": "Isabelle Augenstein", "authors": "Mareike Hartmann and Tallulah Jansen and Isabelle Augenstein and\n  Anders S{\\o}gaard", "title": "Issue Framing in Online Discussion Fora", "comments": "To appear in NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online discussion fora, speakers often make arguments for or against\nsomething, say birth control, by highlighting certain aspects of the topic. In\nsocial science, this is referred to as issue framing. In this paper, we\nintroduce a new issue frame annotated corpus of online discussions. We explore\nto what extent models trained to detect issue frames in newswire and social\nmedia can be transferred to the domain of discussion fora, using a combination\nof multi-task and adversarial training, assuming only unlabeled training data\nin the target domain.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 11:36:53 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 07:58:22 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Hartmann", "Mareike", ""], ["Jansen", "Tallulah", ""], ["Augenstein", "Isabelle", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1904.03971", "submitter": "Ehsan Montahaei", "authors": "Ehsan Montahaei, Danial Alihosseini and Mahdieh Soleymani Baghshah", "title": "Jointly Measuring Diversity and Quality in Text Generation Models", "comments": "Accepted by NAACL 2019 workshop (NeuralGen 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generation is an important Natural Language Processing task with various\napplications. Although several metrics have already been introduced to evaluate\nthe text generation methods, each of them has its own shortcomings. The most\nwidely used metrics such as BLEU only consider the quality of generated\nsentences and neglect their diversity. For example, repeatedly generation of\nonly one high quality sentence would result in a high BLEU score. On the other\nhand, the more recent metric introduced to evaluate the diversity of generated\ntexts known as Self-BLEU ignores the quality of generated texts. In this paper,\nwe propose metrics to evaluate both the quality and diversity simultaneously by\napproximating the distance of the learned generative model and the real data\ndistribution. For this purpose, we first introduce a metric that approximates\nthis distance using n-gram based measures. Then, a feature-based measure which\nis based on a recent highly deep model trained on a large text corpus called\nBERT is introduced. Finally, for oracle training mode in which the generator's\ndensity can also be calculated, we propose to use the distance measures between\nthe corresponding explicit distributions. Eventually, the most popular and\nrecent text generation models are evaluated using both the existing and the\nproposed metrics and the preferences of the proposed metrics are determined.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 11:44:41 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 21:14:54 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Montahaei", "Ehsan", ""], ["Alihosseini", "Danial", ""], ["Baghshah", "Mahdieh Soleymani", ""]]}, {"id": "1904.03991", "submitter": "Michael Ramscar", "authors": "Michael Ramscar", "title": "Source codes in human communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although information theoretic characterizations of human communication have\nbecome increasingly popular in linguistics, to date they have largely involved\ngrafting probabilistic constructs onto older ideas about grammar. Similarities\nbetween human and digital communication have been strongly emphasized, and\ndifferences largely ignored. However, some of these differences matter:\ncommunication systems are based on predefined codes shared by every\nsender-receiver, whereas the distributions of words in natural languages\nguarantee that no speaker-hearer ever has access to an entire linguistic code,\nwhich seemingly undermines the idea that natural languages are probabilistic\nsystems in any meaningful sense. This paper describes how the distributional\nproperties of languages meet the various challenges arising from the\ndifferences between information systems and natural languages, along with the\nvery different view of human communication these properties suggest.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 14:45:53 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ramscar", "Michael", ""]]}, {"id": "1904.04019", "submitter": "Giosu\\'e Lo Bosco", "authors": "Mattia Antonino Di Gangi, Giosu\\'e Lo Bosco, Giovanni Pilato", "title": "Effectiveness of Data-Driven Induction of Semantic Spaces and\n  Traditional Classifiers for Sarcasm Detection", "comments": "37 pages, 7 figures, version 4", "journal-ref": "Natural Language Engineering, 25(2), 257-285 (2019)", "doi": "10.1017/S1351324919000019", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irony and sarcasm are two complex linguistic phenomena that are widely used\nin everyday language and especially over the social media, but they represent\ntwo serious issues for automated text understanding. Many labeled corpora have\nbeen extracted from several sources to accomplish this task, and it seems that\nsarcasm is conveyed in different ways for different domains. Nonetheless, very\nlittle work has been done for comparing different methods among the available\ncorpora. Furthermore, usually, each author collects and uses their own datasets\nto evaluate his own method. In this paper, we show that sarcasm detection can\nbe tackled by applying classical machine learning algorithms to input texts\nsub-symbolically represented in a Latent Semantic space. The main consequence\nis that our studies establish both reference datasets and baselines for the\nsarcasm detection problem that could serve the scientific community to test\nnewly proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:49:02 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 10:20:39 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 09:17:12 GMT"}, {"version": "v4", "created": "Fri, 6 Dec 2019 16:45:25 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Di Gangi", "Mattia Antonino", ""], ["Bosco", "Giosu\u00e9 Lo", ""], ["Pilato", "Giovanni", ""]]}, {"id": "1904.04021", "submitter": "Tung Nguyen Thanh", "authors": "Tasnim Mohiuddin, Thanh-Tung Nguyen and Shafiq Joty", "title": "Adaptation of Hierarchical Structured Models for Speech Act Recognition\n  in Asynchronous Conversation", "comments": "To appear in NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of speech act recognition (SAR) in asynchronous\nconversations (forums, emails). Unlike synchronous conversations (e.g.,\nmeetings, phone), asynchronous domains lack large labeled datasets to train an\neffective SAR model. In this paper, we propose methods to effectively leverage\nabundant unlabeled conversational data and the available labeled data from\nsynchronous domains. We carry out our research in three main steps. First, we\nintroduce a neural architecture based on hierarchical LSTMs and conditional\nrandom fields (CRF) for SAR, and show that our method outperforms existing\nmethods when trained on in-domain data only. Second, we improve our initial SAR\nmodels by semi-supervised learning in the form of pretrained word embeddings\nlearned from a large unlabeled conversational corpus. Finally, we employ\nadversarial training to improve the results further by leveraging the labeled\ndata from synchronous domains and by explicitly modeling the distributional\nshift in two domains.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 04:57:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mohiuddin", "Tasnim", ""], ["Nguyen", "Thanh-Tung", ""], ["Joty", "Shafiq", ""]]}, {"id": "1904.04032", "submitter": "Stamatis Outsios", "authors": "Stamatis Outsios, Christos Karatsalos, Konstantinos Skianis, Michalis\n  Vazirgiannis", "title": "Evaluation of Greek Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since word embeddings have been the most popular input for many NLP tasks,\nevaluating their quality is of critical importance. Most research efforts are\nfocusing on English word embeddings. This paper addresses the problem of\nconstructing and evaluating such models for the Greek language. We created a\nnew word analogy corpus considering the original English Word2vec word analogy\ncorpus and some specific linguistic aspects of the Greek language as well.\nMoreover, we created a Greek version of WordSim353 corpora for a basic\nevaluation of word similarities. We tested seven word vector models and our\nevaluation showed that we are able to create meaningful representations. Last,\nwe discovered that the morphological complexity of the Greek language and\npolysemy can influence the quality of the resulting word embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:56:34 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 19:27:04 GMT"}, {"version": "v3", "created": "Sat, 4 Apr 2020 14:44:15 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Outsios", "Stamatis", ""], ["Karatsalos", "Christos", ""], ["Skianis", "Konstantinos", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1904.04047", "submitter": "Thomas Manzini", "authors": "Thomas Manzini, Yao Chong Lim, Yulia Tsvetkov, Alan W Black", "title": "Black is to Criminal as Caucasian is to Police: Detecting and Removing\n  Multiclass Bias in Word Embeddings", "comments": "Accepted as a conference paper at NAACL. 5 Pages excluding\n  references, additional page for appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online texts -- across genres, registers, domains, and styles -- are riddled\nwith human stereotypes, expressed in overt or subtle ways. Word embeddings,\ntrained on these texts, perpetuate and amplify these stereotypes, and propagate\nbiases to machine learning models that use word embeddings as features. In this\nwork, we propose a method to debias word embeddings in multiclass settings such\nas race and religion, extending the work of (Bolukbasi et al., 2016) from the\nbinary setting, such as binary gender. Next, we propose a novel methodology for\nthe evaluation of multiclass debiasing. We demonstrate that our multiclass\ndebiasing is robust and maintains the efficacy in standard NLP tasks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 22:17:27 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 16:59:50 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 01:16:15 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Manzini", "Thomas", ""], ["Lim", "Yao Chong", ""], ["Tsvetkov", "Yulia", ""], ["Black", "Alan W", ""]]}, {"id": "1904.04049", "submitter": "Wenbo Zhao", "authors": "Wenbo Zhao, Tagyoung Chung, Anuj Goyal, Angeliki Metallinou", "title": "Simple Question Answering with Subgraph Ranking and Joint-Scoring", "comments": "Accepted by The 2019 Annual Conference of the North American Chapter\n  of the Association for Computational Linguistics (NAACL-HLT 2019). 11 pages,\n  1 figure", "journal-ref": null, "doi": "10.18653/v1/N19-1029", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph based simple question answering (KBSQA) is a major area of\nresearch within question answering. Although only dealing with simple\nquestions, i.e., questions that can be answered through a single knowledge base\n(KB) fact, this task is neither simple nor close to being solved. Targeting on\nthe two main steps, subgraph selection and fact selection, the research\ncommunity has developed sophisticated approaches. However, the importance of\nsubgraph ranking and leveraging the subject--relation dependency of a KB fact\nhave not been sufficiently explored. Motivated by this, we present a unified\nframework to describe and analyze existing approaches. Using this framework as\na starting point, we focus on two aspects: improving subgraph selection through\na novel ranking method and leveraging the subject--relation dependency by\nproposing a joint scoring CNN model with a novel loss function that enforces\nthe well-order of scores. Our methods achieve a new state of the art (85.44% in\naccuracy) on the SimpleQuestions dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 02:20:50 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Zhao", "Wenbo", ""], ["Chung", "Tagyoung", ""], ["Goyal", "Anuj", ""], ["Metallinou", "Angeliki", ""]]}, {"id": "1904.04055", "submitter": "Jan Koco\\'n", "authors": "Jan Koco\\'n, Micha{\\l} Gawor", "title": "Evaluating KGR10 Polish word embeddings in the recognition of temporal\n  expressions using BiLSTM-CRF", "comments": "Presented at TFML 2019 (Theoretical Foundations of Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article introduces a new set of Polish word embeddings, built using KGR10\ncorpus, which contains more than 4 billion words. These embeddings are\nevaluated in the problem of recognition of temporal expressions (timexes) for\nthe Polish language. We described the process of KGR10 corpus creation and a\nnew approach to the recognition problem using Bidirectional Long-Short Term\nMemory (BiLSTM) network with additional CRF layer, where specific embeddings\nare essential. We presented experiments and conclusions drawn from them.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:47:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Koco\u0144", "Jan", ""], ["Gawor", "Micha\u0142", ""]]}, {"id": "1904.04063", "submitter": "Afra Alishahi", "authors": "Afra Alishahi and Grzegorz Chrupa{\\l}a and Tal Linzen", "title": "Analyzing and Interpreting Neural Networks for NLP: A Report on the\n  First BlackboxNLP Workshop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EMNLP 2018 workshop BlackboxNLP was dedicated to resources and techniques\nspecifically developed for analyzing and understanding the inner-workings and\nrepresentations acquired by neural models of language. Approaches included:\nsystematic manipulation of input to neural networks and investigating the\nimpact on their performance, testing whether interpretable knowledge can be\ndecoded from intermediate representations acquired by neural networks,\nproposing modifications to neural network architectures to make their knowledge\nstate or generated output more explainable, and examining the performance of\nnetworks on simplified or formal languages. Here we review a number of\nrepresentative studies in each category.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:15:45 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Alishahi", "Afra", ""], ["Chrupa\u0142a", "Grzegorz", ""], ["Linzen", "Tal", ""]]}, {"id": "1904.04073", "submitter": "Pushkar Mishra", "authors": "Pushkar Mishra, Marco Del Tredici, Helen Yannakoudakis, Ekaterina\n  Shutova", "title": "Abusive Language Detection with Graph Convolutional Networks", "comments": "Proceedings of the 2019 Annual Conference of the North American\n  Chapter of the Association for Computational Linguistics: Human Language\n  Technologies (NAACL-HLT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abuse on the Internet represents a significant societal problem of our time.\nPrevious research on automated abusive language detection in Twitter has shown\nthat community-based profiling of users is a promising technique for this task.\nHowever, existing approaches only capture shallow properties of online\ncommunities by modeling follower-following relationships. In contrast, working\nwith graph convolutional networks (GCNs), we present the first approach that\ncaptures not only the structure of online communities but also the linguistic\nbehavior of the users within them. We show that such a heterogeneous\ngraph-structured modeling of communities significantly advances the current\nstate of the art in abusive language detection.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 03:49:22 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mishra", "Pushkar", ""], ["Del Tredici", "Marco", ""], ["Yannakoudakis", "Helen", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "1904.04079", "submitter": "Weijia Xu", "authors": "Weijia Xu, Xing Niu, Marine Carpuat", "title": "Differentiable Sampling with Flexible Reference Word Order for Neural\n  Machine Translation", "comments": "Accepted at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite some empirical success at correcting exposure bias in machine\ntranslation, scheduled sampling algorithms suffer from a major drawback: they\nincorrectly assume that words in the reference translations and in sampled\nsequences are aligned at each time step. Our new differentiable sampling\nalgorithm addresses this issue by optimizing the probability that the reference\ncan be aligned with the sampled output, based on a soft alignment predicted by\nthe model itself. As a result, the output distribution at each time step is\nevaluated with respect to the whole predicted sequence. Experiments on IWSLT\ntranslation tasks show that our approach improves BLEU compared to maximum\nlikelihood and scheduled sampling baselines. In addition, our approach is\nsimpler to train with no need for sampling schedule and yields models that\nachieve larger improvements with smaller beam sizes.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 04:48:07 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 21:46:15 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Xu", "Weijia", ""], ["Niu", "Xing", ""], ["Carpuat", "Marine", ""]]}, {"id": "1904.04096", "submitter": "Fatma Nasoz", "authors": "Nishit Shrestha and Fatma Nasoz", "title": "Deep Learning Sentiment Analysis of Amazon.com Reviews and Ratings", "comments": "15 pages, 10 figures, 3 tables, journal article", "journal-ref": "International Journal on Soft Computing, Artificial Intelligence\n  and Applications (IJSCAI), Vol.8, No.1, February 2019", "doi": "10.5121/ijscai.2019.8101", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our study employs sentiment analysis to evaluate the compatibility of\nAmazon.com reviews with their corresponding ratings. Sentiment analysis is the\ntask of identifying and classifying the sentiment expressed in a piece of text\nas being positive or negative. On e-commerce websites such as Amazon.com,\nconsumers can submit their reviews along with a specific polarity rating. In\nsome instances, there is a mismatch between the review and the rating. To\nidentify the reviews with mismatched ratings we performed sentiment analysis\nusing deep learning on Amazon.com product review data. Product reviews were\nconverted to vectors using paragraph vector, which then was used to train a\nrecurrent neural network with gated recurrent unit. Our model incorporated both\nsemantic relationship of review text and product information. We also developed\na web service application that predicts the rating score for a submitted review\nusing the trained model and if there is a mismatch between predicted rating\nscore and submitted rating score, it provides feedback to the reviewer.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 21:34:45 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Shrestha", "Nishit", ""], ["Nasoz", "Fatma", ""]]}, {"id": "1904.04100", "submitter": "Kuan-Yu Chen", "authors": "Kuan-Yu Chen, Che-Ping Tsai, Da-Rong Liu, Hung-Yi Lee, Lin-shan Lee", "title": "Completely Unsupervised Speech Recognition By A Generative Adversarial\n  Network Harmonized With Iteratively Refined Hidden Markov Models", "comments": "Accepted by Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Producing a large annotated speech corpus for training ASR systems remains\ndifficult for more than 95% of languages all over the world which are\nlow-resourced, but collecting a relatively big unlabeled data set for such\nlanguages is more achievable. This is why some initial effort have been\nreported on completely unsupervised speech recognition learned from unlabeled\ndata only, although with relatively high error rates. In this paper, we develop\na Generative Adversarial Network (GAN) to achieve this purpose, in which a\nGenerator and a Discriminator learn from each other iteratively to improve the\nperformance. We further use a set of Hidden Markov Models (HMMs) iteratively\nrefined from the machine generated labels to work in harmony with the GAN. The\ninitial experiments on TIMIT data set achieve an phone error rate of 33.1%,\nwhich is 8.5% lower than the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 14:47:45 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 06:04:44 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 15:58:10 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Chen", "Kuan-Yu", ""], ["Tsai", "Che-Ping", ""], ["Liu", "Da-Rong", ""], ["Lee", "Hung-Yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1904.04116", "submitter": "Tasnim Mohiuddin", "authors": "Tasnim Mohiuddin and Shafiq Joty", "title": "Revisiting Adversarial Autoencoder for Unsupervised Word Translation\n  with Cycle Consistency and Improved Training", "comments": "Published in NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has shown impressive success in learning bilingual\ndictionary without any parallel data by mapping monolingual embeddings to a\nshared space. However, recent work has shown superior performance for\nnon-adversarial methods in more challenging language pairs. In this work, we\nrevisit adversarial autoencoder for unsupervised word translation and propose\ntwo novel extensions to it that yield more stable training and improved\nresults. Our method includes regularization terms to enforce cycle consistency\nand input reconstruction, and puts the target encoders as an adversary against\nthe corresponding discriminator. Extensive experimentations with European,\nnon-European and low-resource languages show that our method is more robust and\nachieves better performance than recently proposed adversarial and\nnon-adversarial approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 12:46:07 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mohiuddin", "Tasnim", ""], ["Joty", "Shafiq", ""]]}, {"id": "1904.04153", "submitter": "Han Guo", "authors": "Han Guo, Ramakanth Pasunuru, Mohit Bansal", "title": "AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning", "comments": "NAACL 2019 (12 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) has achieved success over a wide range of problems,\nwhere the goal is to improve the performance of a primary task using a set of\nrelevant auxiliary tasks. However, when the usefulness of the auxiliary tasks\nw.r.t. the primary task is not known a priori, the success of MTL models\ndepends on the correct choice of these auxiliary tasks and also a balanced\nmixing ratio of these tasks during alternate training. These two problems could\nbe resolved via manual intuition or hyper-parameter tuning over all\ncombinatorial task choices, but this introduces inductive bias or is not\nscalable when the number of candidate auxiliary tasks is very large. To address\nthese issues, we present AutoSeM, a two-stage MTL pipeline, where the first\nstage automatically selects the most useful auxiliary tasks via a\nBeta-Bernoulli multi-armed bandit with Thompson Sampling, and the second stage\nlearns the training mixing ratio of these selected auxiliary tasks via a\nGaussian Process based Bayesian optimization framework. We conduct several MTL\nexperiments on the GLUE language understanding tasks, and show that our AutoSeM\nframework can successfully find relevant auxiliary tasks and automatically\nlearn their mixing ratio, achieving significant performance boosts on several\nprimary tasks. Finally, we present ablations for each stage of AutoSeM and\nanalyze the learned auxiliary task choices.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:05:43 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Guo", "Han", ""], ["Pasunuru", "Ramakanth", ""], ["Bansal", "Mohit", ""]]}, {"id": "1904.04163", "submitter": "Yangyang Shi", "authors": "Yangyang Shi and Mei-Yuh Hwang and Xin Lei and Haoyu Sheng", "title": "Knowledge Distillation For Recurrent Neural Network Language Modeling\n  With Trust Regularization", "comments": "ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have dominated language modeling because of\ntheir superior performance over traditional N-gram based models. In many\napplications, a large Recurrent Neural Network language model (RNNLM) or an\nensemble of several RNNLMs is used. These models have large memory footprints\nand require heavy computation. In this paper, we examine the effect of applying\nknowledge distillation in reducing the model size for RNNLMs. In addition, we\npropose a trust regularization method to improve the knowledge distillation\ntraining for RNNLMs. Using knowledge distillation with trust regularization, we\nreduce the parameter size to a third of that of the previously published best\nmodel while maintaining the state-of-the-art perplexity result on Penn Treebank\ndata. In a speech recognition N-bestrescoring task, we reduce the RNNLM model\nsize to 18.5% of the baseline system, with no degradation in word error\nrate(WER) performance on Wall Street Journal data set.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:16:01 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Shi", "Yangyang", ""], ["Hwang", "Mei-Yuh", ""], ["Lei", "Xin", ""], ["Sheng", "Haoyu", ""]]}, {"id": "1904.04195", "submitter": "Hao Tan", "authors": "Hao Tan, Licheng Yu, Mohit Bansal", "title": "Learning to Navigate Unseen Environments: Back Translation with\n  Environmental Dropout", "comments": "NAACL 2019 (12 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A grand goal in AI is to build a robot that can accurately navigate based on\nnatural language instructions, which requires the agent to perceive the scene,\nunderstand and ground language, and act in the real-world environment. One key\nchallenge here is to learn to navigate in new environments that are unseen\nduring training. Most of the existing approaches perform dramatically worse in\nunseen environments as compared to seen ones. In this paper, we present a\ngeneralizable navigational agent. Our agent is trained in two stages. The first\nstage is training via mixed imitation and reinforcement learning, combining the\nbenefits from both off-policy and on-policy optimization. The second stage is\nfine-tuning via newly-introduced 'unseen' triplets (environment, path,\ninstruction). To generate these unseen triplets, we propose a simple but\neffective 'environmental dropout' method to mimic unseen environments, which\novercomes the problem of limited seen environment variability. Next, we apply\nsemi-supervised learning (via back-translation) on these dropped-out\nenvironments to generate new paths and instructions. Empirically, we show that\nour agent is substantially better at generalizability when fine-tuned with\nthese triplets, outperforming the state-of-art approaches by a large margin on\nthe private unseen test set of the Room-to-Room task, and achieving the top\nrank on the leaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:14:52 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Tan", "Hao", ""], ["Yu", "Licheng", ""], ["Bansal", "Mohit", ""]]}, {"id": "1904.04206", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Elham Azimi, AmirAli Abdolrashidi", "title": "Deep-Sentiment: Sentiment Analysis Using Ensemble of CNN and Bi-LSTM\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of social networks, and e-commerce websites, sentiment\nanalysis has become a more active area of research in the past few years. On a\nhigh level, sentiment analysis tries to understand the public opinion about a\nspecific product or topic, or trends from reviews or tweets. Sentiment analysis\nplays an important role in better understanding customer/user opinion, and also\nextracting social/political trends. There has been a lot of previous works for\nsentiment analysis, some based on hand-engineering relevant textual features,\nand others based on different neural network architectures. In this work, we\npresent a model based on an ensemble of long-short-term-memory (LSTM), and\nconvolutional neural network (CNN), one to capture the temporal information of\nthe data, and the other one to extract the local structure thereof. Through\nexperimental results, we show that using this ensemble model we can outperform\nboth individual models. We are also able to achieve a very high accuracy rate\ncompared to the previous works.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:26:20 GMT"}], "update_date": "2019-04-14", "authors_parsed": [["Minaee", "Shervin", ""], ["Azimi", "Elham", ""], ["Abdolrashidi", "AmirAli", ""]]}, {"id": "1904.04294", "submitter": "Xiaofei Wang", "authors": "Xiaofei Wang, Jinyi Yang, Ruizhi Li, Samik Sadhu, Hynek Hermansky", "title": "Exploring Methods for the Automatic Detection of Errors in Manual\n  Transcription", "comments": "Submitted in Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality of data plays an important role in most deep learning tasks. In the\nspeech community, transcription of speech recording is indispensable. Since the\ntranscription is usually generated artificially, automatically finding errors\nin manual transcriptions not only saves time and labors but benefits the\nperformance of tasks that need the training process. Inspired by the success of\nhybrid automatic speech recognition using both language model and acoustic\nmodel, two approaches of automatic error detection in the transcriptions have\nbeen explored in this work. Previous study using a biased language model\napproach, relying on a strong transcription-dependent language model, has been\nreviewed. In this work, we propose a novel acoustic model based approach,\nfocusing on the phonetic sequence of speech. Both methods have been evaluated\non a completely real dataset, which was originally transcribed with errors and\nstrictly corrected manually afterwards.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:48:45 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 23:42:50 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Wang", "Xiaofei", ""], ["Yang", "Jinyi", ""], ["Li", "Ruizhi", ""], ["Sadhu", "Samik", ""], ["Hermansky", "Hynek", ""]]}, {"id": "1904.04307", "submitter": "Gerhard Wohlgenannt Dr.", "authors": "Ponrudee Netisopakul, Gerhard Wohlgenannt, Aleksei Pulich", "title": "Word Similarity Datasets for Thai: Construction and Evaluation", "comments": "submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributional semantics in the form of word embeddings are an essential\ningredient to many modern natural language processing systems. The\nquantification of semantic similarity between words can be used to evaluate the\nability of a system to perform semantic interpretation. To this end, a number\nof word similarity datasets have been created for the English language over the\nlast decades. For Thai language few such resources are available. In this work,\nwe create three Thai word similarity datasets by translating and re-rating the\npopular WordSim-353, SimLex-999 and SemEval-2017-Task-2 datasets. The three\ndatasets contain 1852 word pairs in total and have different characteristics in\nterms of difficulty, domain coverage, and notion of similarity (relatedness\nvs.~similarity). These features help to gain a broader picture of the\nproperties of an evaluated word embedding model. We include baseline\nevaluations with existing Thai embedding models, and identify the high ratio of\nout-of-vocabulary words as one of the biggest challenges. All datasets,\nevaluation results, and a tool for easy evaluation of new Thai embedding models\nare available to the NLP community online.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 19:18:09 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Netisopakul", "Ponrudee", ""], ["Wohlgenannt", "Gerhard", ""], ["Pulich", "Aleksei", ""]]}, {"id": "1904.04358", "submitter": "Pramit Saha", "authors": "Pramit Saha, Muhammad Abdul-Mageed and Sidney Fels", "title": "Deep Learning the EEG Manifold for Phonological Categorization from\n  Active Thoughts", "comments": "Accepted for publication in IEEE ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an\nalternative vocal communication pathway for people with speaking disabilities.\nAs a step towards full decoding of imagined speech from active thoughts, we\npresent a BCI system for subject-independent classification of phonological\ncategories exploiting a novel deep learning based hierarchical feature\nextraction scheme. To better capture the complex representation of\nhigh-dimensional electroencephalography (EEG) data, we compute the joint\nvariability of EEG electrodes into a channel cross-covariance matrix. We then\nextract the spatio-temporal information encoded within the matrix using a mixed\ndeep neural network strategy. Our model framework is composed of a\nconvolutional neural network (CNN), a long-short term network (LSTM), and a\ndeep autoencoder. We train the individual networks hierarchically, feeding\ntheir combined outputs in a final gradient boosting classification step. Our\nbest models achieve an average accuracy of 77.9% across five different binary\nclassification tasks, providing a significant 22.5% improvement over previous\nmethods. As we also show visually, our work demonstrates that the speech\nimagery EEG possesses significant discriminative information about the intended\narticulatory movements responsible for natural speech synthesis.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:11:40 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Saha", "Pramit", ""], ["Abdul-Mageed", "Muhammad", ""], ["Fels", "Sidney", ""]]}, {"id": "1904.04364", "submitter": "Hiromitsu Nishizaki", "authors": "Masaki Okawa, Takuya Saito, Naoki Sawada, Hiromitsu Nishizaki", "title": "Audio Classification of Bit-Representation Waveform", "comments": "Accepted at INTERSPEECH2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigated the waveform representation for audio signal\nclassification. Recently, many studies on audio waveform classification such as\nacoustic event detection and music genre classification have been published.\nMost studies on audio waveform classification have proposed the use of a deep\nlearning (neural network) framework. Generally, a frequency analysis method\nsuch as Fourier transform is applied to extract the frequency or spectral\ninformation from the input audio waveform before inputting the raw audio\nwaveform into the neural network. In contrast to these previous studies, in\nthis paper, we propose a novel waveform representation method, in which audio\nwaveforms are represented as a bit sequence, for audio classification. In our\nexperiment, we compare the proposed bit representation waveform, which is\ndirectly given to a neural network, to other representations of audio waveforms\nsuch as a raw audio waveform and a power spectrum with two classification\ntasks: one is an acoustic event classification task and the other is a\nsound/music classification task. The experimental results showed that the bit\nrepresentation waveform achieved the best classification performance for both\nthe tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:24:31 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 14:22:59 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Okawa", "Masaki", ""], ["Saito", "Takuya", ""], ["Sawada", "Naoki", ""], ["Nishizaki", "Hiromitsu", ""]]}, {"id": "1904.04365", "submitter": "Jared Fernandez", "authors": "Michael Chen, Mike D'Arcy, Alisa Liu, Jared Fernandez, Doug Downey", "title": "CODAH: An Adversarially Authored Question-Answer Dataset for Common\n  Sense", "comments": "8 pages, Appeared in RepEval 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense reasoning is a critical AI capability, but it is difficult to\nconstruct challenging datasets that test common sense. Recent neural question\nanswering systems, based on large pre-trained models of language, have already\nachieved near-human-level performance on commonsense knowledge benchmarks.\nThese systems do not possess human-level common sense, but are able to exploit\nlimitations of the datasets to achieve human-level scores.\n  We introduce the CODAH dataset, an adversarially-constructed evaluation\ndataset for testing common sense. CODAH forms a challenging extension to the\nrecently-proposed SWAG dataset, which tests commonsense knowledge using\nsentence-completion questions that describe situations observed in video. To\nproduce a more difficult dataset, we introduce a novel procedure for question\nacquisition in which workers author questions designed to target weaknesses of\nstate-of-the-art neural question answering systems. Workers are rewarded for\nsubmissions that models fail to answer correctly both before and after\nfine-tuning (in cross-validation). We create 2.8k questions via this procedure\nand evaluate the performance of multiple state-of-the-art question answering\nsystems on our dataset. We observe a significant gap between human performance,\nwhich is 95.3%, and the performance of the best baseline accuracy of 67.5% by\nthe BERT-Large model.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:25:50 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 23:26:27 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 19:27:18 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2019 06:16:45 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Chen", "Michael", ""], ["D'Arcy", "Mike", ""], ["Liu", "Alisa", ""], ["Fernandez", "Jared", ""], ["Downey", "Doug", ""]]}, {"id": "1904.04388", "submitter": "Vicky Zayats", "authors": "Vicky Zayats and Mari Ostendorf", "title": "Giving Attention to the Unexpected: Using Prosody Innovations in\n  Disfluency Detection", "comments": "Accepted at NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disfluencies in spontaneous speech are known to be associated with prosodic\ndisruptions. However, most algorithms for disfluency detection use only word\ntranscripts. Integrating prosodic cues has proved difficult because of the many\nsources of variability affecting the acoustic correlates. This paper introduces\na new approach to extracting acoustic-prosodic cues using text-based\ndistributional prediction of acoustic cues to derive vector z-score features\n(innovations). We explore both early and late fusion techniques for integrating\ntext and prosody, showing gains over a high-accuracy text-only model.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 22:47:37 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zayats", "Vicky", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1904.04398", "submitter": "Vicky Zayats", "authors": "Vicky Zayats, Trang Tran, Richard Wright, Courtney Mansfield, Mari\n  Ostendorf", "title": "Disfluencies and Human Speech Transcription Errors", "comments": "Submitted to INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores contexts associated with errors in transcrip-tion of\nspontaneous speech, shedding light on human perceptionof disfluencies and other\nconversational speech phenomena. Anew version of the Switchboard corpus is\nprovided with disfluency annotations for careful speech transcripts, together\nwith results showing the impact of transcription errors on evaluation of\nautomatic disfluency detection.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 23:43:16 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zayats", "Vicky", ""], ["Tran", "Trang", ""], ["Wright", "Richard", ""], ["Mansfield", "Courtney", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1904.04428", "submitter": "Hao Peng", "authors": "Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhingra, Dipanjan\n  Das", "title": "Text Generation with Exemplar-based Adaptive Decoding", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel conditioned text generation model. It draws inspiration\nfrom traditional template-based text generation techniques, where the source\nprovides the content (i.e., what to say), and the template influences how to\nsay it. Building on the successful encoder-decoder paradigm, it first encodes\nthe content representation from the given input text; to produce the output, it\nretrieves exemplar text from the training data as \"soft templates,\" which are\nthen used to construct an exemplar-specific decoder. We evaluate the proposed\nmodel on abstractive text summarization and data-to-text generation. Empirical\nresults show that this model achieves strong performance and outperforms\ncomparable baselines.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 02:34:30 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 22:03:53 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Peng", "Hao", ""], ["Parikh", "Ankur P.", ""], ["Faruqui", "Manaal", ""], ["Dhingra", "Bhuwan", ""], ["Das", "Dipanjan", ""]]}, {"id": "1904.04446", "submitter": "Wenxiang Jiao", "authors": "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R. Lyu", "title": "HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion\n  Recognition", "comments": "NAACL 2019 (10 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address three challenges in utterance-level emotion\nrecognition in dialogue systems: (1) the same word can deliver different\nemotions in different contexts; (2) some emotions are rarely seen in general\ndialogues; (3) long-range contextual information is hard to be effectively\ncaptured. We therefore propose a hierarchical Gated Recurrent Unit (HiGRU)\nframework with a lower-level GRU to model the word-level inputs and an\nupper-level GRU to capture the contexts of utterance-level embeddings.\nMoreover, we promote the framework to two variants, HiGRU with individual\nfeatures fusion (HiGRU-f) and HiGRU with self-attention and features fusion\n(HiGRU-sf), so that the word/utterance-level individual inputs and the\nlong-range contextual information can be sufficiently utilized. Experiments on\nthree dialogue emotion datasets, IEMOCAP, Friends, and EmotionPush demonstrate\nthat our proposed HiGRU models attain at least 8.7%, 7.5%, 6.0% improvement\nover the state-of-the-art methods on each dataset, respectively. Particularly,\nby utilizing only the textual feature in IEMOCAP, our HiGRU models gain at\nleast 3.8% improvement over the state-of-the-art conversational memory network\n(CMN) with the trimodal features of text, video, and audio.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 03:25:53 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Jiao", "Wenxiang", ""], ["Yang", "Haiqin", ""], ["King", "Irwin", ""], ["Lyu", "Michael R.", ""]]}, {"id": "1904.04458", "submitter": "Angli Liu", "authors": "Angli Liu, Jingfei Du, Veselin Stoyanov", "title": "Knowledge-Augmented Language Model and its Application to Unsupervised\n  Named-Entity Recognition", "comments": "NAACL 2019; updated to cite Zhou et al. (2018) EMNLP as a piece of\n  related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional language models are unable to efficiently model entity names\nobserved in text. All but the most popular named entities appear infrequently\nin text providing insufficient context. Recent efforts have recognized that\ncontext can be generalized between entity names that share the same type (e.g.,\n\\emph{person} or \\emph{location}) and have equipped language models with access\nto an external knowledge base (KB). Our Knowledge-Augmented Language Model\n(KALM) continues this line of work by augmenting a traditional model with a KB.\nUnlike previous methods, however, we train with an end-to-end predictive\nobjective optimizing the perplexity of text. We do not require any additional\ninformation such as named entity tags. In addition to improving language\nmodeling performance, KALM learns to recognize named entities in an entirely\nunsupervised way by using entity type information latent in the model. On a\nNamed Entity Recognition (NER) task, KALM achieves performance comparable with\nstate-of-the-art supervised models. Our work demonstrates that named entities\n(and possibly other types of world knowledge) can be modeled successfully using\npredictive learning and training on large corpora of text without any\nadditional information.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 04:09:45 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 07:48:21 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Liu", "Angli", ""], ["Du", "Jingfei", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "1904.04460", "submitter": "Zeyuan Wang", "authors": "Zeyuan Wang, Josiah Poon, Shiding Sun, Simon Poon", "title": "Attention-based Multi-instance Neural Network for Medical Diagnosis from\n  Incomplete and Low Quality Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to extract patterns from clinical records is to consider each patient\nrecord as a bag with various number of instances in the form of symptoms.\nMedical diagnosis is to discover informative ones first and then map them to\none or more diseases. In many cases, patients are represented as vectors in\nsome feature space and a classifier is applied after to generate diagnosis\nresults. However, in many real-world cases, data is often of low-quality due to\na variety of reasons, such as data consistency, integrity, completeness,\naccuracy, etc. In this paper, we propose a novel approach, attention based\nmulti-instance neural network (AMI-Net), to make the single disease\nclassification only based on the existing and valid information in the\nreal-world outpatient records. In the context of a patient, it takes a bag of\ninstances as input and output the bag label directly in end-to-end way.\nEmbedding layer is adopted at the beginning, mapping instances into an\nembedding space which represents the individual patient condition. The\ncorrelations among instances and their importance for the final classification\nare captured by multi-head attention transformer, instance-level multi-instance\npooling and bag-level multi-instance pooling. The proposed approach was test on\ntwo non-standardized and highly imbalanced datasets, one in the Traditional\nChinese Medicine (TCM) domain and the other in the Western Medicine (WM)\ndomain. Our preliminary results show that the proposed approach outperforms all\nbaselines results by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 04:21:52 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Wang", "Zeyuan", ""], ["Poon", "Josiah", ""], ["Sun", "Shiding", ""], ["Poon", "Simon", ""]]}, {"id": "1904.04461", "submitter": "Louis Chartrand", "authors": "Louis Chartrand and Mohamed Bouguessa", "title": "Mixing syntagmatic and paradigmatic information for concept detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decades, philosophers have begun using empirical data for\nconceptual analysis, but corpus-based conceptual analysis has so far failed to\ndevelop, in part because of the absence of reliable methods to automatically\ndetect concepts in textual data. Previous attempts have shown that topic models\ncan constitute efficient concept detection heuristics, but while they leverage\nthe syntagmatic relations in a corpus, they fail to exploit paradigmatic\nrelations, and thus probably fail to model concepts accurately. In this\narticle, we show that using a topic model that models concepts on a space of\nword embeddings (Hu and Tsujii, 2016) can lead to significant increases in\nconcept detection performance, as well as enable the target concept to be\nexpressed in more flexible ways using word vectors.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 04:27:31 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 14:58:58 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Chartrand", "Louis", ""], ["Bouguessa", "Mohamed", ""]]}, {"id": "1904.04479", "submitter": "Tatiana Likhomanenko", "authors": "Tatiana Likhomanenko, Gabriel Synnaeve, Ronan Collobert", "title": "Who Needs Words? Lexicon-Free Speech Recognition", "comments": "8 pages, 1 figure", "journal-ref": "Proc. Interspeech 2019", "doi": "10.21437/Interspeech.2019-3107", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexicon-free speech recognition naturally deals with the problem of\nout-of-vocabulary (OOV) words. In this paper, we show that character-based\nlanguage models (LM) can perform as well as word-based LMs for speech\nrecognition, in word error rates (WER), even without restricting the decoding\nto a lexicon. We study character-based LMs and show that convolutional LMs can\neffectively leverage large (character) contexts, which is key for good speech\nrecognition performance downstream. We specifically show that the lexicon-free\ndecoding performance (WER) on utterances with OOV words using character-based\nLMs is better than lexicon-based decoding, both with character or word-based\nLMs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 06:06:54 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 19:35:52 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 20:35:52 GMT"}, {"version": "v4", "created": "Fri, 13 Sep 2019 15:13:24 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Likhomanenko", "Tatiana", ""], ["Synnaeve", "Gabriel", ""], ["Collobert", "Ronan", ""]]}, {"id": "1904.04498", "submitter": "Zijian Zhao", "authors": "Zijian Zhao, Su Zhu and Kai Yu", "title": "A Hierarchical Decoding Model For Spoken Language Understanding From\n  Unaligned Data", "comments": "Accepted by ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding (SLU) systems can be trained on two types of\nlabelled data: aligned or unaligned. Unaligned data do not require word by word\nannotation and is easier to be obtained. In the paper, we focus on spoken\nlanguage understanding from unaligned data whose annotation is a set of\nact-slot-value triples. Previous works usually focus on improve slot-value pair\nprediction and estimate dialogue act types separately, which ignores the\nhierarchical structure of the act-slot-value triples. Here, we propose a novel\nhierarchical decoding model which dynamically parses act, slot and value in a\nstructured way and employs pointer network to handle out-of-vocabulary (OOV)\nvalues. Experiments on DSTC2 dataset, a benchmark unaligned dataset, show that\nthe proposed model not only outperforms previous state-of-the-art model, but\nalso can be generalized effectively and efficiently to unseen act-slot type\npairs and OOV values.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 07:26:25 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zhao", "Zijian", ""], ["Zhu", "Su", ""], ["Yu", "Kai", ""]]}, {"id": "1904.04697", "submitter": "Xipeng Qiu", "authors": "Hang Yan, Xipeng Qiu, Xuanjing Huang", "title": "A Graph-based Model for Joint Chinese Word Segmentation and Dependency\n  Parsing", "comments": "Accepted at Transactions of the Association for Computational\n  Linguistics (TACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese word segmentation and dependency parsing are two fundamental tasks\nfor Chinese natural language processing. The dependency parsing is defined on\nword-level. Therefore word segmentation is the precondition of dependency\nparsing, which makes dependency parsing suffer from error propagation and\nunable to directly make use of the character-level pre-trained language model\n(such as BERT). In this paper, we propose a graph-based model to integrate\nChinese word segmentation and dependency parsing. Different from previous\ntransition-based joint models, our proposed model is more concise, which\nresults in fewer efforts of feature engineering. Our graph-based joint model\nachieves better performance than previous joint models and state-of-the-art\nresults in both Chinese word segmentation and dependency parsing. Besides, when\nBERT is combined, our model can substantially reduce the performance gap of\ndependency parsing between joint models and gold-segmented word-based models.\nOur code is publicly available at https://github.com/fastnlp/JointCwsParser.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:25:17 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 10:07:33 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Yan", "Hang", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1904.04733", "submitter": "Marco Dinarelli", "authors": "Marco Dinarelli, Lo\\\"ic Grobol", "title": "Seq2Biseq: Bidirectional Output-wise Recurrent Neural Networks for\n  Sequence Modelling", "comments": "Slightly improved version of the paper accepted to the CICling 2019\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last couple of years, Recurrent Neural Networks (RNN) have reached\nstate-of-the-art performances on most of the sequence modelling problems. In\nparticular, the \"sequence to sequence\" model and the neural CRF have proved to\nbe very effective in this domain. In this article, we propose a new RNN\narchitecture for sequence labelling, leveraging gated recurrent layers to take\narbitrarily long contexts into account, and using two decoders operating\nforward and backward. We compare several variants of the proposed solution and\ntheir performances to the state-of-the-art. Most of our results are better than\nthe state-of-the-art or very close to it and thanks to the use of recent\ntechnologies, our architecture can scale on corpora larger than those used in\nthis work.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:33:59 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 22:44:10 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 09:26:52 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Dinarelli", "Marco", ""], ["Grobol", "Lo\u00efc", ""]]}, {"id": "1904.04742", "submitter": "Ahmad Rashid", "authors": "Ahmad Rashid, Alan Do-Omri, Md. Akmal Haidar, Qun Liu and Mehdi\n  Rezagholizadeh", "title": "Bilingual-GAN: A Step Towards Parallel Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent space based GAN methods and attention based sequence to sequence\nmodels have achieved impressive results in text generation and unsupervised\nmachine translation respectively. Leveraging the two domains, we propose an\nadversarial latent space based model capable of generating parallel sentences\nin two languages concurrently and translating bidirectionally. The bilingual\ngeneration goal is achieved by sampling from the latent space that is shared\nbetween both languages. First two denoising autoencoders are trained, with\nshared encoders and back-translation to enforce a shared latent state between\nthe two languages. The decoder is shared for the two translation directions.\nNext, a GAN is trained to generate synthetic \"code\" mimicking the languages'\nshared latent space. This code is then fed into the decoder to generate text in\neither language. We perform our experiments on Europarl and Multi30k datasets,\non the English-French language pair, and document our performance using both\nsupervised and unsupervised machine translation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:42:08 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 19:57:24 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Rashid", "Ahmad", ""], ["Do-Omri", "Alan", ""], ["Haidar", "Md. Akmal", ""], ["Liu", "Qun", ""], ["Rezagholizadeh", "Mehdi", ""]]}, {"id": "1904.04745", "submitter": "Linwei Ye", "authors": "Linwei Ye, Mrigank Rochan, Zhi Liu and Yang Wang", "title": "Cross-Modal Self-Attention Network for Referring Image Segmentation", "comments": "Accepted to CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of referring image segmentation. Given an input image\nand a natural language expression, the goal is to segment the object referred\nby the language expression in the image. Existing works in this area treat the\nlanguage expression and the input image separately in their representations.\nThey do not sufficiently capture long-range correlations between these two\nmodalities. In this paper, we propose a cross-modal self-attention (CMSA)\nmodule that effectively captures the long-range dependencies between linguistic\nand visual features. Our model can adaptively focus on informative words in the\nreferring expression and important regions in the input image. In addition, we\npropose a gated multi-level fusion module to selectively integrate\nself-attentive cross-modal features corresponding to different levels in the\nimage. This module controls the information flow of features at different\nlevels. We validate the proposed approach on four evaluation datasets. Our\nproposed approach consistently outperforms existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:51:07 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Ye", "Linwei", ""], ["Rochan", "Mrigank", ""], ["Liu", "Zhi", ""], ["Wang", "Yang", ""]]}, {"id": "1904.04764", "submitter": "Haohan Guo", "authors": "Haohan Guo, Frank K. Soong, Lei He, Lei Xie", "title": "Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS", "comments": "Submitted to Interspeech 2019, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The end-to-end TTS, which can predict speech directly from a given sequence\nof graphemes or phonemes, has shown improved performance over the conventional\nTTS. However, its predicting capability is still limited by the\nacoustic/phonetic coverage of the training data, usually constrained by the\ntraining set size. To further improve the TTS quality in pronunciation, prosody\nand perceived naturalness, we propose to exploit the information embedded in a\nsyntactically parsed tree where the inter-phrase/word information of a sentence\nis organized in a multilevel tree structure. Specifically, two key features:\nphrase structure and relations between adjacent words are investigated.\nExperimental results in subjective listening, measured on three test sets, show\nthat the proposed approach is effective to improve the pronunciation clarity,\nprosody and naturalness of the synthesized speech of the baseline system.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:20:52 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Guo", "Haohan", ""], ["Soong", "Frank K.", ""], ["He", "Lei", ""], ["Xie", "Lei", ""]]}, {"id": "1904.04775", "submitter": "Haohan Guo", "authors": "Haohan Guo, Frank K. Soong, Lei He, Lei Xie", "title": "A New GAN-based End-to-End TTS Training Algorithm", "comments": "Submitted to Interspeech 2019, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end, autoregressive model-based TTS has shown significant performance\nimprovements over the conventional one. However, the autoregressive module\ntraining is affected by the exposure bias, or the mismatch between the\ndifferent distributions of real and predicted data. While real data is\navailable in training, but in testing, only predicted data is available to feed\nthe autoregressive module. By introducing both real and generated data\nsequences in training, we can alleviate the effects of the exposure bias. We\npropose to use Generative Adversarial Network (GAN) along with the key idea of\nProfessor Forcing in training. A discriminator in GAN is jointly trained to\nequalize the difference between real and predicted data. In AB subjective\nlistening test, the results show that the new approach is preferred over the\nstandard transfer learning with a CMOS improvement of 0.1. Sentence level\nintelligibility tests show significant improvement in a pathological test set.\nThe GAN-trained new model is also more stable than the baseline to produce\nbetter alignments for the Tacotron output.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:37:35 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Guo", "Haohan", ""], ["Soong", "Frank K.", ""], ["He", "Lei", ""], ["Xie", "Lei", ""]]}, {"id": "1904.04790", "submitter": "Markus Freitag", "authors": "Markus Freitag, Isaac Caswell, Scott Roy", "title": "APE at Scale and its Implications on MT Evaluation Biases", "comments": "Accepted at WMT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we train an Automatic Post-Editing (APE) model and use it to\nreveal biases in standard Machine Translation (MT) evaluation procedures. The\ngoal of our APE model is to correct typical errors introduced by the\ntranslation process, and convert the \"translationese\" output into natural text.\nOur APE model is trained entirely on monolingual data that has been round-trip\ntranslated through English, to mimic errors that are similar to the ones\nintroduced by NMT. We apply our model to the output of existing NMT systems,\nand demonstrate that, while the human-judged quality improves in all cases,\nBLEU scores drop with forward-translated test sets. We verify these results for\nthe WMT18 English to German, WMT15 English to French, and WMT16 English to\nRomanian tasks. Furthermore, we selectively apply our APE model on the output\nof the top submissions of the most recent WMT evaluation campaigns. We see\nquality improvements on all tasks of up to 2.5 BLEU points.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:10:15 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 17:32:34 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Freitag", "Markus", ""], ["Caswell", "Isaac", ""], ["Roy", "Scott", ""]]}, {"id": "1904.04792", "submitter": "Pedro Rodriguez", "authors": "Pedro Rodriguez and Shi Feng and Mohit Iyyer and He He and Jordan\n  Boyd-Graber", "title": "Quizbowl: The Case for Incremental Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scholastic trivia competitions test knowledge and intelligence through\nmastery of question answering. Modern question answering benchmarks are one\nvariant of the Turing test. Specifically, answering a set of questions as well\nas a human is a minimum bar towards demonstrating human-like intelligence. This\npaper makes the case that the format of one competition -- where participants\ncan answer in the middle of hearing a question (incremental) -- better\ndifferentiates the skill between (human or machine) players. Additionally,\nmerging a sequential decision-making sub-task with question answering (QA)\nprovides a good setting for research in model calibration and opponent\nmodeling. Thus, embedded in this task are three machine learning challenges:\n(1) factoid QA over thousands of Wikipedia-like answers, (2) calibration of the\nQA model's confidence scores, and (3) sequential decision-making that\nincorporates knowledge of the QA model, its calibration, and what the opponent\nmay do. We make two contributions: (1) collecting and curating a large factoid\nQA dataset and an accompanying gameplay dataset, and (2) developing a model\nthat addresses these three machine learning challenges. In addition to offline\nevaluation, we pitted our model against some of the most accomplished trivia\nplayers in the world in a series of exhibition matches spanning several years.\nThroughout this paper, we show that collaborations with the vibrant trivia\ncommunity have contributed to the quality of our dataset, spawned new research\ndirections, and doubled as an exciting way to engage the public with research\nin machine learning and natural language processing.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:13:36 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 04:03:02 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Rodriguez", "Pedro", ""], ["Feng", "Shi", ""], ["Iyyer", "Mohit", ""], ["He", "He", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "1904.04866", "submitter": "Denis Newman-Griffis", "authors": "Brendan Whitaker, Denis Newman-Griffis, Aparajita Haldar, Hakan\n  Ferhatosmanoglu, Eric Fosler-Lussier", "title": "Characterizing the impact of geometric properties of word embeddings on\n  task performance", "comments": "Appearing in the Third Workshop on Evaluating Vector Space\n  Representations for NLP (RepEval 2019). 7 pages + references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of word embedding properties to inform their use in downstream NLP\ntasks has largely been studied by assessing nearest neighbors. However,\ngeometric properties of the continuous feature space contribute directly to the\nuse of embedding features in downstream models, and are largely unexplored. We\nconsider four properties of word embedding geometry, namely: position relative\nto the origin, distribution of features in the vector space, global pairwise\ndistances, and local pairwise distances. We define a sequence of\ntransformations to generate new embeddings that expose subsets of these\nproperties to downstream models and evaluate change in task performance to\nunderstand the contribution of each property to NLP models. We transform\npublicly available pretrained embeddings from three popular toolkits (word2vec,\nGloVe, and FastText) and evaluate on a variety of intrinsic tasks, which model\nlinguistic information in the vector space, and extrinsic tasks, which use\nvectors as input to machine learning models. We find that intrinsic evaluations\nare highly sensitive to absolute position, while extrinsic tasks rely primarily\non local similarity. Our findings suggest that future embedding models and\npost-processing techniques should focus primarily on similarity to nearby\npoints in vector space.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:53:00 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Whitaker", "Brendan", ""], ["Newman-Griffis", "Denis", ""], ["Haldar", "Aparajita", ""], ["Ferhatosmanoglu", "Hakan", ""], ["Fosler-Lussier", "Eric", ""]]}, {"id": "1904.04896", "submitter": "Ruizhi Li", "authors": "Ruizhi Li, Gregory Sell, Hynek Hermansky", "title": "Performance Monitoring for End-to-End Speech Recognition", "comments": "Submitted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring performance of an automatic speech recognition (ASR) system without\nground-truth could be beneficial in many scenarios, especially with data from\nunseen domains, where performance can be highly inconsistent. In conventional\nASR systems, several performance monitoring (PM) techniques have been\nwell-developed to monitor performance by looking at tri-phone posteriors or\npre-softmax activations from neural network acoustic modeling. However,\nstrategies for monitoring more recently developed end-to-end ASR systems have\nnot yet been explored, and so that is the focus of this paper. We adapt\nprevious PM measures (Entropy, M-measure and Auto-encoder) and apply our\nproposed RNN predictor in the end-to-end setting. These measures utilize the\ndecoder output layer and attention probability vectors, and their predictive\npower is measured with simple linear models. Our findings suggest that\ndecoder-level features are more feasible and informative than attention-level\nprobabilities for PM measures, and that M-measure on the decoder posteriors\nachieves the best overall predictive performance with an average prediction\nerror 8.8%. Entropy measures and RNN-based prediction also show competitive\npredictability, especially for unseen conditions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 20:35:25 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Li", "Ruizhi", ""], ["Sell", "Gregory", ""], ["Hermansky", "Hynek", ""]]}, {"id": "1904.04900", "submitter": "Lucia Santamaria", "authors": "Luc\\'ia Santamar\\'ia, Amittai Axelrod", "title": "Data Selection with Cluster-Based Language Difference Models and Cynical\n  Selection", "comments": "9 pages, 7 figures, IWSLT 2017", "journal-ref": "Proceedings of the International Workshop on Spoken Language\n  Translation (IWSLT) 2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and apply two methods for addressing the problem of selecting\nrelevant training data out of a general pool for use in tasks such as machine\ntranslation. Building on existing work on class-based language difference\nmodels, we first introduce a cluster-based method that uses Brown clusters to\ncondense the vocabulary of the corpora. Secondly, we implement the cynical data\nselection method, which incrementally constructs a training corpus to\nefficiently model the task corpus. Both the cluster-based and the cynical data\nselection approaches are used for the first time within a machine translation\nsystem, and we perform a head-to-head comparison. Our intrinsic evaluations\nshow that both new methods outperform the standard Moore-Lewis approach\n(cross-entropy difference), in terms of better perplexity and OOV rates on\nin-domain data. The cynical approach converges much quicker, covering nearly\nall of the in-domain vocabulary with 84% less data than the other methods.\nFurthermore, the new approaches can be used to select machine translation\ntraining data for training better systems. Our results confirm that class-based\nselection using Brown clusters is a viable alternative to POS-based class-based\nmethods, and removes the reliance on a part-of-speech tagger. Additionally, we\nare able to validate the recently proposed cynical data selection method,\nshowing that its performance in SMT models surpasses that of traditional\ncross-entropy difference methods and more closely matches the sentence length\nof the task corpus.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 20:39:07 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Santamar\u00eda", "Luc\u00eda", ""], ["Axelrod", "Amittai", ""]]}, {"id": "1904.04956", "submitter": "Wei Zhang", "authors": "Wei Zhang, Xiaodong Cui, Ulrich Finkler, Brian Kingsbury, George Saon,\n  David Kung, Michael Picheny", "title": "Distributed Deep Learning Strategies For Automatic Speech Recognition", "comments": "Published in ICASSP'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and investigate a variety of distributed deep\nlearning strategies for automatic speech recognition (ASR) and evaluate them\nwith a state-of-the-art Long short-term memory (LSTM) acoustic model on the\n2000-hour Switchboard (SWB2000), which is one of the most widely used datasets\nfor ASR performance benchmark. We first investigate what are the proper\nhyper-parameters (e.g., learning rate) to enable the training with sufficiently\nlarge batch size without impairing the model accuracy. We then implement\nvarious distributed strategies, including Synchronous (SYNC), Asynchronous\nDecentralized Parallel SGD (ADPSGD) and the hybrid of the two HYBRID, to study\ntheir runtime/accuracy trade-off. We show that we can train the LSTM model\nusing ADPSGD in 14 hours with 16 NVIDIA P100 GPUs to reach a 7.6% WER on the\nHub5- 2000 Switchboard (SWB) test set and a 13.1% WER on the CallHome (CH) test\nset. Furthermore, we can train the model using HYBRID in 11.5 hours with 32\nNVIDIA V100 GPUs without loss in accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:00:26 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zhang", "Wei", ""], ["Cui", "Xiaodong", ""], ["Finkler", "Ulrich", ""], ["Kingsbury", "Brian", ""], ["Saon", "George", ""], ["Kung", "David", ""], ["Picheny", "Michael", ""]]}, {"id": "1904.04969", "submitter": "Yu Cao", "authors": "Yu Cao, Meng Fang, Dacheng Tao", "title": "BAG: Bi-directional Attention Entity Graph Convolutional Network for\n  Multi-hop Reasoning Question Answering", "comments": "5 pages, 1 figure, accepted short paper on NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop reasoning question answering requires deep comprehension of\nrelationships between various documents and queries. We propose a\nBi-directional Attention Entity Graph Convolutional Network (BAG), leveraging\nrelationships between nodes in an entity graph and attention information\nbetween a query and the entity graph, to solve this task. Graph convolutional\nnetworks are used to obtain a relation-aware representation of nodes for entity\ngraphs built from documents with multi-level features. Bidirectional attention\nis then applied on graphs and queries to generate a query-aware nodes\nrepresentation, which will be used for the final prediction. Experimental\nevaluation shows BAG achieves state-of-the-art accuracy performance on the\nQAngaroo WIKIHOP dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:40:08 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Cao", "Yu", ""], ["Fang", "Meng", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.04995", "submitter": "Xuewei Tang", "authors": "Shanshan Huang, Xiaojun Wan, Xuewei Tang", "title": "AMRec: An Intelligent System for Academic Method Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding new academic Methods for research problems is the key task in a\nresearcher's research career. It is usually very difficult for new researchers\nto find good Methods for their research problems since they lack of research\nexperiences. In order to help researchers carry out their researches in a more\nconvenient way, we describe a novel recommendation system called AMRec to\nrecommend new academic Methods for research problems in this paper. Our\nproposed system first extracts academic concepts (Tasks and Methods) and their\nrelations from academic literatures, and then leverages the regularized matrix\nfactorization Method for academic Method recommendation. Preliminary evaluation\nresults verify the effectiveness of our proposed system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 03:49:37 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Huang", "Shanshan", ""], ["Wan", "Xiaojun", ""], ["Tang", "Xuewei", ""]]}, {"id": "1904.05033", "submitter": "Prakhar Gupta", "authors": "Prakhar Gupta, Matteo Pagliardini and Martin Jaggi", "title": "Better Word Embeddings by Disentangling Contextual n-Gram Information", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained word vectors are ubiquitous in Natural Language Processing\napplications. In this paper, we show how training word embeddings jointly with\nbigram and even trigram embeddings, results in improved unigram embeddings. We\nclaim that training word embeddings along with higher n-gram embeddings helps\nin the removal of the contextual information from the unigrams, resulting in\nbetter stand-alone word embeddings. We empirically show the validity of our\nhypothesis by outperforming other competing word representation models by a\nsignificant margin on a wide variety of tasks. We make our models publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 07:44:06 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Gupta", "Prakhar", ""], ["Pagliardini", "Matteo", ""], ["Jaggi", "Martin", ""]]}, {"id": "1904.05054", "submitter": "Mehmet Saygin Seyfioglu", "authors": "Semih Yagcioglu, Mehmet Saygin Seyfioglu, Begum Citamak, Batuhan\n  Bardak, Seren Guldamlasioglu, Azmi Yuksel, Emin Islam Tatli", "title": "Detecting Cybersecurity Events from Noisy Short Text", "comments": "Accepted February 2019 to North American Chapter of the Association\n  for Computational Linguistics (NAACL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is very critical to analyze messages shared over social networks for cyber\nthreat intelligence and cyber-crime prevention. In this study, we propose a\nmethod that leverages both domain-specific word embeddings and task-specific\nfeatures to detect cyber security events from tweets. Our model employs a\nconvolutional neural network (CNN) and a long short-term memory (LSTM)\nrecurrent neural network which takes word level meta-embeddings as inputs and\nincorporates contextual embeddings to classify noisy short text. We collected a\nnew dataset of cyber security related tweets from Twitter and manually\nannotated a subset of 2K of them. We experimented with this dataset and\nconcluded that the proposed model outperforms both traditional and neural\nbaselines. The results suggest that our method works well for detecting cyber\nsecurity events from noisy short text.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:23:31 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 18:38:51 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Yagcioglu", "Semih", ""], ["Seyfioglu", "Mehmet Saygin", ""], ["Citamak", "Begum", ""], ["Bardak", "Batuhan", ""], ["Guldamlasioglu", "Seren", ""], ["Yuksel", "Azmi", ""], ["Tatli", "Emin Islam", ""]]}, {"id": "1904.05055", "submitter": "Ziqian Zeng", "authors": "Ziqian Zeng, Wenxuan Zhou, Xin Liu, and Yangqiu Song", "title": "A Variational Approach to Weakly Supervised Document-Level Multi-Aspect\n  Sentiment Classification", "comments": "Accepted by NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we propose a variational approach to weakly supervised\ndocument-level multi-aspect sentiment classification. Instead of using\nuser-generated ratings or annotations provided by domain experts, we use\ntarget-opinion word pairs as \"supervision.\" These word pairs can be extracted\nby using dependency parsers and simple rules. Our objective is to predict an\nopinion word given a target word while our ultimate goal is to learn a\nsentiment polarity classifier to predict the sentiment polarity of each aspect\ngiven a document. By introducing a latent variable, i.e., the sentiment\npolarity, to the objective function, we can inject the sentiment polarity\nclassifier to the objective via the variational lower bound. We can learn a\nsentiment polarity classifier by optimizing the lower bound. We show that our\nmethod can outperform weakly supervised baselines on TripAdvisor and\nBeerAdvocate datasets and can be comparable to the state-of-the-art supervised\nmethod with hundreds of labels per aspect.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:24:06 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zeng", "Ziqian", ""], ["Zhou", "Wenxuan", ""], ["Liu", "Xin", ""], ["Song", "Yangqiu", ""]]}, {"id": "1904.05078", "submitter": "Yi-Chen Chen", "authors": "Yi-Chen Chen, Sung-Feng Huang, Hung-yi Lee, Lin-shan Lee", "title": "From Semi-supervised to Almost-unsupervised Speech Recognition with\n  Very-low Resource by Jointly Learning Phonetic Structures from Audio and Text\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Producing a large amount of annotated speech data for training ASR systems\nremains difficult for more than 95% of languages all over the world which are\nlow-resourced. However, we note human babies start to learn the language by the\nsounds (or phonetic structures) of a small number of exemplar words, and\n\"generalize\" such knowledge to other words without hearing a large amount of\ndata. We initiate some preliminary work in this direction. Audio Word2Vec is\nused to learn the phonetic structures from spoken words (signal segments),\nwhile another autoencoder is used to learn the phonetic structures from text\nwords. The relationships among the above two can be learned jointly, or\nseparately after the above two are well trained. This relationship can be used\nin speech recognition with very low resource. In the initial experiments on the\nTIMIT dataset, only 2.1 hours of speech data (in which 2500 spoken words were\nannotated and the rest unlabeled) gave a word error rate of 44.6%, and this\nnumber can be reduced to 34.2% if 4.1 hr of speech data (in which 20000 spoken\nwords were annotated) were given. These results are not satisfactory, but a\ngood starting point.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 09:16:24 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Chen", "Yi-Chen", ""], ["Huang", "Sung-Feng", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1904.05092", "submitter": "Desmond Elliott", "authors": "Spandana Gella, Desmond Elliott, Frank Keller", "title": "Cross-lingual Visual Verb Sense Disambiguation", "comments": "NAACL 2019; fix typo in author name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that visual context improves cross-lingual sense\ndisambiguation for nouns. We extend this line of work to the more challenging\ntask of cross-lingual verb sense disambiguation, introducing the MultiSense\ndataset of 9,504 images annotated with English, German, and Spanish verbs. Each\nimage in MultiSense is annotated with an English verb and its translation in\nGerman or Spanish. We show that cross-lingual verb sense disambiguation models\nbenefit from visual context, compared to unimodal baselines. We also show that\nthe verb sense predicted by our best disambiguation model can improve the\nresults of a text-only machine translation system when used for a multimodal\ntranslation task.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 09:43:06 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 12:52:07 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Gella", "Spandana", ""], ["Elliott", "Desmond", ""], ["Keller", "Frank", ""]]}, {"id": "1904.05152", "submitter": "Alessandro Seganti", "authors": "Alessandro Seganti, Helena Sobol, Iryna Orlova, Hannam Kim, Jakub\n  Staniszewski, Tymoteusz Krumholc, Krystian Koziel", "title": "NLPR@SRPOL at SemEval-2019 Task 6 and Task 5: Linguistically enhanced\n  deep learning offensive sentence classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a system developed for the SemEval-2019 competition Task 5\nhat-Eval Basile et al. (2019) (team name: LU Team) and Task 6 OffensEval\nZampieri et al. (2019b) (team name: NLPR@SRPOL), where we achieved 2nd position\nin Subtask C. The system combines in an ensemble several models (LSTM,\nTransformer, OpenAI's GPT, Random forest, SVM) with various embeddings (custom,\nELMo, fastText, Universal Encoder) together with additional linguistic features\n(number of blacklisted words, special characters, etc.). The system works with\na multi-tier blacklist and a large corpus of crawled data, annotated for\ngeneral offensiveness. In the paper we do an extensive analysis of our results\nand show how the combination of features and embedding affect the performance\nof the models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 12:56:50 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Seganti", "Alessandro", ""], ["Sobol", "Helena", ""], ["Orlova", "Iryna", ""], ["Kim", "Hannam", ""], ["Staniszewski", "Jakub", ""], ["Krumholc", "Tymoteusz", ""], ["Koziel", "Krystian", ""]]}, {"id": "1904.05154", "submitter": "Heinrich Dinkel", "authors": "Heinrich Dinkel, Mengyue Wu and Kai Yu", "title": "Text-based depression detection on sparse data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous text-based depression detection is commonly based on large\nuser-generated data. Sparse scenarios like clinical conversations are less\ninvestigated. This work proposes a text-based multi-task BGRU network with\npretrained word embeddings to model patients' responses during clinical\ninterviews. Our main approach uses a novel multi-task loss function, aiming at\nmodeling both depression severity and binary health state. We independently\ninvestigate word- and sentence-level word-embeddings as well as the use of\nlarge-data pretraining for depression detection. To strengthen our findings, we\nreport mean-averaged results for a multitude of independent runs on sparse\ndata. First, we show that pretraining is helpful for word-level text-based\ndepression detection. Second, our results demonstrate that sentence-level\nword-embeddings should be mostly preferred over word-level ones. While the\nchoice of pooling function is less crucial, mean and attention pooling should\nbe preferred over last-timestep pooling. Our method outputs depression presence\nresults as well as predicted severity score, culminating a macro F1 score of\n0.84 and MAE of 3.48 on the DAIC-WOZ development set.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 03:47:15 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 08:58:17 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 05:23:15 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Dinkel", "Heinrich", ""], ["Wu", "Mengyue", ""], ["Yu", "Kai", ""]]}, {"id": "1904.05233", "submitter": "Alexey Romanov", "authors": "Alexey Romanov, Maria De-Arteaga, Hanna Wallach, Jennifer Chayes,\n  Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi,\n  Anna Rumshisky, Adam Tauman Kalai", "title": "What's in a Name? Reducing Bias in Bios without Access to Protected\n  Attributes", "comments": "Accepted at NAACL 2019; Best Thematic Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing body of work that proposes methods for mitigating bias in\nmachine learning systems. These methods typically rely on access to protected\nattributes such as race, gender, or age. However, this raises two significant\nchallenges: (1) protected attributes may not be available or it may not be\nlegal to use them, and (2) it is often desirable to simultaneously consider\nmultiple protected attributes, as well as their intersections. In the context\nof mitigating bias in occupation classification, we propose a method for\ndiscouraging correlation between the predicted probability of an individual's\ntrue occupation and a word embedding of their name. This method leverages the\nsocietal biases that are encoded in word embeddings, eliminating the need for\naccess to protected attributes. Crucially, it only requires access to\nindividuals' names at training time and not at deployment time. We evaluate two\nvariations of our proposed method using a large-scale dataset of online\nbiographies. We find that both variations simultaneously reduce race and gender\nbiases, with almost no reduction in the classifier's overall true positive\nrate.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:10:37 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Romanov", "Alexey", ""], ["De-Arteaga", "Maria", ""], ["Wallach", "Hanna", ""], ["Chayes", "Jennifer", ""], ["Borgs", "Christian", ""], ["Chouldechova", "Alexandra", ""], ["Geyik", "Sahin", ""], ["Kenthapadi", "Krishnaram", ""], ["Rumshisky", "Anna", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1904.05255", "submitter": "Jimmy Lin", "authors": "Peng Shi and Jimmy Lin", "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling", "comments": "work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple BERT-based models for relation extraction and semantic role\nlabeling. In recent years, state-of-the-art performance has been achieved using\nneural models by incorporating lexical and syntactic features such as\npart-of-speech tags and dependency trees. In this paper, extensive experiments\non datasets for these two tasks show that without using any external features,\na simple BERT-based model can achieve state-of-the-art performance. To our\nknowledge, we are the first to successfully apply BERT in this manner. Our\nmodels provide strong baselines for future research.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:52:13 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Shi", "Peng", ""], ["Lin", "Jimmy", ""]]}, {"id": "1904.05276", "submitter": "Anupiya Nugaliyadde Mr", "authors": "K.S.D. Ishwari, A.K.R.R.Aneeze, S.Sudheesan, H.J.D.A. Karunaratne, A.\n  Nugaliyadde, Y. Mallawarrachchi", "title": "Advances in Natural Language Question Answering: A Review", "comments": "arXiv admin note: text overlap with arXiv:1609.04667 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering has recently received high attention from artificial\nintelligence communities due to the advancements in learning technologies.\nEarly question answering models used rule-based approaches and moved to the\nstatistical approach to address the vastly available information. However,\nstatistical approaches are shown to underperform in handling the dynamic nature\nand the variation of language. Therefore, learning models have shown the\ncapability of handling the dynamic nature and variations in language. Many deep\nlearning methods have been introduced to question answering. Most of the deep\nlearning approaches have shown to achieve higher results compared to machine\nlearning and statistical methods. The dynamic nature of language has profited\nfrom the nonlinear learning in deep learning. This has created prominent\nsuccess and a spike in work on question answering. This paper discusses the\nsuccesses and challenges in question answering question answering systems and\ntechniques that are used in these challenges.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:26:51 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ishwari", "K. S. D.", ""], ["Aneeze", "A. K. R. R.", ""], ["Sudheesan", "S.", ""], ["Karunaratne", "H. J. D. A.", ""], ["Nugaliyadde", "A.", ""], ["Mallawarrachchi", "Y.", ""]]}, {"id": "1904.05298", "submitter": "Qiuchi Li", "authors": "Qiuchi Li, Benyou Wang and Massimo Melucci", "title": "CNM: An Interpretable Complex-valued Network for Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper seeks to model human language by the mathematical framework of\nquantum physics. With the well-designed mathematical formulations in quantum\nphysics, this framework unifies different linguistic units in a single\ncomplex-valued vector space, e.g. words as particles in quantum states and\nsentences as mixed systems. A complex-valued network is built to implement this\nframework for semantic matching. With well-constrained complex-valued\ncomponents, the network admits interpretations to explicit physical meanings.\nThe proposed complex-valued network for matching (CNM) achieves comparable\nperformances to strong CNN and RNN baselines on two benchmarking question\nanswering (QA) datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:59:29 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Li", "Qiuchi", ""], ["Wang", "Benyou", ""], ["Melucci", "Massimo", ""]]}, {"id": "1904.05308", "submitter": "Davy Weissenbacher", "authors": "Davy Weissenbacher, Abeed Sarker, Ari Klein, Karen O'Connor, Arjun\n  Magge Ranganatha, Graciela Gonzalez-Hernandez", "title": "Deep Neural Networks Ensemble for Detecting Medication Mentions in\n  Tweets", "comments": "This is a pre-copy-editing, author-produced PDF of an article\n  accepted for publication in JAMIA following peer review. The definitive\n  publisher-authenticated version is \"D. Weissenbacher, A. Sarker, A. Klein, K.\n  O'Connor, A. Magge, G. Gonzalez-Hernandez, Deep neural networks ensemble for\n  detecting medication mentions in tweets, Journal of the American Medical\n  Informatics Association, ocz156, 2019\"", "journal-ref": "Journal of the American Medical Informatics Association, ocz156,\n  2019", "doi": "10.1093/jamia/ocz156", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: After years of research, Twitter posts are now recognized as an\nimportant source of patient-generated data, providing unique insights into\npopulation health. A fundamental step to incorporating Twitter data in\npharmacoepidemiological research is to automatically recognize medication\nmentions in tweets. Given that lexical searches for medication names may fail\ndue to misspellings or ambiguity with common words, we propose a more advanced\nmethod to recognize them. Methods: We present Kusuri, an Ensemble Learning\nclassifier, able to identify tweets mentioning drug products and dietary\nsupplements. Kusuri (\"medication\" in Japanese) is composed of two modules.\nFirst, four different classifiers (lexicon-based, spelling-variant-based,\npattern-based and one based on a weakly-trained neural network) are applied in\nparallel to discover tweets potentially containing medication names. Second, an\nensemble of deep neural networks encoding morphological, semantical and\nlong-range dependencies of important words in the tweets discovered is used to\nmake the final decision. Results: On a balanced (50-50) corpus of 15,005\ntweets, Kusuri demonstrated performances close to human annotators with 93.7%\nF1-score, the best score achieved thus far on this corpus. On a corpus made of\nall tweets posted by 113 Twitter users (98,959 tweets, with only 0.26%\nmentioning medications), Kusuri obtained 76.3% F1-score. There is not a prior\ndrug extraction system that compares running on such an extremely unbalanced\ndataset. Conclusion: The system identifies tweets mentioning drug names with\nperformance high enough to ensure its usefulness and ready to be integrated in\nlarger natural language processing systems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:18:17 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 16:34:33 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Weissenbacher", "Davy", ""], ["Sarker", "Abeed", ""], ["Klein", "Ari", ""], ["O'Connor", "Karen", ""], ["Ranganatha", "Arjun Magge", ""], ["Gonzalez-Hernandez", "Graciela", ""]]}, {"id": "1904.05342", "submitter": "Kexin Huang", "authors": "Kexin Huang, Jaan Altosaar, Rajesh Ranganath", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital\n  Readmission", "comments": "CHIL 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical notes contain information about patients that goes beyond structured\ndata like lab values and medications. However, clinical notes have been\nunderused relative to structured data, because notes are high-dimensional and\nsparse. This work develops and evaluates representations of clinical notes\nusing bidirectional transformers (ClinicalBERT). ClinicalBERT uncovers\nhigh-quality relationships between medical concepts as judged by humans.\nClinicalBert outperforms baselines on 30-day hospital readmission prediction\nusing both discharge summaries and the first few days of notes in the intensive\ncare unit. Code and model parameters are available.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:53:13 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 16:51:52 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 03:40:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Huang", "Kexin", ""], ["Altosaar", "Jaan", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "1904.05426", "submitter": "Ronald Cardenas Acosta", "authors": "Ronald Cardenas, Ying Lin, Heng Ji, Jonathan May", "title": "A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource\n  Languages", "comments": "NAACL-HLT 2019, 12 pages, code available at\n  https://github.com/isi-nlp/universal-cipher-pos-tagging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised part of speech (POS) tagging is often framed as a clustering\nproblem, but practical taggers need to \\textit{ground} their clusters as well.\nGrounding generally requires reference labeled data, a luxury a low-resource\nlanguage might not have. In this work, we describe an approach for low-resource\nunsupervised POS tagging that yields fully grounded output and requires no\nlabeled training data. We find the classic method of Brown et al. (1992)\nclusters well in our use case and employ a decipherment-based approach to\ngrounding. This approach presumes a sequence of cluster IDs is a `ciphertext'\nand seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We\nshow intrinsically that, despite the difficulty of the task, we obtain\nreasonable performance across a variety of languages. We also show\nextrinsically that incorporating our POS tagger into a name tagger leads to\nstate-of-the-art tagging performance in Sinhalese and Kinyarwanda, two\nlanguages with nearly no labeled POS data available. We further demonstrate our\ntagger's utility by incorporating it into a true `zero-resource' variant of the\nMalopa (Ammar et al., 2016) dependency parser model that removes the current\nreliance on multilingual resources and gold POS tags for new languages.\nExperiments show that including our tagger makes up much of the accuracy lost\nwhen gold POS tags are unavailable.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 20:22:31 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Cardenas", "Ronald", ""], ["Lin", "Ying", ""], ["Ji", "Heng", ""], ["May", "Jonathan", ""]]}, {"id": "1904.05439", "submitter": "Marco Rovera", "authors": "Marco Rovera, Federico Nanni, Simone Paolo Ponzetto", "title": "Event-based Access to Historical Italian War Memoirs", "comments": "23 pages, 6 figures", "journal-ref": "J. Comput. Cult. Herit. 14, 1, Article 2 (February 2021)", "doi": "10.1145/3406210", "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progressive digitization of historical archives provides new, often\ndomain specific, textual resources that report on facts and events which have\nhappened in the past; among these, memoirs are a very common type of primary\nsource. In this paper, we present an approach for extracting information from\nItalian historical war memoirs and turning it into structured knowledge. This\nis based on the semantic notions of events, participants and roles. We evaluate\nquantitatively each of the key-steps of our approach and provide a graph-based\nrepresentation of the extracted knowledge, which allows to move between a Close\nand a Distant Reading of the collection.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:30:36 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 13:19:25 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 14:42:18 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Rovera", "Marco", ""], ["Nanni", "Federico", ""], ["Ponzetto", "Simone Paolo", ""]]}, {"id": "1904.05440", "submitter": "Ashutosh Modi", "authors": "Yeyao Zhang, Eleftheria Tsipidi, Sasha Schriber, Mubbasir Kapadia,\n  Markus Gross, Ashutosh Modi", "title": "Generating Animations from Screenplays", "comments": "9+1+6 Pages, Accepted at StarSEM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.GR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating animation from natural language text finds\napplication in a number of areas e.g. movie script writing, instructional\nvideos, and public safety. However, translating natural language text into\nanimation is a challenging task. Existing text-to-animation systems can handle\nonly very simple sentences, which limits their applications. In this paper, we\ndevelop a text-to-animation system which is capable of handling complex\nsentences. We achieve this by introducing a text simplification step into the\nprocess. Building on an existing animation generation system for screenwriting,\nwe create a robust NLP pipeline to extract information from screenplays and map\nthem to the system's knowledge base. We develop a set of linguistic\ntransformation rules that simplify complex sentences. Information extracted\nfrom the simplified sentences is used to generate a rough storyboard and video\ndepicting the text. Our sentence simplification module outperforms existing\nsystems in terms of BLEU and SARI metrics.We further evaluated our system via a\nuser study: 68 % participants believe that our system generates reasonable\nanimation from input screenplays.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:04:54 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Zhang", "Yeyao", ""], ["Tsipidi", "Eleftheria", ""], ["Schriber", "Sasha", ""], ["Kapadia", "Mubbasir", ""], ["Gross", "Markus", ""], ["Modi", "Ashutosh", ""]]}, {"id": "1904.05506", "submitter": "Sorami Hisamoto", "authors": "Sorami Hisamoto, Matt Post, Kevin Duh", "title": "Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data\n  In Your Machine Translation System?", "comments": null, "journal-ref": "Tansactions of the Association for Computational Linguistics\n  (TACL) Volume 8, 2020 p.49-63", "doi": "10.1162/tacl_a_00299", "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data privacy is an important issue for \"machine learning as a service\"\nproviders. We focus on the problem of membership inference attacks: given a\ndata sample and black-box access to a model's API, determine whether the sample\nexisted in the model's training data. Our contribution is an investigation of\nthis problem in the context of sequence-to-sequence models, which are important\nin applications such as machine translation and video captioning. We define the\nmembership inference problem for sequence generation, provide an open dataset\nbased on state-of-the-art machine translation models, and report initial\nresults on whether these models leak private information against several kinds\nof membership inference attacks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 02:53:21 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 10:27:24 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hisamoto", "Sorami", ""], ["Post", "Matt", ""], ["Duh", "Kevin", ""]]}, {"id": "1904.05521", "submitter": "Hao Wu", "authors": "Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun,\n  Wei-Ying Ma", "title": "UniVSE: Robust Visual Semantic Embeddings via Structured Semantic\n  Representations", "comments": "v1 is the full version which is accepted by CVPR 2019. v2 is the\n  short version accepted by NAACL 2019 SpLU-RoboNLP workshop (in non-archival\n  proceedings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Unified Visual-Semantic Embeddings (UniVSE) for learning a joint\nspace of visual and textual concepts. The space unifies the concepts at\ndifferent levels, including objects, attributes, relations, and full scenes. A\ncontrastive learning approach is proposed for the fine-grained alignment from\nonly image-caption pairs. Moreover, we present an effective approach for\nenforcing the coverage of semantic components that appear in the sentence. We\ndemonstrate the robustness of Unified VSE in defending text-domain adversarial\nattacks on cross-modal retrieval tasks. Such robustness also empowers the use\nof visual cues to resolve word dependencies in novel sentences.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 04:04:06 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 03:21:28 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Wu", "Hao", ""], ["Mao", "Jiayuan", ""], ["Zhang", "Yufeng", ""], ["Jiang", "Yuning", ""], ["Li", "Lei", ""], ["Sun", "Weiwei", ""], ["Ma", "Wei-Ying", ""]]}, {"id": "1904.05527", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn", "title": "Modeling Global Syntactic Variation in English Using Dialect\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates global-scale dialect identification for 14 national\nvarieties of English as a means for studying syntactic variation. The paper\nmakes three main contributions: (i) introducing data-driven language mapping as\na method for selecting the inventory of national varieties to include in the\ntask; (ii) producing a large and dynamic set of syntactic features using\ngrammar induction rather than focusing on a few hand-selected features such as\nfunction words; and (iii) comparing models across both web corpora and social\nmedia corpora in order to measure the robustness of syntactic variation across\nregisters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 04:38:35 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Dunn", "Jonathan", ""]]}, {"id": "1904.05529", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn", "title": "Frequency vs. Association for Constraint Selection in Usage-Based\n  Construction Grammar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A usage-based Construction Grammar (CxG) posits that slot-constraints\ngeneralize from common exemplar constructions. But what is the best model of\nconstraint generalization? This paper evaluates competing frequency-based and\nassociation-based models across eight languages using a metric derived from the\nMinimum Description Length paradigm. The experiments show that\nassociation-based models produce better generalizations across all languages by\na significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 04:41:46 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Dunn", "Jonathan", ""]]}, {"id": "1904.05530", "submitter": "Xiang Ren", "authors": "Woojeong Jin, Meng Qu, Xisen Jin, Xiang Ren", "title": "Recurrent Event Network: Autoregressive Structure Inference over\n  Temporal Knowledge Graphs", "comments": "15 pages, 8 figures, accepted at as full paper in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph reasoning is a critical task in natural language processing.\nThe task becomes more challenging on temporal knowledge graphs, where each fact\nis associated with a timestamp. Most existing methods focus on reasoning at\npast timestamps and they are not able to predict facts happening in the future.\nThis paper proposes Recurrent Event Network (RE-NET), a novel autoregressive\narchitecture for predicting future interactions. The occurrence of a fact\n(event) is modeled as a probability distribution conditioned on temporal\nsequences of past knowledge graphs. Specifically, our RE-NET employs a\nrecurrent event encoder to encode past facts and uses a neighborhood aggregator\nto model the connection of facts at the same timestamp. Future facts can then\nbe inferred in a sequential manner based on the two modules. We evaluate our\nproposed method via link prediction at future times on five public datasets.\nThrough extensive experiments, we demonstrate the strength of RENET, especially\non multi-step inference over future timestamps, and achieve state-of-the-art\nperformance on all five datasets. Code and data can be found at\nhttps://github.com/INK-USC/RE-Net.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 04:45:42 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 19:06:37 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 03:32:40 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2020 18:40:59 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Jin", "Woojeong", ""], ["Qu", "Meng", ""], ["Jin", "Xisen", ""], ["Ren", "Xiang", ""]]}, {"id": "1904.05542", "submitter": "Hanan Aldarmaki", "authors": "Hanan Aldarmaki, Mona Diab", "title": "Scalable Cross-Lingual Transfer of Neural Sentence Embeddings", "comments": "accepted in *SEM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and investigate several cross-lingual alignment approaches for\nneural sentence embedding models, such as the supervised inference classifier,\nInferSent, and sequential encoder-decoder models. We evaluate three alignment\nframeworks applied to these models: joint modeling, representation transfer\nlearning, and sentence mapping, using parallel text to guide the alignment. Our\nresults support representation transfer as a scalable approach for modular\ncross-lingual alignment of neural sentence embeddings, where we observe better\nperformance compared to joint models in intrinsic and extrinsic evaluations,\nparticularly with smaller sets of parallel data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 05:41:27 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Aldarmaki", "Hanan", ""], ["Diab", "Mona", ""]]}, {"id": "1904.05544", "submitter": "Zhuo Lei", "authors": "Zhuo Lei, Chao Zhang, Qian Zhang and Guoping Qiu", "title": "FrameRank: A Text Processing Approach to Video Summarization", "comments": "accepted by ICME 2019 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization has been extensively studied in the past decades.\nHowever, user-generated video summarization is much less explored since there\nlack large-scale video datasets within which human-generated video summaries\nare unambiguously defined and annotated. Toward this end, we propose a\nuser-generated video summarization dataset - UGSum52 - that consists of 52\nvideos (207 minutes). In constructing the dataset, because of the subjectivity\nof user-generated video summarization, we manually annotate 25 summaries for\neach video, which are in total 1300 summaries. To the best of our knowledge, it\nis currently the largest dataset for user-generated video summarization.\n  Based on this dataset, we present FrameRank, an unsupervised video\nsummarization method that employs a frame-to-frame level affinity graph to\nidentify coherent and informative frames to summarize a video. We use the\nKullback-Leibler(KL)-divergence-based graph to rank temporal segments according\nto the amount of semantic information contained in their frames. We illustrate\nthe effectiveness of our method by applying it to three datasets SumMe, TVSum\nand UGSum52 and show it achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 06:16:17 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 10:45:19 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Lei", "Zhuo", ""], ["Zhang", "Chao", ""], ["Zhang", "Qian", ""], ["Qiu", "Guoping", ""]]}, {"id": "1904.05548", "submitter": "Zilong Zheng", "authors": "Zilong Zheng, Wenguan Wang, Siyuan Qi, Song-Chun Zhu", "title": "Reasoning Visual Dialogs with Structural and Partial Observations", "comments": "CVPR 2019 Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model to address the task of Visual Dialog which exhibits\ncomplex dialog structures. To obtain a reasonable answer based on the current\nquestion and the dialog history, the underlying semantic dependencies between\ndialog entities are essential. In this paper, we explicitly formalize this task\nas inference in a graphical model with partially observed nodes and unknown\ngraph structures (relations in dialog). The given dialog entities are viewed as\nthe observed nodes. The answer to a given question is represented by a node\nwith missing value. We first introduce an Expectation Maximization algorithm to\ninfer both the underlying dialog structures and the missing node values\n(desired answers). Based on this, we proceed to propose a differentiable graph\nneural network (GNN) solution that approximates this process. Experiment\nresults on the VisDial and VisDial-Q datasets show that our model outperforms\ncomparative methods. It is also observed that our method can infer the\nunderlying dialog structure for better dialog reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 06:46:15 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 23:40:33 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Zheng", "Zilong", ""], ["Wang", "Wenguan", ""], ["Qi", "Siyuan", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1904.05557", "submitter": "Xavier Tannier", "authors": "Charlotte Rudnik and Thibault Ehrhart and Olivier Ferret and Denis\n  Teyssou and Rapha\\\"el Troncy and Xavier Tannier", "title": "Searching News Articles Using an Event Knowledge Graph Leveraged by\n  Wikidata", "comments": null, "journal-ref": "WikiWorkshop at The Web Conference 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News agencies produce thousands of multimedia stories describing events\nhappening in the world that are either scheduled such as sports competitions,\npolitical summits and elections, or breaking events such as military conflicts,\nterrorist attacks, natural disasters, etc. When writing up those stories,\njournalists refer to contextual background and to compare with past similar\nevents. However, searching for precise facts described in stories is hard. In\nthis paper, we propose a general method that leverages the Wikidata knowledge\nbase to produce semantic annotations of news articles. Next, we describe a\nsemantic search engine that supports both keyword based search in news articles\nand structured data search providing filters for properties belonging to\nspecific event schemas that are automatically inferred.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 07:08:09 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Rudnik", "Charlotte", ""], ["Ehrhart", "Thibault", ""], ["Ferret", "Olivier", ""], ["Teyssou", "Denis", ""], ["Troncy", "Rapha\u00ebl", ""], ["Tannier", "Xavier", ""]]}, {"id": "1904.05569", "submitter": "Pham Ngoc Phuong", "authors": "Pham Ngoc Phuong, Quoc Truong Do, Luong Chi Mai", "title": "A high quality and phonetic balanced speech corpus for Vietnamese", "comments": "5 pages", "journal-ref": "Oriental COCOSDA 2018", "doi": null, "report-no": "7-8 May 2018, Miyazaki, Japan", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a high quality Vietnamese speech corpus that can be used\nfor analyzing Vietnamese speech characteristic as well as building speech\nsynthesis models. The corpus consists of 5400 clean-speech utterances spoken by\n12 speakers including 6 males and 6 females. The corpus is designed with\nphonetic balanced in mind so that it can be used for speech synthesis,\nespecially, speech adaptation approaches. Specifically, all speakers utter a\ncommon dataset contains 250 phonetic balanced sentences. To increase the\nvariety of speech context, each speaker also utters another 200 non-shared,\nphonetic-balanced sentences. The speakers are selected to cover a wide range of\nage and come from different regions of the North of Vietnam. The audios are\nrecorded in a soundproof studio room, they are sampling at 48 kHz, 16 bits PCM,\nmono channel.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 07:57:51 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Phuong", "Pham Ngoc", ""], ["Do", "Quoc Truong", ""], ["Mai", "Luong Chi", ""]]}, {"id": "1904.05576", "submitter": "Galina Lavrentyeva", "authors": "Galina Lavrentyeva, Sergey Novoselov, Andzhukaev Tseren, Marina\n  Volkova, Artem Gorlanov, Alexandr Kozlov", "title": "STC Antispoofing Systems for the ASVspoof2019 Challenge", "comments": "Submitted to Interspeech 2019, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.CR cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Speech Technology Center (STC) antispoofing systems\nsubmitted to the ASVspoof 2019 challenge. The ASVspoof2019 is the extended\nversion of the previous challenges and includes 2 evaluation conditions:\nlogical access use-case scenario with speech synthesis and voice conversion\nattack types and physical access use-case scenario with replay attacks. During\nthe challenge we developed anti-spoofing solutions for both scenarios. The\nproposed systems are implemented using deep learning approach and are based on\ndifferent types of acoustic features. We enhanced Light CNN architecture\npreviously considered by the authors for replay attacks detection and which\nperformed high spoofing detection quality during the ASVspoof2017 challenge. In\nparticular here we investigate the efficiency of angular margin based softmax\nactivation for training robust deep Light CNN classifier to solve the\nmentioned-above tasks. Submitted systems achieved EER of 1.86% in logical\naccess scenario and 0.54% in physical access scenario on the evaluation part of\nthe Challenge corpora. High performance obtained for the unknown types of\nspoofing attacks demonstrates the stability of the offered approach in both\nevaluation conditions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 08:37:43 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Lavrentyeva", "Galina", ""], ["Novoselov", "Sergey", ""], ["Tseren", "Andzhukaev", ""], ["Volkova", "Marina", ""], ["Gorlanov", "Artem", ""], ["Kozlov", "Alexandr", ""]]}, {"id": "1904.05584", "submitter": "Jorge Balazs", "authors": "Jorge A. Balazs and Yutaka Matsuo", "title": "Gating Mechanisms for Combining Character and Word-level Word\n  Representations: An Empirical Study", "comments": "Proceedings of the 2019 Conference of the North American Chapter of\n  the Association for Computational Linguistics: Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study how different ways of combining character and\nword-level representations affect the quality of both final word and sentence\nrepresentations. We provide strong empirical evidence that modeling characters\nimproves the learned representations at the word and sentence levels, and that\ndoing so is particularly useful when representing less frequent words. We\nfurther show that a feature-wise sigmoid gating mechanism is a robust method\nfor creating representations that encode semantic similarity, as it performed\nreasonably well in several word similarity datasets. Finally, our findings\nsuggest that properly capturing semantic similarity at the word level does not\nconsistently yield improved performance in downstream sentence-level tasks. Our\ncode is available at https://github.com/jabalazs/gating\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 08:56:48 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Balazs", "Jorge A.", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1904.05588", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn", "title": "Modeling the Complexity and Descriptive Adequacy of Construction\n  Grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses the Minimum Description Length paradigm to model the\ncomplexity of CxGs (operationalized as the encoding size of a grammar)\nalongside their descriptive adequacy (operationalized as the encoding size of a\ncorpus given a grammar). These two quantities are combined to measure the\nquality of potential CxGs against unannotated corpora, supporting\ndiscovery-device CxGs for English, Spanish, French, German, and Italian. The\nresults show (i) that these grammars provide significant generalizations as\nmeasured using compression and (ii) that more complex CxGs with access to\nmultiple levels of representation provide greater generalizations than\nsingle-representation CxGs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 09:06:24 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Dunn", "Jonathan", ""]]}, {"id": "1904.05606", "submitter": "Pavel Kral", "authors": "Ji\\v{r}\\'i Mart\\'inek, Pavel Kr\\'al, Ladislav Lenc, Christophe\n  Cerisara", "title": "Multi-lingual Dialogue Act Recognition with Deep Learning Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with multi-lingual dialogue act (DA) recognition. The\nproposed approaches are based on deep neural networks and use word2vec\nembeddings for word representation. Two multi-lingual models are proposed for\nthis task. The first approach uses one general model trained on the embeddings\nfrom all available languages. The second method trains the model on a single\npivot language and a linear transformation method is used to project other\nlanguages onto the pivot language. The popular convolutional neural network and\nLSTM architectures with different set-ups are used as classifiers. To the best\nof our knowledge this is the first attempt at multi-lingual DA recognition\nusing neural networks. The multi-lingual models are validated experimentally on\ntwo languages from the Verbmobil corpus.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 09:55:41 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Mart\u00ednek", "Ji\u0159\u00ed", ""], ["Kr\u00e1l", "Pavel", ""], ["Lenc", "Ladislav", ""], ["Cerisara", "Christophe", ""]]}, {"id": "1904.05643", "submitter": "Jisun An", "authors": "Jisun An, Haewoon Kwak, Oliver Posegga, Andreas Jungherr", "title": "Political Discussions in Homogeneous and Cross-Cutting Communication\n  Spaces", "comments": "Proc. 13th International Conference on Web and Social Media\n  (ICWSM'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online platforms, such as Facebook, Twitter, and Reddit, provide users with a\nrich set of features for sharing and consuming political information,\nexpressing political opinions, and exchanging potentially contrary political\nviews. In such activities, two types of communication spaces naturally emerge:\nthose dominated by exchanges between politically homogeneous users and those\nthat allow and encourage cross-cutting exchanges in politically heterogeneous\ngroups. While research on political talk in online environments abounds, we\nknow surprisingly little about the potentially varying nature of discussions in\npolitically homogeneous spaces as compared to cross-cutting communication\nspaces. To fill this gap, we use Reddit to explore the nature of political\ndiscussions in homogeneous and cross-cutting communication spaces. In\nparticular, we develop an analytical template to study interaction and\nlinguistic patterns within and between politically homogeneous and\nheterogeneous communication spaces. Our analyses reveal different behavioral\npatterns in homogeneous and cross-cutting communications spaces. We discuss\ntheoretical and practical implications in the context of research on political\ntalk online.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 11:46:07 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["An", "Jisun", ""], ["Kwak", "Haewoon", ""], ["Posegga", "Oliver", ""], ["Jungherr", "Andreas", ""]]}, {"id": "1904.05674", "submitter": "Nikos Athanasiou", "authors": "Eleftheria Briakou, Nikos Athanasiou, Alexandros Potamianos", "title": "Cross-topic distributional semantic representations via unsupervised\n  mappings", "comments": "NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional Distributional Semantic Models (DSMs) the multiple senses of a\npolysemous word are conflated into a single vector space representation. In\nthis work, we propose a DSM that learns multiple distributional representations\nof a word based on different topics. First, a separate DSM is trained for each\ntopic and then each of the topic-based DSMs is aligned to a common vector\nspace. Our unsupervised mapping approach is motivated by the hypothesis that\nwords preserving their relative distances in different topic semantic\nsub-spaces constitute robust \\textit{semantic anchors} that define the mappings\nbetween them. Aligned cross-topic representations achieve state-of-the-art\nresults for the task of contextual word similarity. Furthermore, evaluation on\nNLP downstream tasks shows that multiple topic-based embeddings outperform\nsingle-prototype models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 13:09:57 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Briakou", "Eleftheria", ""], ["Athanasiou", "Nikos", ""], ["Potamianos", "Alexandros", ""]]}, {"id": "1904.05746", "submitter": "Pramit Saha", "authors": "Pramit Saha, Muhammad Abdul-Mageed, Sidney Fels", "title": "SPEAK YOUR MIND! Towards Imagined Speech Recognition With Hierarchical\n  Deep Learning", "comments": "Under review in INTERSPEECH 2019. arXiv admin note: text overlap with\n  arXiv:1904.04358", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-related Brain Computer Interface (BCI) technologies provide effective\nvocal communication strategies for controlling devices through speech commands\ninterpreted from brain signals. In order to infer imagined speech from active\nthoughts, we propose a novel hierarchical deep learning BCI system for\nsubject-independent classification of 11 speech tokens including phonemes and\nwords. Our novel approach exploits predicted articulatory information of six\nphonological categories (e.g., nasal, bilabial) as an intermediate step for\nclassifying the phonemes and words, thereby finding discriminative signal\nresponsible for natural speech synthesis. The proposed network is composed of\nhierarchical combination of spatial and temporal CNN cascaded with a deep\nautoencoder. Our best models on the KARA database achieve an average accuracy\nof 83.42% across the six different binary phonological classification tasks,\nand 53.36% for the individual token identification task, significantly\noutperforming our baselines. Ultimately, our work suggests the possible\nexistence of a brain imagery footprint for the underlying articulatory movement\nrelated to different sounds that can be used to aid imagined speech decoding.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:41:54 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Saha", "Pramit", ""], ["Abdul-Mageed", "Muhammad", ""], ["Fels", "Sidney", ""]]}, {"id": "1904.05780", "submitter": "Shankar Kumar", "authors": "Jared Lichtarge, Chris Alberti, Shankar Kumar, Noam Shazeer, Niki\n  Parmar, Simon Tong", "title": "Corpora Generation for Grammatical Error Correction", "comments": "Accepted at NAACL 2019. arXiv admin note: text overlap with\n  arXiv:1811.01710", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammatical Error Correction (GEC) has been recently modeled using the\nsequence-to-sequence framework. However, unlike sequence transduction problems\nsuch as machine translation, GEC suffers from the lack of plentiful parallel\ndata. We describe two approaches for generating large parallel datasets for GEC\nusing publicly available Wikipedia data. The first method extracts\nsource-target pairs from Wikipedia edit histories with minimal filtration\nheuristics, while the second method introduces noise into Wikipedia sentences\nvia round-trip translation through bridge languages. Both strategies yield\nsimilar sized parallel corpora containing around 4B tokens. We employ an\niterative decoding strategy that is tailored to the loosely supervised nature\nof our constructed corpora. We demonstrate that neural GEC models trained using\neither type of corpora give similar performance. Fine-tuning these models on\nthe Lang-8 corpus and ensembling allows us to surpass the state of the art on\nboth the CoNLL-2014 benchmark and the JFLEG task. We provide systematic\nanalysis that compares the two approaches to data generation and highlights the\neffectiveness of ensembling.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 05:47:15 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Lichtarge", "Jared", ""], ["Alberti", "Chris", ""], ["Kumar", "Shankar", ""], ["Shazeer", "Noam", ""], ["Parmar", "Niki", ""], ["Tong", "Simon", ""]]}, {"id": "1904.05829", "submitter": "Kechen Qin", "authors": "Kechen Qin, Cheng Li, Virgil Pavlu, Javed A. Aslam", "title": "Adapting RNN Sequence Prediction Model to Multi-label Set Prediction", "comments": "Accepted to NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptation of RNN sequence models to the problem of multi-label\nclassification for text, where the target is a set of labels, not a sequence.\nPrevious such RNN models define probabilities for sequences but not for sets;\nattempts to obtain a set probability are after-thoughts of the network design,\nincluding pre-specifying the label order, or relating the sequence probability\nto the set probability in ad hoc ways.\n  Our formulation is derived from a principled notion of set probability, as\nthe sum of probabilities of corresponding permutation sequences for the set. We\nprovide a new training objective that maximizes this set probability, and a new\nprediction objective that finds the most probable set on a test document. These\nnew objectives are theoretically appealing because they give the RNN model\nfreedom to discover the best label order, which often is the natural one (but\ndifferent among documents).\n  We develop efficient procedures to tackle the computation difficulties\ninvolved in training and prediction. Experiments on benchmark datasets\ndemonstrate that we outperform state-of-the-art methods for this task.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:33:54 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Qin", "Kechen", ""], ["Li", "Cheng", ""], ["Pavlu", "Virgil", ""], ["Aslam", "Javed A.", ""]]}, {"id": "1904.05862", "submitter": "Steffen Schneider", "authors": "Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli", "title": "wav2vec: Unsupervised Pre-training for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore unsupervised pre-training for speech recognition by learning\nrepresentations of raw audio. wav2vec is trained on large amounts of unlabeled\naudio data and the resulting representations are then used to improve acoustic\nmodel training. We pre-train a simple multi-layer convolutional neural network\noptimized via a noise contrastive binary classification task. Our experiments\non WSJ reduce WER of a strong character-based log-mel filterbank baseline by up\nto 36% when only a few hours of transcribed data is available. Our approach\nachieves 2.43% WER on the nov92 test set. This outperforms Deep Speech 2, the\nbest reported character-based system in the literature while using two orders\nof magnitude less labeled training data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:29:30 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 06:09:03 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 06:16:44 GMT"}, {"version": "v4", "created": "Wed, 11 Sep 2019 08:19:49 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Schneider", "Steffen", ""], ["Baevski", "Alexei", ""], ["Collobert", "Ronan", ""], ["Auli", "Michael", ""]]}, {"id": "1904.05873", "submitter": "Jifeng Dai", "authors": "Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, Jifeng Dai", "title": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have become a popular component in deep neural networks,\nyet there has been little examination of how different influencing factors and\nmethods for computing attention from these factors affect performance. Toward a\nbetter general understanding of attention mechanisms, we present an empirical\nstudy that ablates various spatial attention elements within a generalized\nattention formulation, encompassing the dominant Transformer attention as well\nas the prevalent deformable convolution and dynamic convolution modules.\nConducted on a variety of applications, the study yields significant findings\nabout spatial attention in deep networks, some of which run counter to\nconventional understanding. For example, we find that the query and key content\ncomparison in Transformer attention is negligible for self-attention, but vital\nfor encoder-decoder attention. A proper combination of deformable convolution\nwith key content only saliency achieves the best accuracy-efficiency tradeoff\nin self-attention. Our results suggest that there exists much room for\nimprovement in the design of attention mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:58:37 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Zhu", "Xizhou", ""], ["Cheng", "Dazhi", ""], ["Zhang", "Zheng", ""], ["Lin", "Stephen", ""], ["Dai", "Jifeng", ""]]}, {"id": "1904.05876", "submitter": "Idan Schwartz", "authors": "Idan Schwartz, Alexander Schwing and Tamir Hazan", "title": "A Simple Baseline for Audio-Visual Scene-Aware Dialog", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed audio-visual scene-aware dialog task paves the way to a\nmore data-driven way of learning virtual assistants, smart speakers and car\nnavigation systems. However, very little is known to date about how to\neffectively extract meaningful information from a plethora of sensors that\npound the computational engine of those devices. Therefore, in this paper, we\nprovide and carefully analyze a simple baseline for audio-visual scene-aware\ndialog which is trained end-to-end. Our method differentiates in a data-driven\nmanner useful signals from distracting ones using an attention mechanism. We\nevaluate the proposed approach on the recently introduced and challenging\naudio-visual scene-aware dataset, and demonstrate the key features that permit\nto outperform the current state-of-the-art by more than 20\\% on CIDEr.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:51 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Schwartz", "Idan", ""], ["Schwing", "Alexander", ""], ["Hazan", "Tamir", ""]]}, {"id": "1904.05880", "submitter": "Idan Schwartz", "authors": "Idan Schwartz and Seunghak Yu and Tamir Hazan and Alexander Schwing", "title": "Factor Graph Attention", "comments": "Accepted to CVPR 2019; revised version includes bottom-up features", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog is an effective way to exchange information, but subtle details and\nnuances are extremely important. While significant progress has paved a path to\naddress visual dialog with algorithms, details and nuances remain a challenge.\nAttention mechanisms have demonstrated compelling results to extract details in\nvisual question answering and also provide a convincing framework for visual\ndialog due to their interpretability and effectiveness. However, the many data\nutilities that accompany visual dialog challenge existing attention techniques.\nWe address this issue and develop a general attention mechanism for visual\ndialog which operates on any number of data utilities. To this end, we design a\nfactor graph based attention mechanism which combines any number of utility\nrepresentations. We illustrate the applicability of the proposed approach on\nthe challenging and recently introduced VisDial datasets, outperforming recent\nstate-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on\nMRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:58 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 20:05:12 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 23:35:13 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Schwartz", "Idan", ""], ["Yu", "Seunghak", ""], ["Hazan", "Tamir", ""], ["Schwing", "Alexander", ""]]}, {"id": "1904.05929", "submitter": "Ori Shapira", "authors": "Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth Pasunuru,\n  Mohit Bansal, Yael Amsterdamer, Ido Dagan", "title": "Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation", "comments": "5 pages, 2 graphs, 1 table. Published in NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conducting a manual evaluation is considered an essential part of summary\nevaluation methodology. Traditionally, the Pyramid protocol, which exhaustively\ncompares system summaries to references, has been perceived as very reliable,\nproviding objective scores. Yet, due to the high cost of the Pyramid method and\nthe required expertise, researchers resorted to cheaper and less thorough\nmanual evaluation methods, such as Responsiveness and pairwise comparison,\nattainable via crowdsourcing. We revisit the Pyramid approach, proposing a\nlightweight sampling-based version that is crowdsourcable. We analyze the\nperformance of our method in comparison to original expert-based Pyramid\nevaluations, showing higher correlation relative to the common Responsiveness\nmethod. We release our crowdsourced Summary-Content-Units, along with all\ncrowdsourcing scripts, for future evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 19:04:57 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Shapira", "Ori", ""], ["Gabay", "David", ""], ["Gao", "Yang", ""], ["Ronen", "Hadar", ""], ["Pasunuru", "Ramakanth", ""], ["Bansal", "Mohit", ""], ["Amsterdamer", "Yael", ""], ["Dagan", "Ido", ""]]}, {"id": "1904.05953", "submitter": "Fernando Alva-Manchego", "authors": "Pierre Finnimore, Elisabeth Fritzsch, Daniel King, Alison Sneyd, Aneeq\n  Ur Rehman, Fernando Alva-Manchego, Andreas Vlachos", "title": "Strong Baselines for Complex Word Identification across Multiple\n  Languages", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex Word Identification (CWI) is the task of identifying which words or\nphrases in a sentence are difficult to understand by a target audience. The\nlatest CWI Shared Task released data for two settings: monolingual (i.e. train\nand test in the same language) and cross-lingual (i.e. test in a language not\nseen during training). The best monolingual models relied on language-dependent\nfeatures, which do not generalise in the cross-lingual setting, while the best\ncross-lingual model used neural networks with multi-task learning. In this\npaper, we present monolingual and cross-lingual CWI models that perform as well\nas (or better than) most models submitted to the latest CWI Shared Task. We\nshow that carefully selected features and simple learning models can achieve\nstate-of-the-art performance, and result in strong baselines for future\ndevelopment in this area. Finally, we discuss how inconsistencies in the\nannotation of the data can explain some of the results obtained.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 20:50:18 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Finnimore", "Pierre", ""], ["Fritzsch", "Elisabeth", ""], ["King", "Daniel", ""], ["Sneyd", "Alison", ""], ["Rehman", "Aneeq Ur", ""], ["Alva-Manchego", "Fernando", ""], ["Vlachos", "Andreas", ""]]}, {"id": "1904.06002", "submitter": "Panayiotis Georgiou", "authors": "Md Nasir, Sandeep Nallan Chakravarthula, Brian Baucom, David C.\n  Atkins, Panayiotis Georgiou, and Shrikanth Narayanan", "title": "Modeling Interpersonal Linguistic Coordination in Conversations using\n  Word Mover's Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linguistic coordination is a well-established phenomenon in spoken\nconversations and often associated with positive social behaviors and outcomes.\nWhile there have been many attempts to measure lexical coordination or\nentrainment in literature, only a few have explored coordination in syntactic\nor semantic space. In this work, we attempt to combine these different aspects\nof coordination into a single measure by leveraging distances in a neural word\nrepresentation space. In particular, we adopt the recently proposed Word\nMover's Distance with word2vec embeddings and extend it to measure the\ndissimilarity in language used in multiple consecutive speaker turns. To\nvalidate our approach, we apply this measure for two case studies in the\nclinical psychology domain. We find that our proposed measure is correlated\nwith the therapist's empathy towards their patient in Motivational Interviewing\nand with affective behaviors in Couples Therapy. In both case studies, our\nproposed metric exhibits higher correlation than previously proposed measures.\nWhen applied to the couples with relationship improvement, we also notice a\nsignificant decrease in the proposed measure over the course of therapy,\nindicating higher linguistic coordination.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 01:54:01 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Nasir", "Md", ""], ["Chakravarthula", "Sandeep Nallan", ""], ["Baucom", "Brian", ""], ["Atkins", "David C.", ""], ["Georgiou", "Panayiotis", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "1904.06022", "submitter": "Gaurav Sahu", "authors": "Gaurav Sahu", "title": "Multimodal Speech Emotion Recognition and Ambiguity Resolution", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying emotion from speech is a non-trivial task pertaining to the\nambiguous definition of emotion itself. In this work, we adopt a\nfeature-engineering based approach to tackle the task of speech emotion\nrecognition. Formalizing our problem as a multi-class classification problem,\nwe compare the performance of two categories of models. For both, we extract\neight hand-crafted features from the audio signal. In the first approach, the\nextracted features are used to train six traditional machine learning\nclassifiers, whereas the second approach is based on deep learning wherein a\nbaseline feed-forward neural network and an LSTM-based classifier are trained\nover the same features. In order to resolve ambiguity in communication, we also\ninclude features from the text domain. We report accuracy, f-score, precision,\nand recall for the different experiment settings we evaluated our models in.\nOverall, we show that lighter machine learning based models trained over a few\nhand-crafted features are able to achieve performance comparable to the current\ndeep learning based state-of-the-art method for emotion recognition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 03:22:13 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Sahu", "Gaurav", ""]]}, {"id": "1904.06037", "submitter": "Ye Jia", "authors": "Ye Jia, Ron J. Weiss, Fadi Biadsy, Wolfgang Macherey, Melvin Johnson,\n  Zhifeng Chen, Yonghui Wu", "title": "Direct speech-to-speech translation with a sequence-to-sequence model", "comments": "Accepted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an attention-based sequence-to-sequence neural network which can\ndirectly translate speech from one language into speech in another language,\nwithout relying on an intermediate text representation. The network is trained\nend-to-end, learning to map speech spectrograms into target spectrograms in\nanother language, corresponding to the translated content (in a different\ncanonical voice). We further demonstrate the ability to synthesize translated\nspeech using the voice of the source speaker. We conduct experiments on two\nSpanish-to-English speech translation datasets, and find that the proposed\nmodel slightly underperforms a baseline cascade of a direct speech-to-text\ntranslation model and a text-to-speech synthesis model, demonstrating the\nfeasibility of the approach on this very challenging task.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 05:15:31 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 21:34:10 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Jia", "Ye", ""], ["Weiss", "Ron J.", ""], ["Biadsy", "Fadi", ""], ["Macherey", "Wolfgang", ""], ["Johnson", "Melvin", ""], ["Chen", "Zhifeng", ""], ["Wu", "Yonghui", ""]]}, {"id": "1904.06038", "submitter": "Ravi Shekhar", "authors": "Ravi Shekhar, Ece Takmaz, Raquel Fern\\'andez, Raffaella Bernardi", "title": "Evaluating the Representational Hub of Language and Vision Models", "comments": "Accepted to IWCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The multimodal models used in the emerging field at the intersection of\ncomputational linguistics and computer vision implement the bottom-up\nprocessing of the `Hub and Spoke' architecture proposed in cognitive science to\nrepresent how the brain processes and combines multi-sensory inputs. In\nparticular, the Hub is implemented as a neural network encoder. We investigate\nthe effect on this encoder of various vision-and-language tasks proposed in the\nliterature: visual question answering, visual reference resolution, and\nvisually grounded dialogue. To measure the quality of the representations\nlearned by the encoder, we use two kinds of analyses. First, we evaluate the\nencoder pre-trained on the different vision-and-language tasks on an existing\ndiagnostic task designed to assess multimodal semantic understanding. Second,\nwe carry out a battery of analyses aimed at studying how the encoder merges and\nexploits the two modalities.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 05:18:35 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Shekhar", "Ravi", ""], ["Takmaz", "Ece", ""], ["Fern\u00e1ndez", "Raquel", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1904.06063", "submitter": "Zhizheng Wu", "authors": "Liumeng Xue, Wei Song, Guanghui Xu, Lei Xie, Zhizheng Wu", "title": "Building a mixed-lingual neural TTS system with only monolingual data", "comments": "To appear in INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When deploying a Chinese neural text-to-speech (TTS) synthesis system, one of\nthe challenges is to synthesize Chinese utterances with English phrases or\nwords embedded. This paper looks into the problem in the encoder-decoder\nframework when only monolingual data from a target speaker is available.\nSpecifically, we view the problem from two aspects: speaker consistency within\nan utterance and naturalness. We start the investigation with an Average Voice\nModel which is built from multi-speaker monolingual data, i.e. Mandarin and\nEnglish data. On the basis of that, we look into speaker embedding for speaker\nconsistency within an utterance and phoneme embedding for naturalness and\nintelligibility and study the choice of data for model training. We report the\nfindings and discuss the challenges to build a mixed-lingual TTS system with\nonly monolingual data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 06:54:34 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 04:26:03 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Xue", "Liumeng", ""], ["Song", "Wei", ""], ["Xu", "Guanghui", ""], ["Xie", "Lei", ""], ["Wu", "Zhizheng", ""]]}, {"id": "1904.06086", "submitter": "Hyung-Min Park", "authors": "Jong-Hyeon Park, Myungwoo Oh, Hyung-Min Park", "title": "Unsupervised Speech Domain Adaptation Based on Disentangled\n  Representation Learning for Robust Speech Recognition", "comments": "Submitted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, the performance of automatic speech recognition (ASR) systems is\nsignificantly degraded due to the mismatch between training and test\nenvironments. Recently, a deep-learning-based image-to-image translation\ntechnique to translate an image from a source domain to a desired domain was\npresented, and cycle-consistent adversarial network (CycleGAN) was applied to\nlearn a mapping for speech-to-speech conversion from a speaker to a target\nspeaker. However, this method might not be adequate to remove corrupting noise\ncomponents for robust ASR because it was designed to convert speech itself. In\nthis paper, we propose a domain adaptation method based on generative\nadversarial nets (GANs) with disentangled representation learning to achieve\nrobustness in ASR systems. In particular, two separated encoders, context and\ndomain encoders, are introduced to learn distinct latent variables. The latent\nvariables allow us to convert the domain of speech according to its context and\ndomain representation. We improved word accuracies by 6.55~15.70\\% for the\nCHiME4 challenge corpus by applying a noisy-to-clean environment adaptation for\nrobust ASR. In addition, similar to the method based on the CycleGAN, this\nmethod can be used for gender adaptation in gender-mismatched recognition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 07:55:53 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Park", "Jong-Hyeon", ""], ["Oh", "Myungwoo", ""], ["Park", "Hyung-Min", ""]]}, {"id": "1904.06093", "submitter": "Sergey Novoselov", "authors": "Sergey Novoselov, Aleksei Gusev, Artem Ivanov, Timur Pekhovsky, Andrey\n  Shulipa, Galina Lavrentyeva, Vladimir Volokhov, Alexandr Kozlov", "title": "STC Speaker Recognition Systems for the VOiCES From a Distance Challenge", "comments": "Submitted to Interspeech 2019, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Speech Technology Center (STC) speaker recognition\n(SR) systems submitted to the VOiCES From a Distance challenge 2019. The\nchallenge's SR task is focused on the problem of speaker recognition in single\nchannel distant/far-field audio under noisy conditions. In this work we\ninvestigate different deep neural networks architectures for speaker embedding\nextraction to solve the task. We show that deep networks with residual frame\nlevel connections outperform more shallow architectures. Simple energy based\nspeech activity detector (SAD) and automatic speech recognition (ASR) based SAD\nare investigated in this work. We also address the problem of data preparation\nfor robust embedding extractors training. The reverberation for the data\naugmentation was performed using automatic room impulse response generator. In\nour systems we used discriminatively trained cosine similarity metric learning\nmodel as embedding backend. Scores normalization procedure was applied for each\nindividual subsystem we used. Our final submitted systems were based on the\nfusion of different subsystems. The results obtained on the VOiCES development\nand evaluation sets demonstrate effectiveness and robustness of the proposed\nsystems when dealing with distant/far-field audio under noisy conditions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 08:23:26 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Novoselov", "Sergey", ""], ["Gusev", "Aleksei", ""], ["Ivanov", "Artem", ""], ["Pekhovsky", "Timur", ""], ["Shulipa", "Andrey", ""], ["Lavrentyeva", "Galina", ""], ["Volokhov", "Vladimir", ""], ["Kozlov", "Alexandr", ""]]}, {"id": "1904.06100", "submitter": "Ismini Lourentzou", "authors": "Ismini Lourentzou, Kabir Manghnani, ChengXiang Zhai", "title": "Adapting Sequence to Sequence models for Text Normalization in Social\n  Media", "comments": "Accepted at the 13th International AAAI Conference on Web and Social\n  Media (ICWSM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media offer an abundant source of valuable raw data, however informal\nwriting can quickly become a bottleneck for many natural language processing\n(NLP) tasks. Off-the-shelf tools are usually trained on formal text and cannot\nexplicitly handle noise found in short online posts. Moreover, the variety of\nfrequently occurring linguistic variations presents several challenges, even\nfor humans who might not be able to comprehend the meaning of such posts,\nespecially when they contain slang and abbreviations. Text Normalization aims\nto transform online user-generated text to a canonical form. Current text\nnormalization systems rely on string or phonetic similarity and classification\nmodels that work on a local fashion. We argue that processing contextual\ninformation is crucial for this task and introduce a social media text\nnormalization hybrid word-character attention-based encoder-decoder model that\ncan serve as a pre-processing step for NLP applications to adapt to noisy text\nin social media. Our character-based component is trained on synthetic\nadversarial examples that are designed to capture errors commonly found in\nonline user-generated text. Experiments show that our model surpasses neural\narchitectures designed for text normalization and achieves comparable\nperformance with state-of-the-art related work.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 08:45:43 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Lourentzou", "Ismini", ""], ["Manghnani", "Kabir", ""], ["Zhai", "ChengXiang", ""]]}, {"id": "1904.06101", "submitter": "Anca Dumitrache", "authors": "Anca Dumitrache, Lora Aroyo, Chris Welty", "title": "A Crowdsourced Frame Disambiguation Corpus with Ambiguity", "comments": "Accepted to NAACL-HLT2019", "journal-ref": null, "doi": "10.18653/v1/N19-1224", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a resource for the task of FrameNet semantic frame disambiguation\nof over 5,000 word-sentence pairs from the Wikipedia corpus. The annotations\nwere collected using a novel crowdsourcing approach with multiple workers per\nsentence to capture inter-annotator disagreement. In contrast to the typical\napproach of attributing the best single frame to each word, we provide a list\nof frames with disagreement-based scores that express the confidence with which\neach frame applies to the word. This is based on the idea that inter-annotator\ndisagreement is at least partly caused by ambiguity that is inherent to the\ntext and frames. We have found many examples where the semantics of individual\nframes overlap sufficiently to make them acceptable alternatives for\ninterpreting a sentence. We have argued that ignoring this ambiguity creates an\noverly arbitrary target for training and evaluating natural language processing\nsystems - if humans cannot agree, why would we expect the correct answer from a\nmachine to be any different? To process this data we also utilized an expanded\nlemma-set provided by the Framester system, which merges FN with WordNet to\nenhance coverage. Our dataset includes annotations of 1,000 sentence-word pairs\nwhose lemmas are not part of FN. Finally we present metrics for evaluating\nframe disambiguation systems that account for ambiguity.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 08:48:03 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Dumitrache", "Anca", ""], ["Aroyo", "Lora", ""], ["Welty", "Chris", ""]]}, {"id": "1904.06217", "submitter": "Federico Nanni", "authors": "Federico Nanni, Goran Glavas, Simone Paolo Ponzetto, Heiner\n  Stuckenschmidt", "title": "Political Text Scaling Meets Computational Semantics", "comments": "Updated version - corrected typo in Figure 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last fifteen years, text scaling approaches have become a central\nelement for the text-as-data community. However, they are based on the\nassumption that latent positions can be captured just by modeling\nword-frequency information from the different documents under study. We\nchallenge this by presenting a new semantically aware unsupervised scaling\nalgorithm, SemScale, which relies upon distributional representations of the\ndocuments under study. We conduct an extensive quantitative analysis over a\ncollection of speeches from the European Parliament in five different languages\nand from two different legislations, in order to understand whether a) an\napproach that is aware of semantics would better capture known underlying\npolitical dimensions compared to a frequency-based scaling method, b) such\npositioning correlates in particular with a specific subset of linguistic\ntraits, compared to the use of the entire text, and c) these findings hold\nacross different languages. To support further research on this new branch of\ntext scaling approaches, we release the employed dataset and evaluation\nsetting, an easy-to-use online demo, and a Python implementation of SemScale.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 13:05:06 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 12:23:22 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Nanni", "Federico", ""], ["Glavas", "Goran", ""], ["Ponzetto", "Simone Paolo", ""], ["Stuckenschmidt", "Heiner", ""]]}, {"id": "1904.06234", "submitter": "Shreyansh Singh", "authors": "Shreyansh Singh, Avi Chawla, Ayush Sharma, Anil Kumar Singh", "title": "IIT (BHU) Varanasi at MSR-SRST 2018: A Language Model Based Approach for\n  Natural Language Generation", "comments": null, "journal-ref": "Proceedings of the 1st Workshop on Multilingual Surface\n  Realisation (MSR), 56th Annual Meeting of the Association for Computational\n  Linguistics (ACL), July 2018, Melbourne, Australia", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission system for the Shallow Track of Surface\nRealization Shared Task 2018 (SRST'18). The task was to convert genuine UD\nstructures, from which word order information had been removed and the tokens\nhad been lemmatized, into their correct sentential form. We divide the problem\nstatement into two parts, word reinflection and correct word order prediction.\nFor the first sub-problem, we use a Long Short Term Memory based\nEncoder-Decoder approach. For the second sub-problem, we present a Language\nModel (LM) based approach. We apply two different sub-approaches in the LM\nBased approach and the combined result of these two approaches is considered as\nthe final output of the system.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 13:52:29 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Singh", "Shreyansh", ""], ["Chawla", "Avi", ""], ["Sharma", "Ayush", ""], ["Singh", "Anil Kumar", ""]]}, {"id": "1904.06286", "submitter": "Malihe Alikhani", "authors": "Malihe Alikhani, Sreyasi Nag Chowdhury, Gerard de Melo, Matthew Stone", "title": "CITE: A Corpus of Image-Text Discourse Relations", "comments": "7 pages", "journal-ref": "2019 Annual Conference of the North American Chapter of the\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel crowd-sourced resource for multimodal discourse:\nour resource characterizes inferences in image-text contexts in the domain of\ncooking recipes in the form of coherence relations. Like previous corpora\nannotating discourse structure between text arguments, such as the Penn\nDiscourse Treebank, our new corpus aids in establishing a better understanding\nof natural communication and common-sense reasoning, while our findings have\nimplications for a wide range of applications, such as understanding and\ngeneration of multimodal documents.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 15:46:31 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 02:59:07 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Alikhani", "Malihe", ""], ["Chowdhury", "Sreyasi Nag", ""], ["de Melo", "Gerard", ""], ["Stone", "Matthew", ""]]}, {"id": "1904.06470", "submitter": "Jerrold Soh", "authors": "Jerrold Soh Tsin Howe, Lim How Khang, Ian Ernst Chai", "title": "Legal Area Classification: A Comparative Study of Text Classifiers on\n  Singapore Supreme Court Judgments", "comments": "Accepted to the 1st Workshop on Natural Legal Language Processing\n  (co-located with NAACL2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper conducts a comparative study on the performance of various machine\nlearning (``ML'') approaches for classifying judgments into legal areas. Using\na novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how\nstate-of-the-art NLP methods compare against traditional statistical models\nwhen applied to a legal corpus that comprised few but lengthy documents. All\napproaches tested, including topic model, word embedding, and language\nmodel-based classifiers, performed well with as little as a few hundred\njudgments. However, more work needs to be done to optimize state-of-the-art\nmethods for the legal domain.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 02:48:49 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Howe", "Jerrold Soh Tsin", ""], ["Khang", "Lim How", ""], ["Chai", "Ian Ernst", ""]]}, {"id": "1904.06472", "submitter": "Matthew Henderson", "authors": "Matthew Henderson, Pawe{\\l} Budzianowski, I\\~nigo Casanueva, Sam\n  Coope, Daniela Gerz, Girish Kumar, Nikola Mrk\\v{s}i\\'c, Georgios\n  Spithourakis, Pei-Hao Su, Ivan Vuli\\'c, Tsung-Hsien Wen", "title": "A Repository of Conversational Datasets", "comments": null, "journal-ref": "Proceedings of the Workshop on NLP for Conversational AI (2019)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in Machine Learning is often driven by the availability of large\ndatasets, and consistent evaluation metrics for comparing modeling approaches.\nTo this end, we present a repository of conversational datasets consisting of\nhundreds of millions of examples, and a standardised evaluation procedure for\nconversational response selection models using '1-of-100 accuracy'. The\nrepository contains scripts that allow researchers to reproduce the standard\ndatasets, or to adapt the pre-processing and data filtering steps to their\nneeds. We introduce and evaluate several competitive baselines for\nconversational response selection, whose implementations are shared in the\nrepository, as well as a neural encoder model that is trained on the entire\ntraining set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 02:59:48 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 03:06:52 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Henderson", "Matthew", ""], ["Budzianowski", "Pawe\u0142", ""], ["Casanueva", "I\u00f1igo", ""], ["Coope", "Sam", ""], ["Gerz", "Daniela", ""], ["Kumar", "Girish", ""], ["Mrk\u0161i\u0107", "Nikola", ""], ["Spithourakis", "Georgios", ""], ["Su", "Pei-Hao", ""], ["Vuli\u0107", "Ivan", ""], ["Wen", "Tsung-Hsien", ""]]}, {"id": "1904.06475", "submitter": "Bo Chen", "authors": "Bo Chen, Xiaotao Gu, Yufeng Hu, Siliang Tang, Guoping Hu, Yueting\n  Zhuang, Xiang Ren", "title": "Improving Distantly-supervised Entity Typing with Compact Latent Space\n  Clustering", "comments": "accepted by NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, distant supervision has gained great success on Fine-grained Entity\nTyping (FET). Despite its efficiency in reducing manual labeling efforts, it\nalso brings the challenge of dealing with false entity type labels, as distant\nsupervision assigns labels in a context agnostic manner. Existing works\nalleviated this issue with partial-label loss, but usually suffer from\nconfirmation bias, which means the classifier fit a pseudo data distribution\ngiven by itself. In this work, we propose to regularize distantly supervised\nmodels with Compact Latent Space Clustering (CLSC) to bypass this problem and\neffectively utilize noisy data yet. Our proposed method first dynamically\nconstructs a similarity graph of different entity mentions; infer the labels of\nnoisy instances via label propagation. Based on the inferred labels, mention\nembeddings are updated accordingly to encourage entity mentions with close\nsemantics to form a compact cluster in the embedding space,thus leading to\nbetter classification performance. Extensive experiments on standard benchmarks\nshow that our CLSC model consistently outperforms state-of-the-art distantly\nsupervised entity typing systems by a significant margin.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 03:52:56 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Chen", "Bo", ""], ["Gu", "Xiaotao", ""], ["Hu", "Yufeng", ""], ["Tang", "Siliang", ""], ["Hu", "Guoping", ""], ["Zhuang", "Yueting", ""], ["Ren", "Xiang", ""]]}, {"id": "1904.06478", "submitter": "Takuya Yoshioka", "authors": "Takuya Yoshioka, Zhuo Chen, Changliang Liu, Xiong Xiao, Hakan Erdogan,\n  Dimitrios Dimitriadis", "title": "Low-Latency Speaker-Independent Continuous Speech Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker independent continuous speech separation (SI-CSS) is a task of\nconverting a continuous audio stream, which may contain overlapping voices of\nunknown speakers, into a fixed number of continuous signals each of which\ncontains no overlapping speech segment. A separated, or cleaned, version of\neach utterance is generated from one of SI-CSS's output channels\nnondeterministically without being split up and distributed to multiple\nchannels. A typical application scenario is transcribing multi-party\nconversations, such as meetings, recorded with microphone arrays. The output\nsignals can be simply sent to a speech recognition engine because they do not\ninclude speech overlaps. The previous SI-CSS method uses a neural network\ntrained with permutation invariant training and a data-driven beamformer and\nthus requires much processing latency. This paper proposes a low-latency SI-CSS\nmethod whose performance is comparable to that of the previous method in a\nmicrophone array-based meeting transcription task.This is achieved (1) by using\na new speech separation network architecture combined with a double buffering\nscheme and (2) by performing enhancement with a set of fixed beamformers\nfollowed by a neural post-filter.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 04:24:12 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Yoshioka", "Takuya", ""], ["Chen", "Zhuo", ""], ["Liu", "Changliang", ""], ["Xiao", "Xiong", ""], ["Erdogan", "Hakan", ""], ["Dimitriadis", "Dimitrios", ""]]}, {"id": "1904.06508", "submitter": "Tao Tu", "authors": "Tao Tu, Yuan-Jui Chen, Cheng-chieh Yeh, Hung-yi Lee", "title": "End-to-end Text-to-speech for Low-resource Languages by Cross-Lingual\n  Transfer Learning", "comments": "Accepted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end text-to-speech (TTS) has shown great success on large quantities\nof paired text plus speech data. However, laborious data collection remains\ndifficult for at least 95% of the languages over the world, which hinders the\ndevelopment of TTS in different languages. In this paper, we aim to build TTS\nsystems for such low-resource (target) languages where only very limited paired\ndata are available. We show such TTS can be effectively constructed by\ntransferring knowledge from a high-resource (source) language. Since the model\ntrained on source language cannot be directly applied to target language due to\ninput space mismatch, we propose a method to learn a mapping between source and\ntarget linguistic symbols. Benefiting from this learned mapping, pronunciation\ninformation can be preserved throughout the transferring procedure. Preliminary\nexperiments show that we only need around 15 minutes of paired data to obtain a\nrelatively good TTS system. Furthermore, analytic studies demonstrated that the\nautomatically discovered mapping correlate well with the phonetic expertise.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 08:51:11 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 07:43:40 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Tu", "Tao", ""], ["Chen", "Yuan-Jui", ""], ["Yeh", "Cheng-chieh", ""], ["Lee", "Hung-yi", ""]]}, {"id": "1904.06618", "submitter": "Md Kamrul Hasan", "authors": "Md Kamrul Hasan, Wasifur Rahman, Amir Zadeh, Jianyuan Zhong, Md\n  Iftekhar Tanveer, Louis-Philippe Morency, Mohammed (Ehsan) Hoque", "title": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor", "comments": null, "journal-ref": "EMNLP-IJCNLP, 2019, 2046-2056", "doi": "10.18653/v1/D19-1211", "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Humor is a unique and creative communicative behavior displayed during social\ninteractions. It is produced in a multimodal manner, through the usage of words\n(text), gestures (vision) and prosodic cues (acoustic). Understanding humor\nfrom these three modalities falls within boundaries of multimodal language; a\nrecent research trend in natural language processing that models natural\nlanguage as it happens in face-to-face communication. Although humor detection\nis an established research area in NLP, in a multimodal context it is an\nunderstudied area. This paper presents a diverse multimodal dataset, called\nUR-FUNNY, to open the door to understanding multimodal language used in\nexpressing humor. The dataset and accompanying studies, present a framework in\nmultimodal humor detection for the natural language processing community.\nUR-FUNNY is publicly available for research.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 03:15:38 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Hasan", "Md Kamrul", "", "Ehsan"], ["Rahman", "Wasifur", "", "Ehsan"], ["Zadeh", "Amir", "", "Ehsan"], ["Zhong", "Jianyuan", "", "Ehsan"], ["Tanveer", "Md Iftekhar", "", "Ehsan"], ["Morency", "Louis-Philippe", "", "Ehsan"], ["Mohammed", "", "", "Ehsan"], ["Hoque", "", ""]]}, {"id": "1904.06652", "submitter": "Jimmy Lin", "authors": "Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin", "title": "Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a simple combination of passage retrieval using off-the-shelf IR\ntechniques and a BERT reader was found to be very effective for question\nanswering directly on Wikipedia, yielding a large improvement over the previous\nstate of the art on a standard benchmark dataset. In this paper, we present a\ndata augmentation technique using distant supervision that exploits positive as\nwell as negative examples. We apply a stage-wise approach to fine tuning BERT\non multiple datasets, starting with data that is \"furthest\" from the test data\nand ending with the \"closest\". Experimental results show large gains in\neffectiveness over previous approaches on English QA datasets, and we establish\nnew baselines on two recent Chinese QA datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 08:17:06 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Yang", "Wei", ""], ["Xie", "Yuqing", ""], ["Tan", "Luchen", ""], ["Xiong", "Kun", ""], ["Li", "Ming", ""], ["Lin", "Jimmy", ""]]}, {"id": "1904.06682", "submitter": "Elisa Ferracane", "authors": "Elisa Ferracane, Titan Page, Junyi Jessy Li, Katrin Erk", "title": "From News to Medical: Cross-domain Discourse Segmentation", "comments": "NAACL DISRPT Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first step in discourse analysis involves dividing a text into segments.\nWe annotate the first high-quality small-scale medical corpus in English with\ndiscourse segments and analyze how well news-trained segmenters perform on this\ndomain. While we expectedly find a drop in performance, the nature of the\nsegmentation errors suggests some problems can be addressed earlier in the\npipeline, while others would require expanding the corpus to a trainable size\nto learn the nuances of the medical domain.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 11:52:40 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Ferracane", "Elisa", ""], ["Page", "Titan", ""], ["Li", "Junyi Jessy", ""], ["Erk", "Katrin", ""]]}, {"id": "1904.06707", "submitter": "Timo Schick", "authors": "Timo Schick, Hinrich Sch\\\"utze", "title": "Rare Words: A Major Problem for Contextualized Embeddings And How to Fix\n  it by Attentive Mimicking", "comments": "To appear at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining deep neural network architectures with a language modeling\nobjective has brought large improvements for many natural language processing\ntasks. Exemplified by BERT, a recently proposed such architecture, we\ndemonstrate that despite being trained on huge amounts of data, deep language\nmodels still struggle to understand rare words. To fix this problem, we adapt\nAttentive Mimicking, a method that was designed to explicitly learn embeddings\nfor rare words, to deep language models. In order to make this possible, we\nintroduce one-token approximation, a procedure that enables us to use Attentive\nMimicking even when the underlying language model uses subword-based\ntokenization, i.e., it does not assign embeddings to all words. To evaluate our\nmethod, we create a novel dataset that tests the ability of language models to\ncapture semantic properties of words without any task-specific fine-tuning.\nUsing this dataset, we show that adding our adapted version of Attentive\nMimicking to BERT does indeed substantially improve its understanding of rare\nwords.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 15:26:52 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 11:24:37 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 07:46:56 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 14:26:08 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Schick", "Timo", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1904.06725", "submitter": "Saurav Manchanda", "authors": "Saurav Manchanda and George Karypis", "title": "Distributed representation of multi-sense words: A loss-driven approach", "comments": "PAKDD 2018 Best paper award runner-up", "journal-ref": "Advances in Knowledge Discovery and Data Mining. PAKDD 2018.\n  Lecture Notes in Computer Science, vol 10938. Springer, Cham", "doi": "10.1007/978-3-319-93037-4_27", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2Vec's Skip Gram model is the current state-of-the-art approach for\nestimating the distributed representation of words. However, it assumes a\nsingle vector per word, which is not well-suited for representing words that\nhave multiple senses. This work presents LDMI, a new model for estimating\ndistributional representations of words. LDMI relies on the idea that, if a\nword carries multiple senses, then having a different representation for each\nof its senses should lead to a lower loss associated with predicting its\nco-occurring words, as opposed to the case when a single vector representation\nis used for all the senses. After identifying the multi-sense words, LDMI\nclusters the occurrences of these words to assign a sense to each occurrence.\nExperiments on the contextual word similarity task show that LDMI leads to\nbetter performance than competing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 17:01:26 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Manchanda", "Saurav", ""], ["Karypis", "George", ""]]}, {"id": "1904.06730", "submitter": "Saurav Manchanda", "authors": "Saurav Manchanda and George Karypis", "title": "Text segmentation on multilabel documents: A distant-supervised approach", "comments": "Accepted in 2018 IEEE International Conference on Data Mining (ICDM)", "journal-ref": "2018 IEEE International Conference on Data Mining (ICDM),\n  1170-1175", "doi": "10.1109/ICDM.2018.00154", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting text into semantically coherent segments is an important task with\napplications in information retrieval and text summarization. Developing\naccurate topical segmentation requires the availability of training data with\nground truth information at the segment level. However, generating such labeled\ndatasets, especially for applications in which the meaning of the labels is\nuser-defined, is expensive and time-consuming. In this paper, we develop an\napproach that instead of using segment-level ground truth information, it\ninstead uses the set of labels that are associated with a document and are\neasier to obtain as the training data essentially corresponds to a multilabel\ndataset. Our method, which can be thought of as an instance of distant\nsupervision, improves upon the previous approaches by exploiting the fact that\nconsecutive sentences in a document tend to talk about the same topic, and\nhence, probably belong to the same class. Experiments on the text segmentation\ntask on a variety of datasets show that the segmentation produced by our method\nbeats the competing approaches on four out of five datasets and performs at par\non the fifth dataset. On the multilabel text classification task, our method\nperforms at par with the competing approaches, while requiring significantly\nless time to estimate than the competing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 17:32:44 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Manchanda", "Saurav", ""], ["Karypis", "George", ""]]}, {"id": "1904.06779", "submitter": "Walid Saba", "authors": "Walid S. Saba", "title": "No Adjective Ordering Mystery, and No Raven Paradox, Just an Ontological\n  Mishap", "comments": "3 pages (extended abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the concluding remarks of Ontological Promiscuity Hobbs (1985) made what\nwe believe to be a very insightful observation: given that semantics is an\nattempt at specifying the relation between language and the world, if \"one can\nassume a theory of the world that is isomorphic to the way we talk about it ...\nthen semantics becomes nearly trivial\". But how exactly can we rectify our\nlogical formalisms so that semantics, an endeavor that has occupied the most\npenetrating minds for over two centuries, can become (nearly) trivial, and what\nexactly does it mean to assume a theory of the world in our semantics? In this\npaper we hope to provide answers for both questions. First, we believe that a\ncommonsense theory of the world can (and should) be embedded in our semantic\nformalisms resulting in a logical semantics grounded in commonsense\nmetaphysics. Moreover, we believe the first step to accomplishing this vision\nis rectifying what we think was a crucial oversight in logical semantics,\nnamely the failure to distinguish between two fundamentally different types of\nconcepts: (i) ontological concepts, that correspond to what Cocchiarella (2001)\ncalls first-intension concepts and are types in a strongly-typed ontology; and\n(ii) logical concepts (or second intension concepts), that are predicates\ncorresponding to properties of (and relations between) objects of various\nontological types1. In such a framework, which we will refer to henceforth by\nontologik, it will be shown how type unification and other type operations can\nbe used to account for the `missing text phenomenon' (MTP) (see Saba, 2019a)\nthat is at the heart of most challenges in the semantics of natural language,\nby uncovering the significant amount of missing text that is never explicitly\nstated in everyday discourse, but is often implicitly assumed as shared\nbackground knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 23:20:34 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Saba", "Walid S.", ""]]}, {"id": "1904.06828", "submitter": "He He", "authors": "He He and Nanyun Peng and Percy Liang", "title": "Pun Generation with Surprise", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of generating a pun sentence given a pair of homophones\n(e.g., \"died\" and \"dyed\"). Supervised text generation is inappropriate due to\nthe lack of a large corpus of puns, and even if such a corpus existed, mimicry\nis at odds with generating novel content. In this paper, we propose an\nunsupervised approach to pun generation using a corpus of unhumorous text and\nwhat we call the local-global surprisal principle: we posit that in a pun\nsentence, there is a strong association between the pun word (e.g., \"dyed\") and\nthe distant context, as well as a strong association between the alternative\nword (e.g., \"died\") and the immediate context. This contrast creates surprise\nand thus humor. We instantiate this principle for pun generation in two ways:\n(i) as a measure based on the ratio of probabilities under a language model,\nand (ii) a retrieve-and-edit approach based on words suggested by a skip-gram\nmodel. Human evaluation shows that our retrieve-and-edit approach generates\npuns successfully 31% of the time, tripling the success rate of a neural\ngeneration baseline.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 03:40:16 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["He", "He", ""], ["Peng", "Nanyun", ""], ["Liang", "Percy", ""]]}, {"id": "1904.06834", "submitter": "Kartik Goyal", "authors": "Kartik Goyal, Chris Dyer and Taylor Berg-Kirkpatrick", "title": "An Empirical Investigation of Global and Local Normalization for\n  Recurrent Neural Sequence Models Using a Continuous Relaxation to Beam Search", "comments": "Long paper at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Globally normalized neural sequence models are considered superior to their\nlocally normalized equivalents because they may ameliorate the effects of label\nbias. However, when considering high-capacity neural parametrizations that\ncondition on the whole input sequence, both model classes are theoretically\nequivalent in terms of the distributions they are capable of representing.\nThus, the practical advantage of global normalization in the context of modern\nneural methods remains unclear. In this paper, we attempt to shed light on this\nproblem through an empirical study. We extend an approach for search-aware\ntraining via a continuous relaxation of beam search (Goyal et al., 2017b) in\norder to enable training of globally normalized recurrent sequence models\nthrough simple backpropagation. We then use this technique to conduct an\nempirical study of the interaction between global normalization, high-capacity\nencoders, and search-aware optimization. We observe that in the context of\ninexact search, globally normalized neural models are still more effective than\ntheir locally normalized counterparts. Further, since our training approach is\nsensitive to warm-starting with pre-trained models, we also propose a novel\ninitialization strategy based on self-normalization for pre-training globally\nnormalized models. We perform analysis of our approach on two tasks: CCG\nsupertagging and Machine Translation, and demonstrate the importance of global\nnormalization under different conditions while using search-aware training.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 04:17:13 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Goyal", "Kartik", ""], ["Dyer", "Chris", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1904.06861", "submitter": "Junlong Gao", "authors": "Junlong Gao, Shiqi Wang, Shanshe Wang, Siwei Ma, Wen Gao", "title": "Self-critical n-step Training for Image Captioning", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for image captioning are usually trained by cross entropy\nloss, which leads to exposure bias and the inconsistency between the optimizing\nfunction and evaluation metrics. Recently it has been shown that these two\nissues can be addressed by incorporating techniques from reinforcement\nlearning, where one of the popular techniques is the advantage actor-critic\nalgorithm that calculates per-token advantage by estimating state value with a\nparametrized estimator at the cost of introducing estimation bias. In this\npaper, we estimate state value without using a parametrized value estimator.\nWith the properties of image captioning, namely, the deterministic state\ntransition function and the sparse reward, state value is equivalent to its\npreceding state-action value, and we reformulate advantage function by simply\nreplacing the former with the latter. Moreover, the reformulated advantage is\nextended to n-step, which can generally increase the absolute value of the mean\nof reformulated advantage while lowering variance. Then two kinds of rollout\nare adopted to estimate state-action value, which we call self-critical n-step\ntraining. Empirically we find that our method can obtain better performance\ncompared to the state-of-the-art methods that use the sequence level advantage\nand parametrized estimator respectively on the widely used MSCOCO benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:47:23 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Gao", "Junlong", ""], ["Wang", "Shiqi", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "1904.06941", "submitter": "Lewis Mitchell", "authors": "Vanessa Glenny, Jonathan Tuke, Nigel Bean, Lewis Mitchell", "title": "A framework for streamlined statistical prediction using topic models", "comments": "Proceedings of the 2019 Joint SIGHUM Workshop on Computational\n  Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature\n  (LaTeCH-CLfL `19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Humanities and Social Sciences, there is increasing interest in\napproaches to information extraction, prediction, intelligent linkage, and\ndimension reduction applicable to large text corpora. With approaches in these\nfields being grounded in traditional statistical techniques, the need arises\nfor frameworks whereby advanced NLP techniques such as topic modelling may be\nincorporated within classical methodologies. This paper provides a classical,\nsupervised, statistical learning framework for prediction from text, using\ntopic models as a data reduction method and the topics themselves as\npredictors, alongside typical statistical tools for predictive modelling. We\napply this framework in a Social Sciences context (applied animal behaviour) as\nwell as a Humanities context (narrative analysis) as examples of this\nframework. The results show that topic regression models perform comparably to\ntheir much less efficient equivalents that use individual words as predictors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 10:06:47 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Glenny", "Vanessa", ""], ["Tuke", "Jonathan", ""], ["Bean", "Nigel", ""], ["Mitchell", "Lewis", ""]]}, {"id": "1904.07078", "submitter": "Herman Kamper", "authors": "Herman Kamper, Aristotelis Anastassiou, Karen Livescu", "title": "Semantic query-by-example speech search using visual grounding", "comments": "Accepted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent studies have started to investigate how speech systems can\nbe trained on untranscribed speech by leveraging accompanying images at\ntraining time. Examples of tasks include keyword prediction and within- and\nacross-mode retrieval. Here we consider how such models can be used for\nquery-by-example (QbE) search, the task of retrieving utterances relevant to a\ngiven spoken query. We are particularly interested in semantic QbE, where the\ntask is not only to retrieve utterances containing exact instances of the\nquery, but also utterances whose meaning is relevant to the query. We follow a\nsegmental QbE approach where variable-duration speech segments (queries, search\nutterances) are mapped to fixed-dimensional embedding vectors. We show that a\nQbE system using an embedding function trained on visually grounded speech data\noutperforms a purely acoustic QbE system in terms of both exact and semantic\nretrieval performance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:35:40 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kamper", "Herman", ""], ["Anastassiou", "Aristotelis", ""], ["Livescu", "Karen", ""]]}, {"id": "1904.07094", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Andrew Yates, Arman Cohan, Nazli Goharian", "title": "CEDR: Contextualized Embeddings for Document Ranking", "comments": "Appeared in SIGIR 2019, 4 pages", "journal-ref": null, "doi": "10.1145/3331184.3331317", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although considerable attention has been given to neural ranking\narchitectures recently, far less attention has been paid to the term\nrepresentations that are used as input to these models. In this work, we\ninvestigate how two pretrained contextualized language models (ELMo and BERT)\ncan be utilized for ad-hoc document ranking. Through experiments on TREC\nbenchmarks, we find that several existing neural ranking architectures can\nbenefit from the additional context provided by contextualized language models.\nFurthermore, we propose a joint approach that incorporates BERT's\nclassification vector into existing neural models and show that it outperforms\nstate-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR\n(Contextualized Embeddings for Document Ranking). We also address practical\nchallenges in using these models for ranking, including the maximum input\nlength imposed by BERT and runtime performance impacts of contextualized\nlanguage models.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:55:59 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 21:16:59 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 15:03:22 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["MacAvaney", "Sean", ""], ["Yates", "Andrew", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "1904.07142", "submitter": "Sebastian Gehrmann", "authors": "Sebastian Gehrmann and Steven Layne and Franck Dernoncourt", "title": "Improving Human Text Comprehension through Semi-Markov CRF-based Neural\n  Section Title Generation", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Titles of short sections within long documents support readers by guiding\ntheir focus towards relevant passages and by providing anchor-points that help\nto understand the progression of the document. The positive effects of section\ntitles are even more pronounced when measured on readers with less developed\nreading abilities, for example in communities with limited labeled text\nresources.\n  We, therefore, aim to develop techniques to generate section titles in\nlow-resource environments. In particular, we present an extractive pipeline for\nsection title generation by first selecting the most salient sentence and then\napplying deletion-based compression. Our compression approach is based on a\nSemi-Markov Conditional Random Field that leverages unsupervised\nword-representations such as ELMo or BERT, eliminating the need for a complex\nencoder-decoder architecture. The results show that this approach leads to\ncompetitive performance with sequence-to-sequence models with high resources,\nwhile strongly outperforming it with low resources. In a human-subject study\nacross subjects with varying reading abilities, we find that our section titles\nimprove the speed of completing comprehension tasks while retaining similar\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 15:51:15 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Gehrmann", "Sebastian", ""], ["Layne", "Steven", ""], ["Dernoncourt", "Franck", ""]]}, {"id": "1904.07148", "submitter": "Issam Damaj", "authors": "Issam Damaj (1), Mahmoud Imdoukh (1), Rached Zantout (2) ((1) American\n  University of Kuwait, (2) Rafik Hariri University)", "title": "Parallel Hardware for Faster Morphological Analysis", "comments": "16 pages, 19 figures, 7 tables", "journal-ref": "Computer and Information Sciences, Elsevier. 30(2018) 531-546", "doi": "10.1016/j.jksuci.2017.07.003", "report-no": null, "categories": "cs.DC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological analysis in the Arabic language is computationally intensive,\nhas numerous forms and rules, and is intrinsically parallel. The investigation\npresented in this paper confirms that the effective development of parallel\nalgorithms and the derivation of corresponding processors in hardware enable\nimplementations with appealing performance characteristics. The presented\ndevelopments of parallel hardware comprise the application of a variety of\nalgorithm modelling techniques, strategies for concurrent processing, and the\ncreation of pioneering hardware implementations that target modern programmable\ndevices. The investigation includes the creation of a linguistic-based stemmer\nfor Arabic verb root extraction with extended infix processing to attain\nhigh-levels of accuracy. The implementations comprise three versions, namely,\nsoftware, non-pipelined processor, and pipelined processor with high\nthroughput. The targeted systems are high-performance multi-core processors for\nsoftware implementations and high-end Field Programmable Gate Array systems for\nhardware implementations. The investigation includes a thorough evaluation of\nthe methodology, and performance and accuracy analyses of the developed\nsoftware and hardware implementations. The pipelined processor achieved a\nsignificant speedup of 5571.4 over the software implementation. The developed\nstemmer for verb root extraction with infix processing attained accuracies of\n87% and 90.7% for analyzing the texts of the Holy Quran and its Chapter 29 -\nSurat Al-Ankabut.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 23:46:52 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Damaj", "Issam", ""], ["Imdoukh", "Mahmoud", ""], ["Zantout", "Rached", ""]]}, {"id": "1904.07209", "submitter": "Matthias Sperber", "authors": "Matthias Sperber, Graham Neubig, Jan Niehues, Alex Waibel", "title": "Attention-Passing Models for Robust and Data-Efficient End-to-End Speech\n  Translation", "comments": "Authors' final version, accepted at TACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech translation has traditionally been approached through cascaded models\nconsisting of a speech recognizer trained on a corpus of transcribed speech,\nand a machine translation system trained on parallel texts. Several recent\nworks have shown the feasibility of collapsing the cascade into a single,\ndirect model that can be trained in an end-to-end fashion on a corpus of\ntranslated speech. However, experiments are inconclusive on whether the cascade\nor the direct model is stronger, and have only been conducted under the\nunrealistic assumption that both are trained on equal amounts of data, ignoring\nother available speech recognition and machine translation corpora.\n  In this paper, we demonstrate that direct speech translation models require\nmore data to perform well than cascaded models, and while they allow including\nauxiliary data through multi-task training, they are poor at exploiting such\ndata, putting them at a severe disadvantage. As a remedy, we propose the use of\nend-to-end trainable models with two attention mechanisms, the first\nestablishing source speech to source text alignments, the second modeling\nsource to target text alignment. We show that such models naturally decompose\ninto multi-task-trainable recognition and translation tasks and propose an\nattention-passing technique that alleviates error propagation issues in a\nprevious formulation of a model with two attention stages. Our proposed model\noutperforms all examined baselines and is able to exploit auxiliary training\ndata much more effectively than direct attentional models.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:33:38 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Sperber", "Matthias", ""], ["Neubig", "Graham", ""], ["Niehues", "Jan", ""], ["Waibel", "Alex", ""]]}, {"id": "1904.07293", "submitter": "Md. Akmal Haidar", "authors": "Md. Akmal Haidar, Mehdi Rezagholizadeh, Alan Do-Omri, Ahmad Rashid", "title": "Latent Code and Text-based Generative Adversarial Networks for Soft-text\n  Generation", "comments": null, "journal-ref": "2019 Annual Conference of the North American Chapter of the\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generation with generative adversarial networks (GANs) can be divided\ninto the text-based and code-based categories according to the type of signals\nused for discrimination. In this work, we introduce a novel text-based approach\ncalled Soft-GAN to effectively exploit GAN setup for text generation. We\ndemonstrate how autoencoders (AEs) can be used for providing a continuous\nrepresentation of sentences, which we will refer to as soft-text. This soft\nrepresentation will be used in GAN discrimination to synthesize similar\nsoft-texts. We also propose hybrid latent code and text-based GAN (LATEXT-GAN)\napproaches with one or more discriminators, in which a combination of the\nlatent code and the soft-text is used for GAN discriminations. We perform a\nnumber of subjective and objective experiments on two well-known datasets (SNLI\nand Image COCO) to validate our techniques. We discuss the results using\nseveral evaluation metrics and show that the proposed techniques outperform the\ntraditional GAN-based text-generation methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 19:07:49 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 15:05:41 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Haidar", "Md. Akmal", ""], ["Rezagholizadeh", "Mehdi", ""], ["Do-Omri", "Alan", ""], ["Rashid", "Ahmad", ""]]}, {"id": "1904.07307", "submitter": "Alexander W. Wong", "authors": "Alexander William Wong, Ken Wong, Abram Hindle", "title": "Tracing Forum Posts to MOOC Content using Topic Analysis", "comments": "6 pages, 4 figures, Course project for UofA CMPUT 660, Winter 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive Open Online Courses are educational programs that are open and\naccessible to a large number of people through the internet. To facilitate\nlearning, MOOC discussion forums exist where students and instructors\ncommunicate questions, answers, and thoughts related to the course.\n  The primary objective of this paper is to investigate tracing discussion\nforum posts back to course lecture videos and readings using topic analysis. We\nutilize both unsupervised and supervised variants of Latent Dirichlet\nAllocation (LDA) to extract topics from course material and classify forum\nposts. We validate our approach on posts bootstrapped from five Coursera\ncourses and determine that topic models can be used to map student discussion\nposts back to the underlying course lecture or reading. Labeled LDA outperforms\nunsupervised Hierarchical Dirichlet Process LDA and base LDA for our\ntraceability task. This research is useful as it provides an automated approach\nfor clustering student discussions by course material, enabling instructors to\nquickly evaluate student misunderstanding of content and clarify materials\naccordingly.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 19:49:06 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Wong", "Alexander William", ""], ["Wong", "Ken", ""], ["Hindle", "Abram", ""]]}, {"id": "1904.07318", "submitter": "David Schlangen", "authors": "David Schlangen", "title": "Natural Language Semantics With Pictures: Some Language & Vision\n  Datasets and Potential Uses for Computational Semantics", "comments": "Presented at the 13th International Conference on Computational\n  Semantics (IWCS 2019), Gothenburg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propelling, and propelled by, the \"deep learning revolution\", recent years\nhave seen the introduction of ever larger corpora of images annotated with\nnatural language expressions. We survey some of these corpora, taking a\nperspective that reverses the usual directionality, as it were, by viewing the\nimages as semantic annotation of the natural language expressions. We discuss\ndatasets that can be derived from the corpora, and tasks of potential interest\nfor computational semanticists that can be defined on those. In this, we make\nuse of relations provided by the corpora (namely, the link between expression\nand image, and that between two expressions linked to the same image) and\nrelations that we can add (similarity relations between expressions, or between\nimages). Specifically, we show that in this way we can create data that can be\nused to learn and evaluate lexical and compositional grounded semantics, and we\nshow that the \"linked to same image\" relation tracks a semantic implication\nrelation that is recognisable to annotators even in the absence of the linking\nimage as evidence. Finally, as an example of possible benefits of this\napproach, we show that an exemplar-model-based approach to implication beats a\n(simple) distributional space-based one on some derived datasets, while lending\nitself to explainability.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 20:15:46 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Schlangen", "David", ""]]}, {"id": "1904.07334", "submitter": "Mamoru Komachi", "authors": "Masahiro Kaneko and Mamoru Komachi", "title": "Multi-Head Multi-Layer Attention to Deep Language Representations for\n  Grammatical Error Detection", "comments": "12 pages; CICLing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is known that a deep neural network model pre-trained with large-scale\ndata greatly improves the accuracy of various tasks, especially when there are\nresource constraints. However, the information needed to solve a given task can\nvary, and simply using the output of the final layer is not necessarily\nsufficient. Moreover, to our knowledge, exploiting large language\nrepresentation models to detect grammatical errors has not yet been studied. In\nthis work, we investigate the effect of utilizing information not only from the\nfinal layer but also from intermediate layers of a pre-trained language\nrepresentation model to detect grammatical errors. We propose a multi-head\nmulti-layer attention model that determines the appropriate layers in\nBidirectional Encoder Representation from Transformers (BERT). The proposed\nmethod achieved the best scores on three datasets for grammatical error\ndetection tasks, outperforming the current state-of-the-art method by 6.0\npoints on FCE, 8.2 points on CoNLL14, and 12.2 points on JFLEG in terms of\nF_0.5. We also demonstrate that by using multi-head multi-layer attention, our\nmodel can exploit a broader range of information for each token in a sentence\nthan a model that uses only the final layer's information.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 21:36:21 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Kaneko", "Masahiro", ""], ["Komachi", "Mamoru", ""]]}, {"id": "1904.07342", "submitter": "Allison Koenecke", "authors": "Allison Koenecke and Jordi Feliu-Fab\\`a", "title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled\n  Data", "comments": null, "journal-ref": null, "doi": "10.36190/2020.22", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While it is well-documented that climate change accepters and deniers have\nbecome increasingly polarized in the United States over time, there has been no\nlarge-scale examination of whether these individuals are prone to changing\ntheir opinions as a result of natural external occurrences. On the\nsub-population of Twitter users, we examine whether climate change sentiment\nchanges in response to five separate natural disasters occurring in the U.S. in\n2018. We begin by showing that relevant tweets can be classified with over 75%\naccuracy as either accepting or denying climate change when using our\nmethodology to compensate for limited labeled data; results are robust across\nseveral machine learning models and yield geographic-level results in line with\nprior research. We then apply RNNs to conduct a cohort-level analysis showing\nthat the 2018 hurricanes yielded a statistically significant increase in\naverage tweet sentiment affirming climate change. However, this effect does not\nhold for the 2018 blizzard and wildfires studied, implying that Twitter users'\nopinions on climate change are fairly ingrained on this subset of natural\ndisasters.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 21:51:21 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Koenecke", "Allison", ""], ["Feliu-Fab\u00e0", "Jordi", ""]]}, {"id": "1904.07372", "submitter": "Jack Hessel", "authors": "Jack Hessel, Lillian Lee", "title": "Something's Brewing! Early Prediction of Controversy-causing Posts from\n  Discussion Features", "comments": "Accepted at NAACL 2019 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controversial posts are those that split the preferences of a community,\nreceiving both significant positive and significant negative feedback. Our\ninclusion of the word \"community\" here is deliberate: what is controversial to\nsome audiences may not be so to others. Using data from several different\ncommunities on reddit.com, we predict the ultimate controversiality of posts,\nleveraging features drawn from both the textual content and the tree structure\nof the early comments that initiate the discussion. We find that even when only\na handful of comments are available, e.g., the first 5 comments made within 15\nminutes of the original post, discussion features often add predictive capacity\nto strong content-and-rate only baselines. Additional experiments on domain\ntransfer suggest that conversation-structure features often generalize to other\ncommunities better than conversation-content features do.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 23:56:25 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Hessel", "Jack", ""], ["Lee", "Lillian", ""]]}, {"id": "1904.07386", "submitter": "Kong Aik Lee", "authors": "Kong Aik Lee, Ville Hautamaki, Tomi Kinnunen, Hitoshi Yamamoto, Koji\n  Okabe, Ville Vestman, Jing Huang, Guohong Ding, Hanwu Sun, Anthony Larcher,\n  Rohan Kumar Das, Haizhou Li, Mickael Rouvier, Pierre-Michel Bousquet, Wei\n  Rao, Qing Wang, Chunlei Zhang, Fahimeh Bahmaninezhad, Hector Delgado, Jose\n  Patino, Qiongqiong Wang, Ling Guo, Takafumi Koshinaka, Jiacen Zhang, Koichi\n  Shinoda, Trung Ngo Trong, Md Sahidullah, Fan Lu, Yun Tang, Ming Tu, Kah Kuan\n  Teh, Huy Dat Tran, Kuruvachan K. George, Ivan Kukanov, Florent Desnous,\n  Jichen Yang, Emre Yilmaz, Longting Xu, Jean-Francois Bonastre, Chenglin Xu,\n  Zhi Hao Lim, Eng Siong Chng, Shivesh Ranjan, John H.L. Hansen, Massimiliano\n  Todisco, and Nicholas Evans", "title": "I4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared\n  Experiences", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The I4U consortium was established to facilitate a joint entry to NIST\nspeaker recognition evaluations (SRE). The latest edition of such joint\nsubmission was in SRE 2018, in which the I4U submission was among the\nbest-performing systems. SRE'18 also marks the 10-year anniversary of I4U\nconsortium into NIST SRE series of evaluation. The primary objective of the\ncurrent paper is to summarize the results and lessons learned based on the\ntwelve sub-systems and their fusion submitted to SRE'18. It is also our\nintention to present a shared view on the advancements, progresses, and major\nparadigm shifts that we have witnessed as an SRE participant in the past decade\nfrom SRE'08 to SRE'18. In this regard, we have seen, among others, a paradigm\nshift from supervector representation to deep speaker embedding, and a switch\nof research challenge from channel compensation to domain adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 00:55:20 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Lee", "Kong Aik", ""], ["Hautamaki", "Ville", ""], ["Kinnunen", "Tomi", ""], ["Yamamoto", "Hitoshi", ""], ["Okabe", "Koji", ""], ["Vestman", "Ville", ""], ["Huang", "Jing", ""], ["Ding", "Guohong", ""], ["Sun", "Hanwu", ""], ["Larcher", "Anthony", ""], ["Das", "Rohan Kumar", ""], ["Li", "Haizhou", ""], ["Rouvier", "Mickael", ""], ["Bousquet", "Pierre-Michel", ""], ["Rao", "Wei", ""], ["Wang", "Qing", ""], ["Zhang", "Chunlei", ""], ["Bahmaninezhad", "Fahimeh", ""], ["Delgado", "Hector", ""], ["Patino", "Jose", ""], ["Wang", "Qiongqiong", ""], ["Guo", "Ling", ""], ["Koshinaka", "Takafumi", ""], ["Zhang", "Jiacen", ""], ["Shinoda", "Koichi", ""], ["Trong", "Trung Ngo", ""], ["Sahidullah", "Md", ""], ["Lu", "Fan", ""], ["Tang", "Yun", ""], ["Tu", "Ming", ""], ["Teh", "Kah Kuan", ""], ["Tran", "Huy Dat", ""], ["George", "Kuruvachan K.", ""], ["Kukanov", "Ivan", ""], ["Desnous", "Florent", ""], ["Yang", "Jichen", ""], ["Yilmaz", "Emre", ""], ["Xu", "Longting", ""], ["Bonastre", "Jean-Francois", ""], ["Xu", "Chenglin", ""], ["Lim", "Zhi Hao", ""], ["Chng", "Eng Siong", ""], ["Ranjan", "Shivesh", ""], ["Hansen", "John H. L.", ""], ["Todisco", "Massimiliano", ""], ["Evans", "Nicholas", ""]]}, {"id": "1904.07391", "submitter": "Rajarshi Bhowmik", "authors": "Rajarshi Bhowmik and Gerard de Melo", "title": "Be Concise and Precise: Synthesizing Open-Domain Entity Descriptions\n  from Facts", "comments": null, "journal-ref": null, "doi": "10.1145/3308558.3313656", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite being vast repositories of factual information, cross-domain\nknowledge graphs, such as Wikidata and the Google Knowledge Graph, only\nsparsely provide short synoptic descriptions for entities. Such descriptions\nthat briefly identify the most discernible features of an entity provide\nreaders with a near-instantaneous understanding of what kind of entity they are\nbeing presented. They can also aid in tasks such as named entity\ndisambiguation, ontological type determination, and answering entity queries.\nGiven the rapidly increasing numbers of entities in knowledge graphs, a fully\nautomated synthesis of succinct textual descriptions from underlying factual\ninformation is essential. To this end, we propose a novel fact-to-sequence\nencoder-decoder model with a suitable copy mechanism to generate concise and\nprecise textual descriptions of entities. In an in-depth evaluation, we\ndemonstrate that our method significantly outperforms state-of-the-art\nalternatives.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 01:30:00 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Bhowmik", "Rajarshi", ""], ["de Melo", "Gerard", ""]]}, {"id": "1904.07418", "submitter": "Sho Takase", "authors": "Sho Takase, Naoaki Okazaki", "title": "Positional Encoding to Control Output Sequence Length", "comments": "Accepted by NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural encoder-decoder models have been successful in natural language\ngeneration tasks. However, real applications of abstractive summarization must\nconsider additional constraint that a generated summary should not exceed a\ndesired length. In this paper, we propose a simple but effective extension of a\nsinusoidal positional encoding (Vaswani et al., 2017) to enable neural\nencoder-decoder model to preserves the length constraint. Unlike in previous\nstudies where that learn embeddings representing each length, the proposed\nmethod can generate a text of any length even if the target length is not\npresent in training data. The experimental results show that the proposed\nmethod can not only control the generation length but also improve the ROUGE\nscores.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 02:48:11 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Takase", "Sho", ""], ["Okazaki", "Naoaki", ""]]}, {"id": "1904.07531", "submitter": "Yifan Qiao", "authors": "Yifan Qiao, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu", "title": "Understanding the Behaviors of BERT in Ranking", "comments": "There is an error in Table 1 and we will update them to correct\n  results. Please refer to MS MARCO Leaderboard for the actually evaluation\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the performances and behaviors of BERT in ranking tasks.\nWe explore several different ways to leverage the pre-trained BERT and\nfine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web\nTrack ad hoc document ranking. Experimental results on MS MARCO demonstrate the\nstrong effectiveness of BERT in question-answering focused passage ranking\ntasks, as well as the fact that BERT is a strong interaction-based seq2seq\nmatching model. Experimental results on TREC show the gaps between the BERT\npre-trained on surrounding contexts and the needs of ad hoc document ranking.\nAnalyses illustrate how BERT allocates its attentions between query-document\ntokens in its Transformer layers, how it prefers semantic matches between\nparaphrase tokens, and how that differs with the soft match patterns learned by\na click-trained neural ranker.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:30:31 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 04:26:14 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 19:39:00 GMT"}, {"version": "v4", "created": "Fri, 26 Apr 2019 12:44:38 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Qiao", "Yifan", ""], ["Xiong", "Chenyan", ""], ["Liu", "Zhenghao", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1904.07535", "submitter": "Shun Zheng", "authors": "Shun Zheng, Wei Cao, Wei Xu, Jiang Bian", "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial\n  Event Extraction", "comments": "Accepted by EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing event extraction (EE) methods merely extract event arguments\nwithin the sentence scope. However, such sentence-level EE methods struggle to\nhandle soaring amounts of documents from emerging applications, such as\nfinance, legislation, health, etc., where event arguments always scatter across\ndifferent sentences, and even multiple such event mentions frequently co-exist\nin the same document. To address these challenges, we propose a novel\nend-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic\ngraph to fulfill the document-level EE (DEE) effectively. Moreover, we\nreformalize a DEE task with the no-trigger-words design to ease the\ndocument-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we\nbuild a large-scale real-world dataset consisting of Chinese financial\nannouncements with the challenges mentioned above. Extensive experiments with\ncomprehensive analyses illustrate the superiority of Doc2EDAG over\nstate-of-the-art methods. Data and codes can be found at\nhttps://github.com/dolphin-zs/Doc2EDAG.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:39:06 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 03:14:54 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Zheng", "Shun", ""], ["Cao", "Wei", ""], ["Xu", "Wei", ""], ["Bian", "Jiang", ""]]}, {"id": "1904.07556", "submitter": "Herman Kamper", "authors": "Ryan Eloff, Andr\\'e Nortje, Benjamin van Niekerk, Avashna Govender,\n  Leanne Nortje, Arnu Pretorius, Elan van Biljon, Ewald van der Westhuizen,\n  Lisa van Staden, Herman Kamper", "title": "Unsupervised acoustic unit discovery for speech synthesis using discrete\n  latent-variable neural networks", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For our submission to the ZeroSpeech 2019 challenge, we apply discrete\nlatent-variable neural networks to unlabelled speech and use the discovered\nunits for speech synthesis. Unsupervised discrete subword modelling could be\nuseful for studies of phonetic category learning in infants or in low-resource\nspeech technology requiring symbolic input. We use an autoencoder (AE)\narchitecture with intermediate discretisation. We decouple acoustic unit\ndiscovery from speaker modelling by conditioning the AE's decoder on the\ntraining speaker identity. At test time, unit discovery is performed on speech\nfrom an unseen speaker, followed by unit decoding conditioned on a known target\nspeaker to obtain reconstructed filterbanks. This output is fed to a neural\nvocoder to synthesise speech in the target speaker's voice. For discretisation,\ncategorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs)\nand straight-through estimation are compared at different compression levels on\ntwo languages. Our final model uses convolutional encoding, VQ-VAE\ndiscretisation, deconvolutional decoding and an FFTNet vocoder. We show that\ndecoupled speaker conditioning intrinsically improves discrete acoustic\nrepresentations, yielding competitive synthesis quality compared to the\nchallenge baseline.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 09:38:01 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 12:04:07 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Eloff", "Ryan", ""], ["Nortje", "Andr\u00e9", ""], ["van Niekerk", "Benjamin", ""], ["Govender", "Avashna", ""], ["Nortje", "Leanne", ""], ["Pretorius", "Arnu", ""], ["van Biljon", "Elan", ""], ["van der Westhuizen", "Ewald", ""], ["van Staden", "Lisa", ""], ["Kamper", "Herman", ""]]}, {"id": "1904.07629", "submitter": "Zhaoning Li", "authors": "Zhaoning Li, Qi Li, Xiaotian Zou, Jiangtao Ren", "title": "Causality Extraction based on Self-Attentive BiLSTM-CRF with Transferred\n  Embeddings", "comments": "39 pages, 11 figures, 6 tables", "journal-ref": "Neurocomputing, Volume 423, 2021, Pages 207-219", "doi": "10.1016/j.neucom.2020.08.078", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality extraction from natural language texts is a challenging open\nproblem in artificial intelligence. Existing methods utilize patterns,\nconstraints, and machine learning techniques to extract causality, heavily\ndepending on domain knowledge and requiring considerable human effort and time\nfor feature engineering. In this paper, we formulate causality extraction as a\nsequence labeling problem based on a novel causality tagging scheme. On this\nbasis, we propose a neural causality extractor with the BiLSTM-CRF model as the\nbackbone, named SCITE (Self-attentive BiLSTM-CRF wIth Transferred Embeddings),\nwhich can directly extract cause and effect without extracting candidate causal\npairs and identifying their relations separately. To address the problem of\ndata insufficiency, we transfer contextual string embeddings, also known as\nFlair embeddings, which are trained on a large corpus in our task. In addition,\nto improve the performance of causality extraction, we introduce a multihead\nself-attention mechanism into SCITE to learn the dependencies between causal\nwords. We evaluate our method on a public dataset, and experimental results\ndemonstrate that our method achieves significant and consistent improvement\ncompared to baselines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 12:54:00 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 19:00:21 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 13:59:57 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 07:14:23 GMT"}, {"version": "v5", "created": "Wed, 17 Jun 2020 16:28:53 GMT"}, {"version": "v6", "created": "Sun, 8 Nov 2020 13:30:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Zhaoning", ""], ["Li", "Qi", ""], ["Zou", "Xiaotian", ""], ["Ren", "Jiangtao", ""]]}, {"id": "1904.07695", "submitter": "Jipeng Qiang", "authors": "Qiang Jipeng and Qian Zhenyu and Li Yun and Yuan Yunhao and Wu Xindong", "title": "Short Text Topic Modeling Techniques, Applications, and Performance: A\n  Survey", "comments": "arXiv admin note: text overlap with arXiv:1808.02215 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing short texts infers discriminative and coherent latent topics that\nis a critical and fundamental task since many real-world applications require\nsemantic understanding of short texts. Traditional long text topic modeling\nalgorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this\nproblem very well since only very limited word co-occurrence information is\navailable in short texts. Therefore, short text topic modeling has already\nattracted much attention from the machine learning research community in recent\nyears, which aims at overcoming the problem of sparseness in short texts. In\nthis survey, we conduct a comprehensive review of various short text topic\nmodeling techniques proposed in the literature. We present three categories of\nmethods based on Dirichlet multinomial mixture, global word co-occurrences, and\nself-aggregation, with example of representative approaches in each category\nand analysis of their performance on various tasks. We develop the first\ncomprehensive open-source library, called STTM, for use in Java that integrates\nall surveyed algorithms within a unified interface, benchmark datasets, to\nfacilitate the expansion of new methods in this research field. Finally, we\nevaluate these state-of-the-art methods on many real-world datasets and compare\ntheir performance against one another and versus long text topic modeling\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 09:08:46 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Jipeng", "Qiang", ""], ["Zhenyu", "Qian", ""], ["Yun", "Li", ""], ["Yunhao", "Yuan", ""], ["Xindong", "Wu", ""]]}, {"id": "1904.07733", "submitter": "Babak Naderi", "authors": "Babak Naderi, Salar Mohtaj, Kaspar Ensikat, Sebastian M\\\"oller", "title": "Subjective Assessment of Text Complexity: A Dataset for German Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents TextComplexityDE, a dataset consisting of 1000 sentences\nin German language taken from 23 Wikipedia articles in 3 different\narticle-genres to be used for developing text-complexity predictor models and\nautomatic text simplification in German language. The dataset includes\nsubjective assessment of different text-complexity aspects provided by German\nlearners in level A and B. In addition, it contains manual simplification of\n250 of those sentences provided by native speakers and subjective assessment of\nthe simplified sentences by participants from the target group. The subjective\nratings were collected using both laboratory studies and crowdsourcing\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 14:39:21 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Naderi", "Babak", ""], ["Mohtaj", "Salar", ""], ["Ensikat", "Kaspar", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "1904.07741", "submitter": "Elise Jing", "authors": "Elise Jing, Simon DeDeo, Yong-Yeol Ahn", "title": "Sameness Attracts, Novelty Disturbs, but Outliers Flourish in Fanfiction\n  Online", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nature of what people enjoy is not just a central question for the\ncreative industry, it is a driving force of cultural evolution. It is widely\nbelieved that successful cultural products balance novelty and conventionality:\nthey provide something familiar but at least somewhat divergent from what has\ncome before, and occupy a satisfying middle ground between \"more of the same\"\nand \"too strange\". We test this belief using a large dataset of over half a\nmillion works of fanfiction from the website Archive of Our Own (AO3), looking\nat how the recognition a work receives varies with its novelty. We quantify the\nnovelty through a term-based language model, and a topic model, in the context\nof existing works within the same fandom. Contrary to the balance theory, we\nfind that the lowest-novelty are the most popular and that popularity declines\nmonotonically with novelty. A few exceptions can be found: extremely popular\nworks that are among the highest novelty within the fandom. Taken together, our\nfindings not only challenge the traditional theory of the hedonic value of\nnovelty, they invert it: people prefer the least novel things, are repelled by\nthe middle ground, and have an occasional enthusiasm for extreme outliers. It\nsuggests that cultural evolution must work against inertia --- the appetite\npeople have to continually reconsume the familiar, and may resemble a\npunctuated equilibrium rather than a smooth evolution.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 14:50:09 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Jing", "Elise", ""], ["DeDeo", "Simon", ""], ["Ahn", "Yong-Yeol", ""]]}, {"id": "1904.07826", "submitter": "Jack Hessel", "authors": "Jack Hessel, Lillian Lee, David Mimno", "title": "Unsupervised Discovery of Multimodal Links in Multi-image,\n  Multi-sentence Documents", "comments": "Code and data available at\n  http://www.cs.cornell.edu/~jhessel/multiretrieval/multiretrieval.html", "journal-ref": "EMNLP 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images and text co-occur constantly on the web, but explicit links between\nimages and sentences (or other intra-document textual units) are often not\npresent. We present algorithms that discover image-sentence relationships\nwithout relying on explicit multimodal annotation in training. We experiment on\nseven datasets of varying difficulty, ranging from documents consisting of\ngroups of images captioned post hoc by crowdworkers to naturally-occurring\nuser-generated multimodal documents. We find that a structured training\nobjective based on identifying whether collections of images and sentences\nco-occur in documents can suffice to predict links between specific sentences\nand specific images within the same document at test time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:07:20 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 20:43:53 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hessel", "Jack", ""], ["Lee", "Lillian", ""], ["Mimno", "David", ""]]}, {"id": "1904.07839", "submitter": "Marcos Zampieri", "authors": "Gustavo Henrique Paetzold, Shervin Malmasi, Marcos Zampieri", "title": "UTFPR at SemEval-2019 Task 5: Hate Speech Identification with Recurrent\n  Neural Networks", "comments": "Proceedings of SemEval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the problem of automatically identifying hate speech\nin posts from social media. We approach the task using a system based on\nminimalistic compositional Recurrent Neural Networks (RNN). We tested our\napproach on the SemEval-2019 Task 5: Multilingual Detection of Hate Speech\nAgainst Immigrants and Women in Twitter (HatEval) shared task dataset. The\ndataset made available by the HatEval organizers contained English and Spanish\nposts retrieved from Twitter annotated with respect to the presence of hateful\ncontent and its target. In this paper we present the results obtained by our\nsystem in comparison to the other entries in the shared task. Our system\nachieved competitive performance ranking 7th in sub-task A out of 62 systems in\nthe English track.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:41:49 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Paetzold", "Gustavo Henrique", ""], ["Malmasi", "Shervin", ""], ["Zampieri", "Marcos", ""]]}, {"id": "1904.07904", "submitter": "Chia-Hsuan Lee", "authors": "Chia-Hsuan Lee, Yun-Nung Chen, Hung-Yi Lee", "title": "Mitigating the Impact of Speech Recognition Errors on Spoken Question\n  Answering by Adversarial Domain Adaptation", "comments": "Accepted by ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken question answering (SQA) is challenging due to complex reasoning on\ntop of the spoken documents. The recent studies have also shown the\ncatastrophic impact of automatic speech recognition (ASR) errors on SQA.\nTherefore, this work proposes to mitigate the ASR errors by aligning the\nmismatch between ASR hypotheses and their corresponding reference\ntranscriptions. An adversarial model is applied to this domain adaptation task,\nwhich forces the model to learn domain-invariant features the QA model can\neffectively utilize in order to improve the SQA results. The experiments\nsuccessfully demonstrate the effectiveness of our proposed model, and the\nresults are better than the previous best model by 2% EM score.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 18:13:39 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Lee", "Chia-Hsuan", ""], ["Chen", "Yun-Nung", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1904.07953", "submitter": "Kfir Bar", "authors": "Kfir Bar, Vered Zilberstein, Ido Ziv, Heli Baram, Nachum Dershowitz,\n  Samuel Itzikowitz and Eiran Vadim Harel", "title": "Semantic Characteristics of Schizophrenic Speech", "comments": null, "journal-ref": "CLPsych at NAACL 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing tools are used to automatically detect\ndisturbances in transcribed speech of schizophrenia inpatients who speak\nHebrew. We measure topic mutation over time and show that controls maintain\nmore cohesive speech than inpatients. We also examine differences in how\ninpatients and controls use adjectives and adverbs to describe content words\nand show that the ones used by controls are more common than the those of\ninpatients. We provide experimental results and show their potential for\nautomatically detecting schizophrenia in patients by means only of their speech\npatterns.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:09:41 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Bar", "Kfir", ""], ["Zilberstein", "Vered", ""], ["Ziv", "Ido", ""], ["Baram", "Heli", ""], ["Dershowitz", "Nachum", ""], ["Itzikowitz", "Samuel", ""], ["Harel", "Eiran Vadim", ""]]}, {"id": "1904.07982", "submitter": "Muhammad Mahbubur Rahman", "authors": "Muhammad Mahbubur Rahman, Sorami Hisamoto, Kevin Duh", "title": "Query Expansion for Cross-Language Question Re-Ranking", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community question-answering (CQA) platforms have become very popular forums\nfor asking and answering questions daily. While these forums are rich\nrepositories of community knowledge, they present challenges for finding\nrelevant answers and similar questions, due to the open-ended nature of\ninformal discussions. Further, if the platform allows questions and answers in\nmultiple languages, we are faced with the additional challenge of matching\ncross-lingual information. In this work, we focus on the cross-language\nquestion re-ranking shared task, which aims to find existing questions that may\nbe written in different languages. Our contribution is an exploration of query\nexpansion techniques for this problem. We investigate expansions based on Word\nEmbeddings, DBpedia concepts linking, and Hypernym, and show that they\noutperform existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:55:59 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Rahman", "Muhammad Mahbubur", ""], ["Hisamoto", "Sorami", ""], ["Duh", "Kevin", ""]]}, {"id": "1904.07994", "submitter": "Yi Zhu", "authors": "Yi Zhu, Ivan Vuli\\'c, Anna Korhonen", "title": "A Systematic Study of Leveraging Subword Information for Learning Word\n  Representations", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of subword-level information (e.g., characters, character n-grams,\nmorphemes) has become ubiquitous in modern word representation learning. Its\nimportance is attested especially for morphologically rich languages which\ngenerate a large number of rare words. Despite a steadily increasing interest\nin such subword-informed word representations, their systematic comparative\nanalysis across typologically diverse languages and different tasks is still\nmissing. In this work, we deliver such a study focusing on the variation of two\ncrucial components required for subword-level integration into word\nrepresentation models: 1) segmentation of words into subword units, and 2)\nsubword composition functions to obtain final word representations. We propose\na general framework for learning subword-informed word representations that\nallows for easy experimentation with different segmentation and composition\ncomponents, also including more advanced techniques based on position\nembeddings and self-attention. Using the unified framework, we run experiments\nover a large number of subword-informed word representation configurations (60\nin total) on 3 tasks (general and rare word similarity, dependency parsing,\nfine-grained entity typing) for 5 languages representing 3 language types. Our\nmain results clearly indicate that there is no \"one-sizefits-all\"\nconfiguration, as performance is both language- and task-dependent. We also\nshow that configurations based on unsupervised segmentation (e.g., BPE,\nMorfessor) are sometimes comparable to or even outperform the ones based on\nsupervised word segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 21:50:32 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 23:25:41 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Zhu", "Yi", ""], ["Vuli\u0107", "Ivan", ""], ["Korhonen", "Anna", ""]]}, {"id": "1904.08051", "submitter": "Qi Zhang", "authors": "Qi Zhang, Siliang Tang, Xiang Ren, Fei Wu, Shiliang Pu, Yueting Zhuang", "title": "Posterior-regularized REINFORCE for Instance Selection in Distant\n  Supervision", "comments": "Five pages", "journal-ref": "naacl 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a new way to improve the efficiency of the REINFORCE\ntraining process. We apply it to the task of instance selection in distant\nsupervision. Modeling the instance selection in one bag as a sequential\ndecision process, a reinforcement learning agent is trained to determine\nwhether an instance is valuable or not and construct a new bag with less noisy\ninstances. However unbiased methods, such as REINFORCE, could usually take much\ntime to train. This paper adopts posterior regularization (PR) to integrate\nsome domain-specific rules in instance selection using REINFORCE. As the\nexperiment results show, this method remarkably improves the performance of the\nrelation classifier trained on cleaned distant supervision dataset as well as\nthe efficiency of the REINFORCE training.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 02:21:51 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Zhang", "Qi", ""], ["Tang", "Siliang", ""], ["Ren", "Xiang", ""], ["Wu", "Fei", ""], ["Pu", "Shiliang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1904.08061", "submitter": "Jia Li", "authors": "Jia Li, Xiao Sun, Xing Wei, Changliang Li, Jianhua Tao", "title": "Reinforcement Learning Based Emotional Editing Constraint Conversation\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the generation of conversation content based on deep neural\nnetworks has attracted many researchers. However, traditional neural language\nmodels tend to generate general replies, lacking logical and emotional factors.\nThis paper proposes a conversation content generation model that combines\nreinforcement learning with emotional editing constraints to generate more\nmeaningful and customizable emotional replies. The model divides the replies\ninto three clauses based on pre-generated keywords and uses the emotional\neditor to further optimize the final reply. The model combines multi-task\nlearning with multiple indicator rewards to comprehensively optimize the\nquality of replies. Experiments shows that our model can not only improve the\nfluency of the replies, but also significantly enhance the logical relevance\nand emotional relevance of the replies.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:01:16 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Li", "Jia", ""], ["Sun", "Xiao", ""], ["Wei", "Xing", ""], ["Li", "Changliang", ""], ["Tao", "Jianhua", ""]]}, {"id": "1904.08067", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana\n  Mendu, Laura E. Barnes, Donald E. Brown", "title": "Text Classification Algorithms: A Survey", "comments": null, "journal-ref": null, "doi": "10.3390/info10040150", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, there has been an exponential growth in the number of\ncomplex documents and texts that require a deeper understanding of machine\nlearning methods to be able to accurately classify texts in many applications.\nMany machine learning approaches have achieved surpassing results in natural\nlanguage processing. The success of these learning algorithms relies on their\ncapacity to understand complex models and non-linear relationships within data.\nHowever, finding suitable structures, architectures, and techniques for text\nclassification is a challenge for researchers. In this paper, a brief overview\nof text classification algorithms is discussed. This overview covers different\ntext feature extractions, dimensionality reduction methods, existing algorithms\nand techniques, and evaluations methods. Finally, the limitations of each\ntechnique and their application in the real-world problem are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:29:05 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 01:20:53 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 18:28:33 GMT"}, {"version": "v4", "created": "Tue, 25 Jun 2019 22:51:18 GMT"}, {"version": "v5", "created": "Wed, 20 May 2020 16:27:00 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Kowsari", "Kamran", ""], ["Meimandi", "Kiana Jafari", ""], ["Heidarysafa", "Mojtaba", ""], ["Mendu", "Sanjana", ""], ["Barnes", "Laura E.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1904.08075", "submitter": "Yuchen Liu", "authors": "Yuchen Liu, Hao Xiong, Zhongjun He, Jiajun Zhang, Hua Wu, Haifeng\n  Wang, Chengqing Zong", "title": "End-to-End Speech Translation with Knowledge Distillation", "comments": "Submitted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end speech translation (ST), which directly translates from source\nlanguage speech into target language text, has attracted intensive attentions\nin recent years. Compared to conventional pipeline systems, end-to-end ST\nmodels have advantages of lower latency, smaller model size and less error\npropagation. However, the combination of speech recognition and text\ntranslation in one model is more difficult than each of these two tasks. In\nthis paper, we propose a knowledge distillation approach to improve ST model by\ntransferring the knowledge from text translation model. Specifically, we first\ntrain a text translation model, regarded as a teacher model, and then ST model\nis trained to learn output probabilities from teacher model through knowledge\ndistillation. Experiments on English- French Augmented LibriSpeech and\nEnglish-Chinese TED corpus show that end-to-end ST is possible to implement on\nboth similar and dissimilar language pairs. In addition, with the instruction\nof teacher model, end-to-end ST model can gain significant improvements by over\n3.5 BLEU points.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 04:00:52 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Liu", "Yuchen", ""], ["Xiong", "Hao", ""], ["He", "Zhongjun", ""], ["Zhang", "Jiajun", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""], ["Zong", "Chengqing", ""]]}, {"id": "1904.08100", "submitter": "Jiaju Qi", "authors": "Lei Lei, Jiaju Qi and Kan Zheng", "title": "Patent Analytics Based on Feature Vector Space Model: A Case of IoT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of approved patents worldwide increases rapidly each year, which\nrequires new patent analytics to efficiently mine the valuable information\nattached to these patents. Vector space model (VSM) represents documents as\nhigh-dimensional vectors, where each dimension corresponds to a unique term.\nWhile originally proposed for information retrieval systems, VSM has also seen\nwide applications in patent analytics, and used as a fundamental tool to map\npatent documents to structured data. However, VSM method suffers from several\nlimitations when applied to patent analysis tasks, such as loss of\nsentence-level semantics and curse-of-dimensionality problems. In order to\naddress the above limitations, we propose a patent analytics based on feature\nvector space model (FVSM), where the FVSM is constructed by mapping patent\ndocuments to feature vectors extracted by convolutional neural networks (CNN).\nThe applications of FVSM for three typical patent analysis tasks, i.e., patents\nsimilarity comparison, patent clustering, and patent map generation are\ndiscussed. A case study using patents related to Internet of Things (IoT)\ntechnology is illustrated to demonstrate the performance and effectiveness of\nFVSM. The proposed FVSM can be adopted by other patent analysis studies to\nreplace VSM, based on which various big data learning tasks can be performed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 06:20:53 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Lei", "Lei", ""], ["Qi", "Jiaju", ""], ["Zheng", "Kan", ""]]}, {"id": "1904.08109", "submitter": "Lijing Song", "authors": "Liu Yang, Lijing Song", "title": "Contextual Aware Joint Probability Model Towards Question Answering\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the question answering challenge with the SQuAD 2.0\ndataset. We design a model architecture which leverages BERT's capability of\ncontext-aware word embeddings and BiDAF's context interactive exploration\nmechanism. By integrating these two state-of-the-art architectures, our system\ntries to extract the contextual word representation at word and character\nlevels, for better comprehension of both question and context and their\ncorrelations. We also propose our original joint posterior probability\npredictor module and its associated loss functions. Our best model so far\nobtains F1 score of 75.842% and EM score of 72.24% on the test PCE leaderboad.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 07:16:10 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Yang", "Liu", ""], ["Song", "Lijing", ""]]}, {"id": "1904.08138", "submitter": "Feiyang Chen", "authors": "Feiyang Chen, Ziqian Luo, Yanyan Xu, Dengfeng Ke", "title": "Complementary Fusion of Multi-Features and Multi-Modalities in Sentiment\n  Analysis", "comments": "Accepted by AAAI2020 Workshop: AffCon2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis, mostly based on text, has been rapidly developing in the\nlast decade and has attracted widespread attention in both academia and\nindustry. However, the information in the real world usually comes from\nmultiple modalities, such as audio and text. Therefore, in this paper, based on\naudio and text, we consider the task of multimodal sentiment analysis and\npropose a novel fusion strategy including both multi-feature fusion and\nmulti-modality fusion to improve the accuracy of audio-text sentiment analysis.\nWe call it the DFF-ATMF (Deep Feature Fusion - Audio and Text Modality Fusion)\nmodel, which consists of two parallel branches, the audio modality based branch\nand the text modality based branch. Its core mechanisms are the fusion of\nmultiple feature vectors and multiple modality attention. Experiments on the\nCMU-MOSI dataset and the recently released CMU-MOSEI dataset, both collected\nfrom YouTube for sentiment analysis, show the very competitive results of our\nDFF-ATMF model. Furthermore, by virtue of attention weight distribution\nheatmaps, we also demonstrate the deep features learned by using DFF-ATMF are\ncomplementary to each other and robust. Surprisingly, DFF-ATMF also achieves\nnew state-of-the-art results on the IEMOCAP dataset, indicating that the\nproposed fusion strategy also has a good generalization ability for multimodal\nemotion recognition.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:46:53 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 02:43:45 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 03:40:18 GMT"}, {"version": "v4", "created": "Mon, 22 Jul 2019 02:22:51 GMT"}, {"version": "v5", "created": "Wed, 11 Dec 2019 17:29:01 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Chen", "Feiyang", ""], ["Luo", "Ziqian", ""], ["Xu", "Yanyan", ""], ["Ke", "Dengfeng", ""]]}, {"id": "1904.08194", "submitter": "Tom Pelsmaeker", "authors": "Tom Pelsmaeker, Wilker Aziz", "title": "Effective Estimation of Deep Generative Language Models", "comments": "Published in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in variational inference enable parameterisation of probabilistic\nmodels by deep neural networks. This combines the statistical transparency of\nthe probabilistic modelling framework with the representational power of deep\nlearning. Yet, due to a problem known as posterior collapse, it is difficult to\nestimate such models in the context of language modelling effectively. We\nconcentrate on one such model, the variational auto-encoder, which we argue is\nan important building block in hierarchical probabilistic models of language.\nThis paper contributes a sober view of the problem, a survey of techniques to\naddress it, novel techniques, and extensions to the model. To establish a\nranking of techniques, we perform a systematic comparison using Bayesian\noptimisation and find that many techniques perform reasonably similar, given\nenough resources. Still, a favourite can be named based on convenience. We also\nmake several empirical observations and recommendations of best practices that\nshould help researchers interested in this exciting field.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 11:24:58 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 14:57:41 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 18:21:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Pelsmaeker", "Tom", ""], ["Aziz", "Wilker", ""]]}, {"id": "1904.08248", "submitter": "Luca Pasa PhD", "authors": "Luca Pasa, Giovanni Morrone, Leonardo Badino", "title": "An Analysis of Speech Enhancement and Recognition Losses in Limited\n  Resources Multi-talker Single Channel Audio-Visual ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyzed how audio-visual speech enhancement can help to\nperform the ASR task in a cocktail party scenario. Therefore we considered two\nsimple end-to-end LSTM-based models that perform single-channel audio-visual\nspeech enhancement and phone recognition respectively. Then, we studied how the\ntwo models interact, and how to train them jointly affects the final result. We\nanalyzed different training strategies that reveal some interesting and\nunexpected behaviors. The experiments show that during optimization of the ASR\ntask the speech enhancement capability of the model significantly decreases and\nvice-versa. Nevertheless the joint optimization of the two tasks shows a\nremarkable drop of the Phone Error Rate (PER) compared to the audio-visual\nbaseline models trained only to perform phone recognition. We analyzed the\nbehaviors of the proposed models by using two limited-size datasets, and in\nparticular we used the mixed-speech versions of GRID and TCD-TIMIT.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 14:43:19 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 16:37:24 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Pasa", "Luca", ""], ["Morrone", "Giovanni", ""], ["Badino", "Leonardo", ""]]}, {"id": "1904.08292", "submitter": "Alon Rozental", "authors": "Alon Rozental, Dadi Biton", "title": "Amobee at SemEval-2019 Tasks 5 and 6: Multiple Choice CNN Over\n  Contextual Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes Amobee's participation in \"HatEval: Multilingual\ndetection of hate speech against immigrants and women in Twitter\" (task 5) and\n\"OffensEval: Identifying and Categorizing Offensive Language in Social Media\"\n(task 6). The goal of task 5 was to detect hate speech targeted to women and\nimmigrants. The goal of task 6 was to identify and categorized offensive\nlanguage in social media, and identify offense target. We present a novel type\nof convolutional neural network called \"Multiple Choice CNN\" (MC-CNN) that we\nused over our newly developed contextual embedding, Rozental et al. (2019). For\nboth tasks we used this architecture and achieved 4th place out of 69\nparticipants with an F1 score of 0.53 in task 5, in task 6 achieved 2nd place\n(out of 75) in Sub-task B - automatic categorization of offense types (our\nmodel reached places 18/2/7 out of 103/75/65 for sub-tasks A, B and C\nrespectively in task 6).\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 14:34:12 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Rozental", "Alon", ""], ["Biton", "Dadi", ""]]}, {"id": "1904.08301", "submitter": "Juri Opitz", "authors": "Juri Opitz and Anette Frank", "title": "Automatic Accuracy Prediction for AMR Parsing", "comments": "accepted at *SEM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract Meaning Representation (AMR) represents sentences as directed,\nacyclic and rooted graphs, aiming at capturing their meaning in a machine\nreadable format. AMR parsing converts natural language sentences into such\ngraphs. However, evaluating a parser on new data by means of comparison to\nmanually created AMR graphs is very costly. Also, we would like to be able to\ndetect parses of questionable quality, or preferring results of alternative\nsystems by selecting the ones for which we can assess good quality. We propose\nAMR accuracy prediction as the task of predicting several metrics of\ncorrectness for an automatically generated AMR parse - in absence of the\ncorresponding gold parse. We develop a neural end-to-end multi-output\nregression model and perform three case studies: firstly, we evaluate the\nmodel's capacity of predicting AMR parse accuracies and test whether it can\nreliably assign high scores to gold parses. Secondly, we perform parse\nselection based on predicted parse accuracies of candidate parses from\nalternative systems, with the aim of improving overall results. Finally, we\npredict system ranks for submissions from two AMR shared tasks on the basis of\ntheir predicted parse accuracy averages. All experiments are carried out across\ntwo different domains and show that our method is effective.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 14:59:45 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Opitz", "Juri", ""], ["Frank", "Anette", ""]]}, {"id": "1904.08311", "submitter": "Gakuto Kurata", "authors": "Gakuto Kurata, Kartik Audhkhasi", "title": "Guiding CTC Posterior Spike Timings for Improved Posterior Fusion and\n  Knowledge Distillation", "comments": "Accepted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional automatic speech recognition (ASR) systems trained from\nframe-level alignments can easily leverage posterior fusion to improve ASR\naccuracy and build a better single model with knowledge distillation.\nEnd-to-end ASR systems trained using the Connectionist Temporal Classification\n(CTC) loss do not require frame-level alignment and hence simplify model\ntraining. However, sparse and arbitrary posterior spike timings from CTC models\npose a new set of challenges in posterior fusion from multiple models and\nknowledge distillation between CTC models. We propose a method to train a CTC\nmodel so that its spike timings are guided to align with those of a pre-trained\nguiding CTC model. As a result, all models that share the same guiding model\nhave aligned spike timings. We show the advantage of our method in various\nscenarios including posterior fusion of CTC models and knowledge distillation\nbetween CTC models with different architectures. With the 300-hour Switchboard\ntraining data, the single word CTC model distilled from multiple models\nimproved the word error rates to 13.7%/23.1% from 14.9%/24.1% on the Hub5 2000\nSwitchboard/CallHome test sets without using any data augmentation, language\nmodel, or complex decoder.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 15:18:23 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 13:14:30 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Kurata", "Gakuto", ""], ["Audhkhasi", "Kartik", ""]]}, {"id": "1904.08314", "submitter": "Kyriaki Kalimeri", "authors": "Oscar Araque, Lorenzo Gatti and Kyriaki Kalimeri", "title": "MoralStrength: Exploiting a Moral Lexicon and Embedding Similarity for\n  Moral Foundations Prediction", "comments": null, "journal-ref": "Knowledge-Based Systems 2019", "doi": "10.1016/j.knosys.2019.105184", "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Moral rhetoric plays a fundamental role in how we perceive and interpret the\ninformation we receive, greatly influencing our decision-making process.\nEspecially when it comes to controversial social and political issues, our\nopinions and attitudes are hardly ever based on evidence alone. The Moral\nFoundations Dictionary (MFD) was developed to operationalize moral values in\nthe text. In this study, we present MoralStrength, a lexicon of approximately\n1,000 lemmas, obtained as an extension of the Moral Foundations Dictionary,\nbased on WordNet synsets. Moreover, for each lemma it provides with a\ncrowdsourced numeric assessment of Moral Valence, indicating the strength with\nwhich a lemma is expressing the specific value. We evaluated the predictive\npotentials of this moral lexicon, defining three utilization approaches of\nincreased complexity, ranging from lemmas' statistical properties to a deep\nlearning approach of word embeddings based on semantic similarity. Logistic\nregression models trained on the features extracted from MoralStrength,\nsignificantly outperformed the current state-of-the-art, reaching an F1-score\nof 87.6% over the previous 62.4% (p-value<0.01), and an average F1-Score of\n86.25% over six different datasets. Such findings pave the way for further\nresearch, allowing for an in-depth understanding of moral narratives in text\nfor a wide range of social issues.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 15:21:33 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 03:26:43 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Araque", "Oscar", ""], ["Gatti", "Lorenzo", ""], ["Kalimeri", "Kyriaki", ""]]}, {"id": "1904.08386", "submitter": "Shufan Wang", "authors": "Shufan Wang, Mohit Iyyer", "title": "Casting Light on Invisible Cities: Computationally Engaging with\n  Literary Criticism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literary critics often attempt to uncover meaning in a single work of\nliterature through careful reading and analysis. Applying natural language\nprocessing methods to aid in such literary analyses remains a challenge in\ndigital humanities. While most previous work focuses on \"distant reading\" by\nalgorithmically discovering high-level patterns from large collections of\nliterary works, here we sharpen the focus of our methods to a single literary\ntheory about Italo Calvino's postmodern novel Invisible Cities, which consists\nof 55 short descriptions of imaginary cities. Calvino has provided a\nclassification of these cities into eleven thematic groups, but literary\nscholars disagree as to how trustworthy his categorization is. Due to the\nunique structure of this novel, we can computationally weigh in on this debate:\nwe leverage pretrained contextualized representations to embed each city's\ndescription and use unsupervised methods to cluster these embeddings.\nAdditionally, we compare results of our computational approach to similarity\njudgments generated by human readers. Our work is a first step towards\nincorporating natural language processing into literary criticism.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:37:33 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Wang", "Shufan", ""], ["Iyyer", "Mohit", ""]]}, {"id": "1904.08398", "submitter": "Raphael Tang", "authors": "Ashutosh Adhikari, Achyudh Ram, Raphael Tang, Jimmy Lin", "title": "DocBERT: BERT for Document Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present, to our knowledge, the first application of BERT to document\nclassification. A few characteristics of the task might lead one to think that\nBERT is not the most appropriate model: syntactic structures matter less for\ncontent categories, documents can often be longer than typical BERT input, and\ndocuments often have multiple labels. Nevertheless, we show that a\nstraightforward classification model using BERT is able to achieve the state of\nthe art across four popular datasets. To address the computational expense\nassociated with BERT inference, we distill knowledge from BERT-large to small\nbidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x\nfewer parameters. The primary contribution of our paper is improved baselines\nthat can provide the foundation for future work.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:55:18 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 02:14:00 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 05:09:47 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Adhikari", "Ashutosh", ""], ["Ram", "Achyudh", ""], ["Tang", "Raphael", ""], ["Lin", "Jimmy", ""]]}, {"id": "1904.08455", "submitter": "Oleg Vasilyev", "authors": "Oleg Vasilyev, Tom Grek and John Bohannon", "title": "Headline Generation: Learning from Decomposable Document Titles", "comments": "10 pages, 9 figures, 1 table. v3: Better figures, tables and\n  descriptions - by reviewer Anna Venancio-Marques", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for generating titles for unstructured text\ndocuments. We reframe the problem as a sequential question-answering task. A\ndeep neural network is trained on document-title pairs with decomposable\ntitles, meaning that the vocabulary of the title is a subset of the vocabulary\nof the document. To train the model we use a corpus of millions of publicly\navailable document-title pairs: news articles and headlines. We present the\nresults of a randomized double-blind trial in which subjects were unaware of\nwhich titles were human or machine-generated. When trained on approximately 1.5\nmillion news articles, the model generates headlines that humans judge to be as\ngood or better than the original human-written headlines in the majority of\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 19:03:07 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 17:00:22 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 06:17:34 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Vasilyev", "Oleg", ""], ["Grek", "Tom", ""], ["Bohannon", "John", ""]]}, {"id": "1904.08504", "submitter": "Takashi Matsubara", "authors": "Kenta Hama, Takashi Matsubara, Kuniaki Uehara, Jianfei Cai", "title": "Exploring Uncertainty Measures for Image-Caption Embedding-and-Retrieval\n  Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide development of black-box machine learning algorithms,\nparticularly deep neural network (DNN), the practical demand for the\nreliability assessment is rapidly rising. On the basis of the concept that\n`Bayesian deep learning knows what it does not know,' the uncertainty of DNN\noutputs has been investigated as a reliability measure for the classification\nand regression tasks. However, in the image-caption retrieval task, well-known\nsamples are not always easy-to-retrieve samples. This study investigates two\naspects of image-caption embedding-and-retrieval systems. On one hand, we\nquantify feature uncertainty by considering image-caption embedding as a\nregression task, and use it for model averaging, which can improve the\nretrieval performance. On the other hand, we further quantify posterior\nuncertainty by considering the retrieval as a classification task, and use it\nas a reliability measure, which can greatly improve the retrieval performance\nby rejecting uncertain queries. The consistent performance of two uncertainty\nmeasures is observed with different datasets (MS COCO and Flickr30k), different\ndeep learning architectures (dropout and batch normalization), and different\nsimilarity functions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 12:19:09 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Hama", "Kenta", ""], ["Matsubara", "Takashi", ""], ["Uehara", "Kuniaki", ""], ["Cai", "Jianfei", ""]]}, {"id": "1904.08524", "submitter": "Pranav Ravindra Maneriker", "authors": "Nikhita Vedula, Nedim Lipka, Pranav Maneriker, Srinivasan\n  Parthasarathy", "title": "Towards Open Intent Discovery for Conversational Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and identifying user intent from text, both written and spoken,\nplays an important role in modelling and understand dialogs. Existing research\nfor intent discovery model it as a classification task with a predefined set of\nknown categories. To generailze beyond these preexisting classes, we define a\nnew task of \\textit{open intent discovery}. We investigate how intent can be\ngeneralized to those not seen during training. To this end, we propose a\ntwo-stage approach to this task - predicting whether an utterance contains an\nintent, and then tagging the intent in the input utterance. Our model consists\nof a bidirectional LSTM with a CRF on top to capture contextual semantics,\nsubject to some constraints. Self-attention is used to learn long distance\ndependencies. Further, we adapt an adversarial training approach to improve\nrobustness and perforamce across domains. We also present a dataset of 25k\nreal-life utterances that have been labelled via crowd sourcing. Our\nexperiments across different domains and real-world datasets show the\neffectiveness of our approach, with less than 100 annotated examples needed per\nunique domain to recognize diverse intents. The approach outperforms\nstate-of-the-art baselines by 5-15% F1 score points.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 22:40:01 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Vedula", "Nikhita", ""], ["Lipka", "Nedim", ""], ["Maneriker", "Pranav", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1904.08533", "submitter": "Grzegorz Kondrak", "authors": "Bradley Hauer, Grzegorz Kondrak", "title": "One Homonym per Translation", "comments": "8 pages, including references", "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence.\n  Vol. 34. 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of homonymy is vital to resolving fundamental problems in lexical\nsemantics. In this paper, we propose four hypotheses that characterize the\nunique behavior of homonyms in the context of translations, discourses,\ncollocations, and sense clusters. We present a new annotated homonym resource\nthat allows us to test our hypotheses on existing WSD resources. The results of\nthe experiments provide strong empirical evidence for the hypotheses. This\nstudy represents a step towards a computational method for distinguishing\nbetween homonymy and polysemy, and constructing a definitive inventory of\ncoarse-grained senses.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 23:19:25 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 18:26:40 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Hauer", "Bradley", ""], ["Kondrak", "Grzegorz", ""]]}, {"id": "1904.08535", "submitter": "Paria Jamshid Lou", "authors": "Paria Jamshid Lou and Yufei Wang and Mark Johnson", "title": "Neural Constituency Parsing of Speech Transcripts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the performance of a neural self-attentive parser on\ntranscribed speech. Speech presents parsing challenges that do not appear in\nwritten text, such as the lack of punctuation and the presence of speech\ndisfluencies (including filled pauses, repetitions, corrections, etc.).\nDisfluencies are especially problematic for conventional syntactic parsers,\nwhich typically fail to find any EDITED disfluency nodes at all. This motivated\nthe development of special disfluency detection systems, and special mechanisms\nadded to parsers specifically to handle disfluencies. However, we show here\nthat neural parsers can find EDITED disfluency nodes, and the best neural\nparsers find them with an accuracy surpassing that of specialized disfluency\ndetection systems, thus making these specialized mechanisms unnecessary. This\npaper also investigates a modified loss function that puts more weight on\nEDITED nodes. It also describes tree-transformations that simplify the\ndisfluency detection task by providing alternative encodings of disfluencies\nand syntactic information.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 23:30:17 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 04:50:22 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 00:00:11 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 07:37:31 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Lou", "Paria Jamshid", ""], ["Wang", "Yufei", ""], ["Johnson", "Mark", ""]]}, {"id": "1904.08587", "submitter": "Longqi Yang", "authors": "Longqi Yang, Chen Fang, Hailin Jin, Walter Chang, Deborah Estrin", "title": "Creative Procedural-Knowledge Extraction From Web Design Tutorials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex design tasks often require performing diverse actions in a specific\norder. To (semi-)autonomously accomplish these tasks, applications need to\nunderstand and learn a wide range of design procedures, i.e., Creative\nProcedural-Knowledge (CPK). Prior knowledge base construction and mining have\nnot typically addressed the creative fields, such as design and arts. In this\npaper, we formalize an ontology of CPK using five components: goal, workflow,\naction, command and usage; and extract components' values from online design\ntutorials. We scraped 19.6K tutorial-related webpages and built a web\napplication for professional designers to identify and summarize CPK\ncomponents. The annotated dataset consists of 819 unique commands, 47,491\nactions, and 2,022 workflows and goals. Based on this dataset, we propose a\ngeneral CPK extraction pipeline and demonstrate that existing text\nclassification and sequence-to-sequence models are limited in identifying,\npredicting and summarizing complex operations described in heterogeneous\nstyles. Through quantitative and qualitative error analysis, we discuss CPK\nextraction challenges that need to be addressed by future research.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 04:22:23 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Yang", "Longqi", ""], ["Fang", "Chen", ""], ["Jin", "Hailin", ""], ["Chang", "Walter", ""], ["Estrin", "Deborah", ""]]}, {"id": "1904.08637", "submitter": "Sungjin Lee", "authors": "Sungjin Lee, Qi Zhu, Ryuichi Takanobu, Xiang Li, Yaoqin Zhang, Zheng\n  Zhang, Jinchao Li, Baolin Peng, Xiujun Li, Minlie Huang and Jianfeng Gao", "title": "ConvLab: Multi-Domain End-to-End Dialog System Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ConvLab, an open-source multi-domain end-to-end dialog system\nplatform, that enables researchers to quickly set up experiments with reusable\ncomponents and compare a large set of different approaches, ranging from\nconventional pipeline systems to end-to-end neural models, in common\nenvironments. ConvLab offers a set of fully annotated datasets and associated\npre-trained reference models. As a showcase, we extend the MultiWOZ dataset\nwith user dialog act annotations to train all component models and demonstrate\nhow ConvLab makes it easy and effortless to conduct complicated experiments in\nmulti-domain end-to-end dialog settings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 08:35:49 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Lee", "Sungjin", ""], ["Zhu", "Qi", ""], ["Takanobu", "Ryuichi", ""], ["Li", "Xiang", ""], ["Zhang", "Yaoqin", ""], ["Zhang", "Zheng", ""], ["Li", "Jinchao", ""], ["Peng", "Baolin", ""], ["Li", "Xiujun", ""], ["Huang", "Minlie", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1904.08654", "submitter": "Philipp Dufter", "authors": "Philipp Dufter and Hinrich Sch\\\"utze", "title": "Analytical Methods for Interpretable Ultradense Word Embeddings", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are useful for a wide variety of tasks, but they lack\ninterpretability. By rotating word spaces, interpretable dimensions can be\nidentified while preserving the information contained in the embeddings without\nany loss. In this work, we investigate three methods for making word spaces\ninterpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and\nDensRay, a new method we propose. In contrast to Densifier, DensRay can be\ncomputed in closed form, is hyperparameter-free and thus more robust than\nDensifier. We evaluate the three methods on lexicon induction and set-based\nword analogy. In addition we provide qualitative insights as to how\ninterpretable word spaces can be used for removing gender bias from embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 09:47:06 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 16:15:37 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Dufter", "Philipp", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1904.08709", "submitter": "Simone Paolo Ponzetto", "authors": "Lydia Weiland, Ioana Hulpus, Simone Paolo Ponzetto, Wolfgang\n  Effelsberg, Laura Dietz", "title": "Knowledge-rich Image Gist Understanding Beyond Literal Meaning", "comments": null, "journal-ref": "Data & Knowledge Engineering, Volume 117, September 2018, Pages\n  114-132", "doi": "10.1016/j.datak.2018.07.006", "report-no": null, "categories": "cs.IR cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the problem of understanding the message (gist) conveyed by\nimages and their captions as found, for instance, on websites or news articles.\nTo this end, we propose a methodology to capture the meaning of image-caption\npairs on the basis of large amounts of machine-readable knowledge that has\npreviously been shown to be highly effective for text understanding. Our method\nidentifies the connotation of objects beyond their denotation: where most\napproaches to image understanding focus on the denotation of objects, i.e.,\ntheir literal meaning, our work addresses the identification of connotations,\ni.e., iconic meanings of objects, to understand the message of images. We view\nimage understanding as the task of representing an image-caption pair on the\nbasis of a wide-coverage vocabulary of concepts such as the one provided by\nWikipedia, and cast gist detection as a concept-ranking problem with\nimage-caption pairs as queries. To enable a thorough investigation of the\nproblem of gist understanding, we produce a gold standard of over 300\nimage-caption pairs and over 8,000 gist annotations covering a wide variety of\ntopics at different levels of abstraction. We use this dataset to\nexperimentally benchmark the contribution of signals from heterogeneous\nsources, namely image and text. The best result with a Mean Average Precision\n(MAP) of 0.69 indicate that by combining both dimensions we are able to better\nunderstand the meaning of our image-caption pairs than when using language or\nvision information alone. We test the robustness of our gist detection approach\nwhen receiving automatically generated input, i.e., using automatically\ngenerated image tags or generated captions, and prove the feasibility of an\nend-to-end automated process.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:50:20 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Weiland", "Lydia", ""], ["Hulpus", "Ioana", ""], ["Ponzetto", "Simone Paolo", ""], ["Effelsberg", "Wolfgang", ""], ["Dietz", "Laura", ""]]}, {"id": "1904.08721", "submitter": "Tommaso Venturini", "authors": "Erik Borra, Andreas Kaltenbrunner (BMF), Michele Mauri, Esther\n  Weltevrede, David Laniado, Richard Rogers (UvA), Paolo Ciuccarelli, Giovanni\n  Magni, Tommaso Venturini (MEDIALAB, CIS)", "title": "Societal Controversies in Wikipedia Articles", "comments": null, "journal-ref": "the 33rd Annual ACM Conference, Apr 2015, Seoul, France.\n  pp.193-196", "doi": "10.1145/2702123.2702436", "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative content creation inevitably reaches situations where different\npoints of view lead to conflict. We focus on Wikipedia, the free encyclopedia\nanyone may edit, where disputes about content in controversial articles often\nreflect larger societal debates. While Wikipedia has a public edit history and\ndiscussion section for every article, the substance of these sections is\ndifficult to phantom for Wikipedia users interested in the development of an\narticle and in locating which topics were most controversial. In this paper we\npresent Contropedia, a tool that augments Wikipedia articles and gives insight\ninto the development of controversial topics. Contropedia uses an efficient\nlanguage agnostic measure based on the edit history that focuses on wiki links\nto easily identify which topics within a Wikipedia article have been most\ncontroversial and when.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 12:19:09 GMT"}], "update_date": "2019-04-21", "authors_parsed": [["Borra", "Erik", "", "BMF"], ["Kaltenbrunner", "Andreas", "", "BMF"], ["Mauri", "Michele", "", "UvA"], ["Weltevrede", "Esther", "", "UvA"], ["Laniado", "David", "", "UvA"], ["Rogers", "Richard", "", "UvA"], ["Ciuccarelli", "Paolo", "", "MEDIALAB, CIS"], ["Magni", "Giovanni", "", "MEDIALAB, CIS"], ["Venturini", "Tommaso", "", "MEDIALAB, CIS"]]}, {"id": "1904.08770", "submitter": "Sandip Modha", "authors": "Sandip Modha and Prasenjit Majumder", "title": "An Empirical Evaluation of Text Representation Schemes on Multilingual\n  Social Web to Filter the Textual Aggression", "comments": "21 Page, 2 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper attempt to study the effectiveness of text representation schemes\non two tasks namely: User Aggression and Fact Detection from the social media\ncontents. In User Aggression detection, The aim is to identify the level of\naggression from the contents generated in the Social media and written in the\nEnglish, Devanagari Hindi and Romanized Hindi. Aggression levels are\ncategorized into three predefined classes namely: `Non-aggressive`, `Overtly\nAggressive`, and `Covertly Aggressive`. During the disaster-related incident,\nSocial media like, Twitter is flooded with millions of posts. In such emergency\nsituations, identification of factual posts is important for organizations\ninvolved in the relief operation. We anticipated this problem as a combination\nof classification and Ranking problem. This paper presents a comparison of\nvarious text representation scheme based on BoW techniques, distributed\nword/sentence representation, transfer learning on classifiers. Weighted $F_1$\nscore is used as a primary evaluation metric. Results show that text\nrepresentation using BoW performs better than word embedding on machine\nlearning classifiers. While pre-trained Word embedding techniques perform\nbetter on classifiers based on deep neural net. Recent transfer learning model\nlike ELMO, ULMFiT are fine-tuned for the Aggression classification task.\nHowever, results are not at par with pre-trained word embedding model. Overall,\nword embedding using fastText produce best weighted $F_1$-score than Word2Vec\nand Glove. Results are further improved using pre-trained vector model.\nStatistical significance tests are employed to ensure the significance of the\nclassification results. In the case of lexically different test Dataset, other\nthan training Dataset, deep neural models are more robust and perform\nsubstantially better than machine learning classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:10:52 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Modha", "Sandip", ""], ["Majumder", "Prasenjit", ""]]}, {"id": "1904.08779", "submitter": "Daniel Park", "authors": "Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,\n  Ekin D. Cubuk, Quoc V. Le", "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "comments": "5 pages, 3 figures, 6 tables; v3: references added", "journal-ref": "Proc. Interspeech 2019, 2613-2617", "doi": "10.21437/Interspeech.2019-2680", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SpecAugment, a simple data augmentation method for speech\nrecognition. SpecAugment is applied directly to the feature inputs of a neural\nnetwork (i.e., filter bank coefficients). The augmentation policy consists of\nwarping the features, masking blocks of frequency channels, and masking blocks\nof time steps. We apply SpecAugment on Listen, Attend and Spell networks for\nend-to-end speech recognition tasks. We achieve state-of-the-art performance on\nthe LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.\nOn LibriSpeech, we achieve 6.8% WER on test-other without the use of a language\nmodel, and 5.8% WER with shallow fusion with a language model. This compares to\nthe previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set\nwithout the use of a language model, and 6.8%/14.1% with shallow fusion, which\ncompares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:53:38 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 21:56:06 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 18:19:07 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Park", "Daniel S.", ""], ["Chan", "William", ""], ["Zhang", "Yu", ""], ["Chiu", "Chung-Cheng", ""], ["Zoph", "Barret", ""], ["Cubuk", "Ekin D.", ""], ["Le", "Quoc V.", ""]]}, {"id": "1904.08783", "submitter": "Marta R. Costa-juss\\`a", "authors": "Christine Basta, Marta R. Costa-juss\\`a, Noe Casas", "title": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender bias is highly impacting natural language processing applications.\nWord embeddings have clearly been proven both to keep and amplify gender biases\nthat are present in current data sources. Recently, contextualized word\nembeddings have enhanced previous word embedding techniques by computing word\nvector representations dependent on the sentence they appear in.\n  In this paper, we study the impact of this conceptual change in the word\nembedding computation in relation with gender bias. Our analysis includes\ndifferent measures previously applied in the literature to standard word\nembeddings. Our findings suggest that contextualized word embeddings are less\nbiased than standard ones even when the latter are debiased.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:47:00 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Basta", "Christine", ""], ["Costa-juss\u00e0", "Marta R.", ""], ["Casas", "Noe", ""]]}, {"id": "1904.08835", "submitter": "Dongjun Lee", "authors": "Dongjun Lee", "title": "Clause-Wise and Recursive Decoding for Complex and Cross-Domain\n  Text-to-SQL Generation", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most deep learning approaches for text-to-SQL generation are limited to the\nWikiSQL dataset, which only supports very simple queries over a single table.\nWe focus on the Spider dataset, a complex and cross-domain text-to-SQL task,\nwhich includes complex queries over multiple tables. In this paper, we propose\na SQL clause-wise decoding neural architecture with a self-attention based\ndatabase schema encoder to address the Spider task. Each of the clause-specific\ndecoders consists of a set of sub-modules, which is defined by the syntax of\neach clause. Additionally, our model works recursively to support nested\nqueries. When evaluated on the Spider dataset, our approach achieves 4.6\\% and\n9.8\\% accuracy gain in the test and dev sets, respectively. In addition, we\nshow that our model is significantly more effective at predicting complex and\nnested queries than previous work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:20:45 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 08:58:05 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Lee", "Dongjun", ""]]}, {"id": "1904.08850", "submitter": "Thierry Boy de la Tour", "authors": "Thierry Boy de la Tour and Rachid Echahed", "title": "True Parallel Graph Transformations: an Algebraic Approach Based on Weak\n  Spans", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of defining graph transformations by the simultaneous\napplication of direct transformations even when these cannot be applied\nindependently of each other. An algebraic approach is adopted, with production\nrules of the form $L\\xleftarrow{l}K \\xleftarrow{i} I \\xrightarrow{r} R$, called\nweak spans. A parallel coherent transformation is introduced and shown to be a\nconservative extension of the interleaving semantics of parallel independent\ndirect transformations. A categorical construction of finitely attributed\nstructures is proposed, in which parallel coherent transformations can be built\nin a natural way. These notions are introduced and illustrated on detailed\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 14:21:43 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["de la Tour", "Thierry Boy", ""], ["Echahed", "Rachid", ""]]}, {"id": "1904.08920", "submitter": "Amanpreet Singh", "authors": "Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen,\n  Dhruv Batra, Devi Parikh, Marcus Rohrbach", "title": "Towards VQA Models That Can Read", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies have shown that a dominant class of questions asked by visually\nimpaired users on images of their surroundings involves reading text in the\nimage. But today's VQA models can not read! Our paper takes a first step\ntowards addressing this problem. First, we introduce a new \"TextVQA\" dataset to\nfacilitate progress on this important problem. Existing datasets either have a\nsmall proportion of questions about text (e.g., the VQA dataset) or are too\nsmall (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408\nimages that require reasoning about text to answer. Second, we introduce a\nnovel model architecture that reads text in the image, reasons about it in the\ncontext of the image and the question, and predicts an answer which might be a\ndeduction based on the text and the image or composed of the strings found in\nthe image. Consequently, we call our approach Look, Read, Reason & Answer\n(LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on\nour TextVQA dataset. We find that the gap between human performance and machine\nperformance is significantly larger on TextVQA than on VQA 2.0, suggesting that\nTextVQA is well-suited to benchmark progress along directions complementary to\nVQA 2.0.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:55:37 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 23:28:48 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Singh", "Amanpreet", ""], ["Natarajan", "Vivek", ""], ["Shah", "Meet", ""], ["Jiang", "Yu", ""], ["Chen", "Xinlei", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1904.08926", "submitter": "Vladimir Vargas-Calder\\'on", "authors": "Vargas-Calder\\'on Vladimir and Camargo Jorge", "title": "Characterization of citizens using word2vec and latent topic analysis in\n  a large set of tweets", "comments": null, "journal-ref": null, "doi": "10.1016/j.cities.2019.03.019", "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing use of the Internet and mobile devices, social networks\nare becoming the most used media to communicate citizens' ideas and thoughts.\nThis information is very useful to identify communities with common ideas based\non what they publish in the network. This paper presents a method to\nautomatically detect city communities based on machine learning techniques\napplied to a set of tweets from Bogot\\'a's citizens. An analysis was performed\nin a collection of 2,634,176 tweets gathered from Twitter in a period of six\nmonths. Results show that the proposed method is an interesting tool to\ncharacterize a city population based on a machine learning methods and text\nanalytics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 13:25:38 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Vladimir", "Vargas-Calder\u00f3n", ""], ["Jorge", "Camargo", ""]]}, {"id": "1904.08936", "submitter": "Anupiya Nugaliyadde Mr", "authors": "Anupiya Nugaliyadde, Kok Wai Wong, Ferdous Sohel and Hong Xie", "title": "Language Modeling through Long Term Memory Network", "comments": "The paper is accepted to be published in IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTM), and\nMemory Networks which contain memory are popularly used to learn patterns in\nsequential data. Sequential data has long sequences that hold relationships.\nRNN can handle long sequences but suffers from the vanishing and exploding\ngradient problems. While LSTM and other memory networks address this problem,\nthey are not capable of handling long sequences (50 or more data points long\nsequence patterns). Language modelling requiring learning from longer sequences\nare affected by the need for more information in memory. This paper introduces\nLong Term Memory network (LTM), which can tackle the exploding and vanishing\ngradient problems and handles long sequences without forgetting. LTM is\ndesigned to scale data in the memory and gives a higher weight to the input in\nthe sequence. LTM avoid overfitting by scaling the cell state after achieving\nthe optimal results. The LTM is tested on Penn treebank dataset, and Text8\ndataset and LTM achieves test perplexities of 83 and 82 respectively. 650 LTM\ncells achieved a test perplexity of 67 for Penn treebank, and 600 cells\nachieved a test perplexity of 77 for Text8. LTM achieves state of the art\nresults by only using ten hidden LTM cells for both datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 09:19:25 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Nugaliyadde", "Anupiya", ""], ["Wong", "Kok Wai", ""], ["Sohel", "Ferdous", ""], ["Xie", "Hong", ""]]}, {"id": "1904.08950", "submitter": "Xiaochuang Han", "authors": "Xiaochuang Han, Eunsol Choi, Chenhao Tan", "title": "No Permanent Friends or Enemies: Tracking Relationships between Nations\n  from News", "comments": "NAACL 2019; code available at https://github.com/BoulderDS/LARN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the dynamics of international politics is important yet\nchallenging for civilians. In this work, we explore unsupervised neural models\nto infer relations between nations from news articles. We extend existing\nmodels by incorporating shallow linguistics information and propose a new\nautomatic evaluation metric that aligns relationship dynamics with manually\nannotated key events. As understanding international relations requires\ncarefully analyzing complex relationships, we conduct in-person human\nevaluations with three groups of participants. Overall, humans prefer the\noutputs of our model and give insightful feedback that suggests future\ndirections for human-centered models. Furthermore, our model reveals\ninteresting regional differences in news coverage. For instance, with respect\nto US-China relations, Singaporean media focus more on \"strengthening\" and\n\"purchasing\", while US media focus more on \"criticizing\" and \"denouncing\".\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 18:00:30 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Han", "Xiaochuang", ""], ["Choi", "Eunsol", ""], ["Tan", "Chenhao", ""]]}, {"id": "1904.09020", "submitter": "Giovanni Campagna", "authors": "Giovanni Campagna, Silei Xu, Mehrad Moradshahi, Richard Socher, Monica\n  S. Lam", "title": "Genie: A Generator of Natural Language Semantic Parsers for Virtual\n  Assistant Commands", "comments": "To appear in PLDI 2019", "journal-ref": null, "doi": "10.1145/3314221.3314594", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand diverse natural language commands, virtual assistants today are\ntrained with numerous labor-intensive, manually annotated sentences. This paper\npresents a methodology and the Genie toolkit that can handle new compound\ncommands with significantly less manual effort. We advocate formalizing the\ncapability of virtual assistants with a Virtual Assistant Programming Language\n(VAPL) and using a neural semantic parser to translate natural language into\nVAPL code. Genie needs only a small realistic set of input sentences for\nvalidating the neural model. Developers write templates to synthesize data;\nGenie uses crowdsourced paraphrases and data augmentation, along with the\nsynthesized data, to train a semantic parser. We also propose design principles\nthat make VAPL languages amenable to natural language translation. We apply\nthese principles to revise ThingTalk, the language used by the Almond virtual\nassistant. We use Genie to build the first semantic parser that can support\ncompound virtual assistants commands with unquoted free-form parameters. Genie\nachieves a 62% accuracy on realistic user inputs. We demonstrate Genie's\ngenerality by showing a 19% and 31% improvement over the previous state of the\nart on a music skill, aggregate functions, and access control.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 21:33:15 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Campagna", "Giovanni", ""], ["Xu", "Silei", ""], ["Moradshahi", "Mehrad", ""], ["Socher", "Richard", ""], ["Lam", "Monica S.", ""]]}, {"id": "1904.09037", "submitter": "Chu Wang", "authors": "Chu Wang and Lei Tang and Yang Lu and Shujun Bian and Hirohisa Fujita\n  and Da Zhang and Zuohua Zhang and Yongning Wu", "title": "ProductNet: a Collection of High-Quality Datasets for Product\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ProductNet is a collection of high-quality product datasets for better\nproduct understanding. Motivated by ImageNet, ProductNet aims at supporting\nproduct representation learning by curating product datasets of high quality\nwith properly chosen taxonomy. In this paper, the two goals of building\nhigh-quality product datasets and learning product representation support each\nother in an iterative fashion: the product embedding is obtained via a\nmulti-modal deep neural network (master model) designed to leverage product\nimage and catalog information; and in return, the embedding is utilized via\nactive learning (local model) to vastly accelerate the annotation process. For\nthe labeled data, the proposed master model yields high categorization accuracy\n(94.7% top-1 accuracy for 1240 classes), which can be used as search indices,\npartition keys, and input features for machine learning models. The product\nembedding, as well as the fined-tuned master model for a specific business\ntask, can also be used for various transfer learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 23:17:07 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Wang", "Chu", ""], ["Tang", "Lei", ""], ["Lu", "Yang", ""], ["Bian", "Shujun", ""], ["Fujita", "Hirohisa", ""], ["Zhang", "Da", ""], ["Zhang", "Zuohua", ""], ["Wu", "Yongning", ""]]}, {"id": "1904.09049", "submitter": "Aswin Shanmugam Subramanian", "authors": "Aswin Shanmugam Subramanian, Xiaofei Wang, Shinji Watanabe, Toru\n  Taniguchi, Dung Tran, Yuya Fujita", "title": "An Investigation of End-to-End Multichannel Speech Recognition for\n  Reverberant and Mismatch Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence (S2S) modeling is becoming a popular paradigm for\nautomatic speech recognition (ASR) because of its ability to jointly optimize\nall the conventional ASR components in an end-to-end (E2E) fashion. This report\ninvestigates the ability of E2E ASR from standard close-talk to far-field\napplications by encompassing entire multichannel speech enhancement and ASR\ncomponents within the S2S model. There have been previous studies on jointly\noptimizing neural beamforming alongside E2E ASR for denoising. It is clear from\nboth recent challenge outcomes and successful products that far-field systems\nwould be incomplete without solving both denoising and dereverberation\nsimultaneously. This report uses a recently developed architecture for\nfar-field ASR by composing neural extensions of dereverberation and beamforming\nmodules with the S2S ASR module as a single differentiable neural network and\nalso clearly defining the role of each subnetwork. The original implementation\nof this architecture was successfully applied to the noisy speech recognition\ntask (CHiME-4), while we applied this implementation to noisy reverberant tasks\n(DIRHA and REVERB). Our investigation shows that the method achieves better\nperformance than conventional pipeline methods on the DIRHA English dataset and\ncomparable performance on the REVERB dataset. It also has additional advantages\nof being neither iterative nor requiring parallel noisy and clean speech data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 01:36:37 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 17:33:06 GMT"}, {"version": "v3", "created": "Sun, 28 Apr 2019 06:13:42 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Subramanian", "Aswin Shanmugam", ""], ["Wang", "Xiaofei", ""], ["Watanabe", "Shinji", ""], ["Taniguchi", "Toru", ""], ["Tran", "Dung", ""], ["Fujita", "Yuya", ""]]}, {"id": "1904.09051", "submitter": "Abram Handler", "authors": "Abram Handler and Brendan O'Connor", "title": "Query-focused Sentence Compression in Linear Time", "comments": "EMNLP 2019 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search applications often display shortened sentences which must contain\ncertain query terms and must fit within the space constraints of a user\ninterface. This work introduces a new transition-based sentence compression\ntechnique developed for such settings. Our query-focused method constructs\nlength and lexically constrained compressions in linear time, by growing a\nsubgraph in the dependency parse of a sentence. This theoretically efficient\napproach achieves an 11X empirical speedup over baseline ILP methods, while\nbetter reconstructing gold constrained shortenings. Such speedups help\nquery-focused applications, because users are measurably hindered by interface\nlags. Additionally, our technique does not require an ILP solver or a GPU.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 02:19:43 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 22:23:20 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Handler", "Abram", ""], ["O'Connor", "Brendan", ""]]}, {"id": "1904.09067", "submitter": "Michael Cogswell", "authors": "Michael Cogswell, Jiasen Lu, Stefan Lee, Devi Parikh, Dhruv Batra", "title": "Emergence of Compositional Language with Deep Generational Transmission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has studied the emergence of language among deep reinforcement\nlearning agents that must collaborate to solve a task. Of particular interest\nare the factors that cause language to be compositional -- i.e., express\nmeaning by combining words which themselves have meaning. Evolutionary\nlinguists have found that in addition to structural priors like those already\nstudied in deep learning, the dynamics of transmitting language from generation\nto generation contribute significantly to the emergence of compositionality. In\nthis paper, we introduce these cultural evolutionary dynamics into language\nemergence by periodically replacing agents in a population to create a\nknowledge gap, implicitly inducing cultural transmission of language. We show\nthat this implicit cultural transmission encourages the resulting languages to\nexhibit better compositional generalization.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:09:12 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 19:54:23 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Cogswell", "Michael", ""], ["Lu", "Jiasen", ""], ["Lee", "Stefan", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.09068", "submitter": "Liu Yang", "authors": "Liu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jianfeng Gao, W. Bruce\n  Croft, Xiaodong Liu, Yelong Shen, Jingjing Liu", "title": "A Hybrid Retrieval-Generation Neural Conversation Model", "comments": "Accepted as a Full Paper in CIKM 2019. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent personal assistant systems that are able to have multi-turn\nconversations with human users are becoming increasingly popular. Most previous\nresearch has been focused on using either retrieval-based or generation-based\nmethods to develop such systems. Retrieval-based methods have the advantage of\nreturning fluent and informative responses with great diversity. However, the\nperformance of the methods is limited by the size of the response repository.\nOn the other hand, generation-based methods can produce highly coherent\nresponses on any topics. But the generated responses are often generic and not\ninformative due to the lack of grounding knowledge. In this paper, we propose a\nhybrid neural conversation model that combines the merits of both response\nretrieval and generation methods. Experimental results on Twitter and\nFoursquare data show that the proposed model outperforms both retrieval-based\nmethods and generation-based methods (including a recently proposed\nknowledge-grounded neural conversation model) under both automatic evaluation\nmetrics and human evaluation. We hope that the findings in this study provide\nnew insights on how to integrate text retrieval and text generation models for\nbuilding conversation systems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:10:03 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 23:52:20 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Yang", "Liu", ""], ["Hu", "Junjie", ""], ["Qiu", "Minghui", ""], ["Qu", "Chen", ""], ["Gao", "Jianfeng", ""], ["Croft", "W. Bruce", ""], ["Liu", "Xiaodong", ""], ["Shen", "Yelong", ""], ["Liu", "Jingjing", ""]]}, {"id": "1904.09072", "submitter": "Debanjan Mahata", "authors": "Haimin Zhang, Debanjan Mahata, Simra Shahid, Laiba Mehnaz, Sarthak\n  Anand, Yaman Singla, Rajiv Ratn Shah, Karan Uppal", "title": "Identifying Offensive Posts and Targeted Offense from Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our approach and the system description for Sub-task\nA and Sub Task B of SemEval 2019 Task 6: Identifying and Categorizing Offensive\nLanguage in Social Media. Sub-task A involves identifying if a given tweet is\noffensive or not, and Sub Task B involves detecting if an offensive tweet is\ntargeted towards someone (group or an individual). Our models for Sub-task A is\nbased on an ensemble of Convolutional Neural Network, Bidirectional LSTM with\nattention, and Bidirectional LSTM + Bidirectional GRU, whereas for Sub-task B,\nwe rely on a set of heuristics derived from the training data and manual\nobservation. We provide detailed analysis of the results obtained using the\ntrained models. Our team ranked 5th out of 103 participants in Sub-task A,\nachieving a macro F1 score of 0.807, and ranked 8th out of 75 participants in\nSub Task B achieving a macro F1 of 0.695.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:26:36 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Zhang", "Haimin", ""], ["Mahata", "Debanjan", ""], ["Shahid", "Simra", ""], ["Mehnaz", "Laiba", ""], ["Anand", "Sarthak", ""], ["Singla", "Yaman", ""], ["Shah", "Rajiv Ratn", ""], ["Uppal", "Karan", ""]]}, {"id": "1904.09076", "submitter": "Debanjan Mahata", "authors": "Sarthak Anand, Debanjan Mahata, Kartik Aggarwal, Laiba Mehnaz, Simra\n  Shahid, Haimin Zhang, Yaman Kumar, Rajiv Ratn Shah, Karan Uppal", "title": "Suggestion Mining from Online Reviews using ULMFiT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our approach and the system description for Sub Task\nA of SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums.\nGiven a sentence, the task asks to predict whether the sentence consists of a\nsuggestion or not. Our model is based on Universal Language Model Fine-tuning\nfor Text Classification. We apply various pre-processing techniques before\ntraining the language and the classification model. We further provide detailed\nanalysis of the results obtained using the trained model. Our team ranked 10th\nout of 34 participants, achieving an F1 score of 0.7011. We publicly share our\nimplementation at https://github.com/isarth/SemEval9_MIDAS\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:38:32 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Anand", "Sarthak", ""], ["Mahata", "Debanjan", ""], ["Aggarwal", "Kartik", ""], ["Mehnaz", "Laiba", ""], ["Shahid", "Simra", ""], ["Zhang", "Haimin", ""], ["Kumar", "Yaman", ""], ["Shah", "Rajiv Ratn", ""], ["Uppal", "Karan", ""]]}, {"id": "1904.09077", "submitter": "Shijie Wu", "authors": "Shijie Wu and Mark Dredze", "title": "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT", "comments": "EMNLP 2019 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained contextual representation models (Peters et al., 2018; Devlin et\nal., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new\nrelease of BERT (Devlin, 2018) includes a model simultaneously pretrained on\n104 languages with impressive performance for zero-shot cross-lingual transfer\non a natural language inference task. This paper explores the broader\ncross-lingual potential of mBERT (multilingual) as a zero shot language\ntransfer model on 5 NLP tasks covering a total of 39 languages from various\nlanguage families: NLI, document classification, NER, POS tagging, and\ndependency parsing. We compare mBERT with the best-published methods for\nzero-shot cross-lingual transfer and find mBERT competitive on each task.\nAdditionally, we investigate the most effective strategy for utilizing mBERT in\nthis manner, determine to what extent mBERT generalizes away from language\nspecific features, and measure factors that influence cross-lingual transfer.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:45:44 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 17:22:55 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Wu", "Shijie", ""], ["Dredze", "Mark", ""]]}, {"id": "1904.09086", "submitter": "Srinivasan Iyer", "authors": "Srinivasan Iyer, Alvin Cheung, Luke Zettlemoyer", "title": "Learning Programmatic Idioms for Scalable Semantic Parsing", "comments": "Accepted at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmers typically organize executable source code using high-level coding\npatterns or idiomatic structures such as nested loops, exception handlers and\nrecursive blocks, rather than as individual code tokens. In contrast, state of\nthe art (SOTA) semantic parsers still map natural language instructions to\nsource code by building the code syntax tree one node at a time. In this paper,\nwe introduce an iterative method to extract code idioms from large source code\ncorpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax\ntrees, and train semantic parsers to apply these idioms during decoding.\nApplying idiom-based decoding on a recent context-dependent semantic parsing\ntask improves the SOTA by 2.2\\% BLEU score while reducing training time by more\nthan 50\\%. This improved speed enables us to scale up the model by training on\nan extended training set that is 5$\\times$ larger, to further move up the SOTA\nby an additional 2.3\\% BLEU and 0.9\\% exact match. Finally, idioms also\nsignificantly improve accuracy of semantic parsing to SQL on the ATIS-SQL\ndataset, when training data is limited.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 05:56:45 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 06:20:15 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Iyer", "Srinivasan", ""], ["Cheung", "Alvin", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1904.09107", "submitter": "Kai Song", "authors": "Kai Song, Yue Zhang, Heng Yu, Weihua Luo, Kun Wang and Min Zhang", "title": "Code-Switching for Enhancing NMT with Pre-Specified Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging user-provided translation to constrain NMT has practical\nsignificance. Existing methods can be classified into two main categories,\nnamely the use of placeholder tags for lexicon words and the use of hard\nconstraints during decoding. Both methods can hurt translation fidelity for\nvarious reasons. We investigate a data augmentation method, making\ncode-switched training data by replacing source phrases with their target\ntranslations. Our method does not change the MNT model or decoding algorithm,\nallowing the model to learn lexicon translations by copying source-side target\nwords. Extensive experiments show that our method achieves consistent\nimprovements over existing approaches, improving translation of constrained\nwords without hurting unconstrained words.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 07:53:06 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 11:36:35 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 03:02:31 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 01:16:49 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Song", "Kai", ""], ["Zhang", "Yue", ""], ["Yu", "Heng", ""], ["Luo", "Weihua", ""], ["Wang", "Kun", ""], ["Zhang", "Min", ""]]}, {"id": "1904.09108", "submitter": "Eric Laporte", "authors": "Maria Jos\\'e Finatto (UFRGS), Oto Vale (UFSCar), Eric Laporte (LIGM)", "title": "Recognizing the vocabulary of Brazilian popular newspapers with a\n  free-access computational dictionary", "comments": "English version, p. 67-85. Vers{\\~a}o em portugu{\\^e}s, p. 63-80", "journal-ref": "Alfa Revista de Linguistica, 2019, 63 (1), pp.67-85", "doi": "10.1590/1981-5794-1904-3", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report an experiment to check the identification of a set of words in\npopular written Portuguese with two versions of a computational dictionary of\nBrazilian Portuguese, DELAF PB 2004 and DELAF PB 2015. This dictionary is\nfreely available for use in linguistic analyses of Brazilian Portuguese and\nother researches, which justifies critical study. The vocabulary comes from the\nPorPopular corpus, made of popular newspapers Di{\\'a}rio Ga{\\'u}cho (DG) and\nMassa! (MA). From DG, we retained a set of texts with 984.465 words (tokens),\npublished in 2008, with the spelling used before the Portuguese Language\nOrthographic Agreement adopted in 2009. From MA, we examined papers of 2012,\n2014 e 2015, with 215.776 words (tokens), all with the new spelling. The\nchecking involved: a) generating lists of words (types) occurring in DG and MA;\nb) comparing them with the entry lists of both versions of DELAF PB; c)\nassessing the coverage of this vocabulary; d) proposing ways of incorporating\nthe items not covered. The results of the work show that an average of 19% of\nthe types in DG were not found in DELAF PB 2004 or 2015. In MA, this average is\n13%. Switching versions of the dictionary affected slightly the performance in\nrecognizing the words.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 07:59:25 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Finatto", "Maria Jos\u00e9", "", "UFRGS"], ["Vale", "Oto", "", "UFSCar"], ["Laporte", "Eric", "", "LIGM"]]}, {"id": "1904.09122", "submitter": "Soufian Jebbara", "authors": "Soufian Jebbara and Philipp Cimiano", "title": "Zero-Shot Cross-Lingual Opinion Target Extraction", "comments": "Proceedings of the 2019 Conference of the North American Chapter of\n  the Association for Computational Linguistics: Human Language Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aspect-based sentiment analysis involves the recognition of so called opinion\ntarget expressions (OTEs). To automatically extract OTEs, supervised learning\nalgorithms are usually employed which are trained on manually annotated\ncorpora. The creation of these corpora is labor-intensive and sufficiently\nlarge datasets are therefore usually only available for a very narrow selection\nof languages and domains. In this work, we address the lack of available\nannotated data for specific languages by proposing a zero-shot cross-lingual\napproach for the extraction of opinion target expressions. We leverage\nmultilingual word embeddings that share a common vector space across various\nlanguages and incorporate these into a convolutional neural network\narchitecture for OTE extraction. Our experiments with 5 languages give\npromising results: We can successfully train a model on annotated data of a\nsource language and perform accurate prediction on a target language without\never using any annotated samples in that target language. Depending on the\nsource and target language pairs, we reach performances in a zero-shot regime\nof up to 77% of a model trained on target language data. Furthermore, we can\nincrease this performance up to 87% of a baseline model trained on target\nlanguage data by performing cross-lingual learning from multiple source\nlanguages.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 08:59:13 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Jebbara", "Soufian", ""], ["Cimiano", "Philipp", ""]]}, {"id": "1904.09131", "submitter": "Antonin Delpeuch", "authors": "Antonin Delpeuch", "title": "OpenTapioca: Lightweight Entity Linking for Wikidata", "comments": "to appear in proceedings of the Wikidata Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simple Named Entity Linking system that can be trained from\nWikidata only. This demonstrates the strengths and weaknesses of this data\nsource for this task and provides an easily reproducible baseline to compare\nother systems against. Our model is lightweight to train, to run and to keep\nsynchronous with Wikidata in real time.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 09:44:22 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 17:50:32 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Delpeuch", "Antonin", ""]]}, {"id": "1904.09223", "submitter": "Yu Sun", "authors": "Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang,\n  Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu", "title": "ERNIE: Enhanced Representation through Knowledge Integration", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel language representation model enhanced by knowledge called\nERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the\nmasking strategy of BERT, ERNIE is designed to learn language representation\nenhanced by knowledge masking strategies, which includes entity-level masking\nand phrase-level masking. Entity-level strategy masks entities which are\nusually composed of multiple words.Phrase-level strategy masks the whole phrase\nwhich is composed of several words standing together as a conceptual\nunit.Experimental results show that ERNIE outperforms other baseline methods,\nachieving new state-of-the-art results on five Chinese natural language\nprocessing tasks including natural language inference, semantic similarity,\nnamed entity recognition, sentiment analysis and question answering. We also\ndemonstrate that ERNIE has more powerful knowledge inference capacity on a\ncloze test.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 15:10:56 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Sun", "Yu", ""], ["Wang", "Shuohuan", ""], ["Li", "Yukun", ""], ["Feng", "Shikun", ""], ["Chen", "Xuyi", ""], ["Zhang", "Han", ""], ["Tian", "Xin", ""], ["Zhu", "Danxiang", ""], ["Tian", "Hao", ""], ["Wu", "Hua", ""]]}, {"id": "1904.09234", "submitter": "Adam Rambousek", "authors": "Adam Rambousek, Harry Parkin, Ales Horak", "title": "Software Tools for Big Data Resources in Family Names Dictionaries", "comments": "This is an Accepted Manuscript of an article published by Taylor &\n  Francis in Names on 09 Apr 2018, available online:\n  https://www.tandfonline.com/doi/full/10.1080/00277738.2018.1453276", "journal-ref": "Names, 66:4, 246-255 (2018)", "doi": "10.1080/00277738.2018.1453276", "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and development of specific software tools\nused during the creation of Family Names in Britain and Ireland (FaNBI)\nresearch project, started by the University of the West of England in 2010 and\nfinished successfully in 2016. First, the overview of the project and\nmethodology is provided. Next section contains the description of dictionary\nmanagement tools and software tools to combine input data resources.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:25:40 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Rambousek", "Adam", ""], ["Parkin", "Harry", ""], ["Horak", "Ales", ""]]}, {"id": "1904.09286", "submitter": "Nitish Shirish Keskar", "authors": "Nitish Shirish Keskar, Bryan McCann, Caiming Xiong and Richard Socher", "title": "Unifying Question Answering, Text Classification, and Regression via\n  Span Extraction", "comments": "updating paper to also include regression tasks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even as pre-trained language encoders such as BERT are shared across many\ntasks, the output layers of question answering, text classification, and\nregression models are significantly different. Span decoders are frequently\nused for question answering, fixed-class, classification layers for text\nclassification, and similarity-scoring layers for regression tasks, We show\nthat this distinction is not necessary and that all three can be unified as\nspan extraction. A unified, span-extraction approach leads to superior or\ncomparable performance in supplementary supervised pre-trained, low-data, and\nmulti-task learning experiments on several question answering, text\nclassification, and regression benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 17:58:29 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 18:01:55 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Keskar", "Nitish Shirish", ""], ["McCann", "Bryan", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1904.09317", "submitter": "Christopher Kanan", "authors": "Kushal Kafle, Robik Shrestha, Christopher Kanan", "title": "Challenges and Prospects in Vision and Language Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language grounded image understanding tasks have often been proposed as a\nmethod for evaluating progress in artificial intelligence. Ideally, these tasks\nshould test a plethora of capabilities that integrate computer vision,\nreasoning, and natural language understanding. However, rather than behaving as\nvisual Turing tests, recent studies have demonstrated state-of-the-art systems\nare achieving good performance through flaws in datasets and evaluation\nprocedures. We review the current state of affairs and outline a path forward.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 19:04:12 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 22:10:33 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Kafle", "Kushal", ""], ["Shrestha", "Robik", ""], ["Kanan", "Christopher", ""]]}, {"id": "1904.09324", "submitter": "Omer Levy", "authors": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer", "title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most machine translation systems generate text autoregressively from left to\nright. We, instead, use a masked language modeling objective to train a model\nto predict any subset of the target words, conditioned on both the input text\nand a partially masked target translation. This approach allows for efficient\niterative decoding, where we first predict all of the target words\nnon-autoregressively, and then repeatedly mask out and regenerate the subset of\nwords that the model is least confident about. By applying this strategy for a\nconstant number of iterations, our model improves state-of-the-art performance\nlevels for non-autoregressive and parallel decoding translation models by over\n4 BLEU on average. It is also able to reach within about 1 BLEU point of a\ntypical left-to-right transformer model, while decoding significantly faster.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 19:53:01 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 16:31:39 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Ghazvininejad", "Marjan", ""], ["Levy", "Omer", ""], ["Liu", "Yinhan", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1904.09331", "submitter": "Xiang Ren", "authors": "Qinyuan Ye, Liyuan Liu, Maosen Zhang, Xiang Ren", "title": "Looking Beyond Label Noise: Shifted Label Distribution Matters in\n  Distantly Supervised Relation Extraction", "comments": "13 pages: 10 pages paper, 3 pages appendix. Appears at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there is a surge of interest in applying distant supervision\n(DS) to automatically generate training data for relation extraction (RE). In\nthis paper, we study the problem what limits the performance of DS-trained\nneural models, conduct thorough analyses, and identify a factor that can\ninfluence the performance greatly, shifted label distribution. Specifically, we\nfound this problem commonly exists in real-world DS datasets, and without\nspecial handing, typical DS-RE models cannot automatically adapt to this shift,\nthus achieving deteriorated performance. To further validate our intuition, we\ndevelop a simple yet effective adaptation method for DS-trained models, bias\nadjustment, which updates models learned over the source domain (i.e., DS\ntraining set) with a label distribution estimated on the target domain (i.e.,\ntest set). Experiments demonstrate that bias adjustment achieves consistent\nperformance gains on DS-trained models, especially on neural models, with an up\nto 23% relative F1 improvement, which verifies our assumptions. Our code and\ndata can be found at\n\\url{https://github.com/INK-USC/shifted-label-distribution}.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 20:23:27 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 01:00:12 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Ye", "Qinyuan", ""], ["Liu", "Liyuan", ""], ["Zhang", "Maosen", ""], ["Ren", "Xiang", ""]]}, {"id": "1904.09380", "submitter": "Harsh Trivedi", "authors": "Harsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, Niranjan\n  Balasubramanian", "title": "Repurposing Entailment for Multi-Hop Question Answering Tasks", "comments": "Accepted at NAACL'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) naturally reduces to an entailment problem, namely,\nverifying whether some text entails the answer to a question. However, for\nmulti-hop QA tasks, which require reasoning with multiple sentences, it remains\nunclear how best to utilize entailment models pre-trained on large scale\ndatasets such as SNLI, which are based on sentence pairs. We introduce Multee,\na general architecture that can effectively use entailment models for multi-hop\nQA tasks. Multee uses (i) a local module that helps locate important sentences,\nthereby avoiding distracting information, and (ii) a global module that\naggregates information by effectively incorporating importance weights.\nImportantly, we show that both modules can use entailment functions pre-trained\non a large scale NLI datasets. We evaluate performance on MultiRC and\nOpenBookQA, two multihop QA datasets. When using an entailment function\npre-trained on NLI datasets, Multee outperforms QA models trained only on the\ntarget QA datasets and the OpenAI transformer models. The code is available at\nhttps://github.com/StonyBrookNLP/multee.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 00:30:26 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Trivedi", "Harsh", ""], ["Kwon", "Heeyoung", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "1904.09407", "submitter": "Seung Hee Yang", "authors": "Seung Hee Yang, Minhwa Chung", "title": "Self-imitating Feedback Generation Using GAN for Computer-Assisted\n  Pronunciation Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-imitating feedback is an effective and learner-friendly method for\nnon-native learners in Computer-Assisted Pronunciation Training. Acoustic\ncharacteristics in native utterances are extracted and transplanted onto\nlearner's own speech input, and given back to the learner as a corrective\nfeedback. Previous works focused on speech conversion using prosodic\ntransplantation techniques based on PSOLA algorithm. Motivated by the visual\ndifferences found in spectrograms of native and non-native speeches, we\ninvestigated applying GAN to generate self-imitating feedback by utilizing\ngenerator's ability through adversarial training. Because this mapping is\nhighly under-constrained, we also adopt cycle consistency loss to encourage the\noutput to preserve the global structure, which is shared by native and\nnon-native utterances. Trained on 97,200 spectrogram images of short utterances\nproduced by native and non-native speakers of Korean, the generator is able to\nsuccessfully transform the non-native spectrogram input to a spectrogram with\nproperties of self-imitating feedback. Furthermore, the transformed spectrogram\nshows segmental corrections that cannot be obtained by prosodic\ntransplantation. Perceptual test comparing the self-imitating and correcting\nabilities of our method with the baseline PSOLA method shows that the\ngenerative approach with cycle consistency loss is promising.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 06:21:52 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yang", "Seung Hee", ""], ["Chung", "Minhwa", ""]]}, {"id": "1904.09408", "submitter": "Chenguang Wang", "authors": "Chenguang Wang, Mu Li, Alexander J. Smola", "title": "Language Models with Transformers", "comments": "12 pages, 7 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transformer architecture is superior to RNN-based models in computational\nefficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer\nmodels on various NLP tasks using pre-trained language models on large-scale\ncorpora. Surprisingly, these Transformer architectures are suboptimal for\nlanguage model itself. Neither self-attention nor the positional encoding in\nthe Transformer is able to efficiently incorporate the word-level sequential\ncontext crucial to language modeling.\n  In this paper, we explore effective Transformer architectures for language\nmodel, including adding additional LSTM layers to better capture the sequential\ncontext while still keeping the computation efficient. We propose Coordinate\nArchitecture Search (CAS) to find an effective architecture through iterative\nrefinement of the model. Experimental results on the PTB, WikiText-2, and\nWikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all\nproblems, i.e. on average an improvement of 12.0 perplexity units compared to\nstate-of-the-art LSTMs. The source code is publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 06:43:14 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 04:25:15 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Wang", "Chenguang", ""], ["Li", "Mu", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1904.09442", "submitter": "Chenhan Yuan", "authors": "Chenhan Yuan and Yi-Chin Huang", "title": "Personalized sentence generation using generative adversarial networks\n  with author-specific word usage", "comments": "slightly changed version of the paper accepted to the CICling 2019\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The author-specific word usage is a vital feature to let readers perceive the\nwriting style of the author. In this work, a personalized sentence generation\nmethod based on generative adversarial networks (GANs) is proposed to cope with\nthis issue. The frequently used function word and content word are incorporated\nnot only as the input features but also as the sentence structure constraint\nfor the GAN training. For the sentence generation with the related topics\ndecided by the user, the Named Entity Recognition (NER) information of the\ninput words is also used in the network training. We compared the proposed\nmethod with the GAN-based sentence generation methods, and the experimental\nresults showed that the generated sentences using our method are more similar\nto the original sentences of the same author based on the objective evaluation\nsuch as BLEU and SimHash score.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 12:56:39 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yuan", "Chenhan", ""], ["Huang", "Yi-Chin", ""]]}, {"id": "1904.09446", "submitter": "Haozhou Wang", "authors": "Haozhou Wang, James Henderson, Paola Merlo", "title": "Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual\n  Word Embeddings", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of words which map each word to a continuous\nvector have proven useful in capturing important linguistic information not\nonly in a single language but also across different languages. Current\nunsupervised adversarial approaches show that it is possible to build a mapping\nmatrix that align two sets of monolingual word embeddings together without high\nquality parallel data such as a dictionary or a sentence-aligned corpus.\nHowever, without post refinement, the performance of these methods' preliminary\nmapping is not good, leading to poor performance for typologically distant\nlanguages.\n  In this paper, we propose a weakly-supervised adversarial training method to\novercome this limitation, based on the intuition that mapping across languages\nis better done at the concept level than at the word level. We propose a\nconcept-based adversarial training method which for most languages improves the\nperformance of previous unsupervised adversarial methods, especially for\ntypologically distant language pairs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 13:19:24 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Wang", "Haozhou", ""], ["Henderson", "James", ""], ["Merlo", "Paola", ""]]}, {"id": "1904.09447", "submitter": "Martin Schmitt", "authors": "Martin Schmitt and Sahand Sharifzadeh and Volker Tresp and Hinrich\n  Sch\\\"utze", "title": "An Unsupervised Joint System for Text Generation from Knowledge Graphs\n  and Semantic Parsing", "comments": "Accepted as long paper to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore\nsupervised approaches to both graph-to-text generation and text-to-graph\nknowledge extraction (semantic parsing) will always suffer from a shortage of\ndomain-specific parallel graph-text data; at the same time, adapting a model\ntrained on a different domain is often impossible due to little or no overlap\nin entities and relations. This situation calls for an approach that (1) does\nnot need large amounts of annotated data and thus (2) does not need to rely on\ndomain adaptation techniques to work well in different domains. To this end, we\npresent the first approach to unsupervised text generation from KGs and show\nsimultaneously how it can be used for unsupervised semantic parsing. We\nevaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene\ngraphs from Visual Genome. Our system outperforms strong baselines for both\ntext$\\leftrightarrow$graph conversion tasks without any manual adaptation from\none dataset to the other. In additional experiments, we investigate the impact\nof using different unsupervised objectives.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 13:46:36 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 14:01:24 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 10:14:04 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2020 10:07:55 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Schmitt", "Martin", ""], ["Sharifzadeh", "Sahand", ""], ["Tresp", "Volker", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1904.09482", "submitter": "Xiaodong Liu", "authors": "Xiaodong Liu, Pengcheng He, Weizhu Chen and Jianfeng Gao", "title": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for\n  Natural Language Understanding", "comments": "8 pages, 2 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of knowledge distillation to improve a Multi-Task\nDeep Neural Network (MT-DNN) (Liu et al., 2019) for learning text\nrepresentations across multiple natural language understanding tasks. Although\nensemble learning can improve model performance, serving an ensemble of large\nDNNs such as MT-DNN can be prohibitively expensive. Here we apply the knowledge\ndistillation method (Hinton et al., 2015) in the multi-task learning setting.\nFor each task, we train an ensemble of different MT-DNNs (teacher) that\noutperforms any single model, and then train a single MT-DNN (student) via\nmulti-task learning to \\emph{distill} knowledge from these ensemble teachers.\nWe show that the distilled MT-DNN significantly outperforms the original MT-DNN\non 7 out of 9 GLUE tasks, pushing the GLUE benchmark (single model) to 83.7\\%\n(1.5\\% absolute improvement\\footnote{ Based on the GLUE leaderboard at\nhttps://gluebenchmark.com/leaderboard as of April 1, 2019.}). The code and\npre-trained models will be made publicly available at\nhttps://github.com/namisan/mt-dnn.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 19:11:00 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Liu", "Xiaodong", ""], ["He", "Pengcheng", ""], ["Chen", "Weizhu", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1904.09491", "submitter": "Guokan Shang", "authors": "Guokan Shang (1 and 2), Antoine Jean-Pierre Tixier (1), Michalis\n  Vazirgiannis (1 and 3), Jean-Pierre Lorr\\'e (2) ((1) \\'Ecole Polytechnique,\n  (2) Linagora, (3) AUEB)", "title": "Energy-based Self-attentive Learning of Abstractive Communities for\n  Spoken Language Understanding", "comments": "Update baselines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstractive community detection is an important spoken language understanding\ntask, whose goal is to group utterances in a conversation according to whether\nthey can be jointly summarized by a common abstractive sentence. This paper\nprovides a novel approach to this task. We first introduce a neural contextual\nutterance encoder featuring three types of self-attention mechanisms. We then\ntrain it using the siamese and triplet energy-based meta-architectures.\nExperiments on the AMI corpus show that our system outperforms multiple\nenergy-based and non-energy based baselines from the state-of-the-art. Code and\ndata are publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 20:01:52 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 02:18:29 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Shang", "Guokan", "", "1 and 2"], ["Tixier", "Antoine Jean-Pierre", "", "1 and 3"], ["Vazirgiannis", "Michalis", "", "1 and 3"], ["Lorr\u00e9", "Jean-Pierre", ""]]}, {"id": "1904.09521", "submitter": "Zhiyu Chen", "authors": "Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Yang Wang", "title": "Few-Shot NLG with Pre-Trained Language Model", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural-based end-to-end approaches to natural language generation (NLG) from\nstructured data or knowledge are data-hungry, making their adoption for\nreal-world applications difficult with limited data. In this work, we propose\nthe new task of \\textit{few-shot natural language generation}. Motivated by how\nhumans tend to summarize tabular data, we propose a simple yet effective\napproach and show that it not only demonstrates strong performance but also\nprovides good generalization across domains. The design of the model\narchitecture is based on two aspects: content selection from input data and\nlanguage modeling to compose coherent sentences, which can be acquired from\nprior knowledge. With just 200 training examples, across multiple domains, we\nshow that our approach achieves very reasonable performances and outperforms\nthe strongest baseline by an average of over 8.0 BLEU points improvement. Our\ncode and data can be found at \\url{https://github.com/czyssrs/Few-Shot-NLG}\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 00:42:22 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 05:45:49 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 07:27:42 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Chen", "Zhiyu", ""], ["Eavani", "Harini", ""], ["Chen", "Wenhu", ""], ["Liu", "Yinyin", ""], ["Wang", "William Yang", ""]]}, {"id": "1904.09535", "submitter": "Ming Gong", "authors": "Ming Gong, Linjun Shou, Wutao Lin, Zhijie Sang, Quanjia Yan, Ze Yang,\n  Feixiang Cheng and Daxin Jiang", "title": "NeuronBlocks: Building Your NLP DNN Models Like Playing Lego", "comments": "6 pages, 3 figures", "journal-ref": "EMNLP 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNN) have been widely employed in industry to address\nvarious Natural Language Processing (NLP) tasks. However, many engineers find\nit a big overhead when they have to choose from multiple frameworks, compare\ndifferent types of models, and understand various optimization mechanisms. An\nNLP toolkit for DNN models with both generality and flexibility can greatly\nimprove the productivity of engineers by saving their learning cost and guiding\nthem to find optimal solutions to their tasks. In this paper, we introduce\nNeuronBlocks\\footnote{Code: \\url{https://github.com/Microsoft/NeuronBlocks}}\n\\footnote{Demo: \\url{https://youtu.be/x6cOpVSZcdo}}, a toolkit encapsulating a\nsuite of neural network modules as building blocks to construct various DNN\nmodels with complex architecture. This toolkit empowers engineers to build,\ntrain, and test various NLP models through simple configuration of JSON files.\nThe experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003\ndemonstrate the effectiveness of NeuronBlocks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 03:11:24 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 14:11:55 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 12:17:27 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Gong", "Ming", ""], ["Shou", "Linjun", ""], ["Lin", "Wutao", ""], ["Sang", "Zhijie", ""], ["Yan", "Quanjia", ""], ["Yang", "Ze", ""], ["Cheng", "Feixiang", ""], ["Jiang", "Daxin", ""]]}, {"id": "1904.09537", "submitter": "Haitian Sun", "authors": "Haitian Sun, Tania Bedrax-Weiss, William W. Cohen", "title": "PullNet: Open Domain Question Answering with Iterative Retrieval on\n  Knowledge Bases and Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider open-domain queston answering (QA) where answers are drawn from\neither a corpus, a knowledge base (KB), or a combination of both of these. We\nfocus on a setting in which a corpus is supplemented with a large but\nincomplete KB, and on questions that require non-trivial (e.g., ``multi-hop'')\nreasoning. We describe PullNet, an integrated framework for (1) learning what\nto retrieve (from the KB and/or corpus) and (2) reasoning with this\nheterogeneous information to find the best answer. PullNet uses an {iterative}\nprocess to construct a question-specific subgraph that contains information\nrelevant to the question. In each iteration, a graph convolutional network\n(graph CNN) is used to identify subgraph nodes that should be expanded using\nretrieval (or ``pull'') operations on the corpus and/or KB. After the subgraph\nis complete, a similar graph CNN is used to extract the answer from the\nsubgraph. This retrieve-and-reason process allows us to answer multi-hop\nquestions using large KBs and corpora. PullNet is weakly supervised, requiring\nquestion-answer pairs but not gold inference paths. Experimentally PullNet\nimproves over the prior state-of-the art, and in the setting where a corpus is\nused with incomplete KB these improvements are often dramatic. PullNet is also\noften superior to prior systems in a KB-only setting or a text-only setting.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 03:49:09 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Sun", "Haitian", ""], ["Bedrax-Weiss", "Tania", ""], ["Cohen", "William W.", ""]]}, {"id": "1904.09540", "submitter": "Zihao Fu", "authors": "Zihao Fu, Yankai Lin, Zhiyuan Liu, Wai Lam", "title": "Fact Discovery from Knowledge Base via Facet Decomposition", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past few decades, knowledge bases (KBs) have experienced rapid\ngrowth. Nevertheless, most KBs still suffer from serious incompletion.\nResearchers proposed many tasks such as knowledge base completion and relation\nprediction to help build the representation of KBs. However, there are some\nissues unsettled towards enriching the KBs. Knowledge base completion and\nrelation prediction assume that we know two elements of the fact triples and we\nare going to predict the missing one. This assumption is too restricted in\npractice and prevents it from discovering new facts directly. To address this\nissue, we propose a new task, namely, fact discovery from knowledge base. This\ntask only requires that we know the head entity and the goal is to discover\nfacts associated with the head entity. To tackle this new problem, we propose a\nnovel framework that decomposes the discovery problem into several facet\ndiscovery components. We also propose a novel auto-encoder based facet\ncomponent to estimate some facets of the fact. Besides, we propose a feedback\nlearning component to share the information between each facet. We evaluate our\nframework using a benchmark dataset and the experimental results show that our\nframework achieves promising results. We also conduct extensive analysis of our\nframework in discovering different kinds of facts. The source code of this\npaper can be obtained from https://github.com/thunlp/FFD.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 04:13:55 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Fu", "Zihao", ""], ["Lin", "Yankai", ""], ["Liu", "Zhiyuan", ""], ["Lam", "Wai", ""]]}, {"id": "1904.09545", "submitter": "Jacob Andreas", "authors": "Jacob Andreas", "title": "Good-Enough Compositional Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple data augmentation protocol aimed at providing a\ncompositional inductive bias in conditional and unconditional sequence models.\nUnder this protocol, synthetic training examples are constructed by taking real\ntraining examples and replacing (possibly discontinuous) fragments with other\nfragments that appear in at least one similar environment. The protocol is\nmodel-agnostic and useful for a variety of tasks. Applied to neural\nsequence-to-sequence models, it reduces error rate by as much as 87% on\ndiagnostic tasks from the SCAN dataset and 16% on a semantic parsing task.\nApplied to n-gram language models, it reduces perplexity by roughly 1% on small\ncorpora in several languages.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 05:54:30 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 14:24:39 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 22:57:16 GMT"}, {"version": "v4", "created": "Tue, 19 May 2020 12:38:22 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Andreas", "Jacob", ""]]}, {"id": "1904.09552", "submitter": "Grace Lee", "authors": "Grace E. Lee and Aixin Sun", "title": "Understanding the Stability of Medical Concept Embeddings", "comments": "v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequency is one of the major factors for training quality word embeddings.\nSeveral work has recently discussed the stability of word embeddings in general\ndomain and suggested factors influencing the stability. In this work, we\nconduct a detailed analysis on the stability of concept embeddings in medical\ndomain, particularly the relation with concept frequency. The analysis reveals\nthe surprising high stability of low-frequency concepts: low-frequency (<100)\nconcepts have the same high stability as high-frequency (>1000) concepts. To\ndevelop a deeper understanding of this finding, we propose a new factor, the\nnoisiness of context words, which influences the stability of medical concept\nembeddings, regardless of frequency. We evaluate the proposed factor by showing\nthe linear correlation with the stability of medical concept embeddings. The\ncorrelations are clear and consistent with various groups of medical concepts.\nBased on the linear relations, we make suggestions on ways to adjust the\nnoisiness of context words for the improvement of stability. Finally, we\ndemonstrate that the proposed factor extends to the word embedding stability in\ngeneral domain.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 07:07:48 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 07:07:36 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Lee", "Grace E.", ""], ["Sun", "Aixin", ""]]}, {"id": "1904.09557", "submitter": "Grace Lee", "authors": "Grace E. Lee and Aixin Sun", "title": "A Study on Agreement in PICO Span Annotations", "comments": "Accepted in SIGIR 2019 (Short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In evidence-based medicine, relevance of medical literature is determined by\npredefined relevance conditions. The conditions are defined based on PICO\nelements, namely, Patient, Intervention, Comparator, and Outcome. Hence, PICO\nannotations in medical literature are essential for automatic relevant document\nfiltering. However, defining boundaries of text spans for PICO elements is not\nstraightforward. In this paper, we study the agreement of PICO annotations made\nby multiple human annotators, including both experts and non-experts.\nAgreements are estimated by a standard span agreement (i.e., matching both\nlabels and boundaries of text spans), and two types of relaxed span agreement\n(i.e., matching labels without guaranteeing matching boundaries of spans).\nBased on the analysis, we report two observations: (i) Boundaries of PICO span\nannotations by individual human annotators are very diverse. (ii) Despite the\ndisagreement in span boundaries, general areas of the span annotations are\nbroadly agreed by annotators. Our results suggest that applying a standard\nagreement alone may undermine the agreement of PICO spans, and adopting both a\nstandard and a relaxed agreements is more suitable for PICO span evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 07:30:35 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Lee", "Grace E.", ""], ["Sun", "Aixin", ""]]}, {"id": "1904.09585", "submitter": "Serhii Havrylov", "authors": "Zhifeng Hu, Serhii Havrylov, Ivan Titov, Shay B. Cohen", "title": "Obfuscation for Privacy-preserving Syntactic Parsing", "comments": "Accepted to IWPT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of homomorphic encryption is to encrypt data such that another party\ncan operate on it without being explicitly exposed to the content of the\noriginal data. We introduce an idea for a privacy-preserving transformation on\nnatural language data, inspired by homomorphic encryption. Our primary tool is\n{\\em obfuscation}, relying on the properties of natural language. Specifically,\na given English text is obfuscated using a neural model that aims to preserve\nthe syntactic relationships of the original sentence so that the obfuscated\nsentence can be parsed instead of the original one. The model works at the word\nlevel, and learns to obfuscate each word separately by changing it into a new\nword that has a similar syntactic role. The text obfuscated by our model leads\nto better performance on three syntactic parsers (two dependency and one\nconstituency parsers) in comparison to an upper-bound random substitution\nbaseline. More specifically, the results demonstrate that as more terms are\nobfuscated (by their part of speech), the substitution upper bound\nsignificantly degrades, while the neural model maintains a relatively high\nperforming parser. All of this is done without much sacrifice of privacy\ncompared to the random substitution upper bound. We also further analyze the\nresults, and discover that the substituted words have similar syntactic\nproperties, but different semantic content, compared to the original words.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 12:09:39 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 09:38:58 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Hu", "Zhifeng", ""], ["Havrylov", "Serhii", ""], ["Titov", "Ivan", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1904.09636", "submitter": "Ming Gong", "authors": "Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, Daxin Jiang", "title": "Model Compression with Multi-Task Knowledge Distillation for Web-scale\n  Question Answering System", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep pre-training and fine-tuning models (like BERT, OpenAI GPT) have\ndemonstrated excellent results in question answering areas. However, due to the\nsheer amount of model parameters, the inference speed of these models is very\nslow. How to apply these complex models to real business scenarios becomes a\nchallenging but practical problem. Previous works often leverage model\ncompression approaches to resolve this problem. However, these methods usually\ninduce information loss during the model compression procedure, leading to\nincomparable results between compressed model and the original model. To tackle\nthis challenge, we propose a Multi-task Knowledge Distillation Model (MKDM for\nshort) for web-scale Question Answering system, by distilling knowledge from\nmultiple teacher models to a light-weight student model. In this way, more\ngeneralized knowledge can be transferred. The experiment results show that our\nmethod can significantly outperform the baseline methods and even achieve\ncomparable results with the original teacher models, along with significant\nspeedup of model inference.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 17:46:24 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yang", "Ze", ""], ["Shou", "Linjun", ""], ["Gong", "Ming", ""], ["Lin", "Wutao", ""], ["Jiang", "Daxin", ""]]}, {"id": "1904.09646", "submitter": "Zaixiang Zheng", "authors": "Zaixiang Zheng, Shujian Huang, Zhaopeng Tu, Xin-Yu Dai, Jiajun Chen", "title": "Dynamic Past and Future for Neural Machine Translation", "comments": "Camera-ready version. Accepted to EMNLP 2019 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have shown that neural machine translation (NMT) models can\nbenefit from explicitly modeling translated (Past) and untranslated (Future) to\ngroups of translated and untranslated contents through parts-to-wholes\nassignment. The assignment is learned through a novel variant of\nrouting-by-agreement mechanism (Sabour et al., 2017), namely {\\em Guided\nDynamic Routing}, where the translating status at each decoding step {\\em\nguides} the routing process to assign each source word to its associated group\n(i.e., translated or untranslated content) represented by a capsule, enabling\ntranslation to be made from holistic context. Experiments show that our\napproach achieves substantial improvements over both RNMT and Transformer by\nproducing more adequate translations. Extensive analysis demonstrates that our\nmethod is highly interpretable, which is able to recognize the translated and\nuntranslated contents as expected.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 19:07:07 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 23:20:14 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Zheng", "Zaixiang", ""], ["Huang", "Shujian", ""], ["Tu", "Zhaopeng", ""], ["Dai", "Xin-Yu", ""], ["Chen", "Jiajun", ""]]}, {"id": "1904.09675", "submitter": "Tianyi Zhang", "authors": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, Yoav\n  Artzi", "title": "BERTScore: Evaluating Text Generation with BERT", "comments": "Code available at https://github.com/Tiiiger/bert_score; To appear in\n  ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose BERTScore, an automatic evaluation metric for text generation.\nAnalogously to common metrics, BERTScore computes a similarity score for each\ntoken in the candidate sentence with each token in the reference sentence.\nHowever, instead of exact matches, we compute token similarity using contextual\nembeddings. We evaluate using the outputs of 363 machine translation and image\ncaptioning systems. BERTScore correlates better with human judgments and\nprovides stronger model selection performance than existing metrics. Finally,\nwe use an adversarial paraphrase detection task to show that BERTScore is more\nrobust to challenging examples when compared to existing metrics.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 23:08:53 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 16:52:00 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 18:59:28 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Zhang", "Tianyi", ""], ["Kishore", "Varsha", ""], ["Wu", "Felix", ""], ["Weinberger", "Kilian Q.", ""], ["Artzi", "Yoav", ""]]}, {"id": "1904.09678", "submitter": "Ehsaneddin Asgari", "authors": "Ehsaneddin Asgari and Fabienne Braune and Benjamin Roth and Christoph\n  Ringlstetter and Mohammad R.K. Mofrad", "title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce UniSent universal sentiment lexica for $1000+$\nlanguages. Sentiment lexica are vital for sentiment analysis in absence of\ndocument-level annotations, a very common scenario for low-resource languages.\nTo the best of our knowledge, UniSent is the largest sentiment resource to date\nin terms of the number of covered languages, including many low resource ones.\nIn this work, we use a massively parallel Bible corpus to project sentiment\ninformation from English to other languages for sentiment analysis on Twitter\ndata. We introduce a method called DomDrift to mitigate the huge domain\nmismatch between Bible and Twitter by a confidence weighting scheme that uses\ndomain-specific embeddings to compare the nearest neighbors for a candidate\nsentiment word in the source (Bible) and target (Twitter) domain. We evaluate\nthe quality of UniSent in a subset of languages for which manually created\nground truth was available, Macedonian, Czech, German, Spanish, and French. We\nshow that the quality of UniSent is comparable to manually created sentiment\nresources when it is used as the sentiment seed for the task of word sentiment\nprediction on top of embedding representations. In addition, we show that\nemoticon sentiments could be reliably predicted in the Twitter domain using\nonly UniSent and monolingual embeddings in German, Spanish, French, and\nItalian. With the publication of this paper, we release the UniSent sentiment\nlexica.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 23:37:30 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 15:14:55 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Asgari", "Ehsaneddin", ""], ["Braune", "Fabienne", ""], ["Roth", "Benjamin", ""], ["Ringlstetter", "Christoph", ""], ["Mofrad", "Mohammad R. K.", ""]]}, {"id": "1904.09679", "submitter": "Kai Sun", "authors": "Kai Sun, Dian Yu, Dong Yu, Claire Cardie", "title": "Investigating Prior Knowledge for Challenging Chinese Machine Reading\n  Comprehension", "comments": "To appear in TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading comprehension tasks require a machine reader to answer\nquestions relevant to the given document. In this paper, we present the first\nfree-form multiple-Choice Chinese machine reading Comprehension dataset (C^3),\ncontaining 13,369 documents (dialogues or more formally written mixed-genre\ntexts) and their associated 19,577 multiple-choice free-form questions\ncollected from Chinese-as-a-second-language examinations.\n  We present a comprehensive analysis of the prior knowledge (i.e., linguistic,\ndomain-specific, and general world knowledge) needed for these real-world\nproblems. We implement rule-based and popular neural methods and find that\nthere is still a significant performance gap between the best performing model\n(68.5%) and human readers (96.0%), especially on problems that require prior\nknowledge. We further study the effects of distractor plausibility and data\naugmentation based on translated relevant datasets for English on model\nperformance. We expect C^3 to present great challenges to existing systems as\nanswering 86.8% of questions requires both knowledge within and beyond the\naccompanying document, and we hope that C^3 can serve as a platform to study\nhow to leverage various kinds of prior knowledge to better understand a given\nwritten or orally oriented text. C^3 is available at https://dataset.org/c3/.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 23:49:02 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 23:30:18 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 16:44:40 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Sun", "Kai", ""], ["Yu", "Dian", ""], ["Yu", "Dong", ""], ["Cardie", "Claire", ""]]}, {"id": "1904.09688", "submitter": "Dietrich Trautmann", "authors": "Dietrich Trautmann, Johannes Daxenberger, Christian Stab, Hinrich\n  Sch\\\"utze, Iryna Gurevych", "title": "Fine-Grained Argument Unit Recognition and Classification", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work has commonly defined argument retrieval from heterogeneous\ndocument collections as a sentence-level classification task. Consequently,\nargument retrieval suffers both from low recall and from sentence segmentation\nerrors making it difficult for humans and machines to consume the arguments. In\nthis work, we argue that the task should be performed on a more fine-grained\nlevel of sequence labeling. For this, we define the task as Argument Unit\nRecognition and Classification (AURC). We present a dataset of arguments from\nheterogeneous sources annotated as spans of tokens within a sentence, as well\nas with a corresponding stance. We show that and how such difficult argument\nannotations can be effectively collected through crowdsourcing with high\ninterannotator agreement. The new benchmark, AURC-8, contains up to 15% more\narguments per topic as compared to annotations on the sentence level. We\nidentify a number of methods targeted at AURC sequence labeling, achieving\nclose to human performance on known domains. Further analysis also reveals\nthat, contrary to previous approaches, our methods are more robust against\nsentence segmentation errors. We publicly release our code and the AURC-8\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 00:55:37 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 06:02:37 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 09:04:13 GMT"}, {"version": "v4", "created": "Thu, 21 Nov 2019 12:58:47 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Trautmann", "Dietrich", ""], ["Daxenberger", "Johannes", ""], ["Stab", "Christian", ""], ["Sch\u00fctze", "Hinrich", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1904.09705", "submitter": "Yu-Ping Ruan", "authors": "Yu-Ping Ruan, Xiaodan Zhu, Zhen-Hua Ling, Zhan Shi, Quan Liu and Si\n  Wei", "title": "Exploring Unsupervised Pretraining and Sentence Structure Modelling for\n  Winograd Schema Challenge", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Winograd Schema Challenge (WSC) was proposed as an AI-hard problem in testing\ncomputers' intelligence on common sense representation and reasoning. This\npaper presents the new state-of-theart on WSC, achieving an accuracy of 71.1%.\nWe demonstrate that the leading performance benefits from jointly modelling\nsentence structures, utilizing knowledge learned from cutting-edge pretraining\nmodels, and performing fine-tuning. We conduct detailed analyses, showing that\nfine-tuning is critical for achieving the performance, but it helps more on the\nsimpler associative problems. Modelling sentence dependency structures,\nhowever, consistently helps on the harder non-associative subset of WSC.\nAnalysis also shows that larger fine-tuning datasets yield better performances,\nsuggesting the potential benefit of future work on annotating more Winograd\nschema sentences.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 03:00:40 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Ruan", "Yu-Ping", ""], ["Zhu", "Xiaodan", ""], ["Ling", "Zhen-Hua", ""], ["Shi", "Zhan", ""], ["Liu", "Quan", ""], ["Wei", "Si", ""]]}, {"id": "1904.09708", "submitter": "Jacob Russin", "authors": "Jake Russin, Jason Jo, Randall C. O'Reilly, Yoshua Bengio", "title": "Compositional generalization in a deep seq2seq model by separating\n  syntax and semantics", "comments": "18 pages, 15 figures, preprint version of submission to NeurIPS 2019,\n  under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard methods in deep learning for natural language processing fail to\ncapture the compositional structure of human language that allows for\nsystematic generalization outside of the training distribution. However, human\nlearners readily generalize in this way, e.g. by applying known grammatical\nrules to novel words. Inspired by work in neuroscience suggesting separate\nbrain systems for syntactic and semantic processing, we implement a\nmodification to standard approaches in neural machine translation, imposing an\nanalogous separation. The novel model, which we call Syntactic Attention,\nsubstantially outperforms standard methods in deep learning on the SCAN\ndataset, a compositional generalization task, without any hand-engineered\nfeatures or additional supervision. Our work suggests that separating syntactic\nfrom semantic learning may be a useful heuristic for capturing compositional\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 03:12:09 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 16:05:35 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 20:59:12 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Russin", "Jake", ""], ["Jo", "Jason", ""], ["O'Reilly", "Randall C.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1904.09720", "submitter": "Arindam Mitra", "authors": "Arindam Mitra, Ishan Shrivastava, Chitta Baral", "title": "Understanding Roles and Entities: Datasets and Models for Natural\n  Language Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new datasets and a novel attention mechanism for Natural\nLanguage Inference (NLI). Existing neural NLI models, even though when trained\non existing large datasets, do not capture the notion of entity and role well\nand often end up making mistakes such as \"Peter signed a deal\" can be inferred\nfrom \"John signed a deal\". The two datasets have been developed to mitigate\nsuch issues and make the systems better at understanding the notion of\n\"entities\" and \"roles\". After training the existing architectures on the new\ndataset we observe that the existing architectures does not perform well on one\nof the new benchmark. We then propose a modification to the \"word-to-word\"\nattention function which has been uniformly reused across several popular NLI\narchitectures. The resulting architectures perform as well as their unmodified\ncounterparts on the existing benchmarks and perform significantly well on the\nnew benchmark for \"roles\" and \"entities\".\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:06:48 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Mitra", "Arindam", ""], ["Shrivastava", "Ishan", ""], ["Baral", "Chitta", ""]]}, {"id": "1904.09728", "submitter": "Maarten Sap", "authors": "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, Yejin Choi", "title": "SocialIQA: Commonsense Reasoning about Social Interactions", "comments": "the first two authors contributed equally; accepted to EMNLP 2019;\n  camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Social IQa, the first largescale benchmark for commonsense\nreasoning about social situations. Social IQa contains 38,000 multiple choice\nquestions for probing emotional and social intelligence in a variety of\neveryday situations (e.g., Q: \"Jordan wanted to tell Tracy a secret, so Jordan\nleaned towards Tracy. Why did Jordan do this?\" A: \"Make sure no one else could\nhear\"). Through crowdsourcing, we collect commonsense questions along with\ncorrect and incorrect answers about social interactions, using a new framework\nthat mitigates stylistic artifacts in incorrect answers by asking workers to\nprovide the right answer to a different but related question. Empirical results\nshow that our benchmark is challenging for existing question-answering models\nbased on pretrained language models, compared to human performance (>20% gap).\nNotably, we further establish Social IQa as a resource for transfer learning of\ncommonsense knowledge, achieving state-of-the-art performance on multiple\ncommonsense reasoning tasks (Winograd Schemas, COPA).\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:36:37 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 00:10:30 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 17:29:55 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Sap", "Maarten", ""], ["Rashkin", "Hannah", ""], ["Chen", "Derek", ""], ["LeBras", "Ronan", ""], ["Choi", "Yejin", ""]]}, {"id": "1904.09745", "submitter": "Nikita Kitaev", "authors": "Nikita Kitaev, Dan Klein", "title": "Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a constituency parsing algorithm that, like a supertagger, works\nby assigning labels to each word in a sentence. In order to maximally leverage\ncurrent neural architectures, the model scores each word's tags in parallel,\nwith minimal task-specific structure. After scoring, a left-to-right\nreconciliation phase extracts a tree in (empirically) linear time. Our parser\nachieves 95.4 F1 on the WSJ test set while also achieving substantial speedups\ncompared to current state-of-the-art parsers with comparable accuracies.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 06:57:43 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 01:54:49 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kitaev", "Nikita", ""], ["Klein", "Dan", ""]]}, {"id": "1904.09751", "submitter": "Ari Holtzman", "authors": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi", "title": "The Curious Case of Neural Text Degeneration", "comments": "Published in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite considerable advancements with deep neural language models, the\nenigma of neural text degeneration persists when these models are tested as\ntext generators. The counter-intuitive empirical observation is that even\nthough the use of likelihood as training objective leads to high quality models\nfor a broad range of language understanding tasks, using likelihood as a\ndecoding objective leads to text that is bland and strangely repetitive.\n  In this paper, we reveal surprising distributional differences between human\ntext and machine text. In addition, we find that decoding strategies alone can\ndramatically effect the quality of machine text, even when generated from\nexactly the same neural language model. Our findings motivate Nucleus Sampling,\na simple but effective method to draw the best out of neural generation. By\nsampling text from the dynamic nucleus of the probability distribution, which\nallows for diversity while effectively truncating the less reliable tail of the\ndistribution, the resulting text better demonstrates the quality of human text,\nyielding enhanced diversity without sacrificing fluency and coherence.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:17:18 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 21:56:30 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Holtzman", "Ari", ""], ["Buys", "Jan", ""], ["Du", "Li", ""], ["Forbes", "Maxwell", ""], ["Choi", "Yejin", ""]]}, {"id": "1904.09824", "submitter": "Zhuosheng Zhang", "authors": "Shu Jiang, Zhuosheng Zhang, Hai Zhao, Jiangtong Li, Yang Yang,\n  Bao-Liang Lu, Ning Xia", "title": "Judging Chemical Reaction Practicality From Positive Sample only\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical reaction practicality is the core task among all symbol intelligence\nbased chemical information processing, for example, it provides indispensable\nclue for further automatic synthesis route inference. Considering that chemical\nreactions have been represented in a language form, we propose a new solution\nto generally judge the practicality of organic reaction without considering\ncomplex quantum physical modeling or chemistry knowledge. While tackling the\npracticality judgment as a machine learning task from positive and negative\n(chemical reaction) samples, all existing studies have to carefully handle the\nserious insufficiency issue on the negative samples. We propose an\nauto-construction method to well solve the extensively existed long-term\ndifficulty. Experimental results show our model can effectively predict the\npracticality of chemical reactions, which achieves a high accuracy of 99.76\\%\non real large-scale chemical lab reaction practicality judgment.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 12:35:38 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Jiang", "Shu", ""], ["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""], ["Li", "Jiangtong", ""], ["Yang", "Yang", ""], ["Lu", "Bao-Liang", ""], ["Xia", "Ning", ""]]}, {"id": "1904.10151", "submitter": "Qi Wu", "authors": "Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang,\n  Chunhua Shen, and Anton van den Hengel", "title": "REVERIE: Remote Embodied Visual Referring Expression in Real Indoor\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the long-term challenges of robotics is to enable robots to interact\nwith humans in the visual world via natural language, as humans are visual\nanimals that communicate through language. Overcoming this challenge requires\nthe ability to perform a wide variety of complex tasks in response to\nmultifarious instructions from humans. In the hope that it might drive progress\ntowards more flexible and powerful human interactions with robots, we propose a\ndataset of varied and complex robot tasks, described in natural language, in\nterms of objects visible in a large set of real images. Given an instruction,\nsuccess requires navigating through a previously-unseen environment to identify\nan object. This represents a practical challenge, but one that closely reflects\none of the core visual problems in robotics. Several state-of-the-art\nvision-and-language navigation, and referring-expression models are tested to\nverify the difficulty of this new task, but none of them show promising results\nbecause there are many fundamental differences between our task and previous\nones. A novel Interactive Navigator-Pointer model is also proposed that\nprovides a strong baseline on the task. The proposed model especially achieves\nthe best performance on the unseen test split, but still leaves substantial\nroom for improvement compared to the human performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 04:45:28 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 01:38:43 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Qi", "Yuankai", ""], ["Wu", "Qi", ""], ["Anderson", "Peter", ""], ["Wang", "Xin", ""], ["Wang", "William Yang", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1904.10162", "submitter": "Tobias Kahse", "authors": "Tobias Kahse", "title": "Multi-Task Learning for Argumentation Mining", "comments": "Thesis for the M. Sc. Internet and Webbased Systems degree at\n  Technische Universit\\\"at Darmstadt (Germany)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning has recently become a very active field in deep learning\nresearch. In contrast to learning a single task in isolation, multiple tasks\nare learned at the same time, thereby utilizing the training signal of related\ntasks to improve the performance on the respective machine learning tasks.\nRelated work shows various successes in different domains when applying this\nparadigm and this thesis extends the existing empirical results by evaluating\nmulti-task learning in four different scenarios: argumentation mining,\nepistemic segmentation, argumentation component segmentation, and\ngrapheme-to-phoneme conversion. We show that multi-task learning can, indeed,\nimprove the performance compared to single-task learning in all these\nscenarios, but may also hurt the performance. Therefore, we investigate the\nreasons for successful and less successful applications of this paradigm and\nfind that dataset properties such as entropy or the size of the label inventory\nare good indicators for a potential multi-task learning success and that\nmulti-task learning is particularly useful if the task at hand suffers from\ndata sparsity, i.e. a lack of training data. Moreover, multi-task learning is\nparticularly effective for long input sequences in our experiments. We have\nobserved this trend in all evaluated scenarios. Finally, we develop a highly\nconfigurable and extensible sequence tagging framework which supports\nmulti-task learning to conduct our empirical experiments and to aid future\nresearch regarding the multi-task learning paradigm and natural language\nprocessing.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 05:58:54 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Kahse", "Tobias", ""]]}, {"id": "1904.10195", "submitter": "Hala Mulki", "authors": "Hala Mulki, Hatem Haddad, Mourad Gridach, Ismail Babaoglu", "title": "Empirical Evaluation of Leveraging Named Entities for Arabic Sentiment\n  Analysis", "comments": "7 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media reflects the public attitudes towards specific events. Events\nare often related to persons, locations or organizations, the so-called Named\nEntities. This can define Named Entities as sentiment-bearing components. In\nthis paper, we dive beyond Named Entities recognition to the exploitation of\nsentiment-annotated Named Entities in Arabic sentiment analysis. Therefore, we\ndevelop an algorithm to detect the sentiment of Named Entities based on the\nmajority of attitudes towards them. This enabled tagging Named Entities with\nproper tags and, thus, including them in a sentiment analysis framework of two\nmodels: supervised and lexicon-based. Both models were applied on datasets of\nmulti-dialectal content. The results revealed that Named Entities have no\nconsiderable impact on the supervised model, while employing them in the\nlexicon-based model improved the classification performance and outperformed\nmost of the baseline systems.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 08:28:11 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Mulki", "Hala", ""], ["Haddad", "Hatem", ""], ["Gridach", "Mourad", ""], ["Babaoglu", "Ismail", ""]]}, {"id": "1904.10281", "submitter": "Shuai Zhang", "authors": "Shuai Zhang and Yi Tay and Lina Yao and Qi Liu", "title": "Quaternion Knowledge Graph Embeddings", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we move beyond the traditional complex-valued representations,\nintroducing more expressive hypercomplex representations to model entities and\nrelations for knowledge graph embeddings. More specifically, quaternion\nembeddings, hypercomplex-valued embeddings with three imaginary components, are\nutilized to represent entities. Relations are modelled as rotations in the\nquaternion space. The advantages of the proposed approach are: (1) Latent\ninter-dependencies (between all components) are aptly captured with Hamilton\nproduct, encouraging a more compact interaction between entities and relations;\n(2) Quaternions enable expressive rotation in four-dimensional space and have\nmore degree of freedom than rotation in complex plane; (3) The proposed\nframework is a generalization of ComplEx on hypercomplex space while offering\nbetter geometrical interpretations, concurrently satisfying the key desiderata\nof relational representation learning (i.e., modeling symmetry, anti-symmetry\nand inversion). Experimental results demonstrate that our method achieves\nstate-of-the-art performance on four well-established knowledge graph\ncompletion benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 12:36:59 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 06:11:16 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 12:45:00 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Zhang", "Shuai", ""], ["Tay", "Yi", ""], ["Yao", "Lina", ""], ["Liu", "Qi", ""]]}, {"id": "1904.10294", "submitter": "Zihao Wang", "authors": "Zihao Wang, Datong Zhou, Yong Zhang, Hao Wu, Chenglong Bao", "title": "Wasserstein-Fisher-Rao Document Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fundamental problem of natural language processing, it is important to\nmeasure the distance between different documents. Among the existing methods,\nthe Word Mover's Distance (WMD) has shown remarkable success in document\nsemantic matching for its clear physical insight as a parameter-free model.\nHowever, WMD is essentially based on the classical Wasserstein metric, thus it\noften fails to robustly represent the semantic similarity between texts of\ndifferent lengths. In this paper, we apply the newly developed\nWasserstein-Fisher-Rao (WFR) metric from unbalanced optimal transport theory to\nmeasure the distance between different documents. The proposed WFR document\ndistance maintains the great interpretability and simplicity as WMD. We\ndemonstrate that the WFR document distance has significant advantages when\ncomparing the texts of different lengths. In addition, an accelerated Sinkhorn\nbased algorithm with GPU implementation has been developed for the fast\ncomputation of WFR distances. The KNN classification results on eight datasets\nhave shown its clear improvement over WMD.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 13:11:40 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 08:55:21 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wang", "Zihao", ""], ["Zhou", "Datong", ""], ["Zhang", "Yong", ""], ["Wu", "Hao", ""], ["Bao", "Chenglong", ""]]}, {"id": "1904.10419", "submitter": "Yue Yu", "authors": "Yue Yu and Yilun Zhu and Yang Liu and Yan Liu and Siyao Peng and\n  Mackenzie Gong and Amir Zeldes", "title": "GumDrop at the DISRPT2019 Shared Task: A Model Stacking Approach to\n  Discourse Unit Segmentation and Connective Detection", "comments": "Proceedings of Discourse Relation Parsing and Treebanking\n  (DISRPT2019)", "journal-ref": null, "doi": "10.18653/v1/W19-2717", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present GumDrop, Georgetown University's entry at the DISRPT\n2019 Shared Task on automatic discourse unit segmentation and connective\ndetection. Our approach relies on model stacking, creating a heterogeneous\nensemble of classifiers, which feed into a metalearner for each final task. The\nsystem encompasses three trainable component stacks: one for sentence\nsplitting, one for discourse unit segmentation and one for connective\ndetection. The flexibility of each ensemble allows the system to generalize\nwell to datasets of different sizes and with varying levels of homogeneity.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:55:04 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 15:38:18 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Yu", "Yue", ""], ["Zhu", "Yilun", ""], ["Liu", "Yang", ""], ["Liu", "Yan", ""], ["Peng", "Siyao", ""], ["Gong", "Mackenzie", ""], ["Zeldes", "Amir", ""]]}, {"id": "1904.10500", "submitter": "Eda Okur", "authors": "Eda Okur, Shachi H Kumar, Saurav Sahay, Asli Arslan Esme, Lama Nachman", "title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection\n  and Slot Filling from Passenger Utterances", "comments": "Accepted and presented as a full paper at 20th International\n  Conference on Computational Linguistics and Intelligent Text Processing\n  (CICLing 2019), April 7-13, 2019, La Rochelle, France", "journal-ref": "Springer LNCS Proceedings for CICLing 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding passenger intents and extracting relevant slots are important\nbuilding blocks towards developing contextual dialogue systems for natural\ninteractions in autonomous vehicles (AV). In this work, we explored AMIE\n(Automated-vehicle Multi-modal In-cabin Experience), the in-cabin agent\nresponsible for handling certain passenger-vehicle interactions. When the\npassengers give instructions to AMIE, the agent should parse such commands\nproperly and trigger the appropriate functionality of the AV system. In our\ncurrent explorations, we focused on AMIE scenarios describing usages around\nsetting or changing the destination and route, updating driving behavior or\nspeed, finishing the trip and other use-cases to support various natural\ncommands. We collected a multi-modal in-cabin dataset with multi-turn dialogues\nbetween the passengers and AMIE using a Wizard-of-Oz scheme via a realistic\nscavenger hunt game activity. After exploring various recent Recurrent Neural\nNetworks (RNN) based techniques, we introduced our own hierarchical joint\nmodels to recognize passenger intents along with relevant slots associated with\nthe action to be performed in AV scenarios. Our experimental results\noutperformed certain competitive baselines and achieved overall F1 scores of\n0.91 for utterance-level intent detection and 0.96 for slot filling tasks. In\naddition, we conducted initial speech-to-text explorations by comparing\nintent/slot models trained and tested on human transcriptions versus noisy\nAutomatic Speech Recognition (ASR) outputs. Finally, we compared the results\nwith single passenger rides versus the rides with multiple passengers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 19:13:51 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Okur", "Eda", ""], ["Kumar", "Shachi H", ""], ["Sahay", "Saurav", ""], ["Esme", "Asli Arslan", ""], ["Nachman", "Lama", ""]]}, {"id": "1904.10503", "submitter": "Michael Sigamani", "authors": "Cihan Dogan, Aimore Dutra, Adam Gara, Alfredo Gemma, Lei Shi, Michael\n  Sigamani, Ella Walters", "title": "Fine-Grained Named Entity Recognition using ELMo and Wikidata", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained Named Entity Recognition is a task whereby we detect and\nclassify entity mentions to a large set of types. These types can span diverse\ndomains such as finance, healthcare, and politics. We observe that when the\ntype set spans several domains the accuracy of the entity detection becomes a\nlimitation for supervised learning models. The primary reason being the lack of\ndatasets where entity boundaries are properly annotated, whilst covering a\nlarge spectrum of entity types. Furthermore, many named entity systems suffer\nwhen considering the categorization of fine grained entity types. Our work\nattempts to address these issues, in part, by combining state-of-the-art deep\nlearning models (ELMo) with an expansive knowledge base (Wikidata). Using our\nframework, we cross-validate our model on the 112 fine-grained entity types\nbased on the hierarchy given from the Wiki(gold) dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 19:18:26 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Dogan", "Cihan", ""], ["Dutra", "Aimore", ""], ["Gara", "Adam", ""], ["Gemma", "Alfredo", ""], ["Shi", "Lei", ""], ["Sigamani", "Michael", ""], ["Walters", "Ella", ""]]}, {"id": "1904.10610", "submitter": "Yu-Ping Ruan", "authors": "Yu-Ping Ruan, Zhen-Hua Ling, Quan Liu, Zhigang Chen, Nitin Indurkhya", "title": "Condition-Transforming Variational AutoEncoder for Conversation Response\n  Generation", "comments": "ICASSP 2019, oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new model, called condition-transforming variational\nautoencoder (CTVAE), to improve the performance of conversation response\ngeneration using conditional variational autoencoders (CVAEs). In conventional\nCVAEs , the prior distribution of latent variable z follows a multivariate\nGaussian distribution with mean and variance modulated by the input conditions.\nPrevious work found that this distribution tends to become condition\nindependent in practical application. In our proposed CTVAE model, the latent\nvariable z is sampled by performing a non-lineartransformation on the\ncombination of the input conditions and the samples from a\ncondition-independent prior distribution N (0; I). In our objective\nevaluations, the CTVAE model outperforms the CVAE model on fluency metrics and\nsurpasses a sequence-to-sequence (Seq2Seq) model on diversity metrics. In\nsubjective preference tests, our proposed CTVAE model performs significantly\nbetter than CVAE and Seq2Seq models on generating fluency, informative and\ntopic relevant responses.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 02:26:48 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Ruan", "Yu-Ping", ""], ["Ling", "Zhen-Hua", ""], ["Liu", "Quan", ""], ["Chen", "Zhigang", ""], ["Indurkhya", "Nitin", ""]]}, {"id": "1904.10622", "submitter": "Rohit Voleti", "authors": "Rohit Voleti, Stephanie Woolridge, Julie M. Liss, Melissa Milanovic,\n  Christopher R. Bowie, Visar Berisha", "title": "Objective Assessment of Social Skills Using Automated Language Analysis\n  for Identification of Schizophrenia and Bipolar Disorder", "comments": "Accepted to be presented at INTERSPEECH 2019 conference in Graz,\n  Austria. 4 pages + 1 page references. Two figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several studies have shown that speech and language features, automatically\nextracted from clinical interviews or spontaneous discourse, have diagnostic\nvalue for mental disorders such as schizophrenia and bipolar disorder. They\ntypically make use of a large feature set to train a classifier for\ndistinguishing between two groups of interest, i.e. a clinical and control\ngroup. However, a purely data-driven approach runs the risk of overfitting to a\nparticular data set, especially when sample sizes are limited. Here, we first\ndown-select the set of language features to a small subset that is related to a\nwell-validated test of functional ability, the Social Skills Performance\nAssessment (SSPA). This helps establish the concurrent validity of the selected\nfeatures. We use only these features to train a simple classifier to\ndistinguish between groups of interest. Linear regression reveals that a subset\nof language features can effectively model the SSPA, with a correlation\ncoefficient of 0.75. Furthermore, the same feature set can be used to build a\nstrong binary classifier to distinguish between healthy controls and a clinical\ngroup (AUC = 0.96) and also between patients within the clinical group with\nschizophrenia and bipolar I disorder (AUC = 0.83).\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 03:09:47 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 02:34:41 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Voleti", "Rohit", ""], ["Woolridge", "Stephanie", ""], ["Liss", "Julie M.", ""], ["Milanovic", "Melissa", ""], ["Bowie", "Christopher R.", ""], ["Berisha", "Visar", ""]]}, {"id": "1904.10635", "submitter": "Sarik Ghazarian", "authors": "Sarik Ghazarian, Johnny Tian-Zheng Wei, Aram Galstyan, Nanyun Peng", "title": "Better Automatic Evaluation of Open-Domain Dialogue Systems with\n  Contextualized Embeddings", "comments": "8 pages, 2 figures, NAACL 2019 Methods for Optimizing and Evaluating\n  Neural Language Generation (NeuralGen workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite advances in open-domain dialogue systems, automatic evaluation of\nsuch systems is still a challenging problem. Traditional reference-based\nmetrics such as BLEU are ineffective because there could be many valid\nresponses for a given context that share no common words with reference\nresponses. A recent work proposed Referenced metric and Unreferenced metric\nBlended Evaluation Routine (RUBER) to combine a learning-based metric, which\npredicts relatedness between a generated response and a given query, with\nreference-based metric; it showed high correlation with human judgments. In\nthis paper, we explore using contextualized word embeddings to compute more\naccurate relatedness scores, thus better evaluation metrics. Experiments show\nthat our evaluation metrics outperform RUBER, which is trained on static\nembeddings.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 04:16:44 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Ghazarian", "Sarik", ""], ["Wei", "Johnny Tian-Zheng", ""], ["Galstyan", "Aram", ""], ["Peng", "Nanyun", ""]]}, {"id": "1904.10637", "submitter": "Shuailong Liang", "authors": "Shuailong Liang, Olivia Nicol, Yue Zhang", "title": "Who Blames Whom in a Crisis? Detecting Blame Ties from News Articles\n  Using Neural Networks", "comments": "AAAI 2019 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blame games tend to follow major disruptions, be they financial crises,\nnatural disasters or terrorist attacks. To study how the blame game evolves and\nshapes the dominant crisis narratives is of great significance, as sense-making\nprocesses can affect regulatory outcomes, social hierarchies, and cultural\nnorms. However, it takes tremendous time and efforts for social scientists to\nmanually examine each relevant news article and extract the blame ties (A\nblames B). In this study, we define a new task, Blame Tie Extraction, and\nconstruct a new dataset related to the United States financial crisis\n(2007-2010) from The New York Times, The Wall Street Journal and USA Today. We\nbuild a Bi-directional Long Short-Term Memory (BiLSTM) network for contexts\nwhere the entities appear in and it learns to automatically extract such blame\nties at the document level. Leveraging the large unsupervised model such as\nGloVe and ELMo, our best model achieves an F1 score of 70% on the test set for\nblame tie extraction, making it a useful tool for social scientists to extract\nblame ties more efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 04:31:46 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Liang", "Shuailong", ""], ["Nicol", "Olivia", ""], ["Zhang", "Yue", ""]]}, {"id": "1904.10641", "submitter": "Hoang-Quoc Nguyen-Son", "authors": "Hoang-Quoc Nguyen-Son and Tran Phuong Thao and Seira Hidano and\n  Shinsaku Kiyomoto", "title": "Detecting Machine-Translated Paragraphs by Matching Similar Words", "comments": "12 pages, CICLING 2019 (LNCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-translated text plays an important role in modern life by smoothing\ncommunication from various communities using different languages. However,\nunnatural translation may lead to misunderstanding, a detector is thus needed\nto avoid the unfortunate mistakes. While a previous method measured the\nnaturalness of continuous words using a N-gram language model, another method\nmatched noncontinuous words across sentences but this method ignores such words\nin an individual sentence. We have developed a method matching similar words\nthroughout the paragraph and estimating the paragraph-level coherence, that can\nidentify machine-translated text. Experiment evaluates on 2000 English\nhuman-generated and 2000 English machine-translated paragraphs from German\nshowing that the coherence-based method achieves high performance (accuracy =\n87.0%; equal error rate = 13.0%). It is efficiently better than previous\nmethods (best accuracy = 72.4%; equal error rate = 29.7%). Similar experiments\non Dutch and Japanese obtain 89.2% and 97.9% accuracy, respectively. The\nresults demonstrate the persistence of the proposed method in various languages\nwith different resource levels.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 05:03:28 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Nguyen-Son", "Hoang-Quoc", ""], ["Thao", "Tran Phuong", ""], ["Hidano", "Seira", ""], ["Kiyomoto", "Shinsaku", ""]]}, {"id": "1904.10717", "submitter": "James Thorne", "authors": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit\n  Mittal", "title": "Generating Token-Level Explanations for Natural Language Inference", "comments": "Accepted at NAACL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of Natural Language Inference (NLI) is widely modeled as supervised\nsentence pair classification. While there has been a lot of work recently on\ngenerating explanations of the predictions of classifiers on a single piece of\ntext, there have been no attempts to generate explanations of classifiers\noperating on pairs of sentences. In this paper, we show that it is possible to\ngenerate token-level explanations for NLI without the need for training data\nexplicitly annotated for this purpose. We use a simple LSTM architecture and\nevaluate both LIME and Anchor explanations for this task. We compare these to a\nMultiple Instance Learning (MIL) method that uses thresholded attention make\ntoken-level predictions. The approach we present in this paper is a novel\nextension of zero-shot single-sentence tagging to sentence pairs for NLI. We\nconduct our experiments on the well-studied SNLI dataset that was recently\naugmented with manually annotation of the tokens that explain the entailment\nrelation. We find that our white-box MIL-based method, while orders of\nmagnitude faster, does not reach the same accuracy as the black-box methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 09:41:14 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Thorne", "James", ""], ["Vlachos", "Andreas", ""], ["Christodoulopoulos", "Christos", ""], ["Mittal", "Arpit", ""]]}, {"id": "1904.10743", "submitter": "Jiyu Chen", "authors": "Jiyu Chen, Karin Verspoor, Zenan Zhai", "title": "A bag-of-concepts model improves relation extraction in a narrow\n  knowledge domain with limited data", "comments": "To appear in Proceedings of the Student Research Workshop at the\n  North American Association for Computational Linguistics (NAACL) meeting 2019", "journal-ref": "In Proceedings of the Student Research Workshop at North American\n  Association for Computational Linguistics (NAACL) 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a traditional relation extraction task in the context\nof limited annotated data and a narrow knowledge domain. We explore this task\nwith a clinical corpus consisting of 200 breast cancer follow-up treatment\nletters in which 16 distinct types of relations are annotated. We experiment\nwith an approach to extracting typed relations called window-bounded\nco-occurrence (WBC), which uses an adjustable context window around entity\nmentions of a relevant type, and compare its performance with a more typical\nintra-sentential co-occurrence baseline. We further introduce a new\nbag-of-concepts (BoC) approach to feature engineering based on the\nstate-of-the-art word embeddings and word synonyms. We demonstrate the\ncompetitiveness of BoC by comparing with methods of higher complexity, and\nexplore its effectiveness on this small dataset.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 11:06:54 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Chen", "Jiyu", ""], ["Verspoor", "Karin", ""], ["Zhai", "Zenan", ""]]}, {"id": "1904.10760", "submitter": "Albert Haque", "authors": "Michelle Guo, Albert Haque, Prateek Verma", "title": "End-to-End Spoken Language Translation", "comments": "Technical Report. Stanford University, 2017. arXiv admin note: text\n  overlap with arXiv:1804.00047", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of spoken language understanding. We\npresent a method for translating spoken sentences from one language into spoken\nsentences in another language. Given spectrogram-spectrogram pairs, our model\ncan be trained completely from scratch to translate unseen sentences. Our\nmethod consists of a pyramidal-bidirectional recurrent network combined with a\nconvolutional network to output sentence-level spectrograms in the target\nlanguage. Empirically, our model achieves competitive performance with\nstate-of-the-art methods on multiple languages and can generalize to unseen\nspeakers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 05:27:12 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Guo", "Michelle", ""], ["Haque", "Albert", ""], ["Verma", "Prateek", ""]]}, {"id": "1904.10788", "submitter": "Seunghyun Yoon", "authors": "Seunghyun Yoon, Seokhyun Byun, Subhadeep Dey, Kyomin Jung", "title": "Speech Emotion Recognition Using Multi-hop Attention Mechanism", "comments": "5 pages, Accepted as a conference paper at ICASSP 2019 (oral\n  presentation)", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683483", "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in exploiting textual and acoustic data of\nan utterance for the speech emotion classification task. The baseline approach\nmodels the information from audio and text independently using two deep neural\nnetworks (DNNs). The outputs from both the DNNs are then fused for\nclassification. As opposed to using knowledge from both the modalities\nseparately, we propose a framework to exploit acoustic information in tandem\nwith lexical data. The proposed framework uses two bi-directional long\nshort-term memory (BLSTM) for obtaining hidden representations of the\nutterance. Furthermore, we propose an attention mechanism, referred to as the\nmulti-hop, which is trained to automatically infer the correlation between the\nmodalities. The multi-hop attention first computes the relevant segments of the\ntextual data corresponding to the audio signal. The relevant textual data is\nthen applied to attend parts of the audio signal. To evaluate the performance\nof the proposed system, experiments are performed in the IEMOCAP dataset.\nExperimental results show that the proposed technique outperforms the\nstate-of-the-art system by 6.5% relative improvement in terms of weighted\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 13:09:21 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 13:34:00 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Yoon", "Seunghyun", ""], ["Byun", "Seokhyun", ""], ["Dey", "Subhadeep", ""], ["Jung", "Kyomin", ""]]}, {"id": "1904.10820", "submitter": "Lisa Beinborn", "authors": "Lisa Beinborn and Rochelle Choenni", "title": "Semantic Drift in Multilingual Representations", "comments": "Almost final version. Paper will appear in the Computational\n  Linguistics Journal, Volume 46, Issue 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multilingual representations have mostly been evaluated based on their\nperformance on specific tasks. In this article, we look beyond engineering\ngoals and analyze the relations between languages in computational\nrepresentations. We introduce a methodology for comparing languages based on\ntheir organization of semantic concepts. We propose to conduct an adapted\nversion of representational similarity analysis of a selected set of concepts\nin computational multilingual representations. Using this analysis method, we\ncan reconstruct a phylogenetic tree that closely resembles those assumed by\nlinguistic experts. These results indicate that multilingual distributional\nrepresentations which are only trained on monolingual text and bilingual\ndictionaries preserve relations between languages without the need for any\netymological information. In addition, we propose a measure to identify\nsemantic drift between language families. We perform experiments on word-based\nand sentence-based multilingual models and provide both quantitative results\nand qualitative examples. Analyses of semantic drift in multilingual\nrepresentations can serve two purposes: they can indicate unwanted\ncharacteristics of the computational models and they provide a quantitative\nmeans to study linguistic phenomena across languages. The code is available at\nhttps://github.com/beinborn/SemanticDrift.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:55:42 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 09:03:43 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 13:31:59 GMT"}, {"version": "v4", "created": "Mon, 16 Nov 2020 19:48:06 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Beinborn", "Lisa", ""], ["Choenni", "Rochelle", ""]]}, {"id": "1904.10876", "submitter": "Valerio Lorini", "authors": "V. Lorini (European Commission, Joint Research Centre (JRC), Ispra,\n  Italy, Universitat Pompeu Fabra, Barcelona, Spain), C. Castillo (Universitat\n  Pompeu Fabra, Barcelona, Spain), F. Dottori (European Commission, Joint\n  Research Centre (JRC), Ispra, Italy), M. Kalas (KAJO, Bytca, Slovakia), D.\n  Nappo (European Commission, Joint Research Centre (JRC), Ispra, Italy), P.\n  Salamon (European Commission, Joint Research Centre (JRC), Ispra, Italy)", "title": "Integrating Social Media into a Pan-European Flood Awareness System: A\n  Multilingual Approach", "comments": "accepted at ISCRAM2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes a prototype system that integrates social media analysis\ninto the European Flood Awareness System (EFAS). This integration allows the\ncollection of social media data to be automatically triggered by flood risk\nwarnings determined by a hydro-meteorological model. Then, we adopt a\nmulti-lingual approach to find flood-related messages by employing two\nstate-of-the-art methodologies: language-agnostic word embeddings and\nlanguage-aligned word embeddings. Both approaches can be used to bootstrap a\nclassifier of social media messages for a new language with little or no\nlabeled data. Finally, we describe a method for selecting relevant and\nrepresentative messages and displaying them back in the interface of EFAS.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:40:14 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Lorini", "V.", "", "European Commission, Joint Research Centre"], ["Castillo", "C.", "", "Universitat\n  Pompeu Fabra, Barcelona, Spain"], ["Dottori", "F.", "", "European Commission, Joint\n  Research Centre"], ["Kalas", "M.", "", "KAJO, Bytca, Slovakia"], ["Nappo", "D.", "", "European Commission, Joint Research Centre"], ["Salamon", "P.", "", "European Commission, Joint Research Centre"]]}, {"id": "1904.10887", "submitter": "Andrew Yates", "authors": "Anna Tigunova, Andrew Yates, Paramita Mirza, Gerhard Weikum", "title": "Listening between the Lines: Learning Personal Attributes from\n  Conversations", "comments": "published in WWW'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain dialogue agents must be able to converse about many topics while\nincorporating knowledge about the user into the conversation. In this work we\naddress the acquisition of such knowledge, for personalization in downstream\nWeb applications, by extracting personal attributes from conversations. This\nproblem is more challenging than the established task of information extraction\nfrom scientific publications or Wikipedia articles, because dialogues often\ngive merely implicit cues about the speaker. We propose methods for inferring\npersonal attributes, such as profession, age or family status, from\nconversations using deep learning. Specifically, we propose several Hidden\nAttribute Models, which are neural networks leveraging attention mechanisms and\nembeddings. Our methods are trained on a per-predicate basis to output rankings\nof object values for a given subject-predicate combination (e.g., ranking the\ndoctor and nurse professions high when speakers talk about patients, emergency\nrooms, etc). Experiments with various conversational texts including Reddit\ndiscussions, movie scripts and a collection of crowdsourced personal dialogues\ndemonstrate the viability of our methods and their superior performance\ncompared to state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:54:46 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Tigunova", "Anna", ""], ["Yates", "Andrew", ""], ["Mirza", "Paramita", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1904.10947", "submitter": "Ankita Pasad", "authors": "Ankita Pasad, Bowen Shi, Herman Kamper, Karen Livescu", "title": "On the Contributions of Visual and Textual Supervision in Low-Resource\n  Semantic Speech Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that speech paired with images can be used to learn\nsemantically meaningful speech representations even without any textual\nsupervision. In real-world low-resource settings, however, we often have access\nto some transcribed speech. We study whether and how visual grounding is useful\nin the presence of varying amounts of textual supervision. In particular, we\nconsider the task of semantic speech retrieval in a low-resource setting. We\nuse a previously studied data set and task, where models are trained on images\nwith spoken captions and evaluated on human judgments of semantic relevance. We\npropose a multitask learning approach to leverage both visual and textual\nmodalities, with visual supervision in the form of keyword probabilities from\nan external tagger. We find that visual grounding is helpful even in the\npresence of textual supervision, and we analyze this effect over a range of\nsizes of transcribed data sets. With ~5 hours of transcribed speech, we obtain\n23% higher average precision when also using visual supervision.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 17:44:06 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 19:10:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Pasad", "Ankita", ""], ["Shi", "Bowen", ""], ["Kamper", "Herman", ""], ["Livescu", "Karen", ""]]}, {"id": "1904.10997", "submitter": "Nicholas Ruiz", "authors": "Nicholas Ruiz, Mattia Antonino Di Gangi, Nicola Bertoldi, Marcello\n  Federico", "title": "Assessing the Tolerance of Neural Machine Translation Systems Against\n  Speech Recognition Errors", "comments": "Interspeech 2017", "journal-ref": null, "doi": "10.21437/Interspeech.2017-1690", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation systems are conventionally trained on textual resources\nthat do not model phenomena that occur in spoken language. While the evaluation\nof neural machine translation systems on textual inputs is actively researched\nin the literature , little has been discovered about the complexities of\ntranslating spoken language data with neural models. We introduce and motivate\ninteresting problems one faces when considering the translation of automatic\nspeech recognition (ASR) outputs on neural machine translation (NMT) systems.\nWe test the robustness of sentence encoding approaches for NMT encoder-decoder\nmodeling, focusing on word-based over byte-pair encoding. We compare the\ntranslation of utterances containing ASR errors in state-of-the-art NMT\nencoder-decoder systems against a strong phrase-based machine translation\nbaseline in order to better understand which phenomena present in ASR outputs\nare better represented under the NMT framework than approaches that represent\ntranslation as a linear model.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 18:23:05 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Ruiz", "Nicholas", ""], ["Di Gangi", "Mattia Antonino", ""], ["Bertoldi", "Nicola", ""], ["Federico", "Marcello", ""]]}, {"id": "1904.11018", "submitter": "MohammadReza Davari", "authors": "MohammadReza Davari and Leila Kosseim and Tien D. Bui", "title": "Toponym Identification in Epidemiology Articles - A Deep Learning\n  Approach", "comments": "12 pages. pre-print from Proceedings of CICLing 2019: 20th\n  International Conference on Computational Linguistics and Intelligent Text\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing the spread of viruses, epidemiologists often need to identify\nthe location of infected hosts. This information can be found in public\ndatabases, such as GenBank, however, information provided in these databases\nare usually limited to the country or state level. More fine-grained\nlocalization information requires phylogeographers to manually read relevant\nscientific articles. In this work we propose an approach to automate the\nprocess of place name identification from medical (epidemiology) articles. The\nfocus of this paper is to propose a deep learning based model for toponym\ndetection and experiment with the use of external linguistic features and\ndomain specific information. The model was evaluated using a collection of 105\nepidemiology articles from PubMed Central provided by the recent SemEval task\n12. Our best detection model achieves an F1 score of $80.13\\%$, a significant\nimprovement compared to the state of the art of $69.84\\%$. These results\nunderline the importance of domain specific embedding as well as specific\nlinguistic features in toponym detection in medical journals.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 19:00:13 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 02:47:10 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Davari", "MohammadReza", ""], ["Kosseim", "Leila", ""], ["Bui", "Tien D.", ""]]}, {"id": "1904.11024", "submitter": "Nicholas Ruiz", "authors": "Nicholas Ruiz, Marcello Federico", "title": "Phonetically-Oriented Word Error Alignment for Speech Recognition Error\n  Analysis in Speech Translation", "comments": "IEEE Workshop on Automatic Speech Recognition and Understanding,\n  December 2015", "journal-ref": null, "doi": "10.1109/ASRU.2015.7404808", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variation to the commonly used Word Error Rate (WER) metric for\nspeech recognition evaluation which incorporates the alignment of phonemes, in\nthe absence of time boundary information. After computing the Levenshtein\nalignment on words in the reference and hypothesis transcripts, spans of\nadjacent errors are converted into phonemes with word and syllable boundaries\nand a phonetic Levenshtein alignment is performed. The aligned phonemes are\nrecombined into aligned words that adjust the word alignment labels in each\nerror region. We demonstrate that our Phonetically-Oriented Word Error Rate\n(POWER) yields similar scores to WER with the added advantages of better word\nalignments and the ability to capture one-to-many word alignments corresponding\nto homophonic errors in speech recognition hypotheses. These improved\nalignments allow us to better trace the impact of Levenshtein error types on\ndownstream tasks such as speech translation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 19:09:35 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Ruiz", "Nicholas", ""], ["Federico", "Marcello", ""]]}, {"id": "1904.11074", "submitter": "Aitor Arronte Alvarez", "authors": "Aitor Arronte-Alvarez, Francisco Gomez-Martin", "title": "An Attentional Neural Network Architecture for Folk Song Classification", "comments": "Accepted for ICMC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present an attentional neural network for folk song\nclassification. We introduce the concept of musical motif embedding, and show\nhow using melodic local context we are able to model monophonic folk song\nmotifs using the skipgram version of the word2vec algorithm. We use the motif\nembeddings to represent folk songs from Germany, China, and Sweden, and\nclassify them using an attentional neural network that is able to discern\nrelevant motifs in a song. The results show how the network obtains state of\nthe art accuracy in a completely unsupervised manner, and how motif embeddings\nproduce high quality motif representations from folk songs. We conjecture on\nthe advantages of this type of representation in large symbolic music corpora,\nand how it can be helpful in the musicological analysis of folk song\ncollections from different cultures and geographical areas.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 21:27:09 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Arronte-Alvarez", "Aitor", ""], ["Gomez-Martin", "Francisco", ""]]}, {"id": "1904.11469", "submitter": "Ewan Dunbar", "authors": "Ewan Dunbar, Robin Algayres, Julien Karadayi, Mathieu Bernard, Juan\n  Benjumea, Xuan-Nga Cao, Lucie Miskic, Charlotte Dugrain, Lucas Ondel, Alan W.\n  Black, Laurent Besacier, Sakriani Sakti, Emmanuel Dupoux", "title": "The Zero Resource Speech Challenge 2019: TTS without T", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Zero Resource Speech Challenge 2019, which proposes to build a\nspeech synthesizer without any text or phonetic labels: hence, TTS without T\n(text-to-speech without text). We provide raw audio for a target voice in an\nunknown language (the Voice dataset), but no alignment, text or labels.\nParticipants must discover subword units in an unsupervised way (using the Unit\nDiscovery dataset) and align them to the voice recordings in a way that works\nbest for the purpose of synthesizing novel utterances from novel speakers,\nsimilar to the target speaker's voice. We describe the metrics used for\nevaluation, a baseline system consisting of unsupervised subword unit discovery\nplus a standard TTS system, and a topline TTS using gold phoneme\ntranscriptions. We present an overview of the 19 submitted systems from 10\nteams and discuss the main results.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:29:16 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 09:10:45 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Dunbar", "Ewan", ""], ["Algayres", "Robin", ""], ["Karadayi", "Julien", ""], ["Bernard", "Mathieu", ""], ["Benjumea", "Juan", ""], ["Cao", "Xuan-Nga", ""], ["Miskic", "Lucie", ""], ["Dugrain", "Charlotte", ""], ["Ondel", "Lucas", ""], ["Black", "Alan W.", ""], ["Besacier", "Laurent", ""], ["Sakti", "Sakriani", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "1904.11473", "submitter": "Ivan Lerner", "authors": "Ivan Lerner, Nicolas Paris, Xavier Tannier", "title": "Terminologies augmented recurrent neural network model for clinical\n  named entity recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aimed to enhance the performance of a supervised model for clinical\nnamed-entity recognition (NER) using medical terminologies. In order to\nevaluate our system in French, we built a corpus for 5 types of clinical\nentities. We used a terminology-based system as baseline, built upon UMLS and\nSNOMED. Then, we evaluated a biGRU-CRF, and an hybrid system using the\nprediction of the terminology-based system as feature for the biGRU-CRF. In\nEnglish, we evaluated the NER systems on the i2b2-2009 Medication Challenge for\nDrug name recognition, which contained 8,573 entities for 268 documents. In\nFrench, we built APcNER, a corpus of 147 documents annotated for 5 entities\n(drug name, sign or symptom, disease or disorder, diagnostic procedure or lab\ntest and therapeutic procedure). We evaluated each NER systems using exact and\npartial match definition of F-measure for NER. The APcNER contains 4,837\nentities which took 28 hours to annotate, the inter-annotator agreement was\nacceptable for Drug name in exact match (85%) and acceptable for other entity\ntypes in non-exact match (>70%). For drug name recognition on both i2b2-2009\nand APcNER, the biGRU-CRF performed better that the terminology-based system,\nwith an exact-match F-measure of 91.1% versus 73% and 81.9% versus 75%\nrespectively. Moreover, the hybrid system outperformed the biGRU-CRF, with an\nexact-match F-measure of 92.2% versus 91.1% (i2b2-2009) and 88.4% versus 81.9%\n(APcNER). On APcNER corpus, the micro-average F-measure of the hybrid system on\nthe 5 entities was 69.5% in exact match, and 84.1% in non-exact match. APcNER\nis a French corpus for clinical-NER of five type of entities which covers a\nlarge variety of document types. Extending supervised model with terminology\nallowed for an easy performance gain, especially in low regimes of entities,\nand established near state of the art results on the i2b2-2009 corpus.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:37:28 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 09:23:41 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Lerner", "Ivan", ""], ["Paris", "Nicolas", ""], ["Tannier", "Xavier", ""]]}, {"id": "1904.11475", "submitter": "Ilya Gusev", "authors": "Ilya Gusev", "title": "Importance of Copying Mechanism for News Headline Generation", "comments": null, "journal-ref": "Computational Linguistics and Intellectual Technologies, Papers\n  from the Annual International Conference \"Dialogue\" (2019) Issue 18, 229-236", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News headline generation is an essential problem of text summarization\nbecause it is constrained, well-defined, and is still hard to solve. Models\nwith a limited vocabulary can not solve it well, as new named entities can\nappear regularly in the news and these entities often should be in the\nheadline. News articles in morphologically rich languages such as Russian\nrequire model modifications due to a large number of possible word forms. This\nstudy aims to validate that models with a possibility of copying words from the\noriginal article performs better than models without such an option. The\nproposed model achieves a mean ROUGE score of 23 on the provided test dataset,\nwhich is 8 points greater than the result of a similar model without a copying\nmechanism. Moreover, the resulting model performs better than any known model\non the new dataset of Russian news.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:39:01 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Gusev", "Ilya", ""]]}, {"id": "1904.11544", "submitter": "Najoung Kim", "authors": "Najoung Kim, Roma Patel, Adam Poliak, Alex Wang, Patrick Xia, R.\n  Thomas McCoy, Ian Tenney, Alexis Ross, Tal Linzen, Benjamin Van Durme, Samuel\n  R. Bowman, Ellie Pavlick", "title": "Probing What Different NLP Tasks Teach Machines about Function Word\n  Comprehension", "comments": "Accepted to *SEM 2019 (revised submission). Corresponding authors:\n  Najoung Kim (n.kim@jhu.edu), Ellie Pavlick (ellie_pavlick@brown.edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a set of nine challenge tasks that test for the understanding of\nfunction words. These tasks are created by structurally mutating sentences from\nexisting datasets to target the comprehension of specific types of function\nwords (e.g., prepositions, wh-words). Using these probing tasks, we explore the\neffects of various pretraining objectives for sentence encoders (e.g., language\nmodeling, CCG supertagging and natural language inference (NLI)) on the learned\nrepresentations. Our results show that pretraining on language modeling\nperforms the best on average across our probing tasks, supporting its\nwidespread use for pretraining state-of-the-art NLP models, and CCG\nsupertagging and NLI pretraining perform comparably. Overall, no pretraining\nobjective dominates across the board, and our function word probing tasks\nhighlight several intuitive differences between pretraining objectives, e.g.,\nthat NLI helps the comprehension of negation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 19:15:16 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 16:03:09 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Kim", "Najoung", ""], ["Patel", "Roma", ""], ["Poliak", "Adam", ""], ["Wang", "Alex", ""], ["Xia", "Patrick", ""], ["McCoy", "R. Thomas", ""], ["Tenney", "Ian", ""], ["Ross", "Alexis", ""], ["Linzen", "Tal", ""], ["Van Durme", "Benjamin", ""], ["Bowman", "Samuel R.", ""], ["Pavlick", "Ellie", ""]]}, {"id": "1904.11564", "submitter": "Jan Buys", "authors": "Valerie Hajdik and Jan Buys and Michael W. Goodman and Emily M. Bender", "title": "Neural Text Generation from Rich Semantic Representations", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose neural models to generate high-quality text from structured\nrepresentations based on Minimal Recursion Semantics (MRS). MRS is a rich\nsemantic representation that encodes more precise semantic detail than other\nrepresentations such as Abstract Meaning Representation (AMR). We show that a\nsequence-to-sequence model that maps a linearization of Dependency MRS, a\ngraph-based representation of MRS, to English text can achieve a BLEU score of\n66.11 when trained on gold data. The performance can be improved further using\na high-precision, broad coverage grammar-based parser to generate a large\nsilver training corpus, achieving a final BLEU score of 77.17 on the full test\nset, and 83.37 on the subset of test data most closely matching the silver data\ndomain. Our results suggest that MRS-based representations are a good choice\nfor applications that need both structured semantics and the ability to produce\nnatural language text as output.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 20:01:08 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Hajdik", "Valerie", ""], ["Buys", "Jan", ""], ["Goodman", "Michael W.", ""], ["Bender", "Emily M.", ""]]}, {"id": "1904.11574", "submitter": "Jie Lei", "authors": "Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal", "title": "TVQA+: Spatio-Temporal Grounding for Video Question Answering", "comments": "ACL 2020 camera-ready (15 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the task of Spatio-Temporal Video Question Answering, which\nrequires intelligent systems to simultaneously retrieve relevant moments and\ndetect referenced visual concepts (people and objects) to answer natural\nlanguage questions about videos. We first augment the TVQA dataset with 310.8K\nbounding boxes, linking depicted objects to visual concepts in questions and\nanswers. We name this augmented version as TVQA+. We then propose\nSpatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework\nthat grounds evidence in both spatial and temporal domains to answer questions\nabout videos. Comprehensive experiments and analyses demonstrate the\neffectiveness of our framework and how the rich annotations in our TVQA+\ndataset can contribute to the question answering task. Moreover, by performing\nthis joint task, our model is able to produce insightful and interpretable\nspatio-temporal attention visualizations. Dataset and code are publicly\navailable at: http: //tvqa.cs.unc.edu, https://github.com/jayleicn/TVQAplus\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 20:37:26 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 19:43:42 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Lei", "Jie", ""], ["Yu", "Licheng", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "1904.11610", "submitter": "Charlie Welch", "authors": "Charles Welch, Ver\\'onica P\\'erez-Rosas, Jonathan K. Kummerfeld, Rada\n  Mihalcea", "title": "Look Who's Talking: Inferring Speaker Attributes from Personal\n  Longitudinal Dialog", "comments": "15 pages accepted to CICLing 2019", "journal-ref": "Proceedings of the 20th International Conference on Computational\n  Linguistics and Intelligent Text Processing (CICLing 2019)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a large dialog corpus obtained from the conversation history of a\nsingle individual with 104 conversation partners. The corpus consists of half a\nmillion instant messages, across several messaging platforms. We focus our\nanalyses on seven speaker attributes, each of which partitions the set of\nspeakers, namely: gender; relative age; family member; romantic partner;\nclassmate; co-worker; and native to the same country. In addition to the\ncontent of the messages, we examine conversational aspects such as the time\nmessages are sent, messaging frequency, psycholinguistic word categories,\nlinguistic mirroring, and graph-based features reflecting how people in the\ncorpus mention each other. We present two sets of experiments predicting each\nattribute using (1) short context windows; and (2) a larger set of messages. We\nfind that using all features leads to gains of 9-14% over using message text\nonly.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 22:12:43 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Welch", "Charles", ""], ["P\u00e9rez-Rosas", "Ver\u00f3nica", ""], ["Kummerfeld", "Jonathan K.", ""], ["Mihalcea", "Rada", ""]]}, {"id": "1904.11641", "submitter": "Alessandro Lameiras Koerich", "authors": "Mohammed Senoussaoui, Patrick Cardinal, Najim Dehak, Alessandro\n  Lameiras Koerich", "title": "Speaker Sincerity Detection based on Covariance Feature Vectors and\n  Ensemble Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic measuring of speaker sincerity degree is a novel research problem\nin computational paralinguistics. This paper proposes covariance-based feature\nvectors to model speech and ensembles of support vector regressors to estimate\nthe degree of sincerity of a speaker. The elements of each covariance vector\nare pairwise statistics between the short-term feature components. These\nfeatures are used alone as well as in combination with the ComParE acoustic\nfeature set. The experimental results on the development set of the Sincerity\nSpeech Corpus using a cross-validation procedure have shown an 8.1% relative\nimprovement in the Spearman's correlation coefficient over the baseline system.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 01:42:41 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Senoussaoui", "Mohammed", ""], ["Cardinal", "Patrick", ""], ["Dehak", "Najim", ""], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "1904.11660", "submitter": "Abdelrahman Mohamed", "authors": "Abdelrahman Mohamed, Dmytro Okhonko, Luke Zettlemoyer", "title": "Transformers with convolutional context for ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of transformer networks for neural machine translation and\nother NLP tasks has led to a surge in research work trying to apply it for\nspeech recognition. Recent efforts studied key research questions around ways\nof combining positional embedding with speech features, and stability of\noptimization for large scale learning of transformer networks. In this paper,\nwe propose replacing the sinusoidal positional embedding for transformers with\nconvolutionally learned input representations. These contextual representations\nprovide subsequent transformer blocks with relative positional information\nneeded for discovering long-range relationships between local concepts. The\nproposed system has favorable optimization characteristics where our reported\nresults are produced with fixed learning rate of 1.0 and no warmup steps. The\nproposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech\n``test clean'' and ``test other'' subsets when no extra LM text is provided.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 03:00:19 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 23:25:25 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Mohamed", "Abdelrahman", ""], ["Okhonko", "Dmytro", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1904.11679", "submitter": "Xinyi Zhou", "authors": "Xinyi Zhou, Atishay Jain, Vir V. Phoha, Reza Zafarani", "title": "Fake News Early Detection: An Interdisciplinary Study", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive dissemination of fake news and its potential to erode democracy has\nincreased the demand for accurate fake news detection. Recent advancements in\nthis area have proposed novel techniques that aim to detect fake news by\nexploring how it propagates on social networks. Nevertheless, to detect fake\nnews at an early stage, i.e., when it is published on a news outlet but not yet\nspread on social media, one cannot rely on news propagation information as it\ndoes not exist. Hence, there is a strong need to develop approaches that can\ndetect fake news by focusing on news content. In this paper, a theory-driven\nmodel is proposed for fake news detection. The method investigates news content\nat various levels: lexicon-level, syntax-level, semantic-level and\ndiscourse-level. We represent news at each level, relying on well-established\ntheories in social and forensic psychology. Fake news detection is then\nconducted within a supervised machine learning framework. As an\ninterdisciplinary research, our work explores potential fake news patterns,\nenhances the interpretability in fake news feature engineering, and studies the\nrelationships among fake news, deception/disinformation, and clickbaits.\nExperiments conducted on two real-world datasets indicate the proposed method\ncan outperform the state-of-the-art and enable fake news early detection when\nthere is limited content information.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 05:52:05 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 18:42:11 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Zhou", "Xinyi", ""], ["Jain", "Atishay", ""], ["Phoha", "Vir V.", ""], ["Zafarani", "Reza", ""]]}, {"id": "1904.11783", "submitter": "Anne Lauscher", "authors": "Anne Lauscher and Goran Glava\\v{s}", "title": "Are We Consistently Biased? Multidimensional Analysis of Biases in\n  Distributional Word Vectors", "comments": "Accepted for *SEM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have recently been shown to reflect many of the pronounced\nsocietal biases (e.g., gender bias or racial bias). Existing studies are,\nhowever, limited in scope and do not investigate the consistency of biases\nacross relevant dimensions like embedding models, types of texts, and different\nlanguages. In this work, we present a systematic study of biases encoded in\ndistributional word vector spaces: we analyze how consistent the bias effects\nare across languages, corpora, and embedding models. Furthermore, we analyze\nthe cross-lingual biases encoded in bilingual embedding spaces, indicative of\nthe effects of bias transfer encompassed in cross-lingual transfer of NLP\nmodels. Our study yields some unexpected findings, e.g., that biases can be\nemphasized or downplayed by different embedding models or that user-generated\ncontent may be less biased than encyclopedic text. We hope our work catalyzes\nbias research in NLP and informs the development of bias reduction techniques.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 11:56:35 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 08:12:34 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Lauscher", "Anne", ""], ["Glava\u0161", "Goran", ""]]}, {"id": "1904.11816", "submitter": "Alexandre Salle", "authors": "Alexandre Salle, Marcelo Prates", "title": "Think Again Networks and the Delta Loss", "comments": "redacted experiments on language modeling due to evaluation error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper introduces an abstraction called Think Again Networks\n(ThinkNet) which can be applied to any state-dependent function (such as a\nrecurrent neural network).\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 12:57:25 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 20:31:59 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Salle", "Alexandre", ""], ["Prates", "Marcelo", ""]]}, {"id": "1904.11838", "submitter": "Marco Roberti", "authors": "Marco Roberti, Giovanni Bonetta, Rossella Cancelliere, Patrick\n  Gallinari", "title": "Copy mechanism and tailored training for character-based data-to-text\n  generation", "comments": "ECML-PKDD 2019 (Camera ready version)", "journal-ref": null, "doi": "10.1007/978-3-030-46147-8_39", "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, many different methods have been focusing on using\ndeep recurrent neural networks for natural language generation. The most widely\nused sequence-to-sequence neural methods are word-based: as such, they need a\npre-processing step called delexicalization (conversely, relexicalization) to\ndeal with uncommon or unknown words. These forms of processing, however, give\nrise to models that depend on the vocabulary used and are not completely\nneural.\n  In this work, we present an end-to-end sequence-to-sequence model with\nattention mechanism which reads and generates at a character level, no longer\nrequiring delexicalization, tokenization, nor even lowercasing. Moreover, since\ncharacters constitute the common \"building blocks\" of every text, it also\nallows a more general approach to text generation, enabling the possibility to\nexploit transfer learning for training. These skills are obtained thanks to two\nmajor features: (i) the possibility to alternate between the standard\ngeneration mechanism and a copy one, which allows to directly copy input facts\nto produce outputs, and (ii) the use of an original training pipeline that\nfurther improves the quality of the generated texts.\n  We also introduce a new dataset called E2E+, designed to highlight the\ncopying capabilities of character-based models, that is a modified version of\nthe well-known E2E dataset used in the E2E Challenge. We tested our model\naccording to five broadly accepted metrics (including the widely used BLEU),\nshowing that it yields competitive performance with respect to both\ncharacter-based and word-based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 13:33:56 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 11:38:07 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 15:35:14 GMT"}, {"version": "v4", "created": "Mon, 11 May 2020 12:48:10 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Roberti", "Marco", ""], ["Bonetta", "Giovanni", ""], ["Cancelliere", "Rossella", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1904.11942", "submitter": "Rujun Han", "authors": "Rujun Han, Mengyue Liang, Bashar Alhafni, Nanyun Peng", "title": "Contextualized Word Embeddings Enhanced Event Temporal Relation\n  Extraction for Story Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning causal and temporal relationships between events is an important\nstep towards deeper story and commonsense understanding. Though there are\nabundant datasets annotated with event relations for story comprehension, many\nhave no empirical results associated with them. In this work, we establish\nstrong baselines for event temporal relation extraction on two under-explored\nstory narrative datasets: Richer Event Description (RED) and Causal and\nTemporal Relation Scheme (CaTeRS). To the best of our knowledge, these are the\nfirst results reported on these two datasets. We demonstrate that neural\nnetwork-based models can outperform some strong traditional linguistic\nfeature-based models. We also conduct comparative studies to show the\ncontribution of adopting contextualized word embeddings (BERT) for event\ntemporal relation extraction from stories. Detailed analyses are offered to\nbetter understand the results.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:21:59 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Han", "Rujun", ""], ["Liang", "Mengyue", ""], ["Alhafni", "Bashar", ""], ["Peng", "Nanyun", ""]]}, {"id": "1904.11961", "submitter": "Ahmed Fadhil Dr.", "authors": "Ahmed Fadhil, Gianluca Schiavo, Yunlong Wang", "title": "CoachAI: A Conversational Agent Assisted Health Coaching Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Poor lifestyle represents a health risk factor and is the leading cause of\nmorbidity and chronic conditions. The impact of poor lifestyle can be\nsignificantly altered by individual behavior change. Although the current shift\nin healthcare towards a long lasting modifiable behavior, however, with\nincreasing caregiver workload and individuals' continuous needs of care, there\nis a need to ease caregiver's work while ensuring continuous interaction with\nusers. This paper describes the design and validation of CoachAI, a\nconversational agent assisted health coaching system to support health\nintervention delivery to individuals and groups. CoachAI instantiates a text\nbased healthcare chatbot system that bridges the remote human coach and the\nusers. This research provides three main contributions to the preventive\nhealthcare and healthy lifestyle promotion: (1) it presents the conversational\nagent to aid the caregiver; (2) it aims to decrease caregiver's workload and\nenhance care given to users, by handling (automating) repetitive caregiver\ntasks; and (3) it presents a domain independent mobile health conversational\nagent for health intervention delivery. We will discuss our approach and\nanalyze the results of a one month validation study on physical activity,\nhealthy diet and stress management.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:44:04 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Fadhil", "Ahmed", ""], ["Schiavo", "Gianluca", ""], ["Wang", "Yunlong", ""]]}, {"id": "1904.12087", "submitter": "Marcos Zampieri", "authors": "Gustavo Henrique Paetzold, Marcos Zampieri", "title": "Experiments in Cuneiform Language Identification", "comments": "Proceedings of VarDial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents methods to discriminate between languages and dialects\nwritten in Cuneiform script, one of the first writing systems in the world. We\nreport the results obtained by the PZ team in the Cuneiform Language\nIdentification (CLI) shared task organized within the scope of the VarDial\nEvaluation Campaign 2019. The task included two languages, Sumerian and\nAkkadian. The latter is divided into six dialects: Old Babylonian, Middle\nBabylonian peripheral, Standard Babylonian, Neo Babylonian, Late Babylonian,\nand Neo Assyrian. We approach the task using a meta-classifier trained on\nvarious SVM models and we show the effectiveness of the system for this task.\nOur submission achieved 0.738 F1 score in discriminating between the seven\nlanguages and dialects and it was ranked fourth in the competition among eight\nteams.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 01:51:55 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Paetzold", "Gustavo Henrique", ""], ["Zampieri", "Marcos", ""]]}, {"id": "1904.12104", "submitter": "Tianda Li", "authors": "Tianda Li, Xiaodan Zhu, Quan Liu, Qian Chen, Zhigang Chen, Si Wei", "title": "Several Experiments on Investigating Pretraining and Knowledge-Enhanced\n  Models for Natural Language Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language inference (NLI) is among the most challenging tasks in\nnatural language understanding. Recent work on unsupervised pretraining that\nleverages unsupervised signals such as language-model and sentence prediction\nobjectives has shown to be very effective on a wide range of NLP problems. It\nwould still be desirable to further understand how it helps NLI; e.g., if it\nlearns artifacts in data annotation or instead learn true inference knowledge.\nIn addition, external knowledge that does not exist in the limited amount of\nNLI training data may be added to NLI models in two typical ways, e.g., from\nhuman-created resources or an unsupervised pretraining paradigm. We runs\nseveral experiments here to investigate whether they help NLI in the same way,\nand if not,how?\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 04:24:07 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Tianda", ""], ["Zhu", "Xiaodan", ""], ["Liu", "Quan", ""], ["Chen", "Qian", ""], ["Chen", "Zhigang", ""], ["Wei", "Si", ""]]}, {"id": "1904.12106", "submitter": "Jifan Chen", "authors": "Jifan Chen and Greg Durrett", "title": "Understanding Dataset Design Choices for Multi-hop Reasoning", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning multi-hop reasoning has been a key challenge for reading\ncomprehension models, leading to the design of datasets that explicitly focus\non it. Ideally, a model should not be able to perform well on a multi-hop\nquestion answering task without doing multi-hop reasoning. In this paper, we\ninvestigate two recently proposed datasets, WikiHop and HotpotQA. First, we\nexplore sentence-factored models for these tasks; by design, these models\ncannot do multi-hop reasoning, but they are still able to solve a large number\nof examples in both datasets. Furthermore, we find spurious correlations in the\nunmasked version of WikiHop, which make it easy to achieve high performance\nconsidering only the questions and answers. Finally, we investigate one key\ndifference between these datasets, namely span-based vs. multiple-choice\nformulations of the QA task. Multiple-choice versions of both datasets can be\neasily gamed, and two models we examine only marginally exceed a baseline in\nthis setting. Overall, while these datasets are useful testbeds,\nhigh-performing models may not be learning as much multi-hop reasoning as\npreviously thought.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 04:36:57 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Chen", "Jifan", ""], ["Durrett", "Greg", ""]]}, {"id": "1904.12162", "submitter": "Hideaki Hata", "authors": "Rungroj Maipradit, Hideaki Hata, Kenichi Matsumoto", "title": "Sentiment Classification using N-gram IDF and Automated Machine Learning", "comments": "4 pages, IEEE Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sentiment classification method with a general machine learning\nframework. For feature representation, n-gram IDF is used to extract\nsoftware-engineering-related, dataset-specific, positive, neutral, and negative\nn-gram expressions. For classifiers, an automated machine learning tool is\nused. In the comparison using publicly available datasets, our method achieved\nthe highest F1 values in positive and negative sentences on all datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 14:46:34 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 13:30:47 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Maipradit", "Rungroj", ""], ["Hata", "Hideaki", ""], ["Matsumoto", "Kenichi", ""]]}, {"id": "1904.12166", "submitter": "Hitomi Yanaka", "authors": "Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi\n  Sekine, Lasha Abzianidze, Johan Bos", "title": "HELP: A Dataset for Identifying Shortcomings of Neural Models in\n  Monotonicity Reasoning", "comments": "6 pages, 1 figure, accepted as *SEM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large crowdsourced datasets are widely used for training and evaluating\nneural models on natural language inference (NLI). Despite these efforts,\nneural models have a hard time capturing logical inferences, including those\nlicensed by phrase replacements, so-called monotonicity reasoning. Since no\nlarge dataset has been developed for monotonicity reasoning, it is still\nunclear whether the main obstacle is the size of datasets or the model\narchitectures themselves. To investigate this issue, we introduce a new\ndataset, called HELP, for handling entailments with lexical and logical\nphenomena. We add it to training data for the state-of-the-art neural models\nand evaluate them on test sets for monotonicity phenomena. The results showed\nthat our data augmentation improved the overall accuracy. We also find that the\nimprovement is better on monotonicity inferences with lexical replacements than\non downward inferences with disjunction and modification. This suggests that\nsome types of inferences can be improved by our data augmentation while others\nare immune to it.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 15:15:14 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yanaka", "Hitomi", ""], ["Mineshima", "Koji", ""], ["Bekki", "Daisuke", ""], ["Inui", "Kentaro", ""], ["Sekine", "Satoshi", ""], ["Abzianidze", "Lasha", ""], ["Bos", "Johan", ""]]}, {"id": "1904.12213", "submitter": "Yuming Zhai", "authors": "Yuming Zhai, Pooyan Safari, Gabriel Illouz, Alexandre Allauzen, Anne\n  Vilnat", "title": "Towards Recognizing Phrase Translation Processes: Experiments on\n  English-French", "comments": "12 pages, preprint version accepted to the conference CICLING 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When translating phrases (words or group of words), human translators,\nconsciously or not, resort to different translation processes apart from the\nliteral translation, such as Idiom Equivalence, Generalization,\nParticularization, Semantic Modulation, etc. Translators and linguists (such as\nVinay and Darbelnet, Newmark, etc.) have proposed several typologies to\ncharacterize the different translation processes. However, to the best of our\nknowledge, there has not been effort to automatically classify these\nfine-grained translation processes. Recently, an English-French parallel corpus\nof TED Talks has been manually annotated with translation process categories,\nalong with established annotation guidelines. Based on these annotated\nexamples, we propose an automatic classification of translation processes at\nsubsentential level. Experimental results show that we can distinguish\nnon-literal translation from literal translation with an accuracy of 87.09%,\nand 55.20% for classifying among five non-literal translation processes. This\nwork demonstrates that it is possible to automatically classify translation\nprocesses. Even with a small amount of annotated examples, our experiments show\nthe directions that we can follow in future work. One of our long term\nobjectives is leveraging this automatic classification to better control\nparaphrase extraction from bilingual parallel corpora.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 21:14:21 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Zhai", "Yuming", ""], ["Safari", "Pooyan", ""], ["Illouz", "Gabriel", ""], ["Allauzen", "Alexandre", ""], ["Vilnat", "Anne", ""]]}, {"id": "1904.12324", "submitter": "Kiril Gashteovski", "authors": "Kiril Gashteovski, Sebastian Wanner, Sven Hertling, Samuel Broscheit,\n  Rainer Gemulla", "title": "OPIEC: An Open Information Extraction Corpus", "comments": "In Proceedings of the Conference of Automatic Knowledge Base\n  Construction (AKBC) 2019", "journal-ref": "In Proceedings of the Conference of Automatic Knowledge Base\n  Construction (AKBC) 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open information extraction (OIE) systems extract relations and their\narguments from natural language text in an unsupervised manner. The resulting\nextractions are a valuable resource for downstream tasks such as knowledge base\nconstruction, open question answering, or event schema induction. In this\npaper, we release, describe, and analyze an OIE corpus called OPIEC, which was\nextracted from the text of English Wikipedia. OPIEC complements the available\nOIE resources: It is the largest OIE corpus publicly available to date (over\n340M triples) and contains valuable metadata such as provenance information,\nconfidence scores, linguistic annotations, and semantic annotations including\nspatial and temporal information. We analyze the OPIEC corpus by comparing its\ncontent with knowledge bases such as DBpedia or YAGO, which are also based on\nWikipedia. We found that most of the facts between entities present in OPIEC\ncannot be found in DBpedia and/or YAGO, that OIE facts often differ in the\nlevel of specificity compared to knowledge base facts, and that OIE open\nrelations are generally highly polysemous. We believe that the OPIEC corpus is\na valuable resource for future research on automated knowledge base\nconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 13:57:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Gashteovski", "Kiril", ""], ["Wanner", "Sebastian", ""], ["Hertling", "Sven", ""], ["Broscheit", "Samuel", ""], ["Gemulla", "Rainer", ""]]}, {"id": "1904.12399", "submitter": "Zhong Meng", "authors": "Zhong Meng, Jinyu Li, Yong Zhao, Yifan Gong", "title": "Conditional Teacher-Student Learning", "comments": "5 pages, 1 figure, ICASSP 2019", "journal-ref": "2019 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Brighton, UK", "doi": "10.1109/ICASSP.2019.8683438", "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The teacher-student (T/S) learning has been shown to be effective for a\nvariety of problems such as domain adaptation and model compression. One\nshortcoming of the T/S learning is that a teacher model, not always perfect,\nsporadically produces wrong guidance in form of posterior probabilities that\nmisleads the student model towards a suboptimal performance. To overcome this\nproblem, we propose a conditional T/S learning scheme, in which a \"smart\"\nstudent model selectively chooses to learn from either the teacher model or the\nground truth labels conditioned on whether the teacher can correctly predict\nthe ground truth. Unlike a naive linear combination of the two knowledge\nsources, the conditional learning is exclusively engaged with the teacher model\nwhen the teacher model's prediction is correct, and otherwise backs off to the\nground truth. Thus, the student model is able to learn effectively from the\nteacher and even potentially surpass the teacher. We examine the proposed\nlearning scheme on two tasks: domain adaptation on CHiME-3 dataset and speaker\nadaptation on Microsoft short message dictation dataset. The proposed method\nachieves 9.8% and 12.8% relative word error rate reductions, respectively, over\nT/S learning for environment adaptation and speaker-independent model for\nspeaker adaptation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 23:43:20 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Meng", "Zhong", ""], ["Li", "Jinyu", ""], ["Zhao", "Yong", ""], ["Gong", "Yifan", ""]]}, {"id": "1904.12400", "submitter": "Zhong Meng", "authors": "Zhong Meng, Jinyu Li, Yifan Gong", "title": "Attentive Adversarial Learning for Domain-Invariant Training", "comments": "5 pages, 1 figure, ICASSP 2019", "journal-ref": "2019 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Brighton, United Kingdom", "doi": "10.1109/ICASSP.2019.8683486", "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial domain-invariant training (ADIT) proves to be effective in\nsuppressing the effects of domain variability in acoustic modeling and has led\nto improved performance in automatic speech recognition (ASR). In ADIT, an\nauxiliary domain classifier takes in equally-weighted deep features from a deep\nneural network (DNN) acoustic model and is trained to improve their\ndomain-invariance by optimizing an adversarial loss function. In this work, we\npropose an attentive ADIT (AADIT) in which we advance the domain classifier\nwith an attention mechanism to automatically weight the input deep features\naccording to their importance in domain classification. With this attentive\nre-weighting, AADIT can focus on the domain normalization of phonetic\ncomponents that are more susceptible to domain variability and generates deep\nfeatures with improved domain-invariance and senone-discriminativity over ADIT.\nMost importantly, the attention block serves only as an external component to\nthe DNN acoustic model and is not involved in ASR, so AADIT can be used to\nimprove the acoustic modeling with any DNN architectures. More generally, the\nsame methodology can improve any adversarial learning system with an auxiliary\ndiscriminator. Evaluated on CHiME-3 dataset, the AADIT achieves 13.6% and 9.3%\nrelative WER improvements, respectively, over a multi-conditional model and a\nstrong ADIT baseline.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 23:44:29 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Meng", "Zhong", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "1904.12406", "submitter": "Zhong Meng", "authors": "Zhong Meng, Yong Zhao, Jinyu Li, Yifan Gong", "title": "Adversarial Speaker Verification", "comments": "5 pages, 1 figure, ICASSP 2019", "journal-ref": "2019 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Brighton, United Kingdom", "doi": "10.1109/ICASSP.2019.8682488", "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep networks to extract embeddings for speaker recognition has\nproven successfully. However, such embeddings are susceptible to performance\ndegradation due to the mismatches among the training, enrollment, and test\nconditions. In this work, we propose an adversarial speaker verification (ASV)\nscheme to learn the condition-invariant deep embedding via adversarial\nmulti-task training. In ASV, a speaker classification network and a condition\nidentification network are jointly optimized to minimize the speaker\nclassification loss and simultaneously mini-maximize the condition loss. The\ntarget labels of the condition network can be categorical (environment types)\nand continuous (SNR values). We further propose multi-factorial ASV to\nsimultaneously suppress multiple factors that constitute the condition\nvariability. Evaluated on a Microsoft Cortana text-dependent speaker\nverification task, the ASV achieves 8.8% and 14.5% relative improvements in\nequal error rates (EER) for known and unknown conditions, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 00:37:27 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Meng", "Zhong", ""], ["Zhao", "Yong", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "1904.12407", "submitter": "Zhong Meng", "authors": "Zhong Meng, Jinyu Li, Yifan Gong", "title": "Adversarial Speaker Adaptation", "comments": "5 pages, 2 figures, ICASSP 2019", "journal-ref": "2019 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Brighton, United Kingdom", "doi": "10.1109/ICASSP.2019.8682510", "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adversarial speaker adaptation (ASA) scheme, in which\nadversarial learning is applied to regularize the distribution of deep hidden\nfeatures in a speaker-dependent (SD) deep neural network (DNN) acoustic model\nto be close to that of a fixed speaker-independent (SI) DNN acoustic model\nduring adaptation. An additional discriminator network is introduced to\ndistinguish the deep features generated by the SD model from those produced by\nthe SI model. In ASA, with a fixed SI model as the reference, an SD model is\njointly optimized with the discriminator network to minimize the senone\nclassification loss, and simultaneously to mini-maximize the SI/SD\ndiscrimination loss on the adaptation data. With ASA, a senone-discriminative\ndeep feature is learned in the SD model with a similar distribution to that of\nthe SI model. With such a regularized and adapted deep feature, the SD model\ncan perform improved automatic speech recognition on the target speaker's\nspeech. Evaluated on the Microsoft short message dictation dataset, ASA\nachieves 14.4% and 7.9% relative word error rate improvements for supervised\nand unsupervised adaptation, respectively, over an SI model trained from 2600\nhours data, with 200 adaptation utterances per speaker.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 00:38:16 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Meng", "Zhong", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "1904.12535", "submitter": "Ping Li", "authors": "Mingming Sun, Xu Li, Xin Wang, Miao Fan, Yue Feng, Ping Li", "title": "Logician: A Unified End-to-End Neural Approach for Open-Domain\n  Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of open information extraction (OIE)\nfor extracting entity and relation level intermediate structures from sentences\nin open-domain. We focus on four types of valuable intermediate structures\n(Relation, Attribute, Description, and Concept), and propose a unified\nknowledge expression form, SAOKE, to express them. We publicly release a data\nset which contains more than forty thousand sentences and the corresponding\nfacts in the SAOKE format labeled by crowd-sourcing. To our knowledge, this is\nthe largest publicly available human labeled data set for open information\nextraction tasks. Using this labeled SAOKE data set, we train an end-to-end\nneural model using the sequenceto-sequence paradigm, called Logician, to\ntransform sentences into facts. For each sentence, different to existing\nalgorithms which generally focus on extracting each single fact without\nconcerning other possible facts, Logician performs a global optimization over\nall possible involved facts, in which facts not only compete with each other to\nattract the attention of words, but also cooperate to share words. An\nexperimental study on various types of open domain relation extraction tasks\nreveals the consistent superiority of Logician to other states-of-the-art\nalgorithms. The experiments verify the reasonableness of SAOKE format, the\nvaluableness of SAOKE data set, the effectiveness of the proposed Logician\nmodel, and the feasibility of the methodology to apply end-to-end learning\nparadigm on supervised data sets for the challenging tasks of open information\nextraction.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:37:31 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sun", "Mingming", ""], ["Li", "Xu", ""], ["Wang", "Xin", ""], ["Fan", "Miao", ""], ["Feng", "Yue", ""], ["Li", "Ping", ""]]}, {"id": "1904.12550", "submitter": "Mark-Christoph M\\\"uller", "authors": "Mark-Christoph M\\\"uller", "title": "Semantic Matching of Documents from Heterogeneous Collections: A Simple\n  and Transparent Method for Practical Applications", "comments": "To appear at RELATIONS 2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a very simple, unsupervised method for the pairwise matching of\ndocuments from heterogeneous collections. We demonstrate our method with the\nConcept-Project matching task, which is a binary classification task involving\npairs of documents from heterogeneous collections. Although our method only\nemploys standard resources without any domain- or task-specific modifications,\nit clearly outperforms the more complex system of the original authors. In\naddition, our method is transparent, because it provides explicit information\nabout how a similarity score was computed, and efficient, because it is based\non the aggregation of (pre-computable) word-level similarities.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 10:21:58 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["M\u00fcller", "Mark-Christoph", ""]]}, {"id": "1904.12580", "submitter": "Mahidhar Dwarampudi", "authors": "Dwarampudi Mahidhar Reddy, Dr. N V Subba Reddy, Dr. N V Subba Reddy", "title": "Twitter Sentiment Analysis using Distributed Word and Sentence\n  Representation", "comments": "8 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important part of the information gathering and data analysis is to find\nout what people think about, either a product or an entity. Twitter is an\nopinion rich social networking site. The posts or tweets from this data can be\nused for mining people's opinions. The recent surge of activity in this area\ncan be attributed to the computational treatment of data, which made opinion\nextraction and sentiment analysis easier. This paper classifies tweets into\npositive and negative sentiments, but instead of using traditional methods or\npreprocessing text data here we use the distributed representations of words\nand sentences to classify the tweets. We use Long Short Term Memory (LSTM)\nNetworks, Convolutional Neural Networks (CNNs) and Artificial Neural Networks.\nThe first two are used on Distributed Representation of words while the latter\nis used on the distributed representation of sentences. This paper achieves\naccuracies as high as 81%. It also suggests the best and optimal ways for\ncreating distributed representations of words for sentiment analysis, out of\nthe available methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 17:46:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Reddy", "Dwarampudi Mahidhar", ""], ["Reddy", "Dr. N V Subba", ""], ["Reddy", "Dr. N V Subba", ""]]}, {"id": "1904.12584", "submitter": "Jiayuan Mao", "authors": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun\n  Wu", "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and\n  Sentences From Natural Supervision", "comments": "ICLR 2019 (Oral). Project page: http://nscl.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns\nvisual concepts, words, and semantic parsing of sentences without explicit\nsupervision on any of them; instead, our model learns by simply looking at\nimages and reading paired questions and answers. Our model builds an\nobject-based scene representation and translates sentences into executable,\nsymbolic programs. To bridge the learning of two modules, we use a\nneuro-symbolic reasoning module that executes these programs on the latent\nscene representation. Analogical to human concept learning, the perception\nmodule learns visual concepts based on the language description of the object\nbeing referred to. Meanwhile, the learned visual concepts facilitate learning\nnew words and parsing new sentences. We use curriculum learning to guide the\nsearching over the large compositional space of images and language. Extensive\nexperiments demonstrate the accuracy and efficiency of our model on learning\nvisual concepts, word representations, and semantic parsing of sentences.\nFurther, our method allows easy generalization to new object attributes,\ncompositions, language concepts, scenes and questions, and even new program\ndomains. It also empowers applications including visual question answering and\nbidirectional image-text retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:50:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Mao", "Jiayuan", ""], ["Gan", "Chuang", ""], ["Kohli", "Pushmeet", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "1904.12624", "submitter": "Apostol Vassilev", "authors": "Apostol Vassilev", "title": "BowTie - A deep learning feedforward neural network for sentiment\n  analysis", "comments": "12 pages, 7 figures, 4 tables", "journal-ref": null, "doi": "10.1007/978-3-030-37599-7_30", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to model and encode the semantics of human-written text and select the\ntype of neural network to process it are not settled issues in sentiment\nanalysis. Accuracy and transferability are critical issues in machine learning\nin general. These properties are closely related to the loss estimates for the\ntrained model. I present a computationally-efficient and accurate feedforward\nneural network for sentiment prediction capable of maintaining low losses. When\ncoupled with an effective semantics model of the text, it provides highly\naccurate models with low losses. Experimental results on representative\nbenchmark datasets and comparisons to other methods show the advantages of the\nnew approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:38:57 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Vassilev", "Apostol", ""]]}, {"id": "1904.12638", "submitter": "Eloi Zablocki", "authors": "Eloi Zablocki, Patrick Bordes, Benjamin Piwowarski, Laure Soulier,\n  Patrick Gallinari", "title": "Context-Aware Zero-Shot Learning for Object Recognition", "comments": "Accepted at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) aims at classifying unlabeled objects by leveraging\nauxiliary knowledge, such as semantic representations. A limitation of previous\napproaches is that only intrinsic properties of objects, e.g. their visual\nappearance, are taken into account while their context, e.g. the surrounding\nobjects in the image, is ignored. Following the intuitive principle that\nobjects tend to be found in certain contexts but not others, we propose a new\nand challenging approach, context-aware ZSL, that leverages semantic\nrepresentations in a new way to model the conditional likelihood of an object\nto appear in a given context. Finally, through extensive experiments conducted\non Visual Genome, we show that contextual information can substantially improve\nthe standard ZSL approach and is robust to unbalanced classes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 08:50:05 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 11:39:52 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Zablocki", "Eloi", ""], ["Bordes", "Patrick", ""], ["Piwowarski", "Benjamin", ""], ["Soulier", "Laure", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1904.12848", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le", "title": "Unsupervised Data Augmentation for Consistency Training", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning lately has shown much promise in improving deep\nlearning models when labeled data is scarce. Common among recent approaches is\nthe use of consistency training on a large amount of unlabeled data to\nconstrain model predictions to be invariant to input noise. In this work, we\npresent a new perspective on how to effectively noise unlabeled examples and\nargue that the quality of noising, specifically those produced by advanced data\naugmentation methods, plays a crucial role in semi-supervised learning. By\nsubstituting simple noising operations with advanced data augmentation methods\nsuch as RandAugment and back-translation, our method brings substantial\nimprovements across six language and three vision tasks under the same\nconsistency training framework. On the IMDb text classification dataset, with\nonly 20 labeled examples, our method achieves an error rate of 4.20,\noutperforming the state-of-the-art model trained on 25,000 labeled examples. On\na standard semi-supervised learning benchmark, CIFAR-10, our method outperforms\nall previous approaches and achieves an error rate of 5.43 with only 250\nexamples. Our method also combines well with transfer learning, e.g., when\nfinetuning from BERT, and yields improvements in high-data regime, such as\nImageNet, whether when there is only 10% labeled data or when a full labeled\nset with 1.3M extra unlabeled examples is used. Code is available at\nhttps://github.com/google-research/uda.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 17:56:59 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 17:53:48 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 15:32:11 GMT"}, {"version": "v4", "created": "Mon, 30 Sep 2019 15:40:40 GMT"}, {"version": "v5", "created": "Thu, 25 Jun 2020 17:58:43 GMT"}, {"version": "v6", "created": "Thu, 5 Nov 2020 15:11:02 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Xie", "Qizhe", ""], ["Dai", "Zihang", ""], ["Hovy", "Eduard", ""], ["Luong", "Minh-Thang", ""], ["Le", "Quoc V.", ""]]}, {"id": "1904.12907", "submitter": "Haonan Chen", "authors": "Haonan Chen, Hao Tan, Alan Kuntz, Mohit Bansal, Ron Alterovitz", "title": "Enabling Robots to Understand Incomplete Natural Language Instructions\n  Using Commonsense Reasoning", "comments": "7 pages, 4 figures, ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling robots to understand instructions provided via spoken natural\nlanguage would facilitate interaction between robots and people in a variety of\nsettings in homes and workplaces. However, natural language instructions are\noften missing information that would be obvious to a human based on\nenvironmental context and common sense, and hence does not need to be\nexplicitly stated. In this paper, we introduce Language-Model-based Commonsense\nReasoning (LMCR), a new method which enables a robot to listen to a natural\nlanguage instruction from a human, observe the environment around it, and\nautomatically fill in information missing from the instruction using\nenvironmental context and a new commonsense reasoning approach. Our approach\nfirst converts an instruction provided as unconstrained natural language into a\nform that a robot can understand by parsing it into verb frames. Our approach\nthen fills in missing information in the instruction by observing objects in\nits vicinity and leveraging commonsense reasoning. To learn commonsense\nreasoning automatically, our approach distills knowledge from large\nunstructured textual corpora by training a language model. Our results show the\nfeasibility of a robot learning commonsense knowledge automatically from\nweb-based textual corpora, and the power of learned commonsense reasoning\nmodels in enabling a robot to autonomously perform tasks based on incomplete\nnatural language instructions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 18:59:59 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 01:47:13 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chen", "Haonan", ""], ["Tan", "Hao", ""], ["Kuntz", "Alan", ""], ["Bansal", "Mohit", ""], ["Alterovitz", "Ron", ""]]}, {"id": "1904.12973", "submitter": "Gunnar R\\\"atsch", "authors": "Stefan G. Stark, Stephanie L. Hyland, Melanie F. Pradier, Kjong\n  Lehmann, Andreas Wicki, Fernando Perez Cruz, Julia E. Vogt, Gunnar R\\\"atsch", "title": "Unsupervised Extraction of Phenotypes from Cancer Clinical Notes for\n  Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent adoption of Electronic Health Records (EHRs) by health care\nproviders has introduced an important source of data that provides detailed and\nhighly specific insights into patient phenotypes over large cohorts. These\ndatasets, in combination with machine learning and statistical approaches,\ngenerate new opportunities for research and clinical care. However, many\nmethods require the patient representations to be in structured formats, while\nthe information in the EHR is often locked in unstructured texts designed for\nhuman readability. In this work, we develop the methodology to automatically\nextract clinical features from clinical narratives from large EHR corpora\nwithout the need for prior knowledge. We consider medical terms and sentences\nappearing in clinical narratives as atomic information units. We propose an\nefficient clustering strategy suitable for the analysis of large text corpora\nand to utilize the clusters to represent information about the patient\ncompactly. To demonstrate the utility of our approach, we perform an\nassociation study of clinical features with somatic mutation profiles from\n4,007 cancer patients and their tumors. We apply the proposed algorithm to a\ndataset consisting of about 65 thousand documents with a total of about 3.2\nmillion sentences. We identify 341 significant statistical associations between\nthe presence of somatic mutations and clinical features. We annotated these\nassociations according to their novelty, and report several known associations.\nWe also propose 32 testable hypotheses where the underlying biological\nmechanism does not appear to be known but plausible. These results illustrate\nthat the automated discovery of clinical features is possible and the joint\nanalysis of clinical and genetic datasets can generate appealing new\nhypotheses.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 22:15:22 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 14:13:56 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Stark", "Stefan G.", ""], ["Hyland", "Stephanie L.", ""], ["Pradier", "Melanie F.", ""], ["Lehmann", "Kjong", ""], ["Wicki", "Andreas", ""], ["Cruz", "Fernando Perez", ""], ["Vogt", "Julia E.", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1904.13015", "submitter": "Sanghyun Yi", "authors": "Sanghyun Yi, Rahul Goel, Chandra Khatri, Alessandra Cervone, Tagyoung\n  Chung, Behnam Hedayatnia, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tur", "title": "Towards Coherent and Engaging Spoken Dialog Response Generation Using\n  Automatic Conversation Evaluators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder based neural architectures serve as the basis of\nstate-of-the-art approaches in end-to-end open domain dialog systems. Since\nmost of such systems are trained with a maximum likelihood~(MLE) objective they\nsuffer from issues such as lack of generalizability and the generic response\nproblem, i.e., a system response that can be an answer to a large number of\nuser utterances, e.g., \"Maybe, I don't know.\" Having explicit feedback on the\nrelevance and interestingness of a system response at each turn can be a useful\nsignal for mitigating such issues and improving system quality by selecting\nresponses from different approaches. Towards this goal, we present a system\nthat evaluates chatbot responses at each dialog turn for coherence and\nengagement. Our system provides explicit turn-level dialog quality feedback,\nwhich we show to be highly correlated with human evaluation. To show that\nincorporating this feedback in the neural response generation models improves\ndialog quality, we present two different and complementary mechanisms to\nincorporate explicit feedback into a neural response generation model:\nreranking and direct modification of the loss function during training. Our\nstudies show that a response generation model that incorporates these combined\nfeedback mechanisms produce more engaging and coherent responses in an\nopen-domain spoken dialog setting, significantly improving the response quality\nusing both automatic and human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 02:03:05 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 18:50:50 GMT"}, {"version": "v3", "created": "Sat, 21 Sep 2019 18:24:42 GMT"}, {"version": "v4", "created": "Fri, 22 Nov 2019 03:06:41 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Yi", "Sanghyun", ""], ["Goel", "Rahul", ""], ["Khatri", "Chandra", ""], ["Cervone", "Alessandra", ""], ["Chung", "Tagyoung", ""], ["Hedayatnia", "Behnam", ""], ["Venkatesh", "Anu", ""], ["Gabriel", "Raefer", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "1904.13018", "submitter": "Yifan Peng", "authors": "Yifan Peng, Ke Yan, Veit Sandfort, Ronald M. Summers, Zhiyong Lu", "title": "A self-attention based deep learning method for lesion attribute\n  detection from CT reports", "comments": "5 pages, 2 figures, accepted by 2019 IEEE International Conference on\n  Healthcare Informatics (ICHI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In radiology, radiologists not only detect lesions from the medical image,\nbut also describe them with various attributes such as their type, location,\nsize, shape, and intensity. While these lesion attributes are rich and useful\nin many downstream clinical applications, how to extract them from the\nradiology reports is less studied. This paper outlines a novel deep learning\nmethod to automatically extract attributes of lesions of interest from the\nclinical text. Different from classical CNN models, we integrated the\nmulti-head self-attention mechanism to handle the long-distance information in\nthe sentence, and to jointly correlate different portions of sentence\nrepresentation subspaces in parallel. Evaluation on an in-house corpus\ndemonstrates that our method can achieve high performance with 0.848 in\nprecision, 0.788 in recall, and 0.815 in F-score. The new method and\nconstructed corpus will enable us to build automatic systems with a\nhigher-level understanding of the radiological world.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 02:18:23 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Peng", "Yifan", ""], ["Yan", "Ke", ""], ["Sandfort", "Veit", ""], ["Summers", "Ronald M.", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1904.13178", "submitter": "Abhishek", "authors": "Abhishek Abhishek, Sanya Bathla Taneja, Garima Malik, Ashish Anand,\n  Amit Awekar", "title": "Fine-grained Entity Recognition with Reduced False Negatives and Large\n  Type Coverage", "comments": "Camera ready version, AKBC 2019. Code and data available at\n  https://github.com/abhipec/HAnDS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained Entity Recognition (FgER) is the task of detecting and\nclassifying entity mentions to a large set of types spanning diverse domains\nsuch as biomedical, finance and sports. We observe that when the type set spans\nseveral domains, detection of entity mention becomes a limitation for\nsupervised learning models. The primary reason being lack of dataset where\nentity boundaries are properly annotated while covering a large spectrum of\nentity types. Our work directly addresses this issue. We propose Heuristics\nAllied with Distant Supervision (HAnDS) framework to automatically construct a\nquality dataset suitable for the FgER task. HAnDS framework exploits the high\ninterlink among Wikipedia and Freebase in a pipelined manner, reducing\nannotation errors introduced by naively using distant supervision approach.\nUsing HAnDS framework, we create two datasets, one suitable for building FgER\nsystems recognizing up to 118 entity types based on the FIGER type hierarchy\nand another for up to 1115 entity types based on the TypeNet hierarchy. Our\nextensive empirical experimentation warrants the quality of the generated\ndatasets. Along with this, we also provide a manually annotated dataset for\nbenchmarking FgER systems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 11:51:52 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Abhishek", "Abhishek", ""], ["Taneja", "Sanya Bathla", ""], ["Malik", "Garima", ""], ["Anand", "Ashish", ""], ["Awekar", "Amit", ""]]}, {"id": "1904.13258", "submitter": "Samuel Thomas", "authors": "Samuel Thomas, Masayuki Suzuki, Yinghui Huang, Gakuto Kurata, Zoltan\n  Tuske, George Saon, Brian Kingsbury, Michael Picheny, Tom Dibert, Alice\n  Kaiser-Schatzlein, Bern Samko", "title": "English Broadcast News Speech Recognition by Humans and Machines", "comments": "\\copyright 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683211", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances in deep learning, considerable attention has been given\nto achieving automatic speech recognition performance close to human\nperformance on tasks like conversational telephone speech (CTS) recognition. In\nthis paper we evaluate the usefulness of these proposed techniques on broadcast\nnews (BN), a similar challenging task. We also perform a set of recognition\nmeasurements to understand how close the achieved automatic speech recognition\nresults are to human performance on this task. On two publicly available BN\ntest sets, DEV04F and RT04, our speech recognition system using LSTM and\nresidual network based acoustic models with a combination of n-gram and neural\nnetwork language models performs at 6.5% and 5.9% word error rate. By achieving\nnew performance milestones on these test sets, our experiments show that\ntechniques developed on other related tasks, like CTS, can be transferred to\nachieve similar performance. In contrast, the best measured human recognition\nperformance on these test sets is much lower, at 3.6% and 2.8% respectively,\nindicating that there is still room for new techniques and improvements in this\nspace, to reach human performance levels.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:59:18 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Thomas", "Samuel", ""], ["Suzuki", "Masayuki", ""], ["Huang", "Yinghui", ""], ["Kurata", "Gakuto", ""], ["Tuske", "Zoltan", ""], ["Saon", "George", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Dibert", "Tom", ""], ["Kaiser-Schatzlein", "Alice", ""], ["Samko", "Bern", ""]]}, {"id": "1904.13264", "submitter": "Vitalii Zhelezniak", "authors": "Vitalii Zhelezniak, Aleksandar Savkov, April Shen, Francesco\n  Moramarco, Jack Flann, Nils Y. Hammerla", "title": "Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word\n  Vectors", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature suggests that averaged word vectors followed by simple\npost-processing outperform many deep learning methods on semantic textual\nsimilarity tasks. Furthermore, when averaged word vectors are trained\nsupervised on large corpora of paraphrases, they achieve state-of-the-art\nresults on standard STS benchmarks. Inspired by these insights, we push the\nlimits of word embeddings even further. We propose a novel fuzzy bag-of-words\n(FBoW) representation for text that contains all the words in the vocabulary\nsimultaneously but with different degrees of membership, which are derived from\nsimilarities between word vectors. We show that max-pooled word vectors are\nonly a special case of fuzzy BoW and should be compared via fuzzy Jaccard index\nrather than cosine similarity. Finally, we propose DynaMax, a completely\nunsupervised and non-parametric similarity measure that dynamically extracts\nand max-pools good features depending on the sentence pair. This method is both\nefficient and easy to implement, yet outperforms current baselines on STS tasks\nby a large margin and is even competitive with supervised word vectors trained\nto directly optimise cosine similarity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 14:08:37 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Zhelezniak", "Vitalii", ""], ["Savkov", "Aleksandar", ""], ["Shen", "April", ""], ["Moramarco", "Francesco", ""], ["Flann", "Jack", ""], ["Hammerla", "Nils Y.", ""]]}, {"id": "1904.13323", "submitter": "Kamen Brestnichki", "authors": "Francisco Vargas, Kamen Brestnichki, Nils Hammerla", "title": "Model Comparison for Semantic Grouping", "comments": "Proceedings of the 36th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a probabilistic framework for quantifying the semantic\nsimilarity between two groups of embeddings. We formulate the task of semantic\nsimilarity as a model comparison task in which we contrast a generative model\nwhich jointly models two sentences versus one that does not. We illustrate how\nthis framework can be used for the Semantic Textual Similarity tasks using\nclear assumptions about how the embeddings of words are generated. We apply\nmodel comparison that utilises information criteria to address some of the\nshortcomings of Bayesian model comparison, whilst still penalising model\ncomplexity. We achieve competitive results by applying the proposed framework\nwith an appropriate choice of likelihood on the STS datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 15:37:16 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 10:52:54 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Vargas", "Francisco", ""], ["Brestnichki", "Kamen", ""], ["Hammerla", "Nils", ""]]}, {"id": "1904.13377", "submitter": "Ngoc Quan Pham", "authors": "Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus M\\\"uller,\n  Sebastian St\\\"uker, Alexander Waibel", "title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition", "comments": "Submitted to INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, end-to-end sequence-to-sequence models for speech recognition have\ngained significant interest in the research community. While previous\narchitecture choices revolve around time-delay neural networks (TDNN) and long\nshort-term memory (LSTM) recurrent neural networks, we propose to use\nself-attention via the Transformer architecture as an alternative. Our analysis\nshows that deep Transformer networks with high learning capacity are able to\nexceed performance from previous end-to-end approaches and even match the\nconventional hybrid systems. Moreover, we trained very deep models with up to\n48 Transformer layers for both encoder and decoders combined with stochastic\nresidual connections, which greatly improve generalizability and training\nefficiency. The resulting models outperform all previous end-to-end ASR\napproaches on the Switchboard benchmark. An ensemble of these models achieve\n9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This\nfinding brings our end-to-end models to competitive levels with previous hybrid\nsystems. Further, with model ensembling the Transformers can outperform certain\nhybrid systems, which are more complicated in terms of both structure and\ntraining procedure.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 17:20:32 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 14:00:16 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Pham", "Ngoc-Quan", ""], ["Nguyen", "Thai-Son", ""], ["Niehues", "Jan", ""], ["M\u00fcller", "Markus", ""], ["St\u00fcker", "Sebastian", ""], ["Waibel", "Alexander", ""]]}]