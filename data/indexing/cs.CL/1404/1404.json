[{"id": "1404.0850", "submitter": "EPTCS", "authors": "Rui Couto (University of Minho), Ant\\'onio Nestor Ribeiro (University\n  of Minho), Jos\\'e Creissac Campos (University of Minho)", "title": "Application of Ontologies in Identifying Requirements Patterns in Use\n  Cases", "comments": "In Proceedings FESCA 2014, arXiv:1404.0436", "journal-ref": "EPTCS 147, 2014, pp. 62-76", "doi": "10.4204/EPTCS.147.5", "report-no": null, "categories": "cs.SE cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use case specifications have successfully been used for requirements\ndescription. They allow joining, in the same modeling space, the expectations\nof the stakeholders as well as the needs of the software engineer and analyst\ninvolved in the process. While use cases are not meant to describe a system's\nimplementation, by formalizing their description we are able to extract\nimplementation relevant information from them. More specifically, we are\ninterested in identifying requirements patterns (common requirements with\ntypical implementation solutions) in support for a requirements based software\ndevelopment approach. In the paper we propose the transformation of Use Case\ndescriptions expressed in a Controlled Natural Language into an ontology\nexpressed in the Web Ontology Language (OWL). OWL's query engines can then be\nused to identify requirements patterns expressed as queries over the ontology.\nWe describe a tool that we have developed to support the approach and provide\nan example of usage.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 10:44:20 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Couto", "Rui", "", "University of Minho"], ["Ribeiro", "Ant\u00f3nio Nestor", "", "University\n  of Minho"], ["Campos", "Jos\u00e9 Creissac", "", "University of Minho"]]}, {"id": "1404.1521", "submitter": "Vivek Kulkarni", "authors": "Vivek Kulkarni, Rami Al-Rfou', Bryan Perozzi, Steven Skiena", "title": "Exploring the power of GPU's for training Polyglot language models", "comments": "version 2 (just corrected citation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major research trends currently is the evolution of heterogeneous\nparallel computing. GP-GPU computing is being widely used and several\napplications have been designed to exploit the massive parallelism that\nGP-GPU's have to offer. While GPU's have always been widely used in areas of\ncomputer vision for image processing, little has been done to investigate\nwhether the massive parallelism provided by GP-GPU's can be utilized\neffectively for Natural Language Processing(NLP) tasks. In this work, we\ninvestigate and explore the power of GP-GPU's in the task of learning language\nmodels. More specifically, we investigate the performance of training Polyglot\nlanguage models using deep belief neural networks. We evaluate the performance\nof training the model on the GPU and present optimizations that boost the\nperformance on the GPU.One of the key optimizations, we propose increases the\nperformance of a function involved in calculating and updating the gradient by\napproximately 50 times on the GPU for sufficiently large batch sizes. We show\nthat with the above optimizations, the GP-GPU's performance on the task\nincreases by factor of approximately 3-4. The optimizations we made are generic\nTheano optimizations and hence potentially boost the performance of other\nmodels which rely on these operations.We also show that these optimizations\nresult in the GPU's performance at this task being now comparable to that on\nthe CPU. We conclude by presenting a thorough evaluation of the applicability\nof GP-GPU's for this task and highlight the factors limiting the performance of\ntraining a Polyglot model on the GPU.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 21:25:54 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 16:32:22 GMT"}, {"version": "v3", "created": "Tue, 15 Apr 2014 13:18:37 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Kulkarni", "Vivek", ""], ["Al-Rfou'", "Rami", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1404.1847", "submitter": "Mahima Sharma", "authors": "Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, Ajai Kumar, Hemant\n  Darbari", "title": "Evaluation and Ranking of Machine Translated Output in Hindi Language\n  using Precision and Recall Oriented Metrics", "comments": null, "journal-ref": "International Journal of Advanced Computer Research, Volume-4\n  Number-1 Issue-14 March 2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation plays a crucial role in development of Machine translation\nsystems. In order to judge the quality of an existing MT system i.e. if the\ntranslated output is of human translation quality or not, various automatic\nmetrics exist. We here present the implementation results of different metrics\nwhen used on Hindi language along with their comparisons, illustrating how\neffective are these metrics on languages like Hindi (free word order language).\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 16:45:42 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Kalyani", "Aditi", ""], ["Kumud", "Hemant", ""], ["Singh", "Shashi Pal", ""], ["Kumar", "Ajai", ""], ["Darbari", "Hemant", ""]]}, {"id": "1404.1872", "submitter": "Lingu LIGM", "authors": "Anthony Sigogne (LIGM), Matthieu Constant (LIGM), Eric Laporte (LIGM)", "title": "Int\\'egration des donn\\'ees d'un lexique syntaxique dans un analyseur\n  syntaxique probabiliste", "comments": "in French", "journal-ref": "Penser le Lexique-Grammaire. Perspectives actuelles, Fryni\n  Kakoyianni-Doa (Ed.) (2014) 505-516", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reports the evaluation of the integration of data from a\nsyntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntactic\nparser. We show that by changing the set of labels for verbs and predicational\nnouns, we can improve the performance on French of a non-lexicalized\nprobabilistic parser.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 18:12:08 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Sigogne", "Anthony", "", "LIGM"], ["Constant", "Matthieu", "", "LIGM"], ["Laporte", "Eric", "", "LIGM"]]}, {"id": "1404.1890", "submitter": "Agata Fronczak", "authors": "Maksymilian Bujok, Piotr Fronczak, Agata Fronczak", "title": "Polish and English wordnets -- statistical analysis of interconnected\n  networks", "comments": "12 pages, 10 figures, Presented at Summer Solstice 2013 Conference on\n  Discrete Models of Complex Systems, Warsaw, Poland", "journal-ref": "Acta Phys. Pol. B Proc. Suppl. Vol. 7 (2014) 245-256", "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wordnets are semantic networks containing nouns, verbs, adjectives, and\nadverbs organized according to linguistic principles, by means of semantic\nrelations. In this work, we adopt a complex network perspective to perform a\ncomparative analysis of the English and Polish wordnets. We determine their\nsimilarities and show that the networks exhibit some of the typical\ncharacteristics observed in other real-world networks. We analyse interlingual\nrelations between both wordnets and deliberate over the problem of mapping the\nPolish lexicon onto the English one.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 19:23:29 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Bujok", "Maksymilian", ""], ["Fronczak", "Piotr", ""], ["Fronczak", "Agata", ""]]}, {"id": "1404.1982", "submitter": "Amani Samha", "authors": "Amani K Samha, Yuefeng Li and Jinglan Zhang", "title": "Aspect-Based Opinion Extraction from Customer reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text is the main method of communicating information in the digital age.\nMessages, blogs, news articles, reviews, and opinionated information abound on\nthe Internet. People commonly purchase products online and post their opinions\nabout purchased items. This feedback is displayed publicly to assist others\nwith their purchasing decisions, creating the need for a mechanism with which\nto extract and summarize useful information for enhancing the decision-making\nprocess. Our contribution is to improve the accuracy of extraction by combining\ndifferent techniques from three major areas, named Data Mining, Natural\nLanguage Processing techniques and Ontologies. The proposed framework\nsequentially mines products aspects and users opinions, groups representative\naspects by similarity, and generates an output summary. This paper focuses on\nthe task of extracting product aspects and users opinions by extracting all\npossible aspects and opinions from reviews using natural language, ontology,\nand frequent (tag) sets. The proposed framework, when compared with an existing\nbaseline model, yielded promising results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 01:19:14 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Samha", "Amani K", ""], ["Li", "Yuefeng", ""], ["Zhang", "Jinglan", ""]]}, {"id": "1404.2071", "submitter": "Normunds Gr\\=uz\\=itis", "authors": "Dana Dann\\'ells and Normunds Gr\\=uz\\=itis", "title": "Extracting a bilingual semantic grammar from FrameNet-annotated corpora", "comments": null, "journal-ref": "Proceedings of the 9th International Conference on Language\n  Resources and Evaluation (LREC), 2014, pp. 2466-2473", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the creation of an English-Swedish FrameNet-based grammar in\nGrammatical Framework. The aim of this research is to make existing framenets\ncomputationally accessible for multilingual natural language applications via a\ncommon semantic grammar API, and to facilitate the porting of such grammar to\nother languages. In this paper, we describe the abstract syntax of the semantic\ngrammar while focusing on its automatic extraction possibilities. We have\nextracted a shared abstract syntax from ~58,500 annotated sentences in Berkeley\nFrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). The\nabstract syntax defines 769 frame-specific valence patterns that cover 77.8%\nexamples in BFN and 74.9% in SweFN belonging to the shared set of 471 frames.\nAs a side result, we provide a unified method for comparing semantic and\nsyntactic valence patterns across framenets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 10:08:22 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Dann\u00e9lls", "Dana", ""], ["Gr\u016bz\u012btis", "Normunds", ""]]}, {"id": "1404.2188", "submitter": "Nal Kalchbrenner", "authors": "Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom", "title": "A Convolutional Neural Network for Modelling Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately represent sentences is central to language\nunderstanding. We describe a convolutional architecture dubbed the Dynamic\nConvolutional Neural Network (DCNN) that we adopt for the semantic modelling of\nsentences. The network uses Dynamic k-Max Pooling, a global pooling operation\nover linear sequences. The network handles input sentences of varying length\nand induces a feature graph over the sentence that is capable of explicitly\ncapturing short and long-range relations. The network does not rely on a parse\ntree and is easily applicable to any language. We test the DCNN in four\nexperiments: small scale binary and multi-class sentiment prediction, six-way\nquestion classification and Twitter sentiment prediction by distant\nsupervision. The network achieves excellent performance in the first three\ntasks and a greater than 25% error reduction in the last task with respect to\nthe strongest baseline.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 15:46:44 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Kalchbrenner", "Nal", ""], ["Grefenstette", "Edward", ""], ["Blunsom", "Phil", ""]]}, {"id": "1404.2878", "submitter": "Mahima Sharma", "authors": "Dalwadi Bijal, Suthar Sanket", "title": "Overview of Stemming Algorithms for Indian and Non-Indian Languages", "comments": null, "journal-ref": "International Journal of Computer Science and Information\n  Technologies, Vol. 5 (2) , 2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stemming is a pre-processing step in Text Mining applications as well as a\nvery common requirement of Natural Language processing functions. Stemming is\nthe process for reducing inflected words to their stem. The main purpose of\nstemming is to reduce different grammatical forms / word forms of a word like\nits noun, adjective, verb, adverb etc. to its root form. Stemming is widely\nuses in Information Retrieval system and reduces the size of index files. We\ncan say that the goal of stemming is to reduce inflectional forms and sometimes\nderivationally related forms of a word to a common base form. In this paper we\nhave discussed different stemming algorithm for non-Indian and Indian language,\nmethods of stemming, accuracy and errors.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 17:16:01 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Bijal", "Dalwadi", ""], ["Sanket", "Suthar", ""]]}, {"id": "1404.2997", "submitter": "Jean-Gabriel Ganascia", "authors": "Jean-Gabriel Ganascia (LIP6), Pierre Glaudes (CELFF XVI-XXI), Andrea\n  Del Lungo (ALITHILA)", "title": "Automatic Detection of Reuses and Citations in Literary Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than forty years now, modern theories of literature (Compagnon,\n1979) insist on the role of paraphrases, rewritings, citations, reciprocal\nborrowings and mutual contributions of any kinds. The notions of\nintertextuality, transtextuality, hypertextuality/hypotextuality, were\nintroduced in the seventies and eighties to approach these phenomena. The\ncareful analysis of these references is of particular interest in evaluating\nthe distance that the creator voluntarily introduces with his/her masters.\nPhoebus is collaborative project that makes computer scientists from the\nUniversity Pierre and Marie Curie (LIP6-UPMC) collaborate with the literary\nteams of Paris-Sorbonne University with the aim to develop efficient tools for\nliterary studies that take advantage of modern computer science techniques. In\nthis context, we have developed a piece of software that automatically detects\nand explores networks of textual reuses in classical literature. This paper\ndescribes the principles on which is based this program, the significant\nresults that have already been obtained and the perspectives for the near\nfuture.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 04:36:05 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Ganascia", "Jean-Gabriel", "", "LIP6"], ["Glaudes", "Pierre", "", "CELFF XVI-XXI"], ["Del Lungo", "Andrea", "", "ALITHILA"]]}, {"id": "1404.3026", "submitter": "Todd Bodnar", "authors": "Todd Bodnar, Victoria C Barclay, Nilam Ram, Conrad S Tucker, Marcel\n  Salath\\'e", "title": "On the Ground Validation of Online Diagnosis with Twitter and Medical\n  Records", "comments": "Presented at of WWW2014. WWW'14 Companion, April 7-11, 2014, Seoul,\n  Korea", "journal-ref": null, "doi": "10.1145/2567948.2579272", "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has been considered as a data source for tracking disease.\nHowever, most analyses are based on models that prioritize strong correlation\nwith population-level disease rates over determining whether or not specific\nindividual users are actually sick. Taking a different approach, we develop a\nnovel system for social-media based disease detection at the individual level\nusing a sample of professionally diagnosed individuals. Specifically, we\ndevelop a system for making an accurate influenza diagnosis based on an\nindividual's publicly available Twitter data. We find that about half (17/35 =\n48.57%) of the users in our sample that were sick explicitly discuss their\ndisease on Twitter. By developing a meta classifier that combines text\nanalysis, anomaly detection, and social network analysis, we are able to\ndiagnose an individual with greater than 99% accuracy even if she does not\ndiscuss her health.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 07:55:51 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Bodnar", "Todd", ""], ["Barclay", "Victoria C", ""], ["Ram", "Nilam", ""], ["Tucker", "Conrad S", ""], ["Salath\u00e9", "Marcel", ""]]}, {"id": "1404.3233", "submitter": "Joshua Hailpern", "authors": "Joshua Hailpern, Niranjan Damera Venkata, Marina Danilevsky", "title": "Pagination: It's what you say, not how long it takes to say it", "comments": "10 pages, Submitted to DOCENG'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pagination - the process of determining where to break an article across\npages in a multi-article layout is a common layout challenge for most\ncommercially printed newspapers and magazines. To date, no one has created an\nalgorithm that determines a minimal pagination break point based on the content\nof the article. Existing approaches for automatic multi-article layout focus\nexclusively on maximizing content (number of articles) and optimizing aesthetic\npresentation (e.g., spacing between articles). However, disregarding the\nsemantic information within the article can lead to overly aggressive cutting,\nthereby eliminating key content and potentially confusing the reader, or\nsetting too generous of a break point, thereby leaving in superfluous content\nand making automatic layout more difficult. This is one of the remaining\nchallenges on the path from manual layouts to fully automated processes that\nstill ensure article content quality. In this work, we present a new approach\nto calculating a document minimal break point for the task of pagination. Our\napproach uses a statistical language model to predict minimal break points\nbased on the semantic content of an article. We then compare 4 novel candidate\napproaches, and 4 baselines (currently in use by layout algorithms). Results\nfrom this experiment show that one of our approaches strongly outperforms the\nbaselines and alternatives. Results from a second study suggest that humans are\nnot able to agree on a single \"best\" break point. Therefore, this work shows\nthat a semantic-based lower bound break point prediction is necessary for ideal\nautomated document synthesis within a real-world context.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 21:50:02 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Hailpern", "Joshua", ""], ["Venkata", "Niranjan Damera", ""], ["Danilevsky", "Marina", ""]]}, {"id": "1404.3377", "submitter": "Rene Pickhardt", "authors": "Rene Pickhardt, Thomas Gottron, Martin K\\\"orner, Paul Georg Wagner,\n  Till Speicher, Steffen Staab", "title": "A Generalized Language Model as the Combination of Skipped n-grams and\n  Modified Kneser-Ney Smoothing", "comments": "13 pages, 2 figures, ACL 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach for building language models based on a\nsystematic, recursive exploration of skip n-gram models which are interpolated\nusing modified Kneser-Ney smoothing. Our approach generalizes language models\nas it contains the classical interpolation with lower order models as a special\ncase. In this paper we motivate, formalize and present our approach. In an\nextensive empirical experiment over English text corpora we demonstrate that\nour generalized language models lead to a substantial reduction of perplexity\nbetween 3.1% and 12.7% in comparison to traditional language models using\nmodified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over\nthree other languages and a domain specific corpus where we observed consistent\nimprovements. Finally, we also show that the strength of our approach lies in\nits ability to cope in particular with sparse training data. Using a very small\ntraining data set of only 736 KB text we yield improvements of even 25.7%\nreduction of perplexity.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 12:39:41 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Pickhardt", "Rene", ""], ["Gottron", "Thomas", ""], ["K\u00f6rner", "Martin", ""], ["Wagner", "Paul Georg", ""], ["Speicher", "Till", ""], ["Staab", "Steffen", ""]]}, {"id": "1404.3610", "submitter": "Cosme Adrover", "authors": "Cosme Adrover, Todd Bodnar, Marcel Salathe", "title": "Targeting HIV-related Medication Side Effects and Sentiment Using\n  Twitter Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a descriptive analysis of Twitter data. Our study focuses on\nextracting the main side effects associated with HIV treatments. The crux of\nour work was the identification of personal tweets referring to HIV. We\nsummarize our results in an infographic aimed at the general public. In\naddition, we present a measure of user sentiment based on hand-rated tweets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 02:33:17 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Adrover", "Cosme", ""], ["Bodnar", "Todd", ""], ["Salathe", "Marcel", ""]]}, {"id": "1404.3759", "submitter": "Bogdan Babych", "authors": "Bogdan Babych and Anthony Hartley", "title": "Meta-evaluation of comparability metrics using parallel corpora", "comments": "10 pages, 3 figures, 12th International Conference on Intelligent\n  Text Processing and Computational Linguistics CICLing 2011. February 20 to\n  26, 2011, Tokyo, Japan. International Journal of Computational Linguistics\n  and Applications, Proceedings volume of CICLing-2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metrics for measuring the comparability of corpora or texts need to be\ndeveloped and evaluated systematically. Applications based on a corpus, such as\ntraining Statistical MT systems in specialised narrow domains, require finding\na reasonable balance between the size of the corpus and its consistency, with\ncontrolled and benchmarked levels of comparability for any newly added\nsections. In this article we propose a method that can meta-evaluate\ncomparability metrics by calculating monolingual comparability scores\nseparately on the 'source' and 'target' sides of parallel corpora. The range of\nscores on the source side is then correlated (using Pearson's r coefficient)\nwith the range of 'target' scores; the higher the correlation - the more\nreliable is the metric. The intuition is that a good metric should yield the\nsame distance between different domains in different languages. Our method\ngives consistent results for the same metrics on different data sets, which\nindicates that it is reliable and can be used for metric comparison or for\noptimising settings of parametrised metrics.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 21:33:42 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Babych", "Bogdan", ""], ["Hartley", "Anthony", ""]]}, {"id": "1404.3925", "submitter": "EPTCS", "authors": "Antonin Delpeuch (\\'Ecole Normale Sup\\'erieure, Paris)", "title": "Complexity of Grammar Induction for Quantum Types", "comments": "In Proceedings QPL 2014, arXiv:1412.8102", "journal-ref": "EPTCS 172, 2014, pp. 236-248", "doi": "10.4204/EPTCS.172.16", "report-no": null, "categories": "cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most categorical models of meaning use a functor from the syntactic category\nto the semantic category. When semantic information is available, the problem\nof grammar induction can therefore be defined as finding preimages of the\nsemantic types under this forgetful functor, lifting the information flow from\nthe semantic level to a valid reduction at the syntactic level. We study the\ncomplexity of grammar induction, and show that for a variety of type systems,\nincluding pivotal and compact closed categories, the grammar induction problem\nis NP-complete. Our approach could be extended to linguistic type systems such\nas autonomous or bi-closed categories.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 14:19:27 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 03:02:40 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Delpeuch", "Antonin", "", "\u00c9cole Normale Sup\u00e9rieure, Paris"]]}, {"id": "1404.3959", "submitter": "Marco Guerini", "authors": "Marco Guerini, Fabio Pianesi, Oliviero Stock", "title": "Is it morally acceptable for a system to lie to persuade me?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the fast rise of increasingly autonomous artificial agents and robots,\na key acceptability criterion will be the possible moral implications of their\nactions. In particular, intelligent persuasive systems (systems designed to\ninfluence humans via communication) constitute a highly sensitive topic because\nof their intrinsically social nature. Still, ethical studies in this area are\nrare and tend to focus on the output of the required action. Instead, this work\nfocuses on the persuasive acts themselves (e.g. \"is it morally acceptable that\na machine lies or appeals to the emotions of a person to persuade her, even if\nfor a good end?\"). Exploiting a behavioral approach, based on human assessment\nof moral dilemmas -- i.e. without any prior assumption of underlying ethical\ntheories -- this paper reports on a set of experiments. These experiments\naddress the type of persuader (human or machine), the strategies adopted\n(purely argumentative, appeal to positive emotions, appeal to negative\nemotions, lie) and the circumstances. Findings display no differences due to\nthe agent, mild acceptability for persuasion and reveal that truth-conditional\nreasoning (i.e. argument validity) is a significant dimension affecting\nsubjects' judgment. Some implications for the design of intelligent persuasive\nsystems are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 15:41:34 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Guerini", "Marco", ""], ["Pianesi", "Fabio", ""], ["Stock", "Oliviero", ""]]}, {"id": "1404.3992", "submitter": "Mahima Sharma", "authors": "Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, Ajai Kumar", "title": "Assessing the Quality of MT Systems for Hindi to English Translation", "comments": null, "journal-ref": "International Journal of Computer Applications, Volume 89, No 15,\n  March 2014", "doi": "10.5120/15711-4629", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation plays a vital role in checking the quality of MT output. It is\ndone either manually or automatically. Manual evaluation is very time consuming\nand subjective, hence use of automatic metrics is done most of the times. This\npaper evaluates the translation quality of different MT Engines for\nHindi-English (Hindi data is provided as input and English is obtained as\noutput) using various automatic metrics like BLEU, METEOR etc. Further the\ncomparison automatic evaluation results with Human ranking have also been\ngiven.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 17:13:26 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Kalyani", "Aditi", ""], ["Kumud", "Hemant", ""], ["Singh", "Shashi Pal", ""], ["Kumar", "Ajai", ""]]}, {"id": "1404.4314", "submitter": "Noah Smith", "authors": "Lingpeng Kong, Noah A. Smith", "title": "An Empirical Comparison of Parsing Methods for Stanford Dependencies", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stanford typed dependencies are a widely desired representation of natural\nlanguage sentences, but parsing is one of the major computational bottlenecks\nin text analysis systems. In light of the evolving definition of the Stanford\ndependencies and developments in statistical dependency parsing algorithms,\nthis paper revisits the question of Cer et al. (2010): what is the tradeoff\nbetween accuracy and speed in obtaining Stanford dependencies in particular? We\nalso explore the effects of input representations on this tradeoff:\npart-of-speech tags, the novel use of an alternative dependency representation\nas input, and distributional representaions of words. We find that direct\ndependency parsing is a more viable solution than it was found to be in the\npast. An accompanying software release can be found at:\nhttp://www.ark.cs.cmu.edu/TBSD\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 17:06:35 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Kong", "Lingpeng", ""], ["Smith", "Noah A.", ""]]}, {"id": "1404.4326", "submitter": "Jason  Weston", "authors": "Antoine Bordes, Jason Weston and Nicolas Usunier", "title": "Open Question Answering with Weakly Supervised Embedding Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building computers able to answer questions on any subject is a long standing\ngoal of artificial intelligence. Promising progress has recently been achieved\nby methods that learn to map questions to logical forms or database queries.\nSuch approaches can be effective but at the cost of either large amounts of\nhuman-labeled data or by defining lexicons and grammars tailored by\npractitioners. In this paper, we instead take the radical approach of learning\nto map questions to vectorial feature representations. By mapping answers into\nthe same space one can query any knowledge base independent of its schema,\nwithout requiring any grammar or lexicon. Our method is trained with a new\noptimization procedure combining stochastic gradient descent followed by a\nfine-tuning step using the weak supervision provided by blending automatically\nand collaboratively generated resources. We empirically demonstrate that our\nmodel can capture meaningful signals from its noisy supervision leading to\nmajor improvements over paralex, the only existing method able to be trained on\nsimilar weakly labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 17:57:01 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Bordes", "Antoine", ""], ["Weston", "Jason", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1404.4572", "submitter": "Behrang  Qasemizadeh", "authors": "Behrang Qasemizadeh, Saeed Rahimi, Behrooz Mahmoodi Bakhtiari", "title": "The First Parallel Multilingual Corpus of Persian: Toward a Persian\n  BLARK", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we have introduced the first parallel corpus of Persian with\nmore than 10 other European languages. This article describes primary steps\ntoward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now,\nwe have proposed morphosyntactic specification of Persian based on\nEAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The article\nintroduces Persian Language, with emphasis on its orthography and\nmorphosyntactic features, then a new Part-of-Speech categorization and\northography for Persian in digital environments is proposed. Finally, the\ncorpus and related statistic will be analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 16:22:40 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Qasemizadeh", "Behrang", ""], ["Rahimi", "Saeed", ""], ["Bakhtiari", "Behrooz Mahmoodi", ""]]}, {"id": "1404.4606", "submitter": "Derek Greene", "authors": "Derek Greene, Derek O'Callaghan, P\\'adraig Cunningham", "title": "How Many Topics? Stability Analysis for Topic Models", "comments": "Improve readability of plots. Add minor clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling refers to the task of discovering the underlying thematic\nstructure in a text corpus, where the output is commonly presented as a report\nof the top terms appearing in each topic. Despite the diversity of topic\nmodeling algorithms that have been proposed, a common challenge in successfully\napplying these techniques is the selection of an appropriate number of topics\nfor a given corpus. Choosing too few topics will produce results that are\noverly broad, while choosing too many will result in the \"over-clustering\" of a\ncorpus into many small, highly-similar topics. In this paper, we propose a\nterm-centric stability analysis strategy to address this issue, the idea being\nthat a model with an appropriate number of topics will be more robust to\nperturbations in the data. Using a topic modeling approach based on matrix\nfactorization, evaluations performed on a range of corpora show that this\nstrategy can successfully guide the model selection process.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 12:59:29 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 12:58:28 GMT"}, {"version": "v3", "created": "Thu, 19 Jun 2014 12:58:13 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Greene", "Derek", ""], ["O'Callaghan", "Derek", ""], ["Cunningham", "P\u00e1draig", ""]]}, {"id": "1404.4641", "submitter": "Karl Moritz Hermann", "authors": "Karl Moritz Hermann and Phil Blunsom", "title": "Multilingual Models for Compositional Distributed Semantics", "comments": "Proceedings of ACL 2014 (Long papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique for learning semantic representations, which\nextends the distributional hypothesis to multilingual data and joint-space\nembeddings. Our models leverage parallel data and learn to strongly align the\nembeddings of semantically equivalent sentences, while maintaining sufficient\ndistance between those of dissimilar sentences. The models do not rely on word\nalignments or any syntactic information and are successfully applied to a\nnumber of diverse languages. We extend our approach to learn semantic\nrepresentations at the document level, too. We evaluate these models on two\ncross-lingual document classification tasks, outperforming the prior state of\nthe art. Through qualitative analysis and the study of pivoting effects we\ndemonstrate that our representations are semantically plausible and can capture\nsemantic relationships across languages without parallel data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 20:18:03 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Hermann", "Karl Moritz", ""], ["Blunsom", "Phil", ""]]}, {"id": "1404.4714", "submitter": "Duyu Tang", "authors": "Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, Xiaolong Wang", "title": "Radical-Enhanced Chinese Character Embedding", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present a method to leverage radical for learning Chinese character\nembedding. Radical is a semantic and phonetic component of Chinese character.\nIt plays an important role as characters with the same radical usually have\nsimilar semantic meaning and grammatical usage. However, existing Chinese\nprocessing algorithms typically regard word or character as the basic unit but\nignore the crucial radical information. In this paper, we fill this gap by\nleveraging radical for learning continuous representation of Chinese character.\nWe develop a dedicated neural architecture to effectively learn character\nembedding and apply it on Chinese character similarity judgement and Chinese\nword segmentation. Experiment results show that our radical-enhanced method\noutperforms existing embedding learning algorithms on both tasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 07:48:02 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Sun", "Yaming", ""], ["Lin", "Lei", ""], ["Tang", "Duyu", ""], ["Yang", "Nan", ""], ["Ji", "Zhenzhou", ""], ["Wang", "Xiaolong", ""]]}, {"id": "1404.4740", "submitter": "Behrang  Qasemizadeh", "authors": "Behrang QasemiZadeh, Saeed Rahimi and Mehdi Safaee Ghalati", "title": "Challenges in Persian Electronic Text Analysis", "comments": "Appeared in a Local conference 2006, available for the first time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Farsi, also known as Persian, is the official language of Iran and Tajikistan\nand one of the two main languages spoken in Afghanistan. Farsi enjoys a unified\nArabic script as its writing system. In this paper we briefly introduce the\nwriting standards of Farsi and highlight problems one would face when analyzing\nFarsi electronic texts, especially during development of Farsi corpora\nregarding to transcription and encoding of Farsi e-texts. The pointes mentioned\nmay sounds easy but they are crucial when developing and processing written\ncorpora of Farsi.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 10:30:47 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["QasemiZadeh", "Behrang", ""], ["Rahimi", "Saeed", ""], ["Ghalati", "Mehdi Safaee", ""]]}, {"id": "1404.4935", "submitter": "Richa Sharma", "authors": "Richa Sharma, Shweta Nigam, Rekha Jain", "title": "Opinion Mining In Hindi Language: A Survey", "comments": null, "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology (IJFCST) International Journal in Foundations of Computer Science\n  & Technology (IJFCST), Vol.4, No.2, March 2014", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinions are very important in the life of human beings. These Opinions\nhelped the humans to carry out the decisions. As the impact of the Web is\nincreasing day by day, Web documents can be seen as a new source of opinion for\nhuman beings. Web contains a huge amount of information generated by the users\nthrough blogs, forum entries, and social networking websites and so on To\nanalyze this large amount of information it is required to develop a method\nthat automatically classifies the information available on the Web. This domain\nis called Sentiment Analysis and Opinion Mining. Opinion Mining or Sentiment\nAnalysis is a natural language processing task that mine information from\nvarious text forms such as reviews, news, and blogs and classify them on the\nbasis of their polarity as positive, negative or neutral. But, from the last\nfew years, enormous increase has been seen in Hindi language on the Web.\nResearch in opinion mining mostly carried out in English language but it is\nvery important to perform the opinion mining in Hindi language also as large\namount of information in Hindi is also available on the Web. This paper gives\nan overview of the work that has been done Hindi language.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 08:14:39 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Sharma", "Richa", ""], ["Nigam", "Shweta", ""], ["Jain", "Rekha", ""]]}, {"id": "1404.5278", "submitter": "Mehrnoosh Sadrzadeh", "authors": "Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke", "title": "The Frobenius anatomy of word meanings I: subject and object relative\n  pronouns", "comments": "31 pages", "journal-ref": "Journal of Logic and Computation, Special Issue: The Incomputable,\n  an Isaac Newton Institute Workshop, 23(6), pp.1293-1317, 2013", "doi": "10.1093/logcom/ext044", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a compositional vector-based semantics of subject and\nobject relative pronouns within a categorical framework. Frobenius algebras are\nused to formalise the operations required to model the semantics of relative\npronouns, including passing information between the relative clause and the\nmodified noun phrase, as well as copying, combining, and discarding parts of\nthe relative clause. We develop two instantiations of the abstract semantics,\none based on a truth-theoretic approach and one based on corpus statistics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 19:31:48 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Sadrzadeh", "Mehrnoosh", ""], ["Clark", "Stephen", ""], ["Coecke", "Bob", ""]]}, {"id": "1404.5357", "submitter": "Nayan Jyoti Kalita", "authors": "Nayan Jyoti Kalita, Navanath Saharia, and Smriti Kumar Sinha", "title": "Morphological Analysis of the Bishnupriya Manipuri Language using Finite\n  State Transducers", "comments": null, "journal-ref": "Computational Linguistics and Intelligent Text Processing,\n  vol.8403, series: Lecture Notes in Computer Science, pp 206-213, (2014)", "doi": "10.1007/978-3-642-54906-9_16", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a morphological analysis of Bishnupriya Manipuri\nlanguage, an Indo-Aryan language spoken in the north eastern India. As of now,\nthere is no computational work available for the language. Finite state\nmorphology is one of the successful approaches applied in a wide variety of\nlanguages over the year. Therefore we adapted the finite state approach to\nanalyse morphology of the Bishnupriya Manipuri language.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 00:17:26 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Kalita", "Nayan Jyoti", ""], ["Saharia", "Navanath", ""], ["Sinha", "Smriti Kumar", ""]]}, {"id": "1404.5367", "submitter": "Alexandre Passos", "authors": "Alexandre Passos, Vineet Kumar, Andrew McCallum", "title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution", "comments": "Accepted in CoNLL 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art approaches for named-entity recognition (NER) use semi\nsupervised information in the form of word clusters and lexicons. Recently\nneural network-based language models have been explored, as they as a byproduct\ngenerate highly informative vector representations for words, known as word\nembeddings. In this paper we present two contributions: a new form of learning\nword embeddings that can leverage information from relevant lexicons to improve\nthe representations, and the first system to use neural word embeddings to\nachieve state-of-the-art results on named-entity recognition in both CoNLL and\nOntonotes NER. Our system achieves an F1 score of 90.90 on the test set for\nCoNLL 2003---significantly better than any previous system trained on public\ndata, and matching a system employing massive private industrial query-log\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 02:12:06 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Passos", "Alexandre", ""], ["Kumar", "Vineet", ""], ["McCallum", "Andrew", ""]]}, {"id": "1404.5372", "submitter": "Andrea Ballatore", "authors": "Andrea Ballatore and Michela Bertolotto and David C. Wilson", "title": "Linking Geographic Vocabularies through WordNet", "comments": "21 pages, 1 figure", "journal-ref": "Annals of GIS, 20 (2), 2014, 73-84", "doi": "10.1080/19475683.2014.904440", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linked open data (LOD) paradigm has emerged as a promising approach to\nstructuring and sharing geospatial information. One of the major obstacles to\nthis vision lies in the difficulties found in the automatic integration between\nheterogeneous vocabularies and ontologies that provides the semantic backbone\nof the growing constellation of open geo-knowledge bases. In this article, we\nshow how to utilize WordNet as a semantic hub to increase the integration of\nLOD. With this purpose in mind, we devise Voc2WordNet, an unsupervised mapping\ntechnique between a given vocabulary and WordNet, combining intensional and\nextensional aspects of the geographic terms. Voc2WordNet is evaluated against a\nsample of human-generated alignments with the OpenStreetMap (OSM) Semantic\nNetwork, a crowdsourced geospatial resource, and the GeoNames ontology, the\nvocabulary of a large digital gazetteer. These empirical results indicate that\nthe approach can obtain high precision and recall.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 03:28:07 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Ballatore", "Andrea", ""], ["Bertolotto", "Michela", ""], ["Wilson", "David C.", ""]]}, {"id": "1404.5585", "submitter": "Matthew Skala", "authors": "Matthew Skala", "title": "A Structural Query System for Han Characters", "comments": "28 pages, 5 figures, for submission to ACM Transactions on Asian\n  Language Information Processing", "journal-ref": "International Journal of Asian Language Processing 23(2) (2015)\n  127-159", "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IDSgrep structural query system for Han character dictionaries is\npresented. This system includes a data model and syntax for describing the\nspatial structure of Han characters using Extended Ideographic Description\nSequences (EIDSes) based on the Unicode IDS syntax; a language for querying\nEIDS databases, designed to suit the needs of font developers and foreign\nlanguage learners; a bit vector index inspired by Bloom filters for faster\nquery operations; a freely available implementation; and format translation\nfrom popular third-party IDS and XML character databases. Experimental results\nare included, with a comparison to other software used for similar\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 18:26:09 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Skala", "Matthew", ""]]}, {"id": "1404.6312", "submitter": "Yevgeni Berzak", "authors": "Yevgeni Berzak, Roi Reichart and Boris Katz", "title": "Reconstructing Native Language Typology from Foreign Language Usage", "comments": "CoNLL 2014", "journal-ref": "Proceedings of the Eighteenth Conference on Computational Language\n  Learning , pages 21-29, Baltimore, Maryland USA, June 26-27 2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguists and psychologists have long been studying cross-linguistic\ntransfer, the influence of native language properties on linguistic performance\nin a foreign language. In this work we provide empirical evidence for this\nprocess in the form of a strong correlation between language similarities\nderived from structural features in English as Second Language (ESL) texts and\nequivalent similarities obtained from the typological features of the native\nlanguages. We leverage this finding to recover native language typological\nsimilarity structure directly from ESL text, and perform prediction of\ntypological features in an unsupervised fashion with respect to the target\nlanguages. Our method achieves 72.2% accuracy on the typology prediction task,\na result that is highly competitive with equivalent methods that rely on\ntypological resources.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 04:10:57 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 16:31:54 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Berzak", "Yevgeni", ""], ["Reichart", "Roi", ""], ["Katz", "Boris", ""]]}, {"id": "1404.6491", "submitter": "Janyce Wiebe", "authors": "Janyce Wiebe, Lingjia Deng", "title": "An Account of Opinion Implicatures", "comments": "50 Pages. Submitted to the journal, Language Resources and Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While previous sentiment analysis research has concentrated on the\ninterpretation of explicitly stated opinions and attitudes, this work initiates\nthe computational study of a type of opinion implicature (i.e.,\nopinion-oriented inference) in text. This paper described a rule-based\nframework for representing and analyzing opinion implicatures which we hope\nwill contribute to deeper automatic interpretation of subjective language. In\nthe course of understanding implicatures, the system recognizes implicit\nsentiments (and beliefs) toward various events and entities in the sentence,\noften attributed to different sources (holders) and of mixed polarities; thus,\nit produces a richer interpretation than is typical in opinion analysis.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 19:28:55 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Wiebe", "Janyce", ""], ["Deng", "Lingjia", ""]]}, {"id": "1404.7296", "submitter": "Karl Moritz Hermann", "authors": "Edward Grefenstette, Phil Blunsom, Nando de Freitas and Karl Moritz\n  Hermann", "title": "A Deep Architecture for Semantic Parsing", "comments": "In Proceedings of the Semantic Parsing Workshop at ACL 2014\n  (forthcoming)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many successful approaches to semantic parsing build on top of the syntactic\nanalysis of text, and make use of distributional representations or statistical\nmodels to match parses to ontology-specific queries. This paper presents a\nnovel deep learning architecture which provides a semantic parsing system\nthrough the union of two neural models of language semantics. It allows for the\ngeneration of ontology-specific queries from natural language statements and\nquestions without the need for parsing, which makes it especially suitable to\ngrammatically malformed or syntactically atypical text, such as tweets, as well\nas permitting the development of semantic parsers for resource-poor languages.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 10:10:13 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Grefenstette", "Edward", ""], ["Blunsom", "Phil", ""], ["de Freitas", "Nando", ""], ["Hermann", "Karl Moritz", ""]]}, {"id": "1404.7362", "submitter": "Jinzhu Jia", "authors": "Jinzhu Jia, Luke Miratrix, Bin Yu, Brian Gawalt, Laurent El Ghaoui,\n  Luke Barnesmoore, Sophie Clavier", "title": "Concise comparative summaries (CCS) of large text corpora with a human\n  experiment", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS698 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 499-529", "doi": "10.1214/13-AOAS698", "report-no": "IMS-AOAS-AOAS698", "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a general framework for topic-specific summarization\nof large text corpora and illustrate how it can be used for the analysis of\nnews databases. Our framework, concise comparative summarization (CCS), is\nbuilt on sparse classification methods. CCS is a lightweight and flexible tool\nthat offers a compromise between simple word frequency based methods currently\nin wide use and more heavyweight, model-intensive methods such as latent\nDirichlet allocation (LDA). We argue that sparse methods have much to offer for\ntext analysis and hope CCS opens the door for a new branch of research in this\nimportant field. For a particular topic of interest (e.g., China or energy),\nCSS automatically labels documents as being either on- or off-topic (usually\nvia keyword search), and then uses sparse classification methods to predict\nthese labels with the high-dimensional counts of all the other words and\nphrases in the documents. The resulting small set of phrases found as\npredictive are then harvested as the summary. To validate our tool, we, using\nnews articles from the New York Times international section, designed and\nconducted a human survey to compare the different summarizers with human\nunderstanding. We demonstrate our approach with two case studies, a media\nanalysis of the framing of \"Egypt\" in the New York Times throughout the Arab\nSpring and an informal comparison of the New York Times' and Wall Street\nJournal's coverage of \"energy.\" Overall, we find that the Lasso with $L^2$\nnormalization can be effectively and usefully used to summarize large corpora,\nregardless of document size.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 13:53:38 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Jia", "Jinzhu", ""], ["Miratrix", "Luke", ""], ["Yu", "Bin", ""], ["Gawalt", "Brian", ""], ["Ghaoui", "Laurent El", ""], ["Barnesmoore", "Luke", ""], ["Clavier", "Sophie", ""]]}]