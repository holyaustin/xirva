[{"id": "1401.0509", "submitter": "Yann Dauphin", "authors": "Yann N. Dauphin, Gokhan Tur, Dilek Hakkani-Tur, Larry Heck", "title": "Zero-Shot Learning for Semantic Utterance Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose a novel zero-shot learning method for semantic utterance\nclassification (SUC). It learns a classifier $f: X \\to Y$ for problems where\nnone of the semantic categories $Y$ are present in the training set. The\nframework uncovers the link between categories and utterances using a semantic\nspace. We show that this semantic space can be learned by deep neural networks\ntrained on large amounts of search engine query log data. More precisely, we\npropose a novel method that can learn discriminative semantic features without\nsupervision. It uses the zero-shot learning framework to guide the learning of\nthe semantic features. We demonstrate the effectiveness of the zero-shot\nsemantic learning algorithm on the SUC dataset collected by (Tur, 2012).\nFurthermore, we achieve state-of-the-art results by combining the semantic\nfeatures with a supervised method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 17:08:26 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 20:34:08 GMT"}, {"version": "v3", "created": "Fri, 7 Mar 2014 23:31:02 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Dauphin", "Yann N.", ""], ["Tur", "Gokhan", ""], ["Hakkani-Tur", "Dilek", ""], ["Heck", "Larry", ""]]}, {"id": "1401.0569", "submitter": "Son Doan", "authors": "Son Doan, Mike Conway, Tu Minh Phuong, Lucila Ohno-Machado", "title": "Natural Language Processing in Biomedicine: A Unified System\n  Architecture Overview", "comments": "25 pages, 5 figures, book chapter in Clinical Bioinformatics, 2014,\n  edited by Ronand Trent", "journal-ref": null, "doi": "10.1007/978-1-4939-0847-9_16", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern electronic medical records (EMR) much of the clinically important\ndata - signs and symptoms, symptom severity, disease status, etc. - are not\nprovided in structured data fields, but rather are encoded in clinician\ngenerated narrative text. Natural language processing (NLP) provides a means of\n\"unlocking\" this important data source for applications in clinical decision\nsupport, quality assurance, and public health. This chapter provides an\noverview of representative NLP systems in biomedicine based on a unified\narchitectural view. A general architecture in an NLP system consists of two\nmain components: background knowledge that includes biomedical knowledge\nresources and a framework that integrates NLP tools to process text. Systems\ndiffer in both components, which we will review briefly. Additionally,\nchallenges facing current research efforts in biomedical NLP include the\npaucity of large, publicly available annotated corpora, although initiatives\nthat facilitate data sharing, system evaluation, and collaborative work between\nresearchers in clinical NLP are starting to emerge.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 00:57:13 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2014 19:00:04 GMT"}], "update_date": "2014-08-10", "authors_parsed": [["Doan", "Son", ""], ["Conway", "Mike", ""], ["Phuong", "Tu Minh", ""], ["Ohno-Machado", "Lucila", ""]]}, {"id": "1401.0640", "submitter": "Tarek El-Shishtawy Ahmed", "authors": "Fatma El-Ghannam and Tarek El-Shishtawy", "title": "Multi-Topic Multi-Document Summarizer", "comments": null, "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 5, No 6, December 2013", "doi": "10.5121/ijcsit", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current multi-document summarization systems can successfully extract summary\nsentences, however with many limitations including: low coverage, inaccurate\nextraction to important sentences, redundancy and poor coherence among the\nselected sentences. The present study introduces a new concept of centroid\napproach and reports new techniques for extracting summary sentences for\nmulti-document. In both techniques keyphrases are used to weigh sentences and\ndocuments. The first summarization technique (Sen-Rich) prefers maximum\nrichness sentences. While the second (Doc-Rich), prefers sentences from\ncentroid document. To demonstrate the new summarization system application to\nextract summaries of Arabic documents we performed two experiments. First, we\napplied Rouge measure to compare the new techniques among systems presented at\nTAC2011. The results show that Sen-Rich outperformed all systems in ROUGE-S.\nSecond, the system was applied to summarize multi-topic documents. Using human\nevaluators, the results show that Doc-Rich is the superior, where summary\nsentences characterized by extra coverage and more cohesion.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 13:07:29 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["El-Ghannam", "Fatma", ""], ["El-Shishtawy", "Tarek", ""]]}, {"id": "1401.0660", "submitter": "Bruno Mery", "authors": "Bruno Mery (LaBRI), Richard Moot (LaBRI), Christian Retor\\'e (LaBRI)", "title": "Plurals: individuals and sets in a richly typed semantics", "comments": null, "journal-ref": "LENSL'10 - 10th Workshop on Logic and Engineering of Natural\n  Semantics of Language, Japanese Symposium for Artifitial Intelligence,\n  International Society for AI - 2013 (2013)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a type-theoretical framework for natural lan- guage semantics\nthat, in addition to the usual Montagovian treatment of compositional\nsemantics, includes a treatment of some phenomena of lex- ical semantic:\ncoercions, meaning, transfers, (in)felicitous co-predication. In this setting\nwe see how the various readings of plurals (collective, dis- tributive,\ncoverings,...) can be modelled.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 15:37:19 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Mery", "Bruno", "", "LaBRI"], ["Moot", "Richard", "", "LaBRI"], ["Retor\u00e9", "Christian", "", "LaBRI"]]}, {"id": "1401.0708", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama, Sudheer Kolachina, Lakshmi Bai B", "title": "Quantitative methods for Phylogenetic Inference in Historical\n  Linguistics: An experimental case study of South Central Dravidian", "comments": null, "journal-ref": "Indian Linguistics, Volume 70, 2009", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we examine the usefulness of two classes of algorithms Distance\nMethods, Discrete Character Methods (Felsenstein and Felsenstein 2003) widely\nused in genetics, for predicting the family relationships among a set of\nrelated languages and therefore, diachronic language change. Applying these\nalgorithms to the data on the numbers of shared cognates- with-change and\nchanged as well as unchanged cognates for a group of six languages belonging to\na Dravidian language sub-family given in Krishnamurti et al. (1983), we\nobserved that the resultant phylogenetic trees are largely in agreement with\nthe linguistic family tree constructed using the comparative method of\nreconstruction with only a few minor differences. Furthermore, we studied these\nminor differences and found that they were cases of genuine ambiguity even for\na well-trained historical linguist. We evaluated the trees obtained through our\nexperiments using a well-defined criterion and report the results here. We\nfinally conclude that quantitative methods like the ones we examined are quite\nuseful in predicting family relationships among languages. In addition, we\nconclude that a modest degree of confidence attached to the intuition that\nthere could indeed exist a parallelism between the processes of linguistic and\ngenetic change is not totally misplaced.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 20:17:47 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Rama", "Taraka", ""], ["Kolachina", "Sudheer", ""], ["B", "Lakshmi Bai", ""]]}, {"id": "1401.0794", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama, Lars Borin", "title": "Properties of phoneme N -grams across the world's language families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate the properties of phoneme N-grams across half\nof the world's languages. We investigate if the sizes of three different N-gram\ndistributions of the world's language families obey a power law. Further, the\nN-gram distributions of language families parallel the sizes of the families,\nwhich seem to obey a power law distribution. The correlation between N-gram\ndistributions and language family sizes improves with increasing values of N.\nWe applied statistical tests, originally given by physicists, to test the\nhypothesis of power law fit to twelve different datasets. The study also raises\nsome new questions about the use of N-gram distributions in linguistic\nresearch, which we answer by running a statistical test.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 09:50:55 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Rama", "Taraka", ""], ["Borin", "Lars", ""]]}, {"id": "1401.1158", "submitter": "Benjamin Roth", "authors": "Benjamin Roth, Tassilo Barth, Michael Wiegand, Mittul Singh, Dietrich\n  Klakow", "title": "Effective Slot Filling Based on Shallow Distant Supervision Methods", "comments": "to be published in: Proceedings of the Sixth Text Analysis Conference\n  (TAC 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken Language Systems at Saarland University (LSV) participated this year\nwith 5 runs at the TAC KBP English slot filling track. Effective algorithms for\nall parts of the pipeline, from document retrieval to relation prediction and\nresponse post-processing, are bundled in a modular end-to-end relation\nextraction system called RelationFactory. The main run solely focuses on\nshallow techniques and achieved significant improvements over LSV's last year's\nsystem, while using the same training data and patterns. Improvements mainly\nhave been obtained by a feature representation focusing on surface skip n-grams\nand improved scoring for extracted distant supervision patterns. Important\nfactors for effective extraction are the training and tuning scheme for distant\nsupervision classifiers, and the query expansion by a translation model based\non Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, the\nsubmitted main run of the LSV RelationFactory system achieved the top-ranked\nF1-score of 37.3%.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 18:03:11 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Roth", "Benjamin", ""], ["Barth", "Tassilo", ""], ["Wiegand", "Michael", ""], ["Singh", "Mittul", ""], ["Klakow", "Dietrich", ""]]}, {"id": "1401.1486", "submitter": "Zeeshan Bhatti", "authors": "Imdad Ali Ismaili, Zeeshan Bhatti, Azhar Ali Shah", "title": "Design & Development of the Graphical User Interface for Sindhi Language", "comments": null, "journal-ref": "Mehran University Research Journal of Engineering & Technology,\n  Volume 30, No. 4, October 2011 [ISSN 0254-7821]", "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and implementation of a Unicode-based GUISL\n(Graphical User Interface for Sindhi Language). The idea is to provide a\nsoftware platform to the people of Sindh as well as Sindhi diasporas living\nacross the globe to make use of computing for basic tasks such as editing,\ncomposition, formatting, and printing of documents in Sindhi by using GUISL.\nThe implementation of the GUISL has been done in the Java technology to make\nthe system platform independent. The paper describes several design issues of\nSindhi GUI in the context of existing software tools and technologies and\nexplains how mapping and concatenation techniques have been employed to achieve\nthe cursive shape of Sindhi script.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 20:07:51 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Ismaili", "Imdad Ali", ""], ["Bhatti", "Zeeshan", ""], ["Shah", "Azhar Ali", ""]]}, {"id": "1401.1803", "submitter": "Stanislas Lauly", "authors": "Stanislas Lauly, Alex Boulanger, Hugo Larochelle", "title": "Learning Multilingual Word Representations using a Bag-of-Words\n  Autoencoder", "comments": "This workshop paper was accepted on Octoble 30 2013 at the NIPS 2013\n  workshop on deep learning\n  (https://sites.google.com/site/deeplearningworkshopnips2013/accepted-papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on learning multilingual word representations usually relies on\nthe use of word-level alignements (e.g. infered with the help of GIZA++)\nbetween translated sentences, in order to align the word embeddings in\ndifferent languages. In this workshop paper, we investigate an autoencoder\nmodel for learning multilingual word representations that does without such\nword-level alignements. The autoencoder is trained to reconstruct the\nbag-of-word representation of given sentence from an encoded representation\nextracted from its translation. We evaluate our approach on a multilingual\ndocument classification task, where labeled data is available only for one\nlanguage (e.g. English) while classification must be performed in a different\nlanguage (e.g. French). In our experiments, we observe that our method compares\nfavorably with a previously proposed method that exploits word-level alignments\nto learn word representations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 20:36:57 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Lauly", "Stanislas", ""], ["Boulanger", "Alex", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1401.2258", "submitter": "Benjamin Roth", "authors": "Benjamin Roth", "title": "Assessing Wikipedia-Based Cross-Language Retrieval Models", "comments": "74 pages; MSc thesis at Saarland University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work compares concept models for cross-language retrieval: First, we\nadapt probabilistic Latent Semantic Analysis (pLSA) for multilingual documents.\nExperiments with different weighting schemes show that a weighting method\nfavoring documents of similar length in both language sides gives best results.\nConsidering that both monolingual and multilingual Latent Dirichlet Allocation\n(LDA) behave alike when applied for such documents, we use a training corpus\nbuilt on Wikipedia where all documents are length-normalized and obtain\nimprovements over previously reported scores for LDA. Another focus of our work\nis on model combination. For this end we include Explicit Semantic Analysis\n(ESA) in the experiments. We observe that ESA is not competitive with LDA in a\nquery based retrieval task on CLEF 2000 data. The combination of machine\ntranslation with concept models increased performance by 21.1% map in\ncomparison to machine translation alone. Machine translation relies on parallel\ncorpora, which may not be available for many language pairs. We further explore\nhow much cross-lingual information can be carried over by a specific\ninformation source in Wikipedia, namely linked text. The best results are\nobtained using a language modeling approach, entirely without information from\nparallel corpora. The need for smoothing raises interesting questions on\nsoundness and efficiency. Link models capture only a certain kind of\ninformation and suggest weighting schemes to emphasize particular words. For a\ncombined model, another interesting question is therefore how to integrate\ndifferent weighting schemes. Using a very simple combination scheme, we obtain\nresults that compare favorably to previously reported results on the CLEF 2000\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 08:50:54 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Roth", "Benjamin", ""]]}, {"id": "1401.2517", "submitter": "Andrea Ballatore", "authors": "Andrea Ballatore, Michela Bertolotto, David C. Wilson", "title": "The semantic similarity ensemble", "comments": "Special feature on Semantic and Conceptual Issues in GIS (SeCoGIS)", "journal-ref": "Journal of Spatial Information Science (JOSIS), Number 7 (2013),\n  pp. 27-44", "doi": "10.5311/JOSIS.2013.7.128", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Computational measures of semantic similarity between geographic terms\nprovide valuable support across geographic information retrieval, data mining,\nand information integration. To date, a wide variety of approaches to\ngeo-semantic similarity have been devised. A judgment of similarity is not\nintrinsically right or wrong, but obtains a certain degree of cognitive\nplausibility, depending on how closely it mimics human behavior. Thus selecting\nthe most appropriate measure for a specific task is a significant challenge. To\naddress this issue, we make an analogy between computational similarity\nmeasures and soliciting domain expert opinions, which incorporate a subjective\nset of beliefs, perceptions, hypotheses, and epistemic biases. Following this\nanalogy, we define the semantic similarity ensemble (SSE) as a composition of\ndifferent similarity measures, acting as a panel of experts having to reach a\ndecision on the semantic similarity of a set of geographic terms. The approach\nis evaluated in comparison to human judgments, and results indicate that an SSE\nperforms better than the average of its parts. Although the best member tends\nto outperform the ensemble, all ensembles outperform the average performance of\neach ensemble's member. Hence, in contexts where the best measure is unknown,\nthe ensemble provides a more cognitively plausible approach.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 10:35:37 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Ballatore", "Andrea", ""], ["Bertolotto", "Michela", ""], ["Wilson", "David C.", ""]]}, {"id": "1401.2618", "submitter": "Deepali Virmani", "authors": "Deepali Virmani, Vikrant Malhotra, Ridhi Tyagi", "title": "Sentiment Analysis Using Collaborated Opinion Mining", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion mining and Sentiment analysis have emerged as a field of study since\nthe widespread of World Wide Web and internet. Opinion refers to extraction of\nthose lines or phrase in the raw and huge data which express an opinion.\nSentiment analysis on the other hand identifies the polarity of the opinion\nbeing extracted. In this paper we propose the sentiment analysis in\ncollaboration with opinion extraction, summarization, and tracking the records\nof the students. The paper modifies the existing algorithm in order to obtain\nthe collaborated opinion about the students. The resultant opinion is\nrepresented as very high, high, moderate, low and very low. The paper is based\non a case study where teachers give their remarks about the students and by\napplying the proposed sentiment analysis algorithm the opinion is extracted and\nrepresented.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 12:35:57 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Virmani", "Deepali", ""], ["Malhotra", "Vikrant", ""], ["Tyagi", "Ridhi", ""]]}, {"id": "1401.2641", "submitter": "Zeeshan Bhatti", "authors": "Imdad Ali Ismaili, Zeeshan Bhatti, Azhar Ali Shah", "title": "Towards a Generic Framework for the Development of Unicode Based Digital\n  Sindhi Dictionaries", "comments": null, "journal-ref": "Mehran University Research Journal of Engineering & Technology\n  Volume 31, No. 1, January 2012 [ISSN 0254-7821]", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Dictionaries are essence of any language providing vital linguistic recourse\nfor the language learners, researchers and scholars. This paper focuses on the\nmethodology and techniques used in developing software architecture for a\nUBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). The\nproposed system provides an accurate solution for construction and\nrepresentation of Unicode based Sindhi characters in a dictionary implementing\nHash Structure algorithm and a custom java Object as its internal data\nstructure saved in a file. The System provides facilities for Insertion,\nDeletion and Editing of new records of Sindhi. Through this framework any type\nof Sindhi to English and English to Sindhi Dictionary (belonging to different\ndomains of knowledge, e.g. engineering, medicine, computer, biology etc.) could\nbe developed easily with accurate representation of Unicode Characters in font\nindependent manner.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 16:49:53 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Ismaili", "Imdad Ali", ""], ["Bhatti", "Zeeshan", ""], ["Shah", "Azhar Ali", ""]]}, {"id": "1401.2663", "submitter": "Cem Rifki Aydin M.Sc.", "authors": "Cem R{\\i}fk{\\i} Ayd{\\i}n, Ali Erkan, Tunga G\\\"ung\\\"or, and Hidayet\n  Tak\\c{c}{\\i}", "title": "Dictionary-Based Concept Mining: An Application for Turkish", "comments": "12 pages with 3 figures, to be published in \"International Conference\n  on Foundations of Computer Science & Technology (CST 2014), Zurich,\n  Switzerland - January 2014 Proceedings, AIRCC\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a dictionary-based method is used to extract expressive\nconcepts from documents. So far, there have been many studies concerning\nconcept mining in English, but this area of study for Turkish, an agglutinative\nlanguage, is still immature. We used dictionary instead of WordNet, a lexical\ndatabase grouping words into synsets that is widely used for concept\nextraction. The dictionaries are rarely used in the domain of concept mining,\nbut taking into account that dictionary entries have synonyms, hypernyms,\nhyponyms and other relationships in their meaning texts, the success rate has\nbeen high for determining concepts. This concept extraction method is\nimplemented on documents, that are collected from different corpora.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 19:52:49 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Ayd\u0131n", "Cem R\u0131fk\u0131", ""], ["Erkan", "Ali", ""], ["G\u00fcng\u00f6r", "Tunga", ""], ["Tak\u00e7\u0131", "Hidayet", ""]]}, {"id": "1401.2851", "submitter": "Md. Naseef-Ur-Rahman Chowdhury", "authors": "Md. Naseef-Ur-Rahman Chowdhury, Suvankar Paul, and Kazi Zakia Sultana", "title": "Statistical Analysis based Hypothesis Testing Method in Biological\n  Knowledge Discovery", "comments": "9 pages, published on International Journal on Computational Sciences\n  & Applications (IJCSA) Vol.3, No.6, December 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correlation and interactions among different biological entities comprise\nthe biological system. Although already revealed interactions contribute to the\nunderstanding of different existing systems, researchers face many questions\neveryday regarding inter-relationships among entities. Their queries have\npotential role in exploring new relations which may open up a new area of\ninvestigation. In this paper, we introduce a text mining based method for\nanswering the biological queries in terms of statistical computation such that\nresearchers can come up with new knowledge discovery. It facilitates user to\nsubmit their query in natural linguistic form which can be treated as\nhypothesis. Our proposed approach analyzes the hypothesis and measures the\np-value of the hypothesis with respect to the existing literature. Based on the\nmeasured value, the system either accepts or rejects the hypothesis from\nstatistical point of view. Moreover, even it does not find any direct\nrelationship among the entities of the hypothesis, it presents a network to\ngive an integral overview of all the entities through which the entities might\nbe related. This is also congenial for the researchers to widen their view and\nthus think of new hypothesis for further investigation. It assists researcher\nto get a quantitative evaluation of their assumptions such that they can reach\na logical conclusion and thus aids in relevant re-searches of biological\nknowledge discovery. The system also provides the researchers a graphical\ninteractive interface to submit their hypothesis for assessment in a more\nconvenient way.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 18:05:15 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Chowdhury", "Md. Naseef-Ur-Rahman", ""], ["Paul", "Suvankar", ""], ["Sultana", "Kazi Zakia", ""]]}, {"id": "1401.2937", "submitter": "Ralf Steinberger", "authors": "Ralf Steinberger", "title": "A survey of methods to ease the development of highly multilingual text\n  mining applications", "comments": "22 pages. Published online on 12 October 2011", "journal-ref": "Language Resources and Evaluation, Volume 46, Issue 2, pp 155-176,\n  June 2012", "doi": "10.1007/s10579-011-9165-9", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual text processing is useful because the information content found\nin different languages is complementary, both regarding facts and opinions.\nWhile Information Extraction and other text mining software can, in principle,\nbe developed for many languages, most text analysis tools have only been\napplied to small sets of languages because the development effort per language\nis large. Self-training tools obviously alleviate the problem, but even the\neffort of providing training data and of manually tuning the results is usually\nconsiderable. In this paper, we gather insights by various multilingual system\ndevelopers on how to minimise the effort of developing natural language\nprocessing applications for many languages. We also explain the main guidelines\nunderlying our own effort to develop complex text mining software for tens of\nlanguages. While these guidelines - most of all: extreme simplicity - can be\nvery restrictive and limiting, we believe to have shown the feasibility of the\napproach through the development of the Europe Media Monitor (EMM) family of\napplications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex\nmedia monitoring tools that process and analyse up to 100,000 online news\narticles per day in between twenty and fifty languages. We will also touch upon\nthe kind of language resources that would make it easier for all to develop\nhighly multilingual text mining applications. We will argue that - to achieve\nthis - the most needed resources would be freely available, simple, parallel\nand uniform multilingual dictionaries, corpora and software tools.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 18:05:28 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Steinberger", "Ralf", ""]]}, {"id": "1401.2943", "submitter": "Ralf Steinberger", "authors": "Marco Turchi, Martin Atkinson, Alastair Wilcox, Brett Crawley, Stefano\n  Bucci, Ralf Steinberger and Erik Van der Goot", "title": "ONTS: \"Optima\" News Translation System", "comments": null, "journal-ref": "Proceedings of the 13th Conference of the European Chapter of the\n  Association for Computational Linguistics, pages 25-30, Avignon, France,\n  April 23 - 27 2012. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time machine translation system that allows users to select\na news category and to translate the related live news articles from Arabic,\nCzech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and\nTurkish into English. The Moses-based system was optimised for the news domain\nand differs from other available systems in four ways: (1) News items are\nautomatically categorised on the source side, before translation; (2) Named\nentity translation is optimised by recognising and extracting them on the\nsource side and by re-inserting their translation in the target language,\nmaking use of a separate entity repository; (3) News titles are translated with\na separate translation system which is optimised for the specific style of news\ntitles; (4) The system was optimised for speed in order to cope with the large\nvolume of daily news articles.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 18:25:10 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Turchi", "Marco", ""], ["Atkinson", "Martin", ""], ["Wilcox", "Alastair", ""], ["Crawley", "Brett", ""], ["Bucci", "Stefano", ""], ["Steinberger", "Ralf", ""], ["Van der Goot", "Erik", ""]]}, {"id": "1401.3230", "submitter": "K Paramesha", "authors": "K Paramesha and K C Ravishankar", "title": "Optimization Of Cross Domain Sentiment Analysis Using Sentiwordnet", "comments": null, "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology (IJFCST), Vol. 3, No.5, September 2013", "doi": "10.5121/ijfcst.2013.3504", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The task of sentiment analysis of reviews is carried out using manually built\n/ automatically generated lexicon resources of their own with which terms are\nmatched with lexicon to compute the term count for positive and negative\npolarity. On the other hand the Sentiwordnet, which is quite different from\nother lexicon resources that gives scores (weights) of the positive and\nnegative polarity for each word. The polarity of a word namely positive,\nnegative and neutral have the score ranging between 0 to 1 indicates the\nstrength/weight of the word with that sentiment orientation. In this paper, we\nshow that using the Sentiwordnet, how we could enhance the performance of the\nclassification at both sentence and document level.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 13:48:02 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Paramesha", "K", ""], ["Ravishankar", "K C", ""]]}, {"id": "1401.3322", "submitter": "Zoran Cvetkovic", "authors": "Jibran Yousafzai and Zoran Cvetkovic and Peter Sollich and Matthew\n  Ager", "title": "A Subband-Based SVM Front-End for Robust ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel support vector machine (SVM) based robust\nautomatic speech recognition (ASR) front-end that operates on an ensemble of\nthe subband components of high-dimensional acoustic waveforms. The key issues\nof selecting the appropriate SVM kernels for classification in frequency\nsubbands and the combination of individual subband classifiers using ensemble\nmethods are addressed. The proposed front-end is compared with state-of-the-art\nASR front-ends in terms of robustness to additive noise and linear filtering.\nExperiments performed on the TIMIT phoneme classification task demonstrate the\nbenefits of the proposed subband based SVM front-end: it outperforms the\nstandard cepstral front-end in the presence of noise and linear filtering for\nsignal-to-noise ratio (SNR) below 12-dB. A combination of the proposed\nfront-end with a conventional front-end such as MFCC yields further\nimprovements over the individual front ends across the full range of noise\nlevels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 08:45:07 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Yousafzai", "Jibran", ""], ["Cvetkovic", "Zoran", ""], ["Sollich", "Peter", ""], ["Ager", "Matthew", ""]]}, {"id": "1401.3372", "submitter": "Linas Vepstas PhD", "authors": "Linas Vepstas and Ben Goertzel", "title": "Learning Language from a Large (Unannotated) Corpus", "comments": "29 pages, 5 figures, research proposal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach to the fully automated, unsupervised extraction of\ndependency grammars and associated syntax-to-semantic-relationship mappings\nfrom large text corpora is described. The suggested approach builds on the\nauthors' prior work with the Link Grammar, RelEx and OpenCog systems, as well\nas on a number of prior papers and approaches from the statistical language\nlearning literature. If successful, this approach would enable the mining of\nall the information needed to power a natural language comprehension and\ngeneration system, directly from a large, unannotated corpus.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 22:10:30 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Vepstas", "Linas", ""], ["Goertzel", "Ben", ""]]}, {"id": "1401.3457", "submitter": "S.R.K. Branavan", "authors": "S.R.K. Branavan, Harr Chen, Jacob Eisenstein, Regina Barzilay", "title": "Learning Document-Level Semantic Properties from Free-Text Annotations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  569-603, 2009", "doi": "10.1613/jair.2633", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for inferring the semantic properties of\ndocuments by leveraging free-text keyphrase annotations. Such annotations are\nbecoming increasingly abundant due to the recent dramatic growth in\nsemi-structured, user-generated online content. One especially relevant domain\nis product reviews, which are often annotated by their authors with pros/cons\nkeyphrases such as a real bargain or good value. These annotations are\nrepresentative of the underlying semantic properties; however, unlike expert\nannotations, they are noisy: lay authors may use different labels to denote the\nsame property, and some labels may be missing. To learn using such noisy\nannotations, we find a hidden paraphrase structure which clusters the\nkeyphrases. The paraphrase structure is linked with a latent topic model of the\nreview texts, enabling the system to predict the properties of unannotated\ndocuments and to effectively aggregate the semantic properties of multiple\nreviews. Our approach is implemented as a hierarchical Bayesian model with\njoint inference. We find that joint inference increases the robustness of the\nkeyphrase clustering and encourages the latent topics to correlate with\nsemantically meaningful properties. Multiple evaluations demonstrate that our\nmodel substantially outperforms alternative approaches for summarizing single\nand multiple documents into a set of semantically salient keyphrases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:14:31 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Branavan", "S. R. K.", ""], ["Chen", "Harr", ""], ["Eisenstein", "Jacob", ""], ["Barzilay", "Regina", ""]]}, {"id": "1401.3479", "submitter": "Yllias  Chali", "authors": "Yllias Chali, Shafiq Rayhan Joty, Sadid A. Hasan", "title": "Complex Question Answering: Unsupervised Learning Approaches and\n  Experiments", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  1-47, 2009", "doi": "10.1613/jair.2784", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex questions that require inferencing and synthesizing information from\nmultiple documents can be seen as a kind of topic-oriented, informative\nmulti-document summarization where the goal is to produce a single text as a\ncompressed version of a set of documents with a minimum loss of relevant\ninformation. In this paper, we experiment with one empirical method and two\nunsupervised statistical machine learning techniques: K-means and Expectation\nMaximization (EM), for computing relative importance of the sentences. We\ncompare the results of these approaches. Our experiments show that the\nempirical approach outperforms the other two techniques and EM performs better\nthan K-means. However, the performance of these approaches depends entirely on\nthe feature set used and the weighting of these features. In order to measure\nthe importance and relevance to the user query we extract different kinds of\nfeatures (i.e. lexical, lexical semantic, cosine similarity, basic element,\ntree kernel based syntactic and shallow-semantic) for each of the document\nsentences. We use a local search technique to learn the weights of the\nfeatures. To the best of our knowledge, no study has used tree kernel functions\nto encode syntactic/semantic information for more complex tasks such as\ncomputing the relatedness between the query sentences and the document\nsentences in order to generate query-focused summaries (or answers to complex\nquestions). For each of our methods of generating summaries (i.e. empirical,\nK-means and EM) we show the effects of syntactic and shallow-semantic features\nover the bag-of-words (BOW) features.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:33:57 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Chali", "Yllias", ""], ["Joty", "Shafiq Rayhan", ""], ["Hasan", "Sadid A.", ""]]}, {"id": "1401.3482", "submitter": "Estela Saquete", "authors": "Estela Saquete, Jose Luis Vicedo, Patricio Mart\\'inez-Barco, Rafael\n  Mu\\~noz, Hector Llorens", "title": "Enhancing QA Systems with Complex Temporal Question Processing\n  Capabilities", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  775-811, 2009", "doi": "10.1613/jair.2805", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multilayered architecture that enhances the\ncapabilities of current QA systems and allows different types of complex\nquestions or queries to be processed. The answers to these questions need to be\ngathered from factual information scattered throughout different documents.\nSpecifically, we designed a specialized layer to process the different types of\ntemporal questions. Complex temporal questions are first decomposed into simple\nquestions, according to the temporal relations expressed in the original\nquestion. In the same way, the answers to the resulting simple questions are\nrecomposed, fulfilling the temporal restrictions of the original complex\nquestion. A novel aspect of this approach resides in the decomposition which\nuses a minimal quantity of resources, with the final aim of obtaining a\nportable platform that is easily extensible to other languages. In this paper\nwe also present a methodology for evaluation of the decomposition of the\nquestions as well as the ability of the implemented temporal layer to perform\nat a multilingual level. The temporal layer was first performed for English,\nthen evaluated and compared with: a) a general purpose QA system (F-measure\n65.47% for QA plus English temporal layer vs. 38.01% for the general QA\nsystem), and b) a well-known QA system. Much better results were obtained for\ntemporal questions with the multilayered system. This system was therefore\nextended to Spanish and very good results were again obtained in the evaluation\n(F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general\nQA system).\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:35:49 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Saquete", "Estela", ""], ["Vicedo", "Jose Luis", ""], ["Mart\u00ednez-Barco", "Patricio", ""], ["Mu\u00f1oz", "Rafael", ""], ["Llorens", "Hector", ""]]}, {"id": "1401.3488", "submitter": "Harr Chen", "authors": "Harr Chen, S.R.K. Branavan, Regina Barzilay, David R. Karger", "title": "Content Modeling Using Latent Permutations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  129-163, 2009", "doi": "10.1613/jair.2830", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Bayesian topic model for learning discourse-level document\nstructure. Our model leverages insights from discourse theory to constrain\nlatent topic assignments in a way that reflects the underlying organization of\ndocument topics. We propose a global model in which both topic selection and\nordering are biased to be similar across a collection of related documents. We\nshow that this space of orderings can be effectively represented using a\ndistribution over permutations called the Generalized Mallows Model. We apply\nour method to three complementary discourse-level tasks: cross-document\nalignment, document segmentation, and information ordering. Our experiments\nshow that incorporating our permutation-based model in these applications\nyields substantial improvements in performance over previously proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:38:17 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Chen", "Harr", ""], ["Branavan", "S. R. K.", ""], ["Barzilay", "Regina", ""], ["Karger", "David R.", ""]]}, {"id": "1401.3510", "submitter": "Saurabh  Varshney Mr.", "authors": "Saurabh Varshney and Jyoti Bajpai", "title": "Improving Performance Of English-Hindi Cross Language Information\n  Retrieval Using Transliteration Of Query Terms", "comments": "International Journal on Natural Language Computing (IJNLC) Vol. 2,\n  No.6, December 2013 http://airccse.org/journal/ijnlc/index.html", "journal-ref": null, "doi": "10.5121/ijnlc.2013.2604", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main issue in Cross Language Information Retrieval (CLIR) is the poor\nperformance of retrieval in terms of average precision when compared to\nmonolingual retrieval performance. The main reasons behind poor performance of\nCLIR are mismatching of query terms, lexical ambiguity and un-translated query\nterms. The existing problems of CLIR are needed to be addressed in order to\nincrease the performance of the CLIR system. In this paper, we are putting our\neffort to solve the given problem by proposed an algorithm for improving the\nperformance of English-Hindi CLIR system. We used all possible combination of\nHindi translated query using transliteration of English query terms and\nchoosing the best query among them for retrieval of documents. The experiment\nis performed on FIRE 2010 (Forum of Information Retrieval Evaluation) datasets.\nThe experimental result show that the proposed approach gives better\nperformance of English-Hindi CLIR system and also helps in overcoming existing\nproblems and outperforms the existing English-Hindi CLIR system in terms of\naverage precision.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 08:07:08 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Varshney", "Saurabh", ""], ["Bajpai", "Jyoti", ""]]}, {"id": "1401.3669", "submitter": "Doina Tatar", "authors": "D. Tatar, M.Lupea, E. Kapetanios", "title": "Hrebs and Cohesion Chains as similar tools for semantic text properties\n  research", "comments": "13 pages, KNOWLEDGE ENGINEERING: PRINCIPLES AND TECHNIQUES\n  Proceedings of the International Conference on Knowledge Engineering,\n  Principles and Techniques, KEPT 2013 Cluj-Napoca (Romania)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study it is proven that the Hrebs used in Denotation analysis of\ntexts and Cohesion Chains (defined as a fusion between Lexical Chains and\nCoreference Chains) represent similar linguistic tools. This result gives us\nthe possibility to extend to Cohesion Chains (CCs) some important indicators\nas, for example the Kernel of CCs, the topicality of a CC, text concentration,\nCC-diffuseness and mean diffuseness of the text. Let us mention that nowhere in\nthe Lexical Chains or Coreference Chains literature these kinds of indicators\nare introduced and used since now. Similarly, some applications of CCs in the\nstudy of a text (as for example segmentation or summarization of a text) could\nbe realized starting from hrebs. As an illustration of the similarity between\nHrebs and CCs a detailed analyze of the poem \"Lacul\" by Mihai Eminescu is\ngiven.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 17:01:36 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Tatar", "D.", ""], ["Lupea", "M.", ""], ["Kapetanios", "E.", ""]]}, {"id": "1401.3832", "submitter": "Matthew Michelson", "authors": "Matthew Michelson, Craig A. Knoblock", "title": "Constructing Reference Sets from Unstructured, Ungrammatical Text", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  189-221, 2010", "doi": "10.1613/jair.2937", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vast amounts of text on the Web are unstructured and ungrammatical, such as\nclassified ads, auction listings, forum postings, etc. We call such text\n\"posts.\" Despite their inconsistent structure and lack of grammar, posts are\nfull of useful information. This paper presents work on semi-automatically\nbuilding tables of relational information, called \"reference sets,\" by\nanalyzing such posts directly. Reference sets can be applied to a number of\ntasks such as ontology maintenance and information extraction. Our\nreference-set construction method starts with just a small amount of background\nknowledge, and constructs tuples representing the entities in the posts to form\na reference set. We also describe an extension to this approach for the special\ncase where even this small amount of background knowledge is impossible to\ndiscover and use. To evaluate the utility of the machine-constructed reference\nsets, we compare them to manually constructed reference sets in the context of\nreference-set-based information extraction. Our results show the reference sets\nconstructed by our method outperform manually constructed reference sets. We\nalso compare the reference-set-based extraction approach using the\nmachine-constructed reference set to supervised extraction approaches using\ngeneric features. These results demonstrate that using machine-constructed\nreference sets outperforms the supervised methods, even though the supervised\nmethods require training data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:49:45 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Michelson", "Matthew", ""], ["Knoblock", "Craig A.", ""]]}, {"id": "1401.3865", "submitter": "Xavier Tannier", "authors": "Xavier Tannier, Philippe Muller", "title": "Evaluating Temporal Graphs Built from Texts via Transitive Reduction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  375-413, 2011", "doi": "10.1613/jair.3118", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal information has been the focus of recent attention in information\nextraction, leading to some standardization effort, in particular for the task\nof relating events in a text. This task raises the problem of comparing two\nannotations of a given text, because relations between events in a story are\nintrinsically interdependent and cannot be evaluated separately. A proper\nevaluation measure is also crucial in the context of a machine learning\napproach to the problem. Finding a common comparison referent at the text level\nis not obvious, and we argue here in favor of a shift from event-based measures\nto measures on a unique textual object, a minimal underlying temporal graph, or\nmore formally the transitive reduction of the graph of relations between event\nboundaries. We support it by an investigation of its properties on synthetic\ndata and on a well-know temporal corpus.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:05:45 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Tannier", "Xavier", ""], ["Muller", "Philippe", ""]]}, {"id": "1401.3908", "submitter": "Ricardo Ribeiro", "authors": "Ricardo Ribeiro, David Martins de Matos", "title": "Centrality-as-Relevance: Support Sets and Similarity as Geometric\n  Proximity", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  275-308, 2011", "doi": "10.1613/jair.3387", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In automatic summarization, centrality-as-relevance means that the most\nimportant content of an information source, or a collection of information\nsources, corresponds to the most central passages, considering a representation\nwhere such notion makes sense (graph, spatial, etc.). We assess the main\nparadigms, and introduce a new centrality-based relevance model for automatic\nsummarization that relies on the use of support sets to better estimate the\nrelevant content. Geometric proximity is used to compute semantic relatedness.\nCentrality (relevance) is determined by considering the whole input source (and\nnot only local information), and by taking into account the existence of minor\ntopics or lateral subjects in the information sources to be summarized. The\nmethod consists in creating, for each passage of the input source, a support\nset consisting only of the most semantically related passages. Then, the\ndetermination of the most relevant content is achieved by selecting the\npassages that occur in the largest number of support sets. This model produces\nextractive summaries that are generic, and language- and domain-independent.\nThorough automatic evaluation shows that the method achieves state-of-the-art\nperformance, both in written text, and automatically transcribed speech\nsummarization, including when compared to considerably more complex approaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:23:22 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""]]}, {"id": "1401.4205", "submitter": "Maria Kalimeri", "authors": "Maria Kalimeri, Vassilios Constantoudis, Constantinos Papadimitriou,\n  Kostantinos Karamanos, Fotis K. Diakonos and Haris Papageorgiou", "title": "Entropy analysis of word-length series of natural language texts:\n  Effects of text language and genre", "comments": "9 pages, 7 figures", "journal-ref": "International Journal of Bifurcation and Chaos, 22, 1250223,\n  (2012)", "doi": "10.1142/S0218127412502239", "report-no": null, "categories": "cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate the $n$-gram entropies of natural language texts in word-length\nrepresentation and find that these are sensitive to text language and genre. We\nattribute this sensitivity to changes in the probability distribution of the\nlengths of single words and emphasize the crucial role of the uniformity of\nprobabilities of having words with length between five and ten. Furthermore,\ncomparison with the entropies of shuffled data reveals the impact of word\nlength correlations on the estimated $n$-gram entropies.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2014 00:00:32 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Kalimeri", "Maria", ""], ["Constantoudis", "Vassilios", ""], ["Papadimitriou", "Constantinos", ""], ["Karamanos", "Kostantinos", ""], ["Diakonos", "Fotis K.", ""], ["Papageorgiou", "Haris", ""]]}, {"id": "1401.4436", "submitter": "Muhammad Arshad Ul Abedin", "authors": "Muhammad Arshad Ul Abedin, Vincent Ng, Latifur Khan", "title": "Cause Identification from Aviation Safety Incident Reports via Weakly\n  Supervised Semantic Lexicon Construction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  569-631, 2010", "doi": "10.1613/jair.2986", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Aviation Safety Reporting System collects voluntarily submitted reports\non aviation safety incidents to facilitate research work aiming to reduce such\nincidents. To effectively reduce these incidents, it is vital to accurately\nidentify why these incidents occurred. More precisely, given a set of possible\ncauses, or shaping factors, this task of cause identification involves\nidentifying all and only those shaping factors that are responsible for the\nincidents described in a report. We investigate two approaches to cause\nidentification. Both approaches exploit information provided by a semantic\nlexicon, which is automatically constructed via Thelen and Riloffs Basilisk\nframework augmented with our linguistic and algorithmic modifications. The\nfirst approach labels a report using a simple heuristic, which looks for the\nwords and phrases acquired during the semantic lexicon learning process in the\nreport. The second approach recasts cause identification as a text\nclassification problem, employing supervised and transductive text\nclassification algorithms to learn models from incident reports labeled with\nshaping factors and using the models to label unseen reports. Our experiments\nshow that both the heuristic-based approach and the learning-based approach\n(when given sufficient training data) outperform the baseline system\nsignificantly.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:53:44 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Abedin", "Muhammad Arshad Ul", ""], ["Ng", "Vincent", ""], ["Khan", "Latifur", ""]]}, {"id": "1401.4603", "submitter": "Esperanza Albacete", "authors": "Esperanza Albacete, Javier Calle, Elena Castro, Dolores Cuadra", "title": "Semantic Similarity Measures Applied to an Ontology for Human-Like\n  Interaction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  397-421, 2012", "doi": "10.1613/jair.3612", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is the calculation of similarity between two concepts\nfrom an ontology for a Human-Like Interaction system. In order to facilitate\nthis calculation, a similarity function is proposed based on five dimensions\n(sort, compositional, essential, restrictive and descriptive) constituting the\nstructure of ontological knowledge. The paper includes a proposal for computing\na similarity function for each dimension of knowledge. Later on, the similarity\nvalues obtained are weighted and aggregated to obtain a global similarity\nmeasure. In order to calculate those weights associated to each dimension, four\ntraining methods have been proposed. The training methods differ in the element\nto fit: the user, concepts or pairs of concepts, and a hybrid approach. For\nevaluating the proposal, the knowledge base was fed from WordNet and extended\nby using a knowledge editing toolkit (Cognos). The evaluation of the proposal\nis carried out through the comparison of system responses with those given by\nhuman test subjects, both providing a measure of the soundness of the procedure\nand revealing ways in which the proposal may be improved.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:09:42 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Albacete", "Esperanza", ""], ["Calle", "Javier", ""], ["Castro", "Elena", ""], ["Cuadra", "Dolores", ""]]}, {"id": "1401.4634", "submitter": "Farzad Farnoud (Hassanzadeh)", "authors": "Farzad Farnoud (Hassanzadeh), Moshe Schwartz, Jehoshua Bruck", "title": "The Capacity of String-Replication Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the majority of the human genome consists of repeated\nsequences. Furthermore, it is believed that a significant part of the rest of\nthe genome also originated from repeated sequences and has mutated to its\ncurrent form. In this paper, we investigate the possibility of constructing an\nexponentially large number of sequences from a short initial sequence and\nsimple replication rules, including those resembling genomic replication\nprocesses. In other words, our goal is to find out the capacity, or the\nexpressive power, of these string-replication systems. Our results include\nexact capacities, and bounds on the capacities, of four fundamental\nstring-replication systems.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2014 03:57:39 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Farnoud", "Farzad", "", "Hassanzadeh"], ["Schwartz", "Moshe", ""], ["Bruck", "Jehoshua", ""]]}, {"id": "1401.4869", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama, Karthik Gali, Avinesh PVS", "title": "Does Syntactic Knowledge help English-Hindi SMT?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we explore various parameter settings of the state-of-art\nStatistical Machine Translation system to improve the quality of the\ntranslation for a `distant' language pair like English-Hindi. We proposed new\ntechniques for efficient reordering. A slight improvement over the baseline is\nreported using these techniques. We also show that a simple pre-processing step\ncan improve the quality of the translation significantly.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 11:49:11 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Rama", "Taraka", ""], ["Gali", "Karthik", ""], ["PVS", "Avinesh", ""]]}, {"id": "1401.4994", "submitter": "Nikolaos Mavridis", "authors": "Nikolaos Mavridis", "title": "A Review of Verbal and Non-Verbal Human-Robot Interactive Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an overview of human-robot interactive communication is\npresented, covering verbal as well as non-verbal aspects of human-robot\ninteraction. Following a historical introduction, and motivation towards fluid\nhuman-robot communication, ten desiderata are proposed, which provide an\norganizational axis both of recent as well as of future research on human-robot\ncommunication. Then, the ten desiderata are examined in detail, culminating to\na unifying discussion, and a forward-looking conclusion.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 17:31:05 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Mavridis", "Nikolaos", ""]]}, {"id": "1401.5327", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis", "title": "Compositional Operators in Distributional Semantics", "comments": null, "journal-ref": null, "doi": "10.1007/s40362-014-0017-z", "report-no": null, "categories": "cs.CL cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey presents in some detail the main advances that have been recently\ntaking place in Computational Linguistics towards the unification of the two\nprominent semantic paradigms: the compositional formal semantics view and the\ndistributional models of meaning based on vector spaces. After an introduction\nto these two approaches, I review the most important models that aim to provide\ncompositionality in distributional semantics. Then I proceed and present in\nmore detail a particular framework by Coecke, Sadrzadeh and Clark (2010) based\non the abstract mathematical setting of category theory, as a more complete\nexample capable to demonstrate the diversity of techniques and scientific\ndisciplines that this kind of research can draw from. This paper concludes with\na discussion about important open issues that need to be addressed by the\nresearchers in the future.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 14:28:01 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Kartsaklis", "Dimitri", ""]]}, {"id": "1401.5389", "submitter": "Sajib Dasgupta", "authors": "Sajib Dasgupta, Vincent Ng", "title": "Which Clustering Do You Want? Inducing Your Ideal Clustering with\n  Minimal Feedback", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  581-632, 2010", "doi": "10.1613/jair.3003", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While traditional research on text clustering has largely focused on grouping\ndocuments by topic, it is conceivable that a user may want to cluster documents\nalong other dimensions, such as the authors mood, gender, age, or sentiment.\nWithout knowing the users intention, a clustering algorithm will only group\ndocuments along the most prominent dimension, which may not be the one the user\ndesires. To address the problem of clustering documents along the user-desired\ndimension, previous work has focused on learning a similarity metric from data\nmanually annotated with the users intention or having a human construct a\nfeature space in an interactive manner during the clustering process. With the\ngoal of reducing reliance on human knowledge for fine-tuning the similarity\nfunction or selecting the relevant features required by these approaches, we\npropose a novel active clustering algorithm, which allows a user to easily\nselect the dimension along which she wants to cluster the documents by\ninspecting only a small number of words. We demonstrate the viability of our\nalgorithm on a variety of commonly-used sentiment datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:56:03 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Dasgupta", "Sajib", ""], ["Ng", "Vincent", ""]]}, {"id": "1401.5390", "submitter": "S.R.K. Branavan", "authors": "S.R.K. Branavan, David Silver, Regina Barzilay", "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  661-704, 2012", "doi": "10.1613/jair.3484", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain knowledge is crucial for effective performance in autonomous control\nsystems. Typically, human effort is required to encode this knowledge into a\ncontrol algorithm. In this paper, we present an approach to language grounding\nwhich automatically interprets text in the context of a complex control\napplication, such as a game, and uses domain knowledge extracted from the text\nto improve control performance. Both text analysis and control strategies are\nlearned jointly using only a feedback signal inherent to the application. To\neffectively leverage textual information, our method automatically extracts the\ntext segment most relevant to the current game state, and labels it with a\ntask-centric predicate structure. This labeled text is then used to bias an\naction selection policy for the game, guiding it towards promising regions of\nthe action space. We encode our model for text analysis and game playing in a\nmulti-layer neural network, representing linguistic decisions via latent\nvariables in the hidden layers, and game action quality via the output layer.\nOperating within the Monte-Carlo Search framework, we estimate model parameters\nusing feedback from simulated games. We apply our approach to the complex\nstrategy game Civilization II using the official game manual as the text guide.\nOur results show that a linguistically-informed game-playing agent\nsignificantly outperforms its language-unaware counterpart, yielding a 34%\nabsolute improvement and winning over 65% of games when playing against the\nbuilt-in AI of Civilization.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:10:57 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Branavan", "S. R. K.", ""], ["Silver", "David", ""], ["Barzilay", "Regina", ""]]}, {"id": "1401.5644", "submitter": "Issam Sahmoudi issam sahmoudi", "authors": "Issam Sahmoudi and Hanane Froud and Abdelmonaime Lachkar", "title": "A new keyphrases extraction method based on suffix tree data structure\n  for arabic documents clustering", "comments": "17 pages, 3 figures", "journal-ref": "International Journal of Database Management Systems ( IJDMS )\n  Vol.5, No.6, December 2013", "doi": "10.5121/ijdms.2013.5602", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document Clustering is a branch of a larger area of scientific study known as\ndata mining .which is an unsupervised classification using to find a structure\nin a collection of unlabeled data. The useful information in the documents can\nbe accompanied by a large amount of noise words when using Full Text\nRepresentation, and therefore will affect negatively the result of the\nclustering process. So it is with great need to eliminate the noise words and\nkeeping just the useful information in order to enhance the quality of the\nclustering results. This problem occurs with different degree for any language\nsuch as English, European, Hindi, Chinese, and Arabic Language. To overcome\nthis problem, in this paper, we propose a new and efficient Keyphrases\nextraction method based on the Suffix Tree data structure (KpST), the extracted\nKeyphrases are then used in the clustering process instead of Full Text\nRepresentation. The proposed method for Keyphrases extraction is language\nindependent and therefore it may be applied to any language. In this\ninvestigation, we are interested to deal with the Arabic language which is one\nof the most complex languages. To evaluate our method, we conduct an\nexperimental study on Arabic Documents using the most popular Clustering\napproach of Hierarchical algorithms: Agglomerative Hierarchical algorithm with\nseven linkage techniques and a variety of distance functions and similarity\nmeasures to perform Arabic Document Clustering task. The obtained results show\nthat our method for extracting Keyphrases increases the quality of the\nclustering results. We propose also to study the effect of using the stemming\nfor the testing dataset to cluster it with the same documents clustering\ntechniques and similarity/distance measures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 12:36:38 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Sahmoudi", "Issam", ""], ["Froud", "Hanane", ""], ["Lachkar", "Abdelmonaime", ""]]}, {"id": "1401.5674", "submitter": "Felipe S\\'anchez-Mart\\'inez", "authors": "Felipe S\\'anchez-Mart\\'inez, Rafael C. Carrasco, Miguel A.\n  Mart\\'inez-Prieto, Joaquin Adiego", "title": "Generalized Biwords for Bitext Compression and Translation Spotting", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  389-418, 2012", "doi": "10.1613/jair.3500", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large bilingual parallel texts (also known as bitexts) are usually stored in\na compressed form, and previous work has shown that they can be more\nefficiently compressed if the fact that the two texts are mutual translations\nis exploited. For example, a bitext can be seen as a sequence of biwords\n---pairs of parallel words with a high probability of co-occurrence--- that can\nbe used as an intermediate representation in the compression process. However,\nthe simple biword approach described in the literature can only exploit\none-to-one word alignments and cannot tackle the reordering of words. We\ntherefore introduce a generalization of biwords which can describe multi-word\nexpressions and reorderings. We also describe some methods for the binary\ncompression of generalized biword sequences, and compare their performance when\ndifferent schemes are applied to the extraction of the biword sequence. In\naddition, we show that this generalization of biwords allows for the\nimplementation of an efficient algorithm to look on the compressed bitext for\nwords or text segments in one of the texts and retrieve their counterpart\ntranslations in the other text ---an application usually referred to as\ntranslation spotting--- with only some minor modifications in the compression\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:11:30 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["S\u00e1nchez-Mart\u00ednez", "Felipe", ""], ["Carrasco", "Rafael C.", ""], ["Mart\u00ednez-Prieto", "Miguel A.", ""], ["Adiego", "Joaquin", ""]]}, {"id": "1401.5693", "submitter": "Trevor Anthony Cohn", "authors": "Trevor Anthony Cohn, Mirella Lapata", "title": "Sentence Compression as Tree Transduction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  637-674, 2009", "doi": "10.1613/jair.2655", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a tree-to-tree transduction method for sentence\ncompression. Our model is based on synchronous tree substitution grammar, a\nformalism that allows local distortion of the tree topology and can thus\nnaturally capture structural mismatches. We describe an algorithm for decoding\nin this framework and show how the model can be trained discriminatively within\na large margin framework. Experimental results on sentence compression bring\nsignificant improvements over a state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:19:15 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Cohn", "Trevor Anthony", ""], ["Lapata", "Mirella", ""]]}, {"id": "1401.5694", "submitter": "Sebastian Pado", "authors": "Sebastian Pado, Mirella Lapata", "title": "Cross-lingual Annotation Projection for Semantic Roles", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  307-340, 2009", "doi": "10.1613/jair.2863", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the task of automatically inducing role-semantic\nannotations in the FrameNet paradigm for new languages. We propose a general\nframework that is based on annotation projection, phrased as a graph\noptimization problem. It is relatively inexpensive and has the potential to\nreduce the human effort involved in creating role-semantic resources. Within\nthis framework, we present projection models that exploit lexical and syntactic\ninformation. We provide an experimental evaluation on an English-German\nparallel corpus which demonstrates the feasibility of inducing high-precision\nGerman semantic role annotation both for manually and automatically annotated\nEnglish data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:40:37 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Pado", "Sebastian", ""], ["Lapata", "Mirella", ""]]}, {"id": "1401.5695", "submitter": "Tahira Naseem", "authors": "Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, Regina Barzilay", "title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  341-385, 2009", "doi": "10.1613/jair.2843", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the effectiveness of multilingual learning for unsupervised\npart-of-speech tagging. The central assumption of our work is that by combining\ncues from multiple languages, the structure of each becomes more apparent. We\nconsider two ways of applying this intuition to the problem of unsupervised\npart-of-speech tagging: a model that directly merges tag structures for a pair\nof languages into a single sequence and a second model which instead\nincorporates multilingual context using latent variables. Both approaches are\nformulated as hierarchical Bayesian models, using Markov Chain Monte Carlo\nsampling techniques for inference. Our results demonstrate that by\nincorporating multilingual evidence we can achieve impressive performance gains\nacross a range of scenarios. We also found that performance improves steadily\nas the number of available languages increases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:39:01 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Naseem", "Tahira", ""], ["Snyder", "Benjamin", ""], ["Eisenstein", "Jacob", ""], ["Barzilay", "Regina", ""]]}, {"id": "1401.5696", "submitter": "Alexander Pieter Yates", "authors": "Alexander Pieter Yates, Oren Etzioni", "title": "Unsupervised Methods for Determining Object and Relation Synonyms on the\n  Web", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  255-296, 2009", "doi": "10.1613/jair.2772", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of identifying synonymous relations and objects, or synonym\nresolution, is critical for high-quality information extraction. This paper\ninvestigates synonym resolution in the context of unsupervised information\nextraction, where neither hand-tagged training examples nor domain knowledge is\navailable. The paper presents a scalable, fully-implemented system that runs in\nO(KN log N) time in the number of extractions, N, and the maximum number of\nsynonyms per word, K. The system, called Resolver, introduces a probabilistic\nrelational model for predicting whether two strings are co-referential based on\nthe similarity of the assertions containing them. On a set of two million\nassertions extracted from the Web, Resolver resolves objects with 78% precision\nand 68% recall, and resolves relations with 90% precision and 35% recall.\nSeveral variations of resolvers probabilistic model are explored, and\nexperiments demonstrate that under appropriate conditions these variations can\nimprove F1 by 5%. An extension to the basic Resolver system allows it to handle\npolysemous names with 97% precision and 95% recall on a data set from the TREC\ncorpus.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:33:07 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Yates", "Alexander Pieter", ""], ["Etzioni", "Oren", ""]]}, {"id": "1401.5697", "submitter": "Evgeniy Gabrilovich", "authors": "Evgeniy Gabrilovich, Shaul Markovitch", "title": "Wikipedia-based Semantic Interpretation for Natural Language Processing", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  443-498, 2009", "doi": "10.1613/jair.2669", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adequate representation of natural language semantics requires access to vast\namounts of common sense and domain-specific world knowledge. Prior work in the\nfield was based on purely statistical techniques that did not make use of\nbackground knowledge, on limited lexicographic knowledge bases such as WordNet,\nor on huge manual efforts such as the CYC project. Here we propose a novel\nmethod, called Explicit Semantic Analysis (ESA), for fine-grained semantic\ninterpretation of unrestricted natural language texts. Our method represents\nmeaning in a high-dimensional space of concepts derived from Wikipedia, the\nlargest encyclopedia in existence. We explicitly represent the meaning of any\ntext in terms of Wikipedia-based concepts. We evaluate the effectiveness of our\nmethod on text categorization and on computing the degree of semantic\nrelatedness between fragments of natural language text. Using ESA results in\nsignificant improvements over the previous state of the art in both tasks.\nImportantly, due to the use of natural concepts, the ESA model is easy to\nexplain to human users.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:21:01 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Gabrilovich", "Evgeniy", ""], ["Markovitch", "Shaul", ""]]}, {"id": "1401.5698", "submitter": "Yifan Li", "authors": "Yifan Li, Petr Musilek, Marek Reformat, Loren Wyard-Scott", "title": "Identification of Pleonastic It Using the Web", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  339-389, 2009", "doi": "10.1613/jair.2622", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a significant minority of cases, certain pronouns, especially the pronoun\nit, can be used without referring to any specific entity. This phenomenon of\npleonastic pronoun usage poses serious problems for systems aiming at even a\nshallow understanding of natural language texts. In this paper, a novel\napproach is proposed to identify such uses of it: the extrapositional cases are\nidentified using a series of queries against the web, and the cleft cases are\nidentified using a simple set of syntactic rules. The system is evaluated with\nfour sets of news articles containing 679 extrapositional cases as well as 78\ncleft constructs. The identification results are comparable to those obtained\nby human efforts.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:11:43 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Li", "Yifan", ""], ["Musilek", "Petr", ""], ["Reformat", "Marek", ""], ["Wyard-Scott", "Loren", ""]]}, {"id": "1401.5699", "submitter": "George Tsatsaronis", "authors": "George Tsatsaronis, Iraklis Varlamis, Michalis Vazirgiannis", "title": "Text Relatedness Based on a Word Thesaurus", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 37, pages\n  1-39, 2010", "doi": "10.1613/jair.2880", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of relatedness between two fragments of text in an automated\nmanner requires taking into account a wide range of factors pertaining to the\nmeaning the two fragments convey, and the pairwise relations between their\nwords. Without doubt, a measure of relatedness between text segments must take\ninto account both the lexical and the semantic relatedness between words. Such\na measure that captures well both aspects of text relatedness may help in many\ntasks, such as text retrieval, classification and clustering. In this paper we\npresent a new approach for measuring the semantic relatedness between words\nbased on their implicit semantic links. The approach exploits only a word\nthesaurus in order to devise implicit semantic links between words. Based on\nthis approach, we introduce Omiotis, a new measure of semantic relatedness\nbetween texts which capitalizes on the word-to-word semantic relatedness\nmeasure (SR) and extends it to measure the relatedness between texts. We\ngradually validate our method: we first evaluate the performance of the\nsemantic relatedness measure between individual words, covering word-to-word\nsimilarity and relatedness, synonym identification and word analogy; then, we\nproceed with evaluating the performance of our method in measuring text-to-text\nsemantic relatedness in two tasks, namely sentence-to-sentence similarity and\nparaphrase recognition. Experimental evaluation shows that the proposed method\noutperforms every lexicon-based method of semantic relatedness in the selected\ntasks and the used data sets, and competes well against corpus-based and hybrid\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:41:08 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Tsatsaronis", "George", ""], ["Varlamis", "Iraklis", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1401.5700", "submitter": "Felipe S\\'anchez-Mart\\'inez", "authors": "Felipe S\\'anchez-Mart\\'inez, Mikel L. Forcada", "title": "Inferring Shallow-Transfer Machine Translation Rules from Small Parallel\n  Corpora", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  605-635, 2009", "doi": "10.1613/jair.2735", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for the automatic inference of structural\ntransfer rules to be used in a shallow-transfer machine translation (MT) system\nfrom small parallel corpora. The structural transfer rules are based on\nalignment templates, like those used in statistical MT. Alignment templates are\nextracted from sentence-aligned parallel corpora and extended with a set of\nrestrictions which are derived from the bilingual dictionary of the MT system\nand control their application as transfer rules. The experiments conducted\nusing three different language pairs in the free/open-source MT platform\nApertium show that translation quality is improved as compared to word-for-word\ntranslation (when no transfer rules are used), and that the resulting\ntranslation quality is close to that obtained using hand-coded transfer rules.\nThe method we present is entirely unsupervised and benefits from information in\nthe rest of modules of the MT system in which the inferred rules are applied.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:28:26 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["S\u00e1nchez-Mart\u00ednez", "Felipe", ""], ["Forcada", "Mikel L.", ""]]}, {"id": "1401.5980", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, Stephen Pulman, Bob Coecke", "title": "Reasoning about Meaning in Natural Language with Compact Closed\n  Categories and Frobenius Algebras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compact closed categories have found applications in modeling quantum\ninformation protocols by Abramsky-Coecke. They also provide semantics for\nLambek's pregroup algebras, applied to formalizing the grammatical structure of\nnatural language, and are implicit in a distributional model of word meaning\nbased on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadeh\nused the product category of pregroups with vector spaces and provided a\ndistributional model of meaning for sentences. We recast this theory in terms\nof strongly monoidal functors and advance it via Frobenius algebras over vector\nspaces. The former are used to formalize topological quantum field theories by\nAtiyah and Baez-Dolan, and the latter are used to model classical data in\nquantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable us\nto work in a single space in which meanings of words, phrases, and sentences of\nany structure live. Hence we can compare meanings of different language\nconstructs and enhance the applicability of the theory. We report on\nexperimental results on a number of language tasks and verify the theoretical\npredictions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 14:20:58 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Kartsaklis", "Dimitri", ""], ["Sadrzadeh", "Mehrnoosh", ""], ["Pulman", "Stephen", ""], ["Coecke", "Bob", ""]]}, {"id": "1401.6050", "submitter": "Hai  Zhao", "authors": "Hai Zhao, Xiaotian Zhang, Chunyu Kit", "title": "Integrative Semantic Dependency Parsing via Efficient Large-scale\n  Feature Selection", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  203-233, 2013", "doi": "10.1613/jair.3717", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing, i.e., the automatic derivation of meaning representation\nsuch as an instantiated predicate-argument structure for a sentence, plays a\ncritical role in deep processing of natural language. Unlike all other top\nsystems of semantic dependency parsing that have to rely on a pipeline\nframework to chain up a series of submodels each specialized for a specific\nsubtask, the one presented in this article integrates everything into one\nmodel, in hopes of achieving desirable integrity and practicality for real\napplications while maintaining a competitive performance. This integrative\napproach tackles semantic parsing as a word pair classification problem using a\nmaximum entropy classifier. We leverage adaptive pruning of argument candidates\nand large-scale feature selection engineering to allow the largest feature\nspace ever in use so far in this field, it achieves a state-of-the-art\nperformance on the evaluation data set for CoNLL-2008 shared task, on top of\nall but one top pipeline system, confirming its feasibility and effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 16:45:39 GMT"}], "update_date": "2014-01-25", "authors_parsed": [["Zhao", "Hai", ""], ["Zhang", "Xiaotian", ""], ["Kit", "Chunyu", ""]]}, {"id": "1401.6122", "submitter": "Tanmoy Chakraborty", "authors": "Tanmoy Chakraborty, Dipankar Das, Sivaji Bandyopadhyay", "title": "Identifying Bengali Multiword Expressions using Semantic Clustering", "comments": "25 pages, 3 figures, 5 tables, International Journal of Linguistics\n  and Language Resources (Lingvistic{\\ae} Investigationes), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key issues in both natural language understanding and generation\nis the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge\nproblem to the precise language processing due to their idiosyncratic nature\nand diversity in lexical, syntactical and semantic properties. The semantics of\na MWE cannot be expressed after combining the semantics of its constituents.\nTherefore, the formalism of semantic clustering is often viewed as an\ninstrument for extracting MWEs especially for resource constraint languages\nlike Bengali. The present semantic clustering approach contributes to locate\nclusters of the synonymous noun tokens present in the document. These clusters\nin turn help measure the similarity between the constituent words of a\npotentially candidate phrase using a vector space model and judge the\nsuitability of this phrase to be a MWE. In this experiment, we apply the\nsemantic clustering approach for noun-noun bigram MWEs, though it can be\nextended to any types of MWEs. In parallel, the well known statistical models,\nnamely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),\nSignificance function are also employed to extract MWEs from the Bengali\ncorpus. The comparative evaluation shows that the semantic clustering approach\noutperforms all other competing statistical models. As a by-product of this\nexperiment, we have started developing a standard lexicon in Bengali that\nserves as a productive Bengali linguistic thesaurus.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 19:03:18 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Chakraborty", "Tanmoy", ""], ["Das", "Dipankar", ""], ["Bandyopadhyay", "Sivaji", ""]]}, {"id": "1401.6131", "submitter": "Jo\\~ao V. Gra\\c{c}a", "authors": "Jo\\~ao V. Gra\\c{c}a, Kuzman Ganchev, Luisa Coheur, Fernando Pereira,\n  Ben Taskar", "title": "Controlling Complexity in Part-of-Speech Induction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  527-551, 2011", "doi": "10.1613/jair.3348", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fully unsupervised learning of grammatical\n(part-of-speech) categories from unlabeled text. The standard\nmaximum-likelihood hidden Markov model for this task performs poorly, because\nof its weak inductive bias and large model capacity. We address this problem by\nrefining the model and modifying the learning objective to control its capacity\nvia para- metric and non-parametric constraints. Our approach enforces\nword-category association sparsity, adds morphological and orthographic\nfeatures, and eliminates hard-to-estimate parameters for rare words. We develop\nan efficient learning algorithm that is not much more computationally intensive\nthan standard training. We also provide an open-source implementation of the\nalgorithm. Our experiments on five diverse languages (Bulgarian, Danish,\nEnglish, Portuguese, Spanish) achieve significant improvements compared with\nprevious methods for the same task.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:20:08 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Gra\u00e7a", "Jo\u00e3o V.", ""], ["Ganchev", "Kuzman", ""], ["Coheur", "Luisa", ""], ["Pereira", "Fernando", ""], ["Taskar", "Ben", ""]]}, {"id": "1401.6169", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani, David J. Miller", "title": "Parsimonious Topic Models with Salient Word Discovery", "comments": null, "journal-ref": "IEEE Transaction on Knowledge and Data Engineering, 27 (2015)\n  824-837", "doi": "10.1109/TKDE.2014.2345378", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parsimonious topic model for text corpora. In related models\nsuch as Latent Dirichlet Allocation (LDA), all words are modeled\ntopic-specifically, even though many words occur with similar frequencies\nacross different topics. Our modeling determines salient words for each topic,\nwhich have topic-specific probabilities, with the rest explained by a universal\nshared model. Further, in LDA all topics are in principle present in every\ndocument. By contrast our model gives sparse topic representation, determining\nthe (small) subset of relevant topics for each document. We derive a Bayesian\nInformation Criterion (BIC), balancing model complexity and goodness of fit.\nHere, interestingly, we identify an effective sample size and corresponding\npenalty specific to each parameter type in our model. We minimize BIC to\njointly determine our entire model -- the topic-specific words,\ndocument-specific topics, all model parameter values, {\\it and} the total\nnumber of topics -- in a wholly unsupervised fashion. Results on three text\ncorpora and an image dataset show that our model achieves higher test set\nlikelihood and better agreement with ground-truth class labels, compared to LDA\nand to a model designed to incorporate sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 21:47:48 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 20:24:41 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Soleimani", "Hossein", ""], ["Miller", "David J.", ""]]}, {"id": "1401.6224", "submitter": "Maria Kalimeri", "authors": "Maria Kalimeri, Vassilios Constantoudis, Constantinos Papadimitriou,\n  Konstantinos Karamanos, Fotis K. Diakonos and Harris Papageorgiou", "title": "Word-length entropies and correlations of natural language written texts", "comments": "13 pages + 1 page of supporting information, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the frequency distributions and correlations of the word lengths of\nten European languages. Our findings indicate that a) the word-length\ndistribution of short words quantified by the mean value and the entropy\ndistinguishes the Uralic (Finnish) corpus from the others, b) the tails at long\nwords, manifested in the high-order moments of the distributions, differentiate\nthe Germanic languages (except for English) from the Romanic languages and\nGreek and c) the correlations between nearby word lengths measured by the\ncomparison of the real entropies with those of the shuffled texts are found to\nbe smaller in the case of Germanic and Finnish languages.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 00:26:58 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Kalimeri", "Maria", ""], ["Constantoudis", "Vassilios", ""], ["Papadimitriou", "Constantinos", ""], ["Karamanos", "Konstantinos", ""], ["Diakonos", "Fotis K.", ""], ["Papageorgiou", "Harris", ""]]}, {"id": "1401.6330", "submitter": "Li Dong", "authors": "Li Dong, Furu Wei, Shujie Liu, Ming Zhou, Ke Xu", "title": "A Statistical Parsing Framework for Sentiment Classification", "comments": "Accepted by Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a statistical parsing framework for sentence-level sentiment\nclassification in this article. Unlike previous works that employ syntactic\nparsing results for sentiment analysis, we develop a statistical parser to\ndirectly analyze the sentiment structure of a sentence. We show that\ncomplicated phenomena in sentiment analysis (e.g., negation, intensification,\nand contrast) can be handled the same as simple and straightforward sentiment\nexpressions in a unified and probabilistic way. We formulate the sentiment\ngrammar upon Context-Free Grammars (CFGs), and provide a formal description of\nthe sentiment parsing framework. We develop the parsing model to obtain\npossible sentiment parse trees for a sentence, from which the polarity model is\nproposed to derive the sentiment strength and polarity, and the ranking model\nis dedicated to selecting the best sentiment tree. We train the parser directly\nfrom examples of sentences annotated only with sentiment polarity labels but\nwithout any syntactic annotations or polarity annotations of constituents\nwithin sentences. Therefore we can obtain training data easily. In particular,\nwe train a sentiment parser, s.parser, from a large amount of review sentences\nwith users' ratings as rough sentiment polarity labels. Extensive experiments\non existing benchmark datasets show significant improvements over baseline\nsentiment classification approaches.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 12:56:36 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 05:26:13 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Dong", "Li", ""], ["Wei", "Furu", ""], ["Liu", "Shujie", ""], ["Zhou", "Ming", ""], ["Xu", "Ke", ""]]}, {"id": "1401.6422", "submitter": "Christina Sauper", "authors": "Christina Sauper, Regina Barzilay", "title": "Automatic Aggregation by Joint Modeling of Aspects and Values", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  89-127, 2013", "doi": "10.1613/jair.3647", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for aggregation of product review snippets by joint aspect\nidentification and sentiment analysis. Our model simultaneously identifies an\nunderlying set of ratable aspects presented in the reviews of a product (e.g.,\nsushi and miso for a Japanese restaurant) and determines the corresponding\nsentiment of each aspect. This approach directly enables discovery of\nhighly-rated or inconsistent aspects of a product. Our generative model admits\nan efficient variational mean-field inference algorithm. It is also easily\nextensible, and we describe several modifications and their effects on model\nstructure and inference. We test our model on two tasks, joint aspect\nidentification and sentiment analysis on a set of Yelp reviews and aspect\nidentification alone on a set of medical summaries. We evaluate the performance\nof the model on aspect identification, sentiment analysis, and per-word\nlabeling accuracy. We demonstrate that our model outperforms applicable\nbaselines by a considerable margin, yielding up to 32% relative error reduction\non aspect identification and up to 20% relative error reduction on sentiment\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:48:07 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Sauper", "Christina", ""], ["Barzilay", "Regina", ""]]}, {"id": "1401.6427", "submitter": "Seyed Abolghasem Mirroshandel", "authors": "Seyed Abolghasem Mirroshandel, Gholamreza Ghassem-Sani", "title": "Towards Unsupervised Learning of Temporal Relations between Events", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  125-163, 2012", "doi": "10.1613/jair.3693", "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of temporal relations between event pairs is an\nimportant task for several natural language processing applications such as\nQuestion Answering, Information Extraction, and Summarization. Since most\nexisting methods are supervised and require large corpora, which for many\nlanguages do not exist, we have concentrated our efforts to reduce the need for\nannotated data as much as possible. This paper presents two different\nalgorithms towards this goal. The first algorithm is a weakly supervised\nmachine learning approach for classification of temporal relations between\nevents. In the first stage, the algorithm learns a general classifier from an\nannotated corpus. Then, inspired by the hypothesis of \"one type of temporal\nrelation per discourse, it extracts useful information from a cluster of\ntopically related documents. We show that by combining the global information\nof such a cluster with local decisions of a general classifier, a bootstrapping\ncross-document classifier can be built to extract temporal relations between\nevents. Our experiments show that without any additional annotated data, the\naccuracy of the proposed algorithm is higher than that of several previous\nsuccessful systems. The second proposed method for temporal relation extraction\nis based on the expectation maximization (EM) algorithm. Within EM, we used\ndifferent techniques such as a greedy best-first search and integer linear\nprogramming for temporal inconsistency removal. We think that the experimental\nresults of our EM based algorithm, as a first step toward a fully unsupervised\ntemporal relation extraction method, is encouraging.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:50:50 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Mirroshandel", "Seyed Abolghasem", ""], ["Ghassem-Sani", "Gholamreza", ""]]}, {"id": "1401.6567", "submitter": "Kamal Sarkar", "authors": "Vivekananda Gayen, Kamal Sarkar", "title": "A Machine Learning Approach for the Identification of Bengali Noun-Noun\n  Compound Multiword Expressions", "comments": null, "journal-ref": "In Proceedings of ICON-2013: 10th International Conference on\n  Natural Language Processing, pp 290-296", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a machine learning approach for identification of Bengali\nmultiword expressions (MWE) which are bigram nominal compounds. Our proposed\napproach has two steps: (1) candidate extraction using chunk information and\nvarious heuristic rules and (2) training the machine learning algorithm called\nRandom Forest to classify the candidates into two groups: bigram nominal\ncompound MWE or not bigram nominal compound MWE. A variety of association\nmeasures, syntactic and linguistic clues and a set of WordNet-based similarity\nfeatures have been used for our MWE identification task. The approach presented\nin this paper can be used to identify bigram nominal compound MWE in Bengali\nrunning text.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 18:45:29 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Gayen", "Vivekananda", ""], ["Sarkar", "Kamal", ""]]}, {"id": "1401.6571", "submitter": "Shibamouli Lahiri", "authors": "Shibamouli Lahiri, Sagnik Ray Choudhury, Cornelia Caragea", "title": "Keyword and Keyphrase Extraction Using Centrality Measures on\n  Collocation Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword and keyphrase extraction is an important problem in natural language\nprocessing, with applications ranging from summarization to semantic search to\ndocument clustering. Graph-based approaches to keyword and keyphrase extraction\navoid the problem of acquiring a large in-domain training corpus by applying\nvariants of PageRank algorithm on a network of words. Although graph-based\napproaches are knowledge-lean and easily adoptable in online systems, it\nremains largely open whether they can benefit from centrality measures other\nthan PageRank. In this paper, we experiment with an array of centrality\nmeasures on word and noun phrase collocation networks, and analyze their\nperformance on four benchmark datasets. Not only are there centrality measures\nthat perform as well as or better than PageRank, but they are much simpler\n(e.g., degree, strength, and neighborhood size). Furthermore, centrality-based\nmethods give results that are competitive with and, in some cases, better than\ntwo strong unsupervised baselines.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 19:05:45 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Lahiri", "Shibamouli", ""], ["Choudhury", "Sagnik Ray", ""], ["Caragea", "Cornelia", ""]]}, {"id": "1401.6573", "submitter": "Christian Retore", "authors": "Livy-Maria Real-Coelho (LaBRI, UFPR), Christian Retor\\'e (LaBRI, IRIT)", "title": "Deverbal semantics and the Montagovian generative lexicon", "comments": "A revised version will appear in the Journal of Logic, Language and\n  Information", "journal-ref": "Journal of Logic, Language and Information (2014) 21", "doi": "10.1007/s10849-014-9187-y", "report-no": null, "categories": "cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a lexical account of action nominals, in particular of deverbal\nnominalisations, whose meaning is related to the event expressed by their base\nverb. The literature about nominalisations often assumes that the semantics of\nthe base verb completely defines the structure of action nominals. We argue\nthat the information in the base verb is not sufficient to completely determine\nthe semantics of action nominals. We exhibit some data from different\nlanguages, especially from Romance language, which show that nominalisations\nfocus on some aspects of the verb semantics. The selected aspects, however,\nseem to be idiosyncratic and do not automatically result from the internal\nstructure of the verb nor from its interaction with the morphological suffix.\nWe therefore propose a partially lexicalist approach view of deverbal nouns. It\nis made precise and computable by using the Montagovian Generative Lexicon, a\ntype theoretical framework introduced by Bassac, Mery and Retor\\'e in this\njournal in 2010. This extension of Montague semantics with a richer type system\neasily incorporates lexical phenomena like the semantics of action nominals in\nparticular deverbals, including their polysemy and (in)felicitous\ncopredications.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 19:50:51 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Real-Coelho", "Livy-Maria", "", "LaBRI, UFPR"], ["Retor\u00e9", "Christian", "", "LaBRI, IRIT"]]}, {"id": "1401.6574", "submitter": "Christian Retore", "authors": "Jean Gillibert (IMB), Christian Retor\\'e (LaBRI)", "title": "Category theory, logic and formal linguistics: some connections, old and\n  new", "comments": "Survey on the occasion of a special issue of the journal of applied\n  logic", "journal-ref": "Journal of Applied Logic 12, 1 (2014) 1--13", "doi": "10.1016/j.jal.2014.01.001", "report-no": null, "categories": "math.CT cs.CL cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seize the opportunity of the publication of selected papers from the\n\\emph{Logic, categories, semantics} workshop in the \\emph{Journal of Applied\nLogic} to survey some current trends in logic, namely intuitionistic and linear\ntype theories, that interweave categorical, geometrical and computational\nconsiderations. We thereafter present how these rich logical frameworks can\nmodel the way language conveys meaning.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 19:54:18 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Gillibert", "Jean", "", "IMB"], ["Retor\u00e9", "Christian", "", "LaBRI"]]}, {"id": "1401.6875", "submitter": "Shaolin  Qu", "authors": "Shaolin Qu, Joyce Y. Chai", "title": "Context-based Word Acquisition for Situated Dialogue in a Virtual World", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 37, pages\n  247-277, 2010", "doi": "10.1613/jair.2912", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To tackle the vocabulary problem in conversational systems, previous work has\napplied unsupervised learning approaches on co-occurring speech and eye gaze\nduring interaction to automatically acquire new words. Although these\napproaches have shown promise, several issues related to human language\nbehavior and human-machine conversation have not been addressed. First,\npsycholinguistic studies have shown certain temporal regularities between human\neye movement and language production. While these regularities can potentially\nguide the acquisition process, they have not been incorporated in the previous\nunsupervised approaches. Second, conversational systems generally have an\nexisting knowledge base about the domain and vocabulary. While the existing\nknowledge can potentially help bootstrap and constrain the acquired new words,\nit has not been incorporated in the previous models. Third, eye gaze could\nserve different functions in human-machine conversation. Some gaze streams may\nnot be closely coupled with speech stream, and thus are potentially detrimental\nto word acquisition. Automated recognition of closely-coupled speech-gaze\nstreams based on conversation context is important. To address these issues, we\ndeveloped new approaches that incorporate user language behavior, domain\nknowledge, and conversation context in word acquisition. We evaluated these\napproaches in the context of situated dialogue in a virtual world. Our\nexperimental results have shown that incorporating the above three types of\ncontextual information significantly improves word acquisition performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:48:43 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Qu", "Shaolin", ""], ["Chai", "Joyce Y.", ""]]}, {"id": "1401.6876", "submitter": "Preslav Ivanov Nakov", "authors": "Preslav Ivanov Nakov, Hwee Tou Ng", "title": "Improving Statistical Machine Translation for a Resource-Poor Language\n  Using Related Resource-Rich Languages", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  179-222, 2012", "doi": "10.1613/jair.3540", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel language-independent approach for improving machine\ntranslation for resource-poor languages by exploiting their similarity to\nresource-rich ones. More precisely, we improve the translation from a\nresource-poor source language X_1 into a resource-rich language Y given a\nbi-text containing a limited number of parallel sentences for X_1-Y and a\nlarger bi-text for X_2-Y for some resource-rich language X_2 that is closely\nrelated to X_1. This is achieved by taking advantage of the opportunities that\nvocabulary overlap and similarities between the languages X_1 and X_2 in\nspelling, word order, and syntax offer: (1) we improve the word alignments for\nthe resource-poor language, (2) we further augment it with additional\ntranslation options, and (3) we take care of potential spelling differences\nthrough appropriate transliteration. The evaluation for Indonesian- >English\nusing Malay and for Spanish -> English using Portuguese and pretending Spanish\nis resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points,\nrespectively, which is an improvement over the best rivaling approaches, while\nusing much less additional data. Overall, our method cuts the amount of\nnecessary \"real training data by a factor of 2--5.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:42:12 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Nakov", "Preslav Ivanov", ""], ["Ng", "Hwee Tou", ""]]}, {"id": "1401.6984", "submitter": "Yajie Miao", "authors": "Yajie Miao", "title": "Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN", "comments": "unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Kaldi toolkit is becoming popular for constructing automated speech\nrecognition (ASR) systems. Meanwhile, in recent years, deep neural networks\n(DNNs) have shown state-of-the-art performance on various ASR tasks. This\ndocument describes our open-source recipes to implement fully-fledged DNN\nacoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning\ntoolkit developed under the Theano environment. Using these recipes, we can\nbuild up multiple systems including DNN hybrid systems, convolutional neural\nnetwork (CNN) systems and bottleneck feature systems. These recipes are\ndirectly based on the Kaldi Switchboard 110-hour setup. However, adapting them\nto new datasets is easy to achieve.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 19:55:34 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Miao", "Yajie", ""]]}, {"id": "1401.7077", "submitter": "Gerardo Febres", "authors": "Gerardo Febres, Klaus Jaffe", "title": "Quantifying literature quality using complexity criteria", "comments": "Submitted for publication. 29 pages. 8 figures, 4 tables, 4\n  appendixes", "journal-ref": "Journal of Quantitative Linguistics, 2017, Vol 24, Iss 1", "doi": "10.1080/09296174.2016.1169847", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We measured entropy and symbolic diversity for English and Spanish texts\nincluding literature Nobel laureates and other famous authors. Entropy, symbol\ndiversity and symbol frequency profiles were compared for these four groups. We\nalso built a scale sensitive to the quality of writing and evaluated its\nrelationship with the Flesch's readability index for English and the\nSzigriszt's perspicuity index for Spanish. Results suggest a correlation\nbetween entropy and word diversity with quality of writing. Text genre also\ninfluences the resulting entropy and diversity of the text. Results suggest the\nplausibility of automated quality assessment of texts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 03:48:01 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 20:11:28 GMT"}, {"version": "v3", "created": "Sat, 24 Jan 2015 03:38:02 GMT"}, {"version": "v4", "created": "Sun, 15 Jan 2017 15:49:55 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Febres", "Gerardo", ""], ["Jaffe", "Klaus", ""]]}, {"id": "1401.8269", "submitter": "Peter Turney", "authors": "Peter D. Turney and Saif M. Mohammad", "title": "Experiments with Three Approaches to Recognizing Lexical Entailment", "comments": "to appear in Natural Language Engineering", "journal-ref": "Natural Language Engineering, 21 (3), (2015), 437-476", "doi": "10.1017/S1351324913000387", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in natural language often involves recognizing lexical entailment\n(RLE); that is, identifying whether one word entails another. For example,\n\"buy\" entails \"own\". Two general strategies for RLE have been proposed: One\nstrategy is to manually construct an asymmetric similarity measure for context\nvectors (directional similarity) and another is to treat RLE as a problem of\nlearning to recognize semantic relations using supervised machine learning\ntechniques (relation classification). In this paper, we experiment with two\nrecent state-of-the-art representatives of the two general strategies. The\nfirst approach is an asymmetric similarity measure (an instance of the\ndirectional similarity strategy), designed to capture the degree to which the\ncontexts of a word, a, form a subset of the contexts of another word, b. The\nsecond approach (an instance of the relation classification strategy)\nrepresents a word pair, a:b, with a feature vector that is the concatenation of\nthe context vectors of a and b, and then applies supervised learning to a\ntraining set of labeled feature vectors. Additionally, we introduce a third\napproach that is a new instance of the relation classification strategy. The\nthird approach represents a word pair, a:b, with a feature vector in which the\nfeatures are the differences in the similarities of a and b to a set of\nreference words. All three approaches use vector space models (VSMs) of\nsemantics, based on word-context matrices. We perform an extensive evaluation\nof the three approaches using three different datasets. The proposed new\napproach (similarity differences) performs significantly better than the other\ntwo approaches on some datasets and there is no dataset for which it is\nsignificantly worse. Our results suggest it is beneficial to make connections\nbetween the research in lexical entailment and the research in semantic\nrelation classification.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 19:42:19 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Turney", "Peter D.", ""], ["Mohammad", "Saif M.", ""]]}]