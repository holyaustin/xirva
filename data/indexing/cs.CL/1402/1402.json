[{"id": "1402.0543", "submitter": "Bill Rea", "authors": "Jan Koeman and William Rea", "title": "How Does Latent Semantic Analysis Work? A Visualisation Approach", "comments": "13 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By using a small example, an analogy to photographic compression, and a\nsimple visualization using heatmaps, we show that latent semantic analysis\n(LSA) is able to extract what appears to be semantic meaning of words from a\nset of documents by blurring the distinctions between the words.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 23:09:28 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Koeman", "Jan", ""], ["Rea", "William", ""]]}, {"id": "1402.0556", "submitter": "Vahed Qazvinian", "authors": "Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr,\n  David Zajic, Michael Whidby, Taesun Moon", "title": "Generating Extractive Summaries of Scientific Paradigms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  165-201, 2013", "doi": "10.1613/jair.3732", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers and scientists increasingly find themselves in the position of\nhaving to quickly understand large amounts of technical material. Our goal is\nto effectively serve this need by using bibliometric text mining and\nsummarization techniques to generate summaries of scientific literature. We\nshow how we can use citations to produce automatically generated, readily\nconsumable, technical extractive summaries. We first propose C-LexRank, a model\nfor summarizing single scientific articles based on citations, which employs\ncommunity detection and extracts salient information-rich sentences. Next, we\nfurther extend our experiments to summarize a set of papers, which cover the\nsame scientific topic. We generate extractive summaries of a set of Question\nAnswering (QA) and Dependency Parsing (DP) papers, their abstracts, and their\ncitation sentences and show that citations have unique information amenable to\ncreating a summary.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:33:10 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Qazvinian", "Vahed", ""], ["Radev", "Dragomir R.", ""], ["Mohammad", "Saif M.", ""], ["Dorr", "Bonnie", ""], ["Zajic", "David", ""], ["Whidby", "Michael", ""], ["Moon", "Taesun", ""]]}, {"id": "1402.0563", "submitter": "Marta R. Costa-juss\\`a", "authors": "Marta R. Costa-juss\\`a, Carlos A. Henr\\'iquez, Rafael E. Banchs", "title": "Evaluating Indirect Strategies for Chinese-Spanish Statistical Machine\n  Translation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  761-780, 2012", "doi": "10.1613/jair.3786", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although, Chinese and Spanish are two of the most spoken languages in the\nworld, not much research has been done in machine translation for this language\npair. This paper focuses on investigating the state-of-the-art of\nChinese-to-Spanish statistical machine translation (SMT), which nowadays is one\nof the most popular approaches to machine translation. For this purpose, we\nreport details of the available parallel corpus which are Basic Traveller\nExpressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, we\nconduct experimental work with the largest of these three corpora to explore\nalternative SMT strategies by means of using a pivot language. Three\nalternatives are considered for pivoting: cascading, pseudo-corpus and\ntriangulation. As pivot language, we use either English, Arabic or French.\nResults show that, for a phrase-based SMT system, English is the best pivot\nlanguage between Chinese and Spanish. We propose a system output combination\nusing the pivot strategies which is capable of outperforming the direct\ntranslation strategy. The main objective of this work is motivating and\ninvolving the research community to work in this important pair of languages\ngiven their demographic impact.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:34:56 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Costa-juss\u00e0", "Marta R.", ""], ["Henr\u00edquez", "Carlos A.", ""], ["Banchs", "Rafael E.", ""]]}, {"id": "1402.0574", "submitter": "Kira Radinsky", "authors": "Kira Radinsky, Sagie Davidovich, Shaul Markovitch", "title": "Learning to Predict from Textual Data", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  641-684, 2012", "doi": "10.1613/jair.3865", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a current news event, we tackle the problem of generating plausible\npredictions of future events it might cause. We present a new methodology for\nmodeling and predicting such future news events using machine learning and data\nmining techniques. Our Pundit algorithm generalizes examples of causality pairs\nto infer a causality predictor. To obtain precisely labeled causality examples,\nwe mine 150 years of news articles and apply semantic natural language modeling\ntechniques to headlines containing certain predefined causality patterns. For\ngeneralization, the model uses a vast number of world knowledge ontologies.\nEmpirical evaluation on real news articles shows that our Pundit algorithm\nperforms as well as non-expert humans.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:39:12 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Radinsky", "Kira", ""], ["Davidovich", "Sagie", ""], ["Markovitch", "Shaul", ""]]}, {"id": "1402.0578", "submitter": "Maytham Alabbas", "authors": "Maytham Alabbas, Allan Ramsay", "title": "Natural Language Inference for Arabic Using Extended Tree Edit Distance\n  with Subtrees", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 48, pages\n  1-22, 2013", "doi": "10.1613/jair.3892", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many natural language processing (NLP) applications require the computation\nof similarities between pairs of syntactic or semantic trees. Many researchers\nhave used tree edit distance for this task, but this technique suffers from the\ndrawback that it deals with single node operations only. We have extended the\nstandard tree edit distance algorithm to deal with subtree transformation\noperations as well as single nodes. The extended algorithm with subtree\noperations, TED+ST, is more effective and flexible than the standard algorithm,\nespecially for applications that pay attention to relations among nodes (e.g.\nin linguistic trees, deleting a modifier subtree should be cheaper than the sum\nof deleting its components individually). We describe the use of TED+ST for\nchecking entailment between two Arabic text snippets. The preliminary results\nof using TED+ST were encouraging when compared with two string-based approaches\nand with the standard algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:40:42 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Alabbas", "Maytham", ""], ["Ramsay", "Allan", ""]]}, {"id": "1402.0586", "submitter": "Shafiq Rayhan Joty", "authors": "Shafiq Rayhan Joty, Giuseppe Carenini, Raymond T Ng", "title": "Topic Segmentation and Labeling in Asynchronous Conversations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  521-573, 2013", "doi": "10.1613/jair.3940", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic segmentation and labeling is often considered a prerequisite for\nhigher-level conversation analysis and has been shown to be useful in many\nNatural Language Processing (NLP) applications. We present two new corpora of\nemail and blog conversations annotated with topics, and evaluate annotator\nreliability for the segmentation and labeling tasks in these asynchronous\nconversations. We propose a complete computational framework for topic\nsegmentation and labeling in asynchronous conversations. Our approach extends\nstate-of-the-art methods by considering a fine-grained structure of an\nasynchronous conversation, along with other conversational features by applying\nrecent graph-based methods for NLP. For topic segmentation, we propose two\nnovel unsupervised models that exploit the fine-grained conversational\nstructure, and a novel graph-theoretic supervised model that combines lexical,\nconversational and topic features. For topic labeling, we propose two novel\n(unsupervised) random walk models that respectively capture conversation\nspecific clues from two different sources: the leading sentences and the\nfine-grained conversational structure. Empirical evaluation shows that the\nsegmentation and the labeling performed by our best models beat the\nstate-of-the-art, and are highly correlated with human annotations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:43:35 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Joty", "Shafiq Rayhan", ""], ["Carenini", "Giuseppe", ""], ["Ng", "Raymond T", ""]]}, {"id": "1402.1128", "submitter": "Hasim Sak", "authors": "Ha\\c{s}im Sak, Andrew Senior, Fran\\c{c}oise Beaufays", "title": "Long Short-Term Memory Based Recurrent Neural Network Architectures for\n  Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)\narchitecture that has been designed to address the vanishing and exploding\ngradient problems of conventional RNNs. Unlike feedforward neural networks,\nRNNs have cyclic connections making them powerful for modeling sequences. They\nhave been successfully used for sequence labeling and sequence prediction\ntasks, such as handwriting recognition, language modeling, phonetic labeling of\nacoustic frames. However, in contrast to the deep neural networks, the use of\nRNNs in speech recognition has been limited to phone recognition in small scale\ntasks. In this paper, we present novel LSTM based RNN architectures which make\nmore effective use of model parameters to train acoustic models for large\nvocabulary speech recognition. We train and compare LSTM, RNN and DNN models at\nvarious numbers of parameters and configurations. We show that LSTM models\nconverge quickly and give state of the art speech recognition performance for\nrelatively small sized models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 19:01:51 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Sak", "Ha\u015fim", ""], ["Senior", "Andrew", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "1402.1454", "submitter": "Sarath Chandar A P", "authors": "Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh M.\n  Khapra, Balaraman Ravindran, Vikas Raykar, Amrita Saha", "title": "An Autoencoder Approach to Learning Bilingual Word Representations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-language learning allows us to use training data from one language to\nbuild models for a different language. Many approaches to bilingual learning\nrequire that we have word-level alignment of sentences from parallel corpora.\nIn this work we explore the use of autoencoder-based methods for cross-language\nlearning of vectorial word representations that are aligned between two\nlanguages, while not relying on word-level alignments. We show that by simply\nlearning to reconstruct the bag-of-words representations of aligned sentences,\nwithin and between languages, we can in fact learn high-quality representations\nand do without word alignments. Since training autoencoders on word\nobservations presents certain computational issues, we propose and compare\ndifferent variations adapted to this setting. We also propose an explicit\ncorrelation maximizing regularizer that leads to significant improvement in the\nperformance. We empirically investigate the success of our approach on the\nproblem of cross-language test classification, where a classifier trained on a\ngiven language (e.g., English) must learn to generalize to a different language\n(e.g., German). These experiments demonstrate that our approaches are\ncompetitive with the state-of-the-art, achieving up to 10-14 percentage point\nimprovements over the best reported results on this task.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 18:53:30 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["P", "Sarath Chandar A", ""], ["Lauly", "Stanislas", ""], ["Larochelle", "Hugo", ""], ["Khapra", "Mitesh M.", ""], ["Ravindran", "Balaraman", ""], ["Raykar", "Vikas", ""], ["Saha", "Amrita", ""]]}, {"id": "1402.1668", "submitter": "John Osborne", "authors": "John David Osborne, Binod Gyawali, Thamar Solorio", "title": "Evaluation of YTEX and MetaMap for clinical concept recognition", "comments": "6 pages, working notes to the ShareClef eHealth 2013 Shared Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We used MetaMap and YTEX as a basis for the construc- tion of two separate\nsystems to participate in the 2013 ShARe/CLEF eHealth Task 1[9], the\nrecognition of clinical concepts. No modifications were directly made to these\nsystems, but output concepts were filtered using stop concepts, stop concept\ntext and UMLS semantic type. Con- cept boundaries were also adjusted using a\nsmall collection of rules to increase precision on the strict task. Overall\nMetaMap had better per- formance than YTEX on the strict task, primarily due to\na 20% perfor- mance improvement in precision. In the relaxed task YTEX had\nbetter performance in both precision and recall giving it an overall F-Score\n4.6% higher than MetaMap on the test data. Our results also indicated a 1.3%\nhigher accuracy for YTEX in UMLS CUI mapping.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 15:33:44 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Osborne", "John David", ""], ["Gyawali", "Binod", ""], ["Solorio", "Thamar", ""]]}, {"id": "1402.1939", "submitter": "Xiao-Yong Yan", "authors": "Xiao-Yong Yan and Petter Minnhagen", "title": "Maximum Entropy, Word-Frequency, Chinese Characters, and Multiple\n  Meanings", "comments": "15 pages, 10 figures, 2 tables", "journal-ref": "PLoS ONE 10(5): e0125592 (2015)", "doi": "10.1371/journal.pone.0125592", "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The word-frequency distribution of a text written by an author is well\naccounted for by a maximum entropy distribution, the RGF (random group\nformation)-prediction. The RGF-distribution is completely determined by the a\npriori values of the total number of words in the text (M), the number of\ndistinct words (N) and the number of repetitions of the most common word\n(k_max). It is here shown that this maximum entropy prediction also describes a\ntext written in Chinese characters. In particular it is shown that although the\nsame Chinese text written in words and Chinese characters have quite\ndifferently shaped distributions, they are nevertheless both well predicted by\ntheir respective three a priori characteristic values. It is pointed out that\nthis is analogous to the change in the shape of the distribution when\ntranslating a given text to another language. Another consequence of the\nRGF-prediction is that taking a part of a long text will change the input\nparameters (M, N, k_max) and consequently also the shape of the frequency\ndistribution. This is explicitly confirmed for texts written in Chinese\ncharacters. Since the RGF-prediction has no system-specific information beyond\nthe three a priori values (M, N, k_max), any specific language characteristic\nhas to be sought in systematic deviations from the RGF-prediction and the\nmeasured frequencies. One such systematic deviation is identified and, through\na statistical information theoretical argument and an extended RGF-model, it is\nproposed that this deviation is caused by multiple meanings of Chinese\ncharacters. The effect is stronger for Chinese characters than for Chinese\nwords. The relation between Zipf's law, the Simon-model for texts and the\npresent results are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 11:47:15 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2015 07:37:36 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Yan", "Xiao-Yong", ""], ["Minnhagen", "Petter", ""]]}, {"id": "1402.2427", "submitter": "Georg Groh", "authors": "Jan Hauffa, Tobias Lichtenberg, Georg Groh", "title": "An evaluation of keyword extraction from online communication for the\n  characterisation of social relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The set of interpersonal relationships on a social network service or a\nsimilar online community is usually highly heterogenous. The concept of tie\nstrength captures only one aspect of this heterogeneity. Since the unstructured\ntext content of online communication artefacts is a salient source of\ninformation about a social relationship, we investigate the utility of keywords\nextracted from the message body as a representation of the relationship's\ncharacteristics as reflected by the conversation topics. Keyword extraction is\nperformed using standard natural language processing methods. Communication\ndata and human assessments of the extracted keywords are obtained from Facebook\nusers via a custom application. The overall positive quality assessment\nprovides evidence that the keywords indeed convey relevant information about\nthe relationship.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 10:27:43 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Hauffa", "Jan", ""], ["Lichtenberg", "Tobias", ""], ["Groh", "Georg", ""]]}, {"id": "1402.2561", "submitter": "Tiziano Flati", "authors": "Tiziano Flati, Roberto Navigli", "title": "The CQC Algorithm: Cycling in Graphs to Semantically Enrich and Enhance\n  a Bilingual Dictionary", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  135-171, 2012", "doi": "10.1613/jair.3456", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilingual machine-readable dictionaries are knowledge resources useful in\nmany automatic tasks. However, compared to monolingual computational lexicons\nlike WordNet, bilingual dictionaries typically provide a lower amount of\nstructured information, such as lexical and semantic relations, and often do\nnot cover the entire range of possible translations for a word of interest. In\nthis paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for the\nautomated disambiguation of ambiguous translations in the lexical entries of a\nbilingual machine-readable dictionary. The dictionary is represented as a\ngraph, and cyclic patterns are sought in the graph to assign an appropriate\nsense tag to each translation in a lexical entry. Further, we use the\nalgorithms output to improve the quality of the dictionary itself, by\nsuggesting accurate solutions to structural problems such as misalignments,\npartial alignments and missing entries. Finally, we successfully apply CQC to\nthe task of synonym extraction.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:08:40 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Flati", "Tiziano", ""], ["Navigli", "Roberto", ""]]}, {"id": "1402.2562", "submitter": "Nathalie Chaignaud", "authors": "Nathalie Chaignaud (LITIS), Val\\'erie Delavigne (LiDiFra), Maryvonne\n  Holzem (LiDiFra), Jean-Philippe Kotowicz (LITIS), Alain Loisel (LITIS)", "title": "\\'Etude cognitive des processus de construction d'une requ\\^ete dans un\n  syst\\`eme de gestion de connaissances m\\'edicales", "comments": "29 pages", "journal-ref": "Revue Revue Technique et Science Informatiques 29 (2010) 991-1021", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the Cogni-CISMeF project, which aims at improving\nmedical information search in the CISMeF system (Catalog and Index of\nFrench-language health resources) by including a conversational agent to\ninteract with the user in natural language. To study the cognitive processes\ninvolved during the information search, a bottom-up methodology was adopted.\nExperimentation has been set up to obtain human dialogs between a user (playing\nthe role of patient) dealing with medical information search and a CISMeF\nexpert refining the request. The analysis of these dialogs underlined the use\nof discursive evidence: vocabulary, reformulation, implicit or explicit\nexpression of user intentions, conversational sequences, etc. A model of\nartificial agent is proposed. It leads the user in its information search by\nproposing to him examples, assistance and choices. This model was implemented\nand integrated in the CISMeF system. ---- Cet article d\\'ecrit le projet\nCogni-CISMeF qui propose un module de dialogue Homme-Machine \\`a int\\'egrer\ndans le syst\\`eme d'indexation de connaissances m\\'edicales CISMeF (Catalogue\net Index des Sites M\\'edicaux Francophones). Nous avons adopt\\'e une d\\'emarche\nde mod\\'elisation cognitive en proc\\'edant \\`a un recueil de corpus de\ndialogues entre un utilisateur (jouant le r\\^ole d'un patient) d\\'esirant une\ninformation m\\'edicale et un expert CISMeF af inant cette demande pour\nconstruire la requ\\^ete. Nous avons analys\\'e la structure des dialogues ainsi\nobtenus et avons \\'etudi\\'e un certain nombre d'indices discursifs :\nvocabulaire employ\\'e, marques de reformulation, commentaires m\\'eta et\n\\'epilinguistiques, expression implicite ou explicite des intentions de\nl'utilisateur, encha\\^inement conversationnel, etc. De cette analyse, nous\navons construit un mod\\`ele d'agent artificiel dot\\'e de capacit\\'es cognitives\ncapables d'aider l'utilisateur dans sa t\\^ache de recherche d'information. Ce\nmod\\`ele a \\'et\\'e impl\\'ement\\'e et int\\'egr\\'e dans le syst\\`eme CISMeF.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 20:46:24 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Chaignaud", "Nathalie", "", "LITIS"], ["Delavigne", "Val\u00e9rie", "", "LiDiFra"], ["Holzem", "Maryvonne", "", "LiDiFra"], ["Kotowicz", "Jean-Philippe", "", "LITIS"], ["Loisel", "Alain", "", "LITIS"]]}, {"id": "1402.2796", "submitter": "Fabio Celli PhD", "authors": "Fabio Celli and Massimo Poesio", "title": "PR2: A Language Independent Unsupervised Tool for Personality\n  Recognition from Text", "comments": "4 pages, peer reviewed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PR2, a personality recognition system available online, that\nperforms instance-based classification of Big5 personality types from\nunstructured text, using language-independent features. It has been tested on\nEnglish and Italian, achieving performances up to f=.68.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 11:55:31 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Celli", "Fabio", ""], ["Poesio", "Massimo", ""]]}, {"id": "1402.3040", "submitter": "Jia-Fei Hong", "authors": "Jia-Fei Hong, Kathleen Ahrens, Chu-Ren Huang", "title": "Event Structure of Transitive Verb: A MARVS perspective", "comments": "Chinese Lexical Semantics Workshop 2011(CLSW 2011) published in\n  International Journal of Computer Processing of Languages (Vol. 24, No. 01,\n  March 2012)", "journal-ref": "International Journal of Computer Processing of Languages (Vol.\n  24, No. 01, March 2012)", "doi": "10.1142/S179384061240003X", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of\nthe representation of verbal semantics that is based on Mandarin Chinese data\n(Huang et al. 2000). In the MARVS theory, there are two different types of\nmodules: Event Structure Modules and Role Modules. There are also two sets of\nattributes: Event-Internal Attributes and Role-Internal Attributes, which are\nlinked to the Event Structure Module and the Role Module, respectively. In this\nstudy, we focus on four transitive verbs as chi1(eat), wan2(play),\nhuan4(change) and shao1(burn) and explore their event structures by the MARVS\ntheory.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 06:44:24 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Hong", "Jia-Fei", ""], ["Ahrens", "Kathleen", ""], ["Huang", "Chu-Ren", ""]]}, {"id": "1402.3080", "submitter": "Santhy Viswam", "authors": "Santhy Viswam and Sajeer Karattil", "title": "Software Requirement Specification Using Reverse Speech Technology", "comments": "5 pages, International Journal of Computer Trends and Technology\n  (IJCTT) vol.5 no.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech analysis had been taken to a new level with the discovery of Reverse\nSpeech (RS). RS is the discovery of hidden messages, referred as reversals, in\nnormal speech. Works are in progress for exploiting the relevance of RS in\ndifferent real world applications such as investigation, medical field etc. In\nthis paper we represent an innovative method for preparing a reliable Software\nRequirement Specification (SRS) document with the help of reverse speech. As\nSRS act as the backbone for the successful completion of any project, a\nreliable method is needed to overcome the inconsistencies. Using RS such a\nreliable method for SRS documentation was developed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 10:28:43 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Viswam", "Santhy", ""], ["Karattil", "Sajeer", ""]]}, {"id": "1402.3371", "submitter": "Andrea Ballatore", "authors": "Andrea Ballatore, Michela Bertolotto, David C. Wilson", "title": "An evaluative baseline for geo-semantic relatedness and similarity", "comments": "GeoInformatica 2014", "journal-ref": null, "doi": "10.1007/s10707-013-0197-8", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In geographic information science and semantics, the computation of semantic\nsimilarity is widely recognised as key to supporting a vast number of tasks in\ninformation integration and retrieval. By contrast, the role of geo-semantic\nrelatedness has been largely ignored. In natural language processing, semantic\nrelatedness is often confused with the more specific semantic similarity. In\nthis article, we discuss a notion of geo-semantic relatedness based on Lehrer's\nsemantic fields, and we compare it with geo-semantic similarity. We then\ndescribe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a\nnew open dataset designed to evaluate computational measures of geo-semantic\nrelatedness and similarity. This dataset is larger than existing datasets of\nthis kind, and includes 97 geographic terms combined into 50 term pairs rated\nby 203 human subjects. GeReSiD is available online and can be used as an\nevaluation baseline to determine empirically to what degree a given\ncomputational model approximates geo-semantic relatedness and similarity.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 06:06:47 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Ballatore", "Andrea", ""], ["Bertolotto", "Michela", ""], ["Wilson", "David C.", ""]]}, {"id": "1402.3382", "submitter": "Rosy Madaan", "authors": "K.Rajan, Dr.V.Ramalingam, Dr.M.Ganesan", "title": "Machine Learning of Phonologically Conditioned Noun Declensions For\n  Tamil Morphological Generators", "comments": "13 pages. International Journal of Computer Engineering and\n  Applications, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents machine learning solutions to a practical problem of\nNatural Language Generation (NLG), particularly the word formation in\nagglutinative languages like Tamil, in a supervised manner. The morphological\ngenerator is an important component of Natural Language Processing in\nArtificial Intelligence. It generates word forms given a root and affixes. The\nmorphophonemic changes like addition, deletion, alternation etc., occur when\ntwo or more morphemes or words joined together. The Sandhi rules should be\nexplicitly specified in the rule based morphological analyzers and generators.\nIn machine learning framework, these rules can be learned automatically by the\nsystem from the training samples and subsequently be applied for new inputs. In\nthis paper we proposed the machine learning models which learn the\nmorphophonemic rules for noun declensions from the given training data. These\nmodels are trained to learn sandhi rules using various learning algorithms and\nthe performance of those algorithms are presented. From this we conclude that\nmachine learning of morphological processing such as word form generation can\nbe successfully learned in a supervised manner, without explicit description of\nrules. The performance of Decision trees and Bayesian machine learning\nalgorithms on noun declensions are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 06:46:44 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Rajan", "K.", ""], ["Ramalingam", "Dr. V.", ""], ["Ganesan", "Dr. M.", ""]]}, {"id": "1402.3405", "submitter": "Daniele Cerra", "authors": "Daniele Cerra, Mihai Datcu, and Peter Reinartz", "title": "Authorship Analysis based on Data Compression", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2014.01.019", "report-no": null, "categories": "cs.CL cs.DL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to perform authorship analysis using the Fast Compression\nDistance (FCD), a similarity measure based on compression with dictionaries\ndirectly extracted from the written texts. The FCD computes a similarity\nbetween two documents through an effective binary search on the intersection\nset between the two related dictionaries. In the reported experiments the\nproposed method is applied to documents which are heterogeneous in style,\nwritten in five different languages and coming from different historical\nperiods. Results are comparable to the state of the art and outperform\ntraditional compression-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 09:25:59 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Cerra", "Daniele", ""], ["Datcu", "Mihai", ""], ["Reinartz", "Peter", ""]]}, {"id": "1402.3648", "submitter": "Shikha Kabra", "authors": "Shikha Kabra, Ritika Agarwal", "title": "Auto Spell Suggestion for High Quality Speech Synthesis in Hindi", "comments": "4 pages, 5 figures. International Journal of Computer Applications,\n  2014", "journal-ref": null, "doi": "10.5120/15302-4039", "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of Text-to-Speech (TTS) synthesis in a particular language is to\nconvert arbitrary input text to intelligible and natural sounding speech.\nHowever, for a particular language like Hindi, which is a highly confusing\nlanguage (due to very close spellings), it is not an easy task to identify\nerrors/mistakes in input text and an incorrect text degrade the quality of\noutput speech hence this paper is a contribution to the development of high\nquality speech synthesis with the involvement of Spellchecker which generates\nspell suggestions for misspelled words automatically. Involvement of\nspellchecker would increase the efficiency of speech synthesis by providing\nspell suggestions for incorrect input text. Furthermore, we have provided the\ncomparative study for evaluating the resultant effect on to phonetic text by\nadding spellchecker on to input text.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2014 05:11:35 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Kabra", "Shikha", ""], ["Agarwal", "Ritika", ""]]}, {"id": "1402.3722", "submitter": "Yoav Goldberg", "authors": "Yoav Goldberg and Omer Levy", "title": "word2vec Explained: deriving Mikolov et al.'s negative-sampling\n  word-embedding method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The word2vec software of Tomas Mikolov and colleagues\n(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and\nprovides state-of-the-art word embeddings. The learning models behind the\nsoftware are described in two research papers. We found the description of the\nmodels in these papers to be somewhat cryptic and hard to follow. While the\nmotivations and presentation may be obvious to the neural-networks\nlanguage-modeling crowd, we had to struggle quite a bit to figure out the\nrationale behind the equations.\n  This note is an attempt to explain equation (4) (negative sampling) in\n\"Distributed Representations of Words and Phrases and their Compositionality\"\nby Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2014 21:03:02 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Goldberg", "Yoav", ""], ["Levy", "Omer", ""]]}, {"id": "1402.3891", "submitter": "Vinodhini G", "authors": "Vinodhini G Chandrasekaran RM", "title": "Performance Evaluation of Machine Learning Classifiers in Sentiment\n  Mining", "comments": "4 pages 2 tables, International Journal of Computer Trends and\n  Technology, volume 4, Issue 6, june 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the use of machine learning classifiers is of great value in\nsolving a variety of problems in text classification. Sentiment mining is a\nkind of text classification in which, messages are classified according to\nsentiment orientation such as positive or negative. This paper extends the idea\nof evaluating the performance of various classifiers to show their\neffectiveness in sentiment mining of online product reviews. The product\nreviews are collected from Amazon reviews. To evaluate the performance of\nclassifiers various evaluation methods like random sampling, linear sampling\nand bootstrap sampling are used. Our results shows that support vector machine\nwith bootstrap sampling method outperforms others classifiers and sampling\nmethods in terms of misclassification rate.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 05:24:42 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["RM", "Vinodhini G Chandrasekaran", ""]]}, {"id": "1402.4259", "submitter": "Amelia Carolina Sparavigna", "authors": "Roberto Marazzato and Amelia Carolina Sparavigna", "title": "Extracting Networks of Characters and Places from Written Works with\n  CHAPLIN", "comments": "Keywords: Literary experiments, Networks, Graph Visualization\n  Software, Text Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are proposing a tool able to gather information on social networks from\nnarrative texts. Its name is CHAPLIN, CHAracters and PLaces Interaction\nNetwork, implemented in VB.NET. Characters and places of the narrative works\nare extracted in a list of raw words. Aided by the interface, the user selects\nnames out of them. After this choice, the tool allows the user to enter some\nparameters, and, according to them, creates a network where the nodes are the\ncharacters and places, and the edges their interactions. Edges are labelled by\nperformances. The output is a GV file, written in the DOT graph scripting\nlanguage, which is rendered by means of the free open source software Graphviz.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 09:35:19 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Marazzato", "Roberto", ""], ["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1402.4380", "submitter": "Samuel Danso", "authors": "Samuel Danso, Eric Atwell and Owen Johnson", "title": "A Comparative Study of Machine Learning Methods for Verbal Autopsy Text\n  Classification", "comments": "10 pages", "journal-ref": "International Journal of Computer Science Issues, Volume 10, Issue\n  6, No 2, November 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Verbal Autopsy is the record of an interview about the circumstances of an\nuncertified death. In developing countries, if a death occurs away from health\nfacilities, a field-worker interviews a relative of the deceased about the\ncircumstances of the death; this Verbal Autopsy can be reviewed off-site. We\nreport on a comparative study of the processes involved in Text Classification\napplied to classifying Cause of Death: feature value representation; machine\nlearning classification algorithms; and feature reduction strategies in order\nto identify the suitable approaches applicable to the classification of Verbal\nAutopsy text. We demonstrate that normalised term frequency and the standard\nTFiDF achieve comparable performance across a number of classifiers. The\nresults also show Support Vector Machine is superior to other classification\nalgorithms employed in this research. Finally, we demonstrate the effectiveness\nof employing a \"locally-semi-supervised\" feature reduction strategy in order to\nincrease performance accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 16:02:05 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Danso", "Samuel", ""], ["Atwell", "Eric", ""], ["Johnson", "Owen", ""]]}, {"id": "1402.4678", "submitter": "Yelena Mandelshtam", "authors": "Yelena Mandelshtam and Natalia Komarova", "title": "When Learners Surpass their Sources: Mathematical Modeling of Learning\n  from an Inconsistent Source", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm to model and investigate the learning process of a\nlearner mastering a set of grammatical rules from an inconsistent source. The\ncompelling interest of human language acquisition is that the learning succeeds\nin virtually every case, despite the fact that the input data are formally\ninadequate to explain the success of learning. Our model explains how a learner\ncan successfully learn from or even surpass its imperfect source without\npossessing any additional biases or constraints about the types of patterns\nthat exist in the language. We use the data collected by Singleton and Newport\n(2004) on the performance of a 7-year boy Simon, who mastered the American Sign\nLanguage (ASL) by learning it from his parents, both of whom were imperfect\nspeakers of ASL. We show that the algorithm possesses a frequency-boosting\nproperty, whereby the frequency of the most common form of the source is\nincreased by the learner. We also explain several key features of Simon's ASL.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 02:18:10 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Mandelshtam", "Yelena", ""], ["Komarova", "Natalia", ""]]}, {"id": "1402.4802", "submitter": "Lu\\'is F.  Seoane MSc", "authors": "Ricard V. Sol\\'e and Lu\\'is F. Seoane", "title": "Ambiguity in language networks", "comments": "19 pages, 5 figures, review and book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human language defines the most complex outcomes of evolution. The emergence\nof such an elaborated form of communication allowed humans to create extremely\nstructured societies and manage symbols at different levels including, among\nothers, semantics. All linguistic levels have to deal with an astronomic\ncombinatorial potential that stems from the recursive nature of languages. This\nrecursiveness is indeed a key defining trait. However, not all words are\nequally combined nor frequent. In breaking the symmetry between less and more\noften used and between less and more meaning-bearing units, universal scaling\nlaws arise. Such laws, common to all human languages, appear on different\nstages from word inventories to networks of interacting words. Among these\nseemingly universal traits exhibited by language networks, ambiguity appears to\nbe a specially relevant component. Ambiguity is avoided in most computational\napproaches to language processing, and yet it seems to be a crucial element of\nlanguage architecture. Here we review the evidence both from language network\narchitecture and from theoretical reasonings based on a least effort argument.\nAmbiguity is shown to play an essential role in providing a source of language\nefficiency, and is likely to be an inevitable byproduct of network growth.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 09:26:17 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2014 15:10:40 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Sol\u00e9", "Ricard V.", ""], ["Seoane", "Lu\u00eds F.", ""]]}, {"id": "1402.5123", "submitter": "Abdelmalek Amine", "authors": "Abdelmalek Amine, Reda Mohamed Hamou, Michel Simonet", "title": "Detecting Opinions in Tweets", "comments": "13 pages, 2 figures", "journal-ref": "International Journal Of Data Mining And Emerging Technologies,\n  Year : 2013, Volume : 3, Issue : 1 First page : ( 23) Last page : ( 32) Print\n  ISSN : 2249-3212. Online ISSN : 2249-3220", "doi": "10.5958/j.2249-3220.3.1.004", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the incessant growth of documents describing the opinions of different\npeople circulating on the web, including Web 2.0 has made it possible to give\nan opinion on any product in the net. In this paper, we examine the various\nopinions expressed in the tweets and classify them positive, negative or\nneutral by using the emoticons for the Bayesian method and adjectives and\nadverbs for the Turney's method\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 20:15:18 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Amine", "Abdelmalek", ""], ["Hamou", "Reda Mohamed", ""], ["Simonet", "Michel", ""]]}, {"id": "1402.6010", "submitter": "Linhong Zhu", "authors": "Linhong Zhu and Aram Galstyan and James Cheng and Kristina Lerman", "title": "Tripartite Graph Clustering for Dynamic Sentiment Analysis on Social\n  Media", "comments": "A short version is in Proceeding of the 2014 ACM SIGMOD International\n  Conference on Management of data", "journal-ref": null, "doi": "10.1145/2588555.2593682", "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing popularity of social media (e.g, Twitter) allows users to easily\nshare information with each other and influence others by expressing their own\nsentiments on various subjects. In this work, we propose an unsupervised\n\\emph{tri-clustering} framework, which analyzes both user-level and tweet-level\nsentiments through co-clustering of a tripartite graph. A compelling feature of\nthe proposed framework is that the quality of sentiment clustering of tweets,\nusers, and features can be mutually improved by joint clustering. We further\ninvestigate the evolution of user-level sentiments and latent feature vectors\nin an online framework and devise an efficient online algorithm to sequentially\nupdate the clustering of tweets, users and features with newly arrived data.\nThe online framework not only provides better quality of both dynamic\nuser-level and tweet-level sentiment analysis, but also improves the\ncomputational and storage efficiency. We verified the effectiveness and\nefficiency of the proposed approaches on the November 2012 California ballot\nTwitter data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 22:58:28 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2014 18:50:55 GMT"}, {"version": "v3", "created": "Thu, 12 Jun 2014 18:49:21 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Zhu", "Linhong", ""], ["Galstyan", "Aram", ""], ["Cheng", "James", ""], ["Lerman", "Kristina", ""]]}, {"id": "1402.6238", "submitter": "Jobin Wilson", "authors": "Jobin Wilson, Santanu Chaudhury, Brejesh Lall, Prateek Kapadia", "title": "Improving Collaborative Filtering based Recommenders using Topic\n  Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Collaborative Filtering (CF) algorithms make use of interactions\nbetween users and items in the form of implicit or explicit ratings alone for\ngenerating recommendations. Similarity among users or items is calculated\npurely based on rating overlap in this case,without considering explicit\nproperties of users or items involved, limiting their applicability in domains\nwith very sparse rating spaces. In many domains such as movies, news or\nelectronic commerce recommenders, considerable contextual data in text form\ndescribing item properties is available along with the rating data, which could\nbe utilized to improve recommendation quality.In this paper, we propose a novel\napproach to improve standard CF based recommenders by utilizing latent\nDirichlet allocation (LDA) to learn latent properties of items, expressed in\nterms of topic proportions, derived from their textual description. We infer\nuser's topic preferences or persona in the same latent space,based on her\nhistorical ratings. While computing similarity between users, we make use of a\ncombined similarity measure involving rating overlap as well as similarity in\nthe latent topic space. This approach alleviates sparsity problem as it allows\ncalculation of similarity between users even if they have not rated any items\nin common. Our experiments on multiple public datasets indicate that the\nproposed hybrid approach significantly outperforms standard user Based and item\nBased CF recommenders in terms of classification accuracy metrics such as\nprecision, recall and f-measure.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 16:52:05 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Wilson", "Jobin", ""], ["Chaudhury", "Santanu", ""], ["Lall", "Brejesh", ""], ["Kapadia", "Prateek", ""]]}, {"id": "1402.6516", "submitter": "Greg Dubbin", "authors": "Greg Dubbin and Phil Blunsom", "title": "Modelling the Lexicon in Unsupervised Part of Speech Induction", "comments": "To be presented at the 14th Conference of the European Chapter of the\n  Association for Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically inducing the syntactic part-of-speech categories for words in\ntext is a fundamental task in Computational Linguistics. While the performance\nof unsupervised tagging models has been slowly improving, current\nstate-of-the-art systems make the obviously incorrect assumption that all\ntokens of a given word type must share a single part-of-speech tag. This\none-tag-per-type heuristic counters the tendency of Hidden Markov Model based\ntaggers to over generate tags for a given word type. However, it is clearly\nincompatible with basic syntactic theory. In this paper we extend a\nstate-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model\nof the lexicon. In doing so we are able to incorporate a soft bias towards\ninducing few tags per type. We develop a particle filter for drawing samples\nfrom the posterior of our model and present empirical results that show that\nour model is competitive with and faster than the state-of-the-art without\nmaking any unrealistic restrictions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 12:37:04 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Dubbin", "Greg", ""], ["Blunsom", "Phil", ""]]}, {"id": "1402.6690", "submitter": "Jalal Mahmud", "authors": "Jalal Mahmud, Jilin Chen, Jeffrey Nichols", "title": "Why Are You More Engaged? Predicting Social Engagement from Word Use", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study to analyze how word use can predict social engagement\nbehaviors such as replies and retweets in Twitter. We compute psycholinguistic\ncategory scores from word usage, and investigate how people with different\nscores exhibited different reply and retweet behaviors on Twitter. We also\nfound psycholinguistic categories that show significant correlations with such\nsocial engagement behaviors. In addition, we have built predictive models of\nreplies and retweets from such psycholinguistic category based features. Our\nexperiments using a real world dataset collected from Twitter validates that\nsuch predictions can be done with reasonable accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 20:58:00 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Mahmud", "Jalal", ""], ["Chen", "Jilin", ""], ["Nichols", "Jeffrey", ""]]}, {"id": "1402.6764", "submitter": "Hazlina Haron", "authors": "Hazlina Haron, Abdul Azim Abd. Ghani", "title": "A method to identify potential ambiguous Malay words through Ambiguity\n  Attributes mapping: An exploratory Study", "comments": "Paper was presented at The Fourth International Conference of\n  Computer Science and Information Technology (CCSIT2014)in Sydney, Australia\n  on Feb 22, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe here a methodology to identify a list of ambiguous Malay words\nthat are commonly being used in Malay documentations such as Requirement\nSpecification. We compiled several relevant and appropriate requirement quality\nattributes and sentence rules from previous literatures and adopt it to come\nout with a set of ambiguity attributes that most suit Malay words. The\nextracted Malay ambiguous words (potential) are then being mapped onto the\nconstructed ambiguity attributes to confirm their vagueness. The list is then\nverified by Malay linguist experts. This paper aims to identify a list of\npotential ambiguous words in Malay as an attempt to assist writers to avoid\nusing the vague words while documenting Malay Requirement Specification as well\nas to any other related Malay documentation. The result of this study is a list\nof 120 potential ambiguous Malay words that could act as guidelines in writing\nMalay sentences\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 01:47:46 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Haron", "Hazlina", ""], ["Ghani", "Abdul Azim Abd.", ""]]}, {"id": "1402.6792", "submitter": "Lada A. Adamic", "authors": "Lada A. Adamic, Thomas M. Lento, Eytan Adar, Pauline C. Ng", "title": "Information Evolution in Social Networks", "comments": null, "journal-ref": null, "doi": "10.1145/2835776.2835827", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks readily transmit information, albeit with less than perfect\nfidelity. We present a large-scale measurement of this imperfect information\ncopying mechanism by examining the dissemination and evolution of thousands of\nmemes, collectively replicated hundreds of millions of times in the online\nsocial network Facebook. The information undergoes an evolutionary process that\nexhibits several regularities. A meme's mutation rate characterizes the\npopulation distribution of its variants, in accordance with the Yule process.\nVariants further apart in the diffusion cascade have greater edit distance, as\nwould be expected in an iterative, imperfect replication process. Some text\nsequences can confer a replicative advantage; these sequences are abundant and\ntransfer \"laterally\" between different memes. Subpopulations of the social\nnetwork can preferentially transmit a specific variant of a meme if the variant\nmatches their beliefs or culture. Understanding the mechanism driving change in\ndiffusing information has important implications for how we interpret and\nharness the information that reaches us through our social networks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 05:16:27 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Adamic", "Lada A.", ""], ["Lento", "Thomas M.", ""], ["Adar", "Eytan", ""], ["Ng", "Pauline C.", ""]]}, {"id": "1402.6880", "submitter": "Mark Keane", "authors": "M.T. Keane and A. Gerow", "title": "It's distributions all the way down!: Second order changes in\n  statistical distributions also occur", "comments": null, "journal-ref": "Behavioral & Brain Sciences, 2014, 37(1), 87", "doi": "10.1017/S0140525X13001763", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The textual, big-data literature misses Bentley, OBrien, & Brocks (Bentley et\nals) message on distributions; it largely examines the first-order effects of\nhow a single, signature distribution can predict population behaviour,\nneglecting second-order effects involving distributional shifts, either between\nsignature distributions or within a given signature distribution. Indeed,\nBentley et al. themselves under-emphasise the potential richness of the latter,\nwithin-distribution effects.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 12:12:11 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Keane", "M. T.", ""], ["Gerow", "A.", ""]]}, {"id": "1402.7265", "submitter": "Yarin Gal", "authors": "Yarin Gal", "title": "Semantics, Modelling, and the Problem of Representation of Meaning -- a\n  Brief Survey of Recent Literature", "comments": "15 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past 50 years many have debated what representation should be used\nto capture the meaning of natural language utterances. Recently new needs of\nsuch representations have been raised in research. Here I survey some of the\ninteresting representations suggested to answer for these new needs.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 14:49:31 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Gal", "Yarin", ""]]}]