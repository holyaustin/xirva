[{"id": "1306.0963", "submitter": "Been Kim", "authors": "Been Kim, Caleb M. Chacha, Julie Shah", "title": "Inferring Robot Task Plans from Human Team Meetings: A Generative\n  Modeling Approach with Logic-Based Prior", "comments": "Appears in Proceedings of the Twenty-Seventh AAAI Conference on\n  Artificial Intelligence (AAAI-13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to reduce the burden of programming and deploying autonomous systems\nto work in concert with people in time-critical domains, such as military field\noperations and disaster response. Deployment plans for these operations are\nfrequently negotiated on-the-fly by teams of human planners. A human operator\nthen translates the agreed upon plan into machine instructions for the robots.\nWe present an algorithm that reduces this translation burden by inferring the\nfinal plan from a processed form of the human team's planning conversation. Our\napproach combines probabilistic generative modeling with logical plan\nvalidation used to compute a highly structured prior over possible plans. This\nhybrid approach enables us to overcome the challenge of performing inference\nover the large solution space with only a small amount of noisy data from the\nteam planning session. We validate the algorithm through human subject\nexperimentation and show we are able to infer a human team's final plan with\n83% accuracy on average. We also describe a robot demonstration in which two\npeople plan and execute a first-response collaborative task with a PR2 robot.\nTo the best of our knowledge, this is the first work that integrates a logical\nplanning technique within a generative model to perform plan inference.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 02:17:11 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Kim", "Been", ""], ["Chacha", "Caleb M.", ""], ["Shah", "Julie", ""]]}, {"id": "1306.1343", "submitter": "Andrea Esuli", "authors": "Andrea Esuli", "title": "The User Feedback on SentiWordNet", "comments": null, "journal-ref": null, "doi": null, "report-no": "/cnr.isti/2013-TR-015", "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the release of SentiWordNet 3.0 the related Web interface has been\nrestyled and improved in order to allow users to submit feedback on the\nSentiWordNet entries, in the form of the suggestion of alternative triplets of\nvalues for an entry. This paper reports on the release of the user feedback\ncollected so far and on the plans for the future.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 08:56:32 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Esuli", "Andrea", ""]]}, {"id": "1306.1927", "submitter": "Been Kim", "authors": "Been Kim and Cynthia Rudin", "title": "Learning About Meetings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most people participate in meetings almost every day, multiple times a day.\nThe study of meetings is important, but also challenging, as it requires an\nunderstanding of social signals and complex interpersonal dynamics. Our aim\nthis work is to use a data-driven approach to the science of meetings. We\nprovide tentative evidence that: i) it is possible to automatically detect when\nduring the meeting a key decision is taking place, from analyzing only the\nlocal dialogue acts, ii) there are common patterns in the way social dialogue\nacts are interspersed throughout a meeting, iii) at the time key decisions are\nmade, the amount of time left in the meeting can be predicted from the amount\nof time that has passed, iv) it is often possible to predict whether a proposal\nduring a meeting will be accepted or rejected based entirely on the language\n(the set of persuasive words) used by the speaker.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 14:59:52 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Kim", "Been", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1306.2091", "submitter": "Nathan Schneider", "authors": "Nathan Schneider, Brendan O'Connor, Naomi Saphra, David Bamman, Manaal\n  Faruqui, Noah A. Smith, Chris Dyer, Jason Baldridge", "title": "A framework for (under)specifying dependency syntax without overloading\n  annotators", "comments": "This is an expanded version of a paper appearing in Proceedings of\n  the 7th Linguistic Annotation Workshop & Interoperability with Discourse,\n  Sofia, Bulgaria, August 8-9, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for lightweight dependency syntax annotation. Our\nformalism builds upon the typical representation for unlabeled dependencies,\npermitting a simple notation and annotation workflow. Moreover, the formalism\nencourages annotators to underspecify parts of the syntax if doing so would\nstreamline the annotation process. We demonstrate the efficacy of this\nannotation on three languages and develop algorithms to evaluate and compare\nunderspecified annotations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 02:54:10 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2013 01:44:50 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Schneider", "Nathan", ""], ["O'Connor", "Brendan", ""], ["Saphra", "Naomi", ""], ["Bamman", "David", ""], ["Faruqui", "Manaal", ""], ["Smith", "Noah A.", ""], ["Dyer", "Chris", ""], ["Baldridge", "Jason", ""]]}, {"id": "1306.2158", "submitter": "Edward Grefenstette", "authors": "Karl Moritz Hermann, Edward Grefenstette and Phil Blunsom", "title": "\"Not not bad\" is not \"bad\": A distributional account of negation", "comments": "9 pages, to appear in Proceedings of the 2013 Workshop on Continuous\n  Vector Space Models and their Compositionality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing empirical success of distributional models of\ncompositional semantics, it is timely to consider the types of textual logic\nthat such models are capable of capturing. In this paper, we address\nshortcomings in the ability of current models to capture logical operations\nsuch as negation. As a solution we propose a tripartite formulation for a\ncontinuous vector space representation of semantics and subsequently use this\nrepresentation to develop a formal compositional notion of negation within such\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 10:29:09 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Hermann", "Karl Moritz", ""], ["Grefenstette", "Edward", ""], ["Blunsom", "Phil", ""]]}, {"id": "1306.2268", "submitter": "Mi-Young  Park", "authors": "Keehang Kwon and Mi-Young Park", "title": "Accomplishable Tasks in Knowledge Representation", "comments": "arXiv admin note: substantial text overlap with arXiv:1305.2004", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Representation (KR) is traditionally based on the logic of facts,\nexpressed in boolean logic. However, facts about an agent can also be seen as a\nset of accomplished tasks by the agent. This paper proposes a new approach to\nKR: the notion of task logical KR based on Computability Logic. This notion\nallows the user to represent both accomplished tasks and accomplishable tasks\nby the agent. This notion allows us to build sophisticated KRs about many\ninteresting agents, which have not been supported by previous logical\nlanguages.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 04:42:02 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Kwon", "Keehang", ""], ["Park", "Mi-Young", ""]]}, {"id": "1306.2499", "submitter": "Mohammed Alaeddine Abderrahim", "authors": "Mohammed Alaeddine Abderrahim, Mohammed El Amine Abderrahim, Mohammed\n  Amine Chikh", "title": "Using Arabic Wordnet for semantic indexation in information retrieval\n  system", "comments": "6 pages,2 figures,7 tables", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 1, No 2, January 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of arabic Information Retrieval Systems (IRS) guided by arabic\nontology and to enable those systems to better respond to user requirements,\nthis paper aims to representing documents and queries by the best concepts\nextracted from Arabic Wordnet. Identified concepts belonging to Arabic WordNet\nsynsets are extracted from documents and queries, and those having a single\nsense are expanded. The expanded query is then used by the IRS to retrieve the\nrelevant documents searched. Our experiments are based primarily on a medium\nsize corpus of arabic text. The results obtained shown us that there are a\nglobal improvement in the performance of the arabic IRS.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 12:24:55 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 22:05:37 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Abderrahim", "Mohammed Alaeddine", ""], ["Abderrahim", "Mohammed El Amine", ""], ["Chikh", "Mohammed Amine", ""]]}, {"id": "1306.2593", "submitter": "Elaine Tsiang", "authors": "Elaine Y L Tsiang", "title": "A Perceptual Alphabet for the 10-dimensional Phonetic-prosodic Space", "comments": "4 pages, 11 tables The formulation of the phonetic subspace and the\n  alphabet are now based on the model of speech of oral billiards, thereby\n  reducing the size of the IHA from the previous version. It is now only\n  supplemental to the article on oral billiards in providing the detail\n  enumeration of the complete phonetic alphabet", "journal-ref": null, "doi": null, "report-no": "MIMC001", "categories": "cs.SD cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We define an alphabet, the IHA, of the 10-D phonetic-prosodic space. The\ndimensions of this space are perceptual observables, rather than articulatory\nspecifications. Speech is defined as a random chain in time of the 4-D phonetic\nsubspace, that is, a symbolic sequence, augmented with diacritics of the\nremaining 6-D prosodic subspace. The definitions here are based on the model of\nspeech of oral billiards, and supersedes an earlier version. This paper only\nenumerates the IHA in detail as a supplement to the exposition of oral\nbilliards in a separate paper. The IHA has been implemented as the target\nrandom variable in a speech recognizer.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 17:48:01 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 22:54:04 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Tsiang", "Elaine Y L", ""]]}, {"id": "1306.2838", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Jan Broekaert, Sandro Sozzo and Tomas Veloz", "title": "The Quantum Challenge in Concept Theory and Natural Language Processing", "comments": "5 pages", "journal-ref": "Proceedings of the 25th International Conference on System\n  Research, Informatics & Cybernetics (pp. 13-17). Ed.. E. G. Lasker, IIAS,\n  2013", "doi": null, "report-no": null, "categories": "cs.CL cs.IR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical formalism of quantum theory has been successfully used in\nhuman cognition to model decision processes and to deliver representations of\nhuman knowledge. As such, quantum cognition inspired tools have improved\ntechnologies for Natural Language Processing and Information Retrieval. In this\npaper, we overview the quantum cognition approach developed in our Brussels\nteam during the last two decades, specifically our identification of quantum\nstructures in human concepts and language, and the modeling of data from\npsychological and corpus-text-based experiments. We discuss our\nquantum-theoretic framework for concepts and their conjunctions/disjunctions in\na Fock-Hilbert space structure, adequately modeling a large amount of data\ncollected on concept combinations. Inspired by this modeling, we put forward\nelements for a quantum contextual and meaning-based approach to information\ntechnologies in which 'entities of meaning' are inversely reconstructed from\ntexts, which are considered as traces of these entities' states.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 14:35:11 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Aerts", "Diederik", ""], ["Broekaert", "Jan", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}, {"id": "1306.3584", "submitter": "Nal Kalchbrenner", "authors": "Nal Kalchbrenner, Phil Blunsom", "title": "Recurrent Convolutional Neural Networks for Discourse Compositionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compositionality of meaning extends beyond the single sentence. Just as\nwords combine to form the meaning of sentences, so do sentences combine to form\nthe meaning of paragraphs, dialogues and general discourse. We introduce both a\nsentence model and a discourse model corresponding to the two levels of\ncompositionality. The sentence model adopts convolution as the central\noperation for composing semantic vectors and is based on a novel hierarchical\nconvolutional neural network. The discourse model extends the sentence model\nand is based on a recurrent neural network that is conditioned in a novel way\nboth on the current sentence and on the current speaker. The discourse model is\nable to capture both the sequentiality of sentences and the interaction between\ndifferent speakers. Without feature engineering or pretraining and with simple\ngreedy decoding, the discourse model coupled to the sentence model obtains\nstate of the art performance on a dialogue act classification experiment.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2013 14:52:17 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Kalchbrenner", "Nal", ""], ["Blunsom", "Phil", ""]]}, {"id": "1306.3692", "submitter": "Felipe S\\'anchez-Mart\\'inez", "authors": "Felipe S\\'anchez-Mart\\'inez, Isabel Mart\\'inez-Sempere, Xavier\n  Ivars-Ribes, Rafael C. Carrasco", "title": "An open diachronic corpus of historical Spanish: annotation criteria and\n  automatic modernisation of spelling", "comments": "The part of this paper describing the IMPACT-es corpus has been\n  accepted for publication in the journal Language Resources and Evaluation\n  (http://link.springer.com/article/10.1007/s10579-013-9239-y)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IMPACT-es diachronic corpus of historical Spanish compiles over one\nhundred books --containing approximately 8 million words-- in addition to a\ncomplementary lexicon which links more than 10 thousand lemmas with\nattestations of the different variants found in the documents. This textual\ncorpus and the accompanying lexicon have been released under an open license\n(Creative Commons by-nc-sa) in order to permit their intensive exploitation in\nlinguistic research. Approximately 7% of the words in the corpus (a selection\naimed at enhancing the coverage of the most frequent word forms) have been\nannotated with their lemma, part of speech, and modern equivalent. This paper\ndescribes the annotation criteria followed and the standards, based on the Text\nEncoding Initiative recommendations, used to the represent the texts in digital\nform. As an illustration of the possible synergies between diachronic textual\nresources and linguistic research, we describe the application of statistical\nmachine translation techniques to infer probabilistic context-sensitive rules\nfor the automatic modernisation of spelling. The automatic modernisation with\nthis type of statistical methods leads to very low character error rates when\nthe output is compared with the supervised modern version of the text.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2013 18:19:15 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2013 09:59:13 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["S\u00e1nchez-Mart\u00ednez", "Felipe", ""], ["Mart\u00ednez-Sempere", "Isabel", ""], ["Ivars-Ribes", "Xavier", ""], ["Carrasco", "Rafael C.", ""]]}, {"id": "1306.3920", "submitter": "Diego Amancio Raphael", "authors": "Thiago C. Silva and Diego R. Amancio", "title": "Discriminating word senses with tourist walks in complex networks", "comments": null, "journal-ref": "Eur. Phys. J. B, 86 7 (2013) 297", "doi": "10.1140/epjb/e2013-40025-4", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patterns of topological arrangement are widely used for both animal and human\nbrains in the learning process. Nevertheless, automatic learning techniques\nfrequently overlook these patterns. In this paper, we apply a learning\ntechnique based on the structural organization of the data in the attribute\nspace to the problem of discriminating the senses of 10 polysemous words. Using\ntwo types of characterization of meanings, namely semantical and topological\napproaches, we have observed significative accuracy rates in identifying the\nsuitable meanings in both techniques. Most importantly, we have found that the\ncharacterization based on the deterministic tourist walk improves the\ndisambiguation process when one compares with the discrimination achieved with\ntraditional complex networks measurements such as assortativity and clustering\ncoefficient. To our knowledge, this is the first time that such deterministic\nwalk has been applied to such a kind of problem. Therefore, our finding\nsuggests that the tourist walk characterization may be useful in other related\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 16:25:51 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Silva", "Thiago C.", ""], ["Amancio", "Diego R.", ""]]}, {"id": "1306.4134", "submitter": "Suket Arora", "authors": "Suket Arora, Kamaljeet Batra, Sarabjit Singh", "title": "Dialogue System: A Brief Review", "comments": "4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Dialogue System is a system which interacts with human in natural language.\nAt present many universities are developing the dialogue system in their\nregional language. This paper will discuss about dialogue system, its\ncomponents, challenges and its evaluation. This paper helps the researchers for\ngetting info regarding dialogues system.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 10:32:43 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Arora", "Suket", ""], ["Batra", "Kamaljeet", ""], ["Singh", "Sarabjit", ""]]}, {"id": "1306.4139", "submitter": "Suket Arora", "authors": "Preeti Verma, Suket Arora, Kamaljit Batra", "title": "Punjabi Language Interface to Database: a brief review", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike most user-computer interfaces, a natural language interface allows\nusers to communicate fluently with a computer system with very little\npreparation. Databases are often hard to use in cooperating with the users\nbecause of their rigid interface. A good NLIDB allows a user to enter commands\nand ask questions in native language and then after interpreting respond to the\nuser in native language. For a large number of applications requiring\ninteraction between humans and the computer systems, it would be convenient to\nprovide the end-user friendly interface. Punjabi language interface to database\nwould proof fruitful to native people of Punjab, as it provides ease to them to\nuse various e-governance applications like Punjab Sewa, Suwidha, Online Public\nUtility Forms, Online Grievance Cell, Land Records Management System,legacy\nmatters, e-District, agriculture, etc. Punjabi is the mother tongue of more\nthan 110 million people all around the world. According to available\ninformation, Punjabi ranks 10th from top out of a total of 6,900 languages\nrecognized internationally by the United Nations. This paper covers a brief\noverview of the Natural language interface to database, its different\ncomponents, its advantages, disadvantages, approaches and techniques used. The\npaper ends with the work done on Punjabi language interface to database and\nfuture enhancements that can be done.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 11:03:36 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Verma", "Preeti", ""], ["Arora", "Suket", ""], ["Batra", "Kamaljit", ""]]}, {"id": "1306.4886", "submitter": "Luis Marujo", "authors": "Luis Marujo, Anatole Gershman, Jaime Carbonell, Robert Frederking,\n  Jo\\~ao P. Neto", "title": "Supervised Topical Key Phrase Extraction of News Stories using\n  Crowdsourcing, Light Filtering and Co-reference Normalization", "comments": "In 8th International Conference on Language Resources and Evaluation\n  (LREC 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and effective automated indexing is critical for search and personalized\nservices. Key phrases that consist of one or more words and represent the main\nconcepts of the document are often used for the purpose of indexing. In this\npaper, we investigate the use of additional semantic features and\npre-processing steps to improve automatic key phrase extraction. These features\ninclude the use of signal words and freebase categories. Some of these features\nlead to significant improvements in the accuracy of the results. We also\nexperimented with 2 forms of document pre-processing that we call light\nfiltering and co-reference normalization. Light filtering removes sentences\nfrom the document, which are judged peripheral to its main content.\nCo-reference normalization unifies several written forms of the same named\nentity into a unique form. We also needed a \"Gold Standard\" - a set of labeled\ndocuments for training and evaluation. While the subjective nature of key\nphrase selection precludes a true \"Gold Standard\", we used Amazon's Mechanical\nTurk service to obtain a useful approximation. Our data indicates that the\nbiggest improvements in performance were due to shallow semantic features, news\ncategories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion of\ndeeper semantic features such as Freebase sub-categories was not beneficial by\nitself, but in combination with pre-processing, did cause slight improvements\nin the nDCG scores.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 14:22:00 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Marujo", "Luis", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""], ["Frederking", "Robert", ""], ["Neto", "Jo\u00e3o P.", ""]]}, {"id": "1306.4890", "submitter": "Luis Marujo", "authors": "Luis Marujo, Ricardo Ribeiro, David Martins de Matos, Jo\\~ao P. Neto,\n  Anatole Gershman, and Jaime Carbonell", "title": "Key Phrase Extraction of Lightly Filtered Broadcast News", "comments": "In 15th International Conference on Text, Speech and Dialogue (TSD\n  2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the impact of light filtering on automatic key phrase\nextraction (AKE) applied to Broadcast News (BN). Key phrases are words and\nexpressions that best characterize the content of a document. Key phrases are\noften used to index the document or as features in further processing. This\nmakes improvements in AKE accuracy particularly important. We hypothesized that\nfiltering out marginally relevant sentences from a document would improve AKE\naccuracy. Our experiments confirmed this hypothesis. Elimination of as little\nas 10% of the document sentences lead to a 2% improvement in AKE precision and\nrecall. AKE is built over MAUI toolkit that follows a supervised learning\napproach. We trained and tested our AKE method on a gold standard made of 8 BN\nprograms containing 110 manually annotated news stories. The experiments were\nconducted within a Multimedia Monitoring Solution (MMS) system for TV and radio\nnews/programs, running daily, and monitoring 12 TV and 4 radio channels.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 14:35:22 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Marujo", "Luis", ""], ["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""], ["Neto", "Jo\u00e3o P.", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1306.4908", "submitter": "Luis Marujo", "authors": "Luis Marujo, Wang Ling, Anatole Gershman, Jaime Carbonell, Jo\\~ao P.\n  Neto, David Matos", "title": "Recognition of Named-Event Passages in News Articles", "comments": "In 25th International Conference on Computational Linguistics (COLING\n  2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the concept of Named Entities to Named Events - commonly occurring\nevents such as battles and earthquakes. We propose a method for finding\nspecific passages in news articles that contain information about such events\nand report our preliminary evaluation results. Collecting \"Gold Standard\" data\npresents many problems, both practical and conceptual. We present a method for\nobtaining such data using the Amazon Mechanical Turk service.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 15:22:21 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Marujo", "Luis", ""], ["Ling", "Wang", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""], ["Neto", "Jo\u00e3o P.", ""], ["Matos", "David", ""]]}, {"id": "1306.5170", "submitter": "Wafaa Tawfik abdel-moneim", "authors": "Wafaa Tawfik Abdel-moneim, Mohamed Hashem Abdel-Aziz, and Mohamed\n  Monier Hassan", "title": "Clinical Relationships Extraction Techniques from Patient Narratives", "comments": "15 pages 13 figures 7 tables", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol.10,\n  Issue 1, January 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Clinical E-Science Framework (CLEF) project was used to extract important\ninformation from medical texts by building a system for the purpose of clinical\nresearch, evidence-based healthcare and genotype-meets-phenotype informatics.\nThe system is divided into two parts, one part concerns with the identification\nof relationships between clinically important entities in the text. The full\nparses and domain-specific grammars had been used to apply many approaches to\nextract the relationship. In the second part of the system, statistical machine\nlearning (ML) approaches are applied to extract relationship. A corpus of\noncology narratives that hand annotated with clinical relationships can be used\nto train and test a system that has been designed and implemented by supervised\nmachine learning (ML) approaches. Many features can be extracted from these\ntexts that are used to build a model by the classifier. Multiple supervised\nmachine learning algorithms can be applied for relationship extraction. Effects\nof adding the features, changing the size of the corpus, and changing the type\nof the algorithm on relationship extraction are examined. Keywords: Text\nmining; information extraction; NLP; entities; and relations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 15:30:09 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Abdel-moneim", "Wafaa Tawfik", ""], ["Abdel-Aziz", "Mohamed Hashem", ""], ["Hassan", "Mohamed Monier", ""]]}, {"id": "1306.5263", "submitter": "Haonan Yu", "authors": "Haonan Yu, Jeffrey Mark Siskind", "title": "Discriminative Training: Learning to Describe Video with Sentences, from\n  Video Described with Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for learning word meanings from complex and realistic\nvideo clips by discriminatively training (DT) positive sentential labels\nagainst negative ones, and then use the trained word models to generate\nsentential descriptions for new video. This new work is inspired by recent work\nwhich adopts a maximum likelihood (ML) framework to address the same problem\nusing only positive sentential labels. The new method, like the ML-based one,\nis able to automatically determine which words in the sentence correspond to\nwhich concepts in the video (i.e., ground words to meanings) in a weakly\nsupervised fashion. While both DT and ML yield comparable results with\nsufficient training data, DT outperforms ML significantly with smaller training\nsets because it can exploit negative training labels to better constrain the\nlearning problem.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 22:09:36 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Yu", "Haonan", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1306.6078", "submitter": "Cristian Danescu-Niculescu-Mizil", "authors": "Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure\n  Leskovec, Christopher Potts", "title": "A Computational Approach to Politeness with Application to Social\n  Factors", "comments": "To appear at ACL 2013. 10pp, 3 fig. Data and other info available at\n  http://www.mpi-sws.org/~cristian/Politeness.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational framework for identifying linguistic aspects of\npoliteness. Our starting point is a new corpus of requests annotated for\npoliteness, which we use to evaluate aspects of politeness theory and to\nuncover new interactions between politeness markers and context. These findings\nguide our construction of a classifier with domain-independent lexical and\nsyntactic features operationalizing key components of politeness theory, such\nas indirection, deference, impersonalization and modality. Our classifier\nachieves close to human performance and is effective across domains. We use our\nframework to study the relationship between politeness and social power,\nshowing that polite Wikipedia editors are more likely to achieve high status\nthrough elections, but, once elevated, they become less polite. We see a\nsimilar negative correlation between politeness and power on Stack Exchange,\nwhere users at the top of the reputation scale are less polite than those at\nthe bottom. Finally, we apply our classifier to a preliminary analysis of\npoliteness variation by gender and community.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 20:00:09 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Danescu-Niculescu-Mizil", "Cristian", ""], ["Sudhof", "Moritz", ""], ["Jurafsky", "Dan", ""], ["Leskovec", "Jure", ""], ["Potts", "Christopher", ""]]}, {"id": "1306.6130", "submitter": "Robert Bishop Jr", "authors": "Robert Bishop Jr", "title": "Competency Tracking for English as a Second or Foreign Language Learners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  My system utilizes the outcomes feature found in Moodle and other learning\ncontent management systems (LCMSs) to keep track of where students are in terms\nof what language competencies they have mastered and the competencies they need\nto get where they want to go. These competencies are based on the Common\nEuropean Framework for (English) Language Learning. This data can be available\nfor everyone involved with a given student's progress (e.g. educators, parents,\nsupervisors and the students themselves). A given student's record of past\naccomplishments can also be meshed with those of his classmates. Not only are a\nstudent's competencies easily seen and tracked, educators can view competencies\nof a group of students that were achieved prior to enrollment in the class.\nThis should make curriculum decision making easier and more efficient for\neducators.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 05:02:26 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Bishop", "Robert", "Jr"]]}, {"id": "1306.6755", "submitter": "Kareem Darwish", "authors": "Kareem Darwish", "title": "Arabizi Detection and Conversion to Arabic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Arabizi is Arabic text that is written using Latin characters. Arabizi is\nused to present both Modern Standard Arabic (MSA) or Arabic dialects. It is\ncommonly used in informal settings such as social networking sites and is often\nwith mixed with English. In this paper we address the problems of: identifying\nArabizi in text and converting it to Arabic characters. We used word and\nsequence-level features to identify Arabizi that is mixed with English. We\nachieved an identification accuracy of 98.5%. As for conversion, we used\ntransliteration mining with language modeling to generate equivalent Arabic\ntext. We achieved 88.7% conversion accuracy, with roughly a third of errors\nbeing spelling and morphological variants of the forms in ground truth.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 08:46:11 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Darwish", "Kareem", ""]]}, {"id": "1306.6944", "submitter": "Ulf Sch\\\"oneberg", "authors": "Ulf Sch\\\"oneberg and Wolfram Sperber", "title": "The DeLiVerMATH project - Text analysis in mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A high-quality content analysis is essential for retrieval functionalities\nbut the manual extraction of key phrases and classification is expensive.\nNatural language processing provides a framework to automatize the process.\nHere, a machine-based approach for the content analysis of mathematical texts\nis described. A prototype for key phrase extraction and classification of\nmathematical texts is presented.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 15:48:06 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Sch\u00f6neberg", "Ulf", ""], ["Sperber", "Wolfram", ""]]}]