[{"id": "1207.0052", "submitter": "Jacob Andreas", "authors": "Jacob Andreas", "title": "The Complexity of Learning Principles and Parameters Grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We investigate models for learning the class of context-free and\ncontext-sensitive languages (CFLs and CSLs). We begin with a brief discussion\nof some early hardness results which show that unrestricted language learning\nis impossible, and unrestricted CFL learning is computationally infeasible; we\nthen briefly survey the literature on algorithms for learning restricted\nsubclasses of the CFLs. Finally, we introduce a new family of subclasses, the\nprincipled parametric context-free grammars (and a corresponding family of\nprincipled parametric context-sensitive grammars), which roughly model the\n\"Principles and Parameters\" framework in psycholinguistics. We present three\nhardness results: first, that the PPCFGs are not efficiently learnable given\nequivalence and membership oracles, second, that the PPCFGs are not efficiently\nlearnable from positive presentations unless P = NP, and third, that the PPCSGs\nare not efficiently learnable from positive presentations unless integer\nfactorization is in P.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 06:36:04 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2012 03:49:57 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2012 15:20:17 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Andreas", "Jacob", ""]]}, {"id": "1207.0245", "submitter": "Noah Smith", "authors": "Noah A. Smith", "title": "Adversarial Evaluation for Models of Natural Language", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We now have a rich and growing set of modeling tools and algorithms for\ninducing linguistic structure from text that is less than fully annotated. In\nthis paper, we discuss some of the weaknesses of our current methodology. We\npresent a new abstract framework for evaluating natural language processing\n(NLP) models in general and unsupervised NLP models in particular. The central\nidea is to make explicit certain adversarial roles among researchers, so that\nthe different roles in an evaluation are more clearly defined and performers of\nall roles are offered ways to make measurable contributions to the larger goal.\nAdopting this approach may help to characterize model successes and failures by\nencouraging earlier consideration of error analysis. The framework can be\ninstantiated in a variety of ways, simulating some familiar intrinsic and\nextrinsic evaluations as well as some new evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2012 21:13:05 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2012 06:41:31 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Smith", "Noah A.", ""]]}, {"id": "1207.0396", "submitter": "Peratham Wiriyathammabhum Mr.", "authors": "Peratham Wiriyathammabhum, Boonserm Kijsirikul, Hiroya Takamura,\n  Manabu Okumura", "title": "Applying Deep Belief Networks to Word Sense Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we applied a novel learning algorithm, namely, Deep Belief\nNetworks (DBN) to word sense disambiguation (WSD). DBN is a probabilistic\ngenerative model composed of multiple layers of hidden units. DBN uses\nRestricted Boltzmann Machine (RBM) to greedily train layer by layer as a\npretraining. Then, a separate fine tuning step is employed to improve the\ndiscriminative power. We compared DBN with various state-of-the-art supervised\nlearning algorithms in WSD such as Support Vector Machine (SVM), Maximum\nEntropy model (MaxEnt), Naive Bayes classifier (NB) and Kernel Principal\nComponent Analysis (KPCA). We used all words in the given paragraph,\nsurrounding context words and part-of-speech of surrounding words as our\nknowledge sources. We conducted our experiment on the SENSEVAL-2 data set. We\nobserved that DBN outperformed all other learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2012 14:19:21 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Wiriyathammabhum", "Peratham", ""], ["Kijsirikul", "Boonserm", ""], ["Takamura", "Hiroya", ""], ["Okumura", "Manabu", ""]]}, {"id": "1207.0658", "submitter": "Eduardo G. Altmann", "authors": "Eduardo G. Altmann, Giampaolo Cristadoro, and Mirko Degli Esposti", "title": "On the origin of long-range correlations in texts", "comments": "Full paper (8 pages) and Supporting Information (19 pages)", "journal-ref": "Proc. Natl. Acad. Sci. USA 109, 11582 (2012)", "doi": "10.1073/pnas.1117723109", "report-no": null, "categories": "physics.data-an cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of human interactions with social and natural phenomena is\nmirrored in the way we describe our experiences through natural language. In\norder to retain and convey such a high dimensional information, the statistical\nproperties of our linguistic output has to be highly correlated in time. An\nexample are the robust observations, still largely not understood, of\ncorrelations on arbitrary long scales in literary texts. In this paper we\nexplain how long-range correlations flow from highly structured linguistic\nlevels down to the building blocks of a text (words, letters, etc..). By\ncombining calculations and data analysis we show that correlations take form of\na bursty sequence of events once we approach the semantically relevant topics\nof the text. The mechanisms we identify are fairly general and can be equally\napplied to other hierarchical settings.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 12:57:43 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Altmann", "Eduardo G.", ""], ["Cristadoro", "Giampaolo", ""], ["Esposti", "Mirko Degli", ""]]}, {"id": "1207.0742", "submitter": "Marc Dymetman", "authors": "Marc Dymetman and Guillaume Bouchard and Simon Carter", "title": "The OS* Algorithm: a Joint Approach to Exact Optimization and Sampling", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current sampling algorithms for high-dimensional distributions are based\non MCMC techniques and are approximate in the sense that they are valid only\nasymptotically. Rejection sampling, on the other hand, produces valid samples,\nbut is unrealistically slow in high-dimension spaces. The OS* algorithm that we\npropose is a unified approach to exact optimization and sampling, based on\nincremental refinements of a functional upper bound, which combines ideas of\nadaptive rejection sampling and of A* optimization search. We show that the\nchoice of the refinement can be done in a way that ensures tractability in\nhigh-dimension spaces, and we present first experiments in two different\nsettings: inference in high-order HMMs and in large discrete graphical models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 16:35:48 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Dymetman", "Marc", ""], ["Bouchard", "Guillaume", ""], ["Carter", "Simon", ""]]}, {"id": "1207.1420", "submitter": "Luke S. Zettlemoyer", "authors": "Luke S. Zettlemoyer, Michael Collins", "title": "Learning to Map Sentences to Logical Form: Structured Classification\n  with Probabilistic Categorial Grammars", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-658-666", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of mapping natural language sentences to\nlambda-calculus encodings of their meaning. We describe a learning algorithm\nthat takes as input a training set of sentences labeled with expressions in the\nlambda calculus. The algorithm induces a grammar for the problem, along with a\nlog-linear model that represents a distribution over syntactic and semantic\nanalyses conditioned on the input sentence. We apply the method to the task of\nlearning natural language interfaces to databases and show that the learned\nparsers outperform previous methods in two benchmark database domains.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:27:56 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Zettlemoyer", "Luke S.", ""], ["Collins", "Michael", ""]]}, {"id": "1207.1847", "submitter": "Ted Dunning", "authors": "Ted Dunning", "title": "Finding Structure in Text, Genome and Other Symbolic Sequences", "comments": "~ 176 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical methods derived and described in this thesis provide new ways\nto elucidate the structural properties of text and other symbolic sequences.\nGenerically, these methods allow detection of a difference in the frequency of\na single feature, the detection of a difference between the frequencies of an\nensemble of features and the attribution of the source of a text. These three\nabstract tasks suffice to solve problems in a wide variety of settings.\nFurthermore, the techniques described in this thesis can be extended to provide\na wide range of additional tests beyond the ones described here.\n  A variety of applications for these methods are examined in detail. These\napplications are drawn from the area of text analysis and genetic sequence\nanalysis. The textually oriented tasks include finding interesting collocations\nand cooccurent phrases, language identification, and information retrieval. The\nbiologically oriented tasks include species identification and the discovery of\npreviously unreported long range structure in genes. In the applications\nreported here where direct comparison is possible, the performance of these new\nmethods substantially exceeds the state of the art.\n  Overall, the methods described here provide new and effective ways to analyse\ntext and other symbolic sequences. Their particular strength is that they deal\nwell with situations where relatively little data are available. Since these\nmethods are abstract in nature, they can be applied in novel situations with\nrelative ease.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2012 07:02:13 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Dunning", "Ted", ""]]}, {"id": "1207.2265", "submitter": "Daoud Clarke", "authors": "Daoud Clarke", "title": "Challenges for Distributional Compositional Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarises the current state-of-the art in the study of\ncompositionality in distributional semantics, and major challenges for this\narea. We single out generalised quantifiers and intensional semantics as areas\non which to focus attention for the development of the theory. Once suitable\ntheories have been developed, algorithms will be needed to apply the theory to\ntasks. Evaluation is a major problem; we single out application to recognising\ntextual entailment and machine translation for this purpose.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 08:48:13 GMT"}], "update_date": "2012-07-11", "authors_parsed": [["Clarke", "Daoud", ""]]}, {"id": "1207.2334", "submitter": "Reginald Smith", "authors": "Reginald D. Smith", "title": "Distinct word length frequencies: distributions and symbol entropies", "comments": "16 pages, 4 figures", "journal-ref": "Glottometrics 23, 2012, 7-22", "doi": null, "report-no": null, "categories": "cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of frequency counts of distinct words by length in a\nlanguage's vocabulary will be analyzed using two methods. The first, will look\nat the empirical distributions of several languages and derive a distribution\nthat reasonably explains the number of distinct words as a function of length.\nWe will be able to derive the frequency count, mean word length, and variance\nof word length based on the marginal probability of letters and spaces. The\nsecond, based on information theory, will demonstrate that the conditional\nentropies can also be used to estimate the frequency of distinct words of a\ngiven length in a language. In addition, it will be shown how these techniques\ncan also be applied to estimate higher order entropies using vocabulary word\nlength.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 13:00:19 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2012 02:12:11 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Smith", "Reginald D.", ""]]}, {"id": "1207.2714", "submitter": "Mohamed Achraf  Ben Mohamed Mr.", "authors": "Mohamed Achraf Ben Mohamed, Mounir Zrigui and Mohsen Maraoui", "title": "Clustering based approach extracting collocations", "comments": null, "journal-ref": "4th International Conference on Arabic Language Processing. CITALA\n  2012. May 2nd -- 3rd 2012, Rabat, Morocco", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following study presents a collocation extraction approach based on\nclustering technique. This study uses a combination of several classical\nmeasures which cover all aspects of a given corpus then it suggests separating\nbigrams found in the corpus in several disjoint groups according to the\nprobability of presence of collocations. This will allow excluding groups where\nthe presence of collocations is very unlikely and thus reducing in a meaningful\nway the search space.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 17:31:11 GMT"}], "update_date": "2012-07-12", "authors_parsed": [["Mohamed", "Mohamed Achraf Ben", ""], ["Zrigui", "Mounir", ""], ["Maraoui", "Mohsen", ""]]}, {"id": "1207.3169", "submitter": "Ramon Ferrer i Cancho", "authors": "Stuart Semple, Minna J. Hsu, Govindasamy Agoramoorthy, Ramon\n  Ferrer-i-Cancho", "title": "The law of brevity in macaque vocal communication is not an artifact of\n  analyzing mean call durations", "comments": "Little improvements of the statistical arguments", "journal-ref": "Journal of Quantitative Linguistics, 20 (3), 209-217 (2013)", "doi": "10.1080/09296174.2013.799917", "report-no": null, "categories": "q-bio.NC cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Words follow the law of brevity, i.e. more frequent words tend to be shorter.\nFrom a statistical point of view, this qualitative definition of the law states\nthat word length and word frequency are negatively correlated. Here the recent\nfinding of patterning consistent with the law of brevity in Formosan macaque\nvocal communication (Semple et al., 2010) is revisited. It is shown that the\nnegative correlation between mean duration and frequency of use in the\nvocalizations of Formosan macaques is not an artifact of the use of a mean\nduration for each call type instead of the customary 'word' length of studies\nof the law in human language. The key point demonstrated is that the total\nduration of calls of a particular type increases with the number of calls of\nthat type. The finding of the law of brevity in the vocalizations of these\nmacaques therefore defies a trivial explanation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2012 08:43:17 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2012 07:50:47 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Semple", "Stuart", ""], ["Hsu", "Minna J.", ""], ["Agoramoorthy", "Govindasamy", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1207.3932", "submitter": "Kishorjit Nongmeikapam Mr.", "authors": "Kishorjit Nongmeikapam, Vidya Raj RK, Oinam Imocha Singh and Sivaji\n  Bandyopadhyay", "title": "Automatic Segmentation of Manipuri (Meiteilon) Word into Syllabic Units", "comments": "12 Pages, 5 Tables See the link\n  http://airccse.org/journal/jcsit/0612csit11.pdf", "journal-ref": null, "doi": "10.5121/ijcsit.2012.4311", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work of automatic segmentation of a Manipuri language (or Meiteilon) word\ninto syllabic units is demonstrated in this paper. This language is a scheduled\nIndian language of Tibeto-Burman origin, which is also a very highly\nagglutinative language. This language usages two script: a Bengali script and\nMeitei Mayek (Script). The present work is based on the second script. An\nalgorithm is designed so as to identify mainly the syllables of Manipuri origin\nword. The result of the algorithm shows a Recall of 74.77, Precision of 91.21\nand F-Score of 82.18 which is a reasonable score with the first attempt of such\nkind for this language.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2012 10:14:24 GMT"}], "update_date": "2012-07-18", "authors_parsed": [["Nongmeikapam", "Kishorjit", ""], ["RK", "Vidya Raj", ""], ["Singh", "Oinam Imocha", ""], ["Bandyopadhyay", "Sivaji", ""]]}, {"id": "1207.4307", "submitter": "David Martins de Matos", "authors": "Artur Ventura, Nuno Diegues, David Martins de Matos", "title": "Frame Interpretation and Validation in a Open Domain Dialogue System", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal in this paper is to establish a means for a dialogue platform to be\nable to cope with open domains considering the possible interaction between the\nembodied agent and humans. To this end we present an algorithm capable of\nprocessing natural language utterances and validate them against knowledge\nstructures of an intelligent agent's mind. Our algorithm leverages dialogue\ntechniques in order to solve ambiguities and acquire knowledge about unknown\nentities.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 09:09:35 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Ventura", "Artur", ""], ["Diegues", "Nuno", ""], ["de Matos", "David Martins", ""]]}, {"id": "1207.4625", "submitter": "Eric Laporte", "authors": "E. Laporte (LIGM)", "title": "Appropriate Nouns with Obligatory Modifiers", "comments": null, "journal-ref": "Language research, Seoul National University 31, 2 (1995) 251-289", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of appropriate sequence as introduced by Z. Harris provides a\npowerful syntactic way of analysing the detailed meaning of various sentences,\nincluding ambiguous ones. In an adjectival sentence like 'The leather was\nyellow', the introduction of an appropriate noun, here 'colour', specifies\nwhich quality the adjective describes. In some other adjectival sentences with\nan appropriate noun, that noun plays the same part as 'colour' and seems to be\nrelevant to the description of the adjective. These appropriate nouns can\nusually be used in elementary sentences like 'The leather had some colour', but\nin many cases they have a more or less obligatory modifier. For example, you\ncan hardly mention that an object has a colour without qualifying that colour\nat all. About 300 French nouns are appropriate in at least one adjectival\nsentence and have an obligatory modifier. They enter in a number of sentence\nstructures related by several syntactic transformations. The appropriateness of\nthe noun and the fact that the modifier is obligatory are reflected in these\ntransformations. The description of these syntactic phenomena provides a basis\nfor a classification of these nouns. It also concerns the lexical properties of\nthousands of predicative adjectives, and in particular the relations between\nthe sentence without the noun : 'The leather was yellow' and the adjectival\nsentence with the noun : 'The colour of the leather was yellow'.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 12:04:26 GMT"}], "update_date": "2012-07-20", "authors_parsed": [["Laporte", "E.", "", "LIGM"]]}, {"id": "1207.5328", "submitter": "Laurent Romary", "authors": "Kais Haddar (MIRACL), H\\'ela Fehri (MIRACL), Laurent Romary (IDSL,\n  INRIA Saclay - Ile de France)", "title": "A prototype for projecting HPSG syntactic lexica towards LMF", "comments": null, "journal-ref": "Journal of Language Technology and Computational Linguistics 27, 1\n  (2012) 21-46", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The comparative evaluation of Arabic HPSG grammar lexica requires a deep\nstudy of their linguistic coverage. The complexity of this task results mainly\nfrom the heterogeneity of the descriptive components within those lexica\n(underlying linguistic resources and different data categories, for example).\nIt is therefore essential to define more homogeneous representations, which in\nturn will enable us to compare them and eventually merge them. In this context,\nwe present a method for comparing HPSG lexica based on a rule system. This\nmethod is implemented within a prototype for the projection from Arabic HPSG to\na normalised pivot language compliant with LMF (ISO 24613 - Lexical Markup\nFramework) and serialised using a TEI (Text Encoding Initiative) based\nrepresentation. The design of this system is based on an initial study of the\nHPSG formalism looking at its adequacy for the representation of Arabic, and\nfrom this, we identify the appropriate feature structures corresponding to each\nArabic lexical category and their possible LMF counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 09:02:18 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2012 08:34:01 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2012 06:09:08 GMT"}], "update_date": "2012-09-03", "authors_parsed": [["Haddar", "Kais", "", "MIRACL"], ["Fehri", "H\u00e9la", "", "MIRACL"], ["Romary", "Laurent", "", "IDSL,\n  INRIA Saclay - Ile de France"]]}, {"id": "1207.5409", "submitter": "Deepak Kumar", "authors": "Deepak Kumar, Manjeet Singh, Seema Shukla", "title": "FST Based Morphological Analyzer for Hindi Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hindi being a highly inflectional language, FST (Finite State Transducer)\nbased approach is most efficient for developing a morphological analyzer for\nthis language. The work presented in this paper uses the SFST (Stuttgart Finite\nState Transducer) tool for generating the FST. A lexicon of root words is\ncreated. Rules are then added for generating inflectional and derivational\nwords from these root words. The Morph Analyzer developed was used in a Part Of\nSpeech (POS) Tagger based on Stanford POS Tagger. The system was first trained\nusing a manually tagged corpus and MAXENT (Maximum Entropy) approach of\nStanford POS tagger was then used for tagging input sentences. The\nmorphological analyzer gives approximately 97% correct results. POS tagger\ngives an accuracy of approximately 87% for the sentences that have the words\nknown to the trained model file, and 80% accuracy for the sentences that have\nthe words unknown to the trained model file.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 14:24:13 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Kumar", "Deepak", ""], ["Singh", "Manjeet", ""], ["Shukla", "Seema", ""]]}]