[{"id": "1311.0833", "submitter": "Zitao Liu", "authors": "Zitao Liu", "title": "A Comparative Study on Linguistic Feature Selection in Sentiment\n  Polarity Classification", "comments": "arXiv admin note: text overlap with arXiv:cs/0205070 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment polarity classification is perhaps the most widely studied topic.\nIt classifies an opinionated document as expressing a positive or negative\nopinion. In this paper, using movie review dataset, we perform a comparative\nstudy with different single kind linguistic features and the combinations of\nthese features. We find that the classic topic-based classifier(Naive Bayes and\nSupport Vector Machine) do not perform as well on sentiment polarity\nclassification. And we find that with some combination of different linguistic\nfeatures, the classification accuracy can be boosted a lot. We give some\nreasonable explanations about these boosting outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 20:11:35 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Liu", "Zitao", ""]]}, {"id": "1311.1169", "submitter": "D\\'aniel Kondor Mr", "authors": "D\\'aniel Kondor, Istv\\'an Csabai, L\\'aszl\\'o Dobos, J\\'anos Sz\\\"ule,\n  Norbert Barankai, Tam\\'as Hanyecz, Tam\\'as Seb\\H{o}k, Zs\\'ofia Kallus,\n  G\\'abor Vattay", "title": "Using Robust PCA to estimate regional characteristics of language use\n  from geo-tagged Twitter messages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) and related techniques have been\nsuccessfully employed in natural language processing. Text mining applications\nin the age of the online social media (OSM) face new challenges due to\nproperties specific to these use cases (e.g. spelling issues specific to texts\nposted by users, the presence of spammers and bots, service announcements,\netc.). In this paper, we employ a Robust PCA technique to separate typical\noutliers and highly localized topics from the low-dimensional structure present\nin language use in online social networks. Our focus is on identifying\ngeospatial features among the messages posted by the users of the Twitter\nmicroblogging service. Using a dataset which consists of over 200 million\ngeolocated tweets collected over the course of a year, we investigate whether\nthe information present in word usage frequencies can be used to identify\nregional features of language use and topics of interest. Using the PCA pursuit\nmethod, we are able to identify important low-dimensional features, which\nconstitute smoothly varying functions of the geographic location.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 19:31:33 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Kondor", "D\u00e1niel", ""], ["Csabai", "Istv\u00e1n", ""], ["Dobos", "L\u00e1szl\u00f3", ""], ["Sz\u00fcle", "J\u00e1nos", ""], ["Barankai", "Norbert", ""], ["Hanyecz", "Tam\u00e1s", ""], ["Seb\u0151k", "Tam\u00e1s", ""], ["Kallus", "Zs\u00f3fia", ""], ["Vattay", "G\u00e1bor", ""]]}, {"id": "1311.1194", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad, Svetlana Kiritchenko, and Joel Martin", "title": "Identifying Purpose Behind Electoral Tweets", "comments": null, "journal-ref": "In Proceedings of the KDD Workshop on Issues of Sentiment\n  Discovery and Opinion Mining (WISDOM-2013), August 2013, Chicago, USA", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweets pertaining to a single event, such as a national election, can number\nin the hundreds of millions. Automatically analyzing them is beneficial in many\ndownstream natural language applications such as question answering and\nsummarization. In this paper, we propose a new task: identifying the purpose\nbehind electoral tweets--why do people post election-oriented tweets? We show\nthat identifying purpose is correlated with the related phenomenon of sentiment\nand emotion detection, but yet significantly different. Detecting purpose has a\nnumber of applications including detecting the mood of the electorate,\nestimating the popularity of policies, identifying key issues of contention,\nand predicting the course of events. We create a large dataset of electoral\ntweets and annotate a few thousand tweets for purpose. We develop a system that\nautomatically classifies electoral tweets as per their purpose, obtaining an\naccuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-class\ntask (both accuracies well above the most-frequent-class baseline). Finally, we\nshow that resources developed for emotion detection are also helpful for\ndetecting purpose.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 20:55:23 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Mohammad", "Saif M.", ""], ["Kiritchenko", "Svetlana", ""], ["Martin", "Joel", ""]]}, {"id": "1311.1539", "submitter": "Edward Grefenstette", "authors": "Edward Grefenstette", "title": "Category-Theoretic Quantitative Compositional Distributional Models of\n  Natural Language Semantics", "comments": "DPhil Thesis, University of Oxford, Submitted and accepted in 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG math.CT math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis is about the problem of compositionality in distributional\nsemantics. Distributional semantics presupposes that the meanings of words are\na function of their occurrences in textual contexts. It models words as\ndistributions over these contexts and represents them as vectors in high\ndimensional spaces. The problem of compositionality for such models concerns\nitself with how to produce representations for larger units of text by\ncomposing the representations of smaller units of text.\n  This thesis focuses on a particular approach to this compositionality\nproblem, namely using the categorical framework developed by Coecke, Sadrzadeh,\nand Clark, which combines syntactic analysis formalisms with distributional\nsemantic representations of meaning to produce syntactically motivated\ncomposition operations. This thesis shows how this approach can be\ntheoretically extended and practically implemented to produce concrete\ncompositional distributional models of natural language semantics. It\nfurthermore demonstrates that such models can perform on par with, or better\nthan, other competing approaches in the field of natural language processing.\n  There are three principal contributions to computational linguistics in this\nthesis. The first is to extend the DisCoCat framework on the syntactic front\nand semantic front, incorporating a number of syntactic analysis formalisms and\nproviding learning procedures allowing for the generation of concrete\ncompositional distributional models. The second contribution is to evaluate the\nmodels developed from the procedures presented here, showing that they\noutperform other compositional distributional models present in the literature.\nThe third contribution is to show how using category theory to solve linguistic\nproblems forms a sound basis for research, illustrated by examples of work on\nthis topic, that also suggest directions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 22:06:15 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Grefenstette", "Edward", ""]]}, {"id": "1311.1897", "submitter": "Christian Retore", "authors": "Christian Retor\\'e (LaBRI, INRIA Bordeaux - Sud-Ouest)", "title": "Logique math\\'ematique et linguistique formelle", "comments": "Transcription d'une \"le\\c{c}on de math\\'ematique d'aujourd'hui\"\n  donn\\'ee le 7 juillet 2011, in French", "journal-ref": "Le\\c{c}ons de math\\'ematiques d'aujourd'hui, G\\'eraud\n  S\\'enizergues (Ed.) (2013) 24", "doi": null, "report-no": null, "categories": "math.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the etymology of the word shows, logic is intimately related to language,\nas exemplified by the work of philosophers from Antiquity and from the\nMiddle-Age. At the beginning of the XX century, the crisis of the foundations\nof mathematics invented mathematical logic and imposed logic as a\nlanguage-based foundation for mathematics. How did the relations between logic\nand language evolved in this newly defined mathematical framework? After a\nsurvey of the history of the relation between logic and linguistics,\ntraditionally focused on semantics, we focus on some present issues: 1) grammar\nas a deductive system 2) the transformation of the syntactic structure of a\nsentence to a logical formula representing its meaning 3) taking into account\nthe context when interpreting words. This lecture shows that type theory\nprovides a convenient framework both for natural language syntax and for the\ninterpretation of any of tis level (words, sentences, discourse).\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 08:24:08 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Retor\u00e9", "Christian", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"]]}, {"id": "1311.2252", "submitter": "David Yanay", "authors": "Ran El-Yaniv and David Yanay", "title": "Semantic Sort: A Supervised Approach to Personalized Semantic\n  Relatedness", "comments": "37 pages, 8 figures A short version of this paper was already\n  published at ECML/PKDD 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a novel supervised approach to learning statistical\nsemantic relatedness models from subjectively annotated training examples. The\nproposed semantic model consists of parameterized co-occurrence statistics\nassociated with textual units of a large background knowledge corpus. We\npresent an efficient algorithm for learning such semantic models from a\ntraining sample of relatedness preferences. Our method is corpus independent\nand can essentially rely on any sufficiently large (unstructured) collection of\ncoherent texts. Moreover, the approach facilitates the fitting of semantic\nmodels for specific users or groups of users. We present the results of\nextensive range of experiments from small to large scale, indicating that the\nproposed method is effective and competitive with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2013 09:15:16 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["El-Yaniv", "Ran", ""], ["Yanay", "David", ""]]}, {"id": "1311.2702", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn, Alexandre Bergel", "title": "Verifiable Source Code Documentation in Controlled Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing documentation about software internals is rarely considered a\nrewarding activity. It is highly time-consuming and the resulting documentation\nis fragile when the software is continuously evolving in a multi-developer\nsetting. Unfortunately, traditional programming environments poorly support the\nwriting and maintenance of documentation. Consequences are severe as the lack\nof documentation on software structure negatively impacts the overall quality\nof the software product. We show that using a controlled natural language with\na reasoner and a query engine is a viable technique for verifying the\nconsistency and accuracy of documentation and source code. Using ACE, a\nstate-of-the-art controlled natural language, we present positive results on\nthe comprehensibility and the general feasibility of creating and verifying\ndocumentation. As a case study, we used automatic documentation verification to\nidentify and fix severe flaws in the architecture of a non-trivial piece of\nsoftware. Moreover, a user experiment shows that our language is faster and\neasier to learn and understand than other formal languages for software\ndocumentation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 07:44:10 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Kuhn", "Tobias", ""], ["Bergel", "Alexandre", ""]]}, {"id": "1311.2978", "submitter": "Shibamouli Lahiri", "authors": "Shibamouli Lahiri, Rada Mihalcea", "title": "Authorship Attribution Using Word Network Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore a set of novel features for authorship attribution\nof documents. These features are derived from a word network representation of\nnatural language text. As has been noted in previous studies, natural language\ntends to show complex network structure at word level, with low degrees of\nseparation and scale-free (power law) degree distribution. There has also been\nwork on authorship attribution that incorporates ideas from complex networks.\nThe goal of our paper is to explore properties of these complex networks that\nare suitable as features for machine-learning-based authorship attribution of\ndocuments. We performed experiments on three different datasets, and obtained\npromising results.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 23:11:40 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Lahiri", "Shibamouli", ""], ["Mihalcea", "Rada", ""]]}, {"id": "1311.3011", "submitter": "Yoav Artzi", "authors": "Yoav Artzi", "title": "Cornell SPF: Cornell Semantic Parsing Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cornell Semantic Parsing Framework (SPF) is a learning and inference\nframework for mapping natural language to formal representation of its meaning.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 03:58:38 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 22:26:41 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Artzi", "Yoav", ""]]}, {"id": "1311.3175", "submitter": "Athira P M", "authors": "Athira P. M., Sreeja M. and P. C. Reghu Raj", "title": "Architecture of an Ontology-Based Domain-Specific Natural Language\n  Question Answering System", "comments": null, "journal-ref": "International Journal of Web & Semantic Technology (IJWesT) Vol.4,\n  No.4, October 2013", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) system aims at retrieving precise information from a\nlarge collection of documents against a query. This paper describes the\narchitecture of a Natural Language Question Answering (NLQA) system for a\nspecific domain based on the ontological information, a step towards semantic\nweb question answering. The proposed architecture defines four basic modules\nsuitable for enhancing current QA capabilities with the ability of processing\ncomplex questions. The first module was the question processing, which analyses\nand classifies the question and also reformulates the user query. The second\nmodule allows the process of retrieving the relevant documents. The next module\nprocesses the retrieved documents, and the last module performs the extraction\nand generation of a response. Natural language processing techniques are used\nfor processing the question and documents and also for answer extraction.\nOntology and domain knowledge are used for reformulating queries and\nidentifying the relations. The aim of the system is to generate short and\nspecific answer to the question that is asked in the natural language in a\nspecific domain. We have achieved 94 % accuracy of natural language question\nanswering in our implementation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 15:36:12 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["M.", "Athira P.", ""], ["M.", "Sreeja", ""], ["Raj", "P. C. Reghu", ""]]}, {"id": "1311.3961", "submitter": "Nisheeth Joshi", "authors": "Nisheeth Joshi, Iti Mathur, Hemant Darbari and Ajai Kumar", "title": "HEVAL: Yet Another Human Evaluation Metric", "comments": null, "journal-ref": "International Journal on Natural Language Computing Vol. 2, No.5,\n  November 2013", "doi": "10.5121/ijnlc.2013.2502", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation evaluation is a very important activity in machine\ntranslation development. Automatic evaluation metrics proposed in literature\nare inadequate as they require one or more human reference translations to\ncompare them with output produced by machine translation. This does not always\ngive accurate results as a text can have several different translations. Human\nevaluation metrics, on the other hand, lacks inter-annotator agreement and\nrepeatability. In this paper we have proposed a new human evaluation metric\nwhich addresses these issues. Moreover this metric also provides solid grounds\nfor making sound assumptions on the quality of the text produced by a machine\ntranslation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 19:45:25 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""], ["Darbari", "Hemant", ""], ["Kumar", "Ajai", ""]]}, {"id": "1311.3987", "submitter": "Seyed-Mehdi-Reza Beheshti", "authors": "Seyed-Mehdi-Reza Beheshti and Srikumar Venugopal and Seung Hwan Ryu\n  and Boualem Benatallah and Wei Wang", "title": "Big Data and Cross-Document Coreference Resolution: Current State and\n  Future Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Extraction (IE) is the task of automatically extracting\nstructured information from unstructured/semi-structured machine-readable\ndocuments. Among various IE tasks, extracting actionable intelligence from\never-increasing amount of data depends critically upon Cross-Document\nCoreference Resolution (CDCR) - the task of identifying entity mentions across\nmultiple documents that refer to the same underlying entity. Recently, document\ndatasets of the order of peta-/tera-bytes has raised many challenges for\nperforming effective CDCR such as scaling to large numbers of mentions and\nlimited representational power. The problem of analysing such datasets is\ncalled \"big data\". The aim of this paper is to provide readers with an\nunderstanding of the central concepts, subtasks, and the current\nstate-of-the-art in CDCR process. We provide assessment of existing\ntools/techniques for CDCR subtasks and highlight big data challenges in each of\nthem to help readers identify important and outstanding issues for further\ninvestigation. Finally, we provide concluding remarks and discuss possible\ndirections for future work.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 06:10:15 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Beheshti", "Seyed-Mehdi-Reza", ""], ["Venugopal", "Srikumar", ""], ["Ryu", "Seung Hwan", ""], ["Benatallah", "Boualem", ""], ["Wang", "Wei", ""]]}, {"id": "1311.5401", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne", "title": "Clustering and Relational Ambiguity: from Text Data to Natural Data", "comments": null, "journal-ref": "Journal of Data Mining & Digital Humanities, 2014 (June 24, 2014)\n  jdmdh:13", "doi": "10.46298/jdmdh.4", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Text data is often seen as \"take-away\" materials with little noise and easy\nto process information. Main questions are how to get data and transform them\ninto a good document format. But data can be sensitive to noise oftenly called\nambiguities. Ambiguities are aware from a long time, mainly because polysemy is\nobvious in language and context is required to remove uncertainty. I claim in\nthis paper that syntactic context is not suffisant to improve interpretation.\nIn this paper I try to explain that firstly noise can come from natural data\nthemselves, even involving high technology, secondly texts, seen as verified\nbut meaningless, can spoil content of a corpus; it may lead to contradictions\nand background noise.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 13:47:21 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 17:49:51 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Turenne", "Nicolas", ""]]}, {"id": "1311.5427", "submitter": "Gerardo Febres", "authors": "Gerardo Febres, Klaus Jaffe, Carlos Gershenson", "title": "Complexity measurement of natural and artificial languages", "comments": "29 pages, 11 figures, 3 tables, 2 appendixes", "journal-ref": "Complexity 20 6 429- (2015)", "doi": "10.1002/cplx.21529", "report-no": null, "categories": "cs.CL cs.IT math.IT nlin.AO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compared entropy for texts written in natural languages (English, Spanish)\nand artificial languages (computer software) based on a simple expression for\nthe entropy as a function of message length and specific word diversity. Code\ntext written in artificial languages showed higher entropy than text of similar\nlength expressed in natural languages. Spanish texts exhibit more symbolic\ndiversity than English ones. Results showed that algorithms based on complexity\nmeasures differentiate artificial from natural languages, and that text\nanalysis based on complexity measures allows the unveiling of important aspects\nof their nature. We propose specific expressions to examine entropy related\naspects of tests and estimate the values of entropy, emergence,\nself-organization and complexity based on specific diversity and message\nlength.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 02:43:22 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 06:02:18 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Febres", "Gerardo", ""], ["Jaffe", "Klaus", ""], ["Gershenson", "Carlos", ""]]}, {"id": "1311.5836", "submitter": "Nisheeth Joshi", "authors": "Pooja Gupta, Nisheeth Joshi, Iti Mathur", "title": "Automatic Ranking of MT Outputs using Approximations", "comments": null, "journal-ref": "International Journal of Computer Applications 81(17):27-31,\n  November 2013", "doi": "10.5120/14217-2463", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since long, research on machine translation has been ongoing. Still, we do\nnot get good translations from MT engines so developed. Manual ranking of these\noutputs tends to be very time consuming and expensive. Identifying which one is\nbetter or worse than the others is a very taxing task. In this paper, we show\nan approach which can provide automatic ranks to MT outputs (translations)\ntaken from different MT Engines and which is based on N-gram approximations. We\nprovide a solution where no human intervention is required for ranking systems.\nFurther we also show the evaluations of our results which show equivalent\nresults as that of human ranking.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 18:13:06 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Gupta", "Pooja", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1311.6045", "submitter": "Nidhal El-Abbadi", "authors": "Nidhal El-Abbadi, Ahmed Nidhal Khdhair, Adel Al-Nasrawi", "title": "Build Electronic Arabic Lexicon", "comments": "4 pages", "journal-ref": "The International Arab Journal of Information Technology, Vol. 8,\n  No. 2, April 2011", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many known Arabic lexicons organized on different ways, each of\nthem has a different number of Arabic words according to its organization way.\nThis paper has used mathematical relations to count a number of Arabic words,\nwhich proofs the number of Arabic words presented by Al Farahidy. The paper\nalso presents new way to build an electronic Arabic lexicon by using a hash\nfunction that converts each word (as input) to correspond a unique integer\nnumber (as output), these integer numbers will be used as an index to a lexicon\nentry.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 20:10:24 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["El-Abbadi", "Nidhal", ""], ["Khdhair", "Ahmed Nidhal", ""], ["Al-Nasrawi", "Adel", ""]]}, {"id": "1311.6063", "submitter": "Sheng Yu", "authors": "Sheng Yu, Tianrun Cai and Tianxi Cai", "title": "NILE: Fast Natural Language Processing for Electronic Health Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Narrative text in Electronic health records (EHR) contain rich\ninformation for medical and data science studies. This paper introduces the\ndesign and performance of Narrative Information Linear Extraction (NILE), a\nnatural language processing (NLP) package for EHR analysis that we share with\nthe medical informatics community. Methods: NILE uses a modified prefix-tree\nsearch algorithm for named entity recognition, which can detect prefix and\nsuffix sharing. The semantic analyses are implemented as rule-based finite\nstate machines. Analyses include negation, location, modification, family\nhistory, and ignoring. Result: The processing speed of NILE is hundreds to\nthousands times faster than existing NLP software for medical text. The\naccuracy of presence analysis of NILE is on par with the best performing models\non the 2010 i2b2/VA NLP challenge data. Conclusion: The speed, accuracy, and\nbeing able to operate via API make NILE a valuable addition to the NLP software\nfor medical informatics and data science.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 22:39:52 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2014 23:21:43 GMT"}, {"version": "v3", "created": "Wed, 2 Apr 2014 03:34:41 GMT"}, {"version": "v4", "created": "Mon, 13 Oct 2014 20:43:09 GMT"}, {"version": "v5", "created": "Tue, 16 Jul 2019 14:12:22 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Yu", "Sheng", ""], ["Cai", "Tianrun", ""], ["Cai", "Tianxi", ""]]}, {"id": "1311.6421", "submitter": "Daniel Gildea", "authors": "Pierluigi Crescenzi, Daniel Gildea, Andrea Marino, Gianluca Rossi,\n  Giorgio Satta", "title": "Synchronous Context-Free Grammars and Optimal Linear Parsing Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronous Context-Free Grammars (SCFGs), also known as syntax-directed\ntranslation schemata, are unlike context-free grammars in that they do not have\na binary normal form. In general, parsing with SCFGs takes space and time\npolynomial in the length of the input strings, but with the degree of the\npolynomial depending on the permutations of the SCFG rules. We consider linear\nparsing strategies, which add one nonterminal at a time. We show that for a\ngiven input permutation, the problems of finding the linear parsing strategy\nwith the minimum space and time complexity are both NP-hard.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 19:48:30 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Crescenzi", "Pierluigi", ""], ["Gildea", "Daniel", ""], ["Marino", "Andrea", ""], ["Rossi", "Gianluca", ""], ["Satta", "Giorgio", ""]]}]