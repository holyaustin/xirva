[{"id": "0809.0103", "submitter": "Dmitrii Manin", "authors": "Dmitrii Y. Manin", "title": "On the nature of long-range letter correlations in texts", "comments": "14 pages, 5 figures, unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The origin of long-range letter correlations in natural texts is studied\nusing random walk analysis and Jensen-Shannon divergence. It is concluded that\nthey result from slow variations in letter frequency distribution, which are a\nconsequence of slow variations in lexical composition within the text. These\ncorrelations are preserved by random letter shuffling within a moving window.\nAs such, they do reflect structural properties of the text, but in a very\nindirect manner.\n", "versions": [{"version": "v1", "created": "Sun, 31 Aug 2008 06:08:15 GMT"}], "update_date": "2016-11-27", "authors_parsed": [["Manin", "Dmitrii Y.", ""]]}, {"id": "0809.0124", "submitter": "Peter Turney", "authors": "Peter D. Turney (National Research Council of Canada)", "title": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations", "comments": "related work available at http://purl.org/peter.turney/", "journal-ref": "Proceedings of the 22nd International Conference on Computational\n  Linguistics (Coling 2008), August 2008, Manchester, UK, Pages 905-912", "doi": null, "report-no": "NRC 50398", "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing analogies, synonyms, antonyms, and associations appear to be four\ndistinct tasks, requiring distinct NLP algorithms. In the past, the four tasks\nhave been treated independently, using a wide variety of algorithms. These four\nsemantic classes, however, are a tiny sample of the full range of semantic\nphenomena, and we cannot afford to create ad hoc algorithms for each semantic\nphenomenon; we need to seek a unified approach. We propose to subsume a broad\nrange of phenomena under analogies. To limit the scope of this paper, we\nrestrict our attention to the subsumption of synonyms, antonyms, and\nassociations. We introduce a supervised corpus-based machine learning algorithm\nfor classifying analogous word pairs, and we show that it can solve\nmultiple-choice SAT analogy questions, TOEFL synonym questions, ESL\nsynonym-antonym questions, and similar-associated-both questions from cognitive\npsychology.\n", "versions": [{"version": "v1", "created": "Sun, 31 Aug 2008 14:00:26 GMT"}], "update_date": "2008-09-02", "authors_parsed": [["Turney", "Peter D.", "", "National Research Council of Canada"]]}, {"id": "0809.0360", "submitter": "Aniello Murano", "authors": "Piero A. Bonatti, Carsten Lutz, Aniello Murano, Moshe Y. Vardi", "title": "The Complexity of Enriched Mu-Calculi", "comments": "A preliminary version of this paper appears in the Proceedings of the\n  33rd International Colloquium on Automata, Languages and Programming (ICALP),\n  2006. This paper has been selected for a special issue in LMCS", "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 3 (September\n  22, 2008) lmcs:993", "doi": "10.2168/LMCS-4(3:11)2008", "report-no": null, "categories": "cs.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fully enriched &mu;-calculus is the extension of the propositional\n&mu;-calculus with inverse programs, graded modalities, and nominals. While\nsatisfiability in several expressive fragments of the fully enriched\n&mu;-calculus is known to be decidable and ExpTime-complete, it has recently\nbeen proved that the full calculus is undecidable. In this paper, we study the\nfragments of the fully enriched &mu;-calculus that are obtained by dropping at\nleast one of the additional constructs. We show that, in all fragments obtained\nin this way, satisfiability is decidable and ExpTime-complete. Thus, we\nidentify a family of decidable logics that are maximal (and incomparable) in\nexpressive power. Our results are obtained by introducing two new automata\nmodels, showing that their emptiness problems are ExpTime-complete, and then\nreducing satisfiability in the relevant logics to these problems. The automata\nmodels we introduce are two-way graded alternating parity automata over\ninfinite trees (2GAPTs) and fully enriched automata (FEAs) over infinite\nforests. The former are a common generalization of two incomparable automata\nmodels from the literature. The latter extend alternating automata in a similar\nway as the fully enriched &mu;-calculus extends the standard &mu;-calculus.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 07:51:04 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2008 19:10:39 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Bonatti", "Piero A.", ""], ["Lutz", "Carsten", ""], ["Murano", "Aniello", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "0809.3250", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov", "title": "Using descriptive mark-up to formalize translation quality assessment", "comments": "9 pages", "journal-ref": "Published in Russian in 'Translation industry and information\n  supply in international business activities: materials of international\n  conference' - Perm, 2008, pp. 90-101", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The paper deals with using descriptive mark-up to emphasize translation\nmistakes. The author postulates the necessity to develop a standard and formal\nXML-based way of describing translation mistakes. It is considered to be\nimportant for achieving impersonal translation quality assessment. Marked-up\ntranslations can be used in corpus translation studies; moreover, automatic\ntranslation assessment based on marked-up mistakes is possible. The paper\nconcludes with setting up guidelines for further activity within the described\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2008 20:48:13 GMT"}], "update_date": "2008-09-22", "authors_parsed": [["Kutuzov", "Andrey", ""]]}, {"id": "0809.4530", "submitter": "Olena Medelyan", "authors": "Olena Medelyan, David Milne, Catherine Legg and Ian H. Witten", "title": "Mining Meaning from Wikipedia", "comments": "An extensive survey of re-using information in Wikipedia in natural\n  language processing, information retrieval and extraction and ontology\n  building. Accepted for publication in International Journal of Human-Computer\n  Studies", "journal-ref": null, "doi": null, "report-no": "ISSN 1177-777X", "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia is a goldmine of information; not just for its many readers, but\nalso for the growing community of researchers who recognize it as a resource of\nexceptional scale and utility. It represents a vast investment of manual effort\nand judgment: a huge, constantly evolving tapestry of concepts and relations\nthat is being applied to a host of tasks.\n  This article provides a comprehensive description of this work. It focuses on\nresearch that extracts and makes use of the concepts, relations, facts and\ndescriptions found in Wikipedia, and organizes the work into four broad\ncategories: applying Wikipedia to natural language processing; using it to\nfacilitate information retrieval and information extraction; and as a resource\nfor ontology building. The article addresses how Wikipedia is being used as is,\nhow it is being improved and adapted, and how it is being combined with other\nstructures to create entirely new resources. We identify the research groups\nand individuals involved, and how their work has developed in the last few\nyears. We provide a comprehensive list of the open-source software they have\nproduced.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2008 04:47:19 GMT"}, {"version": "v2", "created": "Sun, 10 May 2009 01:51:15 GMT"}], "update_date": "2009-05-10", "authors_parsed": [["Medelyan", "Olena", ""], ["Milne", "David", ""], ["Legg", "Catherine", ""], ["Witten", "Ian H.", ""]]}]