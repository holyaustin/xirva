[{"id": "1308.0658", "submitter": "Jimmy Ren", "authors": "Jimmy SJ. Ren, Wei Wang, Jiawei Wang, Stephen Shaoyi Liao", "title": "Exploring The Contribution of Unlabeled Data in Financial Sentiment\n  Analysis", "comments": "Appeared in The 27th AAAI Conference on Artificial Intelligence\n  (AAAI-13); Proceedings of AAAI-13 (AAAI Press 2013) pp. 1149-1155", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of its applications in various industries, sentiment\nanalysis by using publicly available web data has become an active research\narea in text classification during these years. It is argued by researchers\nthat semi-supervised learning is an effective approach to this problem since it\nis capable to mitigate the manual labeling effort which is usually expensive\nand time-consuming. However, there was a long-term debate on the effectiveness\nof unlabeled data in text classification. This was partially caused by the fact\nthat many assumptions in theoretic analysis often do not hold in practice. We\nargue that this problem may be further understood by adding an additional\ndimension in the experiment. This allows us to address this problem in the\nperspective of bias and variance in a broader view. We show that the well-known\nperformance degradation issue caused by unlabeled data can be reproduced as a\nsubset of the whole scenario. We argue that if the bias-variance trade-off is\nto be better balanced by a more effective feature selection method unlabeled\ndata is very likely to boost the classification performance. We then propose a\nfeature selection framework in which labeled and unlabeled training samples are\nboth considered. We discuss its potential in achieving such a balance. Besides,\nthe application in financial sentiment analysis is chosen because it not only\nexemplifies an important application, the data possesses better illustrative\npower as well. The implications of this study in text classification and\nfinancial sentiment analysis are both discussed.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 04:20:21 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Ren", "Jimmy SJ.", ""], ["Wang", "Wei", ""], ["Wang", "Jiawei", ""], ["Liao", "Stephen Shaoyi", ""]]}, {"id": "1308.0661", "submitter": "Vincent Labatut", "authors": "Samet Atda\\u{g} and Vincent Labatut", "title": "A Comparison of Named Entity Recognition Tools Applied to Biographical\n  Texts", "comments": null, "journal-ref": "2nd International Conference on Systems and Computer Science,\n  Villeneuve d'Ascq (FR), 228-233, 2013", "doi": "10.1109/IcConSCS.2013.6632052", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is a popular domain of natural language\nprocessing. For this reason, many tools exist to perform this task. Amongst\nother points, they differ in the processing method they rely upon, the entity\ntypes they can detect, the nature of the text they can handle, and their\ninput/output formats. This makes it difficult for a user to select an\nappropriate NER tool for a specific situation. In this article, we try to\nanswer this question in the context of biographic texts. For this matter, we\nfirst constitute a new corpus by annotating Wikipedia articles. We then select\npublicly available, well known and free for research NER tools for comparison:\nStanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply\nthem to our corpus, assess their performances and compare them. When\nconsidering overall performances, a clear hierarchy emerges: Stanford has the\nbest results, followed by LingPipe, Illionois and OpenCalais. However, a more\ndetailed evaluation performed relatively to entity types and article categories\nhighlights the fact their performances are diversely influenced by those\nfactors. This complementarity opens an interesting perspective regarding the\ncombination of these individual tools in order to improve performance.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 05:57:48 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Atda\u011f", "Samet", ""], ["Labatut", "Vincent", ""]]}, {"id": "1308.0701", "submitter": "Meisam Booshehri", "authors": "Meisam Booshehri, Abbas Malekpour, Peter Luksch, Kamran Zamanifar,\n  Shahdad Shariatmadari", "title": "Ontology Enrichment by Extracting Hidden Assertional Knowledge from Text", "comments": "9 pages, International Journal of Computer Science and Information\n  Security", "journal-ref": "IJCSIS, 11(5), 64-72", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this position paper we present a new approach for discovering some special\nclasses of assertional knowledge in the text by using large RDF repositories,\nresulting in the extraction of new non-taxonomic ontological relations. Also we\nuse inductive reasoning beside our approach to make it outperform. Then, we\nprepare a case study by applying our approach on sample data and illustrate the\nsoundness of our proposed approach. Moreover in our point of view current LOD\ncloud is not a suitable base for our proposal in all informational domains.\nTherefore we figure out some directions based on prior works to enrich datasets\nof Linked Data by using web mining. The result of such enrichment can be reused\nfor further relation extraction and ontology enrichment from unstructured free\ntext documents.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 14:30:55 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Booshehri", "Meisam", ""], ["Malekpour", "Abbas", ""], ["Luksch", "Peter", ""], ["Zamanifar", "Kamran", ""], ["Shariatmadari", "Shahdad", ""]]}, {"id": "1308.0850", "submitter": "Alex Graves", "authors": "Alex Graves", "title": "Generating Sequences With Recurrent Neural Networks", "comments": "Thanks to Peng Liu and Sergey Zyrianov for various corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how Long Short-term Memory recurrent neural networks can be\nused to generate complex sequences with long-range structure, simply by\npredicting one data point at a time. The approach is demonstrated for text\n(where the data are discrete) and online handwriting (where the data are\nreal-valued). It is then extended to handwriting synthesis by allowing the\nnetwork to condition its predictions on a text sequence. The resulting system\nis able to generate highly realistic cursive handwriting in a wide variety of\nstyles.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 21:04:36 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2013 21:20:24 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2014 17:51:54 GMT"}, {"version": "v4", "created": "Wed, 2 Apr 2014 16:22:09 GMT"}, {"version": "v5", "created": "Thu, 5 Jun 2014 16:04:02 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Graves", "Alex", ""]]}, {"id": "1308.0897", "submitter": "Kowcika A", "authors": "Kowcika A, Uma Maheswari, Geetha T V", "title": "Context Specific Event Model For News Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new context based event indexing and event ranking model for\nNews Articles. The context event clusters formed from the UNL Graphs uses the\nmodified scoring scheme for segmenting events which is followed by clustering\nof events. From the context clusters obtained three models are developed-\nIdentification of Main and Sub events; Event Indexing and Event Ranking. Based\non the properties considered from the UNL Graphs for the modified scoring main\nevents and sub events associated with main-events are identified. The temporal\ndetails obtained from the context cluster are stored using hashmap data\nstructure. The temporal details are place-where the event took; person-who\ninvolved in that event; time-when the event took place. Based on the\ninformation collected from the context clusters three indices are generated-\nTime index, Person index, and Place index. This index gives complete details\nabout every event obtained from context clusters. A new scoring scheme is\nintroduced for ranking the events. The scoring scheme for event ranking gives\nweight-age based on the priority level of the events. The priority level\nincludes the occurrence of the event in the title of the document, event\nfrequency, and inverse document frequency of the events.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 07:22:07 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["A", "Kowcika", ""], ["Maheswari", "Uma", ""], ["T", "Geetha", "V"]]}, {"id": "1308.1004", "submitter": "Azad Dehghan Mr", "authors": "Azad Dehghan", "title": "Boundary identification of events in clinical named entity recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of named entity recognition in the medical/clinical domain has\ngained increasing attention do to its vital role in a wide range of clinical\ndecision support applications. The identification of complete and correct term\nspan is vital for further knowledge synthesis (e.g., coding/mapping concepts\nthesauruses and classification standards). This paper investigates boundary\nadjustment by sequence labeling representations models and post-processing\ntechniques in the problem of clinical named entity recognition (recognition of\nclinical events). Using current state-of-the-art sequence labeling algorithm\n(conditional random fields), we show experimentally that sequence labeling\nrepresentation and post-processing can be significantly helpful in strict\nboundary identification of clinical events.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 15:14:14 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2013 14:08:30 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2013 07:57:15 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Dehghan", "Azad", ""]]}, {"id": "1308.1292", "submitter": "Peter Gloor", "authors": "Elysia Wells", "title": "Science Fiction as a Worldwide Phenomenon: A Study of International\n  Creation, Consumption and Dissemination", "comments": "Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)", "journal-ref": null, "doi": null, "report-no": "coins13/2013/15", "categories": "cs.DL cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the international nature of science fiction. The focus of\nthis research is to determine whether science fiction is primarily English\nspeaking and Western or global; being created and consumed by people in\nnon-Western, non-English speaking countries? Science fiction's international\npresence was found in three ways, by network analysis, by examining a online\nretailer and with a survey. Condor, a program developed by GalaxyAdvisors was\nused to determine if science fiction is being talked about by non-English\nspeakers. An analysis of the international Amazon.com websites was done to\ndiscover if it was being consumed worldwide. A survey was also conducted to see\nif people had experience with science fiction. All three research methods\nrevealed similar results. Science fiction was found to be international, with\nscience fiction creators originating in different countries and writing in a\nhost of different languages. English and non-English science fiction was being\ncreated and consumed all over the world, not just in the English speaking West.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 14:58:25 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Wells", "Elysia", ""]]}, {"id": "1308.1507", "submitter": "Yuriy Ostapov", "authors": "Yuriy Ostapov", "title": "Logical analysis of natural language semantics to solve the problem of\n  computer understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An object--oriented approach to create a natural language understanding\nsystem is considered. The understanding program is a formal system built on the\nbase of predicative calculus. Horn's clauses are used as well--formed formulas.\nAn inference is based on the principle of resolution. Sentences of natural\nlanguage are represented in the view of typical predicate set. These predicates\ndescribe physical objects and processes, abstract objects, categories and\nsemantic relations between objects. Predicates for concrete assertions are\nsaved in a database. To describe the semantics of classes for physical objects,\nabstract concepts and processes, a knowledge base is applied. The proposed\nrepresentation of natural language sentences is a semantic net. Nodes of such\nnet are typical predicates. This approach is perspective as, firstly, such\ntypification of nodes facilitates essentially forming of processing algorithms\nand object descriptions, secondly, the effectiveness of algorithms is increased\n(particularly for the great number of nodes), thirdly, to describe the\nsemantics of words, encyclopedic knowledge is used, and this permits\nessentially to extend the class of solved problems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2013 09:09:47 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Ostapov", "Yuriy", ""]]}, {"id": "1308.1847", "submitter": "Blesson Varghese", "authors": "Vu Dung Nguyen, Blesson Varghese, Adam Barker", "title": "The Royal Birth of 2013: Analysing and Visualising Public Sentiment in\n  the UK Using Twitter", "comments": "http://www.blessonv.com/research/publicsentiment/ 9 pages. Submitted\n  to IEEE BigData 2013: Workshop on Big Humanities, October 2013", "journal-ref": null, "doi": "10.1109/BigData.2013.6691669", "report-no": null, "categories": "cs.CL cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of information retrieved from microblogging services such as Twitter\ncan provide valuable insight into public sentiment in a geographic region. This\ninsight can be enriched by visualising information in its geographic context.\nTwo underlying approaches for sentiment analysis are dictionary-based and\nmachine learning. The former is popular for public sentiment analysis, and the\nlatter has found limited use for aggregating public sentiment from Twitter\ndata. The research presented in this paper aims to extend the machine learning\napproach for aggregating public sentiment. To this end, a framework for\nanalysing and visualising public sentiment from a Twitter corpus is developed.\nA dictionary-based approach and a machine learning approach are implemented\nwithin the framework and compared using one UK case study, namely the royal\nbirth of 2013. The case study validates the feasibility of the framework for\nanalysis and rapid visualisation. One observation is that there is good\ncorrelation between the results produced by the popular dictionary-based\napproach and the machine learning approach when large volumes of tweets are\nanalysed. However, for rapid analysis to be possible faster methods need to be\ndeveloped using big data techniques and parallel methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 13:31:15 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2013 06:53:19 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Nguyen", "Vu Dung", ""], ["Varghese", "Blesson", ""], ["Barker", "Adam", ""]]}, {"id": "1308.2359", "submitter": "Arun Maiya", "authors": "Arun S. Maiya, John P. Thompson, Francisco Loaiza-Lemos, Robert M.\n  Rolfe", "title": "Exploratory Analysis of Highly Heterogeneous Document Collections", "comments": "9 pages; KDD 2013: 19th ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an effective multifaceted system for exploratory analysis of\nhighly heterogeneous document collections. Our system is based on intelligently\ntagging individual documents in a purely automated fashion and exploiting these\ntags in a powerful faceted browsing framework. Tagging strategies employed\ninclude both unsupervised and supervised approaches based on machine learning\nand natural language processing. As one of our key tagging strategies, we\nintroduce the KERA algorithm (Keyword Extraction for Reports and Articles).\nKERA extracts topic-representative terms from individual documents in a purely\nunsupervised fashion and is revealed to be significantly more effective than\nstate-of-the-art methods. Finally, we evaluate our system in its ability to\nhelp users locate documents pertaining to military critical technologies buried\ndeep in a large heterogeneous sea of information.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 02:28:11 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Maiya", "Arun S.", ""], ["Thompson", "John P.", ""], ["Loaiza-Lemos", "Francisco", ""], ["Rolfe", "Robert M.", ""]]}, {"id": "1308.2428", "submitter": "Stevan Harnad", "authors": "Olivier Picard, M\\'elanie Lord, Alexandre Blondin-Mass\\'e, Odile\n  Marcotte, Marcos Lopes and Stevan Harnad", "title": "Hidden Structure and Function in the Lexicon", "comments": "11 pages, 5 figures, 2 tables", "journal-ref": "NLPCS 2013: 10th International Workshop on Natural Language\n  Processing and Cognitive Science, Marseille, France 15-16 October 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many words are needed to define all the words in a dictionary?\nGraph-theoretic analysis reveals that about 10% of a dictionary is a unique\nKernel of words that define one another and all the rest, but this is not the\nsmallest such subset. The Kernel consists of one huge strongly connected\ncomponent (SCC), about half its size, the Core, surrounded by many small SCCs,\nthe Satellites. Core words can define one another but not the rest of the\ndictionary. The Kernel also contains many overlapping Minimal Grounding Sets\n(MGSs), each about the same size as the Core, each part-Core, part-Satellite.\nMGS words can define all the rest of the dictionary. They are learned earlier,\nmore concrete and more frequent than the rest of the dictionary. Satellite\nwords, not correlated with age or frequency, are less concrete (more abstract)\nwords that are also needed for full lexical power.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 20:50:27 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2013 12:06:55 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Picard", "Olivier", ""], ["Lord", "M\u00e9lanie", ""], ["Blondin-Mass\u00e9", "Alexandre", ""], ["Marcotte", "Odile", ""], ["Lopes", "Marcos", ""], ["Harnad", "Stevan", ""]]}, {"id": "1308.2696", "submitter": "Alexandra Paxton", "authors": "A. Paxton and R. Dale", "title": "B(eo)W(u)LF: Facilitating recurrence analysis on multi-level language", "comments": "3 pages plus 6 appendices (including code and sample data)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse analysis may seek to characterize not only the overall composition\nof a given text but also the dynamic patterns within the data. This technical\nreport introduces a data format intended to facilitate multi-level\ninvestigations, which we call the by-word long-form or B(eo)W(u)LF. Inspired by\nthe long-form data format required for mixed-effects modeling, B(eo)W(u)LF\nstructures linguistic data into an expanded matrix encoding any number of\nresearchers-specified markers, making it ideal for recurrence-based analyses.\nWhile we do not necessarily claim to be the first to use methods along these\nlines, we have created a series of tools utilizing Python and MATLAB to enable\nsuch discourse analyses and demonstrate them using 319 lines of the Old English\nepic poem, Beowulf, translated into modern English.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 20:57:02 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Paxton", "A.", ""], ["Dale", "R.", ""]]}, {"id": "1308.3106", "submitter": "Girish Sundaram", "authors": "Sachin Kumar, Ashish Kumar, Pinaki Mitra, Girish Sundaram", "title": "System and Methods for Converting Speech to SQL", "comments": "Appeared In proceedings of International Conference ERCICA 2013 pp:\n  291-298, Published by Elsevier Ltd, ISBN:9789351071020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns with the conversion of a Spoken English Language Query\ninto SQL for retrieving data from RDBMS. A User submits a query as speech\nsignal through the user interface and gets the result of the query in the text\nformat. We have developed the acoustic and language models using which a speech\nutterance can be converted into English text query and thus natural language\nprocessing techniques can be applied on this English text query to generate an\nequivalent SQL query. For conversion of speech into English text HTK and Julius\ntools have been used and for conversion of English text query into SQL query we\nhave implemented a System which uses rule based translation to translate\nEnglish Language Query into SQL Query. The translation uses lexical analyzer,\nparser and syntax directed translation techniques like in compilers. JFLex and\nBYACC tools have been used to build lexical analyzer and parser respectively.\nSystem is domain independent i.e. system can run on different database as it\ngenerates lex files from the underlying database.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 12:54:31 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Kumar", "Sachin", ""], ["Kumar", "Ashish", ""], ["Mitra", "Pinaki", ""], ["Sundaram", "Girish", ""]]}, {"id": "1308.3243", "submitter": "Mohamed Ben Halima", "authors": "M. Ben Halima, H. Karray and A. M. Alimi", "title": "Arabic Text Recognition in Video Sequences", "comments": "10 pages - International Journal of Computational Linguistics\n  Research. arXiv admin note: substantial text overlap with arXiv:1211.2150", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a robust approach for text extraction and\nrecognition from Arabic news video sequence. The text included in video\nsequences is an important needful for indexing and searching system. However,\nthis text is difficult to detect and recognize because of the variability of\nits size, their low resolution characters and the complexity of the\nbackgrounds. To solve these problems, we propose a system performing in two\nmain tasks: extraction and recognition of text. Our system is tested on a\nvaried database composed of different Arabic news programs and the obtained\nresults are encouraging and show the merits of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 20:15:44 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Halima", "M. Ben", ""], ["Karray", "H.", ""], ["Alimi", "A. M.", ""]]}, {"id": "1308.3294", "submitter": "Nicholas Kersting", "authors": "Nicholas Kersting", "title": "A Secure and Comparable Text Encryption Algorithm", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discloses a simple algorithm for encrypting text messages, based\non the NP-completeness of the subset sum problem, such that the similarity\nbetween encryptions is roughly proportional to the semantic similarity between\ntheir generating messages. This allows parties to compare encrypted messages\nfor semantic overlap without trusting an intermediary and might be applied, for\nexample, as a means of finding scientific collaborators over the Internet.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 02:44:12 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Kersting", "Nicholas", ""]]}, {"id": "1308.3785", "submitter": "Md. Ali Hossain Ali", "authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan and Md.\n  Farukuzzaman Khan", "title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition", "comments": "9 pages, 3 figures, 1 table", "journal-ref": "International Journal of Information Sciences and Techniques\n  (IJIST) Vol.3, No.4, July 2013", "doi": "10.5121/ijist.2013.3401", "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2013 14:04:00 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Hossain", "Md. Ali", ""], ["Rahman", "Md. Mijanur", ""], ["Prodhan", "Uzzal Kumar", ""], ["Khan", "Md. Farukuzzaman", ""]]}, {"id": "1308.3830", "submitter": "Rukshan Alexander", "authors": "Rukshan Alexander, Prashanthi Rukshan, and Sinnathamby Mahesan", "title": "Natural Language Web Interface for Database (NLWIDB)", "comments": "8 pages,5 figures, 12 tables, Proceedings of the Third International\n  Symposium, SEUSL: 6-7 July 2013, Oluvil, Sri Lanka", "journal-ref": "Proceedings of the Third International Symposium, SEUSL: 6-7 July\n  2013, Oluvil, Sri Lanka", "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a long term desire of the computer users to minimize the communication\ngap between the computer and a human. On the other hand, almost all ICT\napplications store information in to databases and retrieve from them.\nRetrieving information from the database requires knowledge of technical\nlanguages such as Structured Query Language. However majority of the computer\nusers who interact with the databases do not have a technical background and\nare intimidated by the idea of using languages such as SQL. For above reasons,\na Natural Language Web Interface for Database (NLWIDB) has been developed. The\nNLWIDB allows the user to query the database in a language more like English,\nthrough a convenient interface over the Internet.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2013 04:22:40 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Alexander", "Rukshan", ""], ["Rukshan", "Prashanthi", ""], ["Mahesan", "Sinnathamby", ""]]}, {"id": "1308.3839", "submitter": "Arko Banerjee", "authors": "Tamal Chowdhury, Rabindra Rakshit and Arko Banerjee", "title": "Consensus Sequence Segmentation", "comments": "This paper has been withdrawn by the authors. The paper has been\n  withdrawn due to error data input in table no. 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a method to detect words or phrases in a given\nsequence of alphabets without knowing the lexicon. Our linear time unsupervised\nalgorithm relies entirely on statistical relationships among alphabets in the\ninput sequence to detect location of word boundaries. We compare our algorithm\nto previous approaches from unsupervised sequence segmentation literature and\nprovide superior segmentation over number of benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2013 07:09:03 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 05:42:12 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Chowdhury", "Tamal", ""], ["Rakshit", "Rabindra", ""], ["Banerjee", "Arko", ""]]}, {"id": "1308.4189", "submitter": "Andrei Barbu", "authors": "N. Siddharth, Andrei Barbu, Jeffrey Mark Siskind", "title": "Seeing What You're Told: Sentence-Guided Activity Recognition In Video", "comments": "To appear in CVPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that demonstrates how the compositional structure of\nevents, in concert with the compositional structure of language, can interplay\nwith the underlying focusing mechanisms in video action recognition, thereby\nproviding a medium, not only for top-down and bottom-up integration, but also\nfor multi-modal integration between vision and language. We show how the roles\nplayed by participants (nouns), their characteristics (adjectives), the actions\nperformed (verbs), the manner of such actions (adverbs), and changing spatial\nrelations between participants (prepositions) in the form of whole sentential\ndescriptions mediated by a grammar, guides the activity-recognition process.\nFurther, the utility and expressiveness of our framework is demonstrated by\nperforming three separate tasks in the domain of multi-activity videos:\nsentence-guided focus of attention, generation of sentential descriptions of\nvideo, and query-based video search, simply by leveraging the framework in\ndifferent manners.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 23:28:47 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 18:50:35 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Siddharth", "N.", ""], ["Barbu", "Andrei", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1308.4479", "submitter": "Juan Luo", "authors": "Juan Luo and Yves Lepage", "title": "An Investigation of the Sampling-Based Alignment Method and Its\n  Contributions", "comments": "11 pages", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol. 4, No. 4, July 2013", "doi": "10.5121/ijaia.2013.4402", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By investigating the distribution of phrase pairs in phrase translation\ntables, the work in this paper describes an approach to increase the number of\nn-gram alignments in phrase translation tables output by a sampling-based\nalignment method. This approach consists in enforcing the alignment of n-grams\nin distinct translation subtables so as to increase the number of n-grams.\nStandard normal distribution is used to allot alignment time among translation\nsubtables, which results in adjustment of the distribution of n- grams. This\nleads to better evaluation results on statistical machine translation tasks\nthan the original sampling-based alignment approach. Furthermore, the\ntranslation quality obtained by merging phrase translation tables computed from\nthe sampling-based alignment method and from MGIZA++ is examined.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 03:44:04 GMT"}], "update_date": "2013-08-22", "authors_parsed": [["Luo", "Juan", ""], ["Lepage", "Yves", ""]]}, {"id": "1308.4618", "submitter": "Michael Bell", "authors": "Michael J. Bell, Matthew Collison, Phillip Lord", "title": "Can inferred provenance and its visualisation be used to detect\n  erroneous annotation? A case study using UniProtKB", "comments": "Paper to shortly appear in PLOS ONE. Composed of 21 pages and 16\n  figures", "journal-ref": null, "doi": "10.1371/journal.pone.0075541", "report-no": null, "categories": "cs.CL cs.CE cs.DL q-bio.QM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A constant influx of new data poses a challenge in keeping the annotation in\nbiological databases current. Most biological databases contain significant\nquantities of textual annotation, which often contains the richest source of\nknowledge. Many databases reuse existing knowledge, during the curation process\nannotations are often propagated between entries. However, this is often not\nmade explicit. Therefore, it can be hard, potentially impossible, for a reader\nto identify where an annotation originated from. Within this work we attempt to\nidentify annotation provenance and track its subsequent propagation.\nSpecifically, we exploit annotation reuse within the UniProt Knowledgebase\n(UniProtKB), at the level of individual sentences. We describe a visualisation\napproach for the provenance and propagation of sentences in UniProtKB which\nenables a large-scale statistical analysis. Initially levels of sentence reuse\nwithin UniProtKB were analysed, showing that reuse is heavily prevalent, which\nenables the tracking of provenance and propagation. By analysing sentences\nthroughout UniProtKB, a number of interesting propagation patterns were\nidentified, covering over 100, 000 sentences. Over 8000 sentences remain in the\ndatabase after they have been removed from the entries where they originally\noccurred. Analysing a subset of these sentences suggest that approximately 30%\nare erroneous, whilst 35% appear to be inconsistent. These results suggest that\nbeing able to visualise sentence propagation and provenance can aid in the\ndetermination of the accuracy and quality of textual annotation. Source code\nand supplementary data are available from the authors website.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 15:49:43 GMT"}], "update_date": "2013-08-22", "authors_parsed": [["Bell", "Michael J.", ""], ["Collison", "Matthew", ""], ["Lord", "Phillip", ""]]}, {"id": "1308.4648", "submitter": "Robert Bridges", "authors": "Nikki McNeil, Robert A. Bridges, Michael D. Iannacone, Bogdan Czejdo,\n  Nicolas Perez, John R. Goodall", "title": "PACE: Pattern Accurate Computationally Efficient Bootstrapping for\n  Timely Discovery of Cyber-Security Concepts", "comments": "6 pages, 3 figures, ieeeTran conference. International Conference on\n  Machine Learning and Applications 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public disclosure of important security information, such as knowledge of\nvulnerabilities or exploits, often occurs in blogs, tweets, mailing lists, and\nother online sources months before proper classification into structured\ndatabases. In order to facilitate timely discovery of such knowledge, we\npropose a novel semi-supervised learning algorithm, PACE, for identifying and\nclassifying relevant entities in text sources. The main contribution of this\npaper is an enhancement of the traditional bootstrapping method for entity\nextraction by employing a time-memory trade-off that simultaneously circumvents\na costly corpus search while strengthening pattern nomination, which should\nincrease accuracy. An implementation in the cyber-security domain is discussed\nas well as challenges to Natural Language Processing imposed by the security\ndomain.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 18:01:42 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2013 18:07:22 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 14:13:26 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["McNeil", "Nikki", ""], ["Bridges", "Robert A.", ""], ["Iannacone", "Michael D.", ""], ["Czejdo", "Bogdan", ""], ["Perez", "Nicolas", ""], ["Goodall", "John R.", ""]]}, {"id": "1308.4941", "submitter": "Robert Bridges", "authors": "Robert A. Bridges, Corinne L. Jones, Michael D. Iannacone, Kelly M.\n  Testa, John R. Goodall", "title": "Automatic Labeling for Entity Extraction in Cyber Security", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely analysis of cyber-security information necessitates automated\ninformation extraction from unstructured text. While state-of-the-art\nextraction methods produce extremely accurate results, they require ample\ntraining data, which is generally unavailable for specialized applications,\nsuch as detecting security related entities; moreover, manual annotation of\ncorpora is very costly and often not a viable solution. In response, we develop\na very precise method to automatically label text from several data sources by\nleveraging related, domain-specific, structured data and provide public access\nto a corpus annotated with cyber-security entities. Next, we implement a\nMaximum Entropy Model trained with the average perceptron on a portion of our\ncorpus ($\\sim$750,000 words) and achieve near perfect precision, recall, and\naccuracy, with training times under 17 seconds.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 18:23:25 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2013 18:45:43 GMT"}, {"version": "v3", "created": "Mon, 9 Jun 2014 23:51:25 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Bridges", "Robert A.", ""], ["Jones", "Corinne L.", ""], ["Iannacone", "Michael D.", ""], ["Testa", "Kelly M.", ""], ["Goodall", "John R.", ""]]}, {"id": "1308.4965", "submitter": "Maurice Margenstern", "authors": "Maurice Margenstern, Lan Wu", "title": "A proposal for a Chinese keyboard for cellphones, smartphones, ipads and\n  tablets", "comments": "28 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the possibility to use two tilings of the\nhyperbolic plane as basic frame for devising a way to input texts in Chinese\ncharacters into messages of cellphones, smartphones, ipads and tablets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 17:53:37 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Margenstern", "Maurice", ""], ["Wu", "Lan", ""]]}, {"id": "1308.5010", "submitter": "Yaneer Bar-Yam", "authors": "Karla Z. Bertrand, Maya Bialik, Kawandeep Virdee, Andreas Gros and\n  Yaneer Bar-Yam", "title": "Sentiment in New York City: A High Resolution Spatial and Temporal View", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "New England Complex Systems Institute (NECSI) Report 2013-08-01", "categories": "physics.soc-ph cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring public sentiment is a key task for researchers and policymakers\nalike. The explosion of available social media data allows for a more\ntime-sensitive and geographically specific analysis than ever before. In this\npaper we analyze data from the micro-blogging site Twitter and generate a\nsentiment map of New York City. We develop a classifier specifically tuned for\n140-character Twitter messages, or tweets, using key words, phrases and\nemoticons to determine the mood of each tweet. This method, combined with\ngeotagging provided by users, enables us to gauge public sentiment on extremely\nfine-grained spatial and temporal scales. We find that public mood is generally\nhighest in public parks and lowest at transportation hubs, and locate other\nareas of strong sentiment such as cemeteries, medical centers, a jail, and a\nsewage facility. Sentiment progressively improves with proximity to Times\nSquare. Periodic patterns of sentiment fluctuate on both a daily and a weekly\nscale: more positive tweets are posted on weekends than on weekdays, with a\ndaily peak in sentiment around midnight and a nadir between 9:00 a.m. and noon.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 22:14:25 GMT"}], "update_date": "2013-08-26", "authors_parsed": [["Bertrand", "Karla Z.", ""], ["Bialik", "Maya", ""], ["Virdee", "Kawandeep", ""], ["Gros", "Andreas", ""], ["Bar-Yam", "Yaneer", ""]]}, {"id": "1308.5423", "submitter": "Mahima Sharma", "authors": "M.Thangarasu, R.Manavalan", "title": "A Literature Review: Stemming Algorithms for Indian Languages", "comments": null, "journal-ref": "International Journal of Computer Trends and Technology, Vol 4, No\n  8, 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stemming is the process of extracting root word from the given inflection\nword. It also plays significant role in numerous application of Natural\nLanguage Processing (NLP). The stemming problem has addressed in many contexts\nand by researchers in many disciplines. This expository paper presents survey\nof some of the latest developments on stemming algorithms in data mining and\nalso presents with some of the solutions for various Indian language stemming\nalgorithms along with the results.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2013 16:41:06 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Thangarasu", "M.", ""], ["Manavalan", "R.", ""]]}, {"id": "1308.5499", "submitter": "Bodo Winter", "authors": "Bodo Winter", "title": "Linear models and linear mixed effects models in R with linguistic\n  applications", "comments": "42 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This text is a conceptual introduction to mixed effects modeling with\nlinguistic applications, using the R programming environment. The reader is\nintroduced to linear modeling and assumptions, as well as to mixed\neffects/multilevel modeling, including a discussion of random intercepts,\nrandom slopes and likelihood ratio tests. The example used throughout the text\nfocuses on the phonetic analysis of voice pitch data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 07:18:17 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Winter", "Bodo", ""]]}, {"id": "1308.6242", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu", "title": "NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of\n  Tweets", "comments": null, "journal-ref": "In Proceedings of the seventh international workshop on Semantic\n  Evaluation Exercises (SemEval-2013), June 2013, Atlanta, USA", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe how we created two state-of-the-art SVM\nclassifiers, one to detect the sentiment of messages such as tweets and SMS\n(message-level task) and one to detect the sentiment of a term within a\nsubmissions stood first in both tasks on tweets, obtaining an F-score of 69.02\nin the message-level task and 88.93 in the term-level task. We implemented a\nvariety of surface-form, semantic, and sentiment features. with sentiment-word\nhashtags, and one from tweets with emoticons. In the message-level task, the\nlexicon-based features provided a gain of 5 F-score points over all others.\nBoth of our systems can be replicated us available resources.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 18:23:03 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Mohammad", "Saif M.", ""], ["Kiritchenko", "Svetlana", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "1308.6297", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad and Peter D. Turney", "title": "Crowdsourcing a Word-Emotion Association Lexicon", "comments": null, "journal-ref": "Computational Intelligence, 29 (3), 436-465, Wiley Blackwell\n  Publishing Ltd, 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though considerable attention has been given to the polarity of words\n(positive and negative) and the creation of large polarity lexicons, research\nin emotion analysis has had to rely on limited and small emotion lexicons. In\nthis paper we show how the combined strength and wisdom of the crowds can be\nused to generate a large, high-quality, word-emotion and word-polarity\nassociation lexicon quickly and inexpensively. We enumerate the challenges in\nemotion annotation in a crowdsourcing scenario and propose solutions to address\nthem. Most notably, in addition to questions about emotions associated with\nterms, we show how the inclusion of a word choice question can discourage\nmalicious data entry, help identify instances where the annotator may not be\nfamiliar with the target term (allowing us to reject such annotations), and\nhelp obtain annotations at sense level (rather than at word level). We\nconducted experiments on how to formulate the emotion-annotation questions, and\nshow that asking if a term is associated with an emotion leads to markedly\nhigher inter-annotator agreement than that obtained by asking if a term evokes\nan emotion.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 20:13:32 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Mohammad", "Saif M.", ""], ["Turney", "Peter D.", ""]]}, {"id": "1308.6300", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and Peter D. Turney", "title": "Computing Lexical Contrast", "comments": null, "journal-ref": "Computational Linguistics, 39 (3), 555-590, 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing the degree of semantic contrast between words has widespread\napplication in natural language processing, including machine translation,\ninformation retrieval, and dialogue systems. Manually-created lexicons focus on\nopposites, such as {\\rm hot} and {\\rm cold}. Opposites are of many kinds such\nas antipodals, complementaries, and gradable. However, existing lexicons often\ndo not classify opposites into the different kinds. They also do not explicitly\nlist word pairs that are not opposites but yet have some degree of contrast in\nmeaning, such as {\\rm warm} and {\\rm cold} or {\\rm tropical} and {\\rm\nfreezing}. We propose an automatic method to identify contrasting word pairs\nthat is based on the hypothesis that if a pair of words, $A$ and $B$, are\ncontrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and\n$C$ are strongly related and $B$ and $D$ are strongly related. (For example,\nthere exists the pair of opposites {\\rm hot} and {\\rm cold} such that {\\rm\ntropical} is related to {\\rm hot,} and {\\rm freezing} is related to {\\rm\ncold}.) We will call this the contrast hypothesis. We begin with a large\ncrowdsourcing experiment to determine the amount of human agreement on the\nconcept of oppositeness and its different kinds. In the process, we flesh out\nkey features of different kinds of opposites. We then present an automatic and\nempirical measure of lexical contrast that relies on the contrast hypothesis,\ncorpus statistics, and the structure of a {\\it Roget}-like thesaurus. We show\nthat the proposed measure of lexical contrast obtains high precision and large\ncoverage, outperforming existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 20:24:27 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Mohammad", "Saif M.", ""], ["Dorr", "Bonnie J.", ""], ["Hirst", "Graeme", ""], ["Turney", "Peter D.", ""]]}, {"id": "1308.6628", "submitter": "Kewei Tu", "authors": "Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, Song-Chun Zhu", "title": "Joint Video and Text Parsing for Understanding Events and Answering\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for parsing video and text jointly for understanding\nevents and answering user queries. Our framework produces a parse graph that\nrepresents the compositional structures of spatial information (objects and\nscenes), temporal information (actions and events) and causal information\n(causalities between events and fluents) in the video and text. The knowledge\nrepresentation of our framework is based on a spatial-temporal-causal And-Or\ngraph (S/T/C-AOG), which jointly models possible hierarchical compositions of\nobjects, scenes and events as well as their interactions and mutual contexts,\nand specifies the prior probabilistic distribution of the parse graphs. We\npresent a probabilistic generative model for joint parsing that captures the\nrelations between the input video/text, their corresponding parse graphs and\nthe joint parse graph. Based on the probabilistic model, we propose a joint\nparsing system consisting of three modules: video parsing, text parsing and\njoint inference. Video parsing and text parsing produce two parse graphs from\nthe input video and text respectively. The joint inference module produces a\njoint parse graph by performing matching, deduction and revision on the video\nand text parse graphs. The proposed framework has the following objectives:\nFirstly, we aim at deep semantic parsing of video and text that goes beyond the\ntraditional bag-of-words approaches; Secondly, we perform parsing and reasoning\nacross the spatial, temporal and causal dimensions based on the joint S/T/C-AOG\nrepresentation; Thirdly, we show that deep joint parsing facilitates subsequent\napplications such as generating narrative text descriptions and answering\nqueries in the forms of who, what, when, where and why. We empirically\nevaluated our system based on comparison against ground-truth as well as\naccuracy of query answering and obtained satisfactory results.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 23:45:02 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2014 05:24:09 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Tu", "Kewei", ""], ["Meng", "Meng", ""], ["Lee", "Mun Wai", ""], ["Choe", "Tae Eun", ""], ["Zhu", "Song-Chun", ""]]}]