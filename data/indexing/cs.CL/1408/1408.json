[{"id": "1408.0016", "submitter": "Rolf Schwitter", "authors": "Stephen Guy and Rolf Schwitter", "title": "Architecture of a Web-based Predictive Editor for Controlled Natural\n  Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the architecture of a web-based predictive text\neditor being developed for the controlled natural language PENG$^{ASP)$. This\ncontrolled language can be used to write non-monotonic specifications that have\nthe same expressive power as Answer Set Programs. In order to support the\nwriting process of these specifications, the predictive text editor\ncommunicates asynchronously with the controlled natural language processor that\ngenerates lookahead categories and additional auxiliary information for the\nauthor of a specification text. The text editor can display multiple sets of\nlookahead categories simultaneously for different possible sentence\ncompletions, anaphoric expressions, and supports the addition of new content\nwords to the lexicon.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 01:00:59 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Guy", "Stephen", ""], ["Schwitter", "Rolf", ""]]}, {"id": "1408.0782", "submitter": "Jinho D. Choi", "authors": "Sandeep Ashwini and Jinho D. Choi", "title": "Targetable Named Entity Recognition in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for recognizing what we call targetable named\nentities; that is, named entities in a targeted set (e.g, movies, books, TV\nshows). Unlike many other NER systems that need to retrain their statistical\nmodels as new entities arrive, our approach does not require such retraining,\nwhich makes it more adaptable for types of entities that are frequently\nupdated. For this preliminary study, we focus on one entity type, movie title,\nusing data collected from Twitter. Our system is tested on two evaluation sets,\none including only entities corresponding to movies in our training set, and\nthe other excluding any of those entities. Our final model shows F1-scores of\n76.19% and 78.70% on these evaluation sets, which gives strong evidence that\nour approach is completely unbiased to any par- ticular set of entities found\nduring training.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 19:31:32 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Ashwini", "Sandeep", ""], ["Choi", "Jinho D.", ""]]}, {"id": "1408.0985", "submitter": "Lucas Lacasa", "authors": "Jordi Luque, Bartolo Luque and Lucas Lacasa", "title": "Speech earthquakes: scaling and universality in human voice", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech is a distinctive complex feature of human capabilities. In order to\nunderstand the physics underlying speech production, in this work we\nempirically analyse the statistics of large human speech datasets ranging\nseveral languages. We first show that during speech the energy is unevenly\nreleased and power-law distributed, reporting a universal robust\nGutenberg-Richter-like law in speech. We further show that such earthquakes in\nspeech show temporal correlations, as the interevent statistics are again\npower-law distributed. Since this feature takes place in the intra-phoneme\nrange, we conjecture that the responsible for this complex phenomenon is not\ncognitive, but it resides on the physiological speech production mechanism.\nMoreover, we show that these waiting time distributions are scale invariant\nunder a renormalisation group transformation, suggesting that the process of\nspeech generation is indeed operating close to a critical point. These results\nare put in contrast with current paradigms in speech processing, which point\ntowards low dimensional deterministic chaos as the origin of nonlinear traits\nin speech fluctuations. As these latter fluctuations are indeed the aspects\nthat humanize synthetic speech, these findings may have an impact in future\nspeech synthesis technologies. Results are robust and independent of the\ncommunication language or the number of speakers, pointing towards an universal\npattern and yet another hint of complexity in human speech.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 14:34:20 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Luque", "Jordi", ""], ["Luque", "Bartolo", ""], ["Lacasa", "Lucas", ""]]}, {"id": "1408.1031", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Ahmed Elgammal", "title": "Text to Multi-level MindMaps: A Novel Method for Hierarchical Visual\n  Abstraction of Natural Language Text", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MindMapping is a well-known technique used in note taking, which encourages\nlearning and studying. MindMapping has been manually adopted to help present\nknowledge and concepts in a visual form. Unfortunately, there is no reliable\nautomated approach to generate MindMaps from Natural Language text. This work\nfirstly introduces MindMap Multilevel Visualization concept which is to jointly\nvisualize and summarize textual information. The visualization is achieved\npictorially across multiple levels using semantic information (i.e. ontology),\nwhile the summarization is achieved by the information in the highest levels as\nthey represent abstract information in the text. This work also presents the\nfirst automated approach that takes a text input and generates a MindMap\nvisualization out of it. The approach could visualize text documents in\nmultilevel MindMaps, in which a high-level MindMap node could be expanded into\nchild MindMaps. \\ignore{ As far as we know, this is the first work that view\nMindMapping as a new approach to jointly summarize and visualize textual\ninformation.} The proposed method involves understanding of the input text and\nconverting it into intermediate Detailed Meaning Representation (DMR). The DMR\nis then visualized with two modes; Single level or Multiple levels, which is\nconvenient for larger text. The generated MindMaps from both approaches were\nevaluated based on Human Subject experiments performed on Amazon Mechanical\nTurk with various parameter settings.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 03:18:56 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 06:27:03 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1408.1774", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "Beyond description. Comment on \"Approaching human language with complex\n  networks\" by Cong & Liu", "comments": null, "journal-ref": "Physics of Life Reviews 11 (4), 621-623 (2014)", "doi": "10.1016/j.plrev.2014.07.014", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on \"Approaching human language with complex networks\" by Cong & Liu\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 07:39:13 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1408.1928", "submitter": "Benjamin Good", "authors": "Benjamin M Good, Max Nanis, Andrew I. Su", "title": "Microtask crowdsourcing for disease mention annotation in PubMed\n  abstracts", "comments": "Preprint of an article submitted for consideration in the Pacific\n  Symposium on Biocomputing copyright 2015; World Scientific Publishing Co.,\n  Singapore, 2015; http://psb.stanford.edu/. Data produced for this analysis\n  are available at\n  http://figshare.com/articles/Disease_Mention_Annotation_with_Mechanical_Turk/1126402", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Identifying concepts and relationships in biomedical text enables knowledge\nto be applied in computational analyses. Many biological natural language\nprocess (BioNLP) projects attempt to address this challenge, but the state of\nthe art in BioNLP still leaves much room for improvement. Progress in BioNLP\nresearch depends on large, annotated corpora for evaluating information\nextraction systems and training machine learning models. Traditionally, such\ncorpora are created by small numbers of expert annotators often working over\nextended periods of time. Recent studies have shown that workers on microtask\ncrowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, in\naggregate, generate high-quality annotations of biomedical text. Here, we\ninvestigated the use of the AMT in capturing disease mentions in PubMed\nabstracts. We used the NCBI Disease corpus as a gold standard for refining and\nbenchmarking our crowdsourcing protocol. After several iterations, we arrived\nat a protocol that reproduced the annotations of the 593 documents in the\ntraining set of this gold standard with an overall F measure of 0.872\n(precision 0.862, recall 0.883). The output can also be tuned to optimize for\nprecision (max = 0.984 when recall = 0.269) or recall (max = 0.980 when\nprecision = 0.436). Each document was examined by 15 workers, and their\nannotations were merged based on a simple voting method. In total 145 workers\ncombined to complete all 593 documents in the span of 1 week at a cost of $.06\nper abstract per worker. The quality of the annotations, as judged with the F\nmeasure, increases with the number of workers assigned to each task such that\nthe system can be tuned to balance cost against quality. These results\ndemonstrate that microtask crowdsourcing can be a valuable tool for generating\nwell-annotated corpora in BioNLP.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 17:49:02 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Good", "Benjamin M", ""], ["Nanis", "Max", ""], ["Su", "Andrew I.", ""]]}, {"id": "1408.1985", "submitter": "Janet Pierrehumbert", "authors": "Janet B. Pierrehumbert, Forrest Stonedahl, and Robert Daland", "title": "A model of grassroots changes in linguistic systems", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL nlin.AO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic norms emerge in human communities because people imitate each\nother. A shared linguistic system provides people with the benefits of shared\nknowledge and coordinated planning. Once norms are in place, why would they\never change? This question, echoing broad questions in the theory of social\ndynamics, has particular force in relation to language. By definition, an\ninnovator is in the minority when the innovation first occurs. In some areas of\nsocial dynamics, important minorities can strongly influence the majority\nthrough their power, fame, or use of broadcast media. But most linguistic\nchanges are grassroots developments that originate with ordinary people. Here,\nwe develop a novel model of communicative behavior in communities, and identify\na mechanism for arbitrary innovations by ordinary people to have a good chance\nof being widely adopted.\n  To imitate each other, people must form a mental representation of what other\npeople do. Each time they speak, they must also decide which form to produce\nthemselves. We introduce a new decision function that enables us to smoothly\nexplore the space between two types of behavior: probability matching (matching\nthe probabilities of incoming experience) and regularization (producing some\nforms disproportionately often). Using Monte Carlo methods, we explore the\ninteractions amongst the degree of regularization, the distribution of biases\nin a network, and the network position of the innovator. We identify two\nregimes for the widespread adoption of arbritrary innovations, viewed as\ninformational cascades in the network. With moderate regularization of\nexperienced input, average people (not well-connected people) are the most\nlikely source of successful innovations. Our results shed light on a major\noutstanding puzzle in the theory of language change. The framework also holds\npromise for understanding the dynamics of other social norms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 21:23:55 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Pierrehumbert", "Janet B.", ""], ["Stonedahl", "Forrest", ""], ["Daland", "Robert", ""]]}, {"id": "1408.2359", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama", "title": "Gap-weighted subsequences for automatic cognate identification and\n  phylogenetic inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we describe the problem of cognate identification and its\nrelation to phylogenetic inference. We introduce subsequence based features for\ndiscriminating cognates from non-cognates. We show that subsequence based\nfeatures perform better than the state-of-the-art string similarity measures\nfor the purpose of cognate identification. We use the cognate judgments for the\npurpose of phylogenetic inference and observe that these classifiers infer a\ntree which is close to the gold standard tree. The contribution of this paper\nis the use of subsequence features for cognate identification and to employ the\ncognate judgments for phylogenetic inference.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 09:32:39 GMT"}, {"version": "v2", "created": "Fri, 22 Aug 2014 07:29:44 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Rama", "Taraka", ""]]}, {"id": "1408.2430", "submitter": "Boris Iolis", "authors": "Boris Iolis, Gianluca Bontempi", "title": "Optimizing Component Combination in a Multi-Indexing Paragraph Retrieval\n  System", "comments": "5 pages, 1 figure, unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a method to optimize the combination of distinct components in\na paragraph retrieval system. Our system makes use of several indices, query\ngenerators and filters, each of them potentially contributing to the quality of\nthe returned list of results. The components are combined with a weighed sum,\nand we optimize the weights using a heuristic optimization algorithm. This\nallows us to maximize the quality of our results, but also to determine which\ncomponents are most valuable in our system. We evaluate our approach on the\nparagraph selection task of a Question Answering dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 14:58:30 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Iolis", "Boris", ""], ["Bontempi", "Gianluca", ""]]}, {"id": "1408.2466", "submitter": "Rolf Schwitter", "authors": "Rolf Schwitter", "title": "Controlled Natural Language Processing as Answer Set Programming: an\n  Experiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most controlled natural languages (CNLs) are processed with the help of a\npipeline architecture that relies on different software components. We\ninvestigate in this paper in an experimental way how well answer set\nprogramming (ASP) is suited as a unifying framework for parsing a CNL, deriving\na formal representation for the resulting syntax trees, and for reasoning with\nthat representation. We start from a list of input tokens in ASP notation and\nshow how this input can be transformed into a syntax tree using an ASP grammar\nand then into reified ASP rules in form of a set of facts. These facts are then\nprocessed by an ASP meta-interpreter that allows us to infer new knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 02:20:23 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Schwitter", "Rolf", ""]]}, {"id": "1408.2699", "submitter": "Claudio Castellano", "authors": "Christine F. Cuskley, Martina Pugliese, Claudio Castellano, Francesca\n  Colaiori, Vittorio Loreto, Francesca Tria", "title": "Internal and external dynamics in language: Evidence from verb\n  regularity in a historical corpus of English", "comments": "12 page, 4 figures + Supporting Information (18 pages)", "journal-ref": "PLoS ONE 9(8): e102882 (2014)", "doi": "10.1371/journal.pone.0102882", "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human languages are rule governed, but almost invariably these rules have\nexceptions in the form of irregularities. Since rules in language are efficient\nand productive, the persistence of irregularity is an anomaly. How does\nirregularity linger in the face of internal (endogenous) and external\n(exogenous) pressures to conform to a rule? Here we address this problem by\ntaking a detailed look at simple past tense verbs in the Corpus of Historical\nAmerican English. The data show that the language is open, with many new verbs\nentering. At the same time, existing verbs might tend to regularize or\nirregularize as a consequence of internal dynamics, but overall, the amount of\nirregularity sustained by the language stays roughly constant over time.\nDespite continuous vocabulary growth, and presumably, an attendant increase in\nexpressive power, there is no corresponding growth in irregularity. We analyze\nthe set of irregulars, showing they may adhere to a set of minority rules,\nallowing for increased stability of irregularity over time. These findings\ncontribute to the debate on how language systems become rule governed, and how\nand why they sustain exceptions to rules, providing insight into the interplay\nbetween the emergence and maintenance of rules and exceptions in language.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 12:07:27 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Cuskley", "Christine F.", ""], ["Pugliese", "Martina", ""], ["Castellano", "Claudio", ""], ["Colaiori", "Francesca", ""], ["Loreto", "Vittorio", ""], ["Tria", "Francesca", ""]]}, {"id": "1408.2873", "submitter": "Andrew Maas", "authors": "Awni Y. Hannun, Andrew L. Maas, Daniel Jurafsky, Andrew Y. Ng", "title": "First-Pass Large Vocabulary Continuous Speech Recognition using\n  Bi-Directional Recurrent DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to perform first-pass large vocabulary continuous speech\nrecognition using only a neural network and language model. Deep neural network\nacoustic models are now commonplace in HMM-based speech recognition systems,\nbut building such systems is a complex, domain-specific task. Recent work\ndemonstrated the feasibility of discarding the HMM sequence modeling framework\nby directly predicting transcript text from audio. This paper extends this\napproach in two ways. First, we demonstrate that a straightforward recurrent\nneural network architecture can achieve a high level of accuracy. Second, we\npropose and evaluate a modified prefix-search decoding algorithm. This approach\nto decoding enables first-pass speech recognition with a language model,\ncompletely unaided by the cumbersome infrastructure of HMM-based systems.\nExperiments on the Wall Street Journal corpus demonstrate fairly competitive\nword error rates, and the importance of bi-directional network recurrence.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 22:40:21 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 20:21:52 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Hannun", "Awni Y.", ""], ["Maas", "Andrew L.", ""], ["Jurafsky", "Daniel", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1408.3153", "submitter": "L. Amber Wilcox-O'Hearn", "authors": "L. Amber Wilcox-O'Hearn", "title": "Detection is the central problem in real-word spelling correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Real-word spelling correction differs from non-word spelling correction in\nits aims and its challenges. Here we show that the central problem in real-word\nspelling correction is detection. Methods from non-word spelling correction,\nwhich focus instead on selection among candidate corrections, do not address\ndetection adequately, because detection is either assumed in advance or heavily\nconstrained. As we demonstrate in this paper, merely discriminating between the\nintended word and a random close variation of it within the context of a\nsentence is a task that can be performed with high accuracy using\nstraightforward models. Trigram models are sufficient in almost all cases. The\ndifficulty comes when every word in the sentence is a potential error, with a\nlarge set of possible candidate corrections. Despite their strengths, trigram\nmodels cannot reliably find true errors without introducing many more, at least\nnot when used in the obvious sequential way without added structure. The\ndetection task exposes weakness not visible in the selection task.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 22:09:23 GMT"}, {"version": "v2", "created": "Fri, 15 Aug 2014 15:06:38 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Wilcox-O'Hearn", "L. Amber", ""]]}, {"id": "1408.3456", "submitter": "Felix Hill Mr", "authors": "Felix Hill, Roi Reichart and Anna Korhonen", "title": "SimLex-999: Evaluating Semantic Models with (Genuine) Similarity\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SimLex-999, a gold standard resource for evaluating distributional\nsemantic models that improves on existing resources in several important ways.\nFirst, in contrast to gold standards such as WordSim-353 and MEN, it explicitly\nquantifies similarity rather than association or relatedness, so that pairs of\nentities that are associated but not actually similar [Freud, psychology] have\na low rating. We show that, via this focus on similarity, SimLex-999\nincentivizes the development of models with a different, and arguably wider\nrange of applications than those which reflect conceptual association. Second,\nSimLex-999 contains a range of concrete and abstract adjective, noun and verb\npairs, together with an independent rating of concreteness and (free)\nassociation strength for each pair. This diversity enables fine-grained\nanalyses of the performance of models on concepts of different types, and\nconsequently greater insight into how architectures can be improved. Further,\nunlike existing gold standard evaluations, for which automatic approaches have\nreached or surpassed the inter-annotator agreement ceiling, state-of-the-art\nmodels perform well below this ceiling on SimLex-999. There is therefore plenty\nof scope for SimLex-999 to quantify future improvements to distributional\nsemantic models, guiding the development of the next generation of\nrepresentation-learning architectures.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 01:59:29 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Hill", "Felix", ""], ["Reichart", "Roi", ""], ["Korhonen", "Anna", ""]]}, {"id": "1408.3731", "submitter": "Micha{\\l} {\\L}opuszy\\'nski", "authors": "Micha{\\l} Jungiewicz, Micha{\\l} {\\L}opuszy\\'nski", "title": "Unsupervised Keyword Extraction from Polish Legal Texts", "comments": null, "journal-ref": "Lecture Notes in Computer Science, Volume 8686, Springer 2014, pp\n  65-70", "doi": "10.1007/978-3-319-10888-9_7", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an application of the recently proposed unsupervised\nkeyword extraction algorithm RAKE to a corpus of Polish legal texts from the\nfield of public procurement. RAKE is essentially a language and domain\nindependent method. Its only language-specific input is a stoplist containing a\nset of non-content words. The performance of the method heavily depends on the\nchoice of such a stoplist, which should be domain adopted. Therefore, we\ncomplement RAKE algorithm with an automatic approach to selecting non-content\nwords, which is based on the statistical properties of term distribution.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 10:09:28 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 14:38:44 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Jungiewicz", "Micha\u0142", ""], ["\u0141opuszy\u0144ski", "Micha\u0142", ""]]}, {"id": "1408.3829", "submitter": "Richa Sharma", "authors": "Richa Sharma, Shweta Nigam, Rekha Jain", "title": "Opinion mining of movie reviews at document level", "comments": "International Journal on Information Theory (IJIT), Vol.3, No.3, July\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The whole world is changed rapidly and using the current technologies\nInternet becomes an essential need for everyone. Web is used in every field.\nMost of the people use web for a common purpose like online shopping, chatting\netc. During an online shopping large number of reviews/opinions are given by\nthe users that reflect whether the product is good or bad. These reviews need\nto be explored, analyse and organized for better decision making. Opinion\nMining is a natural language processing task that deals with finding\norientation of opinion in a piece of text with respect to a topic. In this\npaper a document based opinion mining system is proposed that classify the\ndocuments as positive, negative and neutral. Negation is also handled in the\nproposed system. Experimental results using reviews of movies show the\neffectiveness of the system.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 14:56:36 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Sharma", "Richa", ""], ["Nigam", "Shweta", ""], ["Jain", "Rekha", ""]]}, {"id": "1408.3934", "submitter": "Alejandro Mosquera", "authors": "Alejandro Mosquera, Lamine Aouad, Slawomir Grzonkowski, Dylan Morss", "title": "On Detecting Messaging Abuse in Short Text Messages using Linguistic and\n  Behavioral patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of short text messages in social media and instant messaging has\nbecome a popular communication channel during the last years. This rising\npopularity has caused an increment in messaging threats such as spam, phishing\nor malware as well as other threats. The processing of these short text message\nthreats could pose additional challenges such as the presence of lexical\nvariants, SMS-like contractions or advanced obfuscations which can degrade the\nperformance of traditional filtering solutions. By using a real-world SMS data\nset from a large telecommunications operator from the US and a social media\ncorpus, in this paper we analyze the effectiveness of machine learning filters\nbased on linguistic and behavioral patterns in order to detect short text spam\nand abusive users in the network. We have also explored different ways to deal\nwith short text message challenges such as tokenization and entity detection by\nusing text normalization and substring clustering techniques. The obtained\nresults show the validity of the proposed solution by enhancing baseline\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 08:47:55 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Mosquera", "Alejandro", ""], ["Aouad", "Lamine", ""], ["Grzonkowski", "Slawomir", ""], ["Morss", "Dylan", ""]]}, {"id": "1408.4245", "submitter": "Dmitry Ustalov", "authors": "Dmitry Ustalov", "title": "Towards crowdsourcing and cooperation in linguistic resources", "comments": "11 pages, 2 figures, accepted to RuSSIR 2014, the final publication\n  is available at link.springer.com", "journal-ref": null, "doi": "10.1007/978-3-319-25485-2_14", "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic resources can be populated with data through the use of such\napproaches as crowdsourcing and gamification when motivated people are\ninvolved. However, current crowdsourcing genre taxonomies lack the concept of\ncooperation, which is the principal element of modern video games and may\npotentially drive the annotators' interest. This survey on crowdsourcing\ntaxonomies and cooperation in linguistic resources provides recommendations on\nusing cooperation in existent genres of crowdsourcing and an evidence of the\nefficiency of cooperation using a popular Russian linguistic resource created\nthrough crowdsourcing as an example.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 08:32:49 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 12:23:03 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Ustalov", "Dmitry", ""]]}, {"id": "1408.4753", "submitter": "Phillip Alday", "authors": "Phillip M. Alday", "title": "Be Careful When Assuming the Obvious: Commentary on \"The placement of\n  the head that minimizes online memory: a complex systems approach\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ferrer-i-Cancho (2015) presents a mathematical model of both the synchronic\nand diachronic nature of word order based on the assumption that memory costs\nare a never decreasing function of distance and a few very general linguistic\nassumptions. However, even these minimal and seemingly obvious assumptions are\nnot as safe as they appear in light of recent typological and psycholinguistic\nevidence. The interaction of word order and memory has further depths to be\nexplored.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 18:40:13 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Alday", "Phillip M.", ""]]}, {"id": "1408.5403", "submitter": "Peilei Liu", "authors": "Peilei Liu, Ting Wang", "title": "Neural Mechanism of Language", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is based on our previous work on neural coding. It is a\nself-organized model supported by existing evidences. Firstly, we briefly\nintroduce this model in this paper, and then we explain the neural mechanism of\nlanguage and reasoning with it. Moreover, we find that the position of an area\ndetermines its importance. Specifically, language relevant areas are in the\ncapital position of the cortical kingdom. Therefore they are closely related\nwith autonomous consciousness and working memories. In essence, language is a\nminiature of the real world. Briefly, this paper would like to bridge the gap\nbetween molecule mechanism of neurons and advanced functions such as language\nand reasoning.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 12:15:54 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Liu", "Peilei", ""], ["Wang", "Ting", ""]]}, {"id": "1408.5427", "submitter": "Carl Meyer Dr.", "authors": "Daniel Godfrey, Caley Johns, Carl Meyer, Shaina Race, Carol Sadek", "title": "A Case Study in Text Mining: Interpreting Twitter Data From World Cup\n  Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is a field of data analysis that extracts underlying\npatterns in data. One application of cluster analysis is in text-mining, the\nanalysis of large collections of text to find similarities between documents.\nWe used a collection of about 30,000 tweets extracted from Twitter just before\nthe World Cup started. A common problem with real world text data is the\npresence of linguistic noise. In our case it would be extraneous tweets that\nare unrelated to dominant themes. To combat this problem, we created an\nalgorithm that combined the DBSCAN algorithm and a consensus matrix. This way\nwe are left with the tweets that are related to those dominant themes. We then\nused cluster analysis to find those topics that the tweets describe. We\nclustered the tweets using k-means, a commonly used clustering algorithm, and\nNon-Negative Matrix Factorization (NMF) and compared the results. The two\nalgorithms gave similar results, but NMF proved to be faster and provided more\neasily interpreted results. We explored our results using two visualization\ntools, Gephi and Wordle.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 17:58:33 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Godfrey", "Daniel", ""], ["Johns", "Caley", ""], ["Meyer", "Carl", ""], ["Race", "Shaina", ""], ["Sadek", "Carol", ""]]}, {"id": "1408.5882", "submitter": "Yoon Kim", "authors": "Yoon Kim", "title": "Convolutional Neural Networks for Sentence Classification", "comments": "To appear in EMNLP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on a series of experiments with convolutional neural networks (CNN)\ntrained on top of pre-trained word vectors for sentence-level classification\ntasks. We show that a simple CNN with little hyperparameter tuning and static\nvectors achieves excellent results on multiple benchmarks. Learning\ntask-specific vectors through fine-tuning offers further gains in performance.\nWe additionally propose a simple modification to the architecture to allow for\nthe use of both task-specific and static vectors. The CNN models discussed\nherein improve upon the state of the art on 4 out of 7 tasks, which include\nsentiment analysis and question classification.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 19:48:04 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 03:09:02 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Kim", "Yoon", ""]]}, {"id": "1408.6179", "submitter": "Dmitrijs Milajevs", "authors": "Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, Matthew\n  Purver", "title": "Evaluating Neural Word Representations in Tensor-Based Compositional\n  Settings", "comments": "To be published in EMNLP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a comparative study between neural word representations and\ntraditional vector spaces based on co-occurrence counts, in a number of\ncompositional tasks. We use three different semantic spaces and implement seven\ntensor-based compositional models, which we then test (together with simpler\nadditive and multiplicative approaches) in tasks involving verb disambiguation\nand sentence similarity. To check their scalability, we additionally evaluate\nthe spaces using simple compositional methods on larger-scale tasks with less\nconstrained language: paraphrase detection and dialogue act tagging. In the\nmore constrained tasks, co-occurrence vectors are competitive, although choice\nof compositional method is important; on the larger-scale tasks, they are\noutperformed by neural word embeddings, which show robust, stable performance\nacross the tasks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 16:28:21 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Milajevs", "Dmitrijs", ""], ["Kartsaklis", "Dimitri", ""], ["Sadrzadeh", "Mehrnoosh", ""], ["Purver", "Matthew", ""]]}, {"id": "1408.6181", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis, Nal Kalchbrenner, Mehrnoosh Sadrzadeh", "title": "Resolving Lexical Ambiguity in Tensor Regression Models of Meaning", "comments": null, "journal-ref": "Proceedings of ACL 2014, Vol. 2:Short Papers, pp:212-217", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a method for improving tensor-based compositional\ndistributional models of meaning by the addition of an explicit disambiguation\nstep prior to composition. In contrast with previous research where this\nhypothesis has been successfully tested against relatively simple compositional\nmodels, in our work we use a robust model trained with linear regression. The\nresults we get in two experiments show the superiority of the prior\ndisambiguation method and suggest that the effectiveness of this approach is\nmodel-independent.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 16:43:30 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Kartsaklis", "Dimitri", ""], ["Kalchbrenner", "Nal", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1408.6418", "submitter": "Andrei Barbu", "authors": "Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven\n  Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy,\n  Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell\n  Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang", "title": "Video In Sentences Out", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-102-112", "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that produces sentential descriptions of video: who did\nwhat to whom, and where and how they did it. Action class is rendered as a\nverb, participant objects as noun phrases, properties of those objects as\nadjectival modifiers in those noun phrases, spatial relations between those\nparticipants as prepositional phrases, and characteristics of the event as\nprepositional-phrase adjuncts and adverbial modifiers. Extracting the\ninformation needed to render these linguistic entities requires an approach to\nevent recognition that recovers object tracks, the trackto-role assignments,\nand changing body posture.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:43:12 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Barbu", "Andrei", ""], ["Bridge", "Alexander", ""], ["Burchill", "Zachary", ""], ["Coroian", "Dan", ""], ["Dickinson", "Sven", ""], ["Fidler", "Sanja", ""], ["Michaux", "Aaron", ""], ["Mussman", "Sam", ""], ["Narayanaswamy", "Siddharth", ""], ["Salvi", "Dhaval", ""], ["Schmidt", "Lara", ""], ["Shangguan", "Jiangnan", ""], ["Siskind", "Jeffrey Mark", ""], ["Waggoner", "Jarrell", ""], ["Wang", "Song", ""], ["Wei", "Jinlian", ""], ["Yin", "Yifan", ""], ["Zhang", "Zhiqi", ""]]}, {"id": "1408.6746", "submitter": "Slobodan Beliga", "authors": "Slobodan Beliga, Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c", "title": "Non-Standard Words as Features for Text Categorization", "comments": "IEEE 37th International Convention on Information and Communication\n  Technology, Electronics and Microelectronics (MIPRO 2014), pp. 1415-1419,\n  2014", "journal-ref": "IEEE 37th International Convention on Information and\n  Communication Technology, Electronics and Microelectronics (MIPRO 2014), pp.\n  1415-1419, 2014", "doi": "10.1109/MIPRO.2014.6859744", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents categorization of Croatian texts using Non-Standard Words\n(NSW) as features. Non-Standard Words are: numbers, dates, acronyms,\nabbreviations, currency, etc. NSWs in Croatian language are determined\naccording to Croatian NSW taxonomy. For the purpose of this research, 390 text\ndocuments were collected and formed the SKIPEZ collection with 6 classes:\nofficial, literary, informative, popular, educational and scientific. Text\ncategorization experiment was conducted on three different representations of\nthe SKIPEZ collection: in the first representation, the frequencies of NSWs are\nused as features; in the second representation, the statistic measures of NSWs\n(variance, coefficient of variation, standard deviation, etc.) are used as\nfeatures; while the third representation combines the first two feature sets.\nNaive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms\nwere used in text categorization experiments. The best categorization results\nare achieved using the first feature set (NSW frequencies) with the\ncategorization accuracy of 87%. This suggests that the NSWs should be\nconsidered as features in highly inflectional languages, such as Croatian. NSW\nbased features reduce the dimensionality of the feature space without standard\nlemmatization procedures, and therefore the bag-of-NSWs should be considered\nfor further Croatian texts categorization experiments.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 15:06:50 GMT"}, {"version": "v2", "created": "Sun, 16 Nov 2014 21:33:22 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Beliga", "Slobodan", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""]]}, {"id": "1408.6762", "submitter": "Nikolaos Polatidis Mr", "authors": "Nikolaos Polatidis", "title": "Chatbot for admissions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The communication of potential students with a university department is\nperformed manually and it is a very time consuming procedure. The opportunity\nto communicate with on a one-to-one basis is highly valued. However with many\nhundreds of applications each year, one-to-one conversations are not feasible\nin most cases. The communication will require a member of academic staff to\nexpend several hours to find suitable answers and contact each student. It\nwould be useful to reduce his costs and time.\n  The project aims to reduce the burden on the head of admissions, and\npotentially other users, by developing a convincing chatbot. A suitable\nalgorithm must be devised to search through the set of data and find a\npotential answer. The program then replies to the user and provides a relevant\nweb link if the user is not satisfied by the answer. Furthermore a web\ninterface is provided for both users and an administrator.\n  The achievements of the project can be summarised as follows. To prepare the\nbackground of the project a literature review was undertaken, together with an\ninvestigation of existing tools, and consultation with the head of admissions.\nThe requirements of the system were established and a range of algorithms and\ntools were investigated, including keyword and template matching. An algorithm\nthat combines keyword matching with string similarity has been developed. A\nusable system using the proposed algorithm has been implemented. The system was\nevaluated by keeping logs of questions and answers and by feedback received by\npotential students that used it.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 16:01:10 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Polatidis", "Nikolaos", ""]]}, {"id": "1408.6788", "submitter": "Julian Hough", "authors": "Julian Hough and Matthew Purver", "title": "Strongly Incremental Repair Detection", "comments": "12 pages, 6 figures, EMNLP conference long paper 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present STIR (STrongly Incremental Repair detection), a system that\ndetects speech repairs and edit terms on transcripts incrementally with minimal\nlatency. STIR uses information-theoretic measures from n-gram models as its\nprincipal decision features in a pipeline of classifiers detecting the\ndifferent stages of repairs. Results on the Switchboard disfluency tagged\ncorpus show utterance-final accuracy on a par with state-of-the-art incremental\nrepair detection methods, but with better incremental accuracy, faster\ntime-to-detection and less computational overhead. We evaluate its performance\nusing incremental metrics and propose new repair processing evaluation\nstandards.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 17:29:55 GMT"}, {"version": "v2", "created": "Fri, 29 Aug 2014 08:44:09 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Hough", "Julian", ""], ["Purver", "Matthew", ""]]}, {"id": "1408.6988", "submitter": "Zongcheng Ji", "authors": "Zongcheng Ji, Zhengdong Lu, Hang Li", "title": "An Information Retrieval Approach to Short Text Conversation", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human computer conversation is regarded as one of the most difficult problems\nin artificial intelligence. In this paper, we address one of its key\nsub-problems, referred to as short text conversation, in which given a message\nfrom human, the computer returns a reasonable response to the message. We\nleverage the vast amount of short conversation data available on social media\nto study the issue. We propose formalizing short text conversation as a search\nproblem at the first step, and employing state-of-the-art information retrieval\n(IR) techniques to carry out the task. We investigate the significance as well\nas the limitation of the IR approach. Our experiments demonstrate that the\nretrieval-based model can make the system behave rather \"intelligently\", when\ncombined with a huge repository of conversation data from social media.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 12:04:15 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Ji", "Zongcheng", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""]]}]