[{"id": "0811.0453", "submitter": "Cynthia Wagner CW", "authors": "Cynthia Wagner, and Christoph Schommer", "title": "CoZo+ - A Content Zoning Engine for textual documents", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content zoning can be understood as a segmentation of textual documents into\nzones. This is inspired by [6] who initially proposed an approach for the\nargumentative zoning of textual documents. With the prototypical CoZo+ engine,\nwe focus on content zoning towards an automatic processing of textual streams\nwhile considering only the actors as the zones. We gain information that can be\nused to realize an automatic recognition of content for pre-defined actors. We\nunderstand CoZo+ as a necessary pre-step towards an automatic generation of\nsummaries and to make intellectual ownership of documents detectable.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2008 09:08:32 GMT"}], "update_date": "2008-11-05", "authors_parsed": [["Wagner", "Cynthia", ""], ["Schommer", "Christoph", ""]]}, {"id": "0811.0579", "submitter": "Gilles Serasset", "authors": "Gilles s\\'erasset (IMAG, Clips - Imag, Lig), Christian Boitet (IMAG,\n  Clips - Imag, Lig)", "title": "UNL-French deconversion as transfer & generation from an interlingua\n  with possible quality enhancement through offline human interaction", "comments": null, "journal-ref": "MACHINE TRANSLATION SUMMIT VII, Singapour : Singapour (1999)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the architecture of the UNL-French deconverter, which \"generates\"\nfrom the UNL interlingua by first\"localizing\" the UNL form for French, within\nUNL, and then applying slightly adapted but classical transfer and generation\ntechniques, implemented in GETA's Ariane-G5 environment, supplemented by some\nUNL-specific tools. Online interaction can be used during deconversion to\nenhance output quality and is now used for development purposes. We show how\ninteraction could be delayed and embedded in the postedition phase, which would\nthen interact not directly with the output text, but indirectly with several\ncomponents of the deconverter. Interacting online or offline can improve the\nquality not only of the utterance at hand, but also of the utterances processed\nlater, as various preferences may be automatically changed to let the\ndeconverter \"learn\".\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2008 19:31:58 GMT"}], "update_date": "2008-11-05", "authors_parsed": [["s\u00e9rasset", "Gilles", "", "IMAG, Clips - Imag, Lig"], ["Boitet", "Christian", "", "IMAG,\n  Clips - Imag, Lig"]]}, {"id": "0811.1260", "submitter": "R K Bisht", "authors": "Raj Kishor Bisht, H.S.Dhami", "title": "The Application of Fuzzy Logic to Collocation Extraction", "comments": "13 pages,5 figures,5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collocations are important for many tasks of Natural language processing such\nas information retrieval, machine translation, computational lexicography etc.\nSo far many statistical methods have been used for collocation extraction.\nAlmost all the methods form a classical crisp set of collocation. We propose a\nfuzzy logic approach of collocation extraction to form a fuzzy set of\ncollocations in which each word combination has a certain grade of membership\nfor being collocation. Fuzzy logic provides an easy way to express natural\nlanguage into fuzzy logic rules. Two existing methods; Mutual information and\nt-test have been utilized for the input of the fuzzy inference system. The\nresulting membership function could be easily seen and demonstrated. To show\nthe utility of the fuzzy logic some word pairs have been examined as an\nexample. The working data has been based on a corpus of about one million words\ncontained in different novels constituting project Gutenberg available on\nwww.gutenberg.org. The proposed method has all the advantages of the two\nmethods, while overcoming their drawbacks. Hence it provides a better result\nthan the two methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2008 10:44:43 GMT"}], "update_date": "2008-11-11", "authors_parsed": [["Bisht", "Raj Kishor", ""], ["Dhami", "H. S.", ""]]}, {"id": "0811.4717", "submitter": "Daniel Racoceanu", "authors": "Roxana Teodorescu (UPT, LAB), Daniel Racoceanu (LAB, IPAAL), Wee-Kheng\n  Leow (IPAAL, NUS), Vladimir Cretu (UPT)", "title": "Prospective Study for Semantic Inter-Media Fusion in Content-Based\n  Medical Image Retrieval", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": "Onco-media Teodorescu 2008", "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important challenge in modern Content-Based Medical Image Retrieval\n(CBMIR) approaches is represented by the semantic gap, related to the\ncomplexity of the medical knowledge. Among the methods that are able to close\nthis gap in CBMIR, the use of medical thesauri/ontologies has interesting\nperspectives due to the possibility of accessing on-line updated relevant\nwebservices and to extract real-time medical semantic structured information.\nThe CBMIR approach proposed in this paper uses the Unified Medical Language\nSystem's (UMLS) Metathesaurus to perform a semantic indexing and fusion of\nmedical media. This fusion operates before the query processing (retrieval) and\nworks at an UMLS-compliant conceptual indexing level. Our purpose is to study\nvarious techniques related to semantic data alignment, preprocessing, fusion,\nclustering and retrieval, by evaluating the various techniques and highlighting\nfuture research directions. The alignment and the preprocessing are based on\npartial text/image retrieval feedback and on the data structure. We analyze\nvarious probabilistic, fuzzy and evidence-based approaches for the fusion\nprocess and different similarity functions for the retrieval process. All the\nproposed methods are evaluated on the Cross Language Evaluation Forum's (CLEF)\nmedical image retrieval benchmark, by focusing also on a more homogeneous\ncomponent medical image database: the Pathology Education Instructional\nResource (PEIR).\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2008 13:30:23 GMT"}], "update_date": "2008-12-01", "authors_parsed": [["Teodorescu", "Roxana", "", "UPT, LAB"], ["Racoceanu", "Daniel", "", "LAB, IPAAL"], ["Leow", "Wee-Kheng", "", "IPAAL, NUS"], ["Cretu", "Vladimir", "", "UPT"]]}]