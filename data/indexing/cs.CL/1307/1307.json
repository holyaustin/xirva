[{"id": "1307.0087", "submitter": "Fabrizio M.A. Lolli", "authors": "Fabrizio M.A. Lolli", "title": "Semantics and pragmatics in actual software applications and in web\n  search engines: exploring innovations", "comments": null, "journal-ref": "International Symposium on Language and Communication: Exploring\n  Novelties, vol.3, 955-962. IICS, 2013, Turkey", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While new ways to use the Semantic Web are developed every week, which allow\nthe user to find information on web more accurately - for example in search\nengines - some sophisticated pragmatic tools are becoming more important - for\nexample in web interfaces known as Social Intelligence, or in the most famous\nSiri by Apple. The work aims to analyze whether and where we can identify the\nboundary between semantics and pragmatics in the software used by analyzed\nsystems. examining how the linguistic disciplines are fundamental in their\nprogress. Is it possible to assume that the tools of social intelligence have a\npragmatic approach to the questions of the user, or it is just a use of a very\nrich vocabulary, with the use of semantic tools?\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 10:40:59 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Lolli", "Fabrizio M. A.", ""]]}, {"id": "1307.0261", "submitter": "Bhavana Dalvi", "authors": "Bhavana Dalvi, William W. Cohen, and Jamie Callan", "title": "WebSets: Extracting Sets of Entities from the Web Using Unsupervised\n  Information Extraction", "comments": "10 pages; International Conference on Web Search and Data Mining 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a open-domain information extraction method for extracting\nconcept-instance pairs from an HTML corpus. Most earlier approaches to this\nproblem rely on combining clusters of distributionally similar terms and\nconcept-instance pairs obtained with Hearst patterns. In contrast, our method\nrelies on a novel approach for clustering terms found in HTML tables, and then\nassigning concept names to these clusters using Hearst patterns. The method can\nbe efficiently applied to a large corpus, and experimental results on several\ndatasets show that our method can accurately extract large numbers of\nconcept-instance pairs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 02:49:08 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Dalvi", "Bhavana", ""], ["Cohen", "William W.", ""], ["Callan", "Jamie", ""]]}, {"id": "1307.0596", "submitter": "Om Damani", "authors": "Om P. Damani", "title": "Improving Pointwise Mutual Information (PMI) by Incorporating\n  Significant Co-occurrence", "comments": "To appear in the proceedings of 17th Conference on Computational\n  Natural Language Learning, CoNLL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a new co-occurrence based word association measure by incorporating\nthe concept of significant cooccurrence in the popular word association measure\nPointwise Mutual Information (PMI). By extensive experiments with a large\nnumber of publicly available datasets we show that the newly introduced measure\nperforms better than other co-occurrence based measures and despite being\nresource-light, compares well with the best known resource-heavy distributional\nsimilarity and knowledge based word association measures. We investigate the\nsource of this performance improvement and find that of the two types of\nsignificant co-occurrence - corpus-level and document-level, the concept of\ncorpus level significance combined with the use of document counts in place of\nword counts is responsible for all the performance gains observed. The concept\nof document level significance is not helpful for PMI adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 06:25:51 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Damani", "Om P.", ""]]}, {"id": "1307.1662", "submitter": "Rami Al-Rfou", "authors": "Rami Al-Rfou, Bryan Perozzi, Steven Skiena", "title": "Polyglot: Distributed Word Representations for Multilingual NLP", "comments": "10 pages, 2 figures, Proceedings of Conference on Computational\n  Natural Language Learning CoNLL'2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed word representations (word embeddings) have recently contributed\nto competitive performance in language modeling and several NLP tasks. In this\nwork, we train word embeddings for more than 100 languages using their\ncorresponding Wikipedias. We quantitatively demonstrate the utility of our word\nembeddings by using them as the sole features for training a part of speech\ntagger for a subset of these languages. We find their performance to be\ncompetitive with near state-of-art methods in English, Danish and Swedish.\nMoreover, we investigate the semantic features captured by these embeddings\nthrough the proximity of word groupings. We will release these embeddings\npublicly to help researchers in the development and enhancement of multilingual\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 16:52:09 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 17:31:33 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Al-Rfou", "Rami", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1307.1872", "submitter": "Ibrahim Sabek", "authors": "Ibrahim Sabek, Noha A. Yousri, Nagwa Elmakky and Mona Habib", "title": "Intelligent Hybrid Man-Machine Translation Quality Estimation", "comments": "8 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring evaluation scores based on human judgments is invaluable compared\nto using current evaluation metrics which are not suitable for real-time\napplications e.g. post-editing. However, these judgments are much more\nexpensive to collect especially from expert translators, compared to evaluation\nbased on indicators contrasting source and translation texts. This work\nintroduces a novel approach for quality estimation by combining learnt\nconfidence scores from a probabilistic inference model based on human\njudgments, with selective linguistic features-based scores, where the proposed\ninference model infers the credibility of given human ranks to solve the\nscarcity and inconsistency issues of human judgments. Experimental results,\nusing challenging language-pairs, demonstrate improvement in correlation with\nhuman judgments over traditional evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2013 15:04:11 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Sabek", "Ibrahim", ""], ["Yousri", "Noha A.", ""], ["Elmakky", "Nagwa", ""], ["Habib", "Mona", ""]]}, {"id": "1307.3040", "submitter": "Mehul Bhatt", "authors": "Mehul Bhatt", "title": "Between Sense and Sensibility: Declarative narrativisation of mental\n  models as a basis and benchmark for visuo-spatial cognition and computation\n  focussed collaborative cognitive systems", "comments": "5 pages, research statement summarising recent publications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What lies between `\\emph{sensing}' and `\\emph{sensibility}'? In other words,\nwhat kind of cognitive processes mediate sensing capability, and the formation\nof sensible impressions ---e.g., abstractions, analogies, hypotheses and theory\nformation, beliefs and their revision, argument formation--- in domain-specific\nproblem solving, or in regular activities of everyday living, working and\nsimply going around in the environment? How can knowledge and reasoning about\nsuch capabilities, as exhibited by humans in particular problem contexts, be\nused as a model and benchmark for the development of collaborative cognitive\n(interaction) systems concerned with human assistance, assurance, and\nempowerment?\n  We pose these questions in the context of a range of assistive technologies\nconcerned with \\emph{visuo-spatial perception and cognition} tasks encompassing\naspects such as commonsense, creativity, and the application of specialist\ndomain knowledge and problem-solving thought processes. Assistive technologies\nbeing considered include: (a) human activity interpretation; (b) high-level\ncognitive rovotics; (c) people-centred creative design in domains such as\narchitecture & digital media creation, and (d) qualitative analyses geographic\ninformation systems. Computational narratives not only provide a rich cognitive\nbasis, but they also serve as a benchmark of functional performance in our\ndevelopment of computational cognitive assistance systems. We posit that\ncomputational narrativisation pertaining to space, actions, and change provides\na useful model of \\emph{visual} and \\emph{spatio-temporal thinking} within a\nwide-range of problem-solving tasks and application areas where collaborative\ncognitive systems could serve an assistive and empowering function.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 10:01:29 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 10:22:29 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Bhatt", "Mehul", ""]]}, {"id": "1307.3310", "submitter": "Juhi Ameta", "authors": "Juhi Ameta, Nisheeth Joshi and Iti Mathur", "title": "Improving the quality of Gujarati-Hindi Machine Translation through\n  part-of-speech tagging and stemmer-assisted transliteration", "comments": "6 pages; June 2013,\n  url-http://airccse.org/journal/ijnlc/papers/2313ijnlc05.pdf", "journal-ref": null, "doi": "10.5121/ijnlc.2013.2305", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Translation for Indian languages is an emerging research area.\nTransliteration is one such module that we design while designing a translation\nsystem. Transliteration means mapping of source language text into the target\nlanguage. Simple mapping decreases the efficiency of overall translation\nsystem. We propose the use of stemming and part-of-speech tagging for\ntransliteration. The effectiveness of translation can be improved if we use\npart-of-speech tagging and stemming assisted transliteration.We have shown that\nmuch of the content in Gujarati gets transliterated while being processed for\ntranslation to Hindi language.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 03:05:29 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Ameta", "Juhi", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1307.3336", "submitter": "Arti Buche Ms", "authors": "Arti Buche, Dr. M. B. Chandak and Akshay Zadgaonkar", "title": "Opinion Mining and Analysis: A survey", "comments": "10 pages", "journal-ref": "IJNLC Vol. 2, No.3, June 2013", "doi": "10.5121/ijnlc.2013.2304", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The current research is focusing on the area of Opinion Mining also called as\nsentiment analysis due to sheer volume of opinion rich web resources such as\ndiscussion forums, review sites and blogs are available in digital form. One\nimportant problem in sentiment analysis of product reviews is to produce\nsummary of opinions based on product features. We have surveyed and analyzed in\nthis paper, various techniques that have been developed for the key tasks of\nopinion mining. We have provided an overall picture of what is involved in\ndeveloping a software system for opinion mining on the basis of our survey and\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 06:20:36 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Buche", "Arti", ""], ["Chandak", "Dr. M. B.", ""], ["Zadgaonkar", "Akshay", ""]]}, {"id": "1307.3489", "submitter": "Bilel Ben Ali", "authors": "Bilel Ben Ali and Fethi Jarray", "title": "Genetic approach for arabic part of speech tagging", "comments": "12 pages, 8 figures", "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  2, No.3, June 2013", "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing number of textual resources available, the ability to\nunderstand them becomes critical. An essential first step in understanding\nthese sources is the ability to identify the part of speech in each sentence.\nArabic is a morphologically rich language, wich presents a challenge for part\nof speech tagging. In this paper, our goal is to propose, improve and implement\na part of speech tagger based on a genetic alorithm. The accuracy obtained with\nthis method is comparable to that of other probabilistic approaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 10:21:01 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Ali", "Bilel Ben", ""], ["Jarray", "Fethi", ""]]}, {"id": "1307.4038", "submitter": "Bob Coecke", "authors": "Bob Coecke", "title": "An alternative Gospel of structure: order, composition, processes", "comments": "Introductory chapter to C. Heunen, M. Sadrzadeh, and E. Grefenstette.\n  Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse.\n  Oxford University Press, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey some basic mathematical structures, which arguably are more\nprimitive than the structures taught at school. These structures are orders,\nwith or without composition, and (symmetric) monoidal categories. We list\nseveral `real life' incarnations of each of these. This paper also serves as an\nintroduction to these structures and their current and potentially future uses\nin linguistics, physics and knowledge representation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 18:21:51 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Coecke", "Bob", ""]]}, {"id": "1307.4299", "submitter": "Nisheeth Joshi", "authors": "Jyoti Singh, Nisheeth Joshi, Iti Mathur", "title": "Part of Speech Tagging of Marathi Text Using Trigram Method", "comments": "International Journal of Advanced Information Technology (IJAIT) Vol.\n  3, No.2, April2013", "journal-ref": null, "doi": "10.5121/ijait.2013.3203", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a Marathi part of speech tagger. It is a\nmorphologically rich language. It is spoken by the native people of\nMaharashtra. The general approach used for development of tagger is statistical\nusing trigram Method. The main concept of trigram is to explore the most likely\nPOS for a token based on given information of previous two tags by calculating\nprobabilities to determine which is the best sequence of a tag. In this paper\nwe show the development of the tagger. Moreover we have also shown the\nevaluation done.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 15:59:12 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Singh", "Jyoti", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1307.4300", "submitter": "Nisheeth Joshi", "authors": "Deepti Bhalla, Nisheeth Joshi and Iti Mathur", "title": "Rule Based Transliteration Scheme for English to Punjabi", "comments": "International Journal on Natural Language Computing (IJNLC) Vol. 2,\n  No.2, April 2013", "journal-ref": null, "doi": "10.5121/ijnlc.2013.2207", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Transliteration has come out to be an emerging and a very important\nresearch area in the field of machine translation. Transliteration basically\naims to preserve the phonological structure of words. Proper transliteration of\nname entities plays a very significant role in improving the quality of machine\ntranslation. In this paper we are doing machine transliteration for\nEnglish-Punjabi language pair using rule based approach. We have constructed\nsome rules for syllabification. Syllabification is the process to extract or\nseparate the syllable from the words. In this we are calculating the\nprobabilities for name entities (Proper names and location). For those words\nwhich do not come under the category of name entities, separate probabilities\nare being calculated by using relative frequency through a statistical machine\ntranslation toolkit known as MOSES. Using these probabilities we are\ntransliterating our input text from English to Punjabi.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 15:54:43 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Bhalla", "Deepti", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1307.4879", "submitter": "Carlos Castillo", "authors": "Carlos Castillo, Gianmarco De Francisci Morales, Marcelo Mendoza,\n  Nasir Khan", "title": "Says who? Automatic Text-Based Content Analysis of Television News", "comments": "In the 2013 workshop on Mining Unstructured Big Data Using Natural\n  Language Processing, co-located with CIKM 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We perform an automatic analysis of television news programs, based on the\nclosed captions that accompany them. Specifically, we collect all the news\nbroadcasted in over 140 television channels in the US during a period of six\nmonths. We start by segmenting, processing, and annotating the closed captions\nautomatically. Next, we focus on the analysis of their linguistic style and on\nmentions of people using NLP methods. We present a series of key insights about\nnews providers, people in the news, and we discuss the biases that can be\nuncovered by automatic means. These insights are contrasted by looking at the\ndata from multiple points of view, including qualitative assessment.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 09:37:45 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2013 08:12:30 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Castillo", "Carlos", ""], ["Morales", "Gianmarco De Francisci", ""], ["Mendoza", "Marcelo", ""], ["Khan", "Nasir", ""]]}, {"id": "1307.4986", "submitter": "Diego Krivochen", "authors": "Diego Gabriel Krivochen", "title": "On the Necessity of Mixed Models: Dynamical Frustrations in the Mind", "comments": "Withdrawn by the author as the content has been superseded by more\n  recent publications", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD cs.CL math.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In the present work we will present and analyze some basic processes at the\nlocal and global level in linguistic derivations that seem to go beyond the\nlimits of Markovian or Turing-like computation, and require, in our opinion, a\nquantum processor. We will first present briefly the working hypothesis and\nthen focus on the empirical domain. At the same time, we will argue that a\nmodel appealing to only one kind of computation (be it quantum or not) is\nnecessarily insufficient, and thus both linear and non-linear formal models are\nto be invoked in order to pursue a fuller understanding of mental computations\nwithin a unified framework.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 15:44:31 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 10:01:09 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Krivochen", "Diego Gabriel", ""]]}, {"id": "1307.5336", "submitter": "Pekka Malo", "authors": "Pekka Malo, Ankur Sinha, Pyry Takala, Pekka Korhonen, Jyrki Wallenius", "title": "Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts", "comments": "To be published in Journal of the American Society for Information\n  Science and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of robo-readers to analyze news texts is an emerging technology trend\nin computational finance. In recent research, a substantial effort has been\ninvested to develop sophisticated financial polarity-lexicons that can be used\nto investigate how financial sentiments relate to future company performance.\nHowever, based on experience from other fields, where sentiment analysis is\ncommonly applied, it is well-known that the overall semantic orientation of a\nsentence may differ from the prior polarity of individual words. The objective\nof this article is to investigate how semantic orientations can be better\ndetected in financial and economic news by accommodating the overall\nphrase-structure information and domain-specific use of language. Our three\nmain contributions are: (1) establishment of a human-annotated finance\nphrase-bank, which can be used as benchmark for training and evaluating\nalternative models; (2) presentation of a technique to enhance financial\nlexicons with attributes that help to identify expected direction of events\nthat affect overall sentiment; (3) development of a linearized phrase-structure\nmodel for detecting contextual semantic orientations in financial and economic\nnews texts. The relevance of the newly added lexicon features and the benefit\nof using the proposed learning-algorithm are demonstrated in a comparative\nstudy against previously used general sentiment models as well as the popular\nword frequency models used in recent financial studies. The proposed framework\nis parsimonious and avoids the explosion in feature-space caused by the use of\nconventional n-gram features.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 20:49:06 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 19:56:38 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Malo", "Pekka", ""], ["Sinha", "Ankur", ""], ["Takala", "Pyry", ""], ["Korhonen", "Pekka", ""], ["Wallenius", "Jyrki", ""]]}, {"id": "1307.5393", "submitter": "Mahima Sharma", "authors": "Miral Patel and Prem Balani", "title": "Clustering Algorithm for Gujarati Language", "comments": null, "journal-ref": "International Journal for Scientific Research & Development Vol 1,\n  Issue 3, 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing area is still under research. But now a day it is\non platform for worldwide researchers. Natural language processing includes\nanalyzing the language based on its structure and then tagging of each word\nappropriately with its grammar base. Here we have 50,000 tagged words set and\nwe try to cluster those Gujarati words based on proposed algorithm, we have\ndefined our own algorithm for processing. Many clustering techniques are\navailable Ex. Single linkage, complete, linkage,average linkage, Hear no of\nclusters to be formed are not known, so it is all depends on the type of data\nset provided . Clustering is preprocess for stemming . Stemming is the process\nwhere root is extracted from its word. Ex. cats= cat+S, meaning. Cat: Noun and\nplural form.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 09:05:15 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Patel", "Miral", ""], ["Balani", "Prem", ""]]}, {"id": "1307.5736", "submitter": "Abinayaviji  pandiyan", "authors": "R. Sandanalakshmi, P. Abinaya Viji, M. Kiruthiga, M. Manjari, M.\n  Sharina", "title": "Speaker Independent Continuous Speech to Text Converter for Mobile\n  Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE cs.SD", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  An efficient speech to text converter for mobile application is presented in\nthis work. The prime motive is to formulate a system which would give optimum\nperformance in terms of complexity, accuracy, delay and memory requirements for\nmobile environment. The speech to text converter consists of two stages namely\nfront-end analysis and pattern recognition. The front end analysis involves\npreprocessing and feature extraction. The traditional voice activity detection\nalgorithms which track only energy cannot successfully identify potential\nspeech from input because the unwanted part of the speech also has some energy\nand appears to be speech. In the proposed system, VAD that calculates energy of\nhigh frequency part separately as zero crossing rate to differentiate noise\nfrom speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as\nfeature extraction method and Generalized Regression Neural Network is used as\nrecognizer. MFCC provides low word error rate and better feature extraction.\nNeural Network improves the accuracy. Thus a small database containing all\npossible syllable pronunciation of the user is sufficient to give recognition\naccuracy closer to 100%. Thus the proposed technique entertains realization of\nreal time speaker independent applications like mobile phones, PDAs etc.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 05:27:46 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Sandanalakshmi", "R.", ""], ["Viji", "P. Abinaya", ""], ["Kiruthiga", "M.", ""], ["Manjari", "M.", ""], ["Sharina", "M.", ""]]}, {"id": "1307.6163", "submitter": "Nisheeth Joshi", "authors": "Nisheeth Joshi, Hemant Darbari, Iti Mathur", "title": "Human and Automatic Evaluation of English-Hindi Machine Translation", "comments": "in Hindi, International Joint Rajbhasha Conference on Science and\n  Technology, Oct 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the past 60 years, Research in machine translation is going on. For the\ndevelopment in this field, a lot of new techniques are being developed each\nday. As a result, we have witnessed development of many automatic machine\ntranslators. A manager of machine translation development project needs to know\nthe performance increase/decrease, after changes have been done in his system.\nDue to this reason, a need for evaluation of machine translation systems was\nfelt. In this article, we shall present the evaluation of some machine\ntranslators. This evaluation will be done by a human evaluator and by some\nautomatic evaluation metrics, which will be done at sentence, document and\nsystem level. In the end we shall also discuss the comparison between the\nevaluations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 17:15:34 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2013 15:53:33 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Joshi", "Nisheeth", ""], ["Darbari", "Hemant", ""], ["Mathur", "Iti", ""]]}, {"id": "1307.6235", "submitter": "Anindya Kumar Biswas", "authors": "Anindya Kumar Biswas", "title": "Graphical law beneath each written natural language", "comments": "107 pages, 35 figures, all tables given. all Bethe-Peierls curves for\n  $\\gamma$ not equal to four replaced", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.gen-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study twenty four written natural languages. We draw in the log scale,\nnumber of words starting with a letter vs rank of the letter, both normalised.\nWe find that all the graphs are of the similar type. The graphs are\ntantalisingly closer to the curves of reduced magnetisation vs reduced\ntemperature for magnetic materials. We make a weak conjecture that a curve of\nmagnetisation underlies a written natural language.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 11:03:14 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2013 17:39:34 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2013 04:16:23 GMT"}, {"version": "v4", "created": "Tue, 8 Oct 2013 11:33:38 GMT"}, {"version": "v5", "created": "Tue, 21 Jan 2020 06:54:26 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Biswas", "Anindya Kumar", ""]]}, {"id": "1307.6726", "submitter": "Steven Piantadosi", "authors": "Steven T. Piantadosi and Harry Tily and Edward Gibson", "title": "Information content versus word length in natural language: A reply to\n  Ferrer-i-Cancho and Moscoso del Prado Martin [arXiv:1209.1751]", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.PR physics.data-an", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recently, Ferrer i Cancho and Moscoso del Prado Martin [arXiv:1209.1751]\nargued that an observed linear relationship between word length and average\nsurprisal (Piantadosi, Tily, & Gibson, 2011) is not evidence for communicative\nefficiency in human language. We discuss several shortcomings of their approach\nand critique: their model critically rests on inaccurate assumptions, is\nincapable of explaining key surprisal patterns in language, and is incompatible\nwith recent behavioral results. More generally, we argue that statistical\nmodels must not critically rely on assumptions that are incompatible with the\nreal system under study.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 12:53:54 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Piantadosi", "Steven T.", ""], ["Tily", "Harry", ""], ["Gibson", "Edward", ""]]}, {"id": "1307.6937", "submitter": "Rosy Madaan", "authors": "Renu Mudgal, Rosy Madaan, A.K.Sharma, Ashutosh Dixit", "title": "A Novel Architecture For Question Classification Based Indexing Scheme\n  For Efficient Question Answering", "comments": "International Journal of Computer Engineering and Applications,\n  April-June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering system can be seen as the next step in information\nretrieval, allowing users to pose question in natural language and receive\ncompact answers. For the Question answering system to be successful, research\nhas shown that the correct classification of question with respect to the\nexpected answer type is requisite. We propose a novel architecture for question\nclassification and searching in the index, maintained on the basis of expected\nanswer types, for efficient question answering. The system uses the criteria\nfor Answer Relevance Score for finding the relevance of each answer returned by\nthe system. On analysis of the proposed system, it has been found that the\nsystem has shown promising results than the existing systems based on question\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 06:57:37 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Mudgal", "Renu", ""], ["Madaan", "Rosy", ""], ["Sharma", "A. K.", ""], ["Dixit", "Ashutosh", ""]]}, {"id": "1307.7382", "submitter": "Brendan O'Connor", "authors": "Brendan O'Connor", "title": "Learning Frames from Text with an Unsupervised Latent Variable Model", "comments": "21 pages; technical report for Data Analysis Project requirement,\n  Machine Learning Department, Carnegie Mellon University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We develop a probabilistic latent-variable model to discover semantic\nframes---types of events and their participants---from corpora. We present a\nDirichlet-multinomial model in which frames are latent categories that explain\nthe linking of verb-subject-object triples, given document-level sparsity. We\nanalyze what the model learns, and compare it to FrameNet, noting it learns\nsome novel and interesting frames. This document also contains a discussion of\ninference issues, including concentration parameter learning; and a small-scale\nerror analysis of syntactic parsing accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2013 16:55:27 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["O'Connor", "Brendan", ""]]}, {"id": "1307.7973", "submitter": "Antoine Bordes", "authors": "Jason Weston, Antoine Bordes, Oksana Yakhnenko, Nicolas Usunier", "title": "Connecting Language and Knowledge Bases with Embedding Models for\n  Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for relation extraction from free text\nwhich is trained to jointly use information from the text and from existing\nknowledge. Our model is based on two scoring functions that operate by learning\nlow-dimensional embeddings of words and of entities and relationships from a\nknowledge base. We empirically show on New York Times articles aligned with\nFreebase relations that our approach is able to efficiently use the extra\ninformation provided by a large subset of Freebase data (4M entities, 23k\nrelationships) to improve over existing methods that rely on text features\nalone.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 13:37:09 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Weston", "Jason", ""], ["Bordes", "Antoine", ""], ["Yakhnenko", "Oksana", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1307.8057", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams and Robert E. Mercer", "title": "Extracting Connected Concepts from Biomedical Texts using Fog Index", "comments": "12th Conference of the Pacific Association for Computational\n  Linguistics (PACLING 2011), Kuala Lumpur, Malaysia, July 19-21, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish Fog Index (FI) as a text filter to locate the\nsentences in texts that contain connected biomedical concepts of interest. To\ndo so, we have used 24 random papers each containing four pairs of connected\nconcepts. For each pair, we categorize sentences based on whether they contain\nboth, any or none of the concepts. We then use FI to measure difficulty of the\nsentences of each category and find that sentences containing both of the\nconcepts have low readability. We rank sentences of a text according to their\nFI and select 30 percent of the most difficult sentences. We use an association\nmatrix to track the most frequent pairs of concepts in them. This matrix\nreports that the first filter produces some pairs that hold almost no\nconnections. To remove these unwanted pairs, we use the Equally Weighted\nHarmonic Mean of their Positive Predictive Value (PPV) and Sensitivity as a\nsecond filter. Experimental results demonstrate the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 17:27:29 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Shams", "Rushdi", ""], ["Mercer", "Robert E.", ""]]}, {"id": "1307.8060", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams", "title": "Extracting Information-rich Part of Texts using Text Denoising", "comments": "26th Canadian Conference on Artificial Intelligence (CAI-2013),\n  Regina, Canada, May 29-31, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to report on a novel text reduction technique,\ncalled Text Denoising, that highlights information-rich content when processing\na large volume of text data, especially from the biomedical domain. The core\nfeature of the technique, the text readability index, embodies the hypothesis\nthat complex text is more information-rich than the rest. When applied on tasks\nlike biomedical relation bearing text extraction, keyphrase indexing and\nextracting sentences describing protein interactions, it is evident that the\nreduced set of text produced by text denoising is more information-rich than\nthe rest.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 17:36:53 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Shams", "Rushdi", ""]]}, {"id": "1307.8225", "submitter": "Rosy Madaan", "authors": "Deepti Kapri, Rosy Madaan, A. K Sharma, Ashutosh Dixit", "title": "A Novel Architecture for Relevant Blog Page Identifcation", "comments": "13 Pages. International Journal of Computer Engineering and\n  Applications, June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blogs are undoubtedly the richest source of information available in\ncyberspace. Blogs can be of various natures i.e. personal blogs which contain\nposts on mixed issues or blogs can be domain specific which contains posts on\nparticular topics, this is the reason, they offer wide variety of relevant\ninformation which is often focused. A general search engine gives back a huge\ncollection of web pages which may or may not give correct answers, as web is\nthe repository of information of all kinds and a user has to go through various\ndocuments before he gets what he was originally looking for, which is a very\ntime consuming process. So, the search can be made more focused and accurate if\nit is limited to blogosphere instead of web pages. The reason being that the\nblogs are more focused in terms of information. So, User will only get related\nblogs in response to his query. These results will be then ranked according to\nour proposed method and are finally presented in front of user in descending\norder\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 05:40:59 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Kapri", "Deepti", ""], ["Madaan", "Rosy", ""], ["Sharma", "A. K", ""], ["Dixit", "Ashutosh", ""]]}]