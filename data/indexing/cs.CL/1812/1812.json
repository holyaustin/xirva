[{"id": "1812.00049", "submitter": "Rajesh Rao", "authors": "Rajesh P. N. Rao", "title": "The Indus Script and Economics. A Role for Indus Seals and Tablets in\n  Rationing and Administration of Labor", "comments": "Appeared in: Walking with the Unicorn: Social Organization and\n  Material Culture in Ancient South Asia. (Jonathan Mark Kenoyer Felicitation\n  Volume) D. Frenez, G. M. Jamison, R. W. Law, M. Vidale & R. H. Meadow (Eds.),\n  pp. 518-525, Archaeopress, Oxford, UK, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Indus script remains one of the last major undeciphered scripts of the\nancient world. We focus here on Indus inscriptions on a group of miniature\ntablets discovered by Meadow and Kenoyer in Harappa in 1997. By drawing\nparallels with proto-Elamite and proto-Cuneiform inscriptions, we explore how\nthese miniature tablets may have been used to record rations allocated to\nporters or laborers. We then show that similar inscriptions are found on stamp\nseals, leading to the potentially provocative conclusion that rather than\nsimply indicating ownership of property, Indus seals may have been used for\ngenerating tokens, tablets and sealings for repetitive economic transactions\nsuch as rations and exchange of canonical amounts of goods, grains, animals,\nand labor in a barter-based economy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 23:34:06 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Rao", "Rajesh P. N.", ""]]}, {"id": "1812.00161", "submitter": "Gyeongbok Lee", "authors": "Gyeongbok Lee, Sungdong Kim, Seung-won Hwang", "title": "QADiver: Interactive Framework for Diagnosing QA Models", "comments": "AAAI 2019 Demonstration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) extracting answers from text to the given question in\nnatural language, has been actively studied and existing models have shown a\npromise of outperforming human performance when trained and evaluated with\nSQuAD dataset. However, such performance may not be replicated in the actual\nsetting, for which we need to diagnose the cause, which is non-trivial due to\nthe complexity of model. We thus propose a web-based UI that provides how each\nmodel contributes to QA performances, by integrating visualization and analysis\ntools for model explanation. We expect this framework can help QA model\nresearchers to refine and improve their models.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 06:43:55 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Lee", "Gyeongbok", ""], ["Kim", "Sungdong", ""], ["Hwang", "Seung-won", ""]]}, {"id": "1812.00176", "submitter": "Zhouxing Shi", "authors": "Zhouxing Shi, Minlie Huang", "title": "A Deep Sequential Model for Discourse Parsing on Multi-Party Dialogues", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse structures are beneficial for various NLP tasks such as dialogue\nunderstanding, question answering, sentiment analysis, and so on. This paper\npresents a deep sequential model for parsing discourse dependency structures of\nmulti-party dialogues. The proposed model aims to construct a discourse\ndependency tree by predicting dependency relations and constructing the\ndiscourse structure jointly and alternately. It makes a sequential scan of the\nElementary Discourse Units (EDUs) in a dialogue. For each EDU, the model\ndecides to which previous EDU the current one should link and what the\ncorresponding relation type is. The predicted link and relation type are then\nused to build the discourse structure incrementally with a structured encoder.\nDuring link prediction and relation classification, the model utilizes not only\nlocal information that represents the concerned EDUs, but also global\ninformation that encodes the EDU sequence and the discourse structure that is\nalready built at the current step. Experiments show that the proposed model\noutperforms all the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 08:13:48 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Shi", "Zhouxing", ""], ["Huang", "Minlie", ""]]}, {"id": "1812.00195", "submitter": "Thien Nguyen", "authors": "Trung Minh Nguyen and Thien Huu Nguyen", "title": "One for All: Neural Joint Modeling of Entities and Events", "comments": "Accepted at The Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI-19) (Honolulu, Hawaii, USA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The previous work for event extraction has mainly focused on the predictions\nfor event triggers and argument roles, treating entity mentions as being\nprovided by human annotators. This is unrealistic as entity mentions are\nusually predicted by some existing toolkits whose errors might be propagated to\nthe event trigger and argument role recognition. Few of the recent work has\naddressed this problem by jointly predicting entity mentions, event triggers\nand arguments. However, such work is limited to using discrete engineering\nfeatures to represent contextual information for the individual tasks and their\ninteractions. In this work, we propose a novel model to jointly perform\npredictions for entity mentions, event triggers and arguments based on the\nshared hidden representations from deep learning. The experiments demonstrate\nthe benefits of the proposed method, leading to the state-of-the-art\nperformance for event extraction.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 12:13:55 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Nguyen", "Trung Minh", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "1812.00235", "submitter": "Tingke Shen", "authors": "Kevin Shen, Amlan Kar, Sanja Fidler", "title": "Learning to Caption Images through a Lifetime by Asking Questions", "comments": "Fixed typos and added contribution list in intro, results remain the\n  same", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to bring artificial agents into our lives, we will need to go beyond\nsupervised learning on closed datasets to having the ability to continuously\nexpand knowledge. Inspired by a student learning in a classroom, we present an\nagent that can continuously learn by posing natural language questions to\nhumans. Our agent is composed of three interacting modules, one that performs\ncaptioning, another that generates questions and a decision maker that learns\nwhen to ask questions by implicitly reasoning about the uncertainty of the\nagent and expertise of the teacher. As compared to current active learning\nmethods which query images for full captions, our agent is able to ask pointed\nquestions to improve the generated captions. The agent trains on the improved\ncaptions, expanding its knowledge. We show that our approach achieves better\nperformance using less human supervision than the baselines on the challenging\nMSCOCO dataset.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 18:12:35 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 20:09:39 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 16:11:45 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Shen", "Kevin", ""], ["Kar", "Amlan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1812.00271", "submitter": "Mirco Ravanelli", "authors": "Mirco Ravanelli, Yoshua Bengio", "title": "Learning Speaker Representations with Mutual Information", "comments": "Submitted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.NE cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning good representations is of crucial importance in deep learning.\nMutual Information (MI) or similar measures of statistical dependence are\npromising tools for learning these representations in an unsupervised way. Even\nthough the mutual information between two random variables is hard to measure\ndirectly in high dimensional spaces, some recent studies have shown that an\nimplicit optimization of MI can be achieved with an encoder-discriminator\narchitecture similar to that of Generative Adversarial Networks (GANs). In this\nwork, we learn representations that capture speaker identities by maximizing\nthe mutual information between the encoded representations of chunks of speech\nrandomly sampled from the same sentence. The proposed encoder relies on the\nSincNet architecture and transforms raw speech waveform into a compact feature\nvector. The discriminator is fed by either positive samples (of the joint\ndistribution of encoded chunks) or negative samples (from the product of the\nmarginals) and is trained to separate them. We report experiments showing that\nthis approach effectively learns useful speaker representations, leading to\npromising results on speaker identification and verification tasks. Our\nexperiments consider both unsupervised and semi-supervised settings and compare\nthe performance achieved with different objective functions.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 21:48:28 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 22:49:01 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ravanelli", "Mirco", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1812.00315", "submitter": "Xinyi Zhou", "authors": "Xinyi Zhou and Reza Zafarani", "title": "A Survey of Fake News: Fundamental Theories, Detection Methods, and\n  Opportunities", "comments": "ACM Computing Surveys (CSUR), 37 pages", "journal-ref": null, "doi": "10.1145/3395046", "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth in fake news and its erosion to democracy, justice, and\npublic trust has increased the demand for fake news detection and intervention.\nThis survey reviews and evaluates methods that can detect fake news from four\nperspectives: (1) the false knowledge it carries, (2) its writing style, (3)\nits propagation patterns, and (4) the credibility of its source. The survey\nalso highlights some potential research tasks based on the review. In\nparticular, we identify and detail related fundamental theories across various\ndisciplines to encourage interdisciplinary research on fake news. We hope this\nsurvey can facilitate collaborative efforts among experts in computer and\ninformation sciences, social sciences, political science, and journalism to\nresearch fake news, where such efforts can lead to fake news detection that is\nnot only efficient but more importantly, explainable.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 03:27:15 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 21:08:10 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhou", "Xinyi", ""], ["Zafarani", "Reza", ""]]}, {"id": "1812.00350", "submitter": "Heriberto Cuay\\'ahuitl", "authors": "Heriberto Cuay\\'ahuitl, Seonghan Ryu, Donghyeon Lee, Jihie Kim", "title": "A Study on Dialogue Reward Prediction for Open-Ended Conversational\n  Agents", "comments": "In NeurIPS Workshop on Conversational AI: \"Today's Practice and\n  Tomorrow's Potential\", December 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of dialogue history to include in a conversational agent is often\nunderestimated and/or set in an empirical and thus possibly naive way. This\nsuggests that principled investigations into optimal context windows are\nurgently needed given that the amount of dialogue history and corresponding\nrepresentations can play an important role in the overall performance of a\nconversational system. This paper studies the amount of history required by\nconversational agents for reliably predicting dialogue rewards. The task of\ndialogue reward prediction is chosen for investigating the effects of varying\namounts of dialogue history and their impact on system performance.\nExperimental results using a dataset of 18K human-human dialogues report that\nlengthy dialogue histories of at least 10 sentences are preferred (25 sentences\nbeing the best in our experiments) over short ones, and that lengthy histories\nare useful for training dialogue reward predictors with strong positive\ncorrelations between target dialogue rewards and predicted ones.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 08:03:12 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Cuay\u00e1huitl", "Heriberto", ""], ["Ryu", "Seonghan", ""], ["Lee", "Donghyeon", ""], ["Kim", "Jihie", ""]]}, {"id": "1812.00381", "submitter": "Ilia Shumailov", "authors": "Rasika Bhalerao, Maxwell Aliapoulios, Ilia Shumailov, Sadia Afroz,\n  Damon McCoy", "title": "Towards Automatic Discovery of Cybercrime Supply Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybercrime forums enable modern criminal entrepreneurs to collaborate with\nother criminals into increasingly efficient and sophisticated criminal\nendeavors. Understanding the connections between different products and\nservices can often illuminate effective interventions. However, generating this\nunderstanding of supply chains currently requires time-consuming manual effort.\n  In this paper, we propose a language-agnostic method to automatically extract\nsupply chains from cybercrime forum posts and replies. Our supply chain\ndetection algorithm can identify 36% and 58% relevant chains within major\nEnglish and Russian forums, respectively, showing improvements over the\nbaselines of 13% and 36%, respectively. Our analysis of the automatically\ngenerated supply chains demonstrates underlying connections between products\nand services within these forums. For example, the extracted supply chain\nilluminated the connection between hack-for-hire services and the selling of\nrare and valuable `OG' accounts, which has only recently been reported. The\nunderstanding of connections between products and services exposes potentially\neffective intervention points.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 12:38:58 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 23:48:55 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Bhalerao", "Rasika", ""], ["Aliapoulios", "Maxwell", ""], ["Shumailov", "Ilia", ""], ["Afroz", "Sadia", ""], ["McCoy", "Damon", ""]]}, {"id": "1812.00382", "submitter": "Jasper Linmans", "authors": "Jasper Linmans, Bob van de Velde, Evangelos Kanoulas", "title": "Improved and Robust Controversy Detection in General Web Pages Using\n  Semantic Approaches under Large Scale Conditions", "comments": "Presented at the 27th ACM International Conference on Information and\n  Knowledge Management (CIKM 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting controversy in general web pages is a daunting task, but\nincreasingly essential to efficiently moderate discussions and effectively\nfilter problematic content. Unfortunately, controversies occur across many\ntopics and domains, with great changes over time. This paper investigates\nneural classifiers as a more robust methodology for controversy detection in\ngeneral web pages. Current models have often cast controversy detection on\ngeneral web pages as Wikipedia linking, or exact lexical matching tasks. The\ndiverse and changing nature of controversies suggest that semantic approaches\nare better able to detect controversy. We train neural networks that can\ncapture semantic information from texts using weak signal data. By leveraging\nthe semantic properties of word embeddings we robustly improve on existing\ncontroversy detection methods. To evaluate model stability over time and to\nunseen topics, we asses model performance under varying training conditions to\ntest cross-temporal, cross-topic, cross-domain performance and annotator\ncongruence. In doing so, we demonstrate that weak-signal based neural\napproaches are closer to human estimates of controversy and are more robust to\nthe inherent variability of controversies.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 12:41:03 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Linmans", "Jasper", ""], ["van de Velde", "Bob", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1812.00436", "submitter": "Adrian Benton", "authors": "Adrian Benton", "title": "Learning Representations of Social Media Users", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User representations are routinely used in recommendation systems by platform\ndevelopers, targeted advertisements by marketers, and by public policy\nresearchers to gauge public opinion across demographic groups. Computer\nscientists consider the problem of inferring user representations more\nabstractly; how does one extract a stable user representation - effective for\nmany downstream tasks - from a medium as noisy and complicated as social media?\n  The quality of a user representation is ultimately task-dependent (e.g. does\nit improve classifier performance, make more accurate recommendations in a\nrecommendation system) but there are proxies that are less sensitive to the\nspecific task. Is the representation predictive of latent properties such as a\nperson's demographic features, socioeconomic class, or mental health state? Is\nit predictive of the user's future behavior?\n  In this thesis, we begin by showing how user representations can be learned\nfrom multiple types of user behavior on social media. We apply several\nextensions of generalized canonical correlation analysis to learn these\nrepresentations and evaluate them at three tasks: predicting future hashtag\nmentions, friending behavior, and demographic features. We then show how user\nfeatures can be employed as distant supervision to improve topic model fit.\nFinally, we show how user features can be integrated into and improve existing\nclassifiers in the multitask learning framework. We treat user representations\n- ground truth gender and mental health features - as auxiliary tasks to\nimprove mental health state prediction. We also use distributed user\nrepresentations learned in the first chapter to improve tweet-level stance\nclassifiers, showing that distant user information can inform classification\ntasks at the granularity of a single message.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 17:57:04 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Benton", "Adrian", ""]]}, {"id": "1812.00677", "submitter": "Mahnoosh Kholghi", "authors": "Hamed Hassanzadeh, Mahnoosh Kholghi, Anthony Nguyen, Kevin Chu", "title": "Clinical Document Classification Using Labeled and Unlabeled Data Across\n  Hospitals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Reviewing radiology reports in emergency departments is an essential but\nlaborious task. Timely follow-up of patients with abnormal cases in their\nradiology reports may dramatically affect the patient's outcome, especially if\nthey have been discharged with a different initial diagnosis. Machine learning\napproaches have been devised to expedite the process and detect the cases that\ndemand instant follow up. However, these approaches require a large amount of\nlabeled data to train reliable predictive models. Preparing such a large\ndataset, which needs to be manually annotated by health professionals, is\ncostly and time-consuming. This paper investigates a semi-supervised learning\nframework for radiology report classification across three hospitals. The main\ngoal is to leverage clinical unlabeled data in order to augment the learning\nprocess where limited labeled data is available. To further improve the\nclassification performance, we also integrate a transfer learning technique\ninto the semi-supervised learning pipeline . Our experimental findings show\nthat (1) convolutional neural networks (CNNs), while being independent of any\nproblem-specific feature engineering, achieve significantly higher\neffectiveness compared to conventional supervised learning approaches, (2)\nleveraging unlabeled data in training a CNN-based classifier reduces the\ndependency on labeled data by more than 50% to reach the same performance of a\nfully supervised CNN, and (3) transferring the knowledge gained from available\nlabeled data in an external source hospital significantly improves the\nperformance of a semi-supervised CNN model over their fully supervised\ncounterparts in a target hospital.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 11:34:26 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 21:43:24 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Hassanzadeh", "Hamed", ""], ["Kholghi", "Mahnoosh", ""], ["Nguyen", "Anthony", ""], ["Chu", "Kevin", ""]]}, {"id": "1812.00686", "submitter": "Jia-Chen Gu", "authors": "Jia-Chen Gu, Zhen-Hua Ling, Yu-Ping Ruan and Quan Liu", "title": "Building Sequential Inference Models for End-to-End Response Selection", "comments": "Accepted by AAAI 2019, Workshop on DSTC7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end response selection model for Track 1 of the\n7th Dialogue System Technology Challenges (DSTC7). This task focuses on\nselecting the correct next utterance from a set of candidates given a partial\nconversation. We propose an end-to-end neural network based on enhanced\nsequential inference model (ESIM) for this task. Our proposed model differs\nfrom the original ESIM model in the following four aspects. First, a new word\nrepresentation method which combines the general pre-trained word embeddings\nwith those estimated on the task-specific training set is adopted in order to\naddress the challenge of out-of-vocabulary (OOV) words. Second, an attentive\nhierarchical recurrent encoder (AHRE) is designed which is capable to encode\nsentences hierarchically and generate more descriptive representations by\naggregation. Third, a new pooling method which combines multi-dimensional\npooling and last-state pooling is used instead of the simple combination of max\npooling and average pooling in the original ESIM. Last, a modification layer is\nadded before the softmax layer to emphasize the importance of the last\nutterance in the context for response selection. In the released evaluation\nresults of DSTC7, our proposed method ranked second on the Ubuntu dataset and\nthird on the Advising dataset in subtask 1 of Track 1.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 11:46:43 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Gu", "Jia-Chen", ""], ["Ling", "Zhen-Hua", ""], ["Ruan", "Yu-Ping", ""], ["Liu", "Quan", ""]]}, {"id": "1812.00798", "submitter": "Atul Kr. Ojha Mr.", "authors": "Atul Kr. Ojha, Koel Dutta Chowdhury, Chao-Hong Liu, and Karan Saxena", "title": "The RGNLP Machine Translation Systems for WAT 2018", "comments": "Short-Paper at WAT Shared Task 2018, In Proceedings of the 5th\n  Workshop on Asian Translation (WAT2018), Hong Kong, China, December", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the system description of Machine Translation (MT)\nsystem(s) for Indic Languages Multilingual Task for the 2018 edition of the WAT\nShared Task. In our experiments, we (the RGNLP team) explore both statistical\nand neural methods across all language pairs. (We further present an extensive\ncomparison of language-related problems for both the approaches in the context\nof low-resourced settings.) Our PBSMT models were highest score on all\nautomatic evaluation metrics in the English into Telugu, Hindi, Bengali, Tamil\nportion of the shared task.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 14:53:41 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ojha", "Atul Kr.", ""], ["Chowdhury", "Koel Dutta", ""], ["Liu", "Chao-Hong", ""], ["Saxena", "Karan", ""]]}, {"id": "1812.00815", "submitter": "Yerai Doval", "authors": "Yerai Doval and Carlos G\\'omez-Rodr\\'iguez", "title": "Comparing Neural- and N-Gram-Based Language Models for Word Segmentation", "comments": "11 pages, 4 figures, 5 tables, accepted in Journal of the Association\n  for Information Science and Technology", "journal-ref": "Volume 69, Issue 11, 2018, 11 pages", "doi": "10.1002/asi.24082", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word segmentation is the task of inserting or deleting word boundary\ncharacters in order to separate character sequences that correspond to words in\nsome language. In this article we propose an approach based on a beam search\nalgorithm and a language model working at the byte/character level, the latter\ncomponent implemented either as an n-gram model or a recurrent neural network.\nThe resulting system analyzes the text input with no word boundaries one token\nat a time, which can be a character or a byte, and uses the information\ngathered by the language model to determine if a boundary must be placed in the\ncurrent position or not. Our aim is to use this system in a preprocessing step\nfor a microtext normalization system. This means that it needs to effectively\ncope with the data sparsity present on this kind of texts. We also strove to\nsurpass the performance of two readily available word segmentation systems: The\nwell-known and accessible Word Breaker by Microsoft, and the Python module\nWordSegment by Grant Jenks. The results show that we have met our objectives,\nand we hope to continue to improve both the precision and the efficiency of our\nsystem in the future.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 15:04:23 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Doval", "Yerai", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "1812.00855", "submitter": "Ruo Yu Tao", "authors": "Ruo Yu Tao, Marc-Alexandre C\\^ot\\'e, Xingdi Yuan, Layla El Asri", "title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve a text-based game, an agent needs to formulate valid text commands\nfor a given context and find the ones that lead to success. Recent attempts at\nsolving text-based games with deep reinforcement learning have focused on the\nlatter, i.e., learning to act optimally when valid actions are known in\nadvance. In this work, we propose to tackle the first task and train a model\nthat generates the set of all valid commands for a given context. We try three\ngenerative models on a dataset generated with Textworld. The best model can\ngenerate valid commands which were unseen at training and achieve high $F_1$\nscore on the test set.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 16:00:48 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Tao", "Ruo Yu", ""], ["C\u00f4t\u00e9", "Marc-Alexandre", ""], ["Yuan", "Xingdi", ""], ["Asri", "Layla El", ""]]}, {"id": "1812.00898", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Mateusz Malinowski, Felix Hill, Ali Eslami, Oriol\n  Vinyals, Tejas Kulkarni", "title": "Generating Diverse Programs with Instruction Conditioned Reinforced\n  Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Deep Reinforcement Learning have led to agents that perform well\nacross a variety of sensory-motor domains. In this work, we study the setting\nin which an agent must learn to generate programs for diverse scenes\nconditioned on a given symbolic instruction. Final goals are specified to our\nagent via images of the scenes. A symbolic instruction consistent with the goal\nimages is used as the conditioning input for our policies. Since a single\ninstruction corresponds to a diverse set of different but still consistent\nend-goal images, the agent needs to learn to generate a distribution over\nprograms given an instruction. We demonstrate that with simple changes to the\nreinforced adversarial learning objective, we can learn instruction conditioned\npolicies to achieve the corresponding diverse set of goals. Most importantly,\nour agent's stochastic policy is shown to more accurately capture the diversity\nin the goal distribution than a fixed pixel-based reward function baseline. We\ndemonstrate the efficacy of our approach on two domains: (1) drawing MNIST\ndigits with a paint software conditioned on instructions and (2) constructing\nscenes in a 3D editor that satisfies a certain instruction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 16:51:35 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Malinowski", "Mateusz", ""], ["Hill", "Felix", ""], ["Eslami", "Ali", ""], ["Vinyals", "Oriol", ""], ["Kulkarni", "Tejas", ""]]}, {"id": "1812.00899", "submitter": "Elnaz Nouri", "authors": "Elnaz Nouri, Ehsan Hosseini-Asl", "title": "Toward Scalable Neural Dialogue State Tracking Model", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018), 2nd Conversational AI workshop, Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latency in the current neural based dialogue state tracking models\nprohibits them from being used efficiently for deployment in production\nsystems, albeit their highly accurate performance. This paper proposes a new\nscalable and accurate neural dialogue state tracking model, based on the\nrecently proposed Global-Local Self-Attention encoder (GLAD) model by Zhong et\nal. which uses global modules to share parameters between estimators for\ndifferent types (called slots) of dialogue states, and uses local modules to\nlearn slot-specific features. By using only one recurrent networks with global\nconditioning, compared to (1 + \\# slots) recurrent networks with global and\nlocal conditioning used in the GLAD model, our proposed model reduces the\nlatency in training and inference times by $35\\%$ on average, while preserving\nperformance of belief state tracking, by $97.38\\%$ on turn request and\n$88.51\\%$ on joint goal and accuracy. Evaluation on Multi-domain dataset\n(Multi-WoZ) also demonstrates that our model outperforms GLAD on turn inform\nand joint goal accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 16:52:34 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Nouri", "Elnaz", ""], ["Hosseini-Asl", "Ehsan", ""]]}, {"id": "1812.00978", "submitter": "Rajarshi Das", "authors": "Aishwarya Kamath, Rajarshi Das", "title": "A Survey on Semantic Parsing", "comments": "AKBC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant amount of information in today's world is stored in structured\nand semi-structured knowledge bases. Efficient and simple methods to query them\nare essential and must not be restricted to only those who have expertise in\nformal query languages. The field of semantic parsing deals with converting\nnatural language utterances to logical forms that can be easily executed on a\nknowledge base. In this survey, we examine the various components of a semantic\nparsing system and discuss prominent work ranging from the initial rule based\nmethods to the current neural approaches to program synthesis. We also discuss\nmethods that operate using varying levels of supervision and highlight the key\nchallenges involved in the learning of such systems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 18:53:08 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 19:36:16 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 18:08:07 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Kamath", "Aishwarya", ""], ["Das", "Rajarshi", ""]]}, {"id": "1812.01083", "submitter": "Jacqueline Brixey", "authors": "Jacqueline Brixey, Ramesh Manuvinakurike, Nham Le, Tuan Lai, Walter\n  Chang, Trung Bui", "title": "A System for Automated Image Editing from Natural Language Commands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the task of modifying images in an image editing program\nusing natural language written commands. We utilize a corpus of over 6000 image\nedit text requests to alter real world images collected via crowdsourcing. A\nnovel framework composed of actions and entities to map a user's natural\nlanguage request to executable commands in an image editing program is\ndescribed. We resolve previously labeled annotator disagreement through a\nvoting process and complete annotation of the corpus. We experimented with\ndifferent machine learning models and found that the LSTM, the SVM, and the\nbidirectional LSTM-CRF joint models are the best to detect image editing\nactions and associated entities in a given utterance.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 21:12:31 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Brixey", "Jacqueline", ""], ["Manuvinakurike", "Ramesh", ""], ["Le", "Nham", ""], ["Lai", "Tuan", ""], ["Chang", "Walter", ""], ["Bui", "Trung", ""]]}, {"id": "1812.01193", "submitter": "Oana-Maria Camburu", "authors": "Oana-Maria Camburu, Tim Rockt\\\"aschel, Thomas Lukasiewicz, Phil\n  Blunsom", "title": "e-SNLI: Natural Language Inference with Natural Language Explanations", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for machine learning to garner widespread public adoption, models\nmust be able to provide interpretable and robust explanations for their\ndecisions, as well as learn from human-provided explanations at train time. In\nthis work, we extend the Stanford Natural Language Inference dataset with an\nadditional layer of human-annotated natural language explanations of the\nentailment relations. We further implement models that incorporate these\nexplanations into their training process and output them at test time. We show\nhow our corpus of explanations, which we call e-SNLI, can be used for various\ngoals, such as obtaining full sentence justifications of a model's decisions,\nimproving universal sentence representations and transferring to out-of-domain\nNLI datasets. Our dataset thus opens up a range of research directions for\nusing natural language explanations, both for improving models and for\nasserting their trust.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 03:15:38 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 16:11:19 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Camburu", "Oana-Maria", ""], ["Rockt\u00e4schel", "Tim", ""], ["Lukasiewicz", "Thomas", ""], ["Blunsom", "Phil", ""]]}, {"id": "1812.01207", "submitter": "Nikolai Yakovenko", "authors": "Neel Kant, Raul Puri, Nikolai Yakovenko and Bryan Catanzaro", "title": "Practical Text Classification With Large Pre-Trained Language Models", "comments": "8 pages, submitted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-emotion sentiment classification is a natural language processing (NLP)\nproblem with valuable use cases on real-world data. We demonstrate that\nlarge-scale unsupervised language modeling combined with finetuning offers a\npractical solution to this task on difficult datasets, including those with\nlabel class imbalance and domain-specific context. By training an\nattention-based Transformer network (Vaswani et al. 2017) on 40GB of text\n(Amazon reviews) (McAuley et al. 2015) and fine-tuning on the training set, our\nmodel achieves a 0.69 F1 score on the SemEval Task 1:E-c multi-dimensional\nemotion classification problem (Mohammad et al. 2018), based on the Plutchik\nwheel of emotions (Plutchik 1979). These results are competitive with state of\nthe art models, including strong F1 scores on difficult (emotion) categories\nsuch as Fear (0.73), Disgust (0.77) and Anger (0.78), as well as competitive\nresults on rare categories such as Anticipation (0.42) and Surprise (0.37).\nFurthermore, we demonstrate our application on a real world text classification\ntask. We create a narrowly collected text dataset of real tweets on several\ntopics, and show that our finetuned model outperforms general purpose\ncommercially available APIs for sentiment and multidimensional emotion\nclassification on this dataset by a significant margin. We also perform a\nvariety of additional studies, investigating properties of deep learning\narchitectures, datasets and algorithms for achieving practical multidimensional\nsentiment classification. Overall, we find that unsupervised language modeling\nand finetuning is a simple framework for achieving high quality results on\nreal-world sentiment classification.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 04:04:56 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Kant", "Neel", ""], ["Puri", "Raul", ""], ["Yakovenko", "Nikolai", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1812.01245", "submitter": "Hongyu Xiong", "authors": "Hongyu Xiong and Ruixiao Sun", "title": "Transferable Natural Language Interface to Structured Queries aided by\n  Adversarial Generation", "comments": "8 pages, 3 figures; accepted by AAAI Workshop 2019; accepted by\n  International Conference of Semantic Computing (ICSC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural language interface (NLI) to structured query is intriguing due to\nits wide industrial applications and high economical values. In this work, we\ntackle the problem of domain adaptation for NLI with limited data on target\ndomain. Two important approaches are considered: (a) effective\ngeneral-knowledge-learning on source domain semantic parsing, and (b) data\naugmentation on target domain. We present a Structured Query Inference Network\n(SQIN) to enhance learning for domain adaptation, by separating schema\ninformation from NL and decoding SQL in a more structural-aware manner; we also\npropose a GAN-based augmentation technique (AugmentGAN) to mitigate the issue\nof lacking target domain data. We report solid results on GeoQuery, Overnight,\nand WikiSQL to demonstrate state-of-the-art performances for both in-domain and\ndomain-transfer tasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 06:42:49 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 17:36:50 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Xiong", "Hongyu", ""], ["Sun", "Ruixiao", ""]]}, {"id": "1812.01250", "submitter": "Pei Zhou", "authors": "Pei Zhou, Muhao Chen, Kai-Wei Chang, Carlo Zaniolo", "title": "Quantification and Analysis of Scientific Language Variation Across\n  Research Fields", "comments": "Accepted in ICDM Workshop on Cross-disciplinary Data Exchange and\n  Collaboration (CDEC). 2018. 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantifying differences in terminologies from various academic domains has\nbeen a longstanding problem yet to be solved. We propose a computational\napproach for analyzing linguistic variation among scientific research fields by\ncapturing the semantic change of terms based on a neural language model. The\nmodel is trained on a large collection of literature in five computer science\nresearch fields, for which we obtain field-specific vector representations for\nkey terms, and global vector representations for other words. Several\nquantitative approaches are introduced to identify the terms whose semantics\nhave drastically changed, or remain unchanged across different research fields.\nWe also propose a metric to quantify the overall linguistic variation of\nresearch fields. After quantitative evaluation on human annotated data and\nqualitative comparison with other methods, we show that our model can improve\ncross-disciplinary data collaboration by identifying terms that potentially\ninduce confusion during interdisciplinary studies.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 07:05:47 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Zhou", "Pei", ""], ["Chen", "Muhao", ""], ["Chang", "Kai-Wei", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1812.01260", "submitter": "Zachary Kaden", "authors": "George Larionov, Zachary Kaden, Hima Varsha Dureddy, Gabriel Bayomi T.\n  Kalejaiye, Mihir Kale, Srividya Pranavi Potharaju, Ankit Parag Shah,\n  Alexander I Rudnicky", "title": "Tartan: A retrieval-based socialbot powered by a dynamic finite-state\n  machine architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Tartan conversational agent built for the 2018 Alexa\nPrize Competition. Tartan is a non-goal-oriented socialbot focused around\nproviding users with an engaging and fluent casual conversation. Tartan's key\nfeatures include an emphasis on structured conversation based on flexible\nfinite-state models and an approach focused on understanding and using\nconversational acts. To provide engaging conversations, Tartan blends\nscript-like yet dynamic responses with data-based generative and retrieval\nmodels. Unique to Tartan is that our dialog manager is modeled as a dynamic\nFinite State Machine. To our knowledge, no other conversational agent\nimplementation has followed this specific structure.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 07:48:00 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Larionov", "George", ""], ["Kaden", "Zachary", ""], ["Dureddy", "Hima Varsha", ""], ["Kalejaiye", "Gabriel Bayomi T.", ""], ["Kale", "Mihir", ""], ["Potharaju", "Srividya Pranavi", ""], ["Shah", "Ankit Parag", ""], ["Rudnicky", "Alexander I", ""]]}, {"id": "1812.01431", "submitter": "Bohdan Khomtchouk", "authors": "Bohdan Khomtchouk, Shyam Sudhakaran", "title": "Modeling natural language emergence with integral transform theory and\n  reinforcement learning", "comments": "9 pages, 4 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:1603.03153", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zipf's law predicts a power-law relationship between word rank and frequency\nin language communication systems and has been widely reported in a variety of\nnatural language processing applications. However, the emergence of natural\nlanguage is often modeled as a function of bias between speaker and listener\ninterests, which lacks a direct way of relating information-theoretic bias to\nZipfian rank. A function of bias also serves as an unintuitive interpretation\nof the communicative effort exchanged between a speaker and a listener. We\ncounter these shortcomings by proposing a novel integral transform and kernel\nfor mapping communicative bias functions to corresponding word frequency-rank\nrepresentations at any arbitrary phase transition point, resulting in a direct\nway to link communicative effort (modeled by speaker/listener bias) to specific\nvocabulary used (represented by word rank). We demonstrate the practical\nutility of our integral transform by showing how a change from bias to rank\nresults in greater accuracy and performance at an image classification task for\nassigning word labels to images randomly subsampled from CIFAR10. We model this\ntask as a reinforcement learning game between a speaker and listener and\ncompare the relative impact of bias and Zipfian word rank on communicative\nperformance (and accuracy) between the two agents.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 19:12:04 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Khomtchouk", "Bohdan", ""], ["Sudhakaran", "Shyam", ""]]}, {"id": "1812.01525", "submitter": "Hang Chu", "authors": "Hang Chu, Daiqing Li, Sanja Fidler", "title": "A Face-to-Face Neural Conversation Model", "comments": "Published at CVPR 2018", "journal-ref": "CVPR (2018) 7113-7121", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have recently become good at engaging in dialog. However,\ncurrent approaches are based solely on verbal text, lacking the richness of a\nreal face-to-face conversation. We propose a neural conversation model that\naims to read and generate facial gestures alongside with text. This allows our\nmodel to adapt its response based on the \"mood\" of the conversation. In\nparticular, we introduce an RNN encoder-decoder that exploits the movement of\nfacial muscles, as well as the verbal conversation. The decoder consists of two\nlayers, where the lower layer aims at generating the verbal response and coarse\nfacial expressions, while the second layer fills in the subtle gestures, making\nthe generated output more smooth and natural. We train our neural network by\nhaving it \"watch\" 250 movies. We showcase our joint face-text model in\ngenerating more natural conversations through automatic metrics and a human\nstudy. We demonstrate an example application with a face-to-face chatting\navatar.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 16:55:25 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Chu", "Hang", ""], ["Li", "Daiqing", ""], ["Fidler", "Sanja", ""]]}, {"id": "1812.01527", "submitter": "Yan Zeng", "authors": "Yan Zeng, Yangyang Lan, Yazhou Hao, Chen Li, Qinhua Zheng", "title": "Leveraging Multi-grained Sentiment Lexicon Information for Neural\n  Sequence Models", "comments": "I think this paper has very limited contribution now", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence models have achieved great success in sentence-level\nsentiment classification. However, some models are exceptionally complex or\nbased on expensive features. Some other models recognize the value of existed\nlinguistic resource but utilize it insufficiently. This paper proposes a novel\nand general method to incorporate lexicon information, including sentiment\nlexicons(+/-), negation words and intensifiers. Words are annotated in\nfine-grained and coarse-grained labels. The proposed method first encodes the\nfine-grained labels into sentiment embedding and concatenates it with word\nembedding. Second, the coarse-grained labels are utilized to enhance the\nattention mechanism to give large weight on sentiment-related words.\nExperimental results show that our method can increase classification accuracy\nfor neural sequence models on both SST-5 and MR dataset. Specifically, the\nenhanced Bi-LSTM model can even compare with a Tree-LSTM which uses expensive\nphrase-level annotations. Further analysis shows that in most cases the lexicon\nresource can offer the right annotations. Besides, the proposed method is\ncapable of overcoming the effect from inevitably wrong annotations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 17:01:52 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 21:37:37 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Zeng", "Yan", ""], ["Lan", "Yangyang", ""], ["Hao", "Yazhou", ""], ["Li", "Chen", ""], ["Zheng", "Qinhua", ""]]}, {"id": "1812.01628", "submitter": "Prithviraj Ammanabrolu", "authors": "Prithviraj Ammanabrolu and Mark O. Riedl", "title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement\n  Learning", "comments": "Proceedings of NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based adventure games provide a platform on which to explore\nreinforcement learning in the context of a combinatorial action space, such as\nnatural language. We present a deep reinforcement learning architecture that\nrepresents the game state as a knowledge graph which is learned during\nexploration. This graph is used to prune the action space, enabling more\nefficient exploration. The question of which action to take can be reduced to a\nquestion-answering task, a form of transfer learning that pre-trains certain\nparts of our architecture. In experiments using the TextWorld framework, we\nshow that our proposed technique can learn a control policy faster than\nbaseline alternatives. We have also open-sourced our code at\nhttps://github.com/rajammanabrolu/KG-DQN.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 19:06:00 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 17:15:45 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Ammanabrolu", "Prithviraj", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1812.01704", "submitter": "\\'Eloi Brassard-Gourdeau", "authors": "\\'Eloi Brassard-Gourdeau and Richard Khoury", "title": "Impact of Sentiment Detection to Recognize Toxic and Subversive Online\n  Comments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of toxic content has become a major problem for many online\ncommunities. Moderators try to limit this problem by implementing more and more\nrefined comment filters, but toxic users are constantly finding new ways to\ncircumvent them. Our hypothesis is that while modifying toxic content and\nkeywords to fool filters can be easy, hiding sentiment is harder. In this\npaper, we explore various aspects of sentiment detection and their correlation\nto toxicity, and use our results to implement a toxicity detection tool. We\nthen test how adding the sentiment information helps detect toxicity in three\ndifferent real-world datasets, and incorporate subversion to these datasets to\nsimulate a user trying to circumvent the system. Our results show sentiment\ninformation has a positive impact on toxicity detection against a subversive\nuser.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 21:44:36 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Brassard-Gourdeau", "\u00c9loi", ""], ["Khoury", "Richard", ""]]}, {"id": "1812.01828", "submitter": "Piyush Mital", "authors": "Piyush Mital, Saurabh Agarwal, Bhargavi Neti, Yashodhara Haribhakta,\n  Vibhavari Kamble, Krishnanjan Bhattacharjee, Debashri Das, Swati Mehta, Ajai\n  Kumar", "title": "Graph based Question Answering System", "comments": "ICACCI 2018 Camera Ready and Accepted. ICACCI'18 is technically\n  co-sponsored by IEEE and IEEE Communications Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's digital age in the dawning era of big data analytics it is not the\ninformation but the linking of information through entities and actions which\ndefines the discourse. Any textual data either available on the Internet off\noff-line (like newspaper data, Wikipedia dump, etc) is basically connect\ninformation which cannot be treated isolated for its wholesome semantics. There\nis a need for an automated retrieval process with proper information extraction\nto structure the data for relevant and fast text analytics. The first big\nchallenge is the conversion of unstructured textual data to structured data.\nUnlike other databases, graph databases handle relationships and connections\nelegantly. Our project aims at developing a graph-based information extraction\nand retrieval system.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 06:31:48 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Mital", "Piyush", ""], ["Agarwal", "Saurabh", ""], ["Neti", "Bhargavi", ""], ["Haribhakta", "Yashodhara", ""], ["Kamble", "Vibhavari", ""], ["Bhattacharjee", "Krishnanjan", ""], ["Das", "Debashri", ""], ["Mehta", "Swati", ""], ["Kumar", "Ajai", ""]]}, {"id": "1812.01840", "submitter": "Guanyu Li", "authors": "Guanyu Li, Pengfei Zhang, Caiyan Jia", "title": "Attention Boosted Sequential Inference Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanism has been proven effective on natural language processing.\nThis paper proposes an attention boosted natural language inference model named\naESIM by adding word attention and adaptive direction-oriented attention\nmechanisms to the traditional Bi-LSTM layer of natural language inference\nmodels, e.g. ESIM. This makes the inference model aESIM has the ability to\neffectively learn the representation of words and model the local subsentential\ninference between pairs of premise and hypothesis. The empirical studies on the\nSNLI, MultiNLI and Quora benchmarks manifest that aESIM is superior to the\noriginal ESIM model.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 07:24:51 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 01:46:50 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Li", "Guanyu", ""], ["Zhang", "Pengfei", ""], ["Jia", "Caiyan", ""]]}, {"id": "1812.01879", "submitter": "Ying Shen", "authors": "K. Lei, S. Si, D. Wen, and Y. Shen", "title": "An enhanced computational feature selection method for medical synonym\n  identification via bilingualism and multi-corpus training", "comments": null, "journal-ref": null, "doi": "10.1109/ICBDA.2017.8078771", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical synonym identification has been an important part of medical natural\nlanguage processing (NLP). However, in the field of Chinese medical synonym\nidentification, there are problems like low precision and low recall rate. To\nsolve the problem, in this paper, we propose a method for identifying Chinese\nmedical synonyms. We first selected 13 features including Chinese and English\nfeatures. Then we studied the synonym identification results of each feature\nalone and different combinations of the features. Through the comparison among\nidentification results, we present an optimal combination of features for\nChinese medical synonym identification. Experiments show that our selected\nfeatures have achieved 97.37% precision rate, 96.00% recall rate and 97.33% F1\nscore.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 09:45:52 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Lei", "K.", ""], ["Si", "S.", ""], ["Wen", "D.", ""], ["Shen", "Y.", ""]]}, {"id": "1812.01884", "submitter": "Ying Shen", "authors": "Kai Lei, Kaiqi Yuan, Qiang Zhang, Ying Shen", "title": "MedSim: A Novel Semantic Similarity Measure in Bio-medical Knowledge\n  Graphs", "comments": null, "journal-ref": "International Conference on Knowledge Science, Engineering and\n  Management KSEM 2018: Knowledge Science, Engineering and Management pp\n  479-490", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MedSim, a novel semantic SIMilarity method based on public\nwell-established bio-MEDical knowledge graphs (KGs) and large-scale corpus, to\nstudy the therapeutic substitution of antibiotics. Besides hierarchy and corpus\nof KGs, MedSim further interprets medicine characteristics by constructing\nmulti-dimensional medicine-specific feature vectors. Dataset of 528 antibiotic\npairs scored by doctors is applied for evaluation and MedSim has produced\nstatistically significant improvement over other semantic similarity methods.\nFurthermore, some promising applications of MedSim in drug substitution and\ndrug abuse prevention are presented in case study.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 09:58:54 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Lei", "Kai", ""], ["Yuan", "Kaiqi", ""], ["Zhang", "Qiang", ""], ["Shen", "Ying", ""]]}, {"id": "1812.01885", "submitter": "Ying Shen", "authors": "Ying Shen, Qiang Zhang, Jin Zhang, Jiyue Huang, Yuming Lu, Kai Lei", "title": "Improving Medical Short Text Classification with Semantic Expansion\n  Using Word-Cluster Embedding", "comments": null, "journal-ref": "International Conference on Information Science and Applications\n  ICISA 2018: Information Science and Applications 2018 pp 401-411", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text classification (TC) research can be used for real-world\nproblems such as the classification of in-patient discharge summaries and\nmedical text reports, which is beneficial to make medical documents more\nunderstandable to doctors. However, in electronic medical records (EMR), the\ntexts containing sentences are shorter than that in general domain, which leads\nto the lack of semantic features and the ambiguity of semantic. To tackle this\nchallenge, we propose to add word-cluster embedding to deep neural network for\nimproving short text classification. Concretely, we first use hierarchical\nagglomerative clustering to cluster the word vectors in the semantic space.\nThen we calculate the cluster center vector which represents the implicit topic\ninformation of words in the cluster. Finally, we expand word vector with\ncluster center vector, and implement classifiers using CNN and LSTM\nrespectively. To evaluate the performance of our proposed method, we conduct\nexperiments on public data sets TREC and the medical short sentences data sets\nwhich is constructed and released by us. The experimental results demonstrate\nthat our proposed method outperforms state-of-the-art baselines in short\nsentence classification on both medical domain and general domain.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 10:02:59 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Shen", "Ying", ""], ["Zhang", "Qiang", ""], ["Zhang", "Jin", ""], ["Huang", "Jiyue", ""], ["Lu", "Yuming", ""], ["Lei", "Kai", ""]]}, {"id": "1812.01887", "submitter": "Ying Shen", "authors": "Ying Shen, Yang Deng, Kaiqi Yuan, Li Liu, Yong Liu", "title": "Approach for Semi-automatic Construction of Anti-infective Drug Ontology\n  Based on Entity Linking", "comments": null, "journal-ref": "International Conference on Smart Computing and Communication\n  SmartCom 2017: Smart Computing and Communication pp 268-277", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology can be used for the interpretation of natural language. To construct\nan anti-infective drug ontology, one needs to design and deploy a\nmethodological step to carry out the entity discovery and linking. Medical\nsynonym resources have been an important part of medical natural language\nprocessing (NLP). However, there are problems such as low precision and low\nrecall rate. In this study, an NLP approach is adopted to generate candidate\nentities. Open ontology is analyzed to extract semantic relations. Six-word\nvector features and word-level features are selected to perform the entity\nlinking. The extraction results of synonyms with a single feature and different\ncombinations of features are studied. Experiments show that our selected\nfeatures have achieved a precision rate of 86.77%, a recall rate of 89.03% and\nan F1 score of 87.89%. This paper finally presents the structure of the\nproposed ontology and its relevant statistical data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 10:08:29 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Shen", "Ying", ""], ["Deng", "Yang", ""], ["Yuan", "Kaiqi", ""], ["Liu", "Li", ""], ["Liu", "Yong", ""]]}, {"id": "1812.01889", "submitter": "Ying Shen", "authors": "Kai Lei, Bing Zhang, Yong Liu, Yang Deng, Dongyu Zhang, Ying Shen", "title": "A Knowledge Graph Based Solution for Entity Discovery and Linking in\n  Open-Domain Questions", "comments": null, "journal-ref": "International Conference on Smart Computing and Communication\n  SmartCom 2017: Smart Computing and Communication pp 181-190", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity discovery and linking is the fundamental and core component of\nquestion answering. In Question Entity Discovery and Linking (QEDL) problem,\ntraditional methods are challenged because multiple entities in one short\nquestion are difficult to be discovered entirely and the incomplete information\nin short text makes entity linking hard to implement. To overcome these\ndifficulties, we proposed a knowledge graph based solution for QEDL and\ndeveloped a system consists of Question Entity Discovery (QED) module and\nEntity Linking (EL) module. The method of QED module is a tradeoff and ensemble\nof two methods. One is the method based on knowledge graph retrieval, which\ncould extract more entities in questions and guarantee the recall rate, the\nother is the method based on Conditional Random Field (CRF), which improves the\nprecision rate. The EL module is treated as a ranking problem and Learning to\nRank (LTR) method with features such as semantic similarity, text similarity\nand entity popularity is utilized to extract and make full use of the\ninformation in short texts. On the official dataset of a shared QEDL evaluation\ntask, our approach could obtain 64.44% F1 score of QED and 64.86% accuracy of\nEL, which ranks the 2nd place and indicates its practical use for QEDL problem.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 10:10:56 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Lei", "Kai", ""], ["Zhang", "Bing", ""], ["Liu", "Yong", ""], ["Deng", "Yang", ""], ["Zhang", "Dongyu", ""], ["Shen", "Ying", ""]]}, {"id": "1812.01969", "submitter": "Jiri Fajtl", "authors": "Jiri Fajtl, Hajar Sadeghi Sokeh, Vasileios Argyriou, Dorothy\n  Monekosso, Paolo Remagnino", "title": "Summarizing Videos with Attention", "comments": "Presented at ACCV2018 AIU2018 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel method for supervised, keyshots based video\nsummarization by applying a conceptually simple and computationally efficient\nsoft, self-attention mechanism. Current state of the art methods leverage\nbi-directional recurrent networks such as BiLSTM combined with attention. These\nnetworks are complex to implement and computationally demanding compared to\nfully connected networks. To that end we propose a simple, self-attention based\nnetwork for video summarization which performs the entire sequence to sequence\ntransformation in a single feed forward pass and single backward pass during\ntraining. Our method sets a new state of the art results on two benchmarks\nTvSum and SumMe, commonly used in this domain.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 13:00:04 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 09:52:11 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Fajtl", "Jiri", ""], ["Sokeh", "Hajar Sadeghi", ""], ["Argyriou", "Vasileios", ""], ["Monekosso", "Dorothy", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1812.01995", "submitter": "Konno Tomohiko", "authors": "Tomohiko Konno, Hodaka Kurokawa, Fuyuki Nabeshima, Yuki Sakishita, Ryo\n  Ogawa, Iwao Hosako, Atsutaka Maeda", "title": "Deep Learning Model for Finding New Superconductors", "comments": "10 pages in main text. Deep learning, Machine learning, Material\n  search, Superconductors", "journal-ref": "Phys. Rev. B 103, 014509 (2021)", "doi": "10.1103/PhysRevB.103.014509", "report-no": null, "categories": "cs.LG cond-mat.mtrl-sci cond-mat.supr-con cs.CL physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration of new superconductors still relies on the experience and\nintuition of experts and is largely a process of experimental trial and error.\nIn one study, only 3% of the candidate materials showed superconductivity.\nHere, we report the first deep learning model for finding new superconductors.\nWe introduced the method named \"reading periodic table\" which represented the\nperiodic table in a way that allows deep learning to learn to read the periodic\ntable and to learn the law of elements for the purpose of discovering novel\nsuperconductors that are outside the training data. It is recognized that it is\ndifficult for deep learning to predict something outside the training data.\nAlthough we used only the chemical composition of materials as information, we\nobtained an $R^{2}$ value of 0.92 for predicting $T_\\text{c}$ for materials in\na database of superconductors. We also introduced the method named \"garbage-in\"\nto create synthetic data of non-superconductors that do not exist.\nNon-superconductors are not reported, but the data must be required for deep\nlearning to distinguish between superconductors and non-superconductors. We\nobtained three remarkable results. The deep learning can predict\nsuperconductivity for a material with a precision of 62%, which shows the\nusefulness of the model; it found the recently discovered superconductor CaBi2\nand another one Hf0.5Nb0.2V2Zr0.3, neither of which is in the superconductor\ndatabase; and it found Fe-based high-temperature superconductors (discovered in\n2008) from the training data before 2008. These results open the way for the\ndiscovery of new high-temperature superconductor families. The candidate\nmaterials list, data, and method are openly available from the link\nhttps://github.com/tomo835g/Deep-Learning-to-find-Superconductors.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 05:30:34 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 07:22:53 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 14:29:01 GMT"}, {"version": "v4", "created": "Thu, 14 Jan 2021 14:36:38 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Konno", "Tomohiko", ""], ["Kurokawa", "Hodaka", ""], ["Nabeshima", "Fuyuki", ""], ["Sakishita", "Yuki", ""], ["Ogawa", "Ryo", ""], ["Hosako", "Iwao", ""], ["Maeda", "Atsutaka", ""]]}, {"id": "1812.02119", "submitter": "Christian Jilek", "authors": "Christian Jilek, Markus Schr\\\"oder, Rudolf Novik, Sven Schwarz, Heiko\n  Maus, Andreas Dengel", "title": "Inflection-Tolerant Ontology-Based Named Entity Recognition for\n  Real-Time Applications", "comments": "14 pages, 11 figures", "journal-ref": "2nd Conference on Language, Data and Knowledge (LDK 2019),\n  OpenAccess Series in Informatics (OASIcs), Vol. 70, pp. 11:1-11:14", "doi": "10.4230/OASIcs.LDK.2019.11", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of applications users daily interact with have to operate in\n(near) real-time: chatbots, digital companions, knowledge work support systems\n-- just to name a few. To perform the services desired by the user, these\nsystems have to analyze user activity logs or explicit user input extremely\nfast. In particular, text content (e.g. in form of text snippets) needs to be\nprocessed in an information extraction task. Regarding the aforementioned\ntemporal requirements, this has to be accomplished in just a few milliseconds,\nwhich limits the number of methods that can be applied. Practically, only very\nfast methods remain, which on the other hand deliver worse results than slower\nbut more sophisticated Natural Language Processing (NLP) pipelines. In this\npaper, we investigate and propose methods for real-time capable Named Entity\nRecognition (NER). As a first improvement step we address are word variations\ninduced by inflection, for example present in the German language. Our approach\nis ontology-based and makes use of several language information sources like\nWiktionary. We evaluated it using the German Wikipedia (about 9.4B characters),\nfor which the whole NER process took considerably less than an hour. Since\nprecision and recall are higher than with comparably fast methods, we conclude\nthat the quality gap between high speed methods and sophisticated NLP pipelines\ncan be narrowed a bit more without losing too much runtime performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:17:30 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Jilek", "Christian", ""], ["Schr\u00f6der", "Markus", ""], ["Novik", "Rudolf", ""], ["Schwarz", "Sven", ""], ["Maus", "Heiko", ""], ["Dengel", "Andreas", ""]]}, {"id": "1812.02142", "submitter": "Zhehuai Chen", "authors": "Zhehuai Chen, Mahaveer Jain, Yongqiang Wang, Michael L. Seltzer,\n  Christian Fuegen", "title": "End-to-end contextual speech recognition using class language models and\n  a token passing decoder", "comments": "submit to ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end modeling (E2E) of automatic speech recognition (ASR) blends all\nthe components of a traditional speech recognition system into a unified model.\nAlthough it simplifies training and decoding pipelines, the unified model is\nhard to adapt when mismatch exists between training and test data. In this\nwork, we focus on contextual speech recognition, which is particularly\nchallenging for E2E models because it introduces significant mismatch between\ntraining and test data. To improve the performance in the presence of complex\ncontextual information, we propose to use class-based language models(CLM) that\ncan populate the classes with contextdependent information in real-time. To\nenable this approach to scale to a large number of class members and minimize\nsearch errors, we propose a token passing decoder with efficient token\nrecombination for E2E systems for the first time. We evaluate the proposed\nsystem on general and contextual ASR, and achieve relative 62% Word Error\nRate(WER) reduction for contextual ASR without hurting performance for general\nASR. We show that the proposed method performs well without modification of the\ndecoding hyper-parameters across tasks, making it a general solution for E2E\nASR.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 18:00:36 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Chen", "Zhehuai", ""], ["Jain", "Mahaveer", ""], ["Wang", "Yongqiang", ""], ["Seltzer", "Michael L.", ""], ["Fuegen", "Christian", ""]]}, {"id": "1812.02205", "submitter": "Dominika Basaj", "authors": "Barbara Rychalska, Dominika Basaj, Przemyslaw Biecek", "title": "Are you tough enough? Framework for Robustness Validation of Machine\n  Comprehension Systems", "comments": "Accepted to Conference on Neural Information Processing Systems\n  (NeurIPS) 2018 workshop IRASL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning NLP domain lacks procedures for the analysis of model\nrobustness. In this paper we propose a framework which validates robustness of\nany Question Answering model through model explainers. We propose that a robust\nmodel should transgress the initial notion of semantic similarity induced by\nword embeddings to learn a more human-like understanding of meaning. We test\nthis property by manipulating questions in two ways: swapping important\nquestion word for 1) its semantically correct synonym and 2) for word vector\nthat is close in embedding space. We estimate importance of words in asked\nquestions with Locally Interpretable Model Agnostic Explanations method (LIME).\nWith these two steps we compare state-of-the-art Q&A models. We show that\nalthough accuracy of state-of-the-art models is high, they are very fragile to\nchanges in the input. Moreover, we propose 2 adversarial training scenarios\nwhich raise model sensitivity to true synonyms by up to 7% accuracy measure.\nOur findings help to understand which models are more stable and how they can\nbe improved. In addition, we have created and published a new dataset that may\nbe used for validation of robustness of a Q&A model.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 19:55:24 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Rychalska", "Barbara", ""], ["Basaj", "Dominika", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "1812.02253", "submitter": "Aditi Chaudhary", "authors": "Aditi Chaudhary, Bhargavi Paranjape, Michiel de Jong", "title": "Weighted Global Normalization for Multiple Choice ReadingComprehension\n  over Long Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent evidence pointing out the fragility of high-performing\nspan prediction models, we direct our attention to multiple choice reading\ncomprehension. In particular, this work introduces a novel method for improving\nanswer selection on long documents through weighted global normalization of\npredictions over portions of the documents. We show that applying our method to\na span prediction model adapted for answer selection helps model performance on\nlong summaries from NarrativeQA, a challenging reading comprehension dataset\nwith an answer selection task, and we strongly improve on the task baseline\nperformance by +36.2 Mean Reciprocal Rank.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 22:27:20 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Chaudhary", "Aditi", ""], ["Paranjape", "Bhargavi", ""], ["de Jong", "Michiel", ""]]}, {"id": "1812.02303", "submitter": "Tian Shi", "authors": "Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, Chandan K. Reddy", "title": "Neural Abstractive Text Summarization with Sequence-to-Sequence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, neural abstractive text summarization with\nsequence-to-sequence (seq2seq) models have gained a lot of popularity. Many\ninteresting techniques have been proposed to improve seq2seq models, making\nthem capable of handling different challenges, such as saliency, fluency and\nhuman readability, and generate high-quality summaries. Generally speaking,\nmost of these techniques differ in one of these three categories: network\nstructure, parameter inference, and decoding/generation. There are also other\nconcerns, such as efficiency and parallelism for training a model. In this\npaper, we provide a comprehensive literature survey on different seq2seq models\nfor abstractive text summarization from the viewpoint of network structures,\ntraining strategies, and summary generation algorithms. Several models were\nfirst proposed for language modeling and generation tasks, such as machine\ntranslation, and later applied to abstractive text summarization. Hence, we\nalso provide a brief review of these models. As part of this survey, we also\ndevelop an open source library, namely, Neural Abstractive Text Summarizer\n(NATS) toolkit, for the abstractive text summarization. An extensive set of\nexperiments have been conducted on the widely used CNN/Daily Mail dataset to\nexamine the effectiveness of several different neural network components.\nFinally, we benchmark two models implemented in NATS on the two recently\nreleased datasets, namely, Newsroom and Bytecup.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 04:06:27 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 02:11:27 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 14:38:09 GMT"}, {"version": "v4", "created": "Fri, 18 Sep 2020 19:03:14 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Shi", "Tian", ""], ["Keneshloo", "Yaser", ""], ["Ramakrishnan", "Naren", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "1812.02305", "submitter": "Mohammad Tahsin Mostafiz", "authors": "Tahsin Mostafiz, Khalid Ashraf", "title": "Pathology Extraction from Chest X-Ray Radiology Reports: A Performance\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extraction of relevant pathological terms from radiology reports is important\nfor correct image label generation and disease population studies. In this\nletter, we compare the performance of some known application program interface\n(APIs) for the task of thoracic abnormality extraction from radiology reports.\nWe explored several medical domain specific annotation tools like Medical Text\nIndexer(MTI) with Non-MEDLINE and Mesh On Demand(MOD) options and generic\nNatural Language Understanding (NLU) API provided by the IBM cloud. Our results\nshow that although MTI and MOD are intended for extracting medical terms, their\nperformance is worst compared to generic extraction API like IBM NLU. Finally,\nwe trained a DNN-based Named Entity Recognition (NER) model to extract the key\nconcept words from radiology reports. Our model outperforms the medical\nspecific and generic API performance by a large margin. Our results demonstrate\nthe inadequacy of generic APIs for pathology extraction task and establish the\nimportance of domain specific model training for improved results. We hope that\nthese results motivate the research community to release larger de-identified\nradiology reports corpus for building high accuracy machine learning models for\nthe important task of pathology extraction.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 06:56:16 GMT"}], "update_date": "2018-12-08", "authors_parsed": [["Mostafiz", "Tahsin", ""], ["Ashraf", "Khalid", ""]]}, {"id": "1812.02307", "submitter": "Mario Graff", "authors": "Mario Graff and Sabino Miranda-Jim\\'enez and Eric S. Tellez and\n  Daniela Moctezuma", "title": "EvoMSA: A Multilingual Evolutionary Approach for Sentiment Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/MCI.2019.2954668", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis (SA) is a task related to understanding people's feelings\nin written text; the starting point would be to identify the polarity level\n(positive, neutral or negative) of a given text, moving on to identify emotions\nor whether a text is humorous or not. This task has been the subject of several\nresearch competitions in a number of languages, e.g., English, Spanish, and\nArabic, among others. In this contribution, we propose an SA system, namely\nEvoMSA, that unifies our participating systems in various SA competitions,\nmaking it domain independent and multilingual by processing text using only\nlanguage-independent techniques. EvoMSA is a classifier, based on Genetic\nProgramming, that works by combining the output of different text classifiers\nand text models to produce the final prediction. We analyze EvoMSA on different\nSA competitions to provide a global overview of its performance, and as the\nresults show, EvoMSA is competitive obtaining top rankings in several SA\ncompetitions. Furthermore, we performed an analysis of EvoMSA's components to\nmeasure their contribution to the performance; the idea is to facilitate a\npractitioner or newcomer to implement a competitive SA classifier. Finally, it\nis worth to mention that EvoMSA is available as open-source software.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 23:33:59 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 03:12:40 GMT"}, {"version": "v3", "created": "Fri, 14 Jun 2019 18:10:24 GMT"}, {"version": "v4", "created": "Mon, 30 Sep 2019 16:52:27 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Graff", "Mario", ""], ["Miranda-Jim\u00e9nez", "Sabino", ""], ["Tellez", "Eric S.", ""], ["Moctezuma", "Daniela", ""]]}, {"id": "1812.02308", "submitter": "Jan Kremer", "authors": "Jan Kremer, Lasse Borgholt, Lars Maal{\\o}e", "title": "On the Inductive Bias of Word-Character-Level Multi-Task Learning for\n  Speech Recognition", "comments": "Accepted at the IRASL workshop at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end automatic speech recognition (ASR) commonly transcribes audio\nsignals into sequences of characters while its performance is evaluated by\nmeasuring the word-error rate (WER). This suggests that predicting sequences of\nwords directly may be helpful instead. However, training with word-level\nsupervision can be more difficult due to the sparsity of examples per label\nclass. In this paper we analyze an end-to-end ASR model that combines a\nword-and-character representation in a multi-task learning (MTL) framework. We\nshow that it improves on the WER and study how the word-level model can benefit\nfrom character-level supervision by analyzing the learned inductive preference\nbias of each model component empirically. We find that by adding\ncharacter-level supervision, the MTL model interpolates between recognizing\nmore frequent words (preferred by the word-level model) and shorter words\n(preferred by the character-level model).\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 10:37:24 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Kremer", "Jan", ""], ["Borgholt", "Lasse", ""], ["Maal\u00f8e", "Lars", ""]]}, {"id": "1812.02309", "submitter": "Lina Fatima Soualmia", "authors": "Sa\\\"id Abdedda\\\"im, Sylvestre Vimard, Lina Fatima Soualmia", "title": "The MeSH-gram Neural Network Model: Extending Word Embedding Vectors\n  with MeSH Concepts for UMLS Semantic Similarity and Relatedness in the\n  Biomedical Domain", "comments": "6 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliciting semantic similarity between concepts in the biomedical domain\nremains a challenging task. Recent approaches founded on embedding vectors have\ngained in popularity as they risen to efficiently capture semantic\nrelationships The underlying idea is that two words that have close meaning\ngather similar contexts. In this study, we propose a new neural network model\nnamed MeSH-gram which relies on a straighforward approach that extends the\nskip-gram neural network model by considering MeSH (Medical Subject Headings)\ndescriptors instead words. Trained on publicly available corpus PubMed MEDLINE,\nMeSH-gram is evaluated on reference standards manually annotated for semantic\nsimilarity. MeSH-gram is first compared to skip-gram with vectors of size 300\nand at several windows contexts. A deeper comparison is performed with tewenty\nexisting models. All the obtained results of Spearman's rank correlations\nbetween human scores and computed similarities show that MeSH-gram outperforms\nthe skip-gram model, and is comparable to the best methods but that need more\ncomputation and external resources.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 07:48:27 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Abdedda\u00efm", "Sa\u00efd", ""], ["Vimard", "Sylvestre", ""], ["Soualmia", "Lina Fatima", ""]]}, {"id": "1812.02317", "submitter": "Siyao Peng", "authors": "Yilun Zhu, Yang Liu, Siyao Peng, Austin Blodgett, Yushi Zhao, Nathan\n  Schneider", "title": "Adpositional Supersenses for Mandarin Chinese", "comments": "4 pages", "journal-ref": "Zhu, Yilun; Liu, Yang; Peng, Siyao; Blodgett, Austin; Zhao, Yushi;\n  and Schneider, Nathan (2019) \"Adpositional Supersenses for Mandarin Chinese,\"\n  Proceedings of the Society for Computation in Linguistics: Vol. 2 , Article\n  40", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study adapts Semantic Network of Adposition and Case Supersenses (SNACS)\nannotation to Mandarin Chinese and demonstrates that the same supersense\ncategories are appropriate for Chinese adposition semantics. We annotated 15\nchapters of The Little Prince, with high interannotator agreement. The parallel\ncorpus gives insight into differences in construal between the two languages'\nadpositions, namely a number of construals that are frequent in Chinese but\nrare or unattested in the English corpus. The annotated corpus can further\nsupport automatic disambiguation of adpositions in Chinese, and the common\ninventory of supersenses between the two languages can potentially serve\ncross-linguistic tasks such as machine translation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 02:54:49 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Zhu", "Yilun", ""], ["Liu", "Yang", ""], ["Peng", "Siyao", ""], ["Blodgett", "Austin", ""], ["Zhao", "Yushi", ""], ["Schneider", "Nathan", ""]]}, {"id": "1812.02354", "submitter": "Yang Deng", "authors": "Yang Deng, Yuexiang Xie, Yaliang Li, Min Yang, Nan Du, Wei Fan, Kai\n  Lei, Ying Shen", "title": "Multi-Task Learning with Multi-View Attention for Answer Selection and\n  Knowledge Base Question Answering", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer selection and knowledge base question answering (KBQA) are two\nimportant tasks of question answering (QA) systems. Existing methods solve\nthese two tasks separately, which requires large number of repetitive work and\nneglects the rich correlation information between tasks. In this paper, we\ntackle answer selection and KBQA tasks simultaneously via multi-task learning\n(MTL), motivated by the following motivations. First, both answer selection and\nKBQA can be regarded as a ranking problem, with one at text-level while the\nother at knowledge-level. Second, these two tasks can benefit each other:\nanswer selection can incorporate the external knowledge from knowledge base\n(KB), while KBQA can be improved by learning contextual information from answer\nselection. To fulfill the goal of jointly learning these two tasks, we propose\na novel multi-task learning scheme that utilizes multi-view attention learned\nfrom various perspectives to enable these tasks to interact with each other as\nwell as learn more comprehensive sentence representations. The experiments\nconducted on several real-world datasets demonstrate the effectiveness of the\nproposed method, and the performance of answer selection and KBQA is improved.\nAlso, the multi-view attention scheme is proved to be effective in assembling\nattentive information from different representational perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 05:20:59 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Deng", "Yang", ""], ["Xie", "Yuexiang", ""], ["Li", "Yaliang", ""], ["Yang", "Min", ""], ["Du", "Nan", ""], ["Fan", "Wei", ""], ["Lei", "Kai", ""], ["Shen", "Ying", ""]]}, {"id": "1812.02370", "submitter": "Pratik Jayarao", "authors": "Pratik Jayarao, Chirag Jain, Aman Srivastava", "title": "Exploring the importance of context and embeddings in neural NER models\n  for task-oriented dialogue systems", "comments": "6 Pages Accepted at International Conference on Natural Language\n  Processing (2018) - (ACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER), a classic sequence labelling task, is an\nessential component of natural language understanding (NLU) systems in\ntask-oriented dialog systems for slot filling. For well over a decade,\ndifferent methods from lookup using gazetteers and domain ontology, classifiers\nover handcrafted features to end-to-end systems involving neural network\narchitectures have been evaluated mostly in language-independent\nnon-conversational settings. In this paper, we evaluate a modified version of\nthe recent state of the art neural architecture in a conversational setting\nwhere messages are often short and noisy. We perform an array of experiments\nwith different combinations of including the previous utterance in the dialogue\nas a source of additional features and using word and character level\nembeddings trained on a larger external corpus. All methods are evaluated on a\ncombined dataset formed from two public English task-oriented conversational\ndatasets belonging to travel and restaurant domains respectively. For\nadditional evaluation, we also repeat some of our experiments after adding\nautomatically translated and transliterated (from translated) versions to the\nEnglish only dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 06:53:37 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Jayarao", "Pratik", ""], ["Jain", "Chirag", ""], ["Srivastava", "Aman", ""]]}, {"id": "1812.02455", "submitter": "Dan Liu", "authors": "Dan Liu, Junhua Liu, Wu Guo, Shifu Xiong, Zhiqiang Ma, Rui Song,\n  Chongliang Wu, Quan Liu", "title": "The USTC-NEL Speech Translation system at IWSLT 2018", "comments": "5 pages, 8 tabels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the USTC-NEL system to the speech translation task of\nthe IWSLT Evaluation 2018. The system is a conventional pipeline system which\ncontains 3 modules: speech recognition, post-processing and machine\ntranslation. We train a group of hybrid-HMM models for our speech recognition,\nand for machine translation we train transformer based neural machine\ntranslation models with speech recognition output style text as input.\nExperiments conducted on the IWSLT 2018 task indicate that, compared to\nbaseline system from KIT, our system achieved 14.9 BLEU improvement.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 10:57:29 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Liu", "Dan", ""], ["Liu", "Junhua", ""], ["Guo", "Wu", ""], ["Xiong", "Shifu", ""], ["Ma", "Zhiqiang", ""], ["Song", "Rui", ""], ["Wu", "Chongliang", ""], ["Liu", "Quan", ""]]}, {"id": "1812.02536", "submitter": "Sherzod Hakimov", "authors": "Sherzod Hakimov, Soufian Jebbara, Philipp Cimiano", "title": "Evaluating Architectural Choices for Deep Learning Approaches for\n  Question Answering over Knowledge Bases", "comments": "the longer version than the original publication at ICSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of answering natural language questions over knowledge bases has\nreceived wide attention in recent years. Various deep learning architectures\nhave been proposed for this task. However, architectural design choices are\ntypically not systematically compared nor evaluated under the same conditions.\nIn this paper, we contribute to a better understanding of the impact of\narchitectural design choices by evaluating four different architectures under\nthe same conditions. We address the task of answering simple questions,\nconsisting in predicting the subject and predicate of a triple given a\nquestion. In order to provide a fair comparison of different architectures, we\nevaluate them under the same strategy for inferring the subject, and compare\ndifferent architectures for inferring the predicate. The architecture for\ninferring the subject is based on a standard LSTM model trained to recognize\nthe span of the subject in the question and on a linking component that links\nthe subject span to an entity in the knowledge base. The architectures for\npredicate inference are based on i) a standard softmax classifier ranging over\nall predicates as output, iii) a model that predicts a low-dimensional encoding\nof the property given entity representation and question, iii) a model that\nlearns to score a pair of subject and predicate given the question as well as\niv) a model based on the well-known FastText model. The comparison of\narchitectures shows that FastText provides better results than other\narchitectures.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 14:11:25 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 09:36:17 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Hakimov", "Sherzod", ""], ["Jebbara", "Soufian", ""], ["Cimiano", "Philipp", ""]]}, {"id": "1812.02627", "submitter": "Jeffrey Thompson PhD", "authors": "Jeffrey Thompson, Jinxiang Hu, Dinesh Pal Mudaranthakam, David\n  Streeter, Lisa Neums, Michele Park, Devin C. Koestler, Byron Gajewski,\n  Matthew S. Mayo", "title": "Relevant Word Order Vectorization for Improved Natural Language\n  Processing in Electronic Healthcare Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Electronic health records (EHR) represent a rich resource for\nconducting observational studies, supporting clinical trials, and more.\nHowever, much of the relevant information is stored in an unstructured format\nthat makes it difficult to use. Natural language processing approaches that\nattempt to automatically classify the data depend on vectorization algorithms\nthat impose structure on the text, but these algorithms were not designed for\nthe unique characteristics of EHR. Here, we propose a new algorithm for\nstructuring so-called free-text that may help researchers make better use of\nEHR. We call this method Relevant Word Order Vectorization (RWOV).\n  Materials and Methods: As a proof-of-concept, we attempted to classify the\nhormone receptor status of breast cancer patients treated at the University of\nKansas Medical Center during a recent year, from the unstructured text of\npathology reports. Our approach attempts to account for the semi-structured way\nthat healthcare providers often enter information. We compared this approach to\nthe ngrams and word2vec methods.\n  Results: Our approach resulted in the most consistently high accuracy, as\nmeasured by F1 score and area under the receiver operating characteristic curve\n(AUC).\n  Discussion: Our results suggest that methods of structuring free text that\ntake into account its context may show better performance, and that our\napproach is promising.\n  Conclusion: By using a method that accounts for the fact that healthcare\nproviders tend to use certain key words repetitively and that the order of\nthese key words is important, we showed improved performance over methods that\ndo not.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 16:01:13 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Thompson", "Jeffrey", ""], ["Hu", "Jinxiang", ""], ["Mudaranthakam", "Dinesh Pal", ""], ["Streeter", "David", ""], ["Neums", "Lisa", ""], ["Park", "Michele", ""], ["Koestler", "Devin C.", ""], ["Gajewski", "Byron", ""], ["Mayo", "Matthew S.", ""]]}, {"id": "1812.02655", "submitter": "Marco Viviani", "authors": "Elias Bassani and Marco Viviani", "title": "Feature Analysis for Assessing the Quality of Wikipedia Articles through\n  Supervised Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, thanks to Web 2.0 technologies, people have the possibility to\ngenerate and spread contents on different social media in a very easy way. In\nthis context, the evaluation of the quality of the information that is\navailable online is becoming more and more a crucial issue. In fact, a constant\nflow of contents is generated every day by often unknown sources, which are not\ncertified by traditional authoritative entities. This requires the development\nof appropriate methodologies that can evaluate in a systematic way these\ncontents, based on `objective' aspects connected with them. This would help\nindividuals, who nowadays tend to increasingly form their opinions based on\nwhat they read online and on social media, to come into contact with\ninformation that is actually useful and verified. Wikipedia is nowadays one of\nthe biggest online resources on which users rely as a source of information.\nThe amount of collaboratively generated content that is sent to the online\nencyclopedia every day can let to the possible creation of low-quality articles\n(and, consequently, misinformation) if not properly monitored and revised. For\nthis reason, in this paper, the problem of automatically assessing the quality\nof Wikipedia articles is considered. In particular, the focus is on the\nanalysis of hand-crafted features that can be employed by supervised machine\nlearning techniques to perform the classification of Wikipedia articles on\nqualitative bases. With respect to prior literature, a wider set of\ncharacteristics connected to Wikipedia articles are taken into account and\nillustrated in detail. Evaluations are performed by considering a labeled\ndataset provided in a prior work, and different supervised machine learning\nalgorithms, which produced encouraging results with respect to the considered\nfeatures.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 16:45:00 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Bassani", "Elias", ""], ["Viviani", "Marco", ""]]}, {"id": "1812.02793", "submitter": "Jiaqi Guan", "authors": "Jiaqi Guan, Runzhe Li, Sheng Yu, Xuegong Zhang", "title": "Generation of Synthetic Electronic Medical Record Text", "comments": "7 pages, BIBM 2018 regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) and Natural Language Processing (NLP) have achieved\nremarkable success in many fields and have brought new opportunities and high\nexpectation in the analyses of medical data. The most common type of medical\ndata is the massive free-text electronic medical records (EMR). It is widely\nregarded that mining such massive data can bring up important information for\nimproving medical practices as well as for possible new discoveries on complex\ndiseases. However, the free EMR texts are lacking consistent standards, rich of\nprivate information, and limited in availability. Also, as they are accumulated\nfrom everyday practices, it is often hard to have a balanced number of samples\nfor the types of diseases under study. These problems hinder the development of\nML and NLP methods for EMR data analysis. To tackle these problems, we\ndeveloped a model to generate synthetic text of EMRs called Medical Text\nGenerative Adversarial Network or mtGAN. It is based on the GAN framework and\nis trained by the REINFORCE algorithm. It takes disease features as inputs and\ngenerates synthetic texts as EMRs for the corresponding diseases. We evaluate\nthe model from micro-level, macro-level and application-level on a Chinese EMR\ntext dataset. The results show that the method has a good capacity to fit real\ndata and can generate realistic and diverse EMR samples. This provides a novel\nway to avoid potential leakage of patient privacy while still supply sufficient\nwell-controlled cohort data for developing downstream ML and NLP methods. It\ncan also be used as a data augmentation method to assist studies based on real\nEMR data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 20:35:14 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Guan", "Jiaqi", ""], ["Li", "Runzhe", ""], ["Yu", "Sheng", ""], ["Zhang", "Xuegong", ""]]}, {"id": "1812.02802", "submitter": "Raziel Alvarez", "authors": "Alvarez Raziel, and Park Hyun-Jin", "title": "End-to-End Streaming Keyword Spotting", "comments": "Accepted in International Conference on Acoustics, Speech, and Signal\n  Processing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for keyword spotting that, except for a frontend\ncomponent for feature generation, it is entirely contained in a deep neural\nnetwork (DNN) model trained \"end-to-end\" to predict the presence of the keyword\nin a stream of audio. The main contributions of this work are, first, an\nefficient memoized neural network topology that aims at making better use of\nthe parameters and associated computations in the DNN by holding a memory of\nprevious activations distributed over the depth of the DNN. The second\ncontribution is a method to train the DNN, end-to-end, to produce the keyword\nspotting score. This system significantly outperforms previous approaches both\nin terms of quality of detection as well as size and computation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 21:00:58 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 05:05:25 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Raziel", "Alvarez", ""], ["Hyun-Jin", "Park", ""]]}, {"id": "1812.02825", "submitter": "Artit Wangperawong", "authors": "Artit Wangperawong", "title": "Attending to Mathematical Language with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical expressions were generated, evaluated and used to train neural\nnetwork models based on the transformer architecture. The expressions and their\ntargets were analyzed as a character-level sequence transduction task in which\nthe encoder and decoder are built on attention mechanisms. Three models were\ntrained to understand and evaluate symbolic variables and expressions in\nmathematics: (1) the self-attentive and feed-forward transformer without\nrecurrence or convolution, (2) the universal transformer with recurrence, and\n(3) the adaptive universal transformer with recurrence and adaptive computation\ntime. The models respectively achieved test accuracies as high as 76.1%, 78.8%\nand 84.9% in evaluating the expressions to match the target values. For the\ncases inferred incorrectly, the results differed from the targets by only one\nor two characters. The models notably learned to add, subtract and multiply\nboth positive and negative decimal numbers of variable digits assigned to\nsymbolic variables.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 03:05:08 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 00:10:58 GMT"}, {"version": "v3", "created": "Sun, 30 Jun 2019 17:46:58 GMT"}, {"version": "v4", "created": "Thu, 12 Sep 2019 01:09:37 GMT"}, {"version": "v5", "created": "Sat, 14 Sep 2019 20:03:50 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Wangperawong", "Artit", ""]]}, {"id": "1812.02914", "submitter": "Pratik Jayarao", "authors": "Pratik Jayarao, Aman Srivastava", "title": "Intent Detection for code-mix utterances in task oriented dialogue\n  systems", "comments": "5 Pages, Accepted at 2018 Third International Conference on\n  Electrical, Electronics, Communication, Computer Technologies and\n  Optimization Techniques (ICEECCOT) 14-15,December 2018 (IEEE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intent detection is an essential component of task oriented dialogue systems.\nOver the years, extensive research has been conducted resulting in many state\nof the art models directed towards resolving user's intents in dialogue. A\nvariety of vector representations foruser utterances have been explored for the\nsame. However, these models and vectorization approaches have more so been\nevaluated in a single language environment. Dialogude systems generally have to\ndeal with queries in different languages. We thus conduct experiments across\ncombinations of models and various vectors representations for Code Mix as well\nas multi language utterances and evaluate how these models scale to a multi\nlanguage environment. Our aim is to find the best suitable combination of\nvector representation and models for the process of intent detection for Code\nMix utterances. we have evaluated the experiments on two different datasets\nconsisting of only Code Mix utterances and the other dataset consisting of\nEnglish, Hindi and Code Mix English Hindi utterances.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 05:23:12 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Jayarao", "Pratik", ""], ["Srivastava", "Aman", ""]]}, {"id": "1812.02971", "submitter": "Traian Rebedea", "authors": "George-Sebastian Pirtoaca, Traian Rebedea, Stefan Ruseti", "title": "Improving Retrieval-Based Question Answering with Deep Inference Models", "comments": "8 pages, 2 figures, 8 tables, accepted at IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering is one of the most important and difficult applications at\nthe border of information retrieval and natural language processing, especially\nwhen we talk about complex science questions which require some form of\ninference to determine the correct answer. In this paper, we present a two-step\nmethod that combines information retrieval techniques optimized for question\nanswering with deep learning models for natural language inference in order to\ntackle the multi-choice question answering in the science domain. For each\nquestion-answer pair, we use standard retrieval-based models to find relevant\ncandidate contexts and decompose the main problem into two different\nsub-problems. First, assign correctness scores for each candidate answer based\non the context using retrieval models from Lucene. Second, we use deep learning\narchitectures to compute if a candidate answer can be inferred from some\nwell-chosen context consisting of sentences retrieved from the knowledge base.\nIn the end, all these solvers are combined using a simple neural network to\npredict the correct answer. This proposed two-step model outperforms the best\nretrieval-based solver by over 3% in absolute accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 10:44:14 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 18:38:33 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Pirtoaca", "George-Sebastian", ""], ["Rebedea", "Traian", ""], ["Ruseti", "Stefan", ""]]}, {"id": "1812.03258", "submitter": "Amir Karami", "authors": "Amir Karami, Aida Elkouri", "title": "Political Popularity Analysis in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popularity is a critical success factor for a politician and her/his party to\nwin in elections and implement their plans. Finding the reasons behind the\npopularity can provide a stable political movement. This research attempts to\nmeasure popularity in Twitter using a mixed method. In recent years, Twitter\ndata has provided an excellent opportunity for exploring public opinions by\nanalyzing a large number of tweets. This study has collected and examined 4.5\nmillion tweets related to a US politician, Senator Bernie Sanders. This study\ninvestigated eight economic reasons behind the senator's popularity in Twitter.\nThis research has benefits for politicians, informatics experts, and\npolicymakers to explore public opinion. The collected data will also be\navailable for further investigation.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 03:17:29 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Karami", "Amir", ""], ["Elkouri", "Aida", ""]]}, {"id": "1812.03260", "submitter": "Amir Karami", "authors": "George Shaw and Amir Karami", "title": "An Exploratory Study of (#)Exercise in the Twittersphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media analytics allows us to extract, analyze, and establish semantic\nfrom user-generated contents in social media platforms. This study utilized a\nmixed method including a three-step process of data collection, topic modeling,\nand data annotation for recognizing exercise related patterns. Based on the\nfindings, 86% of the detected topics were identified as meaningful topics after\nconducting the data annotation process. The most discussed exercise-related\ntopics were physical activity (18.7%), lifestyle behaviors (6.6%), and dieting\n(4%). The results from our experiment indicate that the exploratory data\nanalysis is a practical approach to summarizing the various characteristics of\ntext data for different health and medical applications.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 03:21:26 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Shaw", "George", ""], ["Karami", "Amir", ""]]}, {"id": "1812.03361", "submitter": "Erfan Ghadery", "authors": "Erfan Ghadery, Sajad Movahedi, Heshaam Faili, Azadeh Shakery", "title": "An Unsupervised Approach for Aspect Category Detection Using Soft Cosine\n  Similarity Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect category detection is one of the important and challenging subtasks of\naspect-based sentiment analysis. Given a set of pre-defined categories, this\ntask aims to detect categories which are indicated implicitly or explicitly in\na given review sentence. Supervised machine learning approaches perform well to\naccomplish this subtask. Note that, the performance of these methods depends on\nthe availability of labeled train data, which is often difficult and costly to\nobtain. Besides, most of these supervised methods require feature engineering\nto perform well. In this paper, we propose an unsupervised method to address\naspect category detection task without the need for any feature engineering.\nOur method utilizes clusters of unlabeled reviews and soft cosine similarity\nmeasure to accomplish aspect category detection task. Experimental results on\nSemEval-2014 restaurant dataset shows that proposed unsupervised approach\noutperforms several baselines by a substantial margin.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 18:11:16 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 06:45:31 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Ghadery", "Erfan", ""], ["Movahedi", "Sajad", ""], ["Faili", "Heshaam", ""], ["Shakery", "Azadeh", ""]]}, {"id": "1812.03483", "submitter": "Yossi Adi", "authors": "Yossi Adi, Neil Zeghidour, Ronan Collobert, Nicolas Usunier, Vitaliy\n  Liptchinsky, Gabriel Synnaeve", "title": "To Reverse the Gradient or Not: An Empirical Comparison of Adversarial\n  and Multi-task Learning in Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcribed datasets typically contain speaker identity for each instance in\nthe data. We investigate two ways to incorporate this information during\ntraining: Multi-Task Learning and Adversarial Learning. In multi-task learning,\nthe goal is speaker prediction; we expect a performance improvement with this\njoint training if the two tasks of speech recognition and speaker recognition\nshare a common set of underlying features. In contrast, adversarial learning is\na means to learn representations invariant to the speaker. We then expect\nbetter performance if this learnt invariance helps generalizing to new\nspeakers. While the two approaches seem natural in the context of speech\nrecognition, they are incompatible because they correspond to opposite\ngradients back-propagated to the model. In order to better understand the\neffect of these approaches in terms of error rates, we compare both strategies\nin controlled settings. Moreover, we explore the use of additional\nuntranscribed data in a semi-supervised, adversarial learning manner to improve\nerror rates. Our results show that deep models trained on big datasets already\ndevelop invariant representations to speakers without any auxiliary loss. When\nconsidering adversarial learning and multi-task learning, the impact on the\nacoustic model seems minor. However, models trained in a semi-supervised manner\ncan improve error-rates.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 13:18:02 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 14:41:58 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 17:05:43 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Adi", "Yossi", ""], ["Zeghidour", "Neil", ""], ["Collobert", "Ronan", ""], ["Usunier", "Nicolas", ""], ["Liptchinsky", "Vitaliy", ""], ["Synnaeve", "Gabriel", ""]]}, {"id": "1812.03509", "submitter": "Ziming Li", "authors": "Ziming Li, Julia Kiseleva, Maarten de Rijke", "title": "Dialogue Generation: From Imitation Learning to Inverse Reinforcement\n  Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of adversarial dialogue generation models relies on the\nquality of the reward signal produced by the discriminator. The reward signal\nfrom a poor discriminator can be very sparse and unstable, which may lead the\ngenerator to fall into a local optimum or to produce nonsense replies. To\nalleviate the first problem, we first extend a recently proposed adversarial\ndialogue generation method to an adversarial imitation learning solution. Then,\nin the framework of adversarial inverse reinforcement learning, we propose a\nnew reward model for dialogue generation that can provide a more accurate and\nprecise reward signal for generator training. We evaluate the performance of\nthe resulting model with automatic metrics and human evaluations in two\nannotation settings. Our experimental results demonstrate that our model can\ngenerate more high-quality responses and achieve higher overall performance\nthan the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 16:05:43 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Li", "Ziming", ""], ["Kiseleva", "Julia", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1812.03519", "submitter": "Barathi Ganesh HB", "authors": "Vinayakumar R and Barathi Ganesh HB and Prabaharan Poornachandran and\n  Anand Kumar M and Soman KP", "title": "Deep-Net: Deep Neural Network for Cyber Security Use Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have witnessed as a powerful approach in this\nyear by solving long-standing Artificial intelligence (AI) supervised and\nunsupervised tasks exists in natural language processing, speech processing,\ncomputer vision and others. In this paper, we attempt to apply DNNs on three\ndifferent cyber security use cases: Android malware classification, incident\ndetection and fraud detection. The data set of each use case contains real\nknown benign and malicious activities samples. The efficient network\narchitecture for DNN is chosen by conducting various trails of experiments for\nnetwork parameters and network structures. The experiments of such chosen\nefficient configurations of DNNs are run up to 1000 epochs with learning rate\nset in the range [0.01-0.5]. Experiments of DNN performed well in comparison to\nthe classical machine learning algorithms in all cases of experiments of cyber\nsecurity use cases. This is due to the fact that DNNs implicitly extract and\nbuild better features, identifies the characteristics of the data that lead to\nbetter accuracy. The best accuracy obtained by DNN and XGBoost on Android\nmalware classification 0.940 and 0.741, incident detection 1.00 and 0.997 fraud\ndetection 0.972 and 0.916 respectively.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 16:44:56 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["R", "Vinayakumar", ""], ["HB", "Barathi Ganesh", ""], ["Poornachandran", "Prabaharan", ""], ["M", "Anand Kumar", ""], ["KP", "Soman", ""]]}, {"id": "1812.03593", "submitter": "Chenguang Zhu", "authors": "Chenguang Zhu and Michael Zeng and Xuedong Huang", "title": "SDNet: Contextualized Attention-based Deep Network for Conversational\n  Question Answering", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational question answering (CQA) is a novel QA task that requires\nunderstanding of dialogue context. Different from traditional single-turn\nmachine reading comprehension (MRC) tasks, CQA includes passage comprehension,\ncoreference resolution, and contextual understanding. In this paper, we propose\nan innovated contextualized attention-based deep neural network, SDNet, to fuse\ncontext into traditional MRC models. Our model leverages both inter-attention\nand self-attention to comprehend conversation context and extract relevant\ninformation from passage. Furthermore, we demonstrated a novel method to\nintegrate the latest BERT contextual model. Empirical results show the\neffectiveness of our model, which sets the new state of the art result in CoQA\nleaderboard, outperforming the previous best model by 1.6% F1. Our ensemble\nmodel further improves the result by 2.7% F1.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 01:43:14 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 12:57:32 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 02:13:10 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 02:14:18 GMT"}, {"version": "v5", "created": "Wed, 2 Jan 2019 23:24:54 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Zhu", "Chenguang", ""], ["Zeng", "Michael", ""], ["Huang", "Xuedong", ""]]}, {"id": "1812.03781", "submitter": "Pranav A", "authors": "Pranav A, Nick Sukiennik, Pan Hui", "title": "Inflo: News Categorization and Keyphrase Extraction for Implementation\n  in an Aggregation System", "comments": "Demo paper, links inside the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work herein describes a system for automatic news category and keyphrase\nlabeling, presented in the context of our motivation to improve the speed at\nwhich a user can find relevant and interesting content within an aggregation\nplatform. A set of 12 discrete categories were applied to over 500,000 news\narticles for training a neural network, to be used to facilitate the more\nin-depth task of extracting the most significant keyphrases. The latter was\ndone using three methods: statistical, graphical and numerical, using the\npre-identified category label to improve relevance of extracted phrases. The\nresults are presented in a demo in which the articles are pre-populated via\nNews API, and upon being selected, the category and keyphrase labels will be\ncomputed via the methods explained herein.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 13:36:09 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["A", "Pranav", ""], ["Sukiennik", "Nick", ""], ["Hui", "Pan", ""]]}, {"id": "1812.03919", "submitter": "Matthew Wiesner", "authors": "Matthew Wiesner, Adithya Renduchintala, Shinji Watanabe, Chunxi Liu,\n  Najim Dehak, Sanjeev Khudanpur", "title": "Pretraining by Backtranslation for End-to-end ASR in Low-Resource\n  Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore training attention-based encoder-decoder ASR in low-resource\nsettings. These models perform poorly when trained on small amounts of\ntranscribed speech, in part because they depend on having sufficient\ntarget-side text to train the attention and decoder networks. In this paper we\naddress this shortcoming by pretraining our network parameters using only\ntext-based data and transcribed speech from other languages. We analyze the\nrelative contributions of both sources of data. Across 3 test languages, our\ntext-based approach resulted in a 20% average relative improvement over a\ntext-based augmentation technique without pretraining. Using transcribed speech\nfrom nearby languages gives a further 20-30% relative reduction in character\nerror rate.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 16:57:21 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 22:40:48 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Wiesner", "Matthew", ""], ["Renduchintala", "Adithya", ""], ["Watanabe", "Shinji", ""], ["Liu", "Chunxi", ""], ["Dehak", "Najim", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "1812.04013", "submitter": "Simon DeDeo", "authors": "William H. W. Thompson and Zachary Wojtowicz and Simon DeDeo", "title": "L\\'{e}vy Flights of the Collective Imagination", "comments": "33 pages, 10 figures, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL nlin.AO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a structured random-walk model that captures key aspects of how\npeople communicate in groups. Our model takes the form of a correlated L\\'{e}vy\nflight that quantifies the balance between focused discussion of an idea and\nlong-distance leaps in semantic space. We apply our model to three cases of\nincreasing structural complexity: philosophical texts by Aristotle, Hume, and\nKant; four days of parliamentary debate during the French Revolution; and\nbranching comment trees on the discussion website Reddit. In the philosophical\nand parliamentary cases, the model parameters that describe this balance\nconverge under coarse-graining to limit regions that demonstrate the emergence\nof large-scale structure, a result which is robust to translation between\nlanguages. Meanwhile, we find that the political forum we consider on Reddit\nexhibits a debate-like pattern, while communities dedicated to the discussion\nof science and news show much less temporal order, and may make use of the\nemergent, tree-like topology of comment replies to structure their epistemic\nexplorations. Our model allows us to quantify the ways in which social\ntechnologies such as parliamentary procedures and online commenting systems\nshape the joint exploration of ideas.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 19:00:09 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Thompson", "William H. W.", ""], ["Wojtowicz", "Zachary", ""], ["DeDeo", "Simon", ""]]}, {"id": "1812.04081", "submitter": "Song Feng", "authors": "Paola Cascante-Bonilla, Xuwang Yin, Vicente Ordonez, Song Feng", "title": "Chat-crowd: A Dialog-based Platform for Visual Layout Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce Chat-crowd, an interactive environment for visual\nlayout composition via conversational interactions. Chat-crowd supports\nmultiple agents with two conversational roles: agents who play the role of a\ndesigner are in charge of placing objects in an editable canvas according to\ninstructions or commands issued by agents with a director role. The system can\nbe integrated with crowdsourcing platforms for both synchronous and\nasynchronous data collection and is equipped with comprehensive quality\ncontrols on the performance of both types of agents. We expect that this system\nwill be useful to build multimodal goal-oriented dialog tasks that require\nspatial and geometric reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 20:52:41 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 12:29:36 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 14:19:06 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Cascante-Bonilla", "Paola", ""], ["Yin", "Xuwang", ""], ["Ordonez", "Vicente", ""], ["Feng", "Song", ""]]}, {"id": "1812.04155", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill Dolan", "title": "Vision-based Navigation with Language-based Assistance via Imitation\n  Learning with Indirect Intervention", "comments": "In CVPR 2019, 16 pages, appendix included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Vision-based Navigation with Language-based Assistance (VNLA), a\ngrounded vision-language task where an agent with visual perception is guided\nvia language to find objects in photorealistic indoor environments. The task\nemulates a real-world scenario in that (a) the requester may not know how to\nnavigate to the target objects and thus makes requests by only specifying\nhigh-level end-goals, and (b) the agent is capable of sensing when it is lost\nand querying an advisor, who is more qualified at the task, to obtain language\nsubgoals to make progress. To model language-based assistance, we develop a\ngeneral framework termed Imitation Learning with Indirect Intervention (I3L),\nand propose a solution that is effective on the VNLA task. Empirical results\nshow that this approach significantly improves the success rate of the learning\nagent over other baselines in both seen and unseen environments. Our code and\ndata are publicly available at https://github.com/debadeepta/vnla .\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 23:48:25 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 13:40:30 GMT"}, {"version": "v3", "created": "Sat, 22 Dec 2018 03:27:28 GMT"}, {"version": "v4", "created": "Sat, 6 Apr 2019 02:02:42 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Nguyen", "Khanh", ""], ["Dey", "Debadeepta", ""], ["Brockett", "Chris", ""], ["Dolan", "Bill", ""]]}, {"id": "1812.04160", "submitter": "Xiao Zhang", "authors": "Xiao Zhang, Ji Wu, Dejing Dou", "title": "Delta Embedding Learning", "comments": "Accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised word embeddings have become a popular approach of word\nrepresentation in NLP tasks. However there are limitations to the semantics\nrepresented by unsupervised embeddings, and inadequate fine-tuning of\nembeddings can lead to suboptimal performance. We propose a novel learning\ntechnique called Delta Embedding Learning, which can be applied to general NLP\ntasks to improve performance by optimized tuning of the word embeddings. A\nstructured regularization is applied to the embeddings to ensure they are tuned\nin an incremental way. As a result, the tuned word embeddings become better\nword representations by absorbing semantic information from supervision without\n\"forgetting.\" We apply the method to various NLP tasks and see a consistent\nimprovement in performance. Evaluation also confirms the tuned word embeddings\nhave better semantic properties.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 00:19:32 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 03:37:17 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Zhang", "Xiao", ""], ["Wu", "Ji", ""], ["Dou", "Dejing", ""]]}, {"id": "1812.04199", "submitter": "Haruna Isah", "authors": "Dev Shah, Haruna Isah, Farhana Zulkernine", "title": "Predicting the Effects of News Sentiments on the Stock Market", "comments": "4 pages", "journal-ref": null, "doi": "10.1109/BigData.2018.8621884", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock market forecasting is very important in the planning of business\nactivities. Stock price prediction has attracted many researchers in multiple\ndisciplines including computer science, statistics, economics, finance, and\noperations research. Recent studies have shown that the vast amount of online\ninformation in the public domain such as Wikipedia usage pattern, news stories\nfrom the mainstream media, and social media discussions can have an observable\neffect on investors opinions towards financial markets. The reliability of the\ncomputational models on stock market prediction is important as it is very\nsensitive to the economy and can directly lead to financial loss. In this\npaper, we retrieved, extracted, and analyzed the effects of news sentiments on\nthe stock market. Our main contributions include the development of a sentiment\nanalysis dictionary for the financial sector, the development of a\ndictionary-based sentiment analysis model, and the evaluation of the model for\ngauging the effects of news sentiments on stocks for the pharmaceutical market.\nUsing only news sentiments, we achieved a directional accuracy of 70.59% in\npredicting the trends in short-term stock price movement.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 03:06:02 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Shah", "Dev", ""], ["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""]]}, {"id": "1812.04224", "submitter": "Zi Yin", "authors": "Zi Yin, Yuanyuan Shen", "title": "On the Dimensionality of Word Embedding", "comments": "18 pages, Advances in Neural Information Processing Systems 31\n  (NeurIPS 2018, Oral Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a theoretical understanding of word embedding and\nits dimensionality. Motivated by the unitary-invariance of word embedding, we\npropose the Pairwise Inner Product (PIP) loss, a novel metric on the\ndissimilarity between word embeddings. Using techniques from matrix\nperturbation theory, we reveal a fundamental bias-variance trade-off in\ndimensionality selection for word embeddings. This bias-variance trade-off\nsheds light on many empirical observations which were previously unexplained,\nfor example the existence of an optimal dimensionality. Moreover, new insights\nand discoveries, like when and how word embeddings are robust to over-fitting,\nare revealed. By optimizing over the bias-variance trade-off of the PIP loss,\nwe can explicitly answer the open question of dimensionality selection for word\nembedding.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 05:25:38 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Yin", "Zi", ""], ["Shen", "Yuanyuan", ""]]}, {"id": "1812.04238", "submitter": "Siddhant Srivastava", "authors": "Siddhant Srivastava, Anupam Shukla, Ritu Tiwari", "title": "Machine Translation : From Statistical to modern Deep-learning practices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation (MT) is an area of study in Natural Language processing\nwhich deals with the automatic translation of human language, from one language\nto another by the computer. Having a rich research history spanning nearly\nthree decades, Machine translation is one of the most sought after area of\nresearch in the linguistics and computational community. In this paper, we\ninvestigate the models based on deep learning that have achieved substantial\nprogress in recent years and becoming the prominent method in MT. We shall\ndiscuss the two main deep-learning based Machine Translation methods, one at\ncomponent or domain level which leverages deep learning models to enhance the\nefficacy of Statistical Machine Translation (SMT) and end-to-end deep learning\nmodels in MT which uses neural networks to find correspondence between the\nsource and target languages using the encoder-decoder architecture. We conclude\nthis paper by providing a time line of the major research problems solved by\nthe researchers and also provide a comprehensive overview of present areas of\nresearch in Neural Machine Translation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 07:04:44 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Srivastava", "Siddhant", ""], ["Shukla", "Anupam", ""], ["Tiwari", "Ritu", ""]]}, {"id": "1812.04342", "submitter": "Yajie Zhang", "authors": "Ya-Jie Zhang, Shifeng Pan, Lei He, Zhen-Hua Ling", "title": "Learning latent representations for style control and transfer in\n  end-to-end speech synthesis", "comments": "Paper accepted by ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the Variational Autoencoder (VAE) to an\nend-to-end speech synthesis model, to learn the latent representation of\nspeaking styles in an unsupervised manner. The style representation learned\nthrough VAE shows good properties such as disentangling, scaling, and\ncombination, which makes it easy for style control. Style transfer can be\nachieved in this framework by first inferring style representation through the\nrecognition network of VAE, then feeding it into TTS network to guide the style\nin synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in\ntraining, several techniques are adopted. Finally, the proposed model shows\ngood performance of style control and outperforms Global Style Token (GST)\nmodel in ABX preference tests on style transfer.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 12:00:06 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 08:20:10 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Zhang", "Ya-Jie", ""], ["Pan", "Shifeng", ""], ["He", "Lei", ""], ["Ling", "Zhen-Hua", ""]]}, {"id": "1812.04361", "submitter": "Shikhar Vashishth", "authors": "Shikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga, Chiranjib\n  Bhattacharyya, Partha Talukdar", "title": "RESIDE: Improving Distantly-Supervised Neural Relation Extraction using\n  Side Information", "comments": "10 pages, 6 figures, EMNLP 2018", "journal-ref": "Proceedings of the 2018 Conference on Empirical Methods in Natural\n  Language Processing", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distantly-supervised Relation Extraction (RE) methods train an extractor by\nautomatically aligning relation instances in a Knowledge Base (KB) with\nunstructured text. In addition to relation instances, KBs often contain other\nrelevant side information, such as aliases of relations (e.g., founded and\nco-founded are aliases for the relation founderOfCompany). RE models usually\nignore such readily available side information. In this paper, we propose\nRESIDE, a distantly-supervised neural relation extraction method which utilizes\nadditional side information from KBs for improved relation extraction. It uses\nentity type and relation alias information for imposing soft constraints while\npredicting relations. RESIDE employs Graph Convolution Networks (GCN) to encode\nsyntactic information from text and improves performance even when limited side\ninformation is available. Through extensive experiments on benchmark datasets,\nwe demonstrate RESIDE's effectiveness. We have made RESIDE's source code\navailable to encourage reproducible research.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 12:41:14 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 04:08:15 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Vashishth", "Shikhar", ""], ["Joshi", "Rishabh", ""], ["Prayaga", "Sai Suman", ""], ["Bhattacharyya", "Chiranjib", ""], ["Talukdar", "Partha", ""]]}, {"id": "1812.04377", "submitter": "Arindam Chowdhury", "authors": "Vishwanath D, Rohit Rahul, Gunjan Sehgal, Swati, Arindam Chowdhury,\n  Monika Sharma, Lovekesh Vig, Gautam Shroff, and Ashwin Srinivasan", "title": "Deep Reader: Information extraction from Document images via relation\n  extraction and Natural Language", "comments": "Published in 3rd International Workshop on Robust Reading at Asian\n  Conference of Computer Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in the area of Computer Vision with state-of-art Neural\nNetworks has given a boost to Optical Character Recognition (OCR) accuracies.\nHowever, extracting characters/text alone is often insufficient for relevant\ninformation extraction as documents also have a visual structure that is not\ncaptured by OCR. Extracting information from tables, charts, footnotes, boxes,\nheadings and retrieving the corresponding structured representation for the\ndocument remains a challenge and finds application in a large number of\nreal-world use cases. In this paper, we propose a novel enterprise based\nend-to-end framework called DeepReader which facilitates information extraction\nfrom document images via identification of visual entities and populating a\nmeta relational model across different entities in the document image. The\nmodel schema allows for an easy to understand abstraction of the entities\ndetected by the deep vision models and the relationships between them.\nDeepReader has a suite of state-of-the-art vision algorithms which are applied\nto recognize handwritten and printed text, eliminate noisy effects, identify\nthe type of documents and detect visual entities like tables, lines and boxes.\nDeep Reader maps the extracted entities into a rich relational schema so as to\ncapture all the relevant relationships between entities (words, textboxes,\nlines etc) detected in the document. Relevant information and fields can then\nbe extracted from the document by writing SQL queries on top of the\nrelationship tables. A natural language based interface is added on top of the\nrelationship schema so that a non-technical user, specifying the queries in\nnatural language, can fetch the information with minimal effort. In this paper,\nwe also demonstrate many different capabilities of Deep Reader and report\nresults on a real-world use case.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 13:09:13 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 16:46:59 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["D", "Vishwanath", ""], ["Rahul", "Rohit", ""], ["Sehgal", "Gunjan", ""], ["Swati", "", ""], ["Chowdhury", "Arindam", ""], ["Sharma", "Monika", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""], ["Srinivasan", "Ashwin", ""]]}, {"id": "1812.04405", "submitter": "Artidoro Pagnoni", "authors": "Artidoro Pagnoni, Kevin Liu, Shangyan Li", "title": "Conditional Variational Autoencoder for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the performance of latent variable models for conditional text\ngeneration in the context of neural machine translation (NMT). Similar to Zhang\net al., we augment the encoder-decoder NMT paradigm by introducing a continuous\nlatent variable to model features of the translation process. We extend this\nmodel with a co-attention mechanism motivated by Parikh et al. in the inference\nnetwork. Compared to the vision domain, latent variable models for text face\nadditional challenges due to the discrete nature of language, namely posterior\ncollapse. We experiment with different approaches to mitigate this issue. We\nshow that our conditional variational model improves upon both discriminative\nattention-based translation and the variational baseline presented in Zhang et\nal. Finally, we present some exploration of the learned latent space to\nillustrate what the latent variable is capable of capturing. This is the first\nreported conditional variational model for text that meaningfully utilizes the\nlatent variable without weakening the translation model.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 14:05:24 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Pagnoni", "Artidoro", ""], ["Liu", "Kevin", ""], ["Li", "Shangyan", ""]]}, {"id": "1812.04606", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Mantas Mazeika and Thomas Dietterich", "title": "Deep Anomaly Detection with Outlier Exposure", "comments": "ICLR 2019; PyTorch code available at\n  https://github.com/hendrycks/outlier-exposure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to detect anomalous inputs when deploying machine learning\nsystems. The use of larger and more complex inputs in deep learning magnifies\nthe difficulty of distinguishing between anomalous and in-distribution\nexamples. At the same time, diverse image and text data are available in\nenormous quantities. We propose leveraging these data to improve deep anomaly\ndetection by training anomaly detectors against an auxiliary dataset of\noutliers, an approach we call Outlier Exposure (OE). This enables anomaly\ndetectors to generalize and detect unseen anomalies. In extensive experiments\non natural language processing and small- and large-scale vision tasks, we find\nthat Outlier Exposure significantly improves detection performance. We also\nobserve that cutting-edge generative models trained on CIFAR-10 may assign\nhigher likelihoods to SVHN images than to CIFAR-10 images; we use OE to\nmitigate this issue. We also analyze the flexibility and robustness of Outlier\nExposure, and identify characteristics of the auxiliary dataset that improve\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 18:49:50 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 18:57:19 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 20:34:44 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Hendrycks", "Dan", ""], ["Mazeika", "Mantas", ""], ["Dietterich", "Thomas", ""]]}, {"id": "1812.04616", "submitter": "Sachin Kumar", "authors": "Sachin Kumar and Yulia Tsvetkov", "title": "Von Mises-Fisher Loss for Training Sequence to Sequence Models with\n  Continuous Outputs", "comments": "Seventh International Conference on Learning Representations (ICLR\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Softmax function is used in the final layer of nearly all existing\nsequence-to-sequence models for language generation. However, it is usually the\nslowest layer to compute which limits the vocabulary size to a subset of most\nfrequent types; and it has a large memory footprint. We propose a general\ntechnique for replacing the softmax layer with a continuous embedding layer.\nOur primary innovations are a novel probabilistic loss, and a training and\ninference procedure in which we generate a probability distribution over\npre-trained word embeddings, instead of a multinomial distribution over the\nvocabulary obtained via softmax. We evaluate this new class of\nsequence-to-sequence models with continuous outputs on the task of neural\nmachine translation. We show that our models obtain upto 2.5x speed-up in\ntraining time while performing on par with the state-of-the-art models in terms\nof translation quality. These models are capable of handling very large\nvocabularies without compromising on translation quality. They also produce\nmore meaningful errors than in the softmax-based models, as these errors\ntypically lie in a subspace of the vector space of the reference translations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 20:00:36 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 07:16:02 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 03:08:01 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Kumar", "Sachin", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "1812.04647", "submitter": "Ankur Gandhe", "authors": "Ankur Gandhe, Ariya Rastrow, Bjorn Hoffmeister", "title": "Scalable language model adaptation for spoken dialogue systems", "comments": "Accepted at SLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models (LM) for interactive speech recognition systems are trained\non large amounts of data and the model parameters are optimized on past user\ndata. New application intents and interaction types are released for these\nsystems over time, imposing challenges to adapt the LMs since the existing\ntraining data is no longer sufficient to model the future user interactions. It\nis unclear how to adapt LMs to new application intents without degrading the\nperformance on existing applications. In this paper, we propose a solution to\n(a) estimate n-gram counts directly from the hand-written grammar for training\nLMs and (b) use constrained optimization to optimize the system parameters for\nfuture use cases, while not degrading the performance on past usage. We\nevaluated our approach on new applications intents for a personal assistant\nsystem and find that the adaptation improves the word error rate by up to 15%\non new applications even when there is no adaptation data available for an\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 19:02:05 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Gandhe", "Ankur", ""], ["Rastrow", "Ariya", ""], ["Hoffmeister", "Bjorn", ""]]}, {"id": "1812.04662", "submitter": "Nicolai Pogrebnyakov", "authors": "Nicolai Pogrebnyakov", "title": "Unsupervised domain-agnostic identification of product names in social\n  media posts", "comments": "IEEE Big Data 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product name recognition is a significant practical problem, spurred by the\ngreater availability of platforms for discussing products such as social media\nand product review functionalities of online marketplaces. Customers, product\nmanufacturers and online marketplaces may want to identify product names in\nunstructured text to extract important insights, such as sentiment, surrounding\na product. Much extant research on product name identification has been\ndomain-specific (e.g., identifying mobile phone models) and used supervised or\nsemi-supervised methods. With massive numbers of new products released to the\nmarket every year such methods may require retraining on updated labeled data\nto stay relevant, and may transfer poorly across domains. This research\naddresses this challenge and develops a domain-agnostic, unsupervised algorithm\nfor identifying product names based on Facebook posts. The algorithm consists\nof two general steps: (a) candidate product name identification using an\noff-the-shelf pretrained conditional random fields (CRF) model, part-of-speech\ntagging and a set of simple patterns; and (b) filtering of candidate names to\nremove spurious entries using clustering and word embeddings generated from the\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 19:34:49 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Pogrebnyakov", "Nicolai", ""]]}, {"id": "1812.04718", "submitter": "Claude Coulombe", "authors": "Claude Coulombe", "title": "Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs", "comments": "33 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, it is common to find oneself with far too little text data to\ntrain a deep neural network. This \"Big Data Wall\" represents a challenge for\nminority language communities on the Internet, organizations, laboratories and\ncompanies that compete the GAFAM (Google, Amazon, Facebook, Apple, Microsoft).\nWhile most of the research effort in text data augmentation aims on the\nlong-term goal of finding end-to-end learning solutions, which is equivalent to\n\"using neural networks to feed neural networks\", this engineering work focuses\non the use of practical, robust, scalable and easy-to-implement data\naugmentation pre-processing techniques similar to those that are successful in\ncomputer vision. Several text augmentation techniques have been experimented.\nSome existing ones have been tested for comparison purposes such as noise\ninjection or the use of regular expressions. Others are modified or improved\ntechniques like lexical replacement. Finally more innovative ones, such as the\ngeneration of paraphrases using back-translation or by the transformation of\nsyntactic trees, are based on robust, scalable, and easy-to-use NLP Cloud APIs.\nAll the text augmentation techniques studied, with an amplification factor of\nonly 5, increased the accuracy of the results in a range of 4.3% to 21.6%, with\nsignificant statistical fluctuations, on a standardized task of text polarity\nprediction. Some standard deep neural network architectures were tested: the\nmultilayer perceptron (MLP), the long short-term memory recurrent network\n(LSTM) and the bidirectional LSTM (biLSTM). Classical XGBoost algorithm has\nbeen tested with up to 2.5% improvements.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 03:59:41 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Coulombe", "Claude", ""]]}, {"id": "1812.04722", "submitter": "Shayne O'Brien", "authors": "David McClure, Shayne O'Brien, and Deb Roy", "title": "Context is Key: New Approaches to Neural Coherence Modeling", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We formulate coherence modeling as a regression task and propose two novel\nmethods to combine techniques from our setup with pairwise approaches. The\nfirst of our methods is a model that we call \"first-next,\" which operates\nsimilarly to selection sorting but conditions decision-making on information\nabout already-sorted sentences. The second consists of a technique for adding\ncontext to regression-based models by concatenating sentence-level\nrepresentations with an encoding of its corresponding out-of-order paragraph.\nThis latter model achieves Kendall-tau distance and positional accuracy scores\nthat match or exceed the current state-of-the-art on these metrics. Our results\nsuggest that many of the gains that come from more complex, machine-translation\ninspired approaches can be achieved with simpler, more efficient models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 03:16:06 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["McClure", "David", ""], ["O'Brien", "Shayne", ""], ["Roy", "Deb", ""]]}, {"id": "1812.04784", "submitter": "Chengyue Gong", "authors": "Chengyue Gong, Xu Tan, Di He, Tao Qin", "title": "Sentence-wise Smooth Regularization for Sequence to Sequence Learning", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum-likelihood estimation (MLE) is widely used in sequence to sequence\ntasks for model training. It uniformly treats the generation/prediction of each\ntarget token as multi-class classification, and yields non-smooth prediction\nprobabilities: in a target sequence, some tokens are predicted with small\nprobabilities while other tokens are with large probabilities. According to our\nempirical study, we find that the non-smoothness of the probabilities results\nin low quality of generated sequences. In this paper, we propose a\nsentence-wise regularization method which aims to output smooth prediction\nprobabilities for all the tokens in the target sequence. Our proposed method\ncan automatically adjust the weights and gradients of each token in one\nsentence to ensure the predictions in a sequence uniformly well. Experiments on\nthree neural machine translation tasks and one text summarization task show\nthat our method outperforms conventional MLE loss on all these tasks and\nachieves promising BLEU scores on WMT14 English-German and WMT17\nChinese-English translation task.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 02:47:11 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Gong", "Chengyue", ""], ["Tan", "Xu", ""], ["He", "Di", ""], ["Qin", "Tao", ""]]}, {"id": "1812.04840", "submitter": "Amir Aly", "authors": "Amir Aly and Tadahiro Taniguchi", "title": "Towards Understanding Language through Perception in Situated\n  Human-Robot Interaction: From Word Grounding to Grammar Induction", "comments": "Proceedings of the International Conference on Social Cognition in\n  Humans and Robots (socSMCs), Germany, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots are widely collaborating with human users in diferent tasks that\nrequire high-level cognitive functions to make them able to discover the\nsurrounding environment. A difcult challenge that we briefy highlight in this\nshort paper is inferring the latent grammatical structure of language, which\nincludes grounding parts of speech (e.g., verbs, nouns, adjectives, and\nprepositions) through visual perception, and induction of Combinatory\nCategorial Grammar (CCG) for phrases. This paves the way towards grounding\nphrases so as to make a robot able to understand human instructions\nappropriately during interaction.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 08:06:30 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 14:58:27 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 08:22:51 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Aly", "Amir", ""], ["Taniguchi", "Tadahiro", ""]]}, {"id": "1812.04891", "submitter": "Arushi Goel", "authors": "Zhi-Xuan Tan, Arushi Goel, Thanh-Son Nguyen, and Desmond C. Ong", "title": "A Multimodal LSTM for Predicting Listener Empathic Responses Over Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  People naturally understand the emotions of-and often also empathize\nwith-those around them. In this paper, we predict the emotional valence of an\nempathic listener over time as they listen to a speaker narrating a life story.\nWe use the dataset provided by the OMG-Empathy Prediction Challenge, a workshop\nheld in conjunction with IEEE FG 2019. We present a multimodal LSTM model with\nfeature-level fusion and local attention that predicts empathic responses from\naudio, text, and visual features. Our best-performing model, which used only\nthe audio and text features, achieved a concordance correlation coefficient\n(CCC) of 0.29 and 0.32 on the Validation set for the Generalized and\nPersonalized track respectively, and achieved a CCC of 0.14 and 0.14 on the\nheld-out Test set. We discuss the difficulties faced and the lessons learnt\ntackling this challenge.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 10:57:52 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 01:48:43 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Tan", "Zhi-Xuan", ""], ["Goel", "Arushi", ""], ["Nguyen", "Thanh-Son", ""], ["Ong", "Desmond C.", ""]]}, {"id": "1812.04898", "submitter": "Sainik Mahata", "authors": "Sainik Kumar Mahata, Soumil Mandal, Dipankar Das, Sivaji Bandyopadhyay", "title": "SMT vs NMT: A Comparison over Hindi & Bengali Simple Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present article, we identified the qualitative differences between\nStatistical Machine Translation (SMT) and Neural Machine Translation (NMT)\noutputs. We have tried to answer two important questions: 1. Does NMT perform\nequivalently well with respect to SMT and 2. Does it add extra flavor in\nimproving the quality of MT output by employing simple sentences as training\nunits. In order to obtain insights, we have developed three core models viz.,\nSMT model based on Moses toolkit, followed by character and word level NMT\nmodels. All of the systems use English-Hindi and English-Bengali language pairs\ncontaining simple sentences as well as sentences of other complexity. In order\nto preserve the translations semantics with respect to the target words of a\nsentence, we have employed soft-attention into our word level NMT model. We\nhave further evaluated all the systems with respect to the scenarios where they\nsucceed and fail. Finally, the quality of translation has been validated using\nBLEU and TER metrics along with manual parameters like fluency, adequacy etc.\nWe observed that NMT outperforms SMT in case of simple sentences whereas SMT\noutperforms in case of all types of sentence.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 11:11:08 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Mahata", "Sainik Kumar", ""], ["Mandal", "Soumil", ""], ["Das", "Dipankar", ""], ["Bandyopadhyay", "Sivaji", ""]]}, {"id": "1812.05001", "submitter": "John McCrae", "authors": "Narumol Prangnawarat, John P. McCrae and Conor Hayes", "title": "Temporal Analysis of Entity Relatedness and its Evolution using\n  Wikipedia and DBpedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many researchers have made use of the Wikipedia network for relatedness and\nsimilarity tasks. However, most approaches use only the most recent information\nand not historical changes in the network. We provide an analysis of entity\nrelatedness using temporal graph-based approaches over different versions of\nthe Wikipedia article link network and DBpedia, which is an open-source\nknowledge base extracted from Wikipedia. We consider creating the Wikipedia\narticle link network as both a union and intersection of edges over multiple\ntime points and present a novel variation of the Jaccard index to weight edges\nbased on their transience. We evaluate our results against the KORE dataset,\nwhich was created in 2010, and show that using the 2010 Wikipedia article link\nnetwork produces the strongest result, suggesting that semantic similarity is\ntime sensitive. We then show that integrating multiple time frames in our\nmethods can give a better overall similarity demonstrating that temporal\nevolution can have an important effect on entity relatedness.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 16:11:31 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Prangnawarat", "Narumol", ""], ["McCrae", "John P.", ""], ["Hayes", "Conor", ""]]}, {"id": "1812.05035", "submitter": "Babak Esmaeili", "authors": "Babak Esmaeili, Hongyi Huang, Byron C. Wallace, Jan-Willem van de\n  Meent", "title": "Structured Neural Topic Models for Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Variational Aspect-based Latent Topic Allocation (VALTA), a family\nof autoencoding topic models that learn aspect-based representations of\nreviews. VALTA defines a user-item encoder that maps bag-of-words vectors for\ncombined reviews associated with each paired user and item onto structured\nembeddings, which in turn define per-aspect topic weights. We model individual\nreviews in a structured manner by inferring an aspect assignment for each\nsentence in a given review, where the per-aspect topic weights obtained by the\nuser-item encoder serve to define a mixture over topics, conditioned on the\naspect. The result is an autoencoding neural topic model for reviews, which can\nbe trained in a fully unsupervised manner to learn topics that are structured\ninto aspects. Experimental evaluation on large number of datasets demonstrates\nthat aspects are interpretable, yield higher coherence scores than\nnon-structured autoencoding topic model variants, and can be utilized to\nperform aspect-based comparison and genre discovery.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 17:12:58 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 02:03:12 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Esmaeili", "Babak", ""], ["Huang", "Hongyi", ""], ["Wallace", "Byron C.", ""], ["van de Meent", "Jan-Willem", ""]]}, {"id": "1812.05083", "submitter": "Miriam Cha", "authors": "Miriam Cha, Youngjune L. Gwon, H.T. Kung", "title": "Adversarial Learning of Semantic Relevance in Text to Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new approach that improves the training of generative\nadversarial nets (GANs) for synthesizing diverse images from a text input. Our\napproach is based on the conditional version of GANs and expands on previous\nwork leveraging an auxiliary task in the discriminator. Our generated images\nare not limited to certain classes and do not suffer from mode collapse while\nsemantically matching the text input. A key to our training methods is how to\nform positive and negative training examples with respect to the class label of\na given image. Instead of selecting random training examples, we perform\nnegative sampling based on the semantic distance from a positive example in the\nclass. We evaluate our approach using the Oxford-102 flower dataset, adopting\nthe inception score and multi-scale structural similarity index (MS-SSIM)\nmetrics to assess discriminability and diversity of the generated images. The\nempirical results indicate greater diversity in the generated images,\nespecially when we gradually select more negative training examples closer to a\npositive example in the semantic space.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 18:44:23 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 21:33:04 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Cha", "Miriam", ""], ["Gwon", "Youngjune L.", ""], ["Kung", "H. T.", ""]]}, {"id": "1812.05199", "submitter": "Liang Qiu", "authors": "Liang Qiu, Yuanyi Ding, Lei He", "title": "Recurrent Neural Networks with Pre-trained Language Model Embedding for\n  Slot Filling Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Recurrent Neural Networks (RNNs) based models have been\napplied to the Slot Filling problem of Spoken Language Understanding and\nachieved the state-of-the-art performances. In this paper, we investigate the\neffect of incorporating pre-trained language models into RNN based Slot Filling\nmodels. Our evaluation on the Airline Travel Information System (ATIS) data\ncorpus shows that we can significantly reduce the size of labeled training data\nand achieve the same level of Slot Filling performance by incorporating extra\nword embedding and language model embedding layers pre-trained on unlabeled\ncorpora.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 23:49:58 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 17:51:26 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Qiu", "Liang", ""], ["Ding", "Yuanyi", ""], ["He", "Lei", ""]]}, {"id": "1812.05253", "submitter": "Lei He", "authors": "Yan Deng, Lei He and Frank Soong", "title": "Modeling Multi-speaker Latent Space to Improve Neural TTS: Quick\n  Enrolling New Speaker and Enhancing Premium Voice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural TTS has shown it can generate high quality synthesized speech. In this\npaper, we investigate the multi-speaker latent space to improve neural TTS for\nadapting the system to new speakers with only several minutes of speech or\nenhancing a premium voice by utilizing the data from other speakers for richer\ncontextual coverage and better generalization. A multi-speaker neural TTS model\nis built with the embedded speaker information in both spectral and speaker\nlatent space. The experimental results show that, with less than 5 minutes of\ntraining data from a new speaker, the new model can achieve an MOS score of\n4.16 in naturalness and 4.64 in speaker similarity close to human recordings\n(4.74). For a well-trained premium voice, we can achieve an MOS score of 4.5\nfor out-of-domain texts, which is comparable to an MOS of 4.58 for professional\nrecordings, and significantly outperforms single speaker result of 4.28.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 03:41:58 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 04:44:35 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 02:52:58 GMT"}, {"version": "v4", "created": "Mon, 2 Sep 2019 02:30:19 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Deng", "Yan", ""], ["He", "Lei", ""], ["Soong", "Frank", ""]]}, {"id": "1812.05270", "submitter": "Mohammed Khalilia", "authors": "Parminder Bhatia, Busra Celikkaya, Mohammed Khalilia", "title": "Joint Entity Extraction and Assertion Detection for Clinical Text", "comments": "Accepted at the 57th Annual Meeting of the Association for\n  Computational Linguistics (ACL 2019)", "journal-ref": "Proceedings of the 57th Annual Meeting of the Association for\n  Computational Linguistics. Florence, Italy, 2019, pp. 954-959", "doi": "10.18653/v1/P19-1091", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative medical findings are prevalent in clinical reports, yet\ndiscriminating them from positive findings remains a challenging task for\ninformation extraction. Most of the existing systems treat this task as a\npipeline of two separate tasks, i.e., named entity recognition (NER) and\nrule-based negation detection. We consider this as a multi-task problem and\npresent a novel end-to-end neural model to jointly extract entities and\nnegations. We extend a standard hierarchical encoder-decoder NER model and\nfirst adopt a shared encoder followed by separate decoders for the two tasks.\nThis architecture performs considerably better than the previous rule-based and\nmachine learning-based systems. To overcome the problem of increased parameter\nsize especially for low-resource settings, we propose the Conditional Softmax\nShared Decoder architecture which achieves state-of-art results for NER and\nnegation detection on the 2010 i2b2/VA challenge dataset and a proprietary\nde-identified clinical dataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 05:32:23 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 04:58:37 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 06:12:58 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 16:34:51 GMT"}, {"version": "v5", "created": "Wed, 22 Jan 2020 16:09:07 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Bhatia", "Parminder", ""], ["Celikkaya", "Busra", ""], ["Khalilia", "Mohammed", ""]]}, {"id": "1812.05271", "submitter": "Tianyu Du", "authors": "Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, Ting Wang", "title": "TextBugger: Generating Adversarial Text Against Real-world Applications", "comments": "To appear in NDSS 2019", "journal-ref": null, "doi": "10.14722/ndss.2019.23138", "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning-based Text Understanding (DLTU) is the backbone technique\nbehind various applications, including question answering, machine translation,\nand text classification. Despite its tremendous popularity, the security\nvulnerabilities of DLTU are still largely unknown, which is highly concerning\ngiven its increasing use in security-sensitive applications such as sentiment\nanalysis and toxic content detection. In this paper, we show that DLTU is\ninherently vulnerable to adversarial text attacks, in which maliciously crafted\ntexts trigger target DLTU systems and services to misbehave. Specifically, we\npresent TextBugger, a general attack framework for generating adversarial\ntexts. In contrast to prior works, TextBugger differs in significant ways: (i)\neffective -- it outperforms state-of-the-art attacks in terms of attack success\nrate; (ii) evasive -- it preserves the utility of benign text, with 94.9\\% of\nthe adversarial text correctly recognized by human readers; and (iii) efficient\n-- it generates adversarial text with computational complexity sub-linear to\nthe text length. We empirically evaluate TextBugger on a set of real-world DLTU\nsystems and services used for sentiment analysis and toxic content detection,\ndemonstrating its effectiveness, evasiveness, and efficiency. For instance,\nTextBugger achieves 100\\% success rate on the IMDB dataset based on Amazon AWS\nComprehend within 4.61 seconds and preserves 97\\% semantic similarity. We\nfurther discuss possible defense mechanisms to mitigate such attack and the\nadversary's potential countermeasures, which leads to promising directions for\nfurther research.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 05:32:43 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Li", "Jinfeng", ""], ["Ji", "Shouling", ""], ["Du", "Tianyu", ""], ["Li", "Bo", ""], ["Wang", "Ting", ""]]}, {"id": "1812.05272", "submitter": "Yu-Hsiang Lin", "authors": "Graham Neubig, Patrick Littell, Chian-Yu Chen, Jean Lee, Zirui Li,\n  Yu-Hsiang Lin, Yuyan Zhang", "title": "Towards a General-Purpose Linguistic Annotation Backend", "comments": "4 pages, 8 figures, accepted by ComputEL-3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language documentation is inherently a time-intensive process; transcription,\nglossing, and corpus management consume a significant portion of documentary\nlinguists' work. Advances in natural language processing can help to accelerate\nthis work, using the linguists' past decisions as training material, but\nquestions remain about how to prioritize human involvement. In this extended\nabstract, we describe the beginnings of a new project that will attempt to ease\nthis language documentation process through the use of natural language\nprocessing (NLP) technology. It is based on (1) methods to adapt NLP tools to\nnew languages, based on recent advances in massively multilingual neural\nnetworks, and (2) backend APIs and interfaces that allow linguists to upload\ntheir data. We then describe our current progress on two fronts: automatic\nphoneme transcription, and glossing. Finally, we briefly describe our future\ndirections.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 05:33:11 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Neubig", "Graham", ""], ["Littell", "Patrick", ""], ["Chen", "Chian-Yu", ""], ["Lee", "Jean", ""], ["Li", "Zirui", ""], ["Lin", "Yu-Hsiang", ""], ["Zhang", "Yuyan", ""]]}, {"id": "1812.05288", "submitter": "Parminder Bhatia", "authors": "Parminder Bhatia, Kristjan Arumae, Busra Celikkaya", "title": "Dynamic Transfer Learning for Named Entity Recognition", "comments": "AAAI 2019 Workshop on Health Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art named entity recognition (NER) systems have been improving\ncontinuously using neural architectures over the past several years. However,\nmany tasks including NER require large sets of annotated data to achieve such\nperformance. In particular, we focus on NER from clinical notes, which is one\nof the most fundamental and critical problems for medical text analysis. Our\nwork centers on effectively adapting these neural architectures towards\nlow-resource settings using parameter transfer methods. We complement a\nstandard hierarchical NER model with a general transfer learning framework\nconsisting of parameter sharing between the source and target tasks, and\nshowcase scores significantly above the baseline architecture. These sharing\nschemes require an exponential search over tied parameter sets to generate an\noptimal configuration. To mitigate the problem of exhaustively searching for\nmodel optimization, we propose the Dynamic Transfer Networks (DTN), a gated\narchitecture which learns the appropriate parameter sharing scheme between\nsource and target datasets. DTN achieves the improvements of the optimized\ntransfer learning framework with just a single training setting, effectively\nremoving the need for exponential search.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 07:02:54 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 05:24:32 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 23:50:45 GMT"}, {"version": "v4", "created": "Mon, 20 Jan 2020 22:30:14 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Bhatia", "Parminder", ""], ["Arumae", "Kristjan", ""], ["Celikkaya", "Busra", ""]]}, {"id": "1812.05366", "submitter": "Longxuan Ma", "authors": "Longxuan Ma, Pengfei Wang and Lei Zhang", "title": "Dynamic Feature Generation Network for Answer Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting appropriate features to represent a corpus is an important task\nfor textual mining. Previous attention based work usually enhance feature at\nthe lexical level, which lacks the exploration of feature augmentation at the\nsentence level. In this paper, we exploit a Dynamic Feature Generation Network\n(DFGN) to solve this problem. Specifically, DFGN generates features based on a\nvariety of attention mechanisms and attaches features to sentence\nrepresentation. Then a thresholder is designed to filter the mined features\nautomatically. DFGN extracts the most significant characteristics from datasets\nto keep its practicability and robustness. Experimental results on multiple\nwell-known answer selection datasets show that our proposed approach\nsignificantly outperforms state-of-the-art baselines. We give a detailed\nanalysis of the experiments to illustrate why DFGN provides excellent retrieval\nand interpretative ability.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 11:23:18 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Ma", "Longxuan", ""], ["Wang", "Pengfei", ""], ["Zhang", "Lei", ""]]}, {"id": "1812.05407", "submitter": "Shen Gao", "authors": "Shen Gao, Xiuying Chen, Piji Li, Zhaochun Ren, Lidong Bing, Dongyan\n  Zhao, Rui Yan", "title": "Abstractive Text Summarization by Incorporating Reader Comments", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural abstractive summarization field, conventional sequence-to-sequence\nbased models often suffer from summarizing the wrong aspect of the document\nwith respect to the main aspect. To tackle this problem, we propose the task of\nreader-aware abstractive summary generation, which utilizes the reader comments\nto help the model produce better summary about the main aspect. Unlike\ntraditional abstractive summarization task, reader-aware summarization\nconfronts two main challenges: (1) Comments are informal and noisy; (2) jointly\nmodeling the news document and the reader comments is challenging. To tackle\nthe above challenges, we design an adversarial learning model named\nreader-aware summary generator (RASG), which consists of four components: (1) a\nsequence-to-sequence based summary generator; (2) a reader attention module\ncapturing the reader focused aspects; (3) a supervisor modeling the semantic\ngap between the generated summary and reader focused aspects; (4) a goal\ntracker producing the goal for each generation step. The supervisor and the\ngoal tacker are used to guide the training of our framework in an adversarial\nmanner. Extensive experiments are conducted on our large-scale real-world text\nsummarization dataset, and the results show that RASG achieves the\nstate-of-the-art performance in terms of both automatic metrics and human\nevaluations. The experimental results also demonstrate the effectiveness of\neach module in our framework. We release our large-scale dataset for further\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 13:13:23 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Gao", "Shen", ""], ["Chen", "Xiuying", ""], ["Li", "Piji", ""], ["Ren", "Zhaochun", ""], ["Bing", "Lidong", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "1812.05411", "submitter": "Zhenxin Fu", "authors": "Mingyue Shang, Zhenxin Fu, Hongzhi Yin, Bo Tang, Dongyan Zhao, Rui Yan", "title": "Find a Reasonable Ending for Stories: Does Logic Relation Help the Story\n  Cloze Test?", "comments": "Student Abstract in AAAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language understanding is a challenging problem that covers a wide\nrange of tasks. While previous methods generally train each task separately, we\nconsider combining the cross-task features to enhance the task performance. In\nthis paper, we incorporate the logic information with the help of the Natural\nLanguage Inference (NLI) task to the Story Cloze Test (SCT). Previous work on\nSCT considered various semantic information, such as sentiment and topic, but\nlack the logic information between sentences which is an essential element of\nstories. Thus we propose to extract the logic information during the course of\nthe story to improve the understanding of the whole story. The logic\ninformation is modeled with the help of the NLI task. Experimental results\nprove the strength of the logic information.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 13:20:40 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Shang", "Mingyue", ""], ["Fu", "Zhenxin", ""], ["Yin", "Hongzhi", ""], ["Tang", "Bo", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "1812.05634", "submitter": "Anna Rohrbach", "authors": "Jae Sung Park, Marcus Rohrbach, Trevor Darrell, Anna Rohrbach", "title": "Adversarial Inference for Multi-Sentence Video Description", "comments": "Accepted to Computer Vision and Pattern Recognition (CVPR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While significant progress has been made in the image captioning task, video\ndescription is still in its infancy due to the complex nature of video data.\nGenerating multi-sentence descriptions for long videos is even more\nchallenging. Among the main issues are the fluency and coherence of the\ngenerated descriptions, and their relevance to the video. Recently,\nreinforcement and adversarial learning based methods have been explored to\nimprove the image captioning models; however, both types of methods suffer from\na number of issues, e.g. poor readability and high redundancy for RL and\nstability issues for GANs. In this work, we instead propose to apply\nadversarial techniques during inference, designing a discriminator which\nencourages better multi-sentence video description. In addition, we find that a\nmulti-discriminator \"hybrid\" design, where each discriminator targets one\naspect of a description, leads to the best results. Specifically, we decouple\nthe discriminator to evaluate on three criteria: 1) visual relevance to the\nvideo, 2) language diversity and fluency, and 3) coherence across sentences.\nOur approach results in more accurate, diverse, and coherent multi-sentence\nvideo descriptions, as shown by automatic as well as human evaluation on the\npopular ActivityNet Captions dataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 19:07:17 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 02:04:44 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Park", "Jae Sung", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Anna", ""]]}, {"id": "1812.05692", "submitter": "Ekaterina Lobacheva Ms", "authors": "Ekaterina Lobacheva, Nadezhda Chirkova, Dmitry Vetrov", "title": "Bayesian Sparsification of Gated Recurrent Neural Networks", "comments": "Published in Workshop on Compact Deep Neural Networks with industrial\n  applications, NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods have been successfully applied to sparsify weights of neural\nnetworks and to remove structure units from the networks, e. g. neurons. We\napply and further develop this approach for gated recurrent architectures.\nSpecifically, in addition to sparsification of individual weights and neurons,\nwe propose to sparsify preactivations of gates and information flow in LSTM. It\nmakes some gates and information flow components constant, speeds up forward\npass and improves compression. Moreover, the resulting structure of gate\nsparsity is interpretable and depends on the task. Code is available on github:\nhttps://github.com/tipt0p/SparseBayesianRNN\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 14:32:16 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Lobacheva", "Ekaterina", ""], ["Chirkova", "Nadezhda", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1812.05710", "submitter": "Zhiba Su", "authors": "Dabiao Ma, Zhiba Su, Wenxuan Wang, Yuhao Lu", "title": "FPETS : Fully Parallel End-to-End Text-to-Speech System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end Text-to-speech (TTS) system can greatly improve the quality of\nsynthesised speech. But it usually suffers form high time latency due to its\nauto-regressive structure. And the synthesised speech may also suffer from some\nerror modes, e.g. repeated words, mispronunciations, and skipped words. In this\npaper, we propose a novel non-autoregressive, fully parallel end-to-end TTS\nsystem (FPETS). It utilizes a new alignment model and the recently proposed\nU-shape convolutional structure, UFANS. Different from RNN, UFANS can capture\nlong term information in a fully parallel manner. Trainable position encoding\nand two-step training strategy are used for learning better alignments.\nExperimental results show FPETS utilizes the power of parallel computation and\nreaches a significant speed up of inference compared with state-of-the-art\nend-to-end TTS systems. More specifically, FPETS is 600X faster than Tacotron2,\n50X faster than DCTTS and 10X faster than Deep Voice3. And FPETS can generates\naudios with equal or better quality and fewer errors comparing with other\nsystem. As far as we know, FPETS is the first end-to-end TTS system which is\nfully parallel.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 05:17:23 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 03:28:18 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 18:39:40 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2019 00:31:29 GMT"}, {"version": "v5", "created": "Sun, 9 Feb 2020 18:19:09 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ma", "Dabiao", ""], ["Su", "Zhiba", ""], ["Wang", "Wenxuan", ""], ["Lu", "Yuhao", ""]]}, {"id": "1812.05718", "submitter": "Youssef Mourchid", "authors": "Youssef Mourchid, Benjamin Renoust, Hocine Cherifi, and Mohammed El\n  Hassouni", "title": "Multilayer Network Model of Movie Script", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models have been increasingly used in the past years to support\nsummarization and analysis of narratives, such as famous TV series, books and\nnews. Inspired by social network analysis, most of these models focus on the\ncharacters at play. The network model well captures all characters\ninteractions, giving a broad picture of the narration's content. A few works\nwent beyond by introducing additional semantic elements, always captured in a\nsingle layer network. In contrast, we introduce in this work a multilayer\nnetwork model to capture more elements of the narration of a movie from its\nscript: people, locations, and other semantic elements. This model enables new\nmeasures and insights on movies. We demonstrate this model on two very popular\nmovies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 22:50:04 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Mourchid", "Youssef", ""], ["Renoust", "Benjamin", ""], ["Cherifi", "Hocine", ""], ["Hassouni", "Mohammed El", ""]]}, {"id": "1812.05774", "submitter": "Liling Tan", "authors": "Maggie Yundi Li and Stanley Kok and Liling Tan", "title": "Don't Classify, Translate: Multi-Level E-Commerce Product Categorization\n  Via Machine Translation", "comments": null, "journal-ref": "Workshop on Information Technologies and Systems 2018 (WITS2018)", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce platforms categorize their products into a multi-level taxonomy\ntree with thousands of leaf categories. Conventional methods for product\ncategorization are typically based on machine learning classification\nalgorithms. These algorithms take product information as input (e.g., titles\nand descriptions) to classify a product into a leaf category. In this paper, we\npropose a new paradigm based on machine translation. In our approach, we\ntranslate a product's natural language description into a sequence of tokens\nrepresenting a root-to-leaf path in a product taxonomy. In our experiments on\ntwo large real-world datasets, we show that our approach achieves better\npredictive accuracy than a state-of-the-art classification system for product\ncategorization. In addition, we demonstrate that our machine translation models\ncan propose meaningful new paths between previously unconnected nodes in a\ntaxonomy tree, thereby transforming the taxonomy into a directed acyclic graph\n(DAG). We discuss how the resultant taxonomy DAG promotes user-friendly\nnavigation, and how it is more adaptable to new products.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 04:12:02 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Li", "Maggie Yundi", ""], ["Kok", "Stanley", ""], ["Tan", "Liling", ""]]}, {"id": "1812.05813", "submitter": "Jean-Philippe Bernardy", "authors": "Jean-Philippe Bernardy, Stergios Chatzikyriakidis", "title": "A corpus of precise natural textual entailment problems", "comments": "34 pages including appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a new corpus of entailment problems. This corpus\ncombines the following characteristics: 1. it is precise (does not leave out\nimplicit hypotheses) 2. it is based on \"real-world\" texts (i.e. most of the\npremises were written for purposes other than testing textual entailment). 3.\nits size is 150. The corpus was constructed by taking problems from the Real\nText Entailment and discovering missing hypotheses using a crowd of experts. We\nbelieve that this corpus constitutes a first step towards wide-coverage testing\nof precise natural-language inference systems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 08:09:29 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Bernardy", "Jean-Philippe", ""], ["Chatzikyriakidis", "Stergios", ""]]}, {"id": "1812.05920", "submitter": "Mirco Ravanelli", "authors": "Mirco Ravanelli, Yoshua Bengio", "title": "Speech and Speaker Recognition from Raw Waveform with SincNet", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.09725,\n  arXiv:1808.00158", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks can learn complex and abstract representations, that are\nprogressively obtained by combining simpler ones. A recent trend in speech and\nspeaker recognition consists in discovering these representations starting from\nraw audio samples directly. Differently from standard hand-crafted features\nsuch as MFCCs or FBANK, the raw waveform can potentially help neural networks\ndiscover better and more customized representations. The high-dimensional raw\ninputs, however, can make training significantly more challenging. This paper\nsummarizes our recent efforts to develop a neural architecture that efficiently\nprocesses speech from audio waveforms. In particular, we propose SincNet, a\nnovel Convolutional Neural Network (CNN) that encourages the first layer to\ndiscover meaningful filters by exploiting parametrized sinc functions. In\ncontrast to standard CNNs, which learn all the elements of each filter, only\nlow and high cutoff frequencies of band-pass filters are directly learned from\ndata. This inductive bias offers a very compact way to derive a customized\nfront-end, that only depends on some parameters with a clear physical meaning.\nOur experiments, conducted on both speaker and speech recognition, show that\nthe proposed architecture converges faster, performs better, and is more\ncomputationally efficient than standard CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 16:01:11 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 19:48:46 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Ravanelli", "Mirco", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1812.05936", "submitter": "Abhik Jana", "authors": "Abhik Jana, Animesh Mukherjee, Pawan Goyal", "title": "Detecting Reliable Novel Word Senses: A Network-Centric Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of Big Data, due to expeditious exchange of information on the\nweb, words are being used to denote newer meanings, causing linguistic shift.\nWith the recent availability of large amounts of digitized texts, an automated\nanalysis of the evolution of language has become possible. Our study mainly\nfocuses on improving the detection of new word senses. This paper presents a\nunique proposal based on network features to improve the precision of new word\nsense detection. For a candidate word where a new sense (birth) has been\ndetected by comparing the sense clusters induced at two different time points,\nwe further compare the network properties of the subgraphs induced from novel\nsense cluster across these two time points. Using the mean fractional change in\nedge density, structural similarity and average path length as features in an\nSVM classifier, manual evaluation gives precision values of 0.86 and 0.74 for\nthe task of new sense detection, when tested on 2 distinct time-point pairs, in\ncomparison to the precision values in the range of 0.23-0.32, when the proposed\nscheme is not used. The outlined method can therefore be used as a new post-hoc\nstep to improve the precision of novel word sense detection in a robust and\nreliable way where the underlying framework uses a graph structure. Another\nimportant observation is that even though our proposal is a post-hoc step, it\ncan be used in isolation and that itself results in a very decent performance\nachieving a precision of 0.54-0.62. Finally, we show that our method is able to\ndetect the well-known historical shifts in 80% cases.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 13:57:43 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Jana", "Abhik", ""], ["Mukherjee", "Animesh", ""], ["Goyal", "Pawan", ""]]}, {"id": "1812.05984", "submitter": "Andras Zsom", "authors": "Ashley Lee, Jo Guldi, Andras Zsom", "title": "Measuring Similarity: Computationally Reproducing the Scholar's\n  Interests", "comments": "to be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computerized document classification already orders the news articles that\nApple's \"News\" app or Google's \"personalized search\" feature groups together to\nmatch a reader's interests. The invisible and therefore illegible decisions\nthat go into these tailored searches have been the subject of a critique by\nscholars who emphasize that our intelligence about documents is only as good as\nour ability to understand the criteria of search. This article will attempt to\nunpack the procedures used in computational classification of texts,\ntranslating them into term legible to humanists, and examining opportunities to\nrender the computational text classification process subject to expert critique\nand improvement.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 15:39:34 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Lee", "Ashley", ""], ["Guldi", "Jo", ""], ["Zsom", "Andras", ""]]}, {"id": "1812.06038", "submitter": "James Bagrow", "authors": "Daniel Berenberg and James P. Bagrow", "title": "Inferring the size of the causal universe: features and fusion of causal\n  attribution networks", "comments": "15 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cause-and-effect reasoning, the attribution of effects to causes, is one of\nthe most powerful and unique skills humans possess. Multiple surveys are\nmapping out causal attributions as networks, but it is unclear how well these\nefforts can be combined. Further, the total size of the collective causal\nattribution network held by humans is currently unknown, making it challenging\nto assess the progress of these surveys. Here we study three causal attribution\nnetworks to determine how well they can be combined into a single network.\nCombining these networks requires dealing with ambiguous nodes, as nodes\nrepresent written descriptions of causes and effects and different descriptions\nmay exist for the same concept. We introduce NetFUSES, a method for combining\nnetworks with ambiguous nodes. Crucially, treating the different causal\nattributions networks as independent samples allows us to use their overlap to\nestimate the total size of the collective causal attribution network. We find\nthat existing surveys capture 5.77% $\\pm$ 0.781% of the $\\approx$293 000 causes\nand effects estimated to exist, and 0.198% $\\pm$ 0.174% of the $\\approx$10 200\n000 attributed cause-effect relationships.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 17:28:20 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Berenberg", "Daniel", ""], ["Bagrow", "James P.", ""]]}, {"id": "1812.06081", "submitter": "Sendong Zhao", "authors": "Sendong Zhao, Ting Liu, Sicheng Zhao, Fei Wang", "title": "A Neural Multi-Task Learning Framework to Jointly Model Medical Named\n  Entity Recognition and Normalization", "comments": "AAAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art studies have demonstrated the superiority of joint modelling\nover pipeline implementation for medical named entity recognition and\nnormalization due to the mutual benefits between the two processes. To exploit\nthese benefits in a more sophisticated way, we propose a novel deep neural\nmulti-task learning framework with explicit feedback strategies to jointly\nmodel recognition and normalization. On one hand, our method benefits from the\ngeneral representations of both tasks provided by multi-task learning. On the\nother hand, our method successfully converts hierarchical tasks into a parallel\nmulti-task setting while maintaining the mutual supports between tasks. Both of\nthese aspects improve the model performance. Experimental results demonstrate\nthat our method performs significantly better than state-of-the-art approaches\non two publicly available medical literature datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 18:59:41 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Zhao", "Sendong", ""], ["Liu", "Ting", ""], ["Zhao", "Sicheng", ""], ["Wang", "Fei", ""]]}, {"id": "1812.06083", "submitter": "Jihwan Lee", "authors": "JIhwan Lee, Dongchan Kim, Ruhi Sarikaya, Young-Bum Kim", "title": "Coupled Representation Learning for Domains, Intents and Slots in Spoken\n  Language Understanding", "comments": "IEEE SLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is an essential problem in a wide range of\napplications and it is important for performing downstream tasks successfully.\nIn this paper, we propose a new model that learns coupled representations of\ndomains, intents, and slots by taking advantage of their hierarchical\ndependency in a Spoken Language Understanding system. Our proposed model learns\nthe vector representation of intents based on the slots tied to these intents\nby aggregating the representations of the slots. Similarly, the vector\nrepresentation of a domain is learned by aggregating the representations of the\nintents tied to a specific domain. To the best of our knowledge, it is the\nfirst approach to jointly learning the representations of domains, intents, and\nslots using their hierarchical relationships. The experimental results\ndemonstrate the effectiveness of the representations learned by our model, as\nevidenced by improved performance on the contextual cross-domain reranking\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 22:23:51 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Lee", "JIhwan", ""], ["Kim", "Dongchan", ""], ["Sarikaya", "Ruhi", ""], ["Kim", "Young-Bum", ""]]}, {"id": "1812.06158", "submitter": "Maksim Kretov", "authors": "Alexander Fritzler and Varvara Logacheva and Maksim Kretov", "title": "Few-shot classification in Named Entity Recognition Task", "comments": "In proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing", "journal-ref": null, "doi": "10.1145/3297280.3297378", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many natural language processing (NLP) tasks the amount of annotated data\nis limited. This urges a need to apply semi-supervised learning techniques,\nsuch as transfer learning or meta-learning. In this work we tackle Named Entity\nRecognition (NER) task using Prototypical Network - a metric learning\ntechnique. It learns intermediate representations of words which cluster well\ninto named entity classes. This property of the model allows classifying words\nwith extremely limited number of training examples, and can potentially be used\nas a zero-shot learning method. By coupling this technique with transfer\nlearning we achieve well-performing classifiers trained on only 20 instances of\na target class.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 20:39:47 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Fritzler", "Alexander", ""], ["Logacheva", "Varvara", ""], ["Kretov", "Maksim", ""]]}, {"id": "1812.06176", "submitter": "Neil Mallinar", "authors": "Neil Mallinar, Abhishek Shah, Rajendra Ugrani, Ayush Gupta, Manikandan\n  Gurusankar, Tin Kam Ho, Q. Vera Liao, Yunfeng Zhang, Rachel K.E. Bellamy,\n  Robert Yates, Chris Desmarais, Blake McGregor", "title": "Bootstrapping Conversational Agents With Weak Supervision", "comments": "6 pages, 3 figures, 1 table, Accepted for publication in IAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many conversational agents in the market today follow a standard bot\ndevelopment framework which requires training intent classifiers to recognize\nuser input. The need to create a proper set of training examples is often the\nbottleneck in the development process. In many occasions agent developers have\naccess to historical chat logs that can provide a good quantity as well as\ncoverage of training examples. However, the cost of labeling them with tens to\nhundreds of intents often prohibits taking full advantage of these chat logs.\nIn this paper, we present a framework called \\textit{search, label, and\npropagate} (SLP) for bootstrapping intents from existing chat logs using weak\nsupervision. The framework reduces hours to days of labeling effort down to\nminutes of work by using a search engine to find examples, then relies on a\ndata programming approach to automatically expand the labels. We report on a\nuser study that shows positive user feedback for this new approach to build\nconversational agents, and demonstrates the effectiveness of using data\nprogramming for auto-labeling. While the system is developed for training\nconversational agents, the framework has broader application in significantly\nreducing labeling effort for training text classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 21:32:40 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Mallinar", "Neil", ""], ["Shah", "Abhishek", ""], ["Ugrani", "Rajendra", ""], ["Gupta", "Ayush", ""], ["Gurusankar", "Manikandan", ""], ["Ho", "Tin Kam", ""], ["Liao", "Q. Vera", ""], ["Zhang", "Yunfeng", ""], ["Bellamy", "Rachel K. E.", ""], ["Yates", "Robert", ""], ["Desmarais", "Chris", ""], ["McGregor", "Blake", ""]]}, {"id": "1812.06199", "submitter": "Enrique Noriega-Atala", "authors": "Enrique Noriega-Atala, Paul D. Hein, Shraddha S. Thumsi, Zechy Wong,\n  Xia Wang and Clayton T. Morrison", "title": "Inter-sentence Relation Extraction for Associating Biological Context\n  with Events in Biomedical Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an analysis of the problem of identifying biological context and\nassociating it with biochemical events in biomedical texts. This constitutes a\nnon-trivial, inter-sentential relation extraction task. We focus on biological\ncontext as descriptions of the species, tissue type and cell type that are\nassociated with biochemical events. We describe the properties of an annotated\ncorpus of context-event relations and present and evaluate several classifiers\nfor context-event association trained on syntactic, distance and frequency\nfeatures.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 23:03:41 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Noriega-Atala", "Enrique", ""], ["Hein", "Paul D.", ""], ["Thumsi", "Shraddha S.", ""], ["Wong", "Zechy", ""], ["Wang", "Xia", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "1812.06280", "submitter": "Ikuya Yamada", "authors": "Ikuya Yamada, Akari Asai, Jin Sakuma, Hiroyuki Shindo, Hideaki Takeda,\n  Yoshiyasu Takefuji, and Yuji Matsumoto", "title": "Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the\n  Embeddings of Words and Entities from Wikipedia", "comments": "EMNLP 2020 (system demonstration)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The embeddings of entities in a large knowledge base (e.g., Wikipedia) are\nhighly beneficial for solving various natural language tasks that involve real\nworld knowledge. In this paper, we present Wikipedia2Vec, a Python-based\nopen-source tool for learning the embeddings of words and entities from\nWikipedia. The proposed tool enables users to learn the embeddings efficiently\nby issuing a single command with a Wikipedia dump file as an argument. We also\nintroduce a web-based demonstration of our tool that allows users to visualize\nand explore the learned embeddings. In our experiments, our tool achieved a\nstate-of-the-art result on the KORE entity relatedness dataset, and competitive\nresults on various standard benchmark datasets. Furthermore, our tool has been\nused as a key component in various recent studies. We publicize the source\ncode, demonstration, and the pretrained embeddings for 12 languages at\nhttps://wikipedia2vec.github.io.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 12:51:39 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 14:25:27 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 10:58:05 GMT"}, {"version": "v4", "created": "Sat, 26 Sep 2020 14:28:42 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yamada", "Ikuya", ""], ["Asai", "Akari", ""], ["Sakuma", "Jin", ""], ["Shindo", "Hiroyuki", ""], ["Takeda", "Hideaki", ""], ["Takefuji", "Yoshiyasu", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1812.06401", "submitter": "Ehsan Abbasnejad M", "authors": "Ehsan Abbasnejad, Qi Wu, Javen Shi, Anton van den Hengel", "title": "What's to know? Uncertainty as a Guide to Asking Goal-oriented Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core challenges in Visual Dialogue problems is asking the question\nthat will provide the most useful information towards achieving the required\nobjective. Encouraging an agent to ask the right questions is difficult because\nwe don't know a-priori what information the agent will need to achieve its\ntask, and we don't have an explicit model of what it knows already. We propose\na solution to this problem based on a Bayesian model of the uncertainty in the\nimplicit model maintained by the visual dialogue agent, and in the function\nused to select an appropriate output. By selecting the question that minimises\nthe predicted regret with respect to this implicit model the agent actively\nreduces ambiguity. The Bayesian model of uncertainty also enables a principled\nmethod for identifying when enough information has been acquired, and an action\nshould be selected. We evaluate our approach on two goal-oriented dialogue\ndatasets, one for visual-based collaboration task and the other for a\nnegotiation-based task. Our uncertainty-aware information-seeking model\noutperforms its counterparts in these two challenging problems.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 06:12:45 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Abbasnejad", "Ehsan", ""], ["Wu", "Qi", ""], ["Shi", "Javen", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1812.06410", "submitter": "Quanming Yao", "authors": "Yongqi Zhang and Quanming Yao and Yingxia Shao and Lei Chen", "title": "NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph (KG) embedding is a fundamental problem in data mining\nresearch with many real-world applications. It aims to encode the entities and\nrelations in the graph into low dimensional vector space, which can be used for\nsubsequent algorithms. Negative sampling, which samples negative triplets from\nnon-observed ones in the training data, is an important step in KG embedding.\nRecently, generative adversarial network (GAN), has been introduced in negative\nsampling. By sampling negative triplets with large scores, these methods avoid\nthe problem of vanishing gradient and thus obtain better performance. However,\nusing GAN makes the original model more complex and hard to train, where\nreinforcement learning must be used. In this paper, motivated by the\nobservation that negative triplets with large scores are important but rare, we\npropose to directly keep track of them with the cache. However, how to sample\nfrom and update the cache are two important questions. We carefully design the\nsolutions, which are not only efficient but also achieve a good balance between\nexploration and exploitation. In this way, our method acts as a \"distilled\"\nversion of previous GA-based methods, which does not waste training time on\nadditional parameters to fit the full distribution of negative triplets. The\nextensive experiments show that our method can gain significant improvement in\nvarious KG embedding models, and outperform the state-of-the-art negative\nsampling methods based on GAN.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 07:32:02 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 11:34:27 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Zhang", "Yongqi", ""], ["Yao", "Quanming", ""], ["Shao", "Yingxia", ""], ["Chen", "Lei", ""]]}, {"id": "1812.06417", "submitter": "Daniela Massiceti", "authors": "Daniela Massiceti, Puneet K. Dokania, N. Siddharth, Philip H.S. Torr", "title": "Visual Dialogue without Vision or Dialogue", "comments": "2018 NeurIPS Workshop on Critiquing and Correcting Trends in Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterise some of the quirks and shortcomings in the exploration of\nVisual Dialogue - a sequential question-answering task where the questions and\ncorresponding answers are related through given visual stimuli. To do so, we\ndevelop an embarrassingly simple method based on Canonical Correlation Analysis\n(CCA) that, on the standard dataset, achieves near state-of-the-art performance\non mean rank (MR). In direct contrast to current complex and over-parametrised\narchitectures that are both compute and time intensive, our method ignores the\nvisual stimuli, ignores the sequencing of dialogue, does not need gradients,\nuses off-the-shelf feature extractors, has at least an order of magnitude fewer\nparameters, and learns in practically no time. We argue that these results are\nindicative of issues in current approaches to Visual Dialogue and conduct\nanalyses to highlight implicit dataset biases and effects of over-constrained\nevaluation metrics. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 08:18:37 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 18:20:00 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 10:09:41 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Massiceti", "Daniela", ""], ["Dokania", "Puneet K.", ""], ["Siddharth", "N.", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1812.06604", "submitter": "Or Biran", "authors": "Yassine Benajiba, Jin Sun, Yong Zhang, Longquan Jiang, Zhiliang Weng\n  and Or Biran", "title": "Siamese Networks for Semantic Pattern Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Pattern Similarity is an interesting, though not often encountered\nNLP task where two sentences are compared not by their specific meaning, but by\ntheir more abstract semantic pattern (e.g., preposition or frame). We utilize\nSiamese Networks to model this task, and show its usefulness in determining SQL\npatterns for unseen questions in a database-backed question answering scenario.\nOur approach achieves high accuracy and contains a built-in proxy for\nconfidence, which can be used to keep precision arbitrarily high.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 03:52:11 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Benajiba", "Yassine", ""], ["Sun", "Jin", ""], ["Zhang", "Yong", ""], ["Jiang", "Longquan", ""], ["Weng", "Zhiliang", ""], ["Biran", "Or", ""]]}, {"id": "1812.06624", "submitter": "Chiranjib Sur", "authors": "Chiranjib Sur", "title": "Feature Fusion Effects of Tensor Product Representation on\n  (De)Compositional Network for Caption Generation for Images", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Progress in image captioning is gradually getting complex as researchers try\nto generalized the model and define the representation between visual features\nand natural language processing. This work tried to define such kind of\nrelationship in the form of representation called Tensor Product Representation\n(TPR) which generalized the scheme of language modeling and structuring the\nlinguistic attributes (related to grammar and parts of speech of language)\nwhich will provide a much better structure and grammatically correct sentence.\nTPR enables better and unique representation and structuring of the feature\nspace and will enable better sentence composition from these representations. A\nlarge part of the different ways of defining and improving these TPR are\ndiscussed and their performance with respect to the traditional procedures and\nfeature representations are evaluated for image captioning application. The new\nmodels achieved considerable improvement than the corresponding previous\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 06:24:03 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Sur", "Chiranjib", ""]]}, {"id": "1812.06700", "submitter": "Binny Mathew", "authors": "Punyajoy Saha, Binny Mathew, Pawan Goyal, Animesh Mukherjee", "title": "Hateminers : Detecting Hate speech against Women", "comments": "5 Pages, 2 Figures, 1 Table, Model Available at\n  https://github.com/punyajoy/Hateminers-EVALITA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the online proliferation of hate speech, there is an urgent need for\nsystems that can detect such harmful content. In this paper, We present the\nmachine learning models developed for the Automatic Misogyny Identification\n(AMI) shared task at EVALITA 2018. We generate three types of features:\nSentence Embeddings, TF-IDF Vectors, and BOW Vectors to represent each tweet.\nThese features are then concatenated and fed into the machine learning models.\nOur model came First for the English Subtask A and Fifth for the English\nSubtask B. We release our winning model for public use and it's available at\nhttps://github.com/punyajoy/Hateminers-EVALITA.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 11:20:15 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Saha", "Punyajoy", ""], ["Mathew", "Binny", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1812.06705", "submitter": "Wu Xing", "authors": "Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, Songlin Hu", "title": "Conditional BERT Contextual Augmentation", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel data augmentation method for labeled sentences called\nconditional BERT contextual augmentation. Data augmentation methods are often\napplied to prevent overfitting and improve generalization of deep neural\nnetwork models. Recently proposed contextual augmentation augments labeled\nsentences by randomly replacing words with more varied substitutions predicted\nby language model. BERT demonstrates that a deep bidirectional language model\nis more powerful than either an unidirectional language model or the shallow\nconcatenation of a forward and backward model. We retrofit BERT to conditional\nBERT by introducing a new conditional masked language model\\footnote{The term\n\"conditional masked language model\" appeared once in original BERT paper, which\nindicates context-conditional, is equivalent to term \"masked language model\".\nIn our paper, \"conditional masked language model\" indicates we apply extra\nlabel-conditional constraint to the \"masked language model\".} task. The well\ntrained conditional BERT can be applied to enhance contextual augmentation.\nExperiments on six various different text classification tasks show that our\nmethod can be easily applied to both convolutional or recurrent neural networks\nclassifier to obtain obvious improvement.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 11:26:42 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Wu", "Xing", ""], ["Lv", "Shangwen", ""], ["Zang", "Liangjun", ""], ["Han", "Jizhong", ""], ["Hu", "Songlin", ""]]}, {"id": "1812.06722", "submitter": "Feiliang Ren", "authors": "Feiliang Ren, Yining Hou, Yan Li, Linfeng Pan, Yi Zhang, Xiaobo Liang,\n  Yongkang Liu, Yu Guo, Rongsheng Zhao, Ruicheng Ming, Huiming Wu", "title": "TechKG: A Large-Scale Chinese Technology-Oriented Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph is a kind of valuable knowledge base which would benefit lots\nof AI-related applications. Up to now, lots of large-scale knowledge graphs\nhave been built. However, most of them are non-Chinese and designed for general\npurpose. In this work, we introduce TechKG, a large scale Chinese knowledge\ngraph that is technology-oriented. It is built automatically from massive\ntechnical papers that are published in Chinese academic journals of different\nresearch domains. Some carefully designed heuristic rules are used to extract\nhigh quality entities and relations. Totally, it comprises of over 260 million\ntriplets that are built upon more than 52 million entities which come from 38\nresearch domains. Our preliminary ex-periments indicate that TechKG has high\nadaptability and can be used as a dataset for many diverse AI-related\napplications. We released TechKG at: http://www.techkg.cn.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 11:59:53 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Ren", "Feiliang", ""], ["Hou", "Yining", ""], ["Li", "Yan", ""], ["Pan", "Linfeng", ""], ["Zhang", "Yi", ""], ["Liang", "Xiaobo", ""], ["Liu", "Yongkang", ""], ["Guo", "Yu", ""], ["Zhao", "Rongsheng", ""], ["Ming", "Ruicheng", ""], ["Wu", "Huiming", ""]]}, {"id": "1812.06834", "submitter": "Yoon Kim", "authors": "Yoon Kim, Sam Wiseman, Alexander M. Rush", "title": "A Tutorial on Deep Latent Variable Models of Natural Language", "comments": "EMNLP 2018 Tutorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much recent, exciting work on combining the complementary\nstrengths of latent variable models and deep learning. Latent variable modeling\nmakes it easy to explicitly specify model constraints through conditional\nindependence properties, while deep learning makes it possible to parameterize\nthese conditional likelihoods with powerful function approximators. While these\n\"deep latent variable\" models provide a rich, flexible framework for modeling\nmany real-world phenomena, difficulties exist: deep parameterizations of\nconditional likelihoods usually make posterior inference intractable, and\nlatent variable objectives often complicate backpropagation by introducing\npoints of non-differentiability. This tutorial explores these issues in depth\nthrough the lens of variational inference.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 15:26:29 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 15:24:26 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 01:14:32 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Kim", "Yoon", ""], ["Wiseman", "Sam", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1812.06864", "submitter": "Neil Zeghidour", "authors": "Neil Zeghidour, Qiantong Xu, Vitaliy Liptchinsky, Nicolas Usunier,\n  Gabriel Synnaeve, Ronan Collobert", "title": "Fully Convolutional Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art speech recognition systems build on recurrent neural\nnetworks for acoustic and/or language modeling, and rely on feature extraction\npipelines to extract mel-filterbanks or cepstral coefficients. In this paper we\npresent an alternative approach based solely on convolutional neural networks,\nleveraging recent advances in acoustic models from the raw waveform and\nlanguage modeling. This fully convolutional approach is trained end-to-end to\npredict characters from the raw waveform, removing the feature extraction step\naltogether. An external convolutional language model is used to decode words.\nOn Wall Street Journal, our model matches the current state-of-the-art. On\nLibrispeech, we report state-of-the-art performance among end-to-end models,\nincluding Deep Speech 2 trained with 12 times more acoustic data and\nsignificantly more linguistic data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 16:07:12 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 11:43:14 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zeghidour", "Neil", ""], ["Xu", "Qiantong", ""], ["Liptchinsky", "Vitaliy", ""], ["Usunier", "Nicolas", ""], ["Synnaeve", "Gabriel", ""], ["Collobert", "Ronan", ""]]}, {"id": "1812.06876", "submitter": "Stefan Constantin", "authors": "Stefan Constantin, Jan Niehues, Alex Waibel", "title": "Multi-task learning to improve natural language understanding", "comments": "11 pages, 4 figures, 2 tables, forthcoming in IWSDS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently advancements in sequence-to-sequence neural network architectures\nhave led to an improved natural language understanding. When building a neural\nnetwork-based Natural Language Understanding component, one main challenge is\nto collect enough training data. The generation of a synthetic dataset is an\ninexpensive and quick way to collect data. Since this data often has less\nvariety than real natural language, neural networks often have problems to\ngeneralize to unseen utterances during testing. In this work, we address this\nchallenge by using multi-task learning. We train out-of-domain real data\nalongside in-domain synthetic data to improve natural language understanding.\nWe evaluate this approach in the domain of airline travel information with two\nsynthetic datasets. As out-of-domain real data, we test two datasets based on\nthe subtitles of movies and series. By using an attention-based encoder-decoder\nmodel, we were able to improve the F1-score over strong baselines from 80.76 %\nto 84.98 % in the smaller synthetic dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 16:32:05 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 17:43:00 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Constantin", "Stefan", ""], ["Niehues", "Jan", ""], ["Waibel", "Alex", ""]]}, {"id": "1812.06953", "submitter": "Saber Malekzadeh", "authors": "Saber Malekzadeh, Mohammad Hossein Gholizadeh, Seyed Naser Razavi", "title": "Persian Vowel recognition with MFCC and ANN on PCVC speech dataset", "comments": "The 5th International Conference of Electrical Engineering, Computer\n  Science and Information Technology 2018", "journal-ref": null, "doi": "10.13140/RG.2.2.12187.72486", "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new method for recognition of consonant-vowel phonemes\ncombination on a new Persian speech dataset titled as PCVC (Persian\nConsonant-Vowel Combination) is proposed which is used to recognize Persian\nphonemes. In PCVC dataset, there are 20 sets of audio samples from 10 speakers\nwhich are combinations of 23 consonant and 6 vowel phonemes of Persian\nlanguage. In each sample, there is a combination of one vowel and one\nconsonant. First, the consonant phoneme is pronounced and just after it, the\nvowel phoneme is pronounced. Each sound sample is a frame of 2 seconds of\naudio. In every 2 seconds, there is an average of 0.5 second speech and the\nrest is silence. In this paper, the proposed method is the implementations of\nthe MFCC (Mel Frequency Cepstrum Coefficients) on every partitioned sound\nsample. Then, every train sample of MFCC vector is given to a multilayer\nperceptron feed-forward ANN (Artificial Neural Network) for training process.\nAt the end, the test samples are examined on ANN model for phoneme recognition.\nAfter training and testing process, the results are presented in recognition of\nvowels. Then, the average percent of recognition for vowel phonemes are\ncomputed.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 18:50:36 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Malekzadeh", "Saber", ""], ["Gholizadeh", "Mohammad Hossein", ""], ["Razavi", "Seyed Naser", ""]]}, {"id": "1812.06974", "submitter": "Jieli Zhou", "authors": "Jieli Zhou, Yuntao Zhou, Yi Xu", "title": "Analogy Search Engine: Finding Analogies in Cross-Domain Research Papers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the rapid proliferation of research publications in the\nfield of Artificial Intelligence, it is becoming increasingly difficult for\nresearchers to effectively keep up with all the latest research in one's own\ndomains. However, history has shown that scientific breakthroughs often come\nfrom collaborations of researchers from different domains. Traditional search\nalgorithms like Lexical search, which look for literal matches or synonyms and\nvariants of the query words, are not effective for discovering cross-domain\nresearch papers and meeting the needs of researchers in this age of information\noverflow. In this paper, we developed and tested an innovative semantic search\nengine, Analogy Search Engine (ASE), for 2000 AI research paper abstracts\nacross domains like Language Technologies, Robotics, Machine Learning,\nComputational Biology, Human Computer Interactions, etc. ASE combines recent\ntheories and methods from Computational Analogy and Natural Language Processing\nto go beyond keyword-based lexical search and discover the deeper analogical\nrelationships among research paper abstracts. We experimentally show that ASE\nis capable of finding more interesting and useful research papers than baseline\nelasticsearch. Furthermore, we believe that the methods used in ASE go beyond\nacademic paper and will benefit many other document search tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 16:15:13 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Zhou", "Jieli", ""], ["Zhou", "Yuntao", ""], ["Xu", "Yi", ""]]}, {"id": "1812.07023", "submitter": "Shikhar Sharma", "authors": "Dat Tien Nguyen, Shikhar Sharma, Hannes Schulz, Layla El Asri", "title": "From FiLM to Video: Multi-turn Question Answering with Multi-modal\n  Context", "comments": "Accepted for an Oral presentation at the DSTC7 workshop at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding audio-visual content and the ability to have an informative\nconversation about it have both been challenging areas for intelligent systems.\nThe Audio Visual Scene-aware Dialog (AVSD) challenge, organized as a track of\nthe Dialog System Technology Challenge 7 (DSTC7), proposes a combined task,\nwhere a system has to answer questions pertaining to a video given a dialogue\nwith previous question-answer pairs and the video itself. We propose for this\ntask a hierarchical encoder-decoder model which computes a multi-modal\nembedding of the dialogue context. It first embeds the dialogue history using\ntwo LSTMs. We extract video and audio frames at regular intervals and compute\nsemantic features using pre-trained I3D and VGGish models, respectively. Before\nsummarizing both modalities into fixed-length vectors using LSTMs, we use FiLM\nblocks to condition them on the embeddings of the current question, which\nallows us to reduce the dimensionality considerably. Finally, we use an LSTM\ndecoder that we train with scheduled sampling and evaluate using beam search.\nCompared to the modality-fusing baseline model released by the AVSD challenge\norganizers, our model achieves a relative improvements of more than 16%,\nscoring 0.36 BLEU-4 and more than 33%, scoring 0.997 CIDEr.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 19:41:37 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Nguyen", "Dat Tien", ""], ["Sharma", "Shikhar", ""], ["Schulz", "Hannes", ""], ["Asri", "Layla El", ""]]}, {"id": "1812.07104", "submitter": "Arindam Chowdhury", "authors": "Rohit Rahul, Arindam Chowdhury, Animesh, Samarth Mittal, Lovekesh Vig", "title": "Reading Industrial Inspection Sheets by Inferring Visual Relations", "comments": "Published in 3rd International Workshop on Robust Reading at Asian\n  Conference on Computer Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional mode of recording faults in heavy factory equipment has been\nvia hand marked inspection sheets, wherein a machine engineer manually marks\nthe faulty machine regions on a paper outline of the machine. Over the years,\nmillions of such inspection sheets have been recorded and the data within these\nsheets has remained inaccessible. However, with industries going digital and\nwaking up to the potential value of fault data for machine health monitoring,\nthere is an increased impetus towards digitization of these hand marked\ninspection records. To target this digitization, we propose a novel visual\npipeline combining state of the art deep learning models, with domain knowledge\nand low level vision techniques, followed by inference of visual relationships.\nOur framework is robust to the presence of both static and non-static\nbackground in the document, variability in the machine template diagrams,\nunstructured shape of graphical objects to be identified and variability in the\nstrokes of handwritten text. The proposed pipeline incorporates a capsule and\nspatial transformer network based classifier for accurate text reading, and a\ncustomized CTPN network for text detection in addition to hybrid techniques for\narrow detection and dialogue cloud removal. We have tested our approach on a\nreal world dataset of 50 inspection sheets for large containers and boilers.\nThe results are visually appealing and the pipeline achieved an accuracy of\n87.1% for text detection and 94.6% for text reading.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 13:10:14 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Rahul", "Rohit", ""], ["Chowdhury", "Arindam", ""], ["Animesh", "", ""], ["Mittal", "Samarth", ""], ["Vig", "Lovekesh", ""]]}, {"id": "1812.07108", "submitter": "Shaoxiong Ji", "authors": "Shaoxiong Ji, Shirui Pan, Guodong Long, Xue Li, Jing Jiang, Zi Huang", "title": "Learning Private Neural Language Modeling with Attentive Aggregation", "comments": "8 pages, 5 figures, to appear in IJCNN 2019", "journal-ref": null, "doi": "10.1109/IJCNN.2019.8852464", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile keyboard suggestion is typically regarded as a word-level language\nmodeling problem. Centralized machine learning technique requires massive user\ndata collected to train on, which may impose privacy concerns for sensitive\npersonal typing data of users. Federated learning (FL) provides a promising\napproach to learning private language modeling for intelligent personalized\nkeyboard suggestion by training models in distributed clients rather than\ntraining in a central server. To obtain a global model for prediction, existing\nFL algorithms simply average the client models and ignore the importance of\neach client during model aggregation. Furthermore, there is no optimization for\nlearning a well-generalized global model on the central server. To solve these\nproblems, we propose a novel model aggregation with the attention mechanism\nconsidering the contribution of clients models to the global model, together\nwith an optimization technique during server aggregation. Our proposed\nattentive aggregation method minimizes the weighted distance between the server\nmodel and client models through iterative parameters updating while attends the\ndistance between the server model and client models. Through experiments on two\npopular language modeling datasets and a social media dataset, our proposed\nmethod outperforms its counterparts in terms of perplexity and communication\ncost in most settings of comparison.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 23:51:58 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 05:34:14 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ji", "Shaoxiong", ""], ["Pan", "Shirui", ""], ["Long", "Guodong", ""], ["Li", "Xue", ""], ["Jiang", "Jing", ""], ["Huang", "Zi", ""]]}, {"id": "1812.07200", "submitter": "Bibliotheque Universitaire Deposants Hal-Avignon", "authors": "Xavier Bost (LIA), Georges Linares (LIA)", "title": "D{\\'e}tection de locuteurs dans les s{\\'e}ries TV", "comments": "in French", "journal-ref": "Coria 2015, Mar 2015, Paris, France", "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker diarization of audio streams turns out to be particularly challenging\nwhen applied to fictional films, where many characters talk in various acoustic\nconditions (background music, sound effects, variations in intonation...).\nDespite this acoustic variability, such movies exhibit specific visual\npatterns, particularly within dialogue scenes. In this paper, we introduce a\ntwo-step method to achieve speaker diarization in TV series: speaker\ndiarization is first performed locally within scenes visually identified as\ndialogues; then, the hypothesized local speakers are compared to each other\nduring a second clustering process in order to detect recurring speakers: this\nsecond stage of clustering is subject to the constraint that the different\nspeakers involved in the same dialogue have to be assigned to different\nclusters. The performances of our approach are compared to those obtained by\nstandard speaker diarization tools applied to the same data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:11:19 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Linares", "Georges", "", "LIA"]]}, {"id": "1812.07205", "submitter": "Xavier Bost", "authors": "Xavier Bost (LIA), Georges Linar\\`es (LIA), Serigne Gueye (LIA)", "title": "Audiovisual speaker diarization of TV series", "comments": null, "journal-ref": "2015 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Apr 2015, Brisbane, Australia. IEEE, pp.4799-4803, 2015", "doi": "10.1109/ICASSP.2015.7178882", "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker diarization may be difficult to achieve when applied to narrative\nfilms, where speakers usually talk in adverse acoustic conditions: background\nmusic, sound effects, wide variations in intonation may hide the inter-speaker\nvariability and make audio-based speaker diarization approaches error prone. On\nthe other hand, such fictional movies exhibit strong regularities at the image\nlevel, particularly within dialogue scenes. In this paper, we propose to\nperform speaker diarization within dialogue scenes of TV series by combining\nthe audio and video modalities: speaker diarization is first performed by using\neach modality, the two resulting partitions of the instance set are then\noptimally matched, before the remaining instances, corresponding to cases of\ndisagreement between both modalities, are finally processed. The results\nobtained by applying such a multi-modal approach to fictional films turn out to\noutperform those obtained by relying on a single modality.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:21:36 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 14:59:28 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Linar\u00e8s", "Georges", "", "LIA"], ["Gueye", "Serigne", "", "LIA"]]}, {"id": "1812.07207", "submitter": "Xavier Bost", "authors": "X. Bost (LIA), G. Senay (LIA), M. El-B\\`eze (LIA), R. De Mori (LIA)", "title": "Multiple topic identification in human/human conversations", "comments": null, "journal-ref": "Computer Speech \\& Language, 2015, 34 (1), pp.18-42", "doi": "10.1016/j.csl.2015.03.006", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the automatic analysis of real-life telephone\nconversations between an agent and a customer of a customer care service (ccs).\nThe application domain is the public transportation system in Paris and the\npurpose is to collect statistics about customer problems in order to monitor\nthe service and decide priorities on the intervention for improving user\nsatisfaction. Of primary importance for the analysis is the detection of themes\nthat are the object of customer problems. Themes are defined in the application\nrequirements and are part of the application ontology that is implicit in the\nccs documentation. Due to variety of customer population, the structure of\nconversations with an agent is unpredictable. A conversation may be about one\nor more themes. Theme mentions can be interleaved with mentions of facts that\nare irrelevant for the application purpose. Furthermore, in certain\nconversations theme mentions are localized in specific conversation segments\nwhile in other conversations mentions cannot be localized. As a consequence,\napproaches to feature extraction with and without mention localization are\nconsidered. Application domain relevant themes identified by an automatic\nprocedure are expressed by specific sentences whose words are hypothesized by\nan automatic speech recognition (asr) system. The asr system is error prone.\nThe word error rates can be very high for many reasons. Among them it is worth\nmentioning unpredictable background noise, speaker accent, and various types of\nspeech disfluencies. As the application task requires the composition of\nproportions of theme mentions, a sequential decision strategy is introduced in\nthis paper for performing a survey of the large amount of conversations made\navailable in a given time period. The strategy has to sample the conversations\nto form a survey containing enough data analyzed with high accuracy so that\nproportions can be estimated with sufficient accuracy. Due to the unpredictable\ntype of theme mentions, it is appropriate to consider methods for theme\nhypothesization based on global as well as local feature extraction. Two\nsystems based on each type of feature extraction will be considered by the\nstrategy. One of the four methods is novel. It is based on a new definition of\ndensity of theme mentions and on the localization of high density zones whose\nboundaries do not need to be precisely detected. The sequential decision\nstrategy starts by grouping theme hypotheses into sets of different expected\naccuracy and coverage levels. For those sets for which accuracy can be improved\nwith a consequent increase of coverage a new system with new features is\nintroduced. Its execution is triggered only when specific preconditions are met\non the hypotheses generated by the basic four systems. Experimental results are\nprovided on a corpus collected in the call center of the Paris transportation\nsystem known as ratp. The results show that surveys with high accuracy and\ncoverage can be composed with the proposed strategy and systems. This makes it\npossible to apply a previously published proportion estimation approach that\ntakes into account hypothesization errors .\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:24:17 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 15:01:26 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Bost", "X.", "", "LIA"], ["Senay", "G.", "", "LIA"], ["El-B\u00e8ze", "M.", "", "LIA"], ["De Mori", "R.", "", "LIA"]]}, {"id": "1812.07209", "submitter": "Xavier Bost", "authors": "Xavier Bost (LIA), Georges Linares (LIA)", "title": "Constrained speaker diarization of TV series based on visual patterns", "comments": null, "journal-ref": "2014 IEEE Spoken Language Technology Workshop (SLT), Dec 2014,\n  South Lake Tahoe, United States. IEEE, pp.390-395, 2014", "doi": "10.1109/SLT.2014.7078606", "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker diarization, usually denoted as the ''who spoke when'' task, turns\nout to be particularly challenging when applied to fictional films, where many\ncharacters talk in various acoustic conditions (background music, sound\neffects...). Despite this acoustic variability , such movies exhibit specific\nvisual patterns in the dialogue scenes. In this paper, we introduce a two-step\nmethod to achieve speaker diarization in TV series: a speaker diarization is\nfirst performed locally in the scenes detected as dialogues; then, the\nhypothesized local speakers are merged in a second agglomerative clustering\nprocess, with the constraint that speakers locally hypothesized to be distinct\nmust not be assigned to the same cluster. The performances of our approach are\ncompared to those obtained by standard speaker diarization tools applied to the\nsame data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:29:27 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 15:04:31 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Linares", "Georges", "", "LIA"]]}, {"id": "1812.07248", "submitter": "Rasmus Berg Palm", "authors": "Rasmus Berg Palm, Florian Laws, Ole Winther", "title": "Attend, Copy, Parse -- End-to-end information extraction from documents", "comments": null, "journal-ref": "ICDAR 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document information extraction tasks performed by humans create data\nconsisting of a PDF or document image input, and extracted string outputs. This\nend-to-end data is naturally consumed and produced when performing the task\nbecause it is valuable in and of itself. It is naturally available, at no\nadditional cost. Unfortunately, state-of-the-art word classification methods\nfor information extraction cannot use this data, instead requiring word-level\nlabels which are expensive to create and consequently not available for many\nreal life tasks. In this paper we propose the Attend, Copy, Parse architecture,\na deep neural network model that can be trained directly on end-to-end data,\nbypassing the need for word-level labels. We evaluate the proposed architecture\non a large diverse set of invoices, and outperform a state-of-the-art\nproduction system based on word classification. We believe our proposed\narchitecture can be used on many real life information extraction tasks where\nword classification cannot be used due to a lack of the required word-level\nlabels.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 09:05:53 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 13:40:42 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 07:03:05 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Palm", "Rasmus Berg", ""], ["Laws", "Florian", ""], ["Winther", "Ole", ""]]}, {"id": "1812.07324", "submitter": "Mihai P\\^irvu", "authors": "Mihai Cristian P\\^irvu, Alexandra Anghel, Ciprian Borodescu, Alexandru\n  Constantin", "title": "Predicting user intent from search queries using both CNNs and RNNs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting user behaviour on a website is a difficult task, which requires\nthe integration of multiple sources of information, such as geo-location, user\nprofile or web surfing history. In this paper we tackle the problem of\npredicting the user intent, based on the queries that were used to access a\ncertain webpage. We make no additional assumptions, such as domain detection,\ndevice used or location, and only use the word information embedded in the\ngiven query. In order to build competitive classifiers, we label a small\nfraction of the EDI query intent prediction dataset\n\\cite{edi-challenge-dataset}, which is used as ground truth. Then, using\nvarious rule-based approaches, we automatically label the rest of the dataset,\ntrain the classifiers and evaluate the quality of the automatic labeling on the\nground truth dataset. We used both recurrent and convolutional networks as the\nmodels, while representing the words in the query with multiple embedding\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 12:15:33 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["P\u00eervu", "Mihai Cristian", ""], ["Anghel", "Alexandra", ""], ["Borodescu", "Ciprian", ""], ["Constantin", "Alexandru", ""]]}, {"id": "1812.07546", "submitter": "Joo-Kyung Kim", "authors": "Joo-Kyung Kim and Young-Bum Kim", "title": "Supervised Domain Enablement Attention for Personalized Domain\n  Classification", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale domain classification for natural language understanding,\nleveraging each user's domain enablement information, which refers to the\npreferred or authenticated domains by the user, with attention mechanism has\nbeen shown to improve the overall domain classification performance. In this\npaper, we propose a supervised enablement attention mechanism, which utilizes\nsigmoid activation for the attention weighting so that the attention can be\ncomputed with more expressive power without the weight sum constraint of\nsoftmax attention. The attention weights are explicitly encouraged to be\nsimilar to the corresponding elements of the ground-truth's one-hot vector by\nsupervised attention, and the attention information of the other enabled\ndomains is leveraged through self-distillation. By evaluating on the actual\nutterances from a large-scale IPDA, we show that our approach significantly\nimproves domain classification performance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 18:21:24 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Kim", "Joo-Kyung", ""], ["Kim", "Young-Bum", ""]]}, {"id": "1812.07617", "submitter": "Raymond Li", "authors": "Raymond Li, Samira Kahou, Hannes Schulz, Vincent Michalski, Laurent\n  Charlin, Chris Pal", "title": "Towards Deep Conversational Recommendations", "comments": "17 pages, 5 figures, Accepted at 32nd Conference on Neural\n  Information Processing Systems (NeurIPS 2018), Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing interest in using neural networks and deep learning\ntechniques to create dialogue systems. Conversational recommendation is an\ninteresting setting for the scientific exploration of dialogue with natural\nlanguage as the associated discourse involves goal-driven dialogue that often\ntransforms naturally into more free-form chat. This paper provides two\ncontributions. First, until now there has been no publicly available\nlarge-scale dataset consisting of real-world dialogues centered around\nrecommendations. To address this issue and to facilitate our exploration here,\nwe have collected ReDial, a dataset consisting of over 10,000 conversations\ncentered around the theme of providing movie recommendations. We make this data\navailable to the community for further research. Second, we use this dataset to\nexplore multiple facets of conversational recommendations. In particular we\nexplore new neural architectures, mechanisms, and methods suitable for\ncomposing conversational recommendation systems. Our dataset allows us to\nsystematically probe model sub-components addressing different parts of the\noverall problem domain ranging from: sentiment analysis and cold-start\nrecommendation generation to detailed aspects of how natural language is used\nin this setting in the real world. We combine such sub-components into a\nfull-blown dialogue system and examine its behavior.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 19:34:32 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 18:54:59 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Li", "Raymond", ""], ["Kahou", "Samira", ""], ["Schulz", "Hannes", ""], ["Michalski", "Vincent", ""], ["Charlin", "Laurent", ""], ["Pal", "Chris", ""]]}, {"id": "1812.07625", "submitter": "Ronan Collobert", "authors": "Vineel Pratap, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel\n  Synnaeve, Vitaliy Liptchinsky, Ronan Collobert", "title": "wav2letter++: The Fastest Open-source Speech Recognition System", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683535", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces wav2letter++, the fastest open-source deep learning\nspeech recognition framework. wav2letter++ is written entirely in C++, and uses\nthe ArrayFire tensor library for maximum efficiency. Here we explain the\narchitecture and design of the wav2letter++ system and compare it to other\nmajor open-source speech recognition systems. In some cases wav2letter++ is\nmore than 2x faster than other optimized frameworks for training end-to-end\nneural networks for speech recognition. We also show that wav2letter++'s\ntraining times scale linearly to 64 GPUs, the highest we tested, for models\nwith 100 million parameters. High-performance frameworks enable fast iteration,\nwhich is often a crucial factor in successful research and model tuning on new\ndatasets and tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 19:57:24 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Pratap", "Vineel", ""], ["Hannun", "Awni", ""], ["Xu", "Qiantong", ""], ["Cai", "Jeff", ""], ["Kahn", "Jacob", ""], ["Synnaeve", "Gabriel", ""], ["Liptchinsky", "Vitaliy", ""], ["Collobert", "Ronan", ""]]}, {"id": "1812.07754", "submitter": "Raphael Tang", "authors": "Raphael Tang, Gefei Yang, Hong Wei, Yajie Mao, Ferhan Ture, Jimmy Lin", "title": "Streaming Voice Query Recognition using Causal Convolutional Recurrent\n  Neural Networks", "comments": "5 pages, 2 figures, submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice-enabled commercial products are ubiquitous, typically enabled by\nlightweight on-device keyword spotting (KWS) and full automatic speech\nrecognition (ASR) in the cloud. ASR systems require significant computational\nresources in training and for inference, not to mention copious amounts of\nannotated speech data. KWS systems, on the other hand, are less\nresource-intensive but have limited capabilities. On the Comcast Xfinity X1\nentertainment platform, we explore a middle ground between ASR and KWS: We\nintroduce a novel, resource-efficient neural network for voice query\nrecognition that is much more accurate than state-of-the-art CNNs for KWS, yet\ncan be easily trained and deployed with limited resources. On an evaluation\ndataset representing the top 200 voice queries, we achieve a low false alarm\nrate of 1% and a query error rate of 6%. Our model performs inference 8.24x\nfaster than the current ASR system.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 04:51:42 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Tang", "Raphael", ""], ["Yang", "Gefei", ""], ["Wei", "Hong", ""], ["Mao", "Yajie", ""], ["Ture", "Ferhan", ""], ["Lin", "Jimmy", ""]]}, {"id": "1812.07805", "submitter": "Zheng Chen", "authors": "Zheng Chen, Yong Zhang, Yue Shang, Xiaohua Hu", "title": "Unifying Topic, Sentiment & Preference in an HDP-Based Rating Regression\n  Model for Online Reviews", "comments": null, "journal-ref": "Asian Conference on Machine Learning. 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new HDP based online review rating regression model\nnamed Topic-Sentiment-Preference Regression Analysis (TSPRA). TSPRA combines\ntopics (i.e. product aspects), word sentiment and user preference as regression\nfactors, and is able to perform topic clustering, review rating prediction,\nsentiment analysis and what we invent as \"critical aspect\" analysis altogether\nin one framework. TSPRA extends sentiment approaches by integrating the key\nconcept \"user preference\" in collaborative filtering (CF) models into\nconsideration, while it is distinct from current CF models by decoupling \"user\npreference\" and \"sentiment\" as independent factors. Our experiments conducted\non 22 Amazon datasets show overwhelming better performance in rating\npredication against a state-of-art model FLAME (2015) in terms of error,\nPearson's Correlation and number of inverted pairs. For sentiment analysis, we\ncompare the derived word sentiments against a public sentiment resource\nSenticNet3 and our sentiment estimations clearly make more sense in the context\nof online reviews. Last, as a result of the de-correlation of \"user preference\"\nfrom \"sentiment\", TSPRA is able to evaluate a new concept \"critical aspects\",\ndefined as the product aspects seriously concerned by users but negatively\ncommented in reviews. Improvement to such \"critical aspects\" could be most\neffective to enhance user experience.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:33:31 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Chen", "Zheng", ""], ["Zhang", "Yong", ""], ["Shang", "Yue", ""], ["Hu", "Xiaohua", ""]]}, {"id": "1812.07807", "submitter": "Fandong Meng", "authors": "Fandong Meng and Jinchao Zhang", "title": "DTMT: A Novel Deep Transition Architecture for Neural Machine\n  Translation", "comments": "Accepted at AAAI 2019. Code is available at:\n  https://github.com/fandongmeng/DTMT_InDec", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past years have witnessed rapid developments in Neural Machine Translation\n(NMT). Most recently, with advanced modeling and training techniques, the\nRNN-based NMT (RNMT) has shown its potential strength, even compared with the\nwell-known Transformer (self-attentional) model. Although the RNMT model can\npossess very deep architectures through stacking layers, the transition depth\nbetween consecutive hidden states along the sequential axis is still shallow.\nIn this paper, we further enhance the RNN-based NMT through increasing the\ntransition depth between consecutive hidden states and build a novel Deep\nTransition RNN-based Architecture for Neural Machine Translation, named DTMT.\nThis model enhances the hidden-to-hidden transition with multiple non-linear\ntransformations, as well as maintains a linear transformation path throughout\nthis deep transition by the well-designed linear transformation mechanism to\nalleviate the gradient vanishing problem. Experiments show that with the\nspecially designed deep transition modules, our DTMT can achieve remarkable\nimprovements on translation quality. Experimental results on Chinese->English\ntranslation task show that DTMT can outperform the Transformer model by +2.09\nBLEU points and achieve the best results ever reported in the same dataset. On\nWMT14 English->German and English->French translation tasks, DTMT shows\nsuperior quality to the state-of-the-art NMT systems, including the Transformer\nand the RNMT+.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:35:38 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 11:50:08 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Meng", "Fandong", ""], ["Zhang", "Jinchao", ""]]}, {"id": "1812.07809", "submitter": "Paul Pu Liang", "authors": "Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe Morency,\n  Barnabas Poczos", "title": "Found in Translation: Learning Robust Joint Representations by Cyclic\n  Translations Between Modalities", "comments": "AAAI 2019, code available at https://github.com/hainow/MCTN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal sentiment analysis is a core research area that studies speaker\nsentiment expressed from the language, visual, and acoustic modalities. The\ncentral challenge in multimodal learning involves inferring joint\nrepresentations that can process and relate information from these modalities.\nHowever, existing work learns joint representations by requiring all modalities\nas input and as a result, the learned representations may be sensitive to noisy\nor missing modalities at test time. With the recent success of sequence to\nsequence (Seq2Seq) models in machine translation, there is an opportunity to\nexplore new ways of learning joint representations that may not require all\ninput modalities at test time. In this paper, we propose a method to learn\nrobust joint representations by translating between modalities. Our method is\nbased on the key insight that translation from a source to a target modality\nprovides a method of learning joint representations using only the source\nmodality as input. We augment modality translations with a cycle consistency\nloss to ensure that our joint representations retain maximal information from\nall modalities. Once our translation model is trained with paired multimodal\ndata, we only need data from the source modality at test time for final\nsentiment prediction. This ensures that our model remains robust from\nperturbations or missing information in the other modalities. We train our\nmodel with a coupled translation-prediction objective and it achieves new\nstate-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI,\nICT-MMMO, and YouTube. Additional experiments show that our model learns\nincreasingly discriminative joint representations with more input modalities\nwhile maintaining robustness to missing or perturbed modalities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:38:21 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 09:20:33 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Pham", "Hai", ""], ["Liang", "Paul Pu", ""], ["Manzini", "Thomas", ""], ["Morency", "Louis-Philippe", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1812.07860", "submitter": "Artaches Ambartsoumian", "authors": "Artaches Ambartsoumian and Fred Popowich", "title": "Self-Attention: A Better Building Block for Sentiment Analysis Neural\n  Network Classifiers", "comments": null, "journal-ref": null, "doi": "10.18653/v1/P17", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment Analysis has seen much progress in the past two decades. For the\npast few years, neural network approaches, primarily RNNs and CNNs, have been\nthe most successful for this task. Recently, a new category of neural networks,\nself-attention networks (SANs), have been created which utilizes the attention\nmechanism as the basic building block. Self-attention networks have been shown\nto be effective for sequence modeling tasks, while having no recurrence or\nconvolutions. In this work we explore the effectiveness of the SANs for\nsentiment analysis. We demonstrate that SANs are superior in performance to\ntheir RNN and CNN counterparts by comparing their classification accuracy on\nsix datasets as well as their model characteristics such as training speed and\nmemory consumption. Finally, we explore the effects of various SAN\nmodifications such as multi-head attention as well as two methods of\nincorporating sequence position information into SANs.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 10:21:20 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Ambartsoumian", "Artaches", ""], ["Popowich", "Fred", ""]]}, {"id": "1812.08033", "submitter": "Jingjing Gong", "authors": "Jingjing Gong, Xinchi Chen, Tao Gui, Xipeng Qiu", "title": "Switch-LSTMs for Multi-Criteria Chinese Word Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-criteria Chinese word segmentation is a promising but challenging task,\nwhich exploits several different segmentation criteria and mines their common\nunderlying knowledge. In this paper, we propose a flexible multi-criteria\nlearning for Chinese word segmentation. Usually, a segmentation criterion could\nbe decomposed into multiple sub-criteria, which are shareable with other\nsegmentation criteria. The process of word segmentation is a routing among\nthese sub-criteria. From this perspective, we present Switch-LSTMs to segment\nwords, which consist of several long short-term memory neural networks (LSTM),\nand a switcher to automatically switch the routing among these LSTMs. With\nthese auto-switched LSTMs, our model provides a more flexible solution for\nmulti-criteria CWS, which is also easy to transfer the learned knowledge to new\ncriteria. Experiments show that our model obtains significant improvements on\neight corpora with heterogeneous segmentation criteria, compared to the\nprevious method and single-criterion learning.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 15:48:01 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Gong", "Jingjing", ""], ["Chen", "Xinchi", ""], ["Gui", "Tao", ""], ["Qiu", "Xipeng", ""]]}, {"id": "1812.08039", "submitter": "Gabriel Marzinotto", "authors": "Gabriel Marzinotto (TALEP), Jeremy Auguste (TALEP), Frederic Bechet\n  (LIF), G\\'eraldine Damnati (FTR\\&D), Alexis Nasr (LIF)", "title": "Semantic Frame Parsing for Information Extraction : the CALOR corpus", "comments": null, "journal-ref": "LREC2018, May 2018, Miyazaki, France", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a publicly available corpus of French encyclopedic\nhistory texts annotated according to the Berkeley FrameNet formalism. The main\ndifference in our approach compared to previous works on semantic parsing with\nFrameNet is that we are not interested here in full text parsing but rather on\npartial parsing. The goal is to select from the FrameNet resources the minimal\nset of frames that are going to be useful for the applicative framework\ntargeted, in our case Information Extraction from encyclopedic documents. Such\nan approach leverages the manual annotation of larger corpora than those\nobtained through full text parsing and therefore opens the door to alternative\nmethods for Frame parsing than those used so far on the FrameNet 1.5 benchmark\ncorpus. The approaches compared in this study rely on an integrated sequence\nlabeling model which jointly optimizes frame identification and semantic role\nsegmentation and identification. The models compared are CRFs and multitasks\nbi-LSTMs.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 15:54:41 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Marzinotto", "Gabriel", "", "TALEP"], ["Auguste", "Jeremy", "", "TALEP"], ["Bechet", "Frederic", "", "LIF"], ["Damnati", "G\u00e9raldine", "", "FTR\\&D"], ["Nasr", "Alexis", "", "LIF"]]}, {"id": "1812.08044", "submitter": "Gabriel Marzinotto", "authors": "Gabriel Marzinotto (TALEP), G\\'eraldine Damnati (FTR\\&D), Frederic\n  Bechet (LIF, TALEP)", "title": "FrameNet automatic analysis : a study on a French corpus of encyclopedic\n  texts", "comments": "in French", "journal-ref": "TALN 2017, 2017, Orl{\\'e}ans, France", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an automatic frame analysis system evaluated on a\ncorpus of French encyclopedic history texts annotated according to the FrameNet\nformalism. The chosen approach relies on an integrated sequence labeling model\nwhich jointly optimizes frame identification and semantic role segmentation and\nidentification. The purpose of this study is to analyze the task complexity\nfrom several dimensions. Hence we provide detailed evaluations from a feature\nselection point of view and from the data point of view.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 15:59:31 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Marzinotto", "Gabriel", "", "TALEP"], ["Damnati", "G\u00e9raldine", "", "FTR\\&D"], ["Bechet", "Frederic", "", "LIF, TALEP"]]}, {"id": "1812.08046", "submitter": "Maral Dadvar", "authors": "Maral Dadvar, Kai Eckert", "title": "Cyberbullying Detection in Social Networks Using Deep Learning Based\n  Models; A Reproducibility Study", "comments": "13 Pages, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyberbullying is a disturbing online misbehaviour with troubling\nconsequences. It appears in different forms, and in most of the social\nnetworks, it is in textual format. Automatic detection of such incidents\nrequires intelligent systems. Most of the existing studies have approached this\nproblem with conventional machine learning models and the majority of the\ndeveloped models in these studies are adaptable to a single social network at a\ntime. In recent studies, deep learning based models have found their way in the\ndetection of cyberbullying incidents, claiming that they can overcome the\nlimitations of the conventional models, and improve the detection performance.\nIn this paper, we investigate the findings of a recent literature in this\nregard. We successfully reproduced the findings of this literature and\nvalidated their findings using the same datasets, namely Wikipedia, Twitter,\nand Formspring, used by the authors. Then we expanded our work by applying the\ndeveloped methods on a new YouTube dataset (~54k posts by ~4k users) and\ninvestigated the performance of the models in new social media platforms. We\nalso transferred and evaluated the performance of the models trained on one\nplatform to another platform. Our findings show that the deep learning based\nmodels outperform the machine learning models previously applied to the same\nYouTube dataset. We believe that the deep learning based models can also\nbenefit from integrating other sources of information and looking into the\nimpact of profile information of the users in social networks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 16:02:08 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Dadvar", "Maral", ""], ["Eckert", "Kai", ""]]}, {"id": "1812.08092", "submitter": "Martin Gerlach", "authors": "Martin Gerlach, Francesc Font-Clos", "title": "A standardized Project Gutenberg corpus for statistical analysis of\n  natural language and quantitative linguistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Project Gutenberg (PG) as a text corpus has been extremely popular\nin statistical analysis of language for more than 25 years. However, in\ncontrast to other major linguistic datasets of similar importance, no\nconsensual full version of PG exists to date. In fact, most PG studies so far\neither consider only a small number of manually selected books, leading to\npotential biased subsets, or employ vastly different pre-processing strategies\n(often specified in insufficient details), raising concerns regarding the\nreproducibility of published results. In order to address these shortcomings,\nhere we present the Standardized Project Gutenberg Corpus (SPGC), an open\nscience approach to a curated version of the complete PG data containing more\nthan 50,000 books and more than $3 \\times 10^9$ word-tokens. Using different\nsources of annotated metadata, we not only provide a broad characterization of\nthe content of PG, but also show different examples highlighting the potential\nof SPGC for investigating language variability across time, subjects, and\nauthors. We publish our methodology in detail, the code to download and process\nthe data, as well as the obtained corpus itself on 3 different levels of\ngranularity (raw text, timeseries of word tokens, and counts of words). In this\nway, we provide a reproducible, pre-processed, full-size version of Project\nGutenberg as a new scientific resource for corpus linguistics, natural language\nprocessing, and information retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:10:14 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Gerlach", "Martin", ""], ["Font-Clos", "Francesc", ""]]}, {"id": "1812.08126", "submitter": "Annika Lindh", "authors": "Annika Lindh, Robert J. Ross, Abhijit Mahalunkar, Giancarlo Salton,\n  John D. Kelleher", "title": "Generating Diverse and Meaningful Captions", "comments": "Accepted for presentation at The 27th International Conference on\n  Artificial Neural Networks (ICANN 2018)", "journal-ref": "Artificial Neural Networks and Machine Learning - ICANN 2018 (pp.\n  176-187). Springer International Publishing", "doi": "10.1007/978-3-030-01418-6_18", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Captioning is a task that requires models to acquire a multi-modal\nunderstanding of the world and to express this understanding in natural\nlanguage text. While the state-of-the-art for this task has rapidly improved in\nterms of n-gram metrics, these models tend to output the same generic captions\nfor similar images. In this work, we address this limitation and train a model\nthat generates more diverse and specific captions through an unsupervised\ntraining approach that incorporates a learning signal from an Image Retrieval\nmodel. We summarize previous results and improve the state-of-the-art on\ncaption diversity and novelty. We make our source code publicly available\nonline.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 18:10:18 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Lindh", "Annika", ""], ["Ross", "Robert J.", ""], ["Mahalunkar", "Abhijit", ""], ["Salton", "Giancarlo", ""], ["Kelleher", "John D.", ""]]}, {"id": "1812.08304", "submitter": "Hamed Jelodar", "authors": "Hamed Jelodar, Yongli Wang, Mahdi Rabbani, Ru-xin Zhao, Seyedvalyallah\n  Ayobi, Peng Hu and Isma Masood", "title": "Recommendation System based on Semantic Scholar Mining and Topic\n  modeling: A behavioral analysis of researchers from six conferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have an important place to help online users in the\ninternet society. Recommendation Systems in computer science are of very\npractical use these days in various aspects of the Internet portals, such as\nsocial networks, and library websites. There are several approaches to\nimplement recommendation systems, Latent Dirichlet Allocation (LDA) is one the\npopular techniques in Topic Modeling. Recently, researchers have proposed many\napproaches based on Recommendation Systems and LDA. According to importance of\nthe subject, in this paper we discover the trends of the topics and find\nrelationship between LDA topics and Scholar-Context-documents. In fact, We\napply probabilistic topic modeling based on Gibbs sampling algorithms for a\nsemantic mining from six conference publications in computer science from DBLP\ndataset. According to our experimental results, our semantic framework can be\neffective to help organizations to better organize these conferences and cover\nfuture research topics.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 01:07:04 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Jelodar", "Hamed", ""], ["Wang", "Yongli", ""], ["Rabbani", "Mahdi", ""], ["Zhao", "Ru-xin", ""], ["Ayobi", "Seyedvalyallah", ""], ["Hu", "Peng", ""], ["Masood", "Isma", ""]]}, {"id": "1812.08318", "submitter": "Hareesh Bahuleyan", "authors": "Olga Vechtomova, Hareesh Bahuleyan, Amirpasha Ghabussi, Vineet John", "title": "Generating lyrics with variational autoencoder and multi-modal artist\n  embeddings", "comments": "5 pages, 5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for generating song lyrics lines conditioned on the style\nof a specified artist. The system uses a variational autoencoder with artist\nembeddings. We propose the pre-training of artist embeddings with the\nrepresentations learned by a CNN classifier, which is trained to predict\nartists based on MEL spectrograms of their song clips. This work is the first\nstep towards combining audio and text modalities of songs for generating lyrics\nconditioned on the artist's style. Our preliminary results suggest that there\nis a benefit in initializing artists' embeddings with the representations\nlearned by a spectrogram classifier.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 02:25:45 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Vechtomova", "Olga", ""], ["Bahuleyan", "Hareesh", ""], ["Ghabussi", "Amirpasha", ""], ["John", "Vineet", ""]]}, {"id": "1812.08407", "submitter": "Shachi Hullumane Kumar", "authors": "Shachi H Kumar, Eda Okur, Saurav Sahay, Juan Jose Alvarado Leanos,\n  Jonathan Huang, Lama Nachman", "title": "Context, Attention and Audio Feature Explorations for Audio Visual\n  Scene-Aware Dialog", "comments": "7 pages, 2 figures, DSTC7 workshop at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advancements in AI, Intelligent Virtual Assistants (IVA) have\nbecome a ubiquitous part of every home. Going forward, we are witnessing a\nconfluence of vision, speech and dialog system technologies that are enabling\nthe IVAs to learn audio-visual groundings of utterances and have conversations\nwith users about the objects, activities and events surrounding them. As a part\nof the 7th Dialog System Technology Challenges (DSTC7), for Audio Visual\nScene-Aware Dialog (AVSD) track, We explore `topics' of the dialog as an\nimportant contextual feature into the architecture along with explorations\naround multimodal Attention. We also incorporate an end-to-end audio\nclassification ConvNet, AclNet, into our models. We present detailed analysis\nof the experiments and show that some of our model variations outperform the\nbaseline system presented for this task.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 08:05:54 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Kumar", "Shachi H", ""], ["Okur", "Eda", ""], ["Sahay", "Saurav", ""], ["Leanos", "Juan Jose Alvarado", ""], ["Huang", "Jonathan", ""], ["Nachman", "Lama", ""]]}, {"id": "1812.08425", "submitter": "Denys Katerenchuk", "authors": "Denys Katerenchuk", "title": "A Survey of Hierarchy Identification in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans are social by nature. Throughout history, people have formed\ncommunities and built relationships. Most relationships with coworkers,\nfriends, and family are developed during face-to-face interactions. These\nrelationships are established through explicit means of communications such as\nwords and implicit such as intonation, body language, etc. By analyzing human\ninteractions we can derive information about the relationships and influence\namong conversation participants. However, with the development of the Internet,\npeople started to communicate through text in online social networks.\nInterestingly, they brought their communicational habits to the Internet. Many\nsocial network users form relationships with each other and establish\ncommunities with leaders and followers. Recognizing these hierarchical\nrelationships is an important task because it will help to understand social\nnetworks and predict future trends, improve recommendations, better target\nadvertisement, and improve national security by identifying leaders of\nanonymous terror groups. In this work, I provide an overview of current\nresearch in this area and present the state-of-the-art approaches to deal with\nthe problem of identifying hierarchical relationships in social networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 08:56:52 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Katerenchuk", "Denys", ""]]}, {"id": "1812.08621", "submitter": "Miguel Domingo", "authors": "Miguel Domingo and Mercedes Garc{\\i}a-Mart{\\i}nez and Alexandre Helle\n  and Francisco Casacuberta and Manuel Herranz", "title": "How Much Does Tokenization Affect Neural Machine Translation?", "comments": "Accepted for publication at CICLing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tokenization or segmentation is a wide concept that covers simple processes\nsuch as separating punctuation from words, or more sophisticated processes such\nas applying morphological knowledge. Neural Machine Translation (NMT) requires\na limited-size vocabulary for computational cost and enough examples to\nestimate word embeddings. Separating punctuation and splitting tokens into\nwords or subwords has proven to be helpful to reduce vocabulary and increase\nthe number of examples of each word, improving the translation quality.\nTokenization is more challenging when dealing with languages with no separator\nbetween words. In order to assess the impact of the tokenization in the quality\nof the final translation on NMT, we experimented on five tokenizers over ten\nlanguage pairs. We reached the conclusion that the tokenization significantly\naffects the final translation quality and that the best tokenizer differs for\ndifferent language pairs.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 15:02:39 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 17:42:08 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 11:43:53 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 13:27:37 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Domingo", "Miguel", ""], ["Garc\u0131a-Mart\u0131nez", "Mercedes", ""], ["Helle", "Alexandre", ""], ["Casacuberta", "Francisco", ""], ["Herranz", "Manuel", ""]]}, {"id": "1812.08658", "submitter": "Harsh Agrawal", "authors": "Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain,\n  Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson", "title": "nocaps: novel object captioning at scale", "comments": null, "journal-ref": "IEEE International Conference on Computer Vision (ICCV) 2019", "doi": "10.1109/ICCV.2019.00904", "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning models have achieved impressive results on datasets\ncontaining limited visual concepts and large amounts of paired image-caption\ntraining data. However, if these models are to ever function in the wild, a\nmuch larger variety of visual concepts must be learned, ideally from less\nsupervision. To encourage the development of image captioning models that can\nlearn visual concepts from alternative data sources, such as object detection\ndatasets, we present the first large-scale benchmark for this task. Dubbed\n'nocaps', for novel object captioning at scale, our benchmark consists of\n166,100 human-generated captions describing 15,100 images from the OpenImages\nvalidation and test sets. The associated training data consists of COCO\nimage-caption pairs, plus OpenImages image-level labels and object bounding\nboxes. Since OpenImages contains many more classes than COCO, nearly 400 object\nclasses seen in test images have no or very few associated training captions\n(hence, nocaps). We extend existing novel object captioning models to establish\nstrong baselines for this benchmark and provide analysis to guide future work\non this task.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 16:04:05 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 23:13:30 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 20:10:33 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Agrawal", "Harsh", ""], ["Desai", "Karan", ""], ["Wang", "Yufei", ""], ["Chen", "Xinlei", ""], ["Jain", "Rishabh", ""], ["Johnson", "Mark", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Lee", "Stefan", ""], ["Anderson", "Peter", ""]]}, {"id": "1812.08718", "submitter": "Tom McCoy", "authors": "R. Thomas McCoy, Tal Linzen, Ewan Dunbar, Paul Smolensky", "title": "RNNs Implicitly Implement Tensor Product Representations", "comments": "Accepted to ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) can learn continuous vector representations\nof symbolic structures such as sequences and sentences; these representations\noften exhibit linear regularities (analogies). Such regularities motivate our\nhypothesis that RNNs that show such regularities implicitly compile symbolic\nstructures into tensor product representations (TPRs; Smolensky, 1990), which\nadditively combine tensor products of vectors representing roles (e.g.,\nsequence positions) and vectors representing fillers (e.g., particular words).\nTo test this hypothesis, we introduce Tensor Product Decomposition Networks\n(TPDNs), which use TPRs to approximate existing vector representations. We\ndemonstrate using synthetic data that TPDNs can successfully approximate linear\nand tree-based RNN autoencoder representations, suggesting that these\nrepresentations exhibit interpretable compositional structure; we explore the\nsettings that lead RNNs to induce such structure-sensitive representations. By\ncontrast, further TPDN experiments show that the representations of four models\ntrained to encode naturally-occurring sentences can be largely approximated\nwith a bag of words, with only marginal improvements from more sophisticated\nstructures. We conclude that TPDNs provide a powerful method for interpreting\nvector representations, and that standard RNNs can induce compositional\nsequence representations that are remarkably well approximated by TPRs; at the\nsame time, existing training tasks for sentence representation learning may not\nbe sufficient for inducing robust structural representations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 17:44:05 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 16:00:06 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["McCoy", "R. Thomas", ""], ["Linzen", "Tal", ""], ["Dunbar", "Ewan", ""], ["Smolensky", "Paul", ""]]}, {"id": "1812.08729", "submitter": "Ahmed Aly", "authors": "Ahmed Aly, Kushal Lakhotia, Shicong Zhao, Mrinal Mohit, Barlas Oguz,\n  Abhinav Arora, Sonal Gupta, Christopher Dewan, Stef Nelson-Lindall, Rushin\n  Shah", "title": "PyText: A Seamless Path from NLP research to production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PyText - a deep learning based NLP modeling framework built on\nPyTorch. PyText addresses the often-conflicting requirements of enabling rapid\nexperimentation and of serving models at scale. It achieves this by providing\nsimple and extensible interfaces for model components, and by using PyTorch's\ncapabilities of exporting models for inference via the optimized Caffe2\nexecution engine. We report our own experience of migrating experimentation and\nproduction workflows to PyText, which enabled us to iterate faster on novel\nmodeling ideas and then seamlessly ship them at industrial scale.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 23:06:43 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Aly", "Ahmed", ""], ["Lakhotia", "Kushal", ""], ["Zhao", "Shicong", ""], ["Mohit", "Mrinal", ""], ["Oguz", "Barlas", ""], ["Arora", "Abhinav", ""], ["Gupta", "Sonal", ""], ["Dewan", "Christopher", ""], ["Nelson-Lindall", "Stef", ""], ["Shah", "Rushin", ""]]}, {"id": "1812.08769", "submitter": "Maria De-Arteaga", "authors": "Nathaniel Swinger, Maria De-Arteaga, Neil Thomas Heffernan IV, Mark DM\n  Leiserson, Adam Tauman Kalai", "title": "What are the biases in my word embedding?", "comments": "At AIES 2019: the AAAI/ACM Conference on Artificial Intelligence,\n  Ethics, and Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for enumerating biases in word embeddings.\nThe algorithm exposes a large number of offensive associations related to\nsensitive features such as race and gender on publicly available embeddings,\nincluding a supposedly \"debiased\" embedding. These biases are concerning in\nlight of the widespread use of word embeddings. The associations are identified\nby geometric patterns in word embeddings that run parallel between people's\nnames and common lower-case tokens. The algorithm is highly unsupervised: it\ndoes not even require the sensitive features to be pre-specified. This is\ndesirable because: (a) many forms of discrimination--such as racial\ndiscrimination--are linked to social constructs that may vary depending on the\ncontext, rather than to categories with fixed definitions; and (b) it makes it\neasier to identify biases against intersectional groups, which depend on\ncombinations of sensitive features. The inputs to our algorithm are a list of\ntarget tokens, e.g. names, and a word embedding. It outputs a number of Word\nEmbedding Association Tests (WEATs) that capture various biases present in the\ndata. We illustrate the utility of our approach on publicly available word\nembeddings and lists of names, and evaluate its output using crowdsourcing. We\nalso show how removing names may not remove potential proxy bias.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 18:53:05 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 07:58:38 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 18:55:53 GMT"}, {"version": "v4", "created": "Wed, 19 Jun 2019 19:52:15 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Swinger", "Nathaniel", ""], ["De-Arteaga", "Maria", ""], ["Heffernan", "Neil Thomas", "IV"], ["Leiserson", "Mark DM", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1812.08879", "submitter": "Bo-Hsiang (Andy) Tseng", "authors": "Bo-Hsiang Tseng, Florian Kreyssig, Pawel Budzianowski, Inigo\n  Casanueva, Yen-Chen Wu, Stefan Ultes, Milica Gasic", "title": "Variational Cross-domain Natural Language Generation for Spoken Dialogue\n  Systems", "comments": "Sigdial 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain natural language generation (NLG) is still a difficult task\nwithin spoken dialogue modelling. Given a semantic representation provided by\nthe dialogue manager, the language generator should generate sentences that\nconvey desired information. Traditional template-based generators can produce\nsentences with all necessary information, but these sentences are not\nsufficiently diverse. With RNN-based models, the diversity of the generated\nsentences can be high, however, in the process some information is lost. In\nthis work, we improve an RNN-based generator by considering latent information\nat the sentence level during generation using the conditional variational\nautoencoder architecture. We demonstrate that our model outperforms the\noriginal RNN-based generator, while yielding highly diverse sentences. In\naddition, our model performs better when the training data is limited.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 22:53:33 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Tseng", "Bo-Hsiang", ""], ["Kreyssig", "Florian", ""], ["Budzianowski", "Pawel", ""], ["Casanueva", "Inigo", ""], ["Wu", "Yen-Chen", ""], ["Ultes", "Stefan", ""], ["Gasic", "Milica", ""]]}, {"id": "1812.08947", "submitter": "Chuan Qin", "authors": "Chuan Qin, Hengshu Zhu, Tong Xu, Chen Zhu, Liang Jiang, Enhong Chen,\n  Hui Xiong", "title": "Enhancing Person-Job Fit for Talent Recruitment: An Ability-aware Neural\n  Network Approach", "comments": "This is an extended version of our SIGIR18 paper", "journal-ref": null, "doi": "10.1145/3209978.3210025", "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide spread use of online recruitment services has led to information\nexplosion in the job market. As a result, the recruiters have to seek the\nintelligent ways for Person Job Fit, which is the bridge for adapting the right\njob seekers to the right positions. Existing studies on Person Job Fit have a\nfocus on measuring the matching degree between the talent qualification and the\njob requirements mainly based on the manual inspection of human resource\nexperts despite of the subjective, incomplete, and inefficient nature of the\nhuman judgement. To this end, in this paper, we propose a novel end to end\nAbility aware Person Job Fit Neural Network model, which has a goal of reducing\nthe dependence on manual labour and can provide better interpretation about the\nfitting results. The key idea is to exploit the rich information available at\nabundant historical job application data. Specifically, we propose a word level\nsemantic representation for both job requirements and job seekers' experiences\nbased on Recurrent Neural Network. Along this line, four hierarchical ability\naware attention strategies are designed to measure the different importance of\njob requirements for semantic representation, as well as measuring the\ndifferent contribution of each job experience to a specific ability\nrequirement. Finally, extensive experiments on a large scale real world data\nset clearly validate the effectiveness and interpretability of the APJFNN\nframework compared with several baselines.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 05:02:02 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Qin", "Chuan", ""], ["Zhu", "Hengshu", ""], ["Xu", "Tong", ""], ["Zhu", "Chen", ""], ["Jiang", "Liang", ""], ["Chen", "Enhong", ""], ["Xiong", "Hui", ""]]}, {"id": "1812.08951", "submitter": "Yonatan Belinkov", "authors": "Yonatan Belinkov, James Glass", "title": "Analysis Methods in Neural Language Processing: A Survey", "comments": "Version including the supplementary materials (3 tables), also\n  available at https://boknilev.github.io/nlp-analysis-methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of natural language processing has seen impressive progress in\nrecent years, with neural network models replacing many of the traditional\nsystems. A plethora of new models have been proposed, many of which are thought\nto be opaque compared to their feature-rich counterparts. This has led\nresearchers to analyze, interpret, and evaluate neural networks in novel and\nmore fine-grained ways. In this survey paper, we review analysis methods in\nneural language processing, categorize them according to prominent research\ntrends, highlight existing limitations, and point to potential directions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 05:13:03 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 16:38:50 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Belinkov", "Yonatan", ""], ["Glass", "James", ""]]}, {"id": "1812.08989", "submitter": "Jianfeng Gao", "authors": "Li Zhou, Jianfeng Gao, Di Li, Heung-Yeung Shum", "title": "The Design and Implementation of XiaoIce, an Empathetic Social Chatbot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the development of Microsoft XiaoIce, the most popular\nsocial chatbot in the world. XiaoIce is uniquely designed as an AI companion\nwith an emotional connection to satisfy the human need for communication,\naffection, and social belonging. We take into account both intelligent quotient\n(IQ) and emotional quotient (EQ) in system design, cast human-machine social\nchat as decision-making over Markov Decision Processes (MDPs), and optimize\nXiaoIce for long-term user engagement, measured in expected Conversation-turns\nPer Session (CPS). We detail the system architecture and key components\nincluding dialogue manager, core chat, skills, and an empathetic computing\nmodule. We show how XiaoIce dynamically recognizes human feelings and states,\nunderstands user intent, and responds to user needs throughout long\nconversations. Since her launch in 2014, XiaoIce has communicated with over 660\nmillion active users and succeeded in establishing long-term relationships with\nmany of them. Analysis of large scale online logs shows that XiaoIce has\nachieved an average CPS of 23, which is significantly higher than that of other\nchatbots and even human conversations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 08:01:31 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 21:23:51 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zhou", "Li", ""], ["Gao", "Jianfeng", ""], ["Li", "Di", ""], ["Shum", "Heung-Yeung", ""]]}, {"id": "1812.09193", "submitter": "Gabriel Marzinotto", "authors": "Gabriel Marzinotto (TALEP), Fr\\'ed\\'eric B\\'echet (LIS, TALEP),\n  G\\'eraldine Damnati, Alexis Nasr (LIS, TALEP)", "title": "Sources of Complexity in Semantic Frame Parsing for Information\n  Extraction", "comments": null, "journal-ref": "International FrameNet Workshop 2018, May 2018, Miyazaki, Japan", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a Semantic Frame parsing System based on sequence\nlabeling methods, precisely BiLSTM models with highway connections, for\nperforming information extraction on a corpus of French encyclopedic history\ntexts annotated according to the Berkeley FrameNet formalism. The approach\nproposed in this study relies on an integrated sequence labeling model which\njointly optimizes frame identification and semantic role segmentation and\nidentification. The purpose of this study is to analyze the task complexity, to\nhighlight the factors that make Semantic Frame parsing a difficult task and to\nprovide detailed evaluations of the performance on different types of frames\nand sentences.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 15:30:17 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Marzinotto", "Gabriel", "", "TALEP"], ["B\u00e9chet", "Fr\u00e9d\u00e9ric", "", "LIS, TALEP"], ["Damnati", "G\u00e9raldine", "", "LIS, TALEP"], ["Nasr", "Alexis", "", "LIS, TALEP"]]}, {"id": "1812.09195", "submitter": "Izzeddin Gur", "authors": "Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek\n  Hakkani-Tur", "title": "Learning to Navigate the Web", "comments": "International Conference on Learning Representations (ICLR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in environments with large state and action spaces, and sparse\nrewards, can hinder a Reinforcement Learning (RL) agent's learning through\ntrial-and-error. For instance, following natural language instructions on the\nWeb (such as booking a flight ticket) leads to RL settings where input\nvocabulary and number of actionable elements on a page can grow very large.\nEven though recent approaches improve the success rate on relatively simple\nenvironments with the help of human demonstrations to guide the exploration,\nthey still fail in environments where the set of possible instructions can\nreach millions. We approach the aforementioned problems from a different\nperspective and propose guided RL approaches that can generate unbounded amount\nof experience for an agent to learn from. Instead of learning from a\ncomplicated instruction with a large vocabulary, we decompose it into multiple\nsub-instructions and schedule a curriculum in which an agent is tasked with a\ngradually increasing subset of these relatively easier sub-instructions. In\naddition, when the expert demonstrations are not available, we propose a novel\nmeta-learning framework that generates new instruction following tasks and\ntrains the agent more effectively. We train DQN, deep reinforcement learning\nagent, with Q-value function approximated with a novel QWeb neural network\narchitecture on these smaller, synthetic instructions. We evaluate the ability\nof our agent to generalize to new instructions on World of Bits benchmark, on\nforms with up to 100 elements, supporting 14 million possible instructions. The\nQWeb agent outperforms the baseline without using any human demonstration\nachieving 100% success rate on several difficult environments.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 15:32:59 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Gur", "Izzeddin", ""], ["Rueckert", "Ulrich", ""], ["Faust", "Aleksandra", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "1812.09244", "submitter": "Grzegorz Chrupa{\\l}a", "authors": "Grzegorz Chrupa{\\l}a", "title": "Symbolic inductive bias for visually grounded learning of spoken\n  language", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A widespread approach to processing spoken language is to first automatically\ntranscribe it into text. An alternative is to use an end-to-end approach:\nrecent works have proposed to learn semantic embeddings of spoken language from\nimages with spoken captions, without an intermediate transcription step. We\npropose to use multitask learning to exploit existing transcribed speech within\nthe end-to-end setting. We describe a three-task architecture which combines\nthe objectives of matching spoken captions with corresponding images, speech\nwith text, and text with images. We show that the addition of the speech/text\ntask leads to substantial performance improvements on image retrieval when\ncompared to training the speech/image task in isolation. We conjecture that\nthis is due to a strong inductive bias transcribed speech provides to the\nmodel, and offer supporting evidence for this.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 16:37:29 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 11:49:19 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 14:08:08 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "1812.09321", "submitter": "Xavier Bost", "authors": "Xavier Bost (LIA), Marc El B\\`eze (LIA), Renato De Mori (LIA)", "title": "Multiple topic identification in telephone conversations", "comments": "arXiv admin note: text overlap with arXiv:1812.07207", "journal-ref": "Interspeech, Aug 2013, Lyon, France", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the automatic analysis of conversations between a\ncustomer and an agent in a call centre of a customer care service. The purpose\nof the analysis is to hypothesize themes about problems and complaints\ndiscussed in the conversation. Themes are defined by the application\ndocumentation topics. A conversation may contain mentions that are irrelevant\nfor the application purpose and multiple themes whose mentions may be\ninterleaved portions of a conversation that cannot be well defined. Two methods\nare proposed for multiple theme hypothesization. One of them is based on a\ncosine similarity measure using a bag of features extracted from the entire\nconversation. The other method introduces the concept of thematic density\ndistributed around specific word positions in a conversation. In addition to\nautomatically selected words, word bi-grams with possible gaps between\nsuccessive words are also considered and selected. Experimental results show\nthat the results obtained with the proposed methods outperform the results\nobtained with support vector machines on the same data. Furthermore, using the\ntheme skeleton of a conversation from which thematic densities are derived, it\nwill be possible to extract components of an automatic conversation report to\nbe used for improving the service performance. Index Terms: multi-topic audio\ndocument classification, hu-man/human conversation analysis, speech analytics,\ndistance bigrams\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 15:31:17 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 15:07:07 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["B\u00e8ze", "Marc El", "", "LIA"], ["De Mori", "Renato", "", "LIA"]]}, {"id": "1812.09323", "submitter": "Jianshu Chen", "authors": "Chih-Kuan Yeh, Jianshu Chen, Chengzhu Yu, Dong Yu", "title": "Unsupervised Speech Recognition via Segmental Empirical Output\n  Distribution Matching", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training speech recognition systems without using\nany labeled data, under the assumption that the learner can only access to the\ninput utterances and a phoneme language model estimated from a non-overlapping\ncorpus. We propose a fully unsupervised learning algorithm that alternates\nbetween solving two sub-problems: (i) learn a phoneme classifier for a given\nset of phoneme segmentation boundaries, and (ii) refining the phoneme\nboundaries based on a given classifier. To solve the first sub-problem, we\nintroduce a novel unsupervised cost function named Segmental Empirical Output\nDistribution Matching, which generalizes the work in (Liu et al., 2017) to\nsegmental structures. For the second sub-problem, we develop an approximate MAP\napproach to refining the boundaries obtained from Wang et al. (2017).\nExperimental results on TIMIT dataset demonstrate the success of this fully\nunsupervised phoneme recognition system, which achieves a phone error rate\n(PER) of 41.6%. Although it is still far away from the state-of-the-art\nsupervised systems, we show that with oracle boundaries and matching language\nmodel, the PER could be improved to 32.5%.This performance approaches the\nsupervised system of the same model architecture, demonstrating the great\npotential of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 01:58:39 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Yeh", "Chih-Kuan", ""], ["Chen", "Jianshu", ""], ["Yu", "Chengzhu", ""], ["Yu", "Dong", ""]]}, {"id": "1812.09336", "submitter": "Yihui He", "authors": "Devesh Walawalkar, Yihui He, Rohit Pillai", "title": "An Empirical Analysis of Deep Audio-Visual Models for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we worked on speech recognition, specifically predicting\nindividual words based on both the video frames and audio. Empowered by\nconvolutional neural networks, the recent speech recognition and lip reading\nmodels are comparable to human level performance. We re-implemented and made\nderivations of the state-of-the-art model. Then, we conducted rich experiments\nincluding the effectiveness of attention mechanism, more accurate residual\nnetwork as the backbone with pre-trained weights and the sensitivity of our\nmodel with respect to audio input with/without noise.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 19:02:52 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Walawalkar", "Devesh", ""], ["He", "Yihui", ""], ["Pillai", "Rohit", ""]]}, {"id": "1812.09355", "submitter": "Hassan Sajjad", "authors": "Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony\n  Bau, James Glass", "title": "What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in\n  Deep NLP Models", "comments": "AAA 2019, pages 10, AAAI Conference on Artificial Intelligence (AAAI\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable evolution of deep neural networks in natural language\nprocessing (NLP), their interpretability remains a challenge. Previous work\nlargely focused on what these models learn at the representation level. We\nbreak this analysis down further and study individual dimensions (neurons) in\nthe vector representation learned by end-to-end neural models in NLP tasks. We\npropose two methods: Linguistic Correlation Analysis, based on a supervised\nmethod to extract the most relevant neurons with respect to an extrinsic task,\nand Cross-model Correlation Analysis, an unsupervised method to extract salient\nneurons w.r.t. the model itself. We evaluate the effectiveness of our\ntechniques by ablating the identified neurons and reevaluating the network's\nperformance for two tasks: neural machine translation (NMT) and neural language\nmodeling (NLM). We further present a comprehensive analysis of neurons with the\naim to address the following questions: i) how localized or distributed are\ndifferent linguistic properties in the models? ii) are certain neurons\nexclusive to some properties and not others? iii) is the information more or\nless distributed in NMT vs. NLM? and iv) how important are the neurons\nidentified through the linguistic correlation method to the overall task? Our\ncode is publicly available as part of the NeuroX toolkit (Dalvi et al. 2019).\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 19:51:47 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Dalvi", "Fahim", ""], ["Durrani", "Nadir", ""], ["Sajjad", "Hassan", ""], ["Belinkov", "Yonatan", ""], ["Bau", "Anthony", ""], ["Glass", "James", ""]]}, {"id": "1812.09359", "submitter": "Hassan Sajjad", "authors": "Fahim Dalvi and Avery Nortonsmith and D. Anthony Bau and Yonatan\n  Belinkov and Hassan Sajjad and Nadir Durrani and James Glass", "title": "NeuroX: A Toolkit for Analyzing Individual Neurons in Neural Networks", "comments": "AAAI Conference on Artificial Intelligence (AAAI 2019), Demonstration\n  track, pages 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a toolkit to facilitate the interpretation and understanding of\nneural network models. The toolkit provides several methods to identify salient\nneurons with respect to the model itself or an external task. A user can\nvisualize selected neurons, ablate them to measure their effect on the model\naccuracy, and manipulate them to control the behavior of the model at the test\ntime. Such an analysis has a potential to serve as a springboard in various\nresearch directions, such as understanding the model, better architectural\nchoices, model distillation and controlling data biases.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 20:20:26 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Dalvi", "Fahim", ""], ["Nortonsmith", "Avery", ""], ["Bau", "D. Anthony", ""], ["Belinkov", "Yonatan", ""], ["Sajjad", "Hassan", ""], ["Durrani", "Nadir", ""], ["Glass", "James", ""]]}, {"id": "1812.09449", "submitter": "Jing Li", "authors": "Jing Li, Aixin Sun, Jianglei Han and Chenliang Li", "title": "A Survey on Deep Learning for Named Entity Recognition", "comments": "20 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is the task to identify mentions of rigid\ndesignators from text belonging to predefined semantic types such as person,\nlocation, organization etc. NER always serves as the foundation for many\nnatural language applications such as question answering, text summarization,\nand machine translation. Early NER systems got a huge success in achieving good\nperformance with the cost of human engineering in designing domain-specific\nfeatures and rules. In recent years, deep learning, empowered by continuous\nreal-valued vector representations and semantic composition through nonlinear\nprocessing, has been employed in NER systems, yielding stat-of-the-art\nperformance. In this paper, we provide a comprehensive review on existing deep\nlearning techniques for NER. We first introduce NER resources, including tagged\nNER corpora and off-the-shelf NER tools. Then, we systematically categorize\nexisting works based on a taxonomy along three axes: distributed\nrepresentations for input, context encoder, and tag decoder. Next, we survey\nthe most representative methods for recent applied techniques of deep learning\nin new NER problem settings and applications. Finally, we present readers with\nthe challenges faced by NER systems and outline future directions in this area.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 04:54:13 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 14:15:55 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 15:57:10 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Li", "Jing", ""], ["Sun", "Aixin", ""], ["Han", "Jianglei", ""], ["Li", "Chenliang", ""]]}, {"id": "1812.09471", "submitter": "Chenwei Zhang", "authors": "Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, Philip S. Yu", "title": "Joint Slot Filling and Intent Detection via Capsule Neural Networks", "comments": "In ACL 2019 as a long paper. Code and data available at\n  https://github.com/czhang99/Capsule-NLU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to recognize words as slots and detect the intent of an utterance\nhas been a keen issue in natural language understanding. The existing works\neither treat slot filling and intent detection separately in a pipeline manner,\nor adopt joint models which sequentially label slots while summarizing the\nutterance-level intent without explicitly preserving the hierarchical\nrelationship among words, slots, and intents. To exploit the semantic hierarchy\nfor effective modeling, we propose a capsule-based neural network model which\naccomplishes slot filling and intent detection via a dynamic\nrouting-by-agreement schema. A re-routing schema is proposed to further\nsynergize the slot filling performance using the inferred intent\nrepresentation. Experiments on two real-world datasets show the effectiveness\nof our model when compared with other alternative model architectures, as well\nas existing natural language understanding services.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 07:49:42 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 02:47:28 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhang", "Chenwei", ""], ["Li", "Yaliang", ""], ["Du", "Nan", ""], ["Fan", "Wei", ""], ["Yu", "Philip S.", ""]]}, {"id": "1812.09516", "submitter": "Changsen Yuan", "authors": "Changsen Yuan, Heyan Huang, Chong Feng, Xiao Liu, Xiaochi Wei", "title": "Distant Supervision for Relation Extraction with Linear Attenuation\n  Simulation and Non-IID Relevance Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision for relation extraction is an efficient method to reduce\nlabor costs and has been widely used to seek novel relational facts in large\ncorpora, which can be identified as a multi-instance multi-label problem.\nHowever, existing distant supervision methods suffer from selecting important\nwords in the sentence and extracting valid sentences in the bag. Towards this\nend, we propose a novel approach to address these problems in this paper.\nFirstly, we propose a linear attenuation simulation to reflect the importance\nof words in the sentence with respect to the distances between entities and\nwords. Secondly, we propose a non-independent and identically distributed\n(non-IID) relevance embedding to capture the relevance of sentences in the bag.\nOur method can not only capture complex information of words about hidden\nrelations, but also express the mutual information of instances in the bag.\nExtensive experiments on a benchmark dataset have well-validated the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 12:04:17 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Yuan", "Changsen", ""], ["Huang", "Heyan", ""], ["Feng", "Chong", ""], ["Liu", "Xiao", ""], ["Wei", "Xiaochi", ""]]}, {"id": "1812.09617", "submitter": "Mozhi Zhang", "authors": "Mozhi Zhang, Yoshinari Fujinuma, Jordan Boyd-Graber", "title": "Exploiting Cross-Lingual Subword Similarities in Low-Resource Document\n  Classification", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification must sometimes be applied in a low-resource language with\nno labeled training data. However, training data may be available in a related\nlanguage. We investigate whether character-level knowledge transfer from a\nrelated language helps text classification. We present a cross-lingual document\nclassification framework (CACO) that exploits cross-lingual subword similarity\nby jointly training a character-based embedder and a word-based classifier. The\nembedder derives vector representations for input words from their written\nforms, and the classifier makes predictions based on the word vectors. We use a\njoint character representation for both the source language and the target\nlanguage, which allows the embedder to generalize knowledge about source\nlanguage words to target language words with similar forms. We propose a\nmulti-task objective that can further improve the model if additional\ncross-lingual or monolingual resources are available. Experiments confirm that\ncharacter-level knowledge transfer is more data-efficient than word-level\ntransfer between related languages.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 22:53:19 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 00:28:09 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 18:57:21 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2020 07:11:34 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Zhang", "Mozhi", ""], ["Fujinuma", "Yoshinari", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "1812.09650", "submitter": "Nikhil Marda", "authors": "Peter Hansel, Nik Marda, William Yin", "title": "Improving Context-Aware Semantic Relationships in Sparse Mobile Datasets", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional semantic similarity models often fail to encapsulate the external\ncontext in which texts are situated. However, textual datasets generated on\nmobile platforms can help us build a truer representation of semantic\nsimilarity by introducing multimodal data. This is especially important in\nsparse datasets, making solely text-driven interpretation of context more\ndifficult. In this paper, we develop new algorithms for building external\nfeatures into sentence embeddings and semantic similarity scores. Then, we test\nthem on embedding spaces on data from Twitter, using each tweet's time and\ngeolocation to better understand its context. Ultimately, we show that applying\nPCA with eight components to the embedding space and appending multimodal\nfeatures yields the best outcomes. This yields a considerable improvement over\npure text-based approaches for discovering similar tweets. Our results suggest\nthat our new algorithm can help improve semantic understanding in various\nsettings.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 03:35:56 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Hansel", "Peter", ""], ["Marda", "Nik", ""], ["Yin", "William", ""]]}, {"id": "1812.09652", "submitter": "Lannan Luo", "authors": "Kimberly Redmond, Lannan Luo, Qiang Zeng", "title": "A Cross-Architecture Instruction Embedding Model for Natural Language\n  Processing-Inspired Binary Code Analysis", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a closed-source program, such as most of proprietary software and\nviruses, binary code analysis is indispensable for many tasks, such as code\nplagiarism detection and malware analysis. Today, source code is very often\ncompiled for various architectures, making cross-architecture binary code\nanalysis increasingly important. A binary, after being disassembled, is\nexpressed in an assembly languages. Thus, recent work starts exploring Natural\nLanguage Processing (NLP) inspired binary code analysis. In NLP, words are\nusually represented in high-dimensional vectors (i.e., embeddings) to\nfacilitate further processing, which is one of the most common and critical\nsteps in many NLP tasks. We regard instructions as words in NLP-inspired binary\ncode analysis, and aim to represent instructions as embeddings as well.\n  To facilitate cross-architecture binary code analysis, our goal is that\nsimilar instructions, regardless of their architectures, have embeddings close\nto each other. To this end, we propose a joint learning approach to generating\ninstruction embeddings that capture not only the semantics of instructions\nwithin an architecture, but also their semantic relationships across\narchitectures. To the best of our knowledge, this is the first work on building\ncross-architecture instruction embedding model. As a showcase, we apply the\nmodel to resolving one of the most fundamental problems for binary code\nsimilarity comparison---semantics-based basic block comparison, and the\nsolution outperforms the code statistics based approach. It demonstrates that\nit is promising to apply the model to other cross-architecture binary code\nanalysis tasks.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 03:44:03 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Redmond", "Kimberly", ""], ["Luo", "Lannan", ""], ["Zeng", "Qiang", ""]]}, {"id": "1812.09653", "submitter": "Achyudh Ram", "authors": "Achyudh Ram, Meiyappan Nagappan", "title": "Supervised Sentiment Classification with CNNs for Diverse SE Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis, a popular technique for opinion mining, has been used by\nthe software engineering research community for tasks such as assessing app\nreviews, developer emotions in issue trackers and developer opinions on APIs.\nPast research indicates that state-of-the-art sentiment analysis techniques\nhave poor performance on SE data. This is because sentiment analysis tools are\noften designed to work on non-technical documents such as movie reviews. In\nthis study, we attempt to solve the issues with existing sentiment analysis\ntechniques for SE texts by proposing a hierarchical model based on\nconvolutional neural networks (CNN) and long short-term memory (LSTM) trained\non top of pre-trained word vectors. We assessed our model's performance and\nreliability by comparing it with a number of frequently used sentiment analysis\ntools on five gold standard datasets. Our results show that our model pushes\nthe state of the art further on all datasets in terms of accuracy. We also show\nthat it is possible to get better accuracy after labelling a small sample of\nthe dataset and re-training our model rather than using an unsupervised\nclassifier.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 03:47:29 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Ram", "Achyudh", ""], ["Nagappan", "Meiyappan", ""]]}, {"id": "1812.09664", "submitter": "Junliang Guo", "authors": "Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, Tie-Yan Liu", "title": "Non-Autoregressive Neural Machine Translation with Enhanced Decoder\n  Input", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-autoregressive translation (NAT) models, which remove the dependence on\nprevious target tokens from the inputs of the decoder, achieve significantly\ninference speedup but at the cost of inferior accuracy compared to\nautoregressive translation (AT) models. Previous work shows that the quality of\nthe inputs of the decoder is important and largely impacts the model accuracy.\nIn this paper, we propose two methods to enhance the decoder inputs so as to\nimprove NAT models. The first one directly leverages a phrase table generated\nby conventional SMT approaches to translate source tokens to target tokens,\nwhich are then fed into the decoder as inputs. The second one transforms\nsource-side word embeddings to target-side word embeddings through\nsentence-level alignment and word-level adversary learning, and then feeds the\ntransformed word embeddings into the decoder as inputs. Experimental results\nshow our method largely outperforms the NAT baseline~\\citep{gu2017non} by\n$5.11$ BLEU scores on WMT14 English-German task and $4.72$ BLEU scores on WMT16\nEnglish-Romanian task.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 06:28:30 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Guo", "Junliang", ""], ["Tan", "Xu", ""], ["He", "Di", ""], ["Qin", "Tao", ""], ["Xu", "Linli", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1812.09718", "submitter": "Francesco Calimeri", "authors": "Francesco Calimeri, Simona Perri, Jessica Zangari", "title": "Optimizing Answer Set Computation via Heuristic-Based Decomposition", "comments": "28 pages, 6 figures, 4 tables, BEST PAPER AWARD at PADL 2018 (Los\n  Angeles, CA, USA), Under consideration in Theory and Practice of Logic\n  Programming (TPLP)", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 603-628", "doi": "10.1017/S1471068419000036", "report-no": null, "categories": "cs.AI cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a purely declarative formalism developed in\nthe field of logic programming and nonmonotonic reasoning: computational\nproblems are encoded by logic programs whose answer sets, corresponding to\nsolutions, are computed by an ASP system. Different, semantically equivalent,\nprograms can be defined for the same problem; however, performance of systems\nevaluating them might significantly vary. We propose an approach for\nautomatically transforming an input logic program into an equivalent one that\ncan be evaluated more efficiently. One can make use of existing\ntree-decomposition techniques for rewriting selected rules into a set of\nmultiple ones; the idea is to guide and adaptively apply them on the basis of\nproper new heuristics, to obtain a smart rewriting algorithm to be integrated\ninto an ASP system. The method is rather general: it can be adapted to any\nsystem and implement different preference policies. Furthermore, we define a\nset of new heuristics tailored at optimizing grounding, one of the main phases\nof the ASP computation; we use them in order to implement the approach into the\nASP system DLV, in particular into its grounding subsystem I-DLV, and carry out\nan extensive experimental activity for assessing the impact of the proposal.\nUnder consideration in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 14:35:58 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 17:27:16 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Calimeri", "Francesco", ""], ["Perri", "Simona", ""], ["Zangari", "Jessica", ""]]}, {"id": "1812.09798", "submitter": "Yoona Choi", "authors": "Yoona Choi and Bowon Lee", "title": "Pansori: ASR Corpus Generation from Open Online Video Contents", "comments": "5 pages with appendix", "journal-ref": "Proceedings of IEEE Seoul Section Student Paper Contest 2018,\n  Hongik University, pp. 117--121, Nov 2018", "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces Pansori, a program used to create ASR (automatic speech\nrecognition) corpora from online video contents. It utilizes a cloud-based\nspeech API to easily create a corpus in different languages. Using this\nprogram, we semi-automatically generated the Pansori-TEDxKR dataset from Korean\nTED conference talks with community-transcribed subtitles. It is the first\nhigh-quality corpus for the Korean language freely available for independent\nresearch. Pansori is released as an open-source software and the generated\ncorpus is released under a permissive public license for community use and\nparticipation.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 23:57:07 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Choi", "Yoona", ""], ["Lee", "Bowon", ""]]}, {"id": "1812.09836", "submitter": "Cong Duy Vu Hoang Mr", "authors": "Cong Duy Vu Hoang, Ioan Calapodescu and Marc Dymetman", "title": "Moment Matching Training for Neural Machine Translation: A Preliminary\n  Study", "comments": "A preliminary study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous works, neural sequence models have been shown to improve\nsignificantly if external prior knowledge can be provided, for instance by\nallowing the model to access the embeddings of explicit features during both\ntraining and inference. In this work, we propose a different point of view on\nhow to incorporate prior knowledge in a principled way, using a moment matching\nframework. In this approach, the standard local cross-entropy training of the\nsequential model is combined with a moment matching training mode that\nencourages the equality of the expectations of certain predefined features\nbetween the model distribution and the empirical distribution. In particular,\nwe show how to derive unbiased estimates of some stochastic gradients that are\ncentral to the training, and compare our framework with a formally related one:\npolicy gradient training in reinforcement learning, pointing out some important\ndifferences in terms of the kinds of prior assumptions in both approaches. Our\ninitial results are promising, showing the effectiveness of our proposed\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 05:29:19 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 01:01:56 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Hoang", "Cong Duy Vu", ""], ["Calapodescu", "Ioan", ""], ["Dymetman", "Marc", ""]]}, {"id": "1812.10037", "submitter": "Jianpeng Cheng J", "authors": "Jianpeng Cheng and Siva Reddy and Mirella Lapata", "title": "Building a Neural Semantic Parser from a Domain Ontology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic parsing is the task of converting natural language utterances into\nmachine interpretable meaning representations which can be executed against a\nreal-world environment such as a database. Scaling semantic parsing to\narbitrary domains faces two interrelated challenges: obtaining broad coverage\ntraining data effectively and cheaply; and developing a model that generalizes\nto compositional utterances and complex intentions. We address these challenges\nwith a framework which allows to elicit training data from a domain ontology\nand bootstrap a neural parser which recursively builds derivations of logical\nforms. In our framework meaning representations are described by sequences of\nnatural language templates, where each template corresponds to a decomposed\nfragment of the underlying meaning representation. Although artificial,\ntemplates can be understood and paraphrased by humans to create natural\nutterances, resulting in parallel triples of utterances, meaning\nrepresentations, and their decompositions. These allow us to train a neural\nsemantic parser which learns to compose rules in deriving meaning\nrepresentations. We crowdsource training data on six domains, covering both\nsingle-turn utterances which exhibit rich compositionality, and sequential\nutterances where a complex task is procedurally performed in steps. We then\ndevelop neural semantic parsers which perform such compositional tasks. In\ngeneral, our approach allows to deploy neural semantic parsers quickly and\ncheaply from a given domain ontology.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 05:30:18 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Reddy", "Siva", ""], ["Lapata", "Mirella", ""]]}, {"id": "1812.10061", "submitter": "Krishan Rajaratnam", "authors": "Krishan Rajaratnam and Jugal Kalita", "title": "Noise Flooding for Detecting Audio Adversarial Examples Against\n  Automatic Speech Recognition", "comments": "Orally presented at the 18th IEEE International Symposium on Signal\n  Processing and Information Technology (ISSPIT) in Louisville, Kentucky, USA,\n  December 2018. 5 pages, 2 figures", "journal-ref": null, "doi": "10.1109/ISSPIT.2018.8642623", "report-no": null, "categories": "cs.SD cs.CL cs.CR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models enjoy widespread use across a variety of tasks and have grown\nto become crucial components of many industrial systems. Despite their\neffectiveness and extensive popularity, they are not without their exploitable\nflaws. Initially applied to computer vision systems, the generation of\nadversarial examples is a process in which seemingly imperceptible\nperturbations are made to an image, with the purpose of inducing a deep\nlearning based classifier to misclassify the image. Due to recent trends in\nspeech processing, this has become a noticeable issue in speech recognition\nmodels. In late 2017, an attack was shown to be quite effective against the\nSpeech Commands classification model. Limited-vocabulary speech classifiers,\nsuch as the Speech Commands model, are used quite frequently in a variety of\napplications, particularly in managing automated attendants in telephony\ncontexts. As such, adversarial examples produced by this attack could have\nreal-world consequences. While previous work in defending against these\nadversarial examples has investigated using audio preprocessing to reduce or\ndistort adversarial noise, this work explores the idea of flooding particular\nfrequency bands of an audio signal with random noise in order to detect\nadversarial examples. This technique of flooding, which does not require\nretraining or modifying the model, is inspired by work done in computer vision\nand builds on the idea that speech classifiers are relatively robust to natural\nnoise. A combined defense incorporating 5 different frequency bands for\nflooding the signal with noise outperformed other existing defenses in the\naudio space, detecting adversarial examples with 91.8% precision and 93.5%\nrecall.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 08:02:01 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Rajaratnam", "Krishan", ""], ["Kalita", "Jugal", ""]]}, {"id": "1812.10119", "submitter": "Salah Zaiem", "authors": "Salah Zaiem and Fatiha Sadat", "title": "Sequence to Sequence Learning for Query Expansion", "comments": "8 pages, 2 figures, AAAI-19 Student Abstract and Poster Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using sequence to sequence algorithms for query expansion has not been\nexplored yet in Information Retrieval literature nor in Question-Answering's.\nWe tried to fill this gap in the literature with a custom Query Expansion\nengine trained and tested on open datasets. Starting from open datasets, we\nbuilt a Query Expansion training set using sentence-embeddings-based Keyword\nExtraction. We therefore assessed the ability of the Sequence to Sequence\nneural networks to capture expanding relations in the words embeddings' space.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 15:24:04 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zaiem", "Salah", ""], ["Sadat", "Fatiha", ""]]}, {"id": "1812.10230", "submitter": "Xinwei Geng", "authors": "Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, Zhaopeng Tu", "title": "Learning to Refine Source Representations for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) models generally adopt an encoder-decoder\narchitecture for modeling the entire translation process. The encoder\nsummarizes the representation of input sentence from scratch, which is\npotentially a problem if the sentence is ambiguous. When translating a text,\nhumans often create an initial understanding of the source sentence and then\nincrementally refine it along the translation on the target side. Starting from\nthis intuition, we propose a novel encoder-refiner-decoder framework, which\ndynamically refines the source representations based on the generated\ntarget-side information at each decoding step. Since the refining operations\nare time-consuming, we propose a strategy, leveraging the power of\nreinforcement learning models, to decide when to refine at specific decoding\nsteps. Experimental results on both Chinese-English and English-German\ntranslation tasks show that the proposed approach significantly and\nconsistently improves translation performance over the standard encoder-decoder\nframework. Furthermore, when refining strategy is applied, results still show\nreasonable improvement over the baseline without much decrease in decoding\nspeed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 05:17:03 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Geng", "Xinwei", ""], ["Wang", "Longyue", ""], ["Wang", "Xing", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "1812.10233", "submitter": "Yangbin Chen", "authors": "Yangbin Chen, Tom Ko, Lifeng Shang, Xiao Chen, Xin Jiang, Qing Li", "title": "An Investigation of Few-Shot Learning in Spoken Term Classification", "comments": "Accepted by INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the feasibility of applying few-shot learning\nalgorithms to a speech task. We formulate a user-defined scenario of spoken\nterm classification as a few-shot learning problem. In most few-shot learning\nstudies, it is assumed that all the N classes are new in a N-way problem. We\nsuggest that this assumption can be relaxed and define a N+M-way problem where\nN and M are the number of new classes and fixed classes respectively. We\npropose a modification to the Model-Agnostic Meta-Learning (MAML) algorithm to\nsolve the problem. Experiments on the Google Speech Commands dataset show that\nour approach outperforms the conventional supervised learning approach and the\noriginal MAML.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 05:43:23 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 01:18:29 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 04:03:40 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chen", "Yangbin", ""], ["Ko", "Tom", ""], ["Shang", "Lifeng", ""], ["Chen", "Xiao", ""], ["Jiang", "Xin", ""], ["Li", "Qing", ""]]}, {"id": "1812.10234", "submitter": "Yu Wang", "authors": "Yu Wang, Abhishek Patel, Hongxia Jin", "title": "A New Concept of Deep Reinforcement Learning based Augmented General\n  Sequence Tagging System", "comments": "Published at 2018 COLING", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new deep reinforcement learning based augmented general\nsequence tagging system is proposed. The new system contains two parts: a deep\nneural network (DNN) based sequence tagging model and a deep reinforcement\nlearning (DRL) based augmented tagger. The augmented tagger helps improve\nsystem performance by modeling the data with minority tags. The new system is\nevaluated on SLU and NLU sequence tagging tasks using ATIS and CoNLL-2003\nbenchmark datasets, to demonstrate the new system's outstanding performance on\ngeneral tagging tasks. Evaluated by F1 scores, it shows that the new system\noutperforms the current state-of-the-art model on ATIS dataset by 1.9% and that\non CoNLL-2003 dataset by 1.4%.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 05:54:34 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Wang", "Yu", ""], ["Patel", "Abhishek", ""], ["Jin", "Hongxia", ""]]}, {"id": "1812.10235", "submitter": "Yu Wang", "authors": "Yu Wang, Yilin Shen, Hongxia Jin", "title": "A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection\n  and Slot Filling", "comments": "5 pages, published at 2018 NAACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intent detection and slot filling are two main tasks for building a spoken\nlanguage understanding(SLU) system. Multiple deep learning based models have\ndemonstrated good results on these tasks . The most effective algorithms are\nbased on the structures of sequence to sequence models (or \"encoder-decoder\"\nmodels), and generate the intents and semantic tags either using separate\nmodels or a joint model. Most of the previous studies, however, either treat\nthe intent detection and slot filling as two separate parallel tasks, or use a\nsequence to sequence model to generate both semantic tags and intent. Most of\nthese approaches use one (joint) NN based model (including encoder-decoder\nstructure) to model two tasks, hence may not fully take advantage of the\ncross-impact between them. In this paper, new Bi-model based RNN semantic frame\nparsing network structures are designed to perform the intent detection and\nslot filling tasks jointly, by considering their cross-impact to each other\nusing two correlated bidirectional LSTMs (BLSTM). Our Bi-model structure with a\ndecoder achieves state-of-the-art result on the benchmark ATIS data, with about\n0.5$\\%$ intent accuracy improvement and 0.9 $\\%$ slot filling improvement.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 05:55:42 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Wang", "Yu", ""], ["Shen", "Yilin", ""], ["Jin", "Hongxia", ""]]}, {"id": "1812.10281", "submitter": "Abhay Sharma", "authors": "Abhay Sharma, Ananya Nandan and Reetika Ralhan", "title": "An Investigation of Supervised Learning Methods for Authorship\n  Attribution in Short Hinglish Texts using Char & Word N-grams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The writing style of a person can be affirmed as a unique identity indicator;\nthe words used, and the structuring of the sentences are clear measures which\ncan identify the author of a specific work. Stylometry and its subset -\nAuthorship Attribution, have a long history beginning from the 19th century,\nand we can still find their use in modern times. The emergence of the Internet\nhas shifted the application of attribution studies towards non-standard texts\nthat are comparatively shorter to and different from the long texts on which\nmost research has been done. The aim of this paper focuses on the study of\nshort online texts, retrieved from messaging application called WhatsApp and\nstudying the distinctive features of a macaronic language (Hinglish), using\nsupervised learning methods and then comparing the models. Various features\nsuch as word n-gram and character n-gram are compared via methods viz., Naive\nBayes Classifier, Support Vector Machine, Conditional Tree, and Random Forest,\nto find the best discriminator for such corpora. Our results showed that SVM\nattained a test accuracy of up to 95.079% while similarly, Naive Bayes attained\nan accuracy of up to 94.455% for the dataset. Conditional Tree & Random Forest\nfailed to perform as well as expected. We also found that word unigram and\ncharacter 3-grams features were more likely to distinguish authors accurately\nthan other features.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 10:54:52 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sharma", "Abhay", ""], ["Nandan", "Ananya", ""], ["Ralhan", "Reetika", ""]]}, {"id": "1812.10315", "submitter": "Amit Kirschenbaum", "authors": "Milan Dojchinovski and Julio Hernandez and Markus Ackermann and Amit\n  Kirschenbaum and Sebastian Hellmann", "title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction\n  Corpus", "comments": "15 pages, 1 figure, 4 tables, 1 listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, the DBpedia community has put significant amount of\neffort on developing technical infrastructure and methods for efficient\nextraction of structured information from Wikipedia. These efforts have been\nprimarily focused on harvesting, refinement and publishing semi-structured\ninformation found in Wikipedia articles, such as information from infoboxes,\ncategorization information, images, wikilinks and citations. Nevertheless,\nstill vast amount of valuable information is contained in the unstructured\nWikipedia article texts. In this paper, we present DBpedia NIF - a large-scale\nand multilingual knowledge extraction corpus. The aim of the dataset is\ntwo-fold: to dramatically broaden and deepen the amount of structured\ninformation in DBpedia, and to provide large-scale and multilingual language\nresource for development of various NLP and IR task. The dataset provides the\ncontent of all articles for 128 Wikipedia languages. We describe the dataset\ncreation process and the NLP Interchange Format (NIF) used to model the\ncontent, links and the structure the information of the Wikipedia articles. The\ndataset has been further enriched with about 25% more links and selected\npartitions published as Linked Data. Finally, we describe the maintenance and\nsustainability plans, and selected use cases of the dataset from the TextExt\nknowledge extraction challenge.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 13:50:50 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Dojchinovski", "Milan", ""], ["Hernandez", "Julio", ""], ["Ackermann", "Markus", ""], ["Kirschenbaum", "Amit", ""], ["Hellmann", "Sebastian", ""]]}, {"id": "1812.10356", "submitter": "Chulaka Gunasekara", "authors": "R. Chulaka Gunasekara, David Nahamoo, Lazaros C. Polymenakos, Jatin\n  Ganhotra, Kshitij P. Fadnis", "title": "Quantized-Dialog Language Model for Goal-Oriented Conversational Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel methodology to address dialog learning in the context of\ngoal-oriented conversational systems. The key idea is to quantize the dialog\nspace into clusters and create a language model across the clusters, thus\nallowing for an accurate choice of the next utterance in the conversation. The\nlanguage model relies on n-grams associated with clusters of utterances. This\nquantized-dialog language model methodology has been applied to the end-to-end\ngoal-oriented track of the latest Dialog System Technology Challenges (DSTC6).\nThe objective is to find the correct system utterance from a pool of candidates\nin order to complete a dialog between a user and an automated\nrestaurant-reservation system. Our results show that the technique proposed in\nthis paper achieves high accuracy regarding selection of the correct candidate\nutterance, and outperforms other state-of-the-art approaches based on neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 16:09:38 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Gunasekara", "R. Chulaka", ""], ["Nahamoo", "David", ""], ["Polymenakos", "Lazaros C.", ""], ["Ganhotra", "Jatin", ""], ["Fadnis", "Kshitij P.", ""]]}, {"id": "1812.10382", "submitter": "Vinayak Sachidananda", "authors": "Zi Yin, Vin Sachidananda, Balaji Prabhakar", "title": "The Global Anchor Method for Quantifying Linguistic Shifts and Domain\n  Adaptation", "comments": "Accepted to NeuRIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language is dynamic, constantly evolving and adapting with respect to time,\ndomain or topic. The adaptability of language is an active research area, where\nresearchers discover social, cultural and domain-specific changes in language\nusing distributional tools such as word embeddings. In this paper, we introduce\nthe global anchor method for detecting corpus-level language shifts. We show\nboth theoretically and empirically that the global anchor method is equivalent\nto the alignment method, a widely-used method for comparing word embeddings, in\nterms of detecting corpus-level language shifts. Despite their equivalence in\nterms of detection abilities, we demonstrate that the global anchor method is\nsuperior in terms of applicability as it can compare embeddings of different\ndimensionalities. Furthermore, the global anchor method has implementation and\nparallelization advantages. We show that the global anchor method reveals fine\nstructures in the evolution of language and domain adaptation. When combined\nwith the graph Laplacian technique, the global anchor method recovers the\nevolution trajectory and domain clustering of disparate text corpora.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 02:38:56 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Yin", "Zi", ""], ["Sachidananda", "Vin", ""], ["Prabhakar", "Balaji", ""]]}, {"id": "1812.10387", "submitter": "Pavlos Fafalios", "authors": "Renato Stoffalette Jo\\~ao, Pavlos Fafalios, Stefan Dietze", "title": "Same but Different: Distant Supervision for Predicting and Understanding\n  Entity Linking Difficulty", "comments": "Preprint of paper accepted for publication in the 34th ACM/SIGAPP\n  Symposium On Applied Computing (SAC 2019)", "journal-ref": null, "doi": "10.1145/3297280.3297381", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Linking (EL) is the task of automatically identifying entity mentions\nin a piece of text and resolving them to a corresponding entity in a reference\nknowledge base like Wikipedia. There is a large number of EL tools available\nfor different types of documents and domains, yet EL remains a challenging task\nwhere the lack of precision on particularly ambiguous mentions often spoils the\nusefulness of automated disambiguation results in real applications. A priori\napproximations of the difficulty to link a particular entity mention can\nfacilitate flagging of critical cases as part of semi-automated EL systems,\nwhile detecting latent factors that affect the EL performance, like\ncorpus-specific features, can provide insights on how to improve a system based\non the special characteristics of the underlying corpus. In this paper, we\nfirst introduce a consensus-based method to generate difficulty labels for\nentity mentions on arbitrary corpora. The difficulty labels are then exploited\nas training data for a supervised classification task able to predict the EL\ndifficulty of entity mentions using a variety of features. Experiments over a\ncorpus of news articles show that EL difficulty can be estimated with high\naccuracy, revealing also latent features that affect EL performance. Finally,\nevaluation results demonstrate the effectiveness of the proposed method to\ninform semi-automated EL pipelines.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 12:48:40 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Jo\u00e3o", "Renato Stoffalette", ""], ["Fafalios", "Pavlos", ""], ["Dietze", "Stefan", ""]]}, {"id": "1812.10400", "submitter": "Bertie Vidgen", "authors": "Bertie Vidgen, Taha Yasseri", "title": "Detecting weak and strong Islamophobic hate speech on social media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Islamophobic hate speech on social media inflicts considerable harm on both\ntargeted individuals and wider society, and also risks reputational damage for\nthe host platforms. Accordingly, there is a pressing need for robust tools to\ndetect and classify Islamophobic hate speech at scale. Previous research has\nlargely approached the detection of Islamophobic hate speech on social media as\na binary task. However, the varied nature of Islamophobia means that this is\noften inappropriate for both theoretically-informed social science and\neffectively monitoring social media. Drawing on in-depth conceptual work we\nbuild a multi-class classifier which distinguishes between non-Islamophobic,\nweak Islamophobic and strong Islamophobic content. Accuracy is 77.6% and\nbalanced accuracy is 83%. We apply the classifier to a dataset of 109,488\ntweets produced by far right Twitter accounts during 2017. Whilst most tweets\nare not Islamophobic, weak Islamophobia is considerably more prevalent (36,963\ntweets) than strong (14,895 tweets). Our main input feature is a gloVe word\nembeddings model trained on a newly collected corpus of 140 million tweets. It\noutperforms a generic word embeddings model by 5.9 percentage points,\ndemonstrating the importan4ce of context. Unexpectedly, we also find that a\none-against-one multi class SVM outperforms a deep learning algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 10:34:21 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Vidgen", "Bertie", ""], ["Yasseri", "Taha", ""]]}, {"id": "1812.10401", "submitter": "Denis Sedov", "authors": "Denis Sedov, Zhirong Yang", "title": "Word Embedding based on Low-Rank Doubly Stochastic Matrix Decomposition", "comments": null, "journal-ref": "Cheng, L., Leung, A., Ozawa, S. (eds.) ICONIP 2018. LNCS, vol.\n  11303, pp. 90-100. Springer, Cham (2018)", "doi": "10.1007/978-3-030-04182-3_9", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding, which encodes words into vectors, is an important starting\npoint in natural language processing and commonly used in many text-based\nmachine learning tasks. However, in most current word embedding approaches, the\nsimilarity in embedding space is not optimized in the learning. In this paper\nwe propose a novel neighbor embedding method which directly learns an embedding\nsimplex where the similarities between the mapped words are optimal in terms of\nminimal discrepancy to the input neighborhoods. Our method is built upon\ntwo-step random walks between words via topics and thus able to better reveal\nthe topics among the words. Experiment results indicate that our method,\ncompared with another existing word embedding approach, is more favorable for\nvarious queries.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 15:38:46 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sedov", "Denis", ""], ["Yang", "Zhirong", ""]]}, {"id": "1812.10408", "submitter": "Hugo Chu", "authors": "Marko Valentin Micic and Hugo Chu", "title": "Hyperbolic Deep Learning for Chinese Natural Language Understanding", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently hyperbolic geometry has proven to be effective in building\nembeddings that encode hierarchical and entailment information. This makes it\nparticularly suited to modelling the complex asymmetrical relationships between\nChinese characters and words. In this paper we first train a large scale\nhyperboloid skip-gram model on a Chinese corpus, then apply the character\nembeddings to a downstream hyperbolic Transformer model derived from the\nprinciples of gyrovector space for Poincare disk model. In our experiments the\ncharacter-based Transformer outperformed its word-based Euclidean equivalent.\nTo the best of our knowledge, this is the first time in Chinese NLP that a\ncharacter-based model outperformed its word-based counterpart, allowing the\ncircumvention of the challenging and domain-dependent task of Chinese Word\nSegmentation (CWS).\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 09:48:17 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Micic", "Marko Valentin", ""], ["Chu", "Hugo", ""]]}, {"id": "1812.10411", "submitter": "Siddique Latif", "authors": "Siddique Latif, Adnan Qayyum, Muhammad Usman, and Junaid Qadir", "title": "Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages", "comments": "IEEE International Conference on Frontiers of Information Technology\n  (FIT), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual speech emotion recognition is an important task for practical\napplications. The performance of automatic speech emotion recognition systems\ndegrades in cross-corpus scenarios, particularly in scenarios involving\nmultiple languages or a previously unseen language such as Urdu for which\nlimited or no data is available. In this study, we investigate the problem of\ncross-lingual emotion recognition for Urdu language and contribute URDU---the\nfirst ever spontaneous Urdu-language speech emotion database. Evaluations are\nperformed using three different Western languages against Urdu and experimental\nresults on different possible scenarios suggest various interesting aspects for\ndesigning more adaptive emotion recognition system for such limited languages.\nIn results, selecting training instances of multiple languages can deliver\ncomparable results to baseline and augmentation a fraction of testing language\ndata while training can help to boost accuracy for speech emotion recognition.\nURDU data is publicly available for further research.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 01:04:18 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 01:42:46 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Latif", "Siddique", ""], ["Qayyum", "Adnan", ""], ["Usman", "Muhammad", ""], ["Qadir", "Junaid", ""]]}, {"id": "1812.10424", "submitter": "Navid Rekabsaz", "authors": "Navid Rekabsaz, Robert West, James Henderson, Allan Hanbury", "title": "Measuring Societal Biases from Text Corpora with Smoothed First-Order\n  Co-occurrence", "comments": "In proceedings of the International AAAI Conference on Web and Social\n  Media (ICWSM) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text corpora are widely used resources for measuring societal biases and\nstereotypes. The common approach to measuring such biases using a corpus is by\ncalculating the similarities between the embedding vector of a word (like\nnurse) and the vectors of the representative words of the concepts of interest\n(such as genders). In this study, we show that, depending on what one aims to\nquantify as bias, this commonly-used approach can introduce non-relevant\nconcepts into bias measurement. We propose an alternative approach to bias\nmeasurement utilizing the smoothed first-order co-occurrence relations between\nthe word and the representative concept words, which we derive by\nreconstructing the co-occurrence estimates inherent in word embedding models.\nWe compare these approaches by conducting several experiments on the scenario\nof measuring gender bias of occupational words, according to an English\nWikipedia corpus. Our experiments show higher correlations of the measured\ngender bias with the actual gender bias statistics of the U.S. job market - on\ntwo collections and with a variety of word embedding models - using the\nfirst-order approach in comparison with the vector similarity-based approaches.\nThe first-order approach also suggests a more severe bias towards female in a\nfew specific occupations than the other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 21:00:05 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 12:08:55 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 07:18:47 GMT"}, {"version": "v4", "created": "Tue, 27 Apr 2021 14:27:41 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Rekabsaz", "Navid", ""], ["West", "Robert", ""], ["Henderson", "James", ""], ["Hanbury", "Allan", ""]]}, {"id": "1812.10464", "submitter": "Mikel Artetxe", "authors": "Mikel Artetxe, Holger Schwenk", "title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual\n  Transfer and Beyond", "comments": "TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an architecture to learn joint multilingual sentence\nrepresentations for 93 languages, belonging to more than 30 different families\nand written in 28 different scripts. Our system uses a single BiLSTM encoder\nwith a shared BPE vocabulary for all languages, which is coupled with an\nauxiliary decoder and trained on publicly available parallel corpora. This\nenables us to learn a classifier on top of the resulting embeddings using\nEnglish annotated data only, and transfer it to any of the 93 languages without\nany modification. Our experiments in cross-lingual natural language inference\n(XNLI dataset), cross-lingual document classification (MLDoc dataset) and\nparallel corpus mining (BUCC dataset) show the effectiveness of our approach.\nWe also introduce a new test set of aligned sentences in 112 languages, and\nshow that our sentence embeddings obtain strong results in multilingual\nsimilarity search even for low-resource languages. Our implementation, the\npre-trained encoder and the multilingual test set are available at\nhttps://github.com/facebookresearch/LASER\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 18:58:39 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 13:16:12 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Artetxe", "Mikel", ""], ["Schwenk", "Holger", ""]]}, {"id": "1812.10479", "submitter": "Marcelo Sardelich", "authors": "Marcelo Sardelich and Suresh Manandhar", "title": "Multimodal deep learning for short-term stock volatility prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CL cs.LG q-fin.RM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stock market volatility forecasting is a task relevant to assessing market\nrisk. We investigate the interaction between news and prices for the\none-day-ahead volatility prediction using state-of-the-art deep learning\napproaches. The proposed models are trained either end-to-end or using sentence\nencoders transfered from other tasks. We evaluate a broad range of stock market\nsectors, namely Consumer Staples, Energy, Utilities, Heathcare, and Financials.\nOur experimental results show that adding news improves the volatility\nforecasting as compared to the mainstream models that rely only on price data.\nIn particular, our model outperforms the widely-recognized GARCH(1,1) model for\nall sectors in terms of coefficient of determination $R^2$, $MSE$ and $MAE$,\nachieving the best performance when training from both news and price data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 14:35:08 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Sardelich", "Marcelo", ""], ["Manandhar", "Suresh", ""]]}, {"id": "1812.10549", "submitter": "Marc Johnson", "authors": "Marc Everett Johnson", "title": "Automatic Summarization of Natural Language", "comments": "6 pages, 1 literature synthesis matrix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic summarization of natural language is a current topic in computer\nscience research and industry, studied for decades because of its usefulness\nacross multiple domains. For example, summarization is necessary to create\nreviews such as this one. Research and applications have achieved some success\nin extractive summarization (where key sentences are curated), however,\nabstractive summarization (synthesis and re-stating) is a hard problem and\ngenerally unsolved in computer science. This literature review contrasts\nhistorical progress up through current state of the art, comparing dimensions\nsuch as: extractive vs. abstractive, supervised vs. unsupervised, NLP (Natural\nLanguage Processing) vs Knowledge-based, deep learning vs algorithms,\nstructured vs. unstructured sources, and measurement metrics such as Rouge and\nBLEU. Multiple dimensions are contrasted since current research uses\ncombinations of approaches as seen in the review matrix. Throughout this\nsummary, synthesis and critique is provided. This review concludes with\ninsights for improved abstractive summarization measurement, with surprising\nimplications for detecting understanding and comprehension in general.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 14:17:56 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Johnson", "Marc Everett", ""]]}, {"id": "1812.10604", "submitter": "Liyuan Liu", "authors": "Yujin Yuan, Liyuan Liu, Siliang Tang, Zhongfei Zhang, Yueting Zhuang,\n  Shiliang Pu, Fei Wu, Xiang Ren", "title": "Cross-relation Cross-bag Attention for Distantly-supervised Relation\n  Extraction", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision leverages knowledge bases to automatically label\ninstances, thus allowing us to train relation extractor without human\nannotations. However, the generated training data typically contain massive\nnoise, and may result in poor performances with the vanilla supervised\nlearning. In this paper, we propose to conduct multi-instance learning with a\nnovel Cross-relation Cross-bag Selective Attention (C$^2$SA), which leads to\nnoise-robust training for distant supervised relation extractor. Specifically,\nwe employ the sentence-level selective attention to reduce the effect of noisy\nor mismatched sentences, while the correlation among relations were captured to\nimprove the quality of attention weights. Moreover, instead of treating all\nentity-pairs equally, we try to pay more attention to entity-pairs with a\nhigher quality. Similarly, we adopt the selective attention mechanism to\nachieve this goal. Experiments with two types of relation extractor demonstrate\nthe superiority of the proposed approach over the state-of-the-art, while\nfurther ablation studies verify our intuitions and demonstrate the\neffectiveness of our proposed two techniques.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 03:03:51 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Yuan", "Yujin", ""], ["Liu", "Liyuan", ""], ["Tang", "Siliang", ""], ["Zhang", "Zhongfei", ""], ["Zhuang", "Yueting", ""], ["Pu", "Shiliang", ""], ["Wu", "Fei", ""], ["Ren", "Xiang", ""]]}, {"id": "1812.10628", "submitter": "Amber Nigam", "authors": "Amber Nigam, Prashik Sahare, Kushagra Pandya", "title": "Intent Detection and Slots Prompt in a Closed-Domain Chatbot", "comments": "Accepted paper for IEEE ICSC 2019 (4 pages, 1 figure, 6 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a methodology for predicting intent and slots of\na query for a chatbot that answers career-related queries. We take a\nmulti-staged approach where both the processes (intent-classification and\nslot-tagging) inform each other's decision-making in different stages. The\nmodel breaks down the problem into stages, solving one problem at a time and\npassing on relevant results of the current stage to the next, thereby reducing\nsearch space for subsequent stages, and eventually making classification and\ntagging more viable after each stage. We also observe that relaxing rules for a\nfuzzy entity-matching in slot-tagging after each stage (by maintaining a\nseparate Named Entity Tagger per stage) helps us improve performance, although\nat a slight cost of false-positives. Our model has achieved state-of-the-art\nperformance with F1-score of 77.63% for intent-classification and 82.24% for\nslot-tagging on our dataset that we would publicly release along with the\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 05:14:49 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 20:23:54 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Nigam", "Amber", ""], ["Sahare", "Prashik", ""], ["Pandya", "Kushagra", ""]]}, {"id": "1812.10720", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Kate Revoredo, Claudio Di Ciccio and Maarten de\n  Rijke", "title": "QRFA: A Data-Driven Model of Information-Seeking Dialogues", "comments": "Advances in Information Retrieval. Proceedings of the 41st European\n  Conference on Information Retrieval (ECIR '19), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the structure of interaction processes helps us to improve\ninformation-seeking dialogue systems. Analyzing an interaction process boils\ndown to discovering patterns in sequences of alternating utterances exchanged\nbetween a user and an agent. Process mining techniques have been successfully\napplied to analyze structured event logs, discovering the underlying process\nmodels or evaluating whether the observed behavior is in conformance with the\nknown process. In this paper, we apply process mining techniques to discover\npatterns in conversational transcripts and extract a new model of\ninformation-seeking dialogues, QRFA, for Query, Request, Feedback, Answer. Our\nresults are grounded in an empirical evaluation across multiple conversational\ndatasets from different domains, which was never attempted before. We show that\nthe QRFA model better reflects conversation flows observed in real\ninformation-seeking conversations than models proposed previously. Moreover,\nQRFA allows us to identify malfunctioning in dialogue system transcripts as\ndeviations from the expected conversation flow described by the model via\nconformance analysis.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 13:36:41 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Revoredo", "Kate", ""], ["Di Ciccio", "Claudio", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1812.10735", "submitter": "Shiwan Zhao Mr", "authors": "Mengting Hu, Shiwan Zhao, Li Zhang, Keke Cai, Zhong Su, Renhong Cheng,\n  Xiaowei Shen", "title": "CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis", "comments": "Accepted by EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect level sentiment classification is a fine-grained sentiment analysis\ntask. To detect the sentiment towards a particular aspect in a sentence,\nprevious studies have developed various attention-based methods for generating\naspect-specific sentence representations. However, the attention may inherently\nintroduce noise and downgrade the performance. In this paper, we propose\nconstrained attention networks (CAN), a simple yet effective solution, to\nregularize the attention for multi-aspect sentiment analysis, which alleviates\nthe drawback of the attention mechanism. Specifically, we introduce orthogonal\nregularization on multiple aspects and sparse regularization on each single\naspect. Experimental results on two public datasets demonstrate the\neffectiveness of our approach. We further extend our approach to multi-task\nsettings and outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 14:49:02 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 14:46:09 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Hu", "Mengting", ""], ["Zhao", "Shiwan", ""], ["Zhang", "Li", ""], ["Cai", "Keke", ""], ["Su", "Zhong", ""], ["Cheng", "Renhong", ""], ["Shen", "Xiaowei", ""]]}, {"id": "1812.10757", "submitter": "Chandra Khatri", "authors": "Chandra Khatri, Behnam Hedayatnia, Anu Venkatesh, Jeff Nunn, Yi Pan,\n  Qing Liu, Han Song, Anna Gottardi, Sanjeev Kwatra, Sanju Pancholi, Ming\n  Cheng, Qinglang Chen, Lauren Stubel, Karthik Gopalakrishnan, Kate Bland,\n  Raefer Gabriel, Arindam Mandal, Dilek Hakkani-Tur, Gene Hwang, Nate Michel,\n  Eric King, Rohit Prasad", "title": "Advancing the State of the Art in Open Domain Dialog Systems through the\n  Alexa Prize", "comments": "2018 Alexa Prize Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building open domain conversational systems that allow users to have engaging\nconversations on topics of their choice is a challenging task. Alexa Prize was\nlaunched in 2016 to tackle the problem of achieving natural, sustained,\ncoherent and engaging open-domain dialogs. In the second iteration of the\ncompetition in 2018, university teams advanced the state of the art by using\ncontext in dialog models, leveraging knowledge graphs for language\nunderstanding, handling complex utterances, building statistical and\nhierarchical dialog managers, and leveraging model-driven signals from user\nresponses. The 2018 competition also included the provision of a suite of tools\nand models to the competitors including the CoBot (conversational bot) toolkit,\ntopic and dialog act detection models, conversation evaluators, and a sensitive\ncontent detection model so that the competing teams could focus on building\nknowledge-rich, coherent and engaging multi-turn dialog systems. This paper\noutlines the advances developed by the university teams as well as the Alexa\nPrize team to achieve the common goal of advancing the science of\nConversational AI. We address several key open-ended problems such as\nconversational speech recognition, open domain natural language understanding,\ncommonsense reasoning, statistical dialog management, and dialog evaluation.\nThese collaborative efforts have driven improved experiences by Alexa users to\nan average rating of 3.61, the median duration of 2 mins 18 seconds, and\naverage turns to 14.6, increases of 14%, 92%, 54% respectively since the launch\nof the 2018 competition. For conversational speech recognition, we have\nimproved our relative Word Error Rate by 55% and our relative Entity Error Rate\nby 34% since the launch of the Alexa Prize. Socialbots improved in quality\nsignificantly more rapidly in 2018, in part due to the release of the CoBot\ntoolkit.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 16:17:30 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Khatri", "Chandra", ""], ["Hedayatnia", "Behnam", ""], ["Venkatesh", "Anu", ""], ["Nunn", "Jeff", ""], ["Pan", "Yi", ""], ["Liu", "Qing", ""], ["Song", "Han", ""], ["Gottardi", "Anna", ""], ["Kwatra", "Sanjeev", ""], ["Pancholi", "Sanju", ""], ["Cheng", "Ming", ""], ["Chen", "Qinglang", ""], ["Stubel", "Lauren", ""], ["Gopalakrishnan", "Karthik", ""], ["Bland", "Kate", ""], ["Gabriel", "Raefer", ""], ["Mandal", "Arindam", ""], ["Hakkani-Tur", "Dilek", ""], ["Hwang", "Gene", ""], ["Michel", "Nate", ""], ["King", "Eric", ""], ["Prasad", "Rohit", ""]]}, {"id": "1812.10847", "submitter": "Martin Potthast", "authors": "Martin Potthast, Tim Gollub, Matthias Hagen, Benno Stein", "title": "The Clickbait Challenge 2017: Towards a Regression Model for Clickbait\n  Strength", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clickbait has grown to become a nuisance to social media users and social\nmedia operators alike. Malicious content publishers misuse social media to\nmanipulate as many users as possible to visit their websites using clickbait\nmessages. Machine learning technology may help to handle this problem, giving\nrise to automatic clickbait detection. To accelerate progress in this\ndirection, we organized the Clickbait Challenge 2017, a shared task inviting\nthe submission of clickbait detectors for a comparative evaluation. A total of\n13 detectors have been submitted, achieving significant improvements over the\nprevious state of the art in terms of detection performance. Also, many of the\nsubmitted approaches have been published open source, rendering them\nreproducible, and a good starting point for newcomers. While the 2017 challenge\nhas passed, we maintain the evaluation system and answer to new registrations\nin support of the ongoing research on better clickbait detectors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 23:42:06 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Potthast", "Martin", ""], ["Gollub", "Tim", ""], ["Hagen", "Matthias", ""], ["Stein", "Benno", ""]]}, {"id": "1812.10860", "submitter": "Alex Wang", "authors": "Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas\n  McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu,\n  Shuning Jin, Berlin Chen, Benjamin Van Durme, Edouard Grave, Ellie Pavlick,\n  Samuel R. Bowman", "title": "Can You Tell Me How to Get Past Sesame Street? Sentence-Level\n  Pretraining Beyond Language Modeling", "comments": "ACL 2019. This paper supercedes \"Looking for ELMo's Friends:\n  Sentence-Level Pretraining Beyond Language Modeling\", an earlier version of\n  this work by the same authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language understanding has recently seen a surge of progress with the\nuse of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et\nal., 2019) which are pretrained on variants of language modeling. We conduct\nthe first large-scale systematic study of candidate pretraining tasks,\ncomparing 19 different tasks both as alternatives and complements to language\nmodeling. Our primary results support the use language modeling, especially\nwhen combined with pretraining on additional labeled-data tasks. However, our\nresults are mixed across pretraining tasks and show some concerning trends: In\nELMo's pretrain-then-freeze paradigm, random baselines are worryingly strong\nand results vary strikingly across target tasks. In addition, fine-tuning BERT\non an intermediate task often negatively impacts downstream transfer. In a more\npositive trend, we see modest gains from multitask training, suggesting the\ndevelopment of more sophisticated multitask and transfer learning techniques as\nan avenue for further research.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 01:21:17 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 21:37:35 GMT"}, {"version": "v3", "created": "Sun, 9 Jun 2019 18:00:27 GMT"}, {"version": "v4", "created": "Wed, 12 Jun 2019 02:08:14 GMT"}, {"version": "v5", "created": "Mon, 22 Jul 2019 22:38:54 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Wang", "Alex", ""], ["Hula", "Jan", ""], ["Xia", "Patrick", ""], ["Pappagari", "Raghavendra", ""], ["McCoy", "R. Thomas", ""], ["Patel", "Roma", ""], ["Kim", "Najoung", ""], ["Tenney", "Ian", ""], ["Huang", "Yinghui", ""], ["Yu", "Katherin", ""], ["Jin", "Shuning", ""], ["Chen", "Berlin", ""], ["Van Durme", "Benjamin", ""], ["Grave", "Edouard", ""], ["Pavlick", "Ellie", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1812.10896", "submitter": "Hoang-Quoc Nguyen-Son", "authors": "Hoang-Quoc Nguyen-Son, Ngoc-Dung T. Tieu, Huy H. Nguyen, Junichi\n  Yamagishi, and Isao Echizen", "title": "Identifying Computer-Translated Paragraphs using Coherence Features", "comments": "9 pages, PACLIC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a method for extracting the coherence features from a\nparagraph by matching similar words in its sentences. We conducted an\nexperiment with a parallel German corpus containing 2000 human-created and 2000\nmachine-translated paragraphs. The result showed that our method achieved the\nbest performance (accuracy = 72.3%, equal error rate = 29.8%) when it is\ncompared with previous methods on various computer-generated text including\ntranslation and paper generation (best accuracy = 67.9%, equal error rate =\n32.0%). Experiments on Dutch, another rich resource language, and a low\nresource one (Japanese) attained similar performances. It demonstrated the\nefficiency of the coherence features at distinguishing computer-translated from\nhuman-created paragraphs on diverse languages.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 05:35:31 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Nguyen-Son", "Hoang-Quoc", ""], ["Tieu", "Ngoc-Dung T.", ""], ["Nguyen", "Huy H.", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "1812.10901", "submitter": "Yankai Lin", "authors": "Yankai Lin, Xu Han, Ruobing Xie, Zhiyuan Liu, Maosong Sun", "title": "Knowledge Representation Learning: A Quantitative Review", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation learning (KRL) aims to represent entities and\nrelations in knowledge graph in low-dimensional semantic space, which have been\nwidely used in massive knowledge-driven tasks. In this article, we introduce\nthe reader to the motivations for KRL, and overview existing approaches for\nKRL. Afterwards, we extensively conduct and quantitative comparison and\nanalysis of several typical KRL methods on three evaluation tasks of knowledge\nacquisition including knowledge graph completion, triple classification, and\nrelation extraction. We also review the real-world applications of KRL, such as\nlanguage modeling, question answering, information retrieval, and recommender\nsystems. Finally, we discuss the remaining challenges and outlook the future\ndirections for KRL. The codes and datasets used in the experiments can be found\nin https://github.com/thunlp/OpenKE.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 06:15:53 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Lin", "Yankai", ""], ["Han", "Xu", ""], ["Xie", "Ruobing", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "1812.10991", "submitter": "Rudolf Hanel Ass Prof Dr", "authors": "Rudolf Hanel, Stefan Thurner", "title": "The role of grammar in transition-probabilities of subsequent words in\n  English text", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence formation is a highly structured, history-dependent, and\nsample-space reducing (SSR) process. While the first word in a sentence can be\nchosen from the entire vocabulary, typically, the freedom of choosing\nsubsequent words gets more and more constrained by grammar and context, as the\nsentence progresses. This sample-space reducing property offers a natural\nexplanation of Zipf's law in word frequencies, however, it fails to capture the\nstructure of the word-to-word transition probability matrices of English text.\nHere we adopt the view that grammatical constraints (such as\nsubject--predicate--object) locally re-order the word order in sentences that\nare sampled with a SSR word generation process. We demonstrate that\nsuperimposing grammatical structure -- as a local word re-ordering\n(permutation) process -- on a sample-space reducing process is sufficient to\nexplain both, word frequencies and word-to-word transition probabilities. We\ncompare the quality of the grammatically ordered SSR model in reproducing\nseveral test statistics of real texts with other text generation models, such\nas the Bernoulli model, the Simon model, and the Monkey typewriting model.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 13:49:49 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Hanel", "Rudolf", ""], ["Thurner", "Stefan", ""]]}, {"id": "1812.11158", "submitter": "Vishwanath D", "authors": "Vishwanath D, Lovekesh Vig, Gautam Shroff and Puneet Agarwal", "title": "MEETING BOT: Reinforcement Learning for Dialogue Based Meeting\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present Meeting Bot, a reinforcement learning based\nconversational system that interacts with multiple users to schedule meetings.\nThe system is able to interpret user utterences and map them to preferred time\nslots, which are then fed to a reinforcement learning (RL) system with the goal\nof converging on an agreeable time slot. The RL system is able to adapt to user\npreferences and environmental changes in meeting arrival rate while still\nscheduling effectively. Learning is performed via policy gradient with\nexploration, by utilizing an MLP as an approximator of the policy function.\nResults demonstrate that the system outperforms standard scheduling algorithms\nin terms of overall scheduling efficiency. Additionally, the system is able to\nadapt its strategy to situations when users consistently reject or accept\nmeetings in certain slots (such as Friday afternoon versus Thursday morning),\nor when the meeting is called by members who are at a more senior designation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 18:44:49 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["D", "Vishwanath", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""], ["Agarwal", "Puneet", ""]]}, {"id": "1812.11270", "submitter": "Yu Meng", "authors": "Yu Meng, Jiaming Shen, Chao Zhang, Jiawei Han", "title": "Weakly-Supervised Hierarchical Text Classification", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical text classification, which aims to classify text documents into\na given hierarchy, is an important task in many real-world applications.\nRecently, deep neural models are gaining increasing popularity for text\nclassification due to their expressive power and minimum requirement for\nfeature engineering. However, applying deep neural networks for hierarchical\ntext classification remains challenging, because they heavily rely on a large\namount of training data and meanwhile cannot easily determine appropriate\nlevels of documents in the hierarchical setting. In this paper, we propose a\nweakly-supervised neural method for hierarchical text classification. Our\nmethod does not require a large amount of training data but requires only\neasy-to-provide weak supervision signals such as a few class-related documents\nor keywords. Our method effectively leverages such weak supervision signals to\ngenerate pseudo documents for model pre-training, and then performs\nself-training on real unlabeled data to iteratively refine the model. During\nthe training process, our model features a hierarchical neural structure, which\nmimics the given hierarchy and is capable of determining the proper levels for\ndocuments with a blocking mechanism. Experiments on three datasets from\ndifferent domains demonstrate the efficacy of our method compared with a\ncomprehensive set of baselines.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 03:04:26 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Meng", "Yu", ""], ["Shen", "Jiaming", ""], ["Zhang", "Chao", ""], ["Han", "Jiawei", ""]]}, {"id": "1812.11275", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen and Karin Verspoor", "title": "End-to-end neural relation extraction using deep biaffine attention", "comments": "Proceedings of the 41st European Conference on Information Retrieval\n  (ECIR 2019), to appear", "journal-ref": null, "doi": "10.1007/978-3-030-15712-8_47", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network model for joint extraction of named entities and\nrelations between them, without any hand-crafted features. The key contribution\nof our model is to extend a BiLSTM-CRF-based entity recognition model with a\ndeep biaffine attention layer to model second-order interactions between latent\nfeatures for relation classification, specifically attending to the role of an\nentity in a directional relationship. On the benchmark \"relation and entity\nrecognition\" dataset CoNLL04, experimental results show that our model\noutperforms previous models, producing new state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 03:32:09 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Verspoor", "Karin", ""]]}, {"id": "1812.11321", "submitter": "Ningyu Zhang", "authors": "Ningyu Zhang, Shumin Deng, Zhanlin Sun, Xi Chen, Wei Zhang, Huajun\n  Chen", "title": "Attention-Based Capsule Networks with Dynamic Routing for Relation\n  Extraction", "comments": "To be published in EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A capsule is a group of neurons, whose activity vector represents the\ninstantiation parameters of a specific type of entity. In this paper, we\nexplore the capsule networks used for relation extraction in a multi-instance\nmulti-label learning framework and propose a novel neural approach based on\ncapsule networks with attention mechanisms. We evaluate our method with\ndifferent benchmarks, and it is demonstrated that our method improves the\nprecision of the predicted relations. Particularly, we show that capsule\nnetworks improve multiple entity pairs relation extraction.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 09:34:23 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Zhang", "Ningyu", ""], ["Deng", "Shumin", ""], ["Sun", "Zhanlin", ""], ["Chen", "Xi", ""], ["Zhang", "Wei", ""], ["Chen", "Huajun", ""]]}, {"id": "1812.11459", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen", "title": "A neural joint model for Vietnamese word segmentation, POS tagging and\n  dependency parsing", "comments": "In Proceedings of the 17th Annual Workshop of the Australasian\n  Language Technology Association (ALTA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first multi-task learning model for joint Vietnamese word\nsegmentation, part-of-speech (POS) tagging and dependency parsing. In\nparticular, our model extends the BIST graph-based dependency parser\n(Kiperwasser and Goldberg, 2016) with BiLSTM-CRF-based neural layers (Huang et\nal., 2015) for word segmentation and POS tagging. On Vietnamese benchmark\ndatasets, experimental results show that our joint model obtains\nstate-of-the-art or competitive performances.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 03:03:28 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 13:25:47 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 11:56:56 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Nguyen", "Dat Quoc", ""]]}, {"id": "1812.11559", "submitter": "Qiang Zhang", "authors": "Qiang Zhang, Shangsong Liang, Emine Yilmaz", "title": "Variational Self-attention Model for Sentence Representation", "comments": "arXiv admin note: text overlap with arXiv:1511.06038 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a variational self-attention model (VSAM) that employs\nvariational inference to derive self-attention. We model the self-attention\nvector as random variables by imposing a probabilistic distribution. The\nself-attention mechanism summarizes source information as an attention vector\nby weighted sum, where the weights are a learned probabilistic distribution.\nCompared with conventional deterministic counterpart, the stochastic units\nincorporated by VSAM allow multi-modal attention distributions. Furthermore, by\nmarginalizing over the latent variables, VSAM is more robust against\noverfitting. Experiments on the stance detection task demonstrate the\nsuperiority of our method.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 15:36:11 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 09:44:11 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 10:24:12 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2020 14:53:05 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Zhang", "Qiang", ""], ["Liang", "Shangsong", ""], ["Yilmaz", "Emine", ""]]}, {"id": "1812.11561", "submitter": "Chen Qu", "authors": "Chen Qu, Feng Ji, Minghui Qiu, Liu Yang, Zhiyu Min, Haiqing Chen, Jun\n  Huang and W. Bruce Croft", "title": "Learning to Selectively Transfer: Reinforced Transfer Learning for Deep\n  Text Matching", "comments": "Accepted to WSDM 2019", "journal-ref": null, "doi": "10.1145/3289600.3290978", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep text matching approaches have been widely studied for many applications\nincluding question answering and information retrieval systems. To deal with a\ndomain that has insufficient labeled data, these approaches can be used in a\nTransfer Learning (TL) setting to leverage labeled data from a resource-rich\nsource domain. To achieve better performance, source domain data selection is\nessential in this process to prevent the \"negative transfer\" problem. However,\nthe emerging deep transfer models do not fit well with most existing data\nselection methods, because the data selection policy and the transfer learning\nmodel are not jointly trained, leading to sub-optimal training efficiency.\n  In this paper, we propose a novel reinforced data selector to select\nhigh-quality source domain data to help the TL model. Specifically, the data\nselector \"acts\" on the source domain data to find a subset for optimization of\nthe TL model, and the performance of the TL model can provide \"rewards\" in turn\nto update the selector. We build the reinforced data selector based on the\nactor-critic framework and integrate it to a DNN based transfer learning model,\nresulting in a Reinforced Transfer Learning (RTL) method. We perform a thorough\nexperimental evaluation on two major tasks for text matching, namely,\nparaphrase identification and natural language inference. Experimental results\nshow the proposed RTL can significantly improve the performance of the TL\nmodel. We further investigate different settings of states, rewards, and policy\noptimization methods to examine the robustness of our method. Last, we conduct\na case study on the selected data and find our method is able to select source\ndomain data whose Wasserstein distance is close to the target domain data. This\nis reasonable and intuitive as such source domain data can provide more\ntransferability power to the model.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 15:39:57 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Qu", "Chen", ""], ["Ji", "Feng", ""], ["Qiu", "Minghui", ""], ["Yang", "Liu", ""], ["Min", "Zhiyu", ""], ["Chen", "Haiqing", ""], ["Huang", "Jun", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1812.11587", "submitter": "Moin Khan", "authors": "Moin Khan, Kamran Malik", "title": "Sentiment Classification of Customer Reviews about Automobiles in Roman\n  Urdu", "comments": "This is a pre-print of a contribution published in Advances in\n  Intelligent Systems and Computing (editors: Kohei Arai, Supriya Kapoor and\n  Rahul Bhatia) published by Springer, Cham. The final authenticated version is\n  available online at: https://doi.org/10.1007/978-3-030-03405-4_44", "journal-ref": "Advances in Intelligent Systems and Computing, vol 887 (2018)\n  630-640", "doi": "10.1007/978-3-030-03405-4_44", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text mining is a broad field having sentiment mining as its important\nconstituent in which we try to deduce the behavior of people towards a specific\nitem, merchandise, politics, sports, social media comments, review sites etc.\nOut of many issues in sentiment mining, analysis and classification, one major\nissue is that the reviews and comments can be in different languages like\nEnglish, Arabic, Urdu etc. Handling each language according to its rules is a\ndifficult task. A lot of research work has been done in English Language for\nsentiment analysis and classification but limited sentiment analysis work is\nbeing carried out on other regional languages like Arabic, Urdu and Hindi. In\nthis paper, Waikato Environment for Knowledge Analysis (WEKA) is used as a\nplatform to execute different classification models for text classification of\nRoman Urdu text. Reviews dataset has been scrapped from different automobiles\nsites. These extracted Roman Urdu reviews, containing 1000 positive and 1000\nnegative reviews, are then saved in WEKA attribute-relation file format (arff)\nas labeled examples. Training is done on 80% of this data and rest of it is\nused for testing purpose which is done using different models and results are\nanalyzed in each case. The results show that Multinomial Naive Bayes\noutperformed Bagging, Deep Neural Network, Decision Tree, Random Forest,\nAdaBoost, k-NN and SVM Classifiers in terms of more accuracy, precision, recall\nand F-measure.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 18:50:35 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Khan", "Moin", ""], ["Malik", "Kamran", ""]]}, {"id": "1812.11709", "submitter": "Zhuoren Jiang", "authors": "Zhuoren Jiang, Yue Yin, Liangcai Gao, Yao Lu, Xiaozhong Liu", "title": "Cross-language Citation Recommendation via Hierarchical Representation\n  Learning on Heterogeneous Graph", "comments": "The 41st International ACM SIGIR Conference on Research & Development\n  in Information Retrieval (SIGIR 2018), 635--644", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the volume of scholarly publications has increased at a frenetic pace,\naccessing and consuming the useful candidate papers, in very large digital\nlibraries, is becoming an essential and challenging task for scholars.\nUnfortunately, because of language barrier, some scientists (especially the\njunior ones or graduate students who do not master other languages) cannot\nefficiently locate the publications hosted in a foreign language repository. In\nthis study, we propose a novel solution, cross-language citation recommendation\nvia Hierarchical Representation Learning on Heterogeneous Graph (HRLHG), to\naddress this new problem. HRLHG can learn a representation function by mapping\nthe publications, from multilingual repositories, to a low-dimensional joint\nembedding space from various kinds of vertexes and relations on a heterogeneous\ngraph. By leveraging both global (task specific) plus local (task independent)\ninformation as well as a novel supervised hierarchical random walk algorithm,\nthe proposed method can optimize the publication representations by maximizing\nthe likelihood of locating the important cross-language neighborhoods on the\ngraph. Experiment results show that the proposed method can not only outperform\nstate-of-the-art baseline models, but also improve the interpretability of the\nrepresentation model for cross-language citation recommendation task.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 07:09:59 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Jiang", "Zhuoren", ""], ["Yin", "Yue", ""], ["Gao", "Liangcai", ""], ["Lu", "Yao", ""], ["Liu", "Xiaozhong", ""]]}, {"id": "1812.11737", "submitter": "Alexander Kuhnle", "authors": "Alexander Kuhnle and Ann Copestake", "title": "The meaning of \"most\" for visual question answering models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correct interpretation of quantifier statements in the context of a\nvisual scene requires non-trivial inference mechanisms. For the example of\n\"most\", we discuss two strategies which rely on fundamentally different\ncognitive concepts. Our aim is to identify what strategy deep learning models\nfor visual question answering learn when trained on such questions. To this\nend, we carefully design data to replicate experiments from psycholinguistics\nwhere the same question was investigated for humans. Focusing on the FiLM\nvisual question answering model, our experiments indicate that a form of\napproximate number system emerges whose performance declines with more\ndifficult scenes as predicted by Weber's law. Moreover, we identify confounding\nfactors, like spatial arrangement of the scene, which impede the effectiveness\nof this system.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 09:41:04 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 08:22:29 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Kuhnle", "Alexander", ""], ["Copestake", "Ann", ""]]}, {"id": "1812.11760", "submitter": "Nikita Kitaev", "authors": "Nikita Kitaev, Steven Cao, Dan Klein", "title": "Multilingual Constituency Parsing with Self-Attention and Pre-Training", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that constituency parsing benefits from unsupervised pre-training\nacross a variety of languages and a range of pre-training conditions. We first\ncompare the benefits of no pre-training, fastText, ELMo, and BERT for English\nand find that BERT outperforms ELMo, in large part due to increased model\ncapacity, whereas ELMo in turn outperforms the non-contextual fastText\nembeddings. We also find that pre-training is beneficial across all 11\nlanguages tested; however, large model sizes (more than 100 million parameters)\nmake it computationally expensive to train separate models for each language.\nTo address this shortcoming, we show that joint multilingual pre-training and\nfine-tuning allows sharing all but a small number of parameters between ten\nlanguages in the final model. The 10x reduction in model size compared to\nfine-tuning one model per language causes only a 3.2% relative error increase\nin aggregate. We further explore the idea of joint fine-tuning and show that it\ngives low-resource languages a way to benefit from the larger datasets of other\nlanguages. Finally, we demonstrate new state-of-the-art results for 11\nlanguages, including English (95.8 F1) and Chinese (91.8 F1).\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 11:01:02 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 12:49:56 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Kitaev", "Nikita", ""], ["Cao", "Steven", ""], ["Klein", "Dan", ""]]}, {"id": "1812.11928", "submitter": "Jinyu Li", "authors": "Amit Das, Jinyu Li, Guoli Ye, Rui Zhao, and Yifan Gong", "title": "Advancing Acoustic-to-Word CTC Model with Attention and Mixed-Units", "comments": "Accepted by IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The acoustic-to-word model based on the Connectionist Temporal Classification\n(CTC) criterion is a natural end-to-end (E2E) system directly targeting word as\noutput unit. Two issues exist in the system: first, the current output of the\nCTC model relies on the current input and does not account for context weighted\ninputs. This is the hard alignment issue. Second, the word-based CTC model\nsuffers from the out-of-vocabulary (OOV) issue. This means it can model only\nfrequently occurring words while tagging the remaining words as OOV. Hence,\nsuch a model is limited in its capacity in recognizing only a fixed set of\nfrequent words. In this study, we propose addressing these problems using a\ncombination of attention mechanism and mixed-units. In particular, we introduce\nAttention CTC, Self-Attention CTC, Hybrid CTC, and Mixed-unit CTC.\n  First, we blend attention modeling capabilities directly into the CTC network\nusing Attention CTC and Self-Attention CTC. Second, to alleviate the OOV issue,\nwe present Hybrid CTC which uses a word and letter CTC with shared hidden\nlayers. The Hybrid CTC consults the letter CTC when the word CTC emits an OOV.\nThen, we propose a much better solution by training a Mixed-unit CTC which\ndecomposes all the OOV words into sequences of frequent words and multi-letter\nunits. Evaluated on a 3400 hours Microsoft Cortana voice assistant task, our\nfinal acoustic-to-word solution using attention and mixed-units achieves a\nrelative reduction in word error rate (WER) over the vanilla word CTC by\n12.09\\%. Such an E2E model without using any language model (LM) or complex\ndecoder also outperforms a traditional context-dependent (CD) phoneme CTC with\nstrong LM and decoder by 6.79% relative.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 18:10:42 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 20:51:45 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Das", "Amit", ""], ["Li", "Jinyu", ""], ["Ye", "Guoli", ""], ["Zhao", "Rui", ""], ["Gong", "Yifan", ""]]}]