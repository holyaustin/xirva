[{"id": "1512.00103", "submitter": "Daniel Gillick", "authors": "Dan Gillick, Cliff Brunk, Oriol Vinyals, Amarnag Subramanya", "title": "Multilingual Language Processing From Bytes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads\ntext as bytes and outputs span annotations of the form [start, length, label]\nwhere start positions, lengths, and labels are separate entries in our\nvocabulary. Because we operate directly on unicode bytes rather than\nlanguage-specific words or characters, we can analyze text in many languages\nwith a single model. Due to the small vocabulary size, these multilingual\nmodels are very compact, but produce results similar to or better than the\nstate-of- the-art in Part-of-Speech tagging and Named Entity Recognition that\nuse only the provided training datasets (no external data sources). Our models\nare learning \"from scratch\" in that they do not rely on any elements of the\nstandard pipeline in Natural Language Processing (including tokenization), and\nthus can run in standalone fashion on raw text.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 00:23:44 GMT"}, {"version": "v2", "created": "Sat, 2 Apr 2016 16:26:23 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Gillick", "Dan", ""], ["Brunk", "Cliff", ""], ["Vinyals", "Oriol", ""], ["Subramanya", "Amarnag", ""]]}, {"id": "1512.00112", "submitter": "Shashank Srivastava", "authors": "Shashank Srivastava, Snigdha Chaturvedi and Tom Mitchell", "title": "Inferring Interpersonal Relations in Narrative Summaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing relationships between people is fundamental for the\nunderstanding of narratives. In this work, we address the problem of inferring\nthe polarity of relationships between people in narrative summaries. We\nformulate the problem as a joint structured prediction for each narrative, and\npresent a model that combines evidence from linguistic and semantic features,\nas well as features based on the structure of the social community in the text.\nWe also provide a clustering-based approach that can exploit regularities in\nnarrative types. e.g., learn an affinity for love-triangles in romantic\nstories. On a dataset of movie summaries from Wikipedia, our structured models\nprovide more than a 30% error-reduction over a competitive baseline that\nconsiders pairs of characters in isolation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 01:11:46 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Srivastava", "Shashank", ""], ["Chaturvedi", "Snigdha", ""], ["Mitchell", "Tom", ""]]}, {"id": "1512.00170", "submitter": "Yiming Cui", "authors": "Yiming Cui, Conghui Zhu, Xiaoning Zhu, and Tiejun Zhao", "title": "Augmenting Phrase Table by Employing Lexicons for Pivot-based SMT", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pivot language is employed as a way to solve the data sparseness problem in\nmachine translation, especially when the data for a particular language pair\ndoes not exist. The combination of source-to-pivot and pivot-to-target\ntranslation models can induce a new translation model through the pivot\nlanguage. However, the errors in two models may compound as noise, and still,\nthe combined model may suffer from a serious phrase sparsity problem. In this\npaper, we directly employ the word lexical model in IBM models as an additional\nresource to augment pivot phrase table. In addition, we also propose a phrase\ntable pruning method which takes into account both of the source and target\nphrasal coverage. Experimental result shows that our pruning method\nsignificantly outperforms the conventional one, which only considers source\nside phrasal coverage. Furthermore, by including the entries in the lexicon\nmodel, the phrase coverage increased, and we achieved improved results in\nChinese-to-Japanese translation using English as pivot language.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 08:10:49 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Cui", "Yiming", ""], ["Zhu", "Conghui", ""], ["Zhu", "Xiaoning", ""], ["Zhao", "Tiejun", ""]]}, {"id": "1512.00177", "submitter": "Yiming Cui", "authors": "Yiming Cui, Shijin Wang, Jianfeng Li", "title": "LSTM Neural Reordering Feature for Statistical Machine Translation", "comments": "6 pages, accepted by NAACL2016 short paper", "journal-ref": null, "doi": "10.18653/v1/N16-1112", "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are powerful models, which have been widely\napplied into many aspects of machine translation, such as language modeling and\ntranslation modeling. Though notable improvements have been made in these\nareas, the reordering problem still remains a challenge in statistical machine\ntranslations. In this paper, we present a novel neural reordering model that\ndirectly models word pairs and alignment. By utilizing LSTM recurrent neural\nnetworks, much longer context could be learned for reordering prediction.\nExperimental results on NIST OpenMT12 Arabic-English and Chinese-English\n1000-best rescoring task show that our LSTM neural reordering feature is robust\nand achieves significant improvements over various baseline systems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 08:43:19 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 01:17:22 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 10:01:49 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Cui", "Yiming", ""], ["Wang", "Shijin", ""], ["Li", "Jianfeng", ""]]}, {"id": "1512.00531", "submitter": "Andrew Reagan", "authors": "Andrew J. Reagan and Brian Tivnan and Jake Ryland Williams and\n  Christopher M. Danforth and Peter Sheridan Dodds", "title": "Benchmarking sentiment analysis methods for large-scale texts: A case\n  for using continuum-scored words and word shift graphs", "comments": "45 pages, 34 figures. More dictionaries added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The emergence and global adoption of social media has rendered possible the\nreal-time estimation of population-scale sentiment, bearing profound\nimplications for our understanding of human behavior. Given the growing\nassortment of sentiment measuring instruments, comparisons between them are\nevidently required. Here, we perform detailed tests of 6 dictionary-based\nmethods applied to 4 different corpora, and briefly examine a further 20\nmethods. We show that a dictionary-based method will only perform both reliably\nand meaningfully if (1) the dictionary covers a sufficiently large enough\nportion of a given text's lexicon when weighted by word usage frequency; and\n(2) words are scored on a continuous scale.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 00:34:51 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 17:42:46 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 22:21:34 GMT"}, {"version": "v4", "created": "Tue, 6 Sep 2016 19:44:07 GMT"}, {"version": "v5", "created": "Wed, 7 Sep 2016 18:53:56 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Reagan", "Andrew J.", ""], ["Tivnan", "Brian", ""], ["Williams", "Jake Ryland", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1512.00576", "submitter": "Derwin Suhartono", "authors": "Derwin Suhartono", "title": "Probabilistic Latent Semantic Analysis (PLSA) untuk Klasifikasi Dokumen\n  Teks Berbahasa Indonesia", "comments": "17 pages, 6 figures, 3 tables, Technical Report Program Studi Doktor\n  Ilmu Komputer Universitas Indonesia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One task that is included in managing documents is how to find substantial\ninformation inside. Topic modeling is a technique that has been developed to\nproduce document representation in form of keywords. The keywords will be used\nin the indexing process and document retrieval as needed by users. In this\nresearch, we will discuss specifically about Probabilistic Latent Semantic\nAnalysis (PLSA). It will cover PLSA mechanism which involves Expectation\nMaximization (EM) as the training algorithm, how to conduct testing, and obtain\nthe accuracy result.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 04:41:58 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Suhartono", "Derwin", ""]]}, {"id": "1512.00578", "submitter": "Derwin Suhartono", "authors": "Derwin Suhartono", "title": "Klasifikasi Komponen Argumen Secara Otomatis pada Dokumen Teks berbentuk\n  Esai Argumentatif", "comments": "16 pages, 3 figures, 2 tables, Technical Report Program Studi Doktor\n  Ilmu Komputer Universitas Indonesia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By automatically recognize argument component, essay writers can do some\ninspections to texts that they have written. It will assist essay scoring\nprocess objectively and precisely because essay grader is able to see how well\nthe argument components are constructed. Some reseachers have tried to do\nargument detection and classification along with its implementation in some\ndomains. The common approach is by doing feature extraction to the text.\nGenerally, the features are structural, lexical, syntactic, indicator, and\ncontextual. In this research, we add new feature to the existing features. It\nadopts keywords list by Knott and Dale (1993). The experiment result shows the\nargument classification achieves 72.45% accuracy. Moreover, we still get the\nsame accuracy without the keyword lists. This concludes that the keyword lists\ndo not affect significantly to the features. All features are still weak to\nclassify major claim and claim, so we need other features which are useful to\ndifferentiate those two kind of argument components.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 04:58:38 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Suhartono", "Derwin", ""]]}, {"id": "1512.00728", "submitter": "David Bamman", "authors": "Philip Massey, Patrick Xia, David Bamman and Noah A. Smith", "title": "Annotating Character Relationships in Literary Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a dataset of manually annotated relationships between characters\nin literary texts, in order to support the training and evaluation of automatic\nmethods for relation type prediction in this domain (Makazhanov et al., 2014;\nKokkinakis, 2013) and the broader computational analysis of literary character\n(Elson et al., 2010; Bamman et al., 2014; Vala et al., 2015; Flekova and\nGurevych, 2015). In this work, we solicit annotations from workers on Amazon\nMechanical Turk for 109 texts ranging from Homer's _Iliad_ to Joyce's _Ulysses_\non four dimensions of interest: for a given pair of characters, we collect\njudgments as to the coarse-grained category (professional, social, familial),\nfine-grained category (friend, lover, parent, rival, employer), and affinity\n(positive, negative, neutral) that describes their primary relationship in a\ntext. We do not assume that this relationship is static; we also collect\njudgments as to whether it changes at any point in the course of the text.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 15:09:31 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Massey", "Philip", ""], ["Xia", "Patrick", ""], ["Bamman", "David", ""], ["Smith", "Noah A.", ""]]}, {"id": "1512.00765", "submitter": "Cedric De Boom", "authors": "Cedric De Boom, Steven Van Canneyt, Steven Bohez, Thomas Demeester,\n  Bart Dhoedt", "title": "Learning Semantic Similarity for Very Short Texts", "comments": "6 pages, 5 figures, 3 tables, ReLSD workshop at ICDM 15", "journal-ref": null, "doi": "10.1109/ICDMW.2015.86", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Levering data on social media, such as Twitter and Facebook, requires\ninformation retrieval algorithms to become able to relate very short text\nfragments to each other. Traditional text similarity methods such as tf-idf\ncosine-similarity, based on word overlap, mostly fail to produce good results\nin this case, since word overlap is little or non-existent. Recently,\ndistributed word representations, or word embeddings, have been shown to\nsuccessfully allow words to match on the semantic level. In order to pair short\ntext fragments - as a concatenation of separate words - an adequate distributed\nsentence representation is needed, in existing literature often obtained by\nnaively combining the individual word representations. We therefore\ninvestigated several text representations as a combination of word embeddings\nin the context of semantic pair matching. This paper investigates the\neffectiveness of several such naive techniques, as well as traditional tf-idf\nsimilarity, for fragments of different lengths. Our main contribution is a\nfirst step towards a hybrid method that combines the strength of dense\ndistributed representations - as opposed to sparse term matching - with the\nstrength of tf-idf based methods to automatically reduce the impact of less\ninformative terms. Our new approach outperforms the existing techniques in a\ntoy experimental set-up, leading to the conclusion that the combination of word\nembeddings and tf-idf information might lead to a better model for semantic\ncontent within very short text fragments.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 16:31:20 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["De Boom", "Cedric", ""], ["Van Canneyt", "Steven", ""], ["Bohez", "Steven", ""], ["Demeester", "Thomas", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1512.00818", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet Sawhney, Ahmed\n  Elgammal", "title": "Zero-Shot Event Detection by Multimodal Distributional Semantic\n  Embedding of Videos", "comments": "To appear in AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new zero-shot Event Detection method by Multi-modal\nDistributional Semantic embedding of videos. Our model embeds object and action\nconcepts as well as other available modalities from videos into a\ndistributional semantic space. To our knowledge, this is the first Zero-Shot\nevent detection model that is built on top of distributional semantics and\nextends it in the following directions: (a) semantic embedding of multimodal\ninformation in videos (with focus on the visual modalities), (b) automatically\ndetermining relevance of concepts/attributes to a free text query, which could\nbe useful for other applications, and (c) retrieving videos by free text event\nquery (e.g., \"changing a vehicle tire\") based on their content. We embed videos\ninto a distributional semantic space and then measure the similarity between\nvideos and the event query in a free text form. We validated our method on the\nlarge TRECVID MED (Multimedia Event Detection) challenge. Using only the event\ntitle as a query, our method outperformed the state-of-the-art that uses big\ndescriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC\nmetric. It is also an order of magnitude faster.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 19:34:00 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 00:58:49 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Liu", "Jingen", ""], ["Cheng", "Hui", ""], ["Sawhney", "Harpreet", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1512.00965", "submitter": "Pengcheng Yin", "authors": "Pengcheng Yin, Zhengdong Lu, Hang Li, Ben Kao", "title": "Neural Enquirer: Learning to Query Tables with Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed Neural Enquirer as a neural network architecture to execute a\nnatural language (NL) query on a knowledge-base (KB) for answers. Basically,\nNeural Enquirer finds the distributed representation of a query and then\nexecutes it on knowledge-base tables to obtain the answer as one of the values\nin the tables. Unlike similar efforts in end-to-end training of semantic\nparsers, Neural Enquirer is fully \"neuralized\": it not only gives\ndistributional representation of the query and the knowledge-base, but also\nrealizes the execution of compositional queries as a series of differentiable\noperations, with intermediate results (consisting of annotations of the tables\nat different levels) saved on multiple layers of memory. Neural Enquirer can be\ntrained with gradient descent, with which not only the parameters of the\ncontrolling components and semantic parsing component, but also the embeddings\nof the tables and query words can be learned from scratch. The training can be\ndone in an end-to-end fashion, but it can take stronger guidance, e.g., the\nstep-by-step supervision for complicated queries, and benefit from it. Neural\nEnquirer is one step towards building neural network systems which seek to\nunderstand language by executing it on real-world. Our experiments show that\nNeural Enquirer can learn to execute fairly complicated NL queries on tables\nwith rich structures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 06:46:27 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 02:46:25 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Yin", "Pengcheng", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Kao", "Ben", ""]]}, {"id": "1512.01043", "submitter": "Harsh Thakkar", "authors": "Harsh Thakkar, Dhiren Patel", "title": "Approaches for Sentiment Analysis on Twitter: A State-of-Art study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Microbloging is an extremely prevalent broadcast medium amidst the Internet\nfraternity these days. People share their opinions and sentiments about variety\nof subjects like products, news, institutions, etc., every day on microbloging\nwebsites. Sentiment analysis plays a key role in prediction systems, opinion\nmining systems, etc. Twitter, one of the microbloging platforms allows a limit\nof 140 characters to its users. This restriction stimulates users to be very\nconcise about their opinion and twitter an ocean of sentiments to analyze.\nTwitter also provides developer friendly streaming API for data retrieval\npurpose allowing the analyst to search real time tweets from various users. In\nthis paper, we discuss the state-of-art of the works which are focused on\nTwitter, the online social network platform, for sentiment analysis. We survey\nvarious lexical, machine learning and hybrid approaches for sentiment analysis\non Twitter.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 11:29:36 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Thakkar", "Harsh", ""], ["Patel", "Dhiren", ""]]}, {"id": "1512.01100", "submitter": "Duyu Tang", "authors": "Duyu Tang, Bing Qin, Xiaocheng Feng, Ting Liu", "title": "Effective LSTMs for Target-Dependent Sentiment Classification", "comments": "7 pages, 3 figures published in COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target-dependent sentiment classification remains a challenge: modeling the\nsemantic relatedness of a target with its context words in a sentence.\nDifferent context words have different influences on determining the sentiment\npolarity of a sentence towards the target. Therefore, it is desirable to\nintegrate the connections between target word and context words when building a\nlearning system. In this paper, we develop two target dependent long short-term\nmemory (LSTM) models, where target information is automatically taken into\naccount. We evaluate our methods on a benchmark dataset from Twitter. Empirical\nresults show that modeling sentence representation with standard LSTM does not\nperform well. Incorporating target information into LSTM can significantly\nboost the classification accuracy. The target-dependent LSTM models achieve\nstate-of-the-art performances without using syntactic parser or external\nsentiment lexicons.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 14:54:39 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 09:40:39 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Tang", "Duyu", ""], ["Qin", "Bing", ""], ["Feng", "Xiaocheng", ""], ["Liu", "Ting", ""]]}, {"id": "1512.01173", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Jun Zhu", "title": "Building Memory with Concept Learning Capabilities from Large-scale\n  Knowledge Base", "comments": "Accepted to NIPS 2015 Cognitive Computation workshop (CoCo@NIPS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new perspective on neural knowledge base (KB) embeddings, from\nwhich we build a framework that can model symbolic knowledge in the KB together\nwith its learning process. We show that this framework well regularizes\nprevious neural KB embedding model for superior performance in reasoning tasks,\nwhile having the capabilities of dealing with unseen entities, that is, to\nlearn their embeddings from natural language descriptions, which is very like\nhuman's behavior of learning semantic concepts.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 17:52:50 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Shi", "Jiaxin", ""], ["Zhu", "Jun", ""]]}, {"id": "1512.01283", "submitter": "Vivek Datla V", "authors": "Vivek Datla and Abhinav Vishnu", "title": "Predicting the top and bottom ranks of billboard songs using Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The music industry is a $130 billion industry. Predicting whether a song\ncatches the pulse of the audience impacts the industry. In this paper we\nanalyze language inside the lyrics of the songs using several computational\nlinguistic algorithms and predict whether a song would make to the top or\nbottom of the billboard rankings based on the language features. We trained and\ntested an SVM classifier with a radial kernel function on the linguistic\nfeatures. Results indicate that we can classify whether a song belongs to top\nand bottom of the billboard charts with a precision of 0.76.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 23:42:10 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Datla", "Vivek", ""], ["Vishnu", "Abhinav", ""]]}, {"id": "1512.01337", "submitter": "Jun Yin", "authors": "Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, Xiaoming Li", "title": "Neural Generative Question Answering", "comments": "Accepted by IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end neural network model, named Neural\nGenerative Question Answering (GENQA), that can generate answers to simple\nfactoid questions, based on the facts in a knowledge-base. More specifically,\nthe model is built on the encoder-decoder framework for sequence-to-sequence\nlearning, while equipped with the ability to enquire the knowledge-base, and is\ntrained on a corpus of question-answer pairs, with their associated triples in\nthe knowledge-base. Empirical study shows the proposed model can effectively\ndeal with the variations of questions and answers, and generate right and\nnatural answers by referring to the facts in the knowledge-base. The experiment\non question answering demonstrates that the proposed model can outperform an\nembedding-based QA model as well as a neural dialogue model trained on the same\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 08:31:02 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 06:43:01 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2016 06:50:33 GMT"}, {"version": "v4", "created": "Fri, 22 Apr 2016 04:50:20 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Yin", "Jun", ""], ["Jiang", "Xin", ""], ["Lu", "Zhengdong", ""], ["Shang", "Lifeng", ""], ["Li", "Hang", ""], ["Li", "Xiaoming", ""]]}, {"id": "1512.01370", "submitter": "Yantao Jia", "authors": "Yantao Jia, Yuanzhuo Wang, Hailun Lin, Xiaolong Jin, Xueqi Cheng", "title": "Locally Adaptive Translation for Knowledge Graph Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding aims to represent entities and relations in a\nlarge-scale knowledge graph as elements in a continuous vector space. Existing\nmethods, e.g., TransE and TransH, learn embedding representation by defining a\nglobal margin-based loss function over the data. However, the optimal loss\nfunction is determined during experiments whose parameters are examined among a\nclosed set of candidates. Moreover, embeddings over two knowledge graphs with\ndifferent entities and relations share the same set of candidate loss\nfunctions, ignoring the locality of both graphs. This leads to the limited\nperformance of embedding related applications. In this paper, we propose a\nlocally adaptive translation method for knowledge graph embedding, called\nTransA, to find the optimal loss function by adaptively determining its margin\nover different knowledge graphs. Experiments on two benchmark data sets\ndemonstrate the superiority of the proposed method, as compared to\nthe-state-of-the-art ones.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 11:09:55 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Jia", "Yantao", ""], ["Wang", "Yuanzhuo", ""], ["Lin", "Hailun", ""], ["Jin", "Xiaolong", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1512.01384", "submitter": "Diego Amancio", "authors": "Henrique F. de Arruda, Luciano da F. Costa and Diego R. Amancio", "title": "Topic segmentation via community detection in complex networks", "comments": null, "journal-ref": "Chaos 26, 063120 (2016)", "doi": "10.1063/1.4954215", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real systems have been modelled in terms of network concepts, and\nwritten texts are a particular example of information networks. In recent\nyears, the use of network methods to analyze language has allowed the discovery\nof several interesting findings, including the proposition of novel models to\nexplain the emergence of fundamental universal patterns. While syntactical\nnetworks, one of the most prevalent networked models of written texts, display\nboth scale-free and small-world properties, such representation fails in\ncapturing other textual features, such as the organization in topics or\nsubjects. In this context, we propose a novel network representation whose main\npurpose is to capture the semantical relationships of words in a simple way. To\ndo so, we link all words co-occurring in the same semantic context, which is\ndefined in a threefold way. We show that the proposed representations favours\nthe emergence of communities of semantically related words, and this feature\nmay be used to identify relevant topics. The proposed methodology to detect\ntopics was applied to segment selected Wikipedia articles. We have found that,\nin general, our methods outperform traditional bag-of-words representations,\nwhich suggests that a high-level textual representation may be useful to study\nsemantical features of texts.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 12:22:26 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["de Arruda", "Henrique F.", ""], ["Costa", "Luciano da F.", ""], ["Amancio", "Diego R.", ""]]}, {"id": "1512.01409", "submitter": "Xiaoping Sun", "authors": "Mengyun Cao, Jiao Tian, Dezhi Cheng, Jin Liu, Xiaoping Sun", "title": "What Makes it Difficult to Understand a Scientific Literature?", "comments": "Accepted by SKG2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the artificial intelligence area, one of the ultimate goals is to make\ncomputers understand human language and offer assistance. In order to achieve\nthis ideal, researchers of computer science have put forward a lot of models\nand algorithms attempting at enabling the machine to analyze and process human\nnatural language on different levels of semantics. Although recent progress in\nthis field offers much hope, we still have to ask whether current research can\nprovide assistance that people really desire in reading and comprehension. To\nthis end, we conducted a reading comprehension test on two scientific papers\nwhich are written in different styles. We use the semantic link models to\nanalyze the understanding obstacles that people will face in the process of\nreading and figure out what makes it difficult for human to understand a\nscientific literature. Through such analysis, we summarized some\ncharacteristics and problems which are reflected by people with different\nlevels of knowledge on the comprehension of difficult science and technology\nliterature, which can be modeled in semantic link network. We believe that\nthese characteristics and problems will help us re-examine the existing machine\nmodels and are helpful in the designing of new one.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 14:01:32 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Cao", "Mengyun", ""], ["Tian", "Jiao", ""], ["Cheng", "Dezhi", ""], ["Liu", "Jin", ""], ["Sun", "Xiaoping", ""]]}, {"id": "1512.01525", "submitter": "Yezhou Yang", "authors": "Yezhou Yang and Yiannis Aloimonos and Cornelia Fermuller and Eren\n  Erdal Aksoy", "title": "Learning the Semantics of Manipulation Action", "comments": null, "journal-ref": "The 53rd Annual Meeting of the Association for Computational\n  Linguistics (ACL) 1 (2015) 676-686", "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a formal computational framework for modeling\nmanipulation actions. The introduced formalism leads to semantics of\nmanipulation action and has applications to both observing and understanding\nhuman manipulation actions as well as executing them with a robotic mechanism\n(e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The\ngoal of the introduced framework is to: (1) represent manipulation actions with\nboth syntax and semantic parts, where the semantic part employs\n$\\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learn\nthe $\\lambda$-calculus representation of manipulation action from an annotated\naction corpus of videos; (3) use (1) and (2) to develop a system that visually\nobserves manipulation actions and understands their meaning while it can reason\nbeyond observations using propositional logic and axiom schemata. The\nexperiments conducted on a public available large manipulation action dataset\nvalidate the theoretical framework and our implementation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 20:00:08 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Yang", "Yezhou", ""], ["Aloimonos", "Yiannis", ""], ["Fermuller", "Cornelia", ""], ["Aksoy", "Eren Erdal", ""]]}, {"id": "1512.01587", "submitter": "Sahil Garg", "authors": "Sahil Garg, Aram Galstyan, Ulf Hermjakob, and Daniel Marcu", "title": "Extracting Biomolecular Interactions Using Semantic Parsing of\n  Biomedical Text", "comments": "Appearing in Proceedings of the Thirtieth AAAI Conference on\n  Artificial Intelligence (AAAI-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advance the state of the art in biomolecular interaction extraction with\nthree contributions: (i) We show that deep, Abstract Meaning Representations\n(AMR) significantly improve the accuracy of a biomolecular interaction\nextraction system when compared to a baseline that relies solely on surface-\nand syntax-based features; (ii) In contrast with previous approaches that infer\nrelations on a sentence-by-sentence basis, we expand our framework to enable\nconsistent predictions over sets of sentences (documents); (iii) We further\nmodify and expand a graph kernel learning framework to enable concurrent\nexploitation of automatically induced AMR (semantic) and dependency structure\n(syntactic) representations. Our experiments show that our approach yields\ninteraction extraction systems that are more robust in environments where there\nis a significant mismatch between training and test conditions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 22:58:29 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Garg", "Sahil", ""], ["Galstyan", "Aram", ""], ["Hermjakob", "Ulf", ""], ["Marcu", "Daniel", ""]]}, {"id": "1512.01639", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "PJAIT Systems for the IWSLT 2015 Evaluation Campaign Enhanced by\n  Comparable Corpora", "comments": null, "journal-ref": "Proceedings of the 12th International Workshop on Spoken Language\n  Translation, Da Nang, Vietnam, December 3-4, 2015, p.101-104", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attempt to improve Statistical Machine Translation (SMT)\nsystems on a very diverse set of language pairs (in both directions): Czech -\nEnglish, Vietnamese - English, French - English and German - English. To\naccomplish this, we performed translation model training, created adaptations\nof training settings for each language pair, and obtained comparable corpora\nfor our SMT systems. Innovative tools and data adaptation techniques were\nemployed. The TED parallel text corpora for the IWSLT 2015 evaluation campaign\nwere used to train language models, and to develop, tune, and test the system.\nIn addition, we prepared Wikipedia-based comparable corpora for use with our\nSMT system. This data was specified as permissible for the IWSLT 2015\nevaluation. We explored the use of domain adaptation techniques, symmetrized\nword alignment models, the unsupervised transliteration models and the KenLM\nlanguage modeling tool. To evaluate the effects of different preparations on\ntranslation results, we conducted experiments and used the BLEU, NIST and TER\nmetrics. Our results indicate that our approach produced a positive impact on\nSMT quality.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 08:55:31 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1512.01641", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Unsupervised comparable corpora preparation and exploration for\n  bi-lingual translation equivalents", "comments": "arXiv admin note: text overlap with arXiv:1509.08639", "journal-ref": "Proceedings of the 12th IWSLT, Da Nang, Vietnam, December 3-4,\n  2015, p.118-125", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multilingual nature of the world makes translation a crucial requirement\ntoday. Parallel dictionaries constructed by humans are a widely-available\nresource, but they are limited and do not provide enough coverage for good\nquality translation purposes, due to out-of-vocabulary words and neologisms.\nThis motivates the use of statistical translation systems, which are\nunfortunately dependent on the quantity and quality of training data. Such\nsystems have a very limited availability especially for some languages and very\nnarrow text domains. In this research we present our improvements to current\ncomparable corpora mining methodologies by re- implementation of the comparison\nalgorithms (using Needleman-Wunch algorithm), introduction of a tuning script\nand computation time improvement by GPU acceleration. Experiments are carried\nout on bilingual data extracted from the Wikipedia, on various domains. For the\nWikipedia itself, additional cross-lingual comparison heuristics were\nintroduced. The modifications made a positive impact on the quality and\nquantity of mined data and on the translation quality.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 08:59:28 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1512.01712", "submitter": "Konstantin Lopyrev", "authors": "Konstantin Lopyrev", "title": "Generating News Headlines with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an application of an encoder-decoder recurrent neural network\nwith LSTM units and attention to generating headlines from the text of news\narticles. We find that the model is quite effective at concisely paraphrasing\nnews articles. Furthermore, we study how the neural network decides which input\nwords to pay attention to, and specifically we identify the function of the\ndifferent neurons in a simplified attention mechanism. Interestingly, our\nsimplified attention mechanism performs better that the more complex attention\nmechanism on a held out set of articles.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 23:41:22 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Lopyrev", "Konstantin", ""]]}, {"id": "1512.01768", "submitter": "Danish .", "authors": "Danish, Yogesh Dahiya, Partha Talukdar", "title": "Want Answers? A Reddit Inspired Study on How to Pose Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Questions form an integral part of our everyday communication, both offline\nand online. Getting responses to our questions from others is fundamental to\nsatisfying our information need and in extending our knowledge boundaries. A\nquestion may be represented using various factors such as social, syntactic,\nsemantic, etc. We hypothesize that these factors contribute with varying\ndegrees towards getting responses from others for a given question. We perform\na thorough empirical study to measure effects of these factors using a novel\nquestion and answer dataset from the website Reddit.com. To the best of our\nknowledge, this is the first such analysis of its kind on this important topic.\nWe also use a sparse nonnegative matrix factorization technique to\nautomatically induce interpretable semantic factors from the question dataset.\nWe also document various patterns on response prediction we observe during our\nanalysis in the data. For instance, we found that preference-probing questions\nare scantily answered. Our method is robust to capture such latent response\nfactors. We hope to make our code and datasets publicly available upon\npublication of the paper.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 10:31:12 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Danish", "", ""], ["Dahiya", "Yogesh", ""], ["Talukdar", "Partha", ""]]}, {"id": "1512.01818", "submitter": "Filipe Ribeiro Filipe Nunes Ribeiro", "authors": "Filipe Nunes Ribeiro, Matheus Ara\\'ujo, Pollyanna Gon\\c{c}alves,\n  Fabr\\'icio Benevenuto, Marcos Andr\\'e Gon\\c{c}alves", "title": "SentiBench - a benchmark comparison of state-of-the-practice sentiment\n  analysis methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years thousands of scientific papers have investigated\nsentiment analysis, several startups that measure opinions on real data have\nemerged and a number of innovative products related to this theme have been\ndeveloped. There are multiple methods for measuring sentiments, including\nlexical-based and supervised machine learning methods. Despite the vast\ninterest on the theme and wide popularity of some methods, it is unclear which\none is better for identifying the polarity (i.e., positive or negative) of a\nmessage. Accordingly, there is a strong need to conduct a thorough\napple-to-apple comparison of sentiment analysis methods, \\textit{as they are\nused in practice}, across multiple datasets originated from different data\nsources. Such a comparison is key for understanding the potential limitations,\nadvantages, and disadvantages of popular methods. This article aims at filling\nthis gap by presenting a benchmark comparison of twenty-four popular sentiment\nanalysis methods (which we call the state-of-the-practice methods). Our\nevaluation is based on a benchmark of eighteen labeled datasets, covering\nmessages posted on social networks, movie and product reviews, as well as\nopinions and comments in news articles. Our results highlight the extent to\nwhich the prediction performance of these methods varies considerably across\ndatasets. Aiming at boosting the development of this research area, we open the\nmethods' codes and datasets used in this article, deploying them in a benchmark\nsystem, which provides an open API for accessing and comparing sentence-level\nsentiment analysis methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 18:52:51 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 00:32:23 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2016 18:54:51 GMT"}, {"version": "v4", "created": "Sat, 4 Jun 2016 16:52:29 GMT"}, {"version": "v5", "created": "Thu, 14 Jul 2016 22:51:39 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Ribeiro", "Filipe Nunes", ""], ["Ara\u00fajo", "Matheus", ""], ["Gon\u00e7alves", "Pollyanna", ""], ["Benevenuto", "Fabr\u00edcio", ""], ["Gon\u00e7alves", "Marcos Andr\u00e9", ""]]}, {"id": "1512.01882", "submitter": "Dong Wang", "authors": "Dong Wang and Xuewei Zhang", "title": "THCHS-30 : A Free Chinese Speech Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech data is crucially important for speech recognition research. There are\nquite some speech databases that can be purchased at prices that are reasonable\nfor most research institutes. However, for young people who just start research\nactivities or those who just gain initial interest in this direction, the cost\nfor data is still an annoying barrier. We support the `free data' movement in\nspeech recognition: research institutes (particularly supported by public\nfunds) publish their data freely so that new researchers can obtain sufficient\ndata to kick of their career. In this paper, we follow this trend and release a\nfree Chinese speech database THCHS-30 that can be used to build a full- edged\nChinese speech recognition system. We report the baseline system established\nwith this database, including the performance under highly noisy conditions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 02:07:21 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 13:35:33 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Wang", "Dong", ""], ["Zhang", "Xuewei", ""]]}, {"id": "1512.01926", "submitter": "Kamil Rocki", "authors": "Kamil Rocki", "title": "Thinking Required", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a theory of a single general-purpose learning algorithm which\ncould explain the principles its operation. It assumes the initial rough\narchitecture, a small library of simple innate circuits which are prewired at\nbirth. and proposes that all significant mental algorithms are learned. Given\ncurrent understanding and observations, this paper reviews and lists the\ningredients of such an algorithm from architectural and functional\nperspectives.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 06:37:49 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Rocki", "Kamil", ""]]}, {"id": "1512.02009", "submitter": "Bei Chen", "authors": "Bei Chen, Jun Zhu, Nan Yang, Tian Tian, Ming Zhou, Bo Zhang", "title": "Jointly Modeling Topics and Intents with Global Order Structure", "comments": "Accepted by AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling document structure is of great importance for discourse analysis and\nrelated applications. The goal of this research is to capture the document\nintent structure by modeling documents as a mixture of topic words and\nrhetorical words. While the topics are relatively unchanged through one\ndocument, the rhetorical functions of sentences usually change following\ncertain orders in discourse. We propose GMM-LDA, a topic modeling based\nBayesian unsupervised model, to analyze the document intent structure\ncooperated with order information. Our model is flexible that has the ability\nto combine the annotations and do supervised learning. Additionally, entropic\nregularization can be introduced to model the significant divergence between\ntopics and intents. We perform experiments in both unsupervised and supervised\nsettings, results show the superiority of our model over several\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 12:16:58 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Chen", "Bei", ""], ["Zhu", "Jun", ""], ["Yang", "Nan", ""], ["Tian", "Tian", ""], ["Zhou", "Ming", ""], ["Zhang", "Bo", ""]]}, {"id": "1512.02167", "submitter": "Bolei Zhou", "authors": "Bolei Zhou and Yuandong Tian and Sainbayar Sukhbaatar and Arthur Szlam\n  and Rob Fergus", "title": "Simple Baseline for Visual Question Answering", "comments": "One comparison method's scores are put into the correct column, and a\n  new experiment of generating attention map is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a very simple bag-of-words baseline for visual question\nanswering. This baseline concatenates the word features from the question and\nCNN features from the image to predict the answer. When evaluated on the\nchallenging VQA dataset [2], it shows comparable performance to many recent\napproaches using recurrent neural networks. To explore the strength and\nweakness of the trained model, we also provide an interactive web demo and\nopen-source code. .\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 19:00:54 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 05:17:49 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Zhou", "Bolei", ""], ["Tian", "Yuandong", ""], ["Sukhbaatar", "Sainbayar", ""], ["Szlam", "Arthur", ""], ["Fergus", "Rob", ""]]}, {"id": "1512.02433", "submitter": "Yang Liu", "authors": "Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and\n  Yang Liu", "title": "Minimum Risk Training for Neural Machine Translation", "comments": "Accepted for publication in Proceedings of ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose minimum risk training for end-to-end neural machine translation.\nUnlike conventional maximum likelihood estimation, minimum risk training is\ncapable of optimizing model parameters directly with respect to arbitrary\nevaluation metrics, which are not necessarily differentiable. Experiments show\nthat our approach achieves significant improvements over maximum likelihood\nestimation on a state-of-the-art neural machine translation system across\nvarious languages pairs. Transparent to architectures, our approach can be\napplied to more neural networks and potentially benefit more NLP tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 12:42:00 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 14:20:36 GMT"}, {"version": "v3", "created": "Wed, 15 Jun 2016 00:07:05 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Shen", "Shiqi", ""], ["Cheng", "Yong", ""], ["He", "Zhongjun", ""], ["He", "Wei", ""], ["Wu", "Hua", ""], ["Sun", "Maosong", ""], ["Liu", "Yang", ""]]}, {"id": "1512.02567", "submitter": "Mojtaba Hajiabadi", "authors": "Mojtaba Hajiabadi", "title": "Distributed Adaptive LMF Algorithm for Sparse Parameter Estimation in\n  Gaussian Mixture Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed adaptive algorithm for estimation of sparse unknown parameters\nin the presence of nonGaussian noise is proposed in this paper based on\nnormalized least mean fourth (NLMF) criterion. At the first step, local\nadaptive NLMF algorithm is modified by zero norm in order to speed up the\nconvergence rate and also to reduce the steady state error power in sparse\nconditions. Then, the proposed algorithm is extended for distributed scenario\nin which more improvement in estimation performance is achieved due to\ncooperation of local adaptive filters. Simulation results show the superiority\nof the proposed algorithm in comparison with conventional NLMF algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 17:54:17 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Hajiabadi", "Mojtaba", ""]]}, {"id": "1512.02595", "submitter": "Awni Hannun", "authors": "Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared\n  Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg\n  Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han,\n  Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew\n  Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David\n  Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani\n  Yogatama, Jun Zhan, Zhenyao Zhu", "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that an end-to-end deep learning approach can be used to recognize\neither English or Mandarin Chinese speech--two vastly different languages.\nBecause it replaces entire pipelines of hand-engineered components with neural\nnetworks, end-to-end learning allows us to handle a diverse variety of speech\nincluding noisy environments, accents and different languages. Key to our\napproach is our application of HPC techniques, resulting in a 7x speedup over\nour previous system. Because of this efficiency, experiments that previously\ntook weeks now run in days. This enables us to iterate more quickly to identify\nsuperior architectures and algorithms. As a result, in several cases, our\nsystem is competitive with the transcription of human workers when benchmarked\non standard datasets. Finally, using a technique called Batch Dispatch with\nGPUs in the data center, we show that our system can be inexpensively deployed\nin an online setting, delivering low latency when serving users at scale.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 19:13:50 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Amodei", "Dario", ""], ["Anubhai", "Rishita", ""], ["Battenberg", "Eric", ""], ["Case", "Carl", ""], ["Casper", "Jared", ""], ["Catanzaro", "Bryan", ""], ["Chen", "Jingdong", ""], ["Chrzanowski", "Mike", ""], ["Coates", "Adam", ""], ["Diamos", "Greg", ""], ["Elsen", "Erich", ""], ["Engel", "Jesse", ""], ["Fan", "Linxi", ""], ["Fougner", "Christopher", ""], ["Han", "Tony", ""], ["Hannun", "Awni", ""], ["Jun", "Billy", ""], ["LeGresley", "Patrick", ""], ["Lin", "Libby", ""], ["Narang", "Sharan", ""], ["Ng", "Andrew", ""], ["Ozair", "Sherjil", ""], ["Prenger", "Ryan", ""], ["Raiman", "Jonathan", ""], ["Satheesh", "Sanjeev", ""], ["Seetapun", "David", ""], ["Sengupta", "Shubho", ""], ["Wang", "Yi", ""], ["Wang", "Zhiqian", ""], ["Wang", "Chong", ""], ["Xiao", "Bo", ""], ["Yogatama", "Dani", ""], ["Zhan", "Jun", ""], ["Zhu", "Zhenyao", ""]]}, {"id": "1512.02902", "submitter": "Makarand Tapaswi", "authors": "Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba,\n  Raquel Urtasun, Sanja Fidler", "title": "MovieQA: Understanding Stories in Movies through Question-Answering", "comments": "CVPR 2016, Spotlight presentation. Benchmark @\n  http://movieqa.cs.toronto.edu/ Code @\n  https://github.com/makarandtapaswi/MovieQA_CVPR2016/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the MovieQA dataset which aims to evaluate automatic story\ncomprehension from both video and text. The dataset consists of 14,944\nquestions about 408 movies with high semantic diversity. The questions range\nfrom simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain events\noccurred. Each question comes with a set of five possible answers; a correct\none and four deceiving answers provided by human annotators. Our dataset is\nunique in that it contains multiple sources of information -- video clips,\nplots, subtitles, scripts, and DVS. We analyze our data through various\nstatistics and methods. We further extend existing QA techniques to show that\nquestion-answering with such open-ended semantics is hard. We make this data\nset public along with an evaluation benchmark to encourage inspiring work in\nthis challenging domain.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 15:34:31 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 04:52:35 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Tapaswi", "Makarand", ""], ["Zhu", "Yukun", ""], ["Stiefelhagen", "Rainer", ""], ["Torralba", "Antonio", ""], ["Urtasun", "Raquel", ""], ["Fidler", "Sanja", ""]]}, {"id": "1512.03460", "submitter": "Yezhou Yang", "authors": "Yezhou Yang and Yi Li and Cornelia Fermuller and Yiannis Aloimonos", "title": "Neural Self Talk: Image Understanding via Continuous Questioning and\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of continuously discovering image\ncontents by actively asking image based questions and subsequently answering\nthe questions being asked. The key components include a Visual Question\nGeneration (VQG) module and a Visual Question Answering module, in which\nRecurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are\nused. Given a dataset that contains images, questions and their answers, both\nmodules are trained at the same time, with the difference being VQG uses the\nimages as input and the corresponding questions as output, while VQA uses\nimages and questions as input and the corresponding answers as output. We\nevaluate the self talk process subjectively using Amazon Mechanical Turk, which\nshow effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 21:58:46 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Yang", "Yezhou", ""], ["Li", "Yi", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1512.03465", "submitter": "Walid Shalaby", "authors": "Walid Shalaby, Wlodek Zadrozny", "title": "Mined Semantic Analysis: A New Concept Space Model for Semantic\n  Representation of Textual Data", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mined Semantic Analysis (MSA) is a novel concept space model which employs\nunsupervised learning to generate semantic representations of text. MSA\nrepresents textual structures (terms, phrases, documents) as a Bag of Concepts\n(BoC) where concepts are derived from concept rich encyclopedic corpora.\nTraditional concept space models exploit only target corpus content to\nconstruct the concept space. MSA, alternatively, uncovers implicit relations\nbetween concepts by mining for their associations (e.g., mining Wikipedia's\n\"See also\" link graph). We evaluate MSA's performance on benchmark datasets for\nmeasuring semantic relatedness of words and sentences. Empirical results show\ncompetitive performance of MSA compared to prior state-of-the-art methods.\nAdditionally, we introduce the first analytical study to examine statistical\nsignificance of results reported by different semantic relatedness methods. Our\nstudy shows that, the nuances of results across top performing methods could be\nstatistically insignificant. The study positions MSA as one of state-of-the-art\nmethods for measuring semantic relatedness, besides the inherent\ninterpretability and simplicity of the generated semantic representation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 22:15:10 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2016 03:46:23 GMT"}, {"version": "v3", "created": "Sun, 31 Dec 2017 21:14:26 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Shalaby", "Walid", ""], ["Zadrozny", "Wlodek", ""]]}, {"id": "1512.03549", "submitter": "Pranjal Singh", "authors": "Pranjal Singh, Amitabha Mukerjee", "title": "Words are not Equal: Graded Weighting Model for building Composite\n  Document Vectors", "comments": "10 Pages, 2 Figures, 11 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of distributional semantics, composing phrases from word\nvectors remains an important challenge. Several methods have been tried for\nbenchmark tasks such as sentiment classification, including word vector\naveraging, matrix-vector approaches based on parsing, and on-the-fly learning\nof paragraph vectors. Most models usually omit stop words from the composition.\nInstead of such an yes-no decision, we consider several graded schemes where\nwords are weighted according to their discriminatory relevance with respect to\nits use in the document (e.g., idf). Some of these methods (particularly\ntf-idf) are seen to result in a significant improvement in performance over\nprior state of the art. Further, combining such approaches into an ensemble\nbased on alternate classifiers such as the RNN model, results in an 1.6%\nperformance improvement on the standard IMDB movie review dataset, and a 7.01%\nimprovement on Amazon product reviews. Since these are language free models and\ncan be obtained in an unsupervised manner, they are of interest also for\nunder-resourced languages such as Hindi as well and many more languages. We\ndemonstrate the language free aspects by showing a gain of 12% for two review\ndatasets over earlier results, and also release a new larger dataset for future\ntesting (Singh,2015).\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 08:44:45 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Singh", "Pranjal", ""], ["Mukerjee", "Amitabha", ""]]}, {"id": "1512.03950", "submitter": "Kamal Sarkar", "authors": "Kamal Sarkar", "title": "A Hidden Markov Model Based System for Entity Extraction from Social\n  Media English Text at FIRE 2015", "comments": "FIRE 2015 Task:Entity Extraction from Social Media Text - Indian\n  Languages (ESM-IL) - See more at:\n  http://fire.irsi.res.in/fire/home#sthash.HpgiwjP5.dpuf. arXiv admin note:\n  substantial text overlap with arXiv:1405.7397", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the experiments carried out by us at Jadavpur University\nas part of the participation in FIRE 2015 task: Entity Extraction from Social\nMedia Text - Indian Languages (ESM-IL). The tool that we have developed for the\ntask is based on Trigram Hidden Markov Model that utilizes information like\ngazetteer list, POS tag and some other word level features to enhance the\nobservation probabilities of the known tokens as well as unknown tokens. We\nsubmitted runs for English only. A statistical HMM (Hidden Markov Models) based\nmodel has been used to implement our system. The system has been trained and\ntested on the datasets released for FIRE 2015 task: Entity Extraction from\nSocial Media Text - Indian Languages (ESM-IL). Our system is the best performer\nfor English language and it obtains precision, recall and F-measures of 61.96,\n39.46 and 48.21 respectively.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 18:57:11 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Sarkar", "Kamal", ""]]}, {"id": "1512.04092", "submitter": "Shagun Sodhani", "authors": "Sanket Mehta, Shagun Sodhani", "title": "Stack Exchange Tagger", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of our project is to develop an accurate tagger for questions posted\non Stack Exchange. Our problem is an instance of the more general problem of\ndeveloping accurate classifiers for large scale text datasets. We are tackling\nthe multilabel classification problem where each item (in this case, question)\ncan belong to multiple classes (in this case, tags). We are predicting the tags\n(or keywords) for a particular Stack Exchange post given only the question text\nand the title of the post. In the process, we compare the performance of\nSupport Vector Classification (SVC) for different kernel functions, loss\nfunction, etc. We found linear SVC with Crammer Singer technique produces best\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 17:52:44 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Mehta", "Sanket", ""], ["Sodhani", "Shagun", ""]]}, {"id": "1512.04280", "submitter": "Liang Lu", "authors": "Liang Lu and Steve Renals", "title": "Small-footprint Deep Neural Networks with Highway Connections for Speech\n  Recognition", "comments": "5 pages, 3 figures, fixed typo, accepted by Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For speech recognition, deep neural networks (DNNs) have significantly\nimproved the recognition accuracy in most of benchmark datasets and application\ndomains. However, compared to the conventional Gaussian mixture models,\nDNN-based acoustic models usually have much larger number of model parameters,\nmaking it challenging for their applications in resource constrained platforms,\ne.g., mobile devices. In this paper, we study the application of the recently\nproposed highway network to train small-footprint DNNs, which are {\\it thinner}\nand {\\it deeper}, and have significantly smaller number of model parameters\ncompared to conventional DNNs. We investigated this approach on the AMI meeting\nspeech transcription corpus which has around 70 hours of audio data. The\nhighway neural networks constantly outperformed their plain DNN counterparts,\nand the number of model parameters can be reduced significantly without\nsacrificing the recognition accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 12:29:32 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 12:14:06 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 10:30:54 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 15:17:27 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Lu", "Liang", ""], ["Renals", "Steve", ""]]}, {"id": "1512.04407", "submitter": "Arjun Chandrasekaran", "authors": "Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit\n  Bansal, Dhruv Batra, C. Lawrence Zitnick and Devi Parikh", "title": "We Are Humor Beings: Understanding and Predicting Visual Humor", "comments": "17 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humor is an integral part of human lives. Despite being tremendously\nimpactful, it is perhaps surprising that we do not have a detailed\nunderstanding of humor yet. As interactions between humans and AI systems\nincrease, it is imperative that these systems are taught to understand\nsubtleties of human expressions such as humor. In this work, we are interested\nin the question - what content in a scene causes it to be funny? As a first\nstep towards understanding visual humor, we analyze the humor manifested in\nabstract scenes and design computational models for them. We collect two\ndatasets of abstract scenes that facilitate the study of humor at both the\nscene-level and the object-level. We analyze the funny scenes and explore the\ndifferent types of humor depicted in them via human studies. We model two tasks\nthat we believe demonstrate an understanding of some aspects of visual humor.\nThe tasks involve predicting the funniness of a scene and altering the\nfunniness of a scene. We show that our models perform well quantitatively, and\nqualitatively through human studies. Our datasets are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 16:59:35 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 02:12:49 GMT"}, {"version": "v3", "created": "Sun, 10 Apr 2016 22:15:43 GMT"}, {"version": "v4", "created": "Thu, 5 May 2016 21:36:13 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Chandrasekaran", "Arjun", ""], ["Vijayakumar", "Ashwin K.", ""], ["Antol", "Stanislaw", ""], ["Bansal", "Mohit", ""], ["Batra", "Dhruv", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""]]}, {"id": "1512.04419", "submitter": "Mehrnoosh Sadrzadeh", "authors": "Esma Balkir, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh", "title": "Sentence Entailment in Compositional Distributional Semantics", "comments": "8 pages, 1 figure, 2 tables, short version presented in the\n  International Symposium on Artificial Intelligence and Mathematics (ISAIM),\n  2016", "journal-ref": "Ann Math Artif Intell (2018) 82: 189.\n  https://doi.org/10.1007/s10472-017-9570-x", "doi": "10.1007/s10472-017-9570-x", "report-no": null, "categories": "cs.CL cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional semantic models provide vector representations for words by\ngathering co-occurrence frequencies from corpora of text. Compositional\ndistributional models extend these from words to phrases and sentences. In\ncategorical compositional distributional semantics, phrase and sentence\nrepresentations are functions of their grammatical structure and\nrepresentations of the words therein. In this setting, grammatical structures\nare formalised by morphisms of a compact closed category and meanings of words\nare formalised by objects of the same category. These can be instantiated in\nthe form of vectors or density matrices. This paper concerns the applications\nof this model to phrase and sentence level entailment. We argue that\nentropy-based distances of vectors and density matrices provide a good\ncandidate to measure word-level entailment, show the advantage of density\nmatrices over vectors for word level entailments, and prove that these\ndistances extend compositionally from words to phrases and sentences. We\nexemplify our theoretical constructions on real data and a toy entailment\ndataset and provide preliminary experimental evidence.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 17:36:35 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 10:49:35 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Balkir", "Esma", ""], ["Kartsaklis", "Dimitri", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1512.04650", "submitter": "Yang Liu", "authors": "Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and\n  Yang Liu", "title": "Agreement-based Joint Training for Bidirectional Attention-based Neural\n  Machine Translation", "comments": "Accepted for publication in IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attentional mechanism has proven to be effective in improving end-to-end\nneural machine translation. However, due to the intricate structural divergence\nbetween natural languages, unidirectional attention-based models might only\ncapture partial aspects of attentional regularities. We propose agreement-based\njoint training for bidirectional attention-based end-to-end neural machine\ntranslation. Instead of training source-to-target and target-to-source\ntranslation models independently,our approach encourages the two complementary\nmodels to agree on word alignment matrices on the same training data.\nExperiments on Chinese-English and English-French translation tasks show that\nagreement-based joint training significantly improves both alignment and\ntranslation quality over independent training.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 04:55:06 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 00:43:03 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Cheng", "Yong", ""], ["Shen", "Shiqi", ""], ["He", "Zhongjun", ""], ["He", "Wei", ""], ["Wu", "Hua", ""], ["Sun", "Maosong", ""], ["Liu", "Yang", ""]]}, {"id": "1512.04701", "submitter": "Weixin Li", "authors": "Weixin Li, Jungseock Joo, Hang Qi, and Song-Chun Zhu", "title": "Joint Image-Text News Topic Detection and Tracking with And-Or Graph\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to develop a method for automatically detecting and\ntracking topics in broadcast news. We present a hierarchical And-Or graph (AOG)\nto jointly represent the latent structure of both texts and visuals. The AOG\nembeds a context sensitive grammar that can describe the hierarchical\ncomposition of news topics by semantic elements about people involved, related\nplaces and what happened, and model contextual relationships between elements\nin the hierarchy. We detect news topics through a cluster sampling process\nwhich groups stories about closely related events. Swendsen-Wang Cuts (SWC), an\neffective cluster sampling algorithm, is adopted for traversing the solution\nspace and obtaining optimal clustering solutions by maximizing a Bayesian\nposterior probability. Topics are tracked to deal with the continuously updated\nnews streams. We generate topic trajectories to show how topics emerge, evolve\nand disappear over time. The experimental results show that our method can\nexplicitly describe the textual and visual data in news videos and produce\nmeaningful topic trajectories. Our method achieves superior performance\ncompared to state-of-the-art methods on both a public dataset Reuters-21578 and\na self-collected dataset named UCLA Broadcast News Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 10:01:37 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Li", "Weixin", ""], ["Joo", "Jungseock", ""], ["Qi", "Hang", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1512.04906", "submitter": "David Grangier", "authors": "Welin Chen and David Grangier and Michael Auli", "title": "Strategies for Training Large Vocabulary Neural Language Models", "comments": "12 pages; journal paper; under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural network language models over large vocabularies is still\ncomputationally very costly compared to count-based models such as Kneser-Ney.\nAt the same time, neural language models are gaining popularity for many\napplications such as speech recognition and machine translation whose success\ndepends on scalability. We present a systematic comparison of strategies to\nrepresent and train large vocabularies, including softmax, hierarchical\nsoftmax, target sampling, noise contrastive estimation and self normalization.\nWe further extend self normalization to be a proper estimator of likelihood and\nintroduce an efficient variant of softmax. We evaluate each method on three\npopular benchmarks, examining performance on rare words, the speed/accuracy\ntrade-off and complementarity to Kneser-Ney.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 19:29:01 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Chen", "Welin", ""], ["Grangier", "David", ""], ["Auli", "Michael", ""]]}, {"id": "1512.04973", "submitter": "Ndapandula Nakashole", "authors": "Ndapandula Nakashole", "title": "An Operator for Entity Extraction in MapReduce", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary-based entity extraction involves finding mentions of dictionary\nentities in text. Text mentions are often noisy, containing spurious or missing\nwords. Efficient algorithms for detecting approximate entity mentions follow\none of two general techniques. The first approach is to build an index on the\nentities and perform index lookups of document substrings. The second approach\nrecognizes that the number of substrings generated from documents can explode\nto large numbers, to get around this, they use a filter to prune many such\nsubstrings which do not match any dictionary entity and then only verify the\nremaining substrings if they are entity mentions of dictionary entities, by\nmeans of a text join. The choice between the index-based approach and the\nfilter & verification-based approach is a case-to-case decision as the best\napproach depends on the characteristics of the input entity dictionary, for\nexample frequency of entity mentions. Choosing the right approach for the\nsetting can make a substantial difference in execution time. Making this choice\nis however non-trivial as there are parameters within each of the approaches\nthat make the space of possible approaches very large. In this paper, we\npresent a cost-based operator for making the choice among execution plans for\nentity extraction. Since we need to deal with large dictionaries and even\nlarger large datasets, our operator is developed for implementations of\nMapReduce distributed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 21:23:20 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Nakashole", "Ndapandula", ""]]}, {"id": "1512.05004", "submitter": "Jaimie Murdock", "authors": "Jaimie Murdock and Jiaan Zeng and Colin Allen", "title": "Towards Evaluation of Cultural-scale Claims in Light of Topic Model\n  Sampling Effects", "comments": "2016 International Conference on Computational Social Science\n  (IC2S2), June 23-26, 2016. 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cultural-scale models of full text documents are prone to over-interpretation\nby researchers making unintentionally strong socio-linguistic claims (Pechenick\net al., 2015) without recognizing that even large digital libraries are merely\nsamples of all the books ever produced. In this study, we test the sensitivity\nof the topic models to the sampling process by taking random samples of books\nin the Hathi Trust Digital Library from different areas of the Library of\nCongress Classification Outline. For each classification area, we train several\ntopic models over the entire class with different random seeds, generating a\nset of spanning models. Then, we train topic models on random samples of books\nfrom the classification area, generating a set of sample models. Finally, we\nperform a topic alignment between each pair of models by computing the\nJensen-Shannon distance (JSD) between the word probability distributions for\neach topic. We take two measures on each model alignment: alignment distance\nand topic overlap. We find that sample models with a large sample size\ntypically have an alignment distance that falls in the range of the alignment\ndistance between spanning models. Unsurprisingly, as sample size increases,\nalignment distance decreases. We also find that the topic overlap increases as\nsample size increases. However, the decomposition of these measures by sample\nsize differs by number of topics and by classification area. We speculate that\nthese measures could be used to find classes which have a common \"canon\"\ndiscussed among all books in the area, as shown by high topic overlap and low\nalignment distance even in small sample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 23:07:58 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 21:12:17 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 15:48:16 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Murdock", "Jaimie", ""], ["Zeng", "Jiaan", ""], ["Allen", "Colin", ""]]}, {"id": "1512.05030", "submitter": "Manaal Faruqui", "authors": "Manaal Faruqui and Ryan McDonald and Radu Soricut", "title": "Morpho-syntactic Lexicon Generation Using Graph-based Semi-supervised\n  Learning", "comments": "Transactions of the Association for Computational Linguistics (TACL)\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morpho-syntactic lexicons provide information about the morphological and\nsyntactic roles of words in a language. Such lexicons are not available for all\nlanguages and even when available, their coverage can be limited. We present a\ngraph-based semi-supervised learning method that uses the morphological,\nsyntactic and semantic relations between words to automatically construct wide\ncoverage lexicons from small seed sets. Our method is language-independent, and\nwe show that we can expand a 1000 word seed lexicon to more than 100 times its\nsize with high quality for 11 languages. In addition, the automatically created\nlexicons provide features that improve performance in two downstream tasks:\nmorphological tagging and dependency parsing.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 02:27:14 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 19:02:56 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2016 04:28:46 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Faruqui", "Manaal", ""], ["McDonald", "Ryan", ""], ["Soricut", "Radu", ""]]}, {"id": "1512.05193", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Hinrich Sch\\\"utze, Bing Xiang, Bowen Zhou", "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling\n  Sentence Pairs", "comments": "TACL Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to model a pair of sentences is a critical issue in many NLP tasks such\nas answer selection (AS), paraphrase identification (PI) and textual entailment\n(TE). Most prior work (i) deals with one individual task by fine-tuning a\nspecific system; (ii) models each sentence's representation separately, rarely\nconsidering the impact of the other sentence; or (iii) relies fully on manually\ndesigned, task-specific linguistic features. This work presents a general\nAttention Based Convolutional Neural Network (ABCNN) for modeling a pair of\nsentences. We make three contributions. (i) ABCNN can be applied to a wide\nvariety of tasks that require modeling of sentence pairs. (ii) We propose three\nattention schemes that integrate mutual influence between sentences into CNN;\nthus, the representation of each sentence takes into consideration its\ncounterpart. These interdependent sentence pair representations are more\npowerful than isolated sentence representations. (iii) ABCNN achieves\nstate-of-the-art performance on AS, PI and TE tasks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 14:55:17 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2015 10:39:53 GMT"}, {"version": "v3", "created": "Sat, 9 Apr 2016 11:59:39 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2018 13:31:07 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Yin", "Wenpeng", ""], ["Sch\u00fctze", "Hinrich", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1512.05582", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "Kauffman's adjacent possible in word order evolution", "comments": "Minor corrections (small errors concerning the parameters of model 1,\n  language, style,...) except for the mathematical arguments at the end of\n  section \"Further details about Model 2\" of the supplementary", "journal-ref": "In S.G. Roberts et al (eds.). The Evolution of Language:\n  Proceedings of the 11th International Conference (EVOLANG11). New Orleans,\n  USA, March 21-24 (2016)", "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word order evolution has been hypothesized to be constrained by a word order\npermutation ring: transitions involving orders that are closer in the\npermutation ring are more likely. The hypothesis can be seen as a particular\ncase of Kauffman's adjacent possible in word order evolution. Here we consider\nthe problem of the association of the six possible orders of S, V and O to\nyield a couple of primary alternating orders as a window to word order\nevolution. We evaluate the suitability of various competing hypotheses to\npredict one member of the couple from the other with the help of information\ntheoretic model selection. Our ensemble of models includes a six-way model that\nis based on the word order permutation ring (Kauffman's adjacent possible) and\nanother model based on the dual two-way of standard typology, that reduces word\norder to basic orders preferences (e.g., a preference for SV over VS and\nanother for SO over OS). Our analysis indicates that the permutation ring\nyields the best model when favoring parsimony strongly, providing support for\nKauffman's general view and a six-way typology.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 14:01:14 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2016 10:01:53 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1512.05670", "submitter": "Amrith Krishna", "authors": "Amrith Krishna and Pawan Goyal", "title": "Towards automating the generation of derivative nouns in Sanskrit by\n  simulating Panini", "comments": "16th World Sanskrit Conference, Bangkok June 28th - July 02 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  About 1115 rules in Astadhyayi from A.4.1.76 to A.5.4.160 deal with\ngeneration of derivative nouns, making it one of the largest topical sections\nin Astadhyayi, called as the Taddhita section owing to the head rule A.4.1.76.\nThis section is a systematic arrangement of rules that enumerates various\naffixes that are used in the derivation under specific semantic relations. We\npropose a system that automates the process of generation of derivative nouns\nas per the rules in Astadhyayi. The proposed system follows a completely object\noriented approach, that models each rule as a class of its own and then groups\nthem as rule groups. The rule groups are decided on the basis of selective\ngrouping of rules by virtue of anuvrtti. The grouping of rules results in an\ninheritance network of rules which is a directed acyclic graph. Every rule\ngroup has a head rule and the head rule notifies all the direct member rules of\nthe group about the environment which contains all the details about data\nentities, participating in the derivation process. The system implements this\nmechanism using multilevel inheritance and observer design patterns. The system\nfocuses not only on generation of the desired final form, but also on the\ncorrectness of sequence of rules applied to make sure that the derivation has\ntaken place in strict adherence to Astadhyayi. The proposed system's design\nallows to incorporate various conflict resolution methods mentioned in\nauthentic texts and hence the effectiveness of those rules can be validated\nwith the results from the system. We also present cases where we have checked\nthe applicability of the system with the rules which are not specifically\napplicable to derivation of derivative nouns, in order to see the effectiveness\nof the proposed schema as a generic system for modeling Astadhyayi.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 16:55:57 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 10:07:35 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Krishna", "Amrith", ""], ["Goyal", "Pawan", ""]]}, {"id": "1512.05726", "submitter": "Tao Lei", "authors": "Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi Jaakkola, Katerina\n  Tymoshenko, Alessandro Moschitti, Lluis Marquez", "title": "Semi-supervised Question Retrieval with Gated Convolutions", "comments": "NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering forums are rapidly growing in size with no effective\nautomated ability to refer to and reuse answers already available for previous\nposted questions. In this paper, we develop a methodology for finding\nsemantically related questions. The task is difficult since 1) key pieces of\ninformation are often buried in extraneous details in the question body and 2)\navailable annotations on similar questions are scarce and fragmented. We design\na recurrent and convolutional model (gated convolution) to effectively map\nquestions to their semantic representations. The models are pre-trained within\nan encoder-decoder framework (from body to title) on the basis of the entire\nraw corpus, and fine-tuned discriminatively from limited annotations. Our\nevaluation demonstrates that our model yields substantial gains over a standard\nIR baseline and various neural network architectures (including CNNs, LSTMs and\nGRUs).\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 19:14:20 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 00:29:15 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Lei", "Tao", ""], ["Joshi", "Hrishikesh", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""], ["Tymoshenko", "Katerina", ""], ["Moschitti", "Alessandro", ""], ["Marquez", "Lluis", ""]]}, {"id": "1512.05742", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin,\n  Joelle Pineau", "title": "A Survey of Available Corpora for Building Data-Driven Dialogue Systems", "comments": "56 pages including references and appendix, 5 tables and 1 figure;\n  Under review for the Dialogue & Discourse journal. Update: paper has been\n  rewritten and now includes several new datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, several areas of speech and language understanding\nhave witnessed substantial breakthroughs from the use of data-driven models. In\nthe area of dialogue systems, the trend is less obvious, and most practical\nsystems are still built through significant engineering and expert knowledge.\nNevertheless, several recent results suggest that data-driven approaches are\nfeasible and quite promising. To facilitate research in this area, we have\ncarried out a wide survey of publicly available datasets suitable for\ndata-driven learning of dialogue systems. We discuss important characteristics\nof these datasets, how they can be used to learn diverse dialogue strategies,\nand their other potential uses. We also examine methods for transfer learning\nbetween datasets and the use of external knowledge. Finally, we discuss\nappropriate choice of evaluation metrics for the learning objective.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 19:52:39 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 04:58:05 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 01:15:32 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Lowe", "Ryan", ""], ["Henderson", "Peter", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1512.05919", "submitter": "Duyu Tang", "authors": "Bing Qin, Duyu Tang, Xinwei Geng, Dandan Ning, Jiahao Liu and Ting Liu", "title": "A Planning based Framework for Essay Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating an article automatically with computer program is a challenging\ntask in artificial intelligence and natural language processing. In this paper,\nwe target at essay generation, which takes as input a topic word in mind and\ngenerates an organized article under the theme of the topic. We follow the idea\nof text planning \\cite{Reiter1997} and develop an essay generation framework.\nThe framework consists of three components, including topic understanding,\nsentence extraction and sentence reordering. For each component, we studied\nseveral statistical algorithms and empirically compared between them in terms\nof qualitative or quantitative analysis. Although we run experiments on Chinese\ncorpus, the method is language independent and can be easily adapted to other\nlanguage. We lay out the remaining challenges and suggest avenues for future\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 12:10:42 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 07:49:07 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Qin", "Bing", ""], ["Tang", "Duyu", ""], ["Geng", "Xinwei", ""], ["Ning", "Dandan", ""], ["Liu", "Jiahao", ""], ["Liu", "Ting", ""]]}, {"id": "1512.06110", "submitter": "Manaal Faruqui", "authors": "Manaal Faruqui and Yulia Tsvetkov and Graham Neubig and Chris Dyer", "title": "Morphological Inflection Generation Using Character Sequence to Sequence\n  Learning", "comments": "Proceedings of NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological inflection generation is the task of generating the inflected\nform of a given lemma corresponding to a particular linguistic transformation.\nWe model the problem of inflection generation as a character sequence to\nsequence learning problem and present a variant of the neural encoder-decoder\nmodel for solving it. Our model is language independent and can be trained in\nboth supervised and semi-supervised settings. We evaluate our system on seven\ndatasets of morphologically rich languages and achieve either better or\ncomparable results to existing state-of-the-art models of inflection\ngeneration.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 20:48:26 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2015 17:23:32 GMT"}, {"version": "v3", "created": "Tue, 22 Mar 2016 01:02:01 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Faruqui", "Manaal", ""], ["Tsvetkov", "Yulia", ""], ["Neubig", "Graham", ""], ["Dyer", "Chris", ""]]}, {"id": "1512.06612", "submitter": "Lili Mou", "authors": "Lili Mou, Rui Yan, Ge Li, Lu Zhang, Zhi Jin", "title": "Backward and Forward Language Modeling for Constrained Sentence\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent language models, especially those based on recurrent neural networks\n(RNNs), make it possible to generate natural language from a learned\nprobability. Language generation has wide applications including machine\ntranslation, summarization, question answering, conversation systems, etc.\nExisting methods typically learn a joint probability of words conditioned on\nadditional information, which is (either statically or dynamically) fed to\nRNN's hidden layer. In many applications, we are likely to impose hard\nconstraints on the generated texts, i.e., a particular word must appear in the\nsentence. Unfortunately, existing approaches could not solve this problem. In\nthis paper, we propose a novel backward and forward language model. Provided a\nspecific word, we use RNNs to generate previous words and future words, either\nsimultaneously or asynchronously, resulting in two model variants. In this way,\nthe given word could appear at any position in the sentence. Experimental\nresults show that the generated texts are comparable to sequential LMs in\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 13:07:31 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2016 20:15:44 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Mou", "Lili", ""], ["Yan", "Rui", ""], ["Li", "Ge", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1512.06643", "submitter": "Mortaza Doulaty", "authors": "Oscar Saz, Mortaza Doulaty, Salil Deena, Rosanna Milner, Raymond W.M.\n  Ng, Madina Hasan, Yulan Liu, Thomas Hain", "title": "The 2015 Sheffield System for Transcription of Multi-Genre Broadcast\n  Media", "comments": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU\n  2015), 13-17 Dec 2015, Scottsdale, Arizona, USA", "journal-ref": null, "doi": "10.1109/ASRU.2015.7404854", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the University of Sheffield system for participation in the 2015\nMulti-Genre Broadcast (MGB) challenge task of transcribing multi-genre\nbroadcast shows. Transcription was one of four tasks proposed in the MGB\nchallenge, with the aim of advancing the state of the art of automatic speech\nrecognition, speaker diarisation and automatic alignment of subtitles for\nbroadcast media. Four topics are investigated in this work: Data selection\ntechniques for training with unreliable data, automatic speech segmentation of\nbroadcast media shows, acoustic modelling and adaptation in highly variable\nenvironments, and language modelling of multi-genre shows. The final system\noperates in multiple passes, using an initial unadapted decoding stage to\nrefine segmentation, followed by three adapted passes: a hybrid DNN pass with\ninput features normalised by speaker-based cepstral normalisation, another\nhybrid stage with input features normalised by speaker feature-MLLR\ntransformations, and finally a bottleneck-based tandem stage with noise and\nspeaker factorisation. The combination of these three system outputs provides a\nfinal error rate of 27.5% on the official development set, consisting of 47\nmulti-genre shows.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 14:31:31 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Saz", "Oscar", ""], ["Doulaty", "Mortaza", ""], ["Deena", "Salil", ""], ["Milner", "Rosanna", ""], ["Ng", "Raymond W. M.", ""], ["Hasan", "Madina", ""], ["Liu", "Yulan", ""], ["Hain", "Thomas", ""]]}, {"id": "1512.07046", "submitter": "Jan Rupnik", "authors": "Jan Rupnik, Andrej Muhic, Gregor Leban, Primoz Skraba, Blaz Fortuna,\n  Marko Grobelnik", "title": "News Across Languages - Cross-Lingual Document Similarity and Event\n  Tracking", "comments": "Accepted for publication in Journal of Artificial Intelligence\n  Research, Special Track on Cross-language Algorithms and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's world, we follow news which is distributed globally. Significant\nevents are reported by different sources and in different languages. In this\nwork, we address the problem of tracking of events in a large multilingual\nstream. Within a recently developed system Event Registry we examine two\naspects of this problem: how to compare articles in different languages and how\nto link collections of articles in different languages which refer to the same\nevent. Taking a multilingual stream and clusters of articles from each\nlanguage, we compare different cross-lingual document similarity measures based\non Wikipedia. This allows us to compute the similarity of any two articles\nregardless of language. Building on previous work, we show there are methods\nwhich scale well and can compute a meaningful similarity between articles from\nlanguages with little or no direct overlap in the training data. Using this\ncapability, we then propose an approach to link clusters of articles across\nlanguages which represent the same event. We provide an extensive evaluation of\nthe system as a whole, as well as an evaluation of the quality and robustness\nof the similarity measure and the linking algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 12:11:32 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Rupnik", "Jan", ""], ["Muhic", "Andrej", ""], ["Leban", "Gregor", ""], ["Skraba", "Primoz", ""], ["Fortuna", "Blaz", ""], ["Grobelnik", "Marko", ""]]}, {"id": "1512.07281", "submitter": "Bruno Gon\\c{c}alves", "authors": "Qian Zhang, Bruno Gon\\c{c}alves", "title": "Topical differences between Chinese language Twitter and Sina Weibo", "comments": "5 pages, 2 figures, 2 tables, 2 algorithms", "journal-ref": "2nd Natural Language Processing for Informal Text (NLPIT), 625\n  (2016)", "doi": "10.1145/2872518.2890562", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sina Weibo, China's most popular microblogging platform, is currently used by\nover $500M$ users and is considered to be a proxy of Chinese social life. In\nthis study, we contrast the discussions occurring on Sina Weibo and on Chinese\nlanguage Twitter in order to observe two different strands of Chinese culture:\npeople within China who use Sina Weibo with its government imposed restrictions\nand those outside that are free to speak completely anonymously. We first\npropose a simple ad-hoc algorithm to identify topics of Tweets and Weibo.\nDifferent from previous works on micro-message topic detection, our algorithm\nconsiders topics of the same contents but with different \\#tags. Our algorithm\ncan also detect topics for Tweets and Weibos without any \\#tags. Using a large\ncorpus of Weibo and Chinese language tweets, covering the period from January\n$1$ to December $31$, $2012$, we obtain a list of topics using clustered \\#tags\nthat we can then use to compare the two platforms. Surprisingly, we find that\nthere are no common entries among the Top $100$ most popular topics.\nFurthermore, only $9.2\\%$ of tweets correspond to the Top $1000$ topics on Sina\nWeibo platform, and conversely only $4.4\\%$ of weibos were found to discuss the\nmost popular Twitter topics. Our results reveal significant differences in\nsocial attention on the two platforms, with most popular topics on Sina Weibo\nrelating to entertainment while most tweets corresponded to cultural or\npolitical contents that is practically non existent in Sina Weibo.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 22:13:31 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Zhang", "Qian", ""], ["Gon\u00e7alves", "Bruno", ""]]}, {"id": "1512.07685", "submitter": "EPTCS", "authors": "Nurulhuda A. Manaf (Computer Science, University of Surrey), Sotiris\n  Moschoyiannis (Computer Science, University of Surrey), Paul Krause (Computer\n  Science, University of Surrey)", "title": "Service Choreography, SBVR, and Time", "comments": "In Proceedings FOCLASA 2015, arXiv:1512.06947", "journal-ref": "EPTCS 201, 2015, pp. 63-77", "doi": "10.4204/EPTCS.201.5", "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of structured natural language (English) in specifying\nservice choreographies, focusing on the what rather than the how of the\nrequired coordination of participant services in realising a business\napplication scenario. The declarative approach we propose uses the OMG standard\nSemantics of Business Vocabulary and Rules (SBVR) as a modelling language. The\nservice choreography approach has been proposed for describing the global\norderings of the invocations on interfaces of participant services. We\ntherefore extend SBVR with a notion of time which can capture the coordination\nof the participant services, in terms of the observable message exchanges\nbetween them. The extension is done using existing modelling constructs in\nSBVR, and hence respects the standard specification. The idea is that users -\ndomain specialists rather than implementation specialists - can verify the\nrequested service composition by directly reading the structured English used\nby SBVR. At the same time, the SBVR model can be represented in formal logic so\nit can be parsed and executed by a machine.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 01:40:20 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Manaf", "Nurulhuda A.", "", "Computer Science, University of Surrey"], ["Moschoyiannis", "Sotiris", "", "Computer Science, University of Surrey"], ["Krause", "Paul", "", "Computer\n  Science, University of Surrey"]]}, {"id": "1512.08066", "submitter": "Chung-Hyok Jang", "authors": "Chung-Hyok Jang (1), Kwang-Hyok Kim (2) ((1) Foreign Language Faculty,\n  Kim Il Sung University, (2) Computer Science College, Kim Il Sung University)", "title": "The Improvement of Negative Sentences Translation in English-to-Korean\n  Machine Translation", "comments": "9 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the algorithm for translating English negative sentences\ninto Korean in English-Korean Machine Translation (EKMT). The proposed\nalgorithm is based on the comparative study of English and Korean negative\nsentences. The earlier translation software cannot translate English negative\nsentences into accurate Korean equivalents. We established a new algorithm for\nthe negative sentence translation and evaluated it.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 01:52:03 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Jang", "Chung-Hyok", ""], ["Kim", "Kwang-Hyok", ""]]}, {"id": "1512.08183", "submitter": "Bofang Li", "authors": "Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao", "title": "Learning Document Embeddings by Predicting N-grams for Sentiment\n  Classification of Long Movie Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the loss of semantic information, bag-of-ngram based methods still\nachieve state-of-the-art results for tasks such as sentiment classification of\nlong movie reviews. Many document embeddings methods have been proposed to\ncapture semantics, but they still can't outperform bag-of-ngram based methods\non this task. In this paper, we modify the architecture of the recently\nproposed Paragraph Vector, allowing it to learn document vectors by predicting\nnot only words, but n-gram features as well. Our model is able to capture both\nsemantics and word order in documents while keeping the expressive power of\nlearned vectors. Experimental results on IMDB movie review dataset shows that\nour model outperforms previous deep learning models and bag-of-ngram based\nmodels due to the above advantages. More robust results are also obtained when\nour model is combined with other models. The source code of our model will be\nalso published together with this paper.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 08:12:53 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 09:03:13 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 10:54:47 GMT"}, {"version": "v4", "created": "Wed, 6 Apr 2016 14:21:56 GMT"}, {"version": "v5", "created": "Sat, 23 Apr 2016 16:00:48 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Li", "Bofang", ""], ["Liu", "Tao", ""], ["Du", "Xiaoyong", ""], ["Zhang", "Deyuan", ""], ["Zhao", "Zhe", ""]]}, {"id": "1512.08347", "submitter": "Yang Lou Mr", "authors": "Yang Lou, Guanrong Chen and Jianwei Hu", "title": "Communicating with sentences: A multi-word naming game model", "comments": "19 pagesa, 11 figures, Physica A (2017)", "journal-ref": "Physica A 490 (2018) 857-868", "doi": "10.1016/j.physa.2017.08.066", "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naming game simulates the process of naming an object by a single word, in\nwhich a population of communicating agents can reach global consensus\nasymptotically through iteratively pair-wise conversations. We propose an\nextension of the single-word model to a multi-word naming game (MWNG),\nsimulating the case of describing a complex object by a sentence (multiple\nwords). Words are defined in categories, and then organized as sentences by\ncombining them from different categories. We refer to a formatted combination\nof several words as a pattern. In such an MWNG, through a pair-wise\nconversation, it requires the hearer to achieve consensus with the speaker with\nrespect to both every single word in the sentence as well as the sentence\npattern, so as to guarantee the correct meaning of the saying, otherwise, they\nfail reaching consensus in the interaction. We validate the model in three\ntypical topologies as the underlying communication network, and employ both\nconventional and man-designed patterns in performing the MWNG.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 08:50:50 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2016 03:22:25 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 13:23:58 GMT"}, {"version": "v4", "created": "Mon, 18 Sep 2017 14:28:44 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lou", "Yang", ""], ["Chen", "Guanrong", ""], ["Hu", "Jianwei", ""]]}, {"id": "1512.08422", "submitter": "Lili Mou", "authors": "Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, Zhi Jin", "title": "Natural Language Inference by Tree-Based Convolution and Heuristic\n  Matching", "comments": "Accepted by ACL'16 as a short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the TBCNN-pair model to recognize entailment and\ncontradiction between two sentences. In our model, a tree-based convolutional\nneural network (TBCNN) captures sentence-level semantics; then heuristic\nmatching layers like concatenation, element-wise product/difference combine the\ninformation in individual sentences. Experimental results show that our model\noutperforms existing sentence encoding-based approaches by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 14:28:21 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 17:49:46 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 16:24:56 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Mou", "Lili", ""], ["Men", "Rui", ""], ["Li", "Ge", ""], ["Xu", "Yan", ""], ["Zhang", "Lu", ""], ["Yan", "Rui", ""], ["Jin", "Zhi", ""]]}, {"id": "1512.08569", "submitter": "Roger Bilisoly", "authors": "Roger Bilisoly", "title": "Analyzing Walter Skeat's Forty-Five Parallel Extracts of William\n  Langland's Piers Plowman", "comments": "Presented at the Joint Statistical Meetings 2015 in Seattle,\n  Washington. 9 pages long", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Walter Skeat published his critical edition of William Langland's 14th\ncentury alliterative poem, Piers Plowman, in 1886. In preparation for this he\nlocated forty-five manuscripts, and to compare dialects, he published excerpts\nfrom each of these. This paper does three statistical analyses using these\nexcerpts, each of which mimics a task he did in writing his critical edition.\nFirst, he combined multiple versions of a poetic line to create a best line,\nwhich is compared to the mean string that is computed by a generalization of\nthe arithmetic mean that uses edit distance. Second, he claims that a certain\nsubset of manuscripts varies little. This is quantified by computing a string\nvariance, which is closely related to the above generalization of the mean.\nThird, he claims that the manuscripts fall into three groups, which is a\nclustering problem that is addressed by using edit distance. The overall goal\nis to develop methodology that would be of use to a literary critic.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 00:39:52 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Bilisoly", "Roger", ""]]}, {"id": "1512.08849", "submitter": "Shuohang Wang", "authors": "Shuohang Wang and Jing Jiang", "title": "Learning Natural Language Inference with LSTM", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language inference (NLI) is a fundamentally important task in natural\nlanguage processing that has many applications. The recently released Stanford\nNatural Language Inference (SNLI) corpus has made it possible to develop and\nevaluate learning-centered methods such as deep neural networks for natural\nlanguage inference (NLI). In this paper, we propose a special long short-term\nmemory (LSTM) architecture for NLI. Our model builds on top of a recently\nproposed neural attention model for NLI but is based on a significantly\ndifferent idea. Instead of deriving sentence embeddings for the premise and the\nhypothesis to be used for classification, our solution uses a match-LSTM to\nperform word-by-word matching of the hypothesis with the premise. This LSTM is\nable to place more emphasis on important word-level matching results. In\nparticular, we observe that this LSTM remembers important mismatches that are\ncritical for predicting the contradiction or the neutral relationship label. On\nthe SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 05:02:53 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 11:54:29 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Wang", "Shuohang", ""], ["Jiang", "Jing", ""]]}, {"id": "1512.08903", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang, Minjae Lee, Wonyong Sung", "title": "Online Keyword Spotting with a Character-Level Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a context-aware keyword spotting model employing a\ncharacter-level recurrent neural network (RNN) for spoken term detection in\ncontinuous speech. The RNN is end-to-end trained with connectionist temporal\nclassification (CTC) to generate the probabilities of character and\nword-boundary labels. There is no need for the phonetic transcription, senone\nmodeling, or system dictionary in training and testing. Also, keywords can\neasily be added and modified by editing the text based keyword list without\nretraining the RNN. Moreover, the unidirectional RNN processes an infinitely\nlong input audio streams without pre-segmentation and keywords are detected\nwith low-latency before the utterance is finished. Experimental results show\nthat the proposed keyword spotter significantly outperforms the deep neural\nnetwork (DNN) and hidden Markov model (HMM) based keyword-filler model even\nwith less computations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 10:32:12 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Lee", "Minjae", ""], ["Sung", "Wonyong", ""]]}, {"id": "1512.08982", "submitter": "Sucheta Ghosh", "authors": "Sucheta Ghosh", "title": "Technical Report: a tool for measuring Prosodic Accommodation", "comments": "Withdrawn by arXiv administrators", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article has been withdrawn by arXiv administrators because the submitter\ndid not have the legal authority to grant the license applied to the work.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 15:46:30 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Ghosh", "Sucheta", ""]]}]