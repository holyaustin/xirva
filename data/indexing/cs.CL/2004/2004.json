[{"id": "2004.00033", "submitter": "Rodrigo Agerri", "authors": "Rodrigo Agerri, I\\~naki San Vicente, Jon Ander Campos, Ander Barrena,\n  Xabier Saralegi, Aitor Soroa, Eneko Agirre", "title": "Give your Text Representation Models some Love: the Case for Basque", "comments": "Accepted at LREC 2020; 8 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word embeddings and pre-trained language models allow to build rich\nrepresentations of text and have enabled improvements across most NLP tasks.\nUnfortunately they are very expensive to train, and many small companies and\nresearch groups tend to use models that have been pre-trained and made\navailable by third parties, rather than building their own. This is suboptimal\nas, for many languages, the models have been trained on smaller (or lower\nquality) corpora. In addition, monolingual pre-trained models for non-English\nlanguages are not always available. At best, models for those languages are\nincluded in multilingual versions, where each language shares the quota of\nsubstrings and parameters with the rest of the languages. This is particularly\ntrue for smaller languages such as Basque. In this paper we show that a number\nof monolingual models (FastText word embeddings, FLAIR and BERT language\nmodels) trained with larger Basque corpora produce much better results than\npublicly available versions in downstream NLP tasks, including topic\nclassification, sentiment classification, PoS tagging and NER. This work sets a\nnew state-of-the-art in those tasks for Basque. All benchmarks and models used\nin this work are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:01:56 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 11:46:52 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Agerri", "Rodrigo", ""], ["Vicente", "I\u00f1aki San", ""], ["Campos", "Jon Ander", ""], ["Barrena", "Ander", ""], ["Saralegi", "Xabier", ""], ["Soroa", "Aitor", ""], ["Agirre", "Eneko", ""]]}, {"id": "2004.00050", "submitter": "Rodrigo Agerri", "authors": "Elena Zotova, Rodrigo Agerri, Manuel Nu\\~nez, German Rigau", "title": "Multilingual Stance Detection: The Catalonia Independence Corpus", "comments": "Accepted at LREC 2020; 8 pages 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stance detection aims to determine the attitude of a given text with respect\nto a specific topic or claim. While stance detection has been fairly well\nresearched in the last years, most the work has been focused on English. This\nis mainly due to the relative lack of annotated data in other languages. The\nTW-10 Referendum Dataset released at IberEval 2018 is a previous effort to\nprovide multilingual stance-annotated data in Catalan and Spanish.\nUnfortunately, the TW-10 Catalan subset is extremely imbalanced. This paper\naddresses these issues by presenting a new multilingual dataset for stance\ndetection in Twitter for the Catalan and Spanish languages, with the aim of\nfacilitating research on stance detection in multilingual and cross-lingual\nsettings. The dataset is annotated with stance towards one topic, namely, the\nindependence of Catalonia. We also provide a semi-automatic method to annotate\nthe dataset based on a categorization of Twitter users. We experiment on the\nnew corpus with a number of supervised approaches, including linear classifiers\nand deep learning methods. Comparison of our new corpus with the with the TW-1O\ndataset shows both the benefits and potential of a well balanced corpus for\nmultilingual and cross-lingual research on stance detection. Finally, we\nestablish new state-of-the-art results on the TW-10 dataset, both for Catalan\nand Spanish.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:28:36 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zotova", "Elena", ""], ["Agerri", "Rodrigo", ""], ["Nu\u00f1ez", "Manuel", ""], ["Rigau", "German", ""]]}, {"id": "2004.00053", "submitter": "Congzheng Song", "authors": "Congzheng Song and Ananth Raghunathan", "title": "Information Leakage in Embedding Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings are functions that map raw input data to low-dimensional vector\nrepresentations, while preserving important semantic information about the\ninputs. Pre-training embeddings on a large amount of unlabeled data and\nfine-tuning them for downstream tasks is now a de facto standard in achieving\nstate of the art learning in many domains.\n  We demonstrate that embeddings, in addition to encoding generic semantics,\noften also present a vector that leaks sensitive information about the input\ndata. We develop three classes of attacks to systematically study information\nthat might be leaked by embeddings. First, embedding vectors can be inverted to\npartially recover some of the input data. As an example, we show that our\nattacks on popular sentence embeddings recover between 50\\%--70\\% of the input\nwords (F1 scores of 0.5--0.7). Second, embeddings may reveal sensitive\nattributes inherent in inputs and independent of the underlying semantic task\nat hand. Attributes such as authorship of text can be easily extracted by\ntraining an inference model on just a handful of labeled embedding vectors.\nThird, embedding models leak moderate amount of membership information for\ninfrequent training data inputs. We extensively evaluate our attacks on various\nstate-of-the-art embedding models in the text domain. We also propose and\nevaluate defenses that can prevent the leakage to some extent at a minor cost\nin utility.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:33:36 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 19:58:14 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Song", "Congzheng", ""], ["Raghunathan", "Ananth", ""]]}, {"id": "2004.00061", "submitter": "Marco Valentino", "authors": "Marco Valentino, Mokanarangan Thayaparan, Andr\\'e Freitas", "title": "Unification-based Reconstruction of Multi-hop Explanations for Science\n  Questions", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for reconstructing multi-hop\nexplanations in science Question Answering (QA). While existing approaches for\nmulti-hop reasoning build explanations considering each question in isolation,\nwe propose a method to leverage explanatory patterns emerging in a corpus of\nscientific explanations. Specifically, the framework ranks a set of atomic\nfacts by integrating lexical relevance with the notion of unification power,\nestimated analysing explanations for similar questions in the corpus.\n  An extensive evaluation is performed on the Worldtree corpus, integrating\nk-NN clustering and Information Retrieval (IR) techniques. We present the\nfollowing conclusions: (1) The proposed method achieves results competitive\nwith Transformers, yet being orders of magnitude faster, a feature that makes\nit scalable to large explanatory corpora (2) The unification-based mechanism\nhas a key role in reducing semantic drift, contributing to the reconstruction\nof many hops explanations (6 or more facts) and the ranking of complex\ninference facts (+12.0 Mean Average Precision) (3) Crucially, the constructed\nexplanations can support downstream QA models, improving the accuracy of BERT\nby up to 10% overall.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 19:07:51 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 09:32:05 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Valentino", "Marco", ""], ["Thayaparan", "Mokanarangan", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2004.00068", "submitter": "Allahsera Auguste Tapo", "authors": "Michael Leventhal, Allahsera Tapo, Sarah Luger, Marcos Zampieri, and\n  Christopher M. Homan", "title": "Assessing Human Translations from French to Bambara for Machine\n  Learning: a Pilot Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel methods for assessing the quality of human-translated\naligned texts for learning machine translation models of under-resourced\nlanguages. Malian university students translated French texts, producing either\nwritten or oral translations to Bambara. Our results suggest that similar\nquality can be obtained from either written or spoken translations for certain\nkinds of texts. They also suggest specific instructions that human translators\nshould be given in order to improve the quality of their work.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 19:28:35 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Leventhal", "Michael", ""], ["Tapo", "Allahsera", ""], ["Luger", "Sarah", ""], ["Zampieri", "Marcos", ""], ["Homan", "Christopher M.", ""]]}, {"id": "2004.00088", "submitter": "Jia Xu Dr.", "authors": "Abdul Rafae Khan, Asim Karim, Hassan Sajjad, Faisal Kamiran, and Jia\n  Xu", "title": "A Clustering Framework for Lexical Normalization of Roman Urdu", "comments": null, "journal-ref": null, "doi": "10.1017/S1351324920000285", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roman Urdu is an informal form of the Urdu language written in Roman script,\nwhich is widely used in South Asia for online textual content. It lacks\nstandard spelling and hence poses several normalization challenges during\nautomatic language processing. In this article, we present a feature-based\nclustering framework for the lexical normalization of Roman Urdu corpora, which\nincludes a phonetic algorithm UrduPhone, a string matching component, a\nfeature-based similarity function, and a clustering algorithm Lex-Var.\nUrduPhone encodes Roman Urdu strings to their pronunciation-based\nrepresentations. The string matching component handles character-level\nvariations that occur when writing Urdu using Roman script.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 20:21:55 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Khan", "Abdul Rafae", ""], ["Karim", "Asim", ""], ["Sajjad", "Hassan", ""], ["Kamiran", "Faisal", ""], ["Xu", "Jia", ""]]}, {"id": "2004.00089", "submitter": "Arijit Das", "authors": "Arijit Das, Tapas Halder and Diganta Saha", "title": "Automatic Extraction of Bengali Root Verbs using Paninian Grammar", "comments": "published in 2017 2nd IEEE International Conference on Recent Trends\n  in Electronics, Information & Communication Technology (RTEICT), Bangalore,\n  2017", "journal-ref": null, "doi": "10.1109/RTEICT.2017.8256739", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research work, we have proposed an algorithm based on supervised\nlearning methodology to extract the root forms of the Bengali verbs using the\ngrammatical rules proposed by Panini [1] in Ashtadhyayi. This methodology can\nbe applied for the languages which are derived from Sanskrit. The proposed\nsystem has been developed based on tense, person and morphological inflections\nof the verbs to find their root forms. The work has been executed in two\nphases: first, the surface level forms or inflected forms of the verbs have\nbeen classified into a certain number of groups of similar tense and person.\nFor this task, a standard pattern, available in Bengali language has been used.\nNext, a set of rules have been applied to extract the root form from the\nsurface level forms of a verb. The system has been tested on 10000 verbs\ncollected from the Bengali text corpus developed in the TDIL project of the\nGovt. of India. The accuracy of the output has been achieved 98% which is\nverified by a linguistic expert. Root verb identification is a key step in\nsemantic searching, multi-sentence search query processing, understanding the\nmeaning of a language, disambiguation of word sense, classification of the\nsentences etc.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 20:22:10 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Das", "Arijit", ""], ["Halder", "Tapas", ""], ["Saha", "Diganta", ""]]}, {"id": "2004.00104", "submitter": "Arijit Das", "authors": "Arijit Das and Diganta Saha", "title": "Improvement of electronic Governance and mobile Governance in\n  Multilingual Countries with Digital Etymology using Sanskrit Grammar", "comments": "7 pages. 2017 IEEE Region 10 Humanitarian Technology Conference\n  (R10-HTC), Dhaka, 2017", "journal-ref": null, "doi": "10.1109/R10-HTC.2017.8289008", "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With huge improvement of digital connectivity (Wifi,3G,4G) and digital\ndevices access to internet has reached in the remotest corners now a days.\nRural people can easily access web or apps from PDAs, laptops, smartphones etc.\nThis is an opportunity of the Government to reach to the citizen in large\nnumber, get their feedback, associate them in policy decision with e governance\nwithout deploying huge man, material or resourses. But the Government of\nmultilingual countries face a lot of problem in successful implementation of\nGovernment to Citizen (G2C) and Citizen to Government (C2G) governance as the\nrural people tend and prefer to interact in their native languages. Presenting\nequal experience over web or app to different language group of speakers is a\nreal challenge. In this research we have sorted out the problems faced by Indo\nAryan speaking netizens which is in general also applicable to any language\nfamily groups or subgroups. Then we have tried to give probable solutions using\nEtymology. Etymology is used to correlate the words using their ROOT forms. In\n5th century BC Panini wrote Astadhyayi where he depicted sutras or rules -- how\na word is changed according to person,tense,gender,number etc. Later this book\nwas followed in Western countries also to derive their grammar of comparatively\nnew languages. We have trained our system for automatic root extraction from\nthe surface level or morphed form of words using Panian Gramatical rules. We\nhave tested our system over 10000 bengali Verbs and extracted the root form\nwith 98% accuracy. We are now working to extend the program to successfully\nlemmatize any words of any language and correlate them by applying those rule\nsets in Artificial Neural Network.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 20:58:14 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Das", "Arijit", ""], ["Saha", "Diganta", ""]]}, {"id": "2004.00132", "submitter": "Jo\\~ao Ant\\^onio Chagas Nunes", "authors": "Jo\\~ao Ant\\^onio Chagas Nunes, David Mac\\^edo, Cleber Zanchettin", "title": "AM-MobileNet1D: A Portable Model for Speaker Recognition", "comments": "2020 International Joint Conference on Neural Networks (IJCNN)", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN48605.2020.9207519", "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker Recognition and Speaker Identification are challenging tasks with\nessential applications such as automation, authentication, and security. Deep\nlearning approaches like SincNet and AM-SincNet presented great results on\nthese tasks. The promising performance took these models to real-world\napplications that becoming fundamentally end-user driven and mostly mobile. The\nmobile computation requires applications with reduced storage size,\nnon-processing and memory intensive and efficient energy-consuming. The deep\nlearning approaches, in contrast, usually are energy expensive, demanding\nstorage, processing power, and memory. To address this demand, we propose a\nportable model called Additive Margin MobileNet1D (AM-MobileNet1D) to Speaker\nIdentification on mobile devices. We evaluated the proposed approach on TIMIT\nand MIT datasets obtaining equivalent or better performances concerning the\nbaseline methods. Additionally, the proposed model takes only 11.6 megabytes on\ndisk storage against 91.2 from SincNet and AM-SincNet architectures, making the\nmodel seven times faster, with eight times fewer parameters.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 21:42:59 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Nunes", "Jo\u00e3o Ant\u00f4nio Chagas", ""], ["Mac\u00eado", "David", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "2004.00139", "submitter": "Larissa Schmidt", "authors": "Larissa Schmidt (1), Lucy Linder (2), Sandra Djambazovska (3),\n  Alexandros Lazaridis (3), Tanja Samard\\v{z}i\\'c (1), and Claudiu Musat (3)\n  ((1) University of Zurich: URPP Language and Space, (2) University of\n  Fribourg, (3) Swisscom AG: Data Analytics & AI (DNA))", "title": "A Swiss German Dictionary: Variation in Speech and Writing", "comments": "6 pages, 1 figure, 2 tables. To be published in: Proceedings of the\n  12th International Conference on Language Resources and Evaluation (LREC\n  2020). Marseille, France. For project reports and to obtain the dictionary\n  see http://tiny.uzh.ch/11X", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a dictionary containing forms of common words in various Swiss\nGerman dialects normalized into High German. As Swiss German is, for now, a\npredominantly spoken language, there is a significant variation in the written\nforms, even between speakers of the same dialect. To alleviate the uncertainty\nassociated with this diversity, we complement the pairs of Swiss German - High\nGerman words with the Swiss German phonetic transcriptions (SAMPA). This\ndictionary becomes thus the first resource to combine large-scale spontaneous\ntranslation with phonetic transcriptions. Moreover, we control for the regional\ndistribution and insure the equal representation of the major Swiss dialects.\nThe coupling of the phonetic and written Swiss German forms is powerful. We\nshow that they are sufficient to train a Transformer-based phoneme to grapheme\nmodel that generates credible novel Swiss German writings. In addition, we show\nthat the inverse mapping - from graphemes to phonemes - can be modeled with a\ntransformer trained with the novel dictionary. This generation of\npronunciations for previously unknown words is key in training extensible\nautomated speech recognition (ASR) systems, which are key beneficiaries of this\ndictionary.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 22:10:43 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Schmidt", "Larissa", ""], ["Linder", "Lucy", ""], ["Djambazovska", "Sandra", ""], ["Lazaridis", "Alexandros", ""], ["Samard\u017ei\u0107", "Tanja", ""], ["Musat", "Claudiu", ""]]}, {"id": "2004.00150", "submitter": "Mohammed Ibrahim", "authors": "Mohammed Ibrahim, Susan Gauch, Omar Salman, Mohammed Alqahatani", "title": "Enriching Consumer Health Vocabulary Using Enhanced GloVe Word Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-Access and Collaborative Consumer Health Vocabulary (OAC CHV, or CHV for\nshort), is a collection of medical terms written in plain English. It provides\na list of simple, easy, and clear terms that laymen prefer to use rather than\nan equivalent professional medical term. The National Library of Medicine (NLM)\nhas integrated and mapped the CHV terms to their Unified Medical Language\nSystem (UMLS). These CHV terms mapped to 56000 professional concepts on the\nUMLS. We found that about 48% of these laymen's terms are still jargon and\nmatched with the professional terms on the UMLS. In this paper, we present an\nenhanced word embedding technique that generates new CHV terms from a\nconsumer-generated text. We downloaded our corpus from a healthcare social\nmedia and evaluated our new method based on iterative feedback to word\nembedding using ground truth built from the existing CHV terms. Our feedback\nalgorithm outperformed unmodified GLoVe and new CHV terms have been detected.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 22:50:24 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 18:02:10 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ibrahim", "Mohammed", ""], ["Gauch", "Susan", ""], ["Salman", "Omar", ""], ["Alqahatani", "Mohammed", ""]]}, {"id": "2004.00248", "submitter": "Jiangyan Yi", "authors": "Jiangyan Yi, Jianhua Tao, Ye Bai, Zhengkun Tian, Cunhang Fan", "title": "Adversarial Transfer Learning for Punctuation Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies demonstrate that word embeddings and part-of-speech (POS)\ntags are helpful for punctuation restoration tasks. However, two drawbacks\nstill exist. One is that word embeddings are pre-trained by unidirectional\nlanguage modeling objectives. Thus the word embeddings only contain\nleft-to-right context information. The other is that POS tags are provided by\nan external POS tagger. So computation cost will be increased and incorrect\npredicted tags may affect the performance of restoring punctuation marks during\ndecoding. This paper proposes adversarial transfer learning to address these\nproblems. A pre-trained bidirectional encoder representations from transformers\n(BERT) model is used to initialize a punctuation model. Thus the transferred\nmodel parameters carry both left-to-right and right-to-left representations.\nFurthermore, adversarial multi-task learning is introduced to learn task\ninvariant knowledge for punctuation prediction. We use an extra POS tagging\ntask to help the training of the punctuation predicting task. Adversarial\ntraining is utilized to prevent the shared parameters from containing task\nspecific information. We only use the punctuation predicting task to restore\nmarks during decoding stage. Therefore, it will not need extra computation and\nnot introduce incorrect tags from the POS tagger. Experiments are conducted on\nIWSLT2011 datasets. The results demonstrate that the punctuation predicting\nmodels obtain further performance improvement with task invariant knowledge\nfrom the POS tagging task. Our best model outperforms the previous\nstate-of-the-art model trained only with lexical features by up to 9.2%\nabsolute overall F_1-score on test set.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 06:19:56 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Yi", "Jiangyan", ""], ["Tao", "Jianhua", ""], ["Bai", "Ye", ""], ["Tian", "Zhengkun", ""], ["Fan", "Cunhang", ""]]}, {"id": "2004.00375", "submitter": "Nkechi Ifeanyi-Reuben Dr.", "authors": "Nkechi Ifeanyi-Reuben, Chidiebere Ugwu, Nwachukwu E.O", "title": "Comparative Analysis of N-gram Text Representation on Igbo Text Document\n  Similarity", "comments": null, "journal-ref": "International Journal of Applied Information Systems (IJAIS).\n  Volume 12 number 9, pages 1-7, December 2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The improvement in Information Technology has encouraged the use of Igbo in\nthe creation of text such as resources and news articles online. Text\nsimilarity is of great importance in any text-based applications. This paper\npresents a comparative analysis of n-gram text representation on Igbo text\ndocument similarity. It adopted Euclidean similarity measure to determine the\nsimilarities between Igbo text documents represented with two word-based n-gram\ntext representation (unigram and bigram) models. The evaluation of the\nsimilarity measure is based on the adopted text representation models. The\nmodel is designed with Object-Oriented Methodology and implemented with Python\nprogramming language with tools from Natural Language Toolkits (NLTK). The\nresult shows that unigram represented text has highest distance values whereas\nbigram has the lowest corresponding distance values. The lower the distance\nvalue, the more similar the two documents and better the quality of the model\nwhen used for a task that requires similarity measure. The similarity of two\ndocuments increases as the distance value moves down to zero (0). Ideally, the\nresult analyzed revealed that Igbo text document similarity measured on bigram\nrepresented text gives accurate similarity result. This will give better,\neffective and accurate result when used for tasks such as text classification,\nclustering and ranking on Igbo text.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 12:24:47 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 00:34:22 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Ifeanyi-Reuben", "Nkechi", ""], ["Ugwu", "Chidiebere", ""], ["O", "Nwachukwu E.", ""]]}, {"id": "2004.00390", "submitter": "Yuanen Zhou", "authors": "Yuanen Zhou, Meng Wang, Daqing Liu, Zhenzhen Hu, Hanwang Zhang", "title": "More Grounded Image Captioning by Distilling Image-Text Matching Model", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention not only improves the performance of image captioners, but\nalso serves as a visual interpretation to qualitatively measure the caption\nrationality and model transparency. Specifically, we expect that a captioner\ncan fix its attentive gaze on the correct objects while generating the\ncorresponding words. This ability is also known as grounded image captioning.\nHowever, the grounding accuracy of existing captioners is far from\nsatisfactory. To improve the grounding accuracy while retaining the captioning\nquality, it is expensive to collect the word-region alignment as strong\nsupervision. To this end, we propose a Part-of-Speech (POS) enhanced image-text\nmatching model (SCAN \\cite{lee2018stacked}): POS-SCAN, as the effective\nknowledge distillation for more grounded image captioning. The benefits are\ntwo-fold: 1) given a sentence and an image, POS-SCAN can ground the objects\nmore accurately than SCAN; 2) POS-SCAN serves as a word-region alignment\nregularization for the captioner's visual attention module. By showing\nbenchmark experimental results, we demonstrate that conventional image\ncaptioners equipped with POS-SCAN can significantly improve the grounding\naccuracy without strong supervision. Last but not the least, we explore the\nindispensable Self-Critical Sequence Training (SCST) \\cite{Rennie_2017_CVPR} in\nthe context of grounded image captioning and show that the image-text matching\nscore can serve as a reward for more grounded captioning\n\\footnote{https://github.com/YuanEZhou/Grounded-Image-Captioning}.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 12:42:06 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zhou", "Yuanen", ""], ["Wang", "Meng", ""], ["Liu", "Daqing", ""], ["Hu", "Zhenzhen", ""], ["Zhang", "Hanwang", ""]]}, {"id": "2004.00499", "submitter": "Shengbin Jia", "authors": "Shengbin Jia", "title": "Unique Chinese Linguistic Phenomena", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistics holds unique characteristics of generality, stability, and\nnationality, which will affect the formulation of extraction strategies and\nshould be incorporated into the relation extraction. Chinese open relation\nextraction is not well-established, because of the complexity of Chinese\nlinguistics makes it harder to operate, and the methods for English are not\ncompatible with that for Chinese. The diversities between Chinese and English\nlinguistics are mainly reflected in morphology and syntax.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 12:13:48 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 10:00:08 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 06:07:07 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Jia", "Shengbin", ""]]}, {"id": "2004.00502", "submitter": "Vinayakumar R", "authors": "Simran K, Sriram S, Vinayakumar R, Soman KP", "title": "Deep Learning Approach for Intelligent Named Entity Recognition of Cyber\n  Security", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the amount of Cyber Security data generated in the form of\nunstructured texts, for example, social media resources, blogs, articles, and\nso on has exceptionally increased. Named Entity Recognition (NER) is an initial\nstep towards converting this unstructured data into structured data which can\nbe used by a lot of applications. The existing methods on NER for Cyber\nSecurity data are based on rules and linguistic characteristics. A Deep\nLearning (DL) based approach embedded with Conditional Random Fields (CRFs) is\nproposed in this paper. Several DL architectures are evaluated to find the most\noptimal architecture. The combination of Bidirectional Gated Recurrent Unit\n(Bi-GRU), Convolutional Neural Network (CNN), and CRF performed better compared\nto various other DL frameworks on a publicly available benchmark dataset. This\nmay be due to the reason that the bidirectional structures preserve the\nfeatures related to the future and previous words in a sequence.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 00:36:19 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["K", "Simran", ""], ["S", "Sriram", ""], ["R", "Vinayakumar", ""], ["KP", "Soman", ""]]}, {"id": "2004.00503", "submitter": "Vinayakumar R", "authors": "Simran K, Prathiksha Balakrishna, Vinayakumar R, Soman KP", "title": "Deep Learning Approach for Enhanced Cyber Threat Indicators in Twitter\n  Stream", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG cs.NE cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent days, the amount of Cyber Security text data shared via social\nmedia resources mainly Twitter has increased. An accurate analysis of this data\ncan help to develop cyber threat situational awareness framework for a cyber\nthreat. This work proposes a deep learning based approach for tweet data\nanalysis. To convert the tweets into numerical representations, various text\nrepresentations are employed. These features are feed into deep learning\narchitecture for optimal feature extraction as well as classification. Various\nhyperparameter tuning approaches are used for identifying optimal text\nrepresentation method as well as optimal network parameters and network\nstructures for deep learning models. For comparative analysis, the classical\ntext representation method with classical machine learning algorithm is\nemployed. From the detailed analysis of experiments, we found that the deep\nlearning architecture with advanced text representation methods performed\nbetter than the classical text representation and classical machine learning\nalgorithms. The primary reason for this is that the advanced text\nrepresentation methods have the capability to learn sequential properties which\nexist among the textual data and deep learning architectures learns the optimal\nfeatures along with decreasing the feature size.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 00:29:42 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["K", "Simran", ""], ["Balakrishna", "Prathiksha", ""], ["R", "Vinayakumar", ""], ["KP", "Soman", ""]]}, {"id": "2004.00526", "submitter": "Jee-Weon Jung", "authors": "Jee-weon Jung, Seung-bin Kim, Hye-jin Shim, Ju-ho Kim, and Ha-Jin Yu", "title": "Improved RawNet with Feature Map Scaling for Text-independent Speaker\n  Verification using Raw Waveforms", "comments": "5 pages, 1 figure, 5 tables, submitted to Interspeech 2020 as a\n  conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have facilitated the design of speaker\nverification systems that directly input raw waveforms. For example, RawNet\nextracts speaker embeddings from raw waveforms, which simplifies the process\npipeline and demonstrates competitive performance. In this study, we improve\nRawNet by scaling feature maps using various methods. The proposed mechanism\nutilizes a scale vector that adopts a sigmoid non-linear function. It refers to\na vector with dimensionality equal to the number of filters in a given feature\nmap. Using a scale vector, we propose to scale the feature map\nmultiplicatively, additively, or both. In addition, we investigate replacing\nthe first convolution layer with the sinc-convolution layer of SincNet.\nExperiments performed on the VoxCeleb1 evaluation dataset demonstrate the\neffectiveness of the proposed methods, and the best performing system reduces\nthe equal error rate by half compared to the original RawNet. Expanded\nevaluation results obtained using the VoxCeleb1-E and VoxCeleb-H protocols\nmarginally outperform existing state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 15:51:56 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 04:45:41 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Jung", "Jee-weon", ""], ["Kim", "Seung-bin", ""], ["Shim", "Hye-jin", ""], ["Kim", "Ju-ho", ""], ["Yu", "Ha-Jin", ""]]}, {"id": "2004.00584", "submitter": "Yuliang Li", "authors": "Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, Wang-Chiew Tan", "title": "Deep Entity Matching with Pre-Trained Language Models", "comments": "To appear in VLDB 2021", "journal-ref": null, "doi": "10.14778/3421424.3421431", "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Ditto, a novel entity matching system based on pre-trained\nTransformer-based language models. We fine-tune and cast EM as a sequence-pair\nclassification problem to leverage such models with a simple architecture. Our\nexperiments show that a straightforward application of language models such as\nBERT, DistilBERT, or RoBERTa pre-trained on large text corpora already\nsignificantly improves the matching quality and outperforms previous\nstate-of-the-art (SOTA), by up to 29% of F1 score on benchmark datasets. We\nalso developed three optimization techniques to further improve Ditto's\nmatching capability. Ditto allows domain knowledge to be injected by\nhighlighting important pieces of input information that may be of interest when\nmaking matching decisions. Ditto also summarizes strings that are too long so\nthat only the essential information is retained and used for EM. Finally, Ditto\nadapts a SOTA technique on data augmentation for text to EM to augment the\ntraining data with (difficult) examples. This way, Ditto is forced to learn\n\"harder\" to improve the model's matching capability. The optimizations we\ndeveloped further boost the performance of Ditto by up to 9.8%. Perhaps more\nsurprisingly, we establish that Ditto can achieve the previous SOTA results\nwith at most half the number of labeled data. Finally, we demonstrate Ditto's\neffectiveness on a real-world large-scale EM task. On matching two company\ndatasets consisting of 789K and 412K records, Ditto achieves a high F1 score of\n96.5%.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:14:10 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 05:40:21 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 19:19:08 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Li", "Yuliang", ""], ["Li", "Jinfeng", ""], ["Suhara", "Yoshihiko", ""], ["Doan", "AnHai", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2004.00588", "submitter": "Kayo Yin", "authors": "Kayo Yin and Jesse Read", "title": "Better Sign Language Translation with STMC-Transformer", "comments": "Proceedings of the 28th International Conference on Computational\n  Linguistics (COLING'2020)", "journal-ref": "28th International Conference on Computational Linguistics 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR)\nsystem to extract sign language glosses from videos. Then, a translation system\ngenerates spoken language translations from the sign language glosses. This\npaper focuses on the translation system and introduces the STMC-Transformer\nwhich improves on the current state-of-the-art by over 5 and 7 BLEU\nrespectively on gloss-to-text and video-to-text translation of the\nPHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase\nof over 16 BLEU.\n  We also demonstrate the problem in current methods that rely on gloss\nsupervision. The video-to-text translation of our STMC-Transformer outperforms\ntranslation of GT glosses. This contradicts previous claims that GT gloss\ntranslation acts as an upper bound for SLT performance and reveals that glosses\nare an inefficient representation of sign language. For future SLT research, we\ntherefore suggest an end-to-end training of the recognition and translation\nmodels, or using a different sign language annotation scheme.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:20:04 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 00:59:54 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yin", "Kayo", ""], ["Read", "Jesse", ""]]}, {"id": "2004.00648", "submitter": "Ignatius Ezeani", "authors": "Ignatius Ezeani, Paul Rayson, Ikechukwu Onyenwe, Chinedu Uchechukwu,\n  Mark Hepple", "title": "Igbo-English Machine Translation: An Evaluation Benchmark", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Although researchers and practitioners are pushing the boundaries and\nenhancing the capacities of NLP tools and methods, works on African languages\nare lagging. A lot of focus on well resourced languages such as English,\nJapanese, German, French, Russian, Mandarin Chinese etc. Over 97% of the\nworld's 7000 languages, including African languages, are low resourced for NLP\ni.e. they have little or no data, tools, and techniques for NLP research. For\ninstance, only 5 out of 2965, 0.19% authors of full text papers in the ACL\nAnthology extracted from the 5 major conferences in 2018 ACL, NAACL, EMNLP,\nCOLING and CoNLL, are affiliated to African institutions. In this work, we\ndiscuss our effort toward building a standard machine translation benchmark\ndataset for Igbo, one of the 3 major Nigerian languages. Igbo is spoken by more\nthan 50 million people globally with over 50% of the speakers are in\nsoutheastern Nigeria. Igbo is low resourced although there have been some\nefforts toward developing IgboNLP such as part of speech tagging and diacritic\nrestoration\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 18:06:21 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Ezeani", "Ignatius", ""], ["Rayson", "Paul", ""], ["Onyenwe", "Ikechukwu", ""], ["Uchechukwu", "Chinedu", ""], ["Hepple", "Mark", ""]]}, {"id": "2004.00798", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn", "title": "Mapping Languages: The Corpus of Global Language Use", "comments": "This is a pre-print of an article published in Language Resources and\n  Evaluation. The final authenticated version is available online at:\n  https://doi.org/10.1007/s10579-020-09489-2", "journal-ref": null, "doi": "10.1007/s10579-020-09489-2", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a web-based corpus of global language use with a focus\non how this corpus can be used for data-driven language mapping. First, the\ncorpus provides a representation of where national varieties of major languages\nare used (e.g., English, Arabic, Russian) together with consistently collected\ndata for each variety. Second, the paper evaluates a language identification\nmodel that supports more local languages with smaller sample sizes than\nalternative off-the-shelf models. Improved language identification is essential\nfor moving beyond majority languages. Given the focus on language mapping, the\npaper analyzes how well this digital language data represents actual\npopulations by (i) systematically comparing the corpus with demographic\nground-truth data and (ii) triangulating the corpus with an alternate\nTwitter-based dataset. In total, the corpus contains 423 billion words\nrepresenting 148 languages (with over 1 million words from each language) and\n158 countries (again with over 1 million words from each country), all\ndistilled from Common Crawl web data. The main contribution of this paper, in\naddition to describing this publicly-available corpus, is to provide a\ncomprehensive analysis of the relationship between two sources of digital data\n(the web and Twitter) as well as their connection to underlying populations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 03:42:14 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Dunn", "Jonathan", ""]]}, {"id": "2004.00809", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn and Ben Adams", "title": "Mapping Languages and Demographics with Georeferenced Corpora", "comments": "Proceedings of GeoComputation 19", "journal-ref": null, "doi": "10.17608/k6.auckland.9869252.v2", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates large georeferenced corpora, taken from both web-crawled\nand social media sources, against ground-truth population and language-census\ndatasets. The goal is to determine (i) which dataset best represents population\ndemographics; (ii) in what parts of the world the datasets are most\nrepresentative of actual populations; and (iii) how to weight the datasets to\nprovide more accurate representations of underlying populations. The paper\nfinds that the two datasets represent very different populations and that they\ncorrelate with actual populations with values of r=0.60 (social media) and\nr=0.49 (web-crawled). Further, Twitter data makes better predictions about the\ninventory of languages used in each country.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 04:34:11 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Dunn", "Jonathan", ""], ["Adams", "Ben", ""]]}, {"id": "2004.00849", "submitter": "Bei Liu", "authors": "Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, Jianlong Fu", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Pixel-BERT to align image pixels with text by deep multi-modal\ntransformers that jointly learn visual and language embedding in a unified\nend-to-end framework. We aim to build a more accurate and thorough connection\nbetween image pixels and language semantics directly from image and sentence\npairs instead of using region-based image features as the most recent vision\nand language tasks. Our Pixel-BERT which aligns semantic connection in pixel\nand text level solves the limitation of task-specific visual representation for\nvision and language tasks. It also relieves the cost of bounding box\nannotations and overcomes the unbalance between semantic labels in visual task\nand language semantic. To provide a better representation for down-stream\ntasks, we pre-train a universal end-to-end model with image and sentence pairs\nfrom Visual Genome dataset and MS-COCO dataset. We propose to use a random\npixel sampling mechanism to enhance the robustness of visual representation and\nto apply the Masked Language Model and Image-Text Matching as pre-training\ntasks. Extensive experiments on downstream tasks with our pre-trained model\nshow that our approach makes the most state-of-the-arts in downstream tasks,\nincluding Visual Question Answering (VQA), image-text retrieval, Natural\nLanguage for Visual Reasoning for Real (NLVR). Particularly, we boost the\nperformance of a single model in VQA task by 2.17 points compared with SOTA\nunder fair comparison.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 07:39:28 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 09:09:22 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Huang", "Zhicheng", ""], ["Zeng", "Zhaoyang", ""], ["Liu", "Bei", ""], ["Fu", "Dongmei", ""], ["Fu", "Jianlong", ""]]}, {"id": "2004.00881", "submitter": "Matthew Purver", "authors": "Jey Han Lau, Carlos S. Armendariz, Shalom Lappin, Matthew Purver,\n  Chang Shu", "title": "How Furiously Can Colourless Green Ideas Sleep? Sentence Acceptability\n  in Context", "comments": "14 pages. Author's final version, accepted for publication in\n  Transactions of the Association for Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the influence of context on sentence acceptability. First we compare\nthe acceptability ratings of sentences judged in isolation, with a relevant\ncontext, and with an irrelevant context. Our results show that context induces\na cognitive load for humans, which compresses the distribution of ratings.\nMoreover, in relevant contexts we observe a discourse coherence effect which\nuniformly raises acceptability. Next, we test unidirectional and bidirectional\nlanguage models in their ability to predict acceptability ratings. The\nbidirectional models show very promising results, with the best model achieving\na new state-of-the-art for unsupervised acceptability prediction. The two sets\nof experiments provide insights into the cognitive aspects of sentence\nprocessing and central issues in the computational modelling of text and\ndiscourse.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 08:58:44 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Lau", "Jey Han", ""], ["Armendariz", "Carlos S.", ""], ["Lappin", "Shalom", ""], ["Purver", "Matthew", ""], ["Shu", "Chang", ""]]}, {"id": "2004.00998", "submitter": "Vivek Gupta", "authors": "Vivek Gupta", "title": "DeepSumm -- Deep Code Summaries using Neural Transformer Architecture", "comments": "arXiv admin note: substantial text overlap with arXiv:1902.01954 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source code summarizing is a task of writing short, natural language\ndescriptions of source code behavior during run time. Such summaries are\nextremely useful for software development and maintenance but are expensive to\nmanually author,hence it is done for small fraction of the code that is\nproduced and is often ignored. Automatic code documentation can possibly solve\nthis at a low cost. This is thus an emerging research field with further\napplications to program comprehension, and software maintenance. Traditional\nmethods often relied on cognitive models that were built in the form of\ntemplates and by heuristics and had varying degree of adoption by the developer\ncommunity. But with recent advancements, end to end data-driven approaches\nbased on neural techniques have largely overtaken the traditional techniques.\nMuch of the current landscape employs neural translation based architectures\nwith recurrence and attention which is resource and time intensive training\nprocedure. In this paper, we employ neural techniques to solve the task of\nsource code summarizing and specifically compare NMT based techniques to more\nsimplified and appealing Transformer architecture on a dataset of Java methods\nand comments. We bring forth an argument to dispense the need of recurrence in\nthe training procedure. To the best of our knowledge, transformer based models\nhave not been used for the task before. With supervised samples of more than\n2.1m comments and code, we reduce the training time by more than 50% and\nachieve the BLEU score of 17.99 for the test set of examples.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 22:43:29 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Gupta", "Vivek", ""]]}, {"id": "2004.00999", "submitter": "Fangzhou Xie", "authors": "Fangzhou Xie", "title": "Pruned Wasserstein Index Generation Model and wigpy Package", "comments": "fix typos and errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent proposal of Wasserstein Index Generation model (WIG) has shown a new\ndirection for automatically generating indices. However, it is challenging in\npractice to fit large datasets for two reasons. First, the Sinkhorn distance is\nnotoriously expensive to compute and suffers from dimensionality severely.\nSecond, it requires to compute a full $N\\times N$ matrix to be fit into memory,\nwhere $N$ is the dimension of vocabulary. When the dimensionality is too large,\nit is even impossible to compute at all. I hereby propose a Lasso-based\nshrinkage method to reduce dimensionality for the vocabulary as a\npre-processing step prior to fitting the WIG model. After we get the word\nembedding from Word2Vec model, we could cluster these high-dimensional vectors\nby $k$-means clustering, and pick most frequent tokens within each cluster to\nform the \"base vocabulary\". Non-base tokens are then regressed on the vectors\nof base token to get a transformation weight and we could thus represent the\nwhole vocabulary by only the \"base tokens\". This variant, called pruned WIG\n(pWIG), will enable us to shrink vocabulary dimension at will but could still\nachieve high accuracy. I also provide a \\textit{wigpy} module in Python to\ncarry out computation in both flavor. Application to Economic Policy\nUncertainty (EPU) index is showcased as comparison with existing methods of\ngenerating time-series sentiment indices.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 18:26:24 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 00:27:02 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 14:42:59 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Xie", "Fangzhou", ""]]}, {"id": "2004.01079", "submitter": "Xutan Peng", "authors": "Xutan Peng, Chenghua Lin, Mark Stevenson, Chen li", "title": "Revisiting the linearity in cross-lingual embedding mappings: from a\n  perspective of word analogies", "comments": "Comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most cross-lingual embedding mapping algorithms assume the optimised\ntransformation functions to be linear. Recent studies showed that on some\noccasions, learning a linear mapping does not work, indicating that the\ncommonly-used assumption may fail. However, it still remains unclear under\nwhich conditions the linearity of cross-lingual embedding mappings holds. In\nthis paper, we rigorously explain that the linearity assumption relies on the\nconsistency of analogical relations encoded by multilingual embeddings. We did\nextensive experiments to validate this claim. Empirical results based on the\nanalogy completion benchmark and the BLI task demonstrate a strong correlation\nbetween whether mappings capture analogical information and are linear.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 15:40:59 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Peng", "Xutan", ""], ["Lin", "Chenghua", ""], ["Stevenson", "Mark", ""], ["li", "Chen", ""]]}, {"id": "2004.01092", "submitter": "Naiara P\\'erez Miguel", "authors": "Salvador Lima, Naiara Perez, Montse Cuadros, and German Rigau", "title": "NUBES: A Corpus of Negation and Uncertainty in Spanish Clinical Texts", "comments": "Accepted at the Twelfth International Conference on Language\n  Resources and Evaluation (LREC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces the first version of the NUBes corpus (Negation and\nUncertainty annotations in Biomedical texts in Spanish). The corpus is part of\nan on-going research and currently consists of 29,682 sentences obtained from\nanonymised health records annotated with negation and uncertainty. The article\nincludes an exhaustive comparison with similar corpora in Spanish, and presents\nthe main annotation and design decisions. Additionally, we perform preliminary\nexperiments using deep learning algorithms to validate the annotated dataset.\nAs far as we know, NUBes is the largest publicly available corpus for negation\nin Spanish and the first that also incorporates the annotation of speculation\ncues, scopes, and events.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 15:51:31 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Lima", "Salvador", ""], ["Perez", "Naiara", ""], ["Cuadros", "Montse", ""], ["Rigau", "German", ""]]}, {"id": "2004.01095", "submitter": "Han Fu", "authors": "Han Fu, Rui Wu, Chenghao Liu, Jianling Sun", "title": "MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images\n  with Latent Variable Model", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, driven by the increasing concern on diet and health, food computing\nhas attracted enormous attention from both industry and research community. One\nof the most popular research topics in this domain is Food Retrieval, due to\nits profound influence on health-oriented applications. In this paper, we focus\non the task of cross-modal retrieval between food images and cooking recipes.\nWe present Modality-Consistent Embedding Network (MCEN) that learns\nmodality-invariant representations by projecting images and texts to the same\nembedding space. To capture the latent alignments between modalities, we\nincorporate stochastic latent variables to explicitly exploit the interactions\nbetween textual and visual features. Importantly, our method learns the\ncross-modal alignments during training but computes embeddings of different\nmodalities independently at inference time for the sake of efficiency.\nExtensive experimental results clearly demonstrate that the proposed MCEN\noutperforms all existing approaches on the benchmark Recipe1M dataset and\nrequires less computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:00:10 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Fu", "Han", ""], ["Wu", "Rui", ""], ["Liu", "Chenghao", ""], ["Sun", "Jianling", ""]]}, {"id": "2004.01097", "submitter": "Ivana Kaji\\'c", "authors": "Ivana Kaji\\'c, Eser Ayg\\\"un and Doina Precup", "title": "Learning to cooperate: Emergent communication in multi-agent navigation", "comments": "Accepted to CogSci 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergent communication in artificial agents has been studied to understand\nlanguage evolution, as well as to develop artificial systems that learn to\ncommunicate with humans. We show that agents performing a cooperative\nnavigation task in various gridworld environments learn an interpretable\ncommunication protocol that enables them to efficiently, and in many cases,\noptimally, solve the task. An analysis of the agents' policies reveals that\nemergent signals spatially cluster the state space, with signals referring to\nspecific locations and spatial directions such as \"left\", \"up\", or \"upper left\nroom\". Using populations of agents, we show that the emergent protocol has\nbasic compositional structure, thus exhibiting a core property of natural\nlanguage.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:03:17 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 15:13:39 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Kaji\u0107", "Ivana", ""], ["Ayg\u00fcn", "Eser", ""], ["Precup", "Doina", ""]]}, {"id": "2004.01168", "submitter": "Tara Safavi", "authors": "Tara Safavi, Danai Koutra, Edgar Meij", "title": "Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy\n  Link Prediction", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Little is known about the trustworthiness of predictions made by knowledge\ngraph embedding (KGE) models. In this paper we take initial steps toward this\ndirection by investigating the calibration of KGE models, or the extent to\nwhich they output confidence scores that reflect the expected correctness of\npredicted knowledge graph triples. We first conduct an evaluation under the\nstandard closed-world assumption (CWA), in which predicted triples not already\nin the knowledge graph are considered false, and show that existing calibration\ntechniques are effective for KGE under this common but narrow assumption. Next,\nwe introduce the more realistic but challenging open-world assumption (OWA), in\nwhich unobserved predictions are not considered true or false until\nground-truth labels are obtained. Here, we show that existing calibration\ntechniques are much less effective under the OWA than the CWA, and provide\nexplanations for this discrepancy. Finally, to motivate the utility of\ncalibration for KGE from a practitioner's perspective, we conduct a unique case\nstudy of human-AI collaboration, showing that calibrated predictions can\nimprove human performance in a knowledge graph completion task.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 17:46:47 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 16:02:54 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 09:31:15 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Safavi", "Tara", ""], ["Koutra", "Danai", ""], ["Meij", "Edgar", ""]]}, {"id": "2004.01174", "submitter": "Noah Weber", "authors": "Noah Weber, Rachel Rudinger, Benjamin Van Durme", "title": "Causal Inference of Script Knowledge", "comments": "Pre-Print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When does a sequence of events define an everyday scenario and how can this\nknowledge be induced from text? Prior works in inducing such scripts have\nrelied on, in one form or another, measures of correlation between instances of\nevents in a corpus. We argue from both a conceptual and practical sense that a\npurely correlation-based approach is insufficient, and instead propose an\napproach to script induction based on the causal effect between events,\nformally defined via interventions. Through both human and automatic\nevaluations, we show that the output of our method based on causal effects\nbetter matches the intuition of what a script represents\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 17:54:24 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Weber", "Noah", ""], ["Rudinger", "Rachel", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2004.01221", "submitter": "Sriram Ganapathy", "authors": "Bharat Padi, Anand Mohan and Sriram Ganapathy", "title": "Towards Relevance and Sequence Modeling in Language Recognition", "comments": "https://github.com/iiscleap/lre-relevance-weighting Accepted to IEEE\n  Transactions on Audio, Speech and Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of automatic language identification (LID) involving multiple\ndialects of the same language family in the presence of noise is a challenging\nproblem. In these scenarios, the identity of the language/dialect may be\nreliably present only in parts of the temporal sequence of the speech signal.\nThe conventional approaches to LID (and for speaker recognition) ignore the\nsequence information by extracting long-term statistical summary of the\nrecording assuming an independence of the feature frames. In this paper, we\npropose a neural network framework utilizing short-sequence information in\nlanguage recognition. In particular, a new model is proposed for incorporating\nrelevance in language recognition, where parts of speech data are weighted more\nbased on their relevance for the language recognition task. This relevance\nweighting is achieved using the bidirectional long short-term memory (BLSTM)\nnetwork with attention modeling. We explore two approaches, the first approach\nuses segment level i-vector/x-vector representations that are aggregated in the\nneural model and the second approach where the acoustic features are directly\nmodeled in an end-to-end neural model. Experiments are performed using the\nlanguage recognition task in NIST LRE 2017 Challenge using clean, noisy and\nmulti-speaker speech data as well as in the RATS language recognition corpus.\nIn these experiments on noisy LRE tasks as well as the RATS dataset, the\nproposed approach yields significant improvements over the conventional\ni-vector/x-vector based language recognition approaches as well as with other\nprevious models incorporating sequence information.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 18:31:18 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Padi", "Bharat", ""], ["Mohan", "Anand", ""], ["Ganapathy", "Sriram", ""]]}, {"id": "2004.01251", "submitter": "Ran Wang", "authors": "Ran Wang, Kun Tao, Dingjie Song, Zhilong Zhang, Xiao Ma, Xi'ao Su,\n  Xinyu Dai", "title": "R3: A Reading Comprehension Benchmark Requiring Reasoning Processes", "comments": "work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing question answering systems can only predict answers without explicit\nreasoning processes, which hinder their explainability and make us overestimate\ntheir ability of understanding and reasoning over natural language. In this\nwork, we propose a novel task of reading comprehension, in which a model is\nrequired to provide final answers and reasoning processes. To this end, we\nintroduce a formalism for reasoning over unstructured text, namely Text\nReasoning Meaning Representation (TRMR). TRMR consists of three phrases, which\nis expressive enough to characterize the reasoning process to answer reading\ncomprehension questions. We develop an annotation platform to facilitate TRMR's\nannotation, and release the R3 dataset, a \\textbf{R}eading comprehension\nbenchmark \\textbf{R}equiring \\textbf{R}easoning processes. R3 contains over 60K\npairs of question-answer pairs and their TRMRs. Our dataset is available at:\n\\url{http://anonymous}.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 20:39:12 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Wang", "Ran", ""], ["Tao", "Kun", ""], ["Song", "Dingjie", ""], ["Zhang", "Zhilong", ""], ["Ma", "Xiao", ""], ["Su", "Xi'ao", ""], ["Dai", "Xinyu", ""]]}, {"id": "2004.01267", "submitter": "Congying Xia", "authors": "Tao Zhang, Congying Xia, Chun-Ta Lu, Philip Yu", "title": "MZET: Memory Augmented Zero-Shot Fine-grained Named Entity Typing", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity typing (NET) is a classification task of assigning an entity\nmention in the context with given semantic types. However, with the growing\nsize and granularity of the entity types, rare researches in previous concern\nwith newly emerged entity types. In this paper, we propose MZET, a novel memory\naugmented FNET (Fine-grained NET) model, to tackle the unseen types in a\nzero-shot manner. MZET incorporates character-level, word-level, and\ncontextural-level information to learn the entity mention representation.\nBesides, MZET considers the semantic meaning and the hierarchical structure\ninto the entity type representation. Finally, through the memory component\nwhich models the relationship between the entity mention and the entity type,\nMZET transfer the knowledge from seen entity types to the zero-shot ones.\nExtensive experiments on three public datasets show prominent performance\nobtained by MZET, which surpasses the state-of-the-art FNET neural network\nmodels with up to 7% gain in Micro-F1 and Macro-F1 score.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 21:17:33 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 19:11:29 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zhang", "Tao", ""], ["Xia", "Congying", ""], ["Lu", "Chun-Ta", ""], ["Yu", "Philip", ""]]}, {"id": "2004.01401", "submitter": "Yaobo Liang", "authors": "Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi,\n  Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,\n  Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung\n  Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, Ming\n  Zhou", "title": "XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training,\n  Understanding and Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce XGLUE, a new benchmark dataset that can be used\nto train large-scale cross-lingual pre-trained models using multilingual and\nbilingual corpora and evaluate their performance across a diverse set of\ncross-lingual tasks. Comparing to GLUE(Wang et al., 2019), which is labeled in\nEnglish for natural language understanding tasks only, XGLUE has two main\nadvantages: (1) it provides 11 diversified tasks that cover both natural\nlanguage understanding and generation scenarios; (2) for each task, it provides\nlabeled data in multiple languages. We extend a recent cross-lingual\npre-trained model Unicoder(Huang et al., 2019) to cover both understanding and\ngeneration tasks, which is evaluated on XGLUE as a strong baseline. We also\nevaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for\ncomparison.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 07:03:12 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 04:45:53 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 05:58:10 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Liang", "Yaobo", ""], ["Duan", "Nan", ""], ["Gong", "Yeyun", ""], ["Wu", "Ning", ""], ["Guo", "Fenfei", ""], ["Qi", "Weizhen", ""], ["Gong", "Ming", ""], ["Shou", "Linjun", ""], ["Jiang", "Daxin", ""], ["Cao", "Guihong", ""], ["Fan", "Xiaodong", ""], ["Zhang", "Ruofei", ""], ["Agrawal", "Rahul", ""], ["Cui", "Edward", ""], ["Wei", "Sining", ""], ["Bharti", "Taroon", ""], ["Qiao", "Ying", ""], ["Chen", "Jiun-Hung", ""], ["Wu", "Winnie", ""], ["Liu", "Shuguang", ""], ["Yang", "Fan", ""], ["Campos", "Daniel", ""], ["Majumder", "Rangan", ""], ["Zhou", "Ming", ""]]}, {"id": "2004.01422", "submitter": "Juan Antonio P\\'erez-Ortiz", "authors": "Felipe S\\'anchez-Mart\\'inez, Juan Antonio P\\'erez-Ortiz, Rafael C.\n  Carrasco", "title": "Learning synchronous context-free grammars with multiple specialised\n  non-terminals for hierarchical phrase-based translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translation models based on hierarchical phrase-based statistical machine\ntranslation (HSMT) have shown better performances than the non-hierarchical\nphrase-based counterparts for some language pairs. The standard approach to\nHSMT learns and apply a synchronous context-free grammar with a single\nnon-terminal. The hypothesis behind the grammar refinement algorithm presented\nin this work is that this single non-terminal is overloaded, and insufficiently\ndiscriminative, and therefore, an adequate split of it into more specialised\nsymbols could lead to improved models. This paper presents a method to learn\nsynchronous context-free grammars with a huge number of initial non-terminals,\nwhich are then grouped via a clustering algorithm. Our experiments show that\nthe resulting smaller set of non-terminals correctly capture the contextual\ninformation that makes it possible to statistically significantly improve the\nBLEU score of the standard HSMT approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 08:09:07 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["S\u00e1nchez-Mart\u00ednez", "Felipe", ""], ["P\u00e9rez-Ortiz", "Juan Antonio", ""], ["Carrasco", "Rafael C.", ""]]}, {"id": "2004.01549", "submitter": "Manikandan Ravikiran", "authors": "Manikandan Ravikiran", "title": "Finding Black Cat in a Coal Cellar -- Keyphrase Extraction &\n  Keyphrase-Rubric Relationship Classification from Complex Assignments", "comments": "v1 preprint. Working paper. More results to be added. Text overlap\n  with arXiv:2003.07019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diversity in content and open-ended questions are inherent in complex\nassignments across online graduate programs. The natural scale of these\nprograms poses a variety of challenges across both peer and expert feedback\nincluding rogue reviews. While the identification of relevant content and\nassociating it to predefined rubrics would simplify and improve the grading\nprocess, the research to date is still in a nascent stage. As such in this\npaper we aim to quantify the effectiveness of supervised and unsupervised\napproaches for the task for keyphrase extraction and generic/specific\nkeyphrase-rubric relationship extraction. Through this study, we find that (i)\nunsupervised MultiPartiteRank produces the best result for keyphrase extraction\n(ii) supervised SVM classifier with BERT features that offer the best\nperformance for both generic and specific keyphrase-rubric relationship\nclassification. We finally present a comprehensive analysis and derive useful\nobservations for those interested in these tasks for the future. The source\ncode is released in \\url{https://github.com/manikandan-ravikiran/cs6460-proj}.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:18:02 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 12:09:54 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 13:17:19 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Ravikiran", "Manikandan", ""]]}, {"id": "2004.01647", "submitter": "Yevgen Matusevych", "authors": "Yevgen Matusevych, Herman Kamper, Sharon Goldwater", "title": "Analyzing autoencoder-based acoustic word embeddings", "comments": "6 pages, 7 figures, accepted to BAICS workshop (ICLR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have introduced methods for learning acoustic word embeddings\n(AWEs)---fixed-size vector representations of words which encode their acoustic\nfeatures. Despite the widespread use of AWEs in speech processing research,\nthey have only been evaluated quantitatively in their ability to discriminate\nbetween whole word tokens. To better understand the applications of AWEs in\nvarious downstream tasks and in cognitive modeling, we need to analyze the\nrepresentation spaces of AWEs. Here we analyze basic properties of AWE spaces\nlearned by a sequence-to-sequence encoder-decoder model in six typologically\ndiverse languages. We first show that these AWEs preserve some information\nabout words' absolute duration and speaker. At the same time, the\nrepresentation space of these AWEs is organized such that the distance between\nwords' embeddings increases with those words' phonetic dissimilarity. Finally,\nthe AWEs exhibit a word onset bias, similar to patterns reported in various\nstudies on human speech processing and lexical access. We argue this is a\npromising result and encourage further evaluation of AWEs as a potentially\nuseful tool in cognitive science, which could provide a link between speech\nprocessing and lexical memory.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:11:57 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Matusevych", "Yevgen", ""], ["Kamper", "Herman", ""], ["Goldwater", "Sharon", ""]]}, {"id": "2004.01655", "submitter": "Marjan Ghazvininejad", "authors": "Marjan Ghazvininejad, Vladimir Karpukhin, Luke Zettlemoyer, Omer Levy", "title": "Aligned Cross Entropy for Non-Autoregressive Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-autoregressive machine translation models significantly speed up decoding\nby allowing for parallel prediction of the entire target sequence. However,\nmodeling word order is more challenging due to the lack of autoregressive\nfactors in the model. This difficultly is compounded during training with cross\nentropy loss, which can highly penalize small shifts in word order. In this\npaper, we propose aligned cross entropy (AXE) as an alternative loss function\nfor training of non-autoregressive models. AXE uses a differentiable dynamic\nprogram to assign loss based on the best possible monotonic alignment between\ntarget tokens and model predictions. AXE-based training of conditional masked\nlanguage models (CMLMs) substantially improves performance on major WMT\nbenchmarks, while setting a new state of the art for non-autoregressive models.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:24:47 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Ghazvininejad", "Marjan", ""], ["Karpukhin", "Vladimir", ""], ["Zettlemoyer", "Luke", ""], ["Levy", "Omer", ""]]}, {"id": "2004.01670", "submitter": "Bertie Vidgen Dr", "authors": "Bertie Vidgen and Leon Derczynski", "title": "Directions in Abusive Language Training Data: Garbage In, Garbage Out", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0243300", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven analysis and detection of abusive online content covers many\ndifferent tasks, phenomena, contexts, and methodologies. This paper\nsystematically reviews abusive language dataset creation and content in\nconjunction with an open website for cataloguing abusive language data. This\ncollection of knowledge leads to a synthesis providing evidence-based\nrecommendations for practitioners working with this complex and highly diverse\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:51:33 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 13:13:12 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 07:40:01 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Vidgen", "Bertie", ""], ["Derczynski", "Leon", ""]]}, {"id": "2004.01694", "submitter": "Samuel L\\\"aubli", "authors": "Samuel L\\\"aubli and Sheila Castilho and Graham Neubig and Rico\n  Sennrich and Qinlan Shen and Antonio Toral", "title": "A Set of Recommendations for Assessing Human-Machine Parity in Language\n  Translation", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research 67 (2020) 653-672", "doi": "10.1613/jair.1.11371", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of machine translation has increased remarkably over the past\nyears, to the degree that it was found to be indistinguishable from\nprofessional human translation in a number of empirical investigations. We\nreassess Hassan et al.'s 2018 investigation into Chinese to English news\ntranslation, showing that the finding of human-machine parity was owed to\nweaknesses in the evaluation design - which is currently considered best\npractice in the field. We show that the professional human translations\ncontained significantly fewer errors, and that perceived quality in human\nevaluation depends on the choice of raters, the availability of linguistic\ncontext, and the creation of reference translations. Our results call for\nrevisiting current best practices to assess strong machine translation systems\nin general and human-machine parity in particular, for which we offer a set of\nrecommendations based on our empirical findings.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 17:49:56 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["L\u00e4ubli", "Samuel", ""], ["Castilho", "Sheila", ""], ["Neubig", "Graham", ""], ["Sennrich", "Rico", ""], ["Shen", "Qinlan", ""], ["Toral", "Antonio", ""]]}, {"id": "2004.01820", "submitter": "Caleb Ziems", "authors": "Caleb Ziems, Ymir Vigfusson, Fred Morstatter", "title": "Aggressive, Repetitive, Intentional, Visible, and Imbalanced: Refining\n  Representations for Cyberbullying Classification", "comments": "12 pages, 5 figures, 22 tables, Accepted to the 14th International\n  AAAI Conference on Web and Social Media, ICWSM'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyberbullying is a pervasive problem in online communities. To identify\ncyberbullying cases in large-scale social networks, content moderators depend\non machine learning classifiers for automatic cyberbullying detection. However,\nexisting models remain unfit for real-world applications, largely due to a\nshortage of publicly available training data and a lack of standard criteria\nfor assigning ground truth labels. In this study, we address the need for\nreliable data using an original annotation framework. Inspired by social\nsciences research into bullying behavior, we characterize the nuanced problem\nof cyberbullying using five explicit factors to represent its social and\nlinguistic aspects. We model this behavior using social network and\nlanguage-based features, which improve classifier performance. These results\ndemonstrate the importance of representing and modeling cyberbullying as a\nsocial phenomenon.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 00:35:16 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Ziems", "Caleb", ""], ["Vigfusson", "Ymir", ""], ["Morstatter", "Fred", ""]]}, {"id": "2004.01853", "submitter": "Yanyan Zou", "authors": "Yanyan Zou, Xingxing Zhang, Wei Lu, Furu Wei and Ming Zhou", "title": "Pre-training for Abstractive Document Summarization by Reinstating\n  Source Text", "comments": "EMNLP2020 Camera-Ready, 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Abstractive document summarization is usually modeled as a\nsequence-to-sequence (Seq2Seq) learning problem. Unfortunately, training large\nSeq2Seq based summarization models on limited supervised summarization data is\nchallenging. This paper presents three pre-training objectives which allow us\nto pre-train a Seq2Seq based abstractive summarization model on unlabeled text.\nThe main idea is that, given an input text artificially constructed from a\ndocument, a model is pre-trained to reinstate the original document. These\nobjectives include sentence reordering, next sentence generation, and masked\ndocument generation, which have close relations with the abstractive document\nsummarization task. Experiments on two benchmark summarization datasets (i.e.,\nCNN/DailyMail and New York Times) show that all three objectives can improve\nperformance upon baselines. Compared to models pre-trained on large-scale data\n(more than 160GB), our method, with only 19GB text for pre-training, achieves\ncomparable results, which demonstrates its effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 05:06:26 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 07:25:33 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 17:27:19 GMT"}, {"version": "v4", "created": "Sun, 11 Oct 2020 14:53:42 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zou", "Yanyan", ""], ["Zhang", "Xingxing", ""], ["Lu", "Wei", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "2004.01862", "submitter": "Pengtao Xie", "authors": "Yuxiao Liang, Pengtao Xie", "title": "Identifying Radiological Findings Related to COVID-19 from Medical\n  Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) has infected more than one million\nindividuals all over the world and caused more than 55,000 deaths, as of April\n3 in 2020. Radiological findings are important sources of information in\nguiding the diagnosis and treatment of COVID-19. However, the existing studies\non how radiological findings are correlated with COVID-19 are conducted\nseparately by different hospitals, which may be inconsistent or even\nconflicting due to population bias. To address this problem, we develop natural\nlanguage processing methods to analyze a large collection of COVID-19\nliterature containing study reports from hospitals all over the world,\nreconcile these results, and draw unbiased and universally-sensible conclusions\nabout the correlation between radiological findings and COVID-19. We apply our\nmethod to the CORD-19 dataset and successfully extract a set of radiological\nfindings that are closely tied to COVID-19.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 05:33:21 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liang", "Yuxiao", ""], ["Xie", "Pengtao", ""]]}, {"id": "2004.01878", "submitter": "Xiao Liu", "authors": "Xiao Liu, Heyan Huang, Yue Zhang, Changsen Yuan", "title": "News-Driven Stock Prediction With Attention-Based Noisy Recurrent State\n  Transition", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider direct modeling of underlying stock value movement sequences over\ntime in the news-driven stock movement prediction. A recurrent state transition\nmodel is constructed, which better captures a gradual process of stock movement\ncontinuously by modeling the correlation between past and future price\nmovements. By separating the effects of news and noise, a noisy random factor\nis also explicitly fitted based on the recurrent states. Results show that the\nproposed model outperforms strong baselines. Thanks to the use of attention\nover news events, our model is also more explainable. To our knowledge, we are\nthe first to explicitly model both events and noise over a fundamental stock\nvalue state for news-driven stock movement prediction.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 07:17:16 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liu", "Xiao", ""], ["Huang", "Heyan", ""], ["Zhang", "Yue", ""], ["Yuan", "Changsen", ""]]}, {"id": "2004.01881", "submitter": "Congying Xia", "authors": "Congying Xia, Chenwei Zhang, Hoang Nguyen, Jiawei Zhang, Philip Yu", "title": "CG-BERT: Conditional Text Generation with BERT for Generalized Few-shot\n  Intent Detection", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formulate a more realistic and difficult problem setup for\nthe intent detection task in natural language understanding, namely Generalized\nFew-Shot Intent Detection (GFSID). GFSID aims to discriminate a joint label\nspace consisting of both existing intents which have enough labeled data and\nnovel intents which only have a few examples for each class. To approach this\nproblem, we propose a novel model, Conditional Text Generation with BERT\n(CG-BERT). CG-BERT effectively leverages a large pre-trained language model to\ngenerate text conditioned on the intent label. By modeling the utterance\ndistribution with variational inference, CG-BERT can generate diverse\nutterances for the novel intents even with only a few utterances available.\nExperimental results show that CG-BERT achieves state-of-the-art performance on\nthe GFSID task with 1-shot and 5-shot settings on two real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 07:31:59 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Xia", "Congying", ""], ["Zhang", "Chenwei", ""], ["Nguyen", "Hoang", ""], ["Zhang", "Jiawei", ""], ["Yu", "Philip", ""]]}, {"id": "2004.01893", "submitter": "Neeraj Bokde", "authors": "Neeraj Dhanraj Bokde and Zaher Mundher Yaseen and Gorm Bruun Andersen", "title": "ForecastTB An R Package as a Test-Bench for Time Series Forecasting\n  Application of Wind Speed and Solar Radiation Modeling", "comments": "Published in Energies", "journal-ref": "2020", "doi": "10.3390/en13102578", "report-no": null, "categories": "stat.ME cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces an R package ForecastTB that can be used to compare the\naccuracy of different forecasting methods as related to the characteristics of\na time series dataset. The ForecastTB is a plug-and-play structured module, and\nseveral forecasting methods can be included with simple instructions. The\nproposed test-bench is not limited to the default forecasting and error metric\nfunctions, and users are able to append, remove, or choose the desired methods\nas per requirements. Besides, several plotting functions and statistical\nperformance metrics are provided to visualize the comparative performance and\naccuracy of different forecasting methods. Furthermore, this paper presents\nreal application examples with natural time series datasets (i.e., wind speed\nand solar radiation) to exhibit the features of the ForecastTB package to\nevaluate forecasting comparison analysis as affected by the characteristics of\na dataset. Modeling results indicated the applicability and robustness of the\nproposed R package ForecastTB for time series forecasting.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 08:52:19 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 13:49:21 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Bokde", "Neeraj Dhanraj", ""], ["Yaseen", "Zaher Mundher", ""], ["Andersen", "Gorm Bruun", ""]]}, {"id": "2004.01894", "submitter": "Oier Lopez De Lacalle", "authors": "Oier Lopez de Lacalle, Ander Salaberria, Aitor Soroa, Gorka Azkune and\n  Eneko Agirre", "title": "Evaluating Multimodal Representations on Visual Semantic Textual\n  Similarity", "comments": "Accepted in ECAI-2020, 8 pages, 6 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of visual and textual representations has produced excellent\nresults in tasks such as image captioning and visual question answering, but\nthe inference capabilities of multimodal representations are largely untested.\nIn the case of textual representations, inference tasks such as Textual\nEntailment and Semantic Textual Similarity have been often used to benchmark\nthe quality of textual representations. The long term goal of our research is\nto devise multimodal representation techniques that improve current inference\ncapabilities. We thus present a novel task, Visual Semantic Textual Similarity\n(vSTS), where such inference ability can be tested directly. Given two items\ncomprised each by an image and its accompanying caption, vSTS systems need to\nassess the degree to which the captions in context are semantically equivalent\nto each other. Our experiments using simple multimodal representations show\nthat the addition of image representations produces better inference, compared\nto text-only representations. The improvement is observed both when directly\ncomputing the similarity between the representations of the two items, and when\nlearning a siamese network based on vSTS training data. Our work shows, for the\nfirst time, the successful contribution of visual information to textual\ninference, with ample room for benchmarking more complex multimodal\nrepresentation options.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 09:03:04 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["de Lacalle", "Oier Lopez", ""], ["Salaberria", "Ander", ""], ["Soroa", "Aitor", ""], ["Azkune", "Gorka", ""], ["Agirre", "Eneko", ""]]}, {"id": "2004.01907", "submitter": "Dianbo Sui", "authors": "Dianbo Sui, Yubo Chen, Binjie Mao, Delai Qiu, Kang Liu and Jun Zhao", "title": "Knowledge Guided Metric Learning for Few-Shot Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of deep-learning-based text classification models relies heavily\non a huge amount of annotation data, which is difficult to obtain. When the\nlabeled data is scarce, models tend to struggle to achieve satisfactory\nperformance. However, human beings can distinguish new categories very\nefficiently with few examples. This is mainly due to the fact that human beings\ncan leverage knowledge obtained from relevant tasks. Inspired by human\nintelligence, we propose to introduce external knowledge into few-shot learning\nto imitate human knowledge. A novel parameter generator network is investigated\nto this end, which is able to use the external knowledge to generate relation\nnetwork parameters. Metrics can be transferred among tasks when equipped with\nthese generated parameters, so that similar tasks use similar metrics while\ndifferent tasks use different metrics. Through experiments, we demonstrate that\nour method outperforms the state-of-the-art few-shot text classification\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 10:56:26 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sui", "Dianbo", ""], ["Chen", "Yubo", ""], ["Mao", "Binjie", ""], ["Qiu", "Delai", ""], ["Liu", "Kang", ""], ["Zhao", "Jun", ""]]}, {"id": "2004.01909", "submitter": "Jimmy Lin", "authors": "Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai,\n  Chuan-Ju Wang, Jimmy Lin", "title": "Conversational Question Reformulation via Sequence-to-Sequence\n  Architectures and Pretrained Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an empirical study of conversational question\nreformulation (CQR) with sequence-to-sequence architectures and pretrained\nlanguage models (PLMs). We leverage PLMs to address the strong token-to-token\nindependence assumption made in the common objective, maximum likelihood\nestimation, for the CQR task. In CQR benchmarks of task-oriented dialogue\nsystems, we evaluate fine-tuned PLMs on the recently-introduced CANARD dataset\nas an in-domain task and validate the models using data from the TREC 2019 CAsT\nTrack as an out-domain task. Examining a variety of architectures with\ndifferent numbers of parameters, we demonstrate that the recent text-to-text\ntransfer transformer (T5) achieves the best results both on CANARD and CAsT\nwith fewer parameters, compared to similar transformer architectures.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 11:07:54 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Lin", "Sheng-Chieh", ""], ["Yang", "Jheng-Hong", ""], ["Nogueira", "Rodrigo", ""], ["Tsai", "Ming-Feng", ""], ["Wang", "Chuan-Ju", ""], ["Lin", "Jimmy", ""]]}, {"id": "2004.01912", "submitter": "Saku Sugawara", "authors": "Saku Sugawara, Pontus Stenetorp, Akiko Aizawa", "title": "Benchmarking Machine Reading Comprehension: A Psychological Perspective", "comments": "21 pages, EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading comprehension (MRC) has received considerable attention as a\nbenchmark for natural language understanding. However, the conventional task\ndesign of MRC lacks explainability beyond the model interpretation, i.e.,\nreading comprehension by a model cannot be explained in human terms. To this\nend, this position paper provides a theoretical basis for the design of MRC\ndatasets based on psychology as well as psychometrics, and summarizes it in\nterms of the prerequisites for benchmarking MRC. We conclude that future\ndatasets should (i) evaluate the capability of the model for constructing a\ncoherent and grounded representation to understand context-dependent situations\nand (ii) ensure substantive validity by shortcut-proof questions and\nexplanation as a part of the task design.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 11:45:27 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 12:06:28 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Sugawara", "Saku", ""], ["Stenetorp", "Pontus", ""], ["Aizawa", "Akiko", ""]]}, {"id": "2004.01926", "submitter": "Yulan Feng", "authors": "Yulan Feng, Shikib Mehri, Maxine Eskenazi, Tiancheng Zhao", "title": "\"None of the Above\":Measure Uncertainty in Dialog Response Retrieval", "comments": "Accepted to ACL 2020 as short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the importance of uncovering uncertainty in end-to-end\ndialog tasks, and presents our experimental results on uncertainty\nclassification on the Ubuntu Dialog Corpus. We show that, instead of retraining\nmodels for this specific purpose, the original retrieval model's underlying\nconfidence concerning the best prediction can be captured with trivial\nadditional computation.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 13:06:03 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 02:04:50 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Feng", "Yulan", ""], ["Mehri", "Shikib", ""], ["Eskenazi", "Maxine", ""], ["Zhao", "Tiancheng", ""]]}, {"id": "2004.01935", "submitter": "Yunlong Liang", "authors": "Yunlong Liang, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen and\n  Jie Zhou", "title": "An Iterative Knowledge Transfer Network with Routing for Aspect-based\n  Sentiment Analysis", "comments": "Code: https://github.com/XL2248/IKTN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based sentiment analysis (ABSA) mainly involves three subtasks: aspect\nterm extraction, opinion term extraction and aspect-level sentiment\nclassification, which are typically handled separately or (partially) jointly.\nHowever, the semantic interrelationships among all the three subtasks are not\nwell exploited in previous approaches, which restricts their performance.\nAdditionally, the linguistic knowledge from document-level labeled sentiment\ncorpora is usually used in a coarse way for the ABSA. To address these issues,\nwe propose a novel Iterative Knowledge Transfer Network (IKTN) for the\nend-to-end ABSA. For one thing, to fully exploit the semantic correlations\namong the three aspect-level subtasks for mutual promotion, the IKTN transfers\nthe task-specific knowledge from any two of the three subtasks to another one\nby leveraging a specially-designed routing algorithm, that is, any two of the\nthree subtasks will help the third one. Besides, the IKTN discriminately\ntransfers the document-level linguistic knowledge, i.e., domain-specific and\nsentiment-related knowledge, to the aspect-level subtasks to benefit the\ncorresponding ones. Experimental results on three benchmark datasets\ndemonstrate the effectiveness of our approach, which significantly outperforms\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 13:49:54 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liang", "Yunlong", ""], ["Meng", "Fandong", ""], ["Zhang", "Jinchao", ""], ["Xu", "Jinan", ""], ["Chen", "Yufeng", ""], ["Zhou", "Jie", ""]]}, {"id": "2004.01940", "submitter": "Jia-Chen Gu", "authors": "Jia-Chen Gu, Tianda Li, Quan Liu, Xiaodan Zhu, Zhen-Hua Ling, Yu-Ping\n  Ruan", "title": "Pre-Trained and Attention-Based Neural Networks for Building Noetic\n  Task-Oriented Dialogue Systems", "comments": "Accepted by AAAI 2020, Workshop on DSTC8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NOESIS II challenge, as the Track 2 of the 8th Dialogue System Technology\nChallenges (DSTC 8), is the extension of DSTC 7. This track incorporates new\nelements that are vital for the creation of a deployed task-oriented dialogue\nsystem. This paper describes our systems that are evaluated on all subtasks\nunder this challenge. We study the problem of employing pre-trained\nattention-based network for multi-turn dialogue systems. Meanwhile, several\nadaptation methods are proposed to adapt the pre-trained language models for\nmulti-turn dialogue systems, in order to keep the intrinsic property of\ndialogue systems. In the released evaluation results of Track 2 of DSTC 8, our\nproposed models ranked fourth in subtask 1, third in subtask 2, and first in\nsubtask 3 and subtask 4 respectively.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 14:14:43 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Gu", "Jia-Chen", ""], ["Li", "Tianda", ""], ["Liu", "Quan", ""], ["Zhu", "Xiaodan", ""], ["Ling", "Zhen-Hua", ""], ["Ruan", "Yu-Ping", ""]]}, {"id": "2004.01951", "submitter": "Yunlong Liang", "authors": "Yunlong Liang, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen and\n  Jie Zhou", "title": "A Dependency Syntactic Knowledge Augmented Interactive Architecture for\n  End-to-End Aspect-based Sentiment Analysis", "comments": "Code: https://github.com/XL2248/DREGCN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aspect-based sentiment analysis (ABSA) task remains to be a long-standing\nchallenge, which aims to extract the aspect term and then identify its\nsentiment orientation.In previous approaches, the explicit syntactic structure\nof a sentence, which reflects the syntax properties of natural language and\nhence is intuitively crucial for aspect term extraction and sentiment\nrecognition, is typically neglected or insufficiently modeled. In this paper,\nwe thus propose a novel dependency syntactic knowledge augmented interactive\narchitecture with multi-task learning for end-to-end ABSA. This model is\ncapable of fully exploiting the syntactic knowledge (dependency relations and\ntypes) by leveraging a well-designed Dependency Relation Embedded Graph\nConvolutional Network (DreGcn). Additionally, we design a simple yet effective\nmessage-passing mechanism to ensure that our model learns from multiple related\ntasks in a multi-task learning framework. Extensive experimental results on\nthree benchmark datasets demonstrate the effectiveness of our approach, which\nsignificantly outperforms existing state-of-the-art methods. Besides, we\nachieve further improvements by using BERT as an additional feature extractor.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 14:59:32 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liang", "Yunlong", ""], ["Meng", "Fandong", ""], ["Zhang", "Jinchao", ""], ["Xu", "Jinan", ""], ["Chen", "Yufeng", ""], ["Zhou", "Jie", ""]]}, {"id": "2004.01970", "submitter": "Siddhant Garg", "authors": "Siddhant Garg, Goutham Ramakrishnan", "title": "BAE: BERT-based Adversarial Examples for Text Classification", "comments": "Accepted at EMNLP 2020 Main Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern text classification models are susceptible to adversarial examples,\nperturbed versions of the original text indiscernible by humans which get\nmisclassified by the model. Recent works in NLP use rule-based synonym\nreplacement strategies to generate adversarial examples. These strategies can\nlead to out-of-context and unnaturally complex token replacements, which are\neasily identifiable by humans. We present BAE, a black box attack for\ngenerating adversarial examples using contextual perturbations from a BERT\nmasked language model. BAE replaces and inserts tokens in the original text by\nmasking a portion of the text and leveraging the BERT-MLM to generate\nalternatives for the masked tokens. Through automatic and human evaluations, we\nshow that BAE performs a stronger attack, in addition to generating adversarial\nexamples with improved grammaticality and semantic coherence as compared to\nprior work.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 16:25:48 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 16:44:29 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 00:41:43 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Garg", "Siddhant", ""], ["Ramakrishnan", "Goutham", ""]]}, {"id": "2004.01972", "submitter": "Yufan Zhao", "authors": "Yufan Zhao, Can Xu, Wei Wu, Lei Yu", "title": "Learning a Simple and Effective Model for Multi-turn Response Generation\n  with Auxiliary Tasks", "comments": null, "journal-ref": "EMNLP2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multi-turn response generation for open-domain dialogues. The\nexisting state-of-the-art addresses the problem with deep neural architectures.\nWhile these models improved response quality, their complexity also hinders the\napplication of the models in real systems. In this work, we pursue a model that\nhas a simple structure yet can effectively leverage conversation contexts for\nresponse generation. To this end, we propose four auxiliary tasks including\nword order recovery, utterance order recovery, masked word recovery, and masked\nutterance recovery, and optimize the objectives of these tasks together with\nmaximizing the likelihood of generation. By this means, the auxiliary tasks\nthat relate to context understanding can guide the learning of the generation\nmodel to achieve a better local optimum. Empirical studies with three\nbenchmarks indicate that our model can significantly outperform\nstate-of-the-art generation models in terms of response quality on both\nautomatic evaluation and human judgment, and at the same time enjoys a much\nfaster decoding process.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 16:37:00 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 07:11:33 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zhao", "Yufan", ""], ["Xu", "Can", ""], ["Wu", "Wei", ""], ["Yu", "Lei", ""]]}, {"id": "2004.01980", "submitter": "Di Jin", "authors": "Di Jin, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii, Peter Szolovits", "title": "Hooks in the Headline: Learning to Generate Headlines with Controlled\n  Styles", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": "12 pages", "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current summarization systems only produce plain, factual headlines, but do\nnot meet the practical needs of creating memorable titles to increase exposure.\nWe propose a new task, Stylistic Headline Generation (SHG), to enrich the\nheadlines with three style options (humor, romance and clickbait), in order to\nattract more readers. With no style-specific article-headline pair (only a\nstandard headline summarization dataset and mono-style corpora), our method\nTitleStylist generates style-specific headlines by combining the summarization\nand reconstruction tasks into a multitasking framework. We also introduced a\nnovel parameter sharing scheme to further disentangle the style from the text.\nThrough both automatic and human evaluation, we demonstrate that TitleStylist\ncan generate relevant, fluent headlines with three target styles: humor,\nromance, and clickbait. The attraction score of our model generated headlines\nsurpasses that of the state-of-the-art summarization model by 9.68%, and even\noutperforms human-written references.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 17:24:47 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 18:48:19 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 02:21:37 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Jin", "Di", ""], ["Jin", "Zhijing", ""], ["Zhou", "Joey Tianyi", ""], ["Orii", "Lisa", ""], ["Szolovits", "Peter", ""]]}, {"id": "2004.01981", "submitter": "Ze Yang", "authors": "Ze Yang, Wei Wu, Huang Hu, Can Xu, Wei Wang, Zhoujun Li", "title": "Open Domain Dialogue Generation with Latent Images", "comments": "AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider grounding open domain dialogues with images. Existing work\nassumes that both an image and a textual context are available, but\nimage-grounded dialogues by nature are more difficult to obtain than textual\ndialogues. Thus, we propose learning a response generation model with both\nimage-grounded dialogues and textual dialogues by assuming that the visual\nscene information at the time of a conversation can be represented by an image,\nand trying to recover the latent images of the textual dialogues through\ntext-to-image generation techniques. The likelihood of the two types of\ndialogues is then formulated by a response generator and an image reconstructor\nthat are learned within a conditional variational auto-encoding framework.\nEmpirical studies are conducted in both image-grounded conversation and\ntext-based conversation. In the first scenario, image-grounded dialogues,\nespecially under a low-resource setting, can be effectively augmented by\ntextual dialogues with latent images; while in the second scenario, latent\nimages can enrich the content of responses and at the same time keep them\nrelevant to contexts.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 17:32:46 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 07:43:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yang", "Ze", ""], ["Wu", "Wei", ""], ["Hu", "Huang", ""], ["Xu", "Can", ""], ["Wang", "Wei", ""], ["Li", "Zhoujun", ""]]}, {"id": "2004.02001", "submitter": "Ming Tu", "authors": "Ming Tu, Jing Huang, Xiaodong He, Bowen Zhou", "title": "Graph Sequential Network for Reasoning over Sequences", "comments": "Part of this paper was presented at NeurIPS 2019 Workshop on Graph\n  Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Graph Neural Network (GNN) has been applied successfully to various\nNLP tasks that require reasoning, such as multi-hop machine reading\ncomprehension. In this paper, we consider a novel case where reasoning is\nneeded over graphs built from sequences, i.e. graph nodes with sequence data.\nExisting GNN models fulfill this goal by first summarizing the node sequences\ninto fixed-dimensional vectors, then applying GNN on these vectors. To avoid\ninformation loss inherent in the early summarization and make sequential\nlabeling tasks on GNN output feasible, we propose a new type of GNN called\nGraph Sequential Network (GSN), which features a new message passing algorithm\nbased on co-attention between a node and each of its neighbors. We validate the\nproposed GSN on two NLP tasks: interpretable multi-hop reading comprehension on\nHotpotQA and graph based fact verification on FEVER. Both tasks require\nreasoning over multiple documents or sentences. Our experimental results show\nthat the proposed GSN attains better performance than the standard GNN based\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 19:18:54 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Tu", "Ming", ""], ["Huang", "Jing", ""], ["He", "Xiaodong", ""], ["Zhou", "Bowen", ""]]}, {"id": "2004.02002", "submitter": "Tiancheng Zhao", "authors": "Tianchang Zhao and Kyusong Lee", "title": "Talk to Papers: Bringing Neural Question Answering to Academic Search", "comments": "demo paper accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Talk to Papers, which exploits the recent open-domain question\nanswering (QA) techniques to improve the current experience of academic search.\nIt's designed to enable researchers to use natural language queries to find\nprecise answers and extract insights from a massive amount of academic papers.\nWe present a large improvement over classic search engine baseline on several\nstandard QA datasets and provide the community a collaborative data collection\ntool to curate the first natural language processing research QA dataset via a\ncommunity effort.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 19:19:55 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 14:38:11 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 20:26:28 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Zhao", "Tianchang", ""], ["Lee", "Kyusong", ""]]}, {"id": "2004.02015", "submitter": "Hanjie Chen", "authors": "Hanjie Chen, Guangtao Zheng, Yangfeng Ji", "title": "Generating Hierarchical Explanations on Text Classification via Feature\n  Interaction Detection", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating explanations for neural networks has become crucial for their\napplications in real-world with respect to reliability and trustworthiness. In\nnatural language processing, existing methods usually provide important\nfeatures which are words or phrases selected from an input text as an\nexplanation, but ignore the interactions between them. It poses challenges for\nhumans to interpret an explanation and connect it to model prediction. In this\nwork, we build hierarchical explanations by detecting feature interactions.\nSuch explanations visualize how words and phrases are combined at different\nlevels of the hierarchy, which can help users understand the decision-making of\nblack-box models. The proposed method is evaluated with three neural text\nclassifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic\nand human evaluations. Experiments show the effectiveness of the proposed\nmethod in providing explanations that are both faithful to models and\ninterpretable to humans.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 20:56:37 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 21:52:28 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 02:30:20 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chen", "Hanjie", ""], ["Zheng", "Guangtao", ""], ["Ji", "Yangfeng", ""]]}, {"id": "2004.02016", "submitter": "Chenguang Zhu", "authors": "Chenguang Zhu, Ruochen Xu, Michael Zeng, Xuedong Huang", "title": "A Hierarchical Network for Abstractive Meeting Summarization with\n  Cross-Domain Pretraining", "comments": "Accepted in Empirical Methods in Natural Language Processing (EMNLP),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the abundance of automatic meeting transcripts, meeting summarization is\nof great interest to both participants and other parties. Traditional methods\nof summarizing meetings depend on complex multi-step pipelines that make joint\noptimization intractable. Meanwhile, there are a handful of deep neural models\nfor text summarization and dialogue systems. However, the semantic structure\nand styles of meeting transcripts are quite different from articles and\nconversations. In this paper, we propose a novel abstractive summary network\nthat adapts to the meeting scenario. We design a hierarchical structure to\naccommodate long meeting transcripts and a role vector to depict the difference\namong speakers. Furthermore, due to the inadequacy of meeting summary data, we\npretrain the model on large-scale news summary data. Empirical results show\nthat our model outperforms previous approaches in both automatic metrics and\nhuman evaluation. For example, on ICSI dataset, the ROUGE-1 score increases\nfrom 34.66% to 46.28%.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 21:00:41 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 19:59:39 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 19:43:37 GMT"}, {"version": "v4", "created": "Sun, 20 Sep 2020 05:47:23 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhu", "Chenguang", ""], ["Xu", "Ruochen", ""], ["Zeng", "Michael", ""], ["Huang", "Xuedong", ""]]}, {"id": "2004.02032", "submitter": "Hammad Ayyubi", "authors": "Hammad A. Ayyubi, Md. Mehrab Tanjim, Julian J. McAuley, and Garrison\n  W. Cottrell", "title": "Generating Rationales in Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in Visual QuestionAnswering (VQA), it remains a\nchallenge todetermine how much success can be attributedto sound reasoning and\ncomprehension ability.We seek to investigate this question by propos-ing a new\ntask ofrationale generation. Es-sentially, we task a VQA model with generat-ing\nrationales for the answers it predicts. Weuse data from the Visual Commonsense\nRea-soning (VCR) task, as it contains ground-truthrationales along with visual\nquestions and an-swers. We first investigate commonsense un-derstanding in one\nof the leading VCR mod-els, ViLBERT, by generating rationales frompretrained\nweights using a state-of-the-art lan-guage model, GPT-2. Next, we seek to\njointlytrain ViLBERT with GPT-2 in an end-to-endfashion with the dual task of\npredicting the an-swer in VQA and generating rationales. Weshow that this kind\nof training injects com-monsense understanding in the VQA modelthrough\nquantitative and qualitative evaluationmetrics\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 22:15:35 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Ayyubi", "Hammad A.", ""], ["Tanjim", "Md. Mehrab", ""], ["McAuley", "Julian J.", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "2004.02071", "submitter": "Mihir Kale", "authors": "Sreyashi Nag and Mihir Kale and Varun Lakshminarasimhan and Swapnil\n  Singhavi", "title": "Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised\n  Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore ways of incorporating bilingual dictionaries to enable\nsemi-supervised neural machine translation. Conventional back-translation\nmethods have shown success in leveraging target side monolingual data. However,\nsince the quality of back-translation models is tied to the size of the\navailable parallel corpora, this could adversely impact the synthetically\ngenerated sentences in a low resource setting. We propose a simple data\naugmentation technique to address both this shortcoming. We incorporate widely\navailable bilingual dictionaries that yield word-by-word translations to\ngenerate synthetic sentences. This automatically expands the vocabulary of the\nmodel while maintaining high quality content. Our method shows an appreciable\nimprovement in performance over strong baselines.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 02:14:14 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Nag", "Sreyashi", ""], ["Kale", "Mihir", ""], ["Lakshminarasimhan", "Varun", ""], ["Singhavi", "Swapnil", ""]]}, {"id": "2004.02077", "submitter": "Mihir Kale", "authors": "Mihir Kale and Scott Roy", "title": "Machine Translation Pre-training for Data-to-Text Generation -- A Case\n  Study in Czech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there is a large body of research studying deep learning methods for\ntext generation from structured data, almost all of it focuses purely on\nEnglish. In this paper, we study the effectiveness of machine translation based\npre-training for data-to-text generation in non-English languages. Since the\nstructured data is generally expressed in English, text generation into other\nlanguages involves elements of translation, transliteration and copying -\nelements already encoded in neural machine translation systems. Moreover, since\ndata-to-text corpora are typically small, this task can benefit greatly from\npre-training. Based on our experiments on Czech, a morphologically complex\nlanguage, we find that pre-training lets us train end-to-end models with\nsignificantly improved performance, as judged by automatic metrics and human\nevaluation. We also show that this approach enjoys several desirable\nproperties, including improved performance in low data scenarios and robustness\nto unseen slot values.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 02:47:16 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kale", "Mihir", ""], ["Roy", "Scott", ""]]}, {"id": "2004.02083", "submitter": "Antonios Anastasopoulos", "authors": "Hilaria Cruz, Gregory Stump, and Antonios Anastasopoulos", "title": "A Resource for Studying Chatino Verbal Morphology", "comments": "accepted at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first resource focusing on the verbal inflectional morphology\nof San Juan Quiahije Chatino, a tonal mesoamerican language spoken in Mexico.\nWe provide a collection of complete inflection tables of 198 lemmata, with\nmorphological tags based on the UniMorph schema. We also provide baseline\nresults on three core NLP tasks: morphological analysis, lemmatization, and\nmorphological inflection.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 03:30:01 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Cruz", "Hilaria", ""], ["Stump", "Gregory", ""], ["Anastasopoulos", "Antonios", ""]]}, {"id": "2004.02105", "submitter": "Roee Aharoni", "authors": "Roee Aharoni and Yoav Goldberg", "title": "Unsupervised Domain Clusters in Pretrained Language Models", "comments": "Accepted as a long paper in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of \"in-domain data\" in NLP is often over-simplistic and vague, as\ntextual data varies in many nuanced linguistic aspects such as topic, style or\nlevel of formality. In addition, domain labels are many times unavailable,\nmaking it challenging to build domain-specific systems. We show that massive\npre-trained language models implicitly learn sentence representations that\ncluster by domains without supervision -- suggesting a simple data-driven\ndefinition of domains in textual data. We harness this property and propose\ndomain data selection methods based on such models, which require only a small\nset of in-domain monolingual data. We evaluate our data selection methods for\nneural machine translation across five diverse domains, where they outperform\nan established approach as measured by both BLEU and by precision and recall of\nsentence selection with respect to an oracle.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 06:22:16 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 16:49:27 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Aharoni", "Roee", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2004.02118", "submitter": "Bang Liu", "authors": "Bang Liu, Weidong Guo, Di Niu, Jinwen Luo, Chaoyue Wang, Zhen Wen, Yu\n  Xu", "title": "GIANT: Scalable Creation of a Web-scale Ontology", "comments": "Accepted as full paper by SIGMOD 2020", "journal-ref": null, "doi": "10.1145/3318464.3386145", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding what online users may pay attention to is key to content\nrecommendation and search services. These services will benefit from a highly\nstructured and web-scale ontology of entities, concepts, events, topics and\ncategories. While existing knowledge bases and taxonomies embody a large volume\nof entities and categories, we argue that they fail to discover properly\ngrained concepts, events and topics in the language style of online population.\nNeither is a logically structured ontology maintained among these notions. In\nthis paper, we present GIANT, a mechanism to construct a user-centered,\nweb-scale, structured ontology, containing a large number of natural language\nphrases conforming to user attentions at various granularities, mined from a\nvast volume of web documents and search click graphs. Various types of edges\nare also constructed to maintain a hierarchy in the ontology. We present our\ngraph-neural-network-based techniques used in GIANT, and evaluate the proposed\nmethods as compared to a variety of baselines. GIANT has produced the Attention\nOntology, which has been deployed in various Tencent applications involving\nover a billion users. Online A/B testing performed on Tencent QQ Browser shows\nthat Attention Ontology can significantly improve click-through rates in news\nrecommendation.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 07:51:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liu", "Bang", ""], ["Guo", "Weidong", ""], ["Niu", "Di", ""], ["Luo", "Jinwen", ""], ["Wang", "Chaoyue", ""], ["Wen", "Zhen", ""], ["Xu", "Yu", ""]]}, {"id": "2004.02127", "submitter": "Zuchao Li", "authors": "Zuchao Li, Hai Zhao, Rui Wang, Masao Utiyama, Eiichiro Sumita", "title": "Reference Language based Unsupervised Neural Machine Translation", "comments": "EMNLP 2020, ACL Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting a common language as an auxiliary for better translation has a\nlong tradition in machine translation and lets supervised learning-based\nmachine translation enjoy the enhancement delivered by the well-used pivot\nlanguage in the absence of a source language to target language parallel\ncorpus. The rise of unsupervised neural machine translation (UNMT) almost\ncompletely relieves the parallel corpus curse, though UNMT is still subject to\nunsatisfactory performance due to the vagueness of the clues available for its\ncore back-translation training. Further enriching the idea of pivot translation\nby extending the use of parallel corpora beyond the source-target paradigm, we\npropose a new reference language-based framework for UNMT, RUNMT, in which the\nreference language only shares a parallel corpus with the source, but this\ncorpus still indicates a signal clear enough to help the reconstruction\ntraining of UNMT through a proposed reference agreement mechanism. Experimental\nresults show that our methods improve the quality of UNMT over that of a strong\nbaseline that uses only one auxiliary language, demonstrating the usefulness of\nthe proposed reference language-based UNMT and establishing a good start for\nthe community.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 08:28:08 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 15:48:59 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Li", "Zuchao", ""], ["Zhao", "Hai", ""], ["Wang", "Rui", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""]]}, {"id": "2004.02135", "submitter": "Peng Jin", "authors": "Xingyuan Chen, Ping Cai, Peng Jin, Hongjun Wang, Xinyu Dai, Jiajun\n  Chen", "title": "Adding A Filter Based on The Discriminator to Improve Unconditional Text\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autoregressive language model (ALM) trained with maximum likelihood\nestimation (MLE) is widely used in unconditional text generation. Due to\nexposure bias, the generated texts still suffer from low quality and diversity.\nThis presents statistically as a discrepancy between the real text and\ngenerated text. Some research shows a discriminator can detect this\ndiscrepancy. Because the discriminator can encode more information than the\ngenerator, discriminator has the potentiality to improve generator. To\nalleviate the exposure bias, generative adversarial networks (GAN) use the\ndiscriminator to update the generator's parameters directly, but they fail by\nbeing evaluated precisely. A critical reason for the failure is the difference\nbetween the discriminator input and the ALM input. We propose a novel mechanism\nby adding a filter which has the same input as the discriminator. First,\ndiscriminator detects the discrepancy signals and passes to filter directly (or\nby learning). Then, we use the filter to reject some generated samples with a\nsampling-based method. Thus, the original generative distribution is revised to\nreduce the discrepancy. Two ALMs, RNN-based and Transformer-based, are\nexperimented. Evaluated precisely by three metrics, our mechanism consistently\noutperforms the ALMs and all kinds of GANs across two benchmark data sets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 09:34:52 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 02:41:21 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 02:51:41 GMT"}, {"version": "v4", "created": "Tue, 19 May 2020 23:16:52 GMT"}, {"version": "v5", "created": "Mon, 22 Jun 2020 04:59:43 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Chen", "Xingyuan", ""], ["Cai", "Ping", ""], ["Jin", "Peng", ""], ["Wang", "Hongjun", ""], ["Dai", "Xinyu", ""], ["Chen", "Jiajun", ""]]}, {"id": "2004.02143", "submitter": "Deepak Gupta", "authors": "Deepak Gupta, Hardik Chauhan, Akella Ravi Tej, Asif Ekbal and Pushpak\n  Bhattacharyya", "title": "Reinforced Multi-task Approach for Multi-hop Question Generation", "comments": "Accepted for publication in COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Question generation (QG) attempts to solve the inverse of question answering\n(QA) problem by generating a natural language question given a document and an\nanswer. While sequence to sequence neural models surpass rule-based systems for\nQG, they are limited in their capacity to focus on more than one supporting\nfact. For QG, we often require multiple supporting facts to generate\nhigh-quality questions. Inspired by recent works on multi-hop reasoning in QA,\nwe take up Multi-hop question generation, which aims at generating relevant\nquestions based on supporting facts in the context. We employ multitask\nlearning with the auxiliary task of answer-aware supporting fact prediction to\nguide the question generator. In addition, we also proposed a question-aware\nreward function in a Reinforcement Learning (RL) framework to maximize the\nutilization of the supporting facts. We demonstrate the effectiveness of our\napproach through experiments on the multi-hop question answering dataset,\nHotPotQA. Empirical evaluation shows our model to outperform the single-hop\nneural question generation models on both automatic evaluation metrics such as\nBLEU, METEOR, and ROUGE, and human evaluation metrics for quality and coverage\nof the generated questions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 10:16:59 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 05:31:39 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 11:35:14 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2020 14:06:12 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Gupta", "Deepak", ""], ["Chauhan", "Hardik", ""], ["Tej", "Akella Ravi", ""], ["Ekbal", "Asif", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "2004.02178", "submitter": "Weijie Liu", "authors": "Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, Qi Ju", "title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time", "comments": "This manuscript has been accepted to appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models like BERT have proven to be highly performant.\nHowever, they are often computationally expensive in many practical scenarios,\nfor such heavy models can hardly be readily implemented with limited resources.\nTo improve their efficiency with an assured model performance, we propose a\nnovel speed-tunable FastBERT with adaptive inference time. The speed at\ninference can be flexibly adjusted under varying demands, while redundant\ncalculation of samples is avoided. Moreover, this model adopts a unique\nself-distillation mechanism at fine-tuning, further enabling a greater\ncomputational efficacy with minimal loss in performance. Our model achieves\npromising results in twelve English and Chinese datasets. It is able to speed\nup by a wide range from 1 to 12 times than BERT if given different speedup\nthresholds to make a speed-performance tradeoff.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 12:29:20 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 15:46:34 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Liu", "Weijie", ""], ["Zhou", "Peng", ""], ["Zhao", "Zhe", ""], ["Wang", "Zhiruo", ""], ["Deng", "Haotang", ""], ["Ju", "Qi", ""]]}, {"id": "2004.02181", "submitter": "Lemao Liu", "authors": "Guanlin Li, Lemao Liu, Conghui Zhu, Tiejun Zhao, Shuming Shi", "title": "Detecting and Understanding Generalization Barriers for Neural Machine\n  Translation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization to unseen instances is our eternal pursuit for all data-driven\nmodels. However, for realistic task like machine translation, the traditional\napproach measuring generalization in an average sense provides poor\nunderstanding for the fine-grained generalization ability. As a remedy, this\npaper attempts to identify and understand generalization barrier words within\nan unseen input sentence that \\textit{cause} the degradation of fine-grained\ngeneralization. We propose a principled definition of generalization barrier\nwords and a modified version which is tractable in computation. Based on the\nmodified one, we propose three simple methods for barrier detection by the\nsearch-aware risk estimation through counterfactual generation. We then conduct\nextensive analyses on those detected generalization barrier words on both\nZh$\\Leftrightarrow$En NIST benchmarks from various perspectives. Potential\nusage of the detected barrier words is also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 12:33:51 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Li", "Guanlin", ""], ["Liu", "Lemao", ""], ["Zhu", "Conghui", ""], ["Zhao", "Tiejun", ""], ["Shi", "Shuming", ""]]}, {"id": "2004.02192", "submitter": "Ahmed Abdelali", "authors": "Hamdy Mubarak, Ammar Rashed, Kareem Darwish, Younes Samih, Ahmed\n  Abdelali", "title": "Arabic Offensive Language on Twitter: Analysis and Experiments", "comments": "10 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting offensive language on Twitter has many applications ranging from\ndetecting/predicting bullying to measuring polarization. In this paper, we\nfocus on building a large Arabic offensive tweet dataset. We introduce a method\nfor building a dataset that is not biased by topic, dialect, or target. We\nproduce the largest Arabic dataset to date with special tags for vulgarity and\nhate speech. We thoroughly analyze the dataset to determine which topics,\ndialects, and gender are most associated with offensive tweets and how Arabic\nspeakers use offensive language. Lastly, we conduct many experiments to produce\nstrong results (F1 = 83.2) on the dataset using SOTA techniques.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 13:05:11 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 10:28:48 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 20:22:18 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Mubarak", "Hamdy", ""], ["Rashed", "Ammar", ""], ["Darwish", "Kareem", ""], ["Samih", "Younes", ""], ["Abdelali", "Ahmed", ""]]}, {"id": "2004.02196", "submitter": "Shanbo Cheng", "authors": "Shanbo Cheng, Shaohui Kuang, Rongxiang Weng, Heng Yu, Changfeng Zhu,\n  Weihua Luo", "title": "AR: Auto-Repair the Synthetic Data for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with only using limited authentic parallel data as training corpus,\nmany studies have proved that incorporating synthetic parallel data, which\ngenerated by back translation (BT) or forward translation (FT, or\nselftraining), into the NMT training process can significantly improve\ntranslation quality. However, as a well-known shortcoming, synthetic parallel\ndata is noisy because they are generated by an imperfect NMT system. As a\nresult, the improvements in translation quality bring by the synthetic parallel\ndata are greatly diminished. In this paper, we propose a novel Auto- Repair\n(AR) framework to improve the quality of synthetic data. Our proposed AR model\ncan learn the transformation from low quality (noisy) input sentence to high\nquality sentence based on large scale monolingual data with BT and FT\ntechniques. The noise in synthetic parallel data will be sufficiently\neliminated by the proposed AR model and then the repaired synthetic parallel\ndata can help the NMT models to achieve larger improvements. Experimental\nresults show that our approach can effective improve the quality of synthetic\nparallel data and the NMT model with the repaired synthetic data achieves\nconsistent improvements on both WMT14 EN!DE and IWSLT14 DE!EN translation\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 13:18:18 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Cheng", "Shanbo", ""], ["Kuang", "Shaohui", ""], ["Weng", "Rongxiang", ""], ["Yu", "Heng", ""], ["Zhu", "Changfeng", ""], ["Luo", "Weihua", ""]]}, {"id": "2004.02199", "submitter": "Lemao Liu", "authors": "Conghui Zhu, Guanlin Li, Lemao Liu, Tiejun Zhao, Shuming Shi", "title": "Understanding Learning Dynamics for Neural Machine Translation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of NMT, there still remains a severe challenge: it\nis hard to interpret the internal dynamics during its training process. In this\npaper we propose to understand learning dynamics of NMT by using a recent\nproposed technique named Loss Change Allocation\n(LCA)~\\citep{lan-2019-loss-change-allocation}. As LCA requires calculating the\ngradient on an entire dataset for each update, we instead present an\napproximate to put it into practice in NMT scenario. %motivated by the lesson\nfrom sgd. Our simulated experiment shows that such approximate calculation is\nefficient and is empirically proved to deliver consistent results to the\nbrute-force implementation. In particular, extensive experiments on two\nstandard translation benchmark datasets reveal some valuable findings.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 13:32:58 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhu", "Conghui", ""], ["Li", "Guanlin", ""], ["Liu", "Lemao", ""], ["Zhao", "Tiejun", ""], ["Shi", "Shuming", ""]]}, {"id": "2004.02202", "submitter": "Yixuan Su", "authors": "Yixuan Su, Deng Cai, Yan Wang, Simon Baker, Anna Korhonen, Nigel\n  Collier, Xiaojiang Liu", "title": "Stylistic Dialogue Generation via Information-Guided Reinforcement\n  Learning Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stylistic response generation is crucial for building an engaging dialogue\nsystem for industrial use. While it has attracted much research interest,\nexisting methods often generate stylistic responses at the cost of the content\nquality (relevance and fluency). To enable better balance between the content\nquality and the style, we introduce a new training strategy, know as\nInformation-Guided Reinforcement Learning (IG-RL). In IG-RL, a training model\nis encouraged to explore stylistic expressions while being constrained to\nmaintain its content quality. This is achieved by adopting reinforcement\nlearning strategy with statistical style information guidance for\nquality-preserving explorations. Experiments on two datasets show that the\nproposed approach outperforms several strong baselines in terms of the overall\nresponse performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 13:58:14 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Su", "Yixuan", ""], ["Cai", "Deng", ""], ["Wang", "Yan", ""], ["Baker", "Simon", ""], ["Korhonen", "Anna", ""], ["Collier", "Nigel", ""], ["Liu", "Xiaojiang", ""]]}, {"id": "2004.02211", "submitter": "No\\'e Casas", "authors": "Noe Casas, Jos\\'e A. R. Fonollosa, Marta R. Costa-juss\\`a", "title": "Syntax-driven Iterative Expansion Language Models for Controllable Text\n  Generation", "comments": "Accepted at the EMNLP 2020 Workshop on Structured Prediction for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant language modeling paradigm handles text as a sequence of\ndiscrete tokens. While that approach can capture the latent structure of the\ntext, it is inherently constrained to sequential dynamics for text generation.\nWe propose a new paradigm for introducing a syntactic inductive bias into\nneural text generation, where the dependency parse tree is used to drive the\nTransformer model to generate sentences iteratively.\n  Our experiments show that this paradigm is effective at text generation, with\nquality between LSTMs and Transformers, and comparable diversity, requiring\nless than half their decoding steps, and its generation process allows direct\ncontrol over the syntactic constructions of the generated text, enabling the\ninduction of stylistic variations.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 14:29:40 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 11:10:41 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Casas", "Noe", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""], ["Costa-juss\u00e0", "Marta R.", ""]]}, {"id": "2004.02214", "submitter": "Yixuan Su", "authors": "Yixuan Su, Yan Wang, Simon Baker, Deng Cai, Xiaojiang Liu, Anna\n  Korhonen, Nigel Collier", "title": "Prototype-to-Style: Dialogue Generation with Style-Aware Editing on\n  Retrieval Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of a dialog system to express prespecified language style during\nconversations has a direct, positive impact on its usability and on user\nsatisfaction. We introduce a new prototype-to-style (PS) framework to tackle\nthe challenge of stylistic dialogue generation. The framework uses an\nInformation Retrieval (IR) system and extracts a response prototype from the\nretrieved response. A stylistic response generator then takes the prototype and\nthe desired language style as model input to obtain a high-quality and\nstylistic response. To effectively train the proposed model, we propose a new\nstyle-aware learning objective as well as a de-noising learning strategy.\nResults on three benchmark datasets from two languages demonstrate that the\nproposed approach significantly outperforms existing baselines in both\nin-domain and cross-domain evaluations\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 14:36:15 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Su", "Yixuan", ""], ["Wang", "Yan", ""], ["Baker", "Simon", ""], ["Cai", "Deng", ""], ["Liu", "Xiaojiang", ""], ["Korhonen", "Anna", ""], ["Collier", "Nigel", ""]]}, {"id": "2004.02219", "submitter": "Mayank Tripathi Mr", "authors": "Mayank Tripathi, Divyanshu Singh, Seba Susan", "title": "Speaker Recognition using SincNet and X-Vector Fusion", "comments": "The 19th International Conference on Artificial Intelligence and Soft\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an innovative approach to perform speaker\nrecognition by fusing two recently introduced deep neural networks (DNNs)\nnamely - SincNet and X-Vector. The idea behind using SincNet filters on the raw\nspeech waveform is to extract more distinguishing frequency-related features in\nthe initial convolution layers of the CNN architecture. X-Vectors are used to\ntake advantage of the fact that this embedding is an efficient method to churn\nout fixed dimension features from variable length speech utterances, something\nwhich is challenging in plain CNN techniques, making it efficient both in terms\nof speed and accuracy. Our approach uses the best of both worlds by combining\nX-vector in the later layers while using SincNet filters in the initial layers\nof our deep model. This approach allows the network to learn better embedding\nand converge quicker. Previous works use either X-Vector or SincNet Filters or\nsome modifications, however we introduce a novel fusion architecture wherein we\nhave combined both the techniques to gather more information about the speech\nsignal hence, giving us better results. Our method focuses on the VoxCeleb1\ndataset for speaker recognition, and we have used it for both training and\ntesting purposes.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 14:44:14 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Tripathi", "Mayank", ""], ["Singh", "Divyanshu", ""], ["Susan", "Seba", ""]]}, {"id": "2004.02251", "submitter": "He Bai", "authors": "He Bai, Peng Shi, Jimmy Lin, Luchen Tan, Kun Xiong, Wen Gao, Jie Liu,\n  Ming Li", "title": "Semantics of the Unwritten: The Effect of End of Paragraph and Sequence\n  Tokens on Text Generation with GPT2", "comments": "Accepted by ACL-IJCNLP SRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantics of a text is manifested not only by what is read, but also by\nwhat is not read. In this article, we will study how the implicit \"not read\"\ninformation such as end-of-paragraph (\\eop) and end-of-sequence (\\eos) affect\nthe quality of text generation. Specifically, we find that the pre-trained\nlanguage model GPT2 can generate better continuations by learning to generate\nthe \\eop in the fine-tuning stage. Experimental results on English story\ngeneration show that \\eop can lead to higher BLEU score and lower \\eos\nperplexity. We also conduct experiments on a self-collected Chinese essay\ndataset with Chinese-GPT2, a character level LM without \\eop or \\eos during\npre-training. Experimental results show that the Chinese GPT2 can generate\nbetter essay endings with \\eop.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 16:55:09 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 18:26:58 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Bai", "He", ""], ["Shi", "Peng", ""], ["Lin", "Jimmy", ""], ["Tan", "Luchen", ""], ["Xiong", "Kun", ""], ["Gao", "Wen", ""], ["Liu", "Jie", ""], ["Li", "Ming", ""]]}, {"id": "2004.02256", "submitter": "K.R. Chowdhary", "authors": "K. R. Chowdhary", "title": "Natural language processing for word sense disambiguation and\n  information extraction", "comments": "150 pages, PhD Thesis", "journal-ref": null, "doi": null, "report-no": "cse-mbm-krc-p-thesis-04", "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research work deals with Natural Language Processing (NLP) and\nextraction of essential information in an explicit form. The most common among\nthe information management strategies is Document Retrieval (DR) and\nInformation Filtering. DR systems may work as combine harvesters, which bring\nback useful material from the vast fields of raw material. With large amount of\npotentially useful information in hand, an Information Extraction (IE) system\ncan then transform the raw material by refining and reducing it to a germ of\noriginal text. A Document Retrieval system collects the relevant documents\ncarrying the required information, from the repository of texts. An IE system\nthen transforms them into information that is more readily digested and\nanalyzed. It isolates relevant text fragments, extracts relevant information\nfrom the fragments, and then arranges together the targeted information in a\ncoherent framework. The thesis presents a new approach for Word Sense\nDisambiguation using thesaurus. The illustrative examples supports the\neffectiveness of this approach for speedy and effective disambiguation. A\nDocument Retrieval method, based on Fuzzy Logic has been described and its\napplication is illustrated. A question-answering system describes the operation\nof information extraction from the retrieved text documents. The process of\ninformation extraction for answering a query is considerably simplified by\nusing a Structured Description Language (SDL) which is based on cardinals of\nqueries in the form of who, what, when, where and why. The thesis concludes\nwith the presentation of a novel strategy based on Dempster-Shafer theory of\nevidential reasoning, for document retrieval and information extraction. This\nstrategy permits relaxation of many limitations, which are inherent in Bayesian\nprobabilistic approach.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 17:13:43 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chowdhary", "K. R.", ""]]}, {"id": "2004.02286", "submitter": "Tongfei Chen", "authors": "Tongfei Chen, Yunmo Chen, Benjamin Van Durme", "title": "Hierarchical Entity Typing via Multi-level Learning to Rank", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for hierarchical entity classification that\nembraces ontological structure at both training and during prediction. At\ntraining, our novel multi-level learning-to-rank loss compares positive types\nagainst negative siblings according to the type tree. During prediction, we\ndefine a coarse-to-fine decoder that restricts viable candidates at each level\nof the ontology based on already predicted parent type(s). We achieve\nstate-of-the-art across multiple datasets, particularly with respect to strict\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 19:27:18 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chen", "Tongfei", ""], ["Chen", "Yunmo", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2004.02288", "submitter": "Subendhu Rongali", "authors": "Subendhu Rongali, Abhyuday Jagannatha, Bhanu Pratap Singh Rawat, and\n  Hong Yu", "title": "Continual Domain-Tuning for Pretrained Language Models", "comments": "Updated from a previous shorter version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (LM) such as BERT, DistilBERT, and RoBERTa can be\ntuned for different domains (domain-tuning) by continuing the pre-training\nphase on a new target domain corpus. This simple domain tuning (SDT) technique\nhas been widely used to create domain-tuned models such as BioBERT, SciBERT and\nClinicalBERT. However, during the pretraining phase on the target domain, the\nLM models may catastrophically forget the patterns learned from their source\ndomain. In this work, we study the effects of catastrophic forgetting on\ndomain-tuned LM models and investigate methods that mitigate its negative\neffects. We propose continual learning (CL) based alternatives for SDT, that\naim to reduce catastrophic forgetting. We show that these methods may increase\nthe performance of LM models on downstream target domain tasks. Additionally,\nwe also show that constraining the LM model from forgetting the source domain\nleads to downstream task models that are more robust to domain shifts. We\nanalyze the computational cost of using our proposed CL methods and provide\nrecommendations for computationally lightweight and effective CL domain-tuning\nprocedures.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 19:31:44 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 14:50:02 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Rongali", "Subendhu", ""], ["Jagannatha", "Abhyuday", ""], ["Rawat", "Bhanu Pratap Singh", ""], ["Yu", "Hong", ""]]}, {"id": "2004.02334", "submitter": "Thamme Gowda", "authors": "Thamme Gowda, Jonathan May", "title": "Finding the Optimal Vocabulary Size for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": "10.18653/v1/2020.findings-emnlp.352", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We cast neural machine translation (NMT) as a classification task in an\nautoregressive setting and analyze the limitations of both classification and\nautoregression components. Classifiers are known to perform better with\nbalanced class distributions during training. Since the Zipfian nature of\nlanguages causes imbalanced classes, we explore its effect on NMT. We analyze\nthe effect of various vocabulary sizes on NMT performance on multiple languages\nwith many data sizes, and reveal an explanation for why certain vocabulary\nsizes are better than others.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:17:34 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:19:16 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gowda", "Thamme", ""], ["May", "Jonathan", ""]]}, {"id": "2004.02346", "submitter": "Osnat Mokryn", "authors": "Osnat Mokryn and Hagit Ben-Shoshan", "title": "Domain-based Latent Personal Analysis and its use for impersonation\n  detection in social media", "comments": "Submitted to Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zipf's law defines an inverse proportion between a word's ranking in a given\ncorpus and its frequency in it, roughly dividing the vocabulary into frequent\nwords and infrequent ones. Here, we stipulate that within a domain an author's\nsignature can be derived from, in loose terms, the author's missing popular\nwords and frequently used infrequent-words. We devise a method, termed Latent\nPersonal Analysis (LPA), for finding domain-based attributes for entities in a\ndomain: their distance from the domain and their signature, which determines\nhow they most differ from a domain. We identify the most suitable distance\nmetric for the method among several and construct the distances and personal\nsignatures for authors, the domain's entities. The signature consists of both\nover-used terms (compared to the average), and missing popular terms. We\nvalidate the correctness and power of the signatures in identifying users and\nset existence conditions. We then show uses for the method in explainable\nauthorship attribution: we define algorithms that utilize LPA to identify two\ntypes of impersonation in social media: (1) authors with sockpuppets (multiple)\naccounts; (2) front users accounts, operated by several authors. We validate\nthe algorithms and employ them over a large scale dataset obtained from a\nsocial media site with over 4000 users. We corroborate these results using\ntemporal rate analysis. LPA can further be used to devise personal attributes\nin a wide range of scientific domains in which the constituents have a\nlong-tail distribution of elements.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 23:00:09 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 15:33:39 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Mokryn", "Osnat", ""], ["Ben-Shoshan", "Hagit", ""]]}, {"id": "2004.02349", "submitter": "Jonathan Herzig", "authors": "Jonathan Herzig, Pawe{\\l} Krzysztof Nowak, Thomas M\\\"uller, Francesco\n  Piccinno, Julian Martin Eisenschlos", "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering natural language questions over tables is usually seen as a\nsemantic parsing task. To alleviate the collection cost of full logical forms,\none popular approach focuses on weak supervision consisting of denotations\ninstead of logical forms. However, training semantic parsers from weak\nsupervision poses difficulties, and in addition, the generated logical forms\nare only used as an intermediate step prior to retrieving the denotation. In\nthis paper, we present TAPAS, an approach to question answering over tables\nwithout generating logical forms. TAPAS trains from weak supervision, and\npredicts the denotation by selecting table cells and optionally applying a\ncorresponding aggregation operator to such selection. TAPAS extends BERT's\narchitecture to encode tables as input, initializes from an effective joint\npre-training of text segments and tables crawled from Wikipedia, and is trained\nend-to-end. We experiment with three different semantic parsing datasets, and\nfind that TAPAS outperforms or rivals semantic parsing models by improving\nstate-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with\nthe state-of-the-art on WIKISQL and WIKITQ, but with a simpler model\narchitecture. We additionally find that transfer learning, which is trivial in\nour setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 23:18:37 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 15:09:48 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Herzig", "Jonathan", ""], ["Nowak", "Pawe\u0142 Krzysztof", ""], ["M\u00fcller", "Thomas", ""], ["Piccinno", "Francesco", ""], ["Eisenschlos", "Julian Martin", ""]]}, {"id": "2004.02363", "submitter": "Kushal Chawla", "authors": "Kushal Chawla, Gale Lucas, Jonathan May, Jonathan Gratch", "title": "Exploring Early Prediction of Buyer-Seller Negotiation Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents that negotiate with humans find broad applications in pedagogy and\nconversational AI. Most efforts in human-agent negotiations rely on restrictive\nmenu-driven interfaces for communication. To advance the research in\nlanguage-based negotiation systems, we explore a novel task of early prediction\nof buyer-seller negotiation outcomes, by varying the fraction of utterances\nthat the model can access. We explore the feasibility of early prediction by\nusing traditional feature-based methods, as well as by incorporating the\nnon-linguistic task context into a pretrained language model using sentence\ntemplates. We further quantify the extent to which linguistic features help in\nmaking better predictions apart from the task-specific price information.\nFinally, probing the pretrained model helps us to identify specific features,\nsuch as trust and agreement, that contribute to the prediction performance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 00:49:20 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 03:17:36 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Chawla", "Kushal", ""], ["Lucas", "Gale", ""], ["May", "Jonathan", ""], ["Gratch", "Jonathan", ""]]}, {"id": "2004.02393", "submitter": "Yufei Feng", "authors": "Yufei Feng, Mo Yu, Wenhan Xiong, Xiaoxiao Guo, Junjie Huang, Shiyu\n  Chang, Murray Campbell, Michael Greenspan and Xiaodan Zhu", "title": "Learning to Recover Reasoning Chains for Multi-Hop Question Answering\n  via Cooperative Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the new problem of learning to recover reasoning chains from\nweakly supervised signals, i.e., the question-answer pairs. We propose a\ncooperative game approach to deal with this problem, in which how the evidence\npassages are selected and how the selected passages are connected are handled\nby two models that cooperate to select the most confident chains from a large\nset of candidates (from distant supervision). For evaluation, we created\nbenchmarks based on two multi-hop QA datasets, HotpotQA and MedHop; and\nhand-labeled reasoning chains for the latter. The experimental results\ndemonstrate the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 03:54:38 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Feng", "Yufei", ""], ["Yu", "Mo", ""], ["Xiong", "Wenhan", ""], ["Guo", "Xiaoxiao", ""], ["Huang", "Junjie", ""], ["Chang", "Shiyu", ""], ["Campbell", "Murray", ""], ["Greenspan", "Michael", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2004.02399", "submitter": "Tian Lan", "authors": "Tian Lan, Xian-Ling Mao, Wei Wei, Xiaoyan Gao, Heyan Huang", "title": "PONE: A Novel Automatic Evaluation Metric for Open-Domain Generative\n  Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain generative dialogue systems have attracted considerable attention\nover the past few years. Currently, how to automatically evaluate them, is\nstill a big challenge problem. As far as we know, there are three kinds of\nautomatic methods to evaluate the open-domain generative dialogue systems: (1)\nWord-overlap-based metrics; (2) Embedding-based metrics; (3) Learning-based\nmetrics. Due to the lack of systematic comparison, it is not clear which kind\nof metrics are more effective. In this paper, we will first measure\nsystematically all kinds of automatic evaluation metrics over the same\nexperimental setting to check which kind is best. Through extensive\nexperiments, the learning-based metrics are demonstrated that they are the most\neffective evaluation metrics for open-domain generative dialogue systems.\nMoreover, we observe that nearly all learning-based metrics depend on the\nnegative sampling mechanism, which obtains an extremely imbalanced and\nlow-quality dataset to train a score model. In order to address this issue, we\npropose a novel and feasible learning-based metric that can significantly\nimprove the correlation with human judgments by using augmented POsitive\nsamples and valuable NEgative samples, called PONE. Extensive experiments\ndemonstrate that our proposed evaluation method significantly outperforms the\nstate-of-the-art learning-based evaluation methods, with an average correlation\nimprovement of 13.18%. In addition, we have publicly released the codes of our\nproposed method and state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 04:36:33 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Lan", "Tian", ""], ["Mao", "Xian-Ling", ""], ["Wei", "Wei", ""], ["Gao", "Xiaoyan", ""], ["Huang", "Heyan", ""]]}, {"id": "2004.02401", "submitter": "Wei Peng", "authors": "Choon Meng Lee, Jianfeng Liu, Wei Peng", "title": "Applying Cyclical Learning Rate to Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In training deep learning networks, the optimizer and related learning rate\nare often used without much thought or with minimal tuning, even though it is\ncrucial in ensuring a fast convergence to a good quality minimum of the loss\nfunction that can also generalize well on the test dataset. Drawing inspiration\nfrom the successful application of cyclical learning rate policy for computer\nvision related convolutional networks and datasets, we explore how cyclical\nlearning rate can be applied to train transformer-based neural networks for\nneural machine translation. From our carefully designed experiments, we show\nthat the choice of optimizers and the associated cyclical learning rate policy\ncan have a significant impact on the performance. In addition, we establish\nguidelines when applying cyclical learning rates to neural machine translation\ntasks. Thus with our work, we hope to raise awareness of the importance of\nselecting the right optimizers and the accompanying learning rate policy, at\nthe same time, encourage further research into easy-to-use learning rate\npolicies.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 04:45:49 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Lee", "Choon Meng", ""], ["Liu", "Jianfeng", ""], ["Peng", "Wei", ""]]}, {"id": "2004.02421", "submitter": "Deng Cai", "authors": "Zibo Lin, Deng Cai, Yan Wang, Xiaojiang Liu, Hai-Tao Zheng, Shuming\n  Shi", "title": "The World is Not Binary: Learning to Rank with Grayscale Data for\n  Dialogue Response Selection", "comments": "EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response selection plays a vital role in building retrieval-based\nconversation systems. Despite that response selection is naturally a\nlearning-to-rank problem, most prior works take a point-wise view and train\nbinary classifiers for this task: each response candidate is labeled either\nrelevant (one) or irrelevant (zero). On the one hand, this formalization can be\nsub-optimal due to its ignorance of the diversity of response quality. On the\nother hand, annotating grayscale data for learning-to-rank can be prohibitively\nexpensive and challenging. In this work, we show that grayscale data can be\nautomatically constructed without human effort. Our method employs\noff-the-shelf response retrieval models and response generation models as\nautomatic grayscale data generators. With the constructed grayscale data, we\npropose multi-level ranking objectives for training, which can (1) teach a\nmatching model to capture more fine-grained context-response relevance\ndifference and (2) reduce the train-test discrepancy in terms of distractor\nstrength. Our method is simple, effective, and universal. Experiments on three\nbenchmark datasets and four state-of-the-art matching models show that the\nproposed approach brings significant and consistent performance improvements.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 06:34:54 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 02:39:39 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 14:08:23 GMT"}, {"version": "v4", "created": "Tue, 13 Oct 2020 07:08:07 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Lin", "Zibo", ""], ["Cai", "Deng", ""], ["Wang", "Yan", ""], ["Liu", "Xiaojiang", ""], ["Zheng", "Hai-Tao", ""], ["Shi", "Shuming", ""]]}, {"id": "2004.02438", "submitter": "Xuming Hu", "authors": "Xuming Hu, Chenwei Zhang, Yusong Xu, Lijie Wen, Philip S. Yu", "title": "SelfORE: Self-supervised Relational Feature Learning for Open Relation\n  Extraction", "comments": "In EMNLP 2020 as a long paper. Code and data are available at\n  https://github.com/THU-BPM/SelfORE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open relation extraction is the task of extracting open-domain relation facts\nfrom natural language sentences. Existing works either utilize heuristics or\ndistant-supervised annotations to train a supervised classifier over\npre-defined relations, or adopt unsupervised methods with additional\nassumptions that have less discriminative power. In this work, we proposed a\nself-supervised framework named SelfORE, which exploits weak, self-supervised\nsignals by leveraging large pretrained language model for adaptive clustering\non contextualized relational features, and bootstraps the self-supervised\nsignals by improving contextualized features in relation classification.\nExperimental results on three datasets show the effectiveness and robustness of\nSelfORE on open-domain Relation Extraction when comparing with competitive\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 07:23:17 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 12:32:20 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Hu", "Xuming", ""], ["Zhang", "Chenwei", ""], ["Xu", "Yusong", ""], ["Wen", "Lijie", ""], ["Yu", "Philip S.", ""]]}, {"id": "2004.02451", "submitter": "Hiroshi Noji", "authors": "Hiroshi Noji, Hiroya Takamura", "title": "An Analysis of the Utility of Explicit Negative Examples to Improve the\n  Syntactic Abilities of Neural Language Models", "comments": "ACL 2020 camera ready (long paper); code is available at\n  https://github.com/aistairc/lm_syntax_negative", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the utilities of explicit negative examples in training neural\nlanguage models. Negative examples here are incorrect words in a sentence, such\nas \"barks\" in \"*The dogs barks\". Neural language models are commonly trained\nonly on positive examples, a set of sentences in the training data, but recent\nstudies suggest that the models trained in this way are not capable of robustly\nhandling complex syntactic constructions, such as long-distance agreement. In\nthis paper, using English data, we first demonstrate that appropriately using\nnegative examples about particular constructions (e.g., subject-verb agreement)\nwill boost the model's robustness on them, with a negligible loss of\nperplexity. The key to our success is an additional margin loss between the\nlog-likelihoods of a correct word and an incorrect word. We then provide a\ndetailed analysis of the trained models. One of our findings is the difficulty\nof object-relative clauses for RNNs. We find that even with our direct learning\nsignals the models still suffer from resolving agreement across an\nobject-relative clause. Augmentation of training sentences involving the\nconstructions somewhat helps, but the accuracy still does not reach the level\nof subject-relative clauses. Although not directly cognitively appealing, our\nmethod can be a tool to analyze the true architectural limitation of neural\nmodels on challenging linguistic constructions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 07:47:34 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 14:40:47 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 07:41:21 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Noji", "Hiroshi", ""], ["Takamura", "Hiroya", ""]]}, {"id": "2004.02509", "submitter": "Ildiko Pilan", "authors": "Ildik\\'o Pil\\'an and P{\\aa}l H. Brekke and Lilja {\\O}vrelid", "title": "Building a Norwegian Lexical Resource for Medical Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large Norwegian lexical resource of categorized medical terms.\nThe resource merges information from large medical databases, and contains over\n77,000 unique entries, including automatically mapped terms from a Norwegian\nmedical dictionary. We describe the methodology behind this automatic\ndictionary entry mapping based on keywords and suffixes and further present the\nresults of a manual evaluation performed on a subset by a domain expert. The\nevaluation indicated that ca. 80% of the mappings were correct.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 09:24:11 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Pil\u00e1n", "Ildik\u00f3", ""], ["Brekke", "P\u00e5l H.", ""], ["\u00d8vrelid", "Lilja", ""]]}, {"id": "2004.02555", "submitter": "Yinglong Ma", "authors": "Jingpeng Zhao and Yinglong Ma", "title": "Joint Embedding of Words and Category Labels for Hierarchical\n  Multi-label Text Classification", "comments": "The submitted paper (Identifier: arXiv:2004.02555) has a problem of\n  authorship disputes within a collaboration of authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification has become increasingly challenging due to the continuous\nrefinement of classification label granularity and the expansion of\nclassification label scale. To address that, some research has been applied\nonto strategies that exploit the hierarchical structure in problems with a\nlarge number of categories. At present, hierarchical text classification (HTC)\nhas received extensive attention and has broad application prospects. Making\nfull use of the relationship between parent category and child category in text\nclassification task can greatly improve the performance of classification. In\nthis paper, We propose a joint embedding of text and parent category based on\nhierarchical fine-tuning ordered neurons LSTM (HFT-ONLSTM) for HTC. Our method\nmakes full use of the connection between the upper-level and lower-level\nlabels. Experiments show that our model outperforms the state-of-the-art\nhierarchical model at a lower computation cost.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:06:08 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 06:35:41 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 03:20:30 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Zhao", "Jingpeng", ""], ["Ma", "Yinglong", ""]]}, {"id": "2004.02557", "submitter": "Nuo Xu", "authors": "Nuo Xu, Pinghui Wang, Long Chen, Li Pan, Xiaoyan Wang, Junzhou Zhao", "title": "Distinguish Confusing Law Articles for Legal Judgment Prediction", "comments": "This work has been accepted by the 58th Annual Meeting of the\n  Association for Computational Linguistics (ACL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legal Judgment Prediction (LJP) is the task of automatically predicting a law\ncase's judgment results given a text describing its facts, which has excellent\nprospects in judicial assistance systems and convenient services for the\npublic. In practice, confusing charges are frequent, because law cases\napplicable to similar law articles are easily misjudged. For addressing this\nissue, the existing method relies heavily on domain experts, which hinders its\napplication in different law systems. In this paper, we present an end-to-end\nmodel, LADAN, to solve the task of LJP. To distinguish confusing charges, we\npropose a novel graph neural network to automatically learn subtle differences\nbetween confusing law articles and design a novel attention mechanism that\nfully exploits the learned differences to extract compelling discriminative\nfeatures from fact descriptions attentively. Experiments conducted on\nreal-world datasets demonstrate the superiority of our LADAN.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:09:44 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 09:02:11 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 13:20:23 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Xu", "Nuo", ""], ["Wang", "Pinghui", ""], ["Chen", "Long", ""], ["Pan", "Li", ""], ["Wang", "Xiaoyan", ""], ["Zhao", "Junzhou", ""]]}, {"id": "2004.02577", "submitter": "Wei Peng", "authors": "Wei Peng, Chongxuan Huang, Tianhao Li, Yun Chen, and Qun Liu", "title": "Dictionary-based Data Augmentation for Cross-Domain Neural Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing data augmentation approaches for neural machine translation (NMT)\nhave predominantly relied on back-translating in-domain (IND) monolingual\ncorpora. These methods suffer from issues associated with a domain information\ngap, which leads to translation errors for low frequency and out-of-vocabulary\nterminology. This paper proposes a dictionary-based data augmentation (DDA)\nmethod for cross-domain NMT. DDA synthesizes a domain-specific dictionary with\ngeneral domain corpora to automatically generate a large-scale pseudo-IND\nparallel corpus. The generated pseudo-IND data can be used to enhance a general\ndomain trained baseline. The experiments show that the DDA-enhanced NMT models\ndemonstrate consistent significant improvements, outperforming the baseline\nmodels by 3.75-11.53 BLEU. The proposed method is also able to further improve\nthe performance of the back-translation based and IND-finetuned NMT models. The\nimprovement is associated with the enhanced domain coverage produced by DDA.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:50:49 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Peng", "Wei", ""], ["Huang", "Chongxuan", ""], ["Li", "Tianhao", ""], ["Chen", "Yun", ""], ["Liu", "Qun", ""]]}, {"id": "2004.02585", "submitter": "Tom Sherborne", "authors": "Tom Sherborne, Yumo Xu, Mirella Lapata", "title": "Bootstrapping a Crosslingual Semantic Parser", "comments": "Camera Ready for EMNLP2020 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in semantic parsing scarcely considers languages other than\nEnglish but professional translation can be prohibitively expensive. We adapt a\nsemantic parser trained on a single language, such as English, to new languages\nand multiple domains with minimal annotation. We query if machine translation\nis an adequate substitute for training data, and extend this to investigate\nbootstrapping using joint training with English, paraphrasing, and multilingual\npre-trained models. We develop a Transformer-based parser combining paraphrases\nby ensembling attention over multiple encoders and present new versions of ATIS\nand Overnight in German and Chinese for evaluation. Experimental results\nindicate that MT can approximate training data in a new language for accurate\nparsing when augmented with paraphrasing through multiple MT engines.\nConsidering when MT is inadequate, we also find that using our approach\nachieves parsing accuracy within 2% of complete translation using only 50% of\ntraining data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 12:05:02 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 11:04:34 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 10:15:10 GMT"}, {"version": "v4", "created": "Wed, 23 Sep 2020 14:33:47 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Sherborne", "Tom", ""], ["Xu", "Yumo", ""], ["Lapata", "Mirella", ""]]}, {"id": "2004.02592", "submitter": "Qingyu Zhou", "authors": "Qingyu Zhou, Furu Wei, Ming Zhou", "title": "Learning to Summarize Passages: Mining Passage-Summary Pairs from\n  Wikipedia Revision Histories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for automatically constructing a\npassage-to-summary dataset by mining the Wikipedia page revision histories. In\nparticular, the method mines the main body passages and the introduction\nsentences which are added to the pages simultaneously. The constructed dataset\ncontains more than one hundred thousand passage-summary pairs. The quality\nanalysis shows that it is promising that the dataset can be used as a training\nand validation set for passage summarization. We validate and analyze the\nperformance of various summarization systems on the proposed dataset. The\ndataset will be available online at https://res.qyzhou.me.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 12:11:50 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhou", "Qingyu", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "2004.02594", "submitter": "Hengyi Cai", "authors": "Hengyi Cai, Hongshen Chen, Yonghao Song, Cheng Zhang, Xiaofang Zhao,\n  Dawei Yin", "title": "Data Manipulation: Towards Effective Instance Learning for Neural\n  Dialogue Generation via Learning to Augment and Reweight", "comments": "To appear at ACL 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art neural dialogue models learn from human\nconversations following the data-driven paradigm. As such, a reliable training\ncorpus is the crux of building a robust and well-behaved dialogue model.\nHowever, due to the open-ended nature of human conversations, the quality of\nuser-generated training data varies greatly, and effective training samples are\ntypically insufficient while noisy samples frequently appear. This impedes the\nlearning of those data-driven neural dialogue models. Therefore, effective\ndialogue learning requires not only more reliable learning samples, but also\nfewer noisy samples. In this paper, we propose a data manipulation framework to\nproactively reshape the data distribution towards reliable samples by\naugmenting and highlighting effective learning samples as well as reducing the\neffect of inefficient samples simultaneously. In particular, the data\nmanipulation model selectively augments the training samples and assigns an\nimportance weight to each instance to reform the training data. Note that, the\nproposed data manipulation framework is fully data-driven and learnable. It not\nonly manipulates training samples to optimize the dialogue generation model,\nbut also learns to increase its manipulation skills through gradient descent\nwith validation samples. Extensive experiments show that our framework can\nimprove the dialogue generation performance with respect to various automatic\nevaluation metrics and human judgments.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 12:14:09 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 01:30:15 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 05:48:22 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2020 09:51:39 GMT"}, {"version": "v5", "created": "Thu, 11 Jun 2020 14:01:55 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Cai", "Hengyi", ""], ["Chen", "Hongshen", ""], ["Song", "Yonghao", ""], ["Zhang", "Cheng", ""], ["Zhao", "Xiaofang", ""], ["Yin", "Dawei", ""]]}, {"id": "2004.02644", "submitter": "Pedro Henrique Martins", "authors": "Pedro Henrique Martins and Zita Marinho and Andr\\'e F. T. Martins", "title": "Sparse Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art text generators build on powerful language models\nsuch as GPT-2, achieving impressive performance. However, to avoid degenerate\ntext, they require sampling from a modified softmax, via temperature parameters\nor ad-hoc truncation techniques, as in top-$k$ or nucleus sampling. This\ncreates a mismatch between training and testing conditions. In this paper, we\nuse the recently introduced entmax transformation to train and sample from a\nnatively sparse language model, avoiding this mismatch. The result is a text\ngenerator with favorable performance in terms of fluency and consistency, fewer\nrepetitions, and n-gram diversity closer to human text. In order to evaluate\nour model, we propose three new metrics for comparing sparse or truncated\ndistributions: $\\epsilon$-perplexity, sparsemax score, and Jensen-Shannon\ndivergence. Human-evaluated experiments in story completion and dialogue\ngeneration show that entmax sampling leads to more engaging and coherent\nstories and conversations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:09:10 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 11:17:53 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 11:20:54 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Martins", "Pedro Henrique", ""], ["Marinho", "Zita", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "2004.02664", "submitter": "Qingyu Zhou", "authors": "Qingyu Zhou, Furu Wei, Ming Zhou", "title": "At Which Level Should We Extract? An Empirical Analysis on Extractive\n  Document Summarization", "comments": "To appear at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extractive methods have been proven effective in automatic document\nsummarization. Previous works perform this task by identifying informative\ncontents at sentence level. However, it is unclear whether performing\nextraction at sentence level is the best solution. In this work, we show that\nunnecessity and redundancy issues exist when extracting full sentences, and\nextracting sub-sentential units is a promising alternative. Specifically, we\npropose extracting sub-sentential units based on the constituency parsing tree.\nA neural extractive model which leverages the sub-sentential information and\nextracts them is presented. Extensive experiments and analyses show that\nextracting sub-sentential units performs competitively comparing to full\nsentence extraction under the evaluation of both automatic and human\nevaluations. Hopefully, our work could provide some inspiration of the basic\nextraction units in extractive summarization for future research.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:35:10 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 08:35:19 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhou", "Qingyu", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "2004.02705", "submitter": "Shen Li", "authors": "Shen Li, Renfen Hu, Jinshan Wu", "title": "Quantum Inspired Word Representation and Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word meaning has different aspects, while the existing word representation\n\"compresses\" these aspects into a single vector, and it needs further analysis\nto recover the information in different dimensions. Inspired by quantum\nprobability, we represent words as density matrices, which are inherently\ncapable of representing mixed states. The experiment shows that the density\nmatrix representation can effectively capture different aspects of word meaning\nwhile maintaining comparable reliability with the vector representation.\nFurthermore, we propose a novel method to combine the coherent summation and\nincoherent summation in the computation of both vectors and density matrices.\nIt achieves consistent improvement on word analogy task.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:37:39 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 03:00:05 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Li", "Shen", ""], ["Hu", "Renfen", ""], ["Wu", "Jinshan", ""]]}, {"id": "2004.02709", "submitter": "Matt Gardner", "authors": "Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben\n  Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth\n  Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel\n  Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang\n  Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric\n  Wallace, Ally Zhang, Ben Zhou", "title": "Evaluating Models' Local Decision Boundaries via Contrast Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard test sets for supervised learning evaluate in-distribution\ngeneralization. Unfortunately, when a dataset has systematic gaps (e.g.,\nannotation artifacts), these evaluations are misleading: a model can learn\nsimple decision rules that perform well on the test set but do not capture a\ndataset's intended capabilities. We propose a new annotation paradigm for NLP\nthat helps to close systematic gaps in the test data. In particular, after a\ndataset is constructed, we recommend that the dataset authors manually perturb\nthe test instances in small but meaningful ways that (typically) change the\ngold label, creating contrast sets. Contrast sets provide a local view of a\nmodel's decision boundary, which can be used to more accurately evaluate a\nmodel's true linguistic capabilities. We demonstrate the efficacy of contrast\nsets by creating them for 10 diverse NLP datasets (e.g., DROP reading\ncomprehension, UD parsing, IMDb sentiment analysis). Although our contrast sets\nare not explicitly adversarial, model performance is significantly lower on\nthem than on the original test sets---up to 25\\% in some cases. We release our\ncontrast sets as new evaluation benchmarks and encourage future dataset\nconstruction efforts to follow similar annotation processes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:47:18 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 21:26:57 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gardner", "Matt", ""], ["Artzi", "Yoav", ""], ["Basmova", "Victoria", ""], ["Berant", "Jonathan", ""], ["Bogin", "Ben", ""], ["Chen", "Sihao", ""], ["Dasigi", "Pradeep", ""], ["Dua", "Dheeru", ""], ["Elazar", "Yanai", ""], ["Gottumukkala", "Ananth", ""], ["Gupta", "Nitish", ""], ["Hajishirzi", "Hanna", ""], ["Ilharco", "Gabriel", ""], ["Khashabi", "Daniel", ""], ["Lin", "Kevin", ""], ["Liu", "Jiangming", ""], ["Liu", "Nelson F.", ""], ["Mulcaire", "Phoebe", ""], ["Ning", "Qiang", ""], ["Singh", "Sameer", ""], ["Smith", "Noah A.", ""], ["Subramanian", "Sanjay", ""], ["Tsarfaty", "Reut", ""], ["Wallace", "Eric", ""], ["Zhang", "Ally", ""], ["Zhou", "Ben", ""]]}, {"id": "2004.02745", "submitter": "Amr Sharaf", "authors": "Amr Sharaf, Hany Hassan, Hal Daum\\'e III", "title": "Meta-Learning for Few-Shot NMT Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present META-MT, a meta-learning approach to adapt Neural Machine\nTranslation (NMT) systems in a few-shot setting. META-MT provides a new\napproach to make NMT models easily adaptable to many target domains with the\nminimal amount of in-domain data. We frame the adaptation of NMT systems as a\nmeta-learning problem, where we learn to adapt to new unseen domains based on\nsimulated offline meta-training domain adaptation tasks. We evaluate the\nproposed meta-learning strategy on ten domains with general large scale NMT\nsystems. We show that META-MT significantly outperforms classical domain\nadaptation when very few in-domain examples are available. Our experiments\nshows that META-MT can outperform classical fine-tuning by up to 2.5 BLEU\npoints after seeing only 4, 000 translated words (300 parallel sentences).\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 15:38:55 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sharaf", "Amr", ""], ["Hassan", "Hany", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "2004.02814", "submitter": "Jeroen Van Hautte", "authors": "Jeroen Van Hautte, Vincent Schelstraete, Mika\\\"el Wornoo", "title": "Leveraging the Inherent Hierarchy of Vacancy Titles for Automated Job\n  Ontology Expansion", "comments": "Accepted to the Proceedings of the 6th International Workshop on\n  Computational Terminology (COMPUTERM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning plays an ever-bigger part in online recruitment, powering\nintelligent matchmaking and job recommendations across many of the world's\nlargest job platforms. However, the main text is rarely enough to fully\nunderstand a job posting: more often than not, much of the required information\nis condensed into the job title. Several organised efforts have been made to\nmap job titles onto a hand-made knowledge base as to provide this information,\nbut these only cover around 60\\% of online vacancies. We introduce a novel,\npurely data-driven approach towards the detection of new job titles. Our method\nis conceptually simple, extremely efficient and competitive with traditional\nNER-based approaches. Although the standalone application of our method does\nnot outperform a finetuned BERT model, it can be applied as a preprocessing\nstep as well, substantially boosting accuracy across several architectures.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:55:41 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Van Hautte", "Jeroen", ""], ["Schelstraete", "Vincent", ""], ["Wornoo", "Mika\u00ebl", ""]]}, {"id": "2004.02843", "submitter": "Alexander LeClair", "authors": "Alexander LeClair, Sakib Haque, Lingfei Wu, Collin McMillan", "title": "Improved Code Summarization via a Graph Neural Network", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic source code summarization is the task of generating natural\nlanguage descriptions for source code. Automatic code summarization is a\nrapidly expanding research area, especially as the community has taken greater\nadvantage of advances in neural network and AI technologies. In general, source\ncode summarization techniques use the source code as input and outputs a\nnatural language description. Yet a strong consensus is developing that using\nstructural information as input leads to improved performance. The first\napproaches to use structural information flattened the AST into a sequence.\nRecently, more complex approaches based on random AST paths or graph neural\nnetworks have improved on the models using flattened ASTs. However, the\nliterature still does not describe the using a graph neural network together\nwith source code sequence as separate inputs to a model. Therefore, in this\npaper, we present an approach that uses a graph-based neural architecture that\nbetter matches the default structure of the AST to generate these summaries. We\nevaluate our technique using a data set of 2.1 million Java method-comment\npairs and show improvement over four baseline techniques, two from the software\nengineering literature, and two from machine learning literature.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:36:42 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 06:29:30 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["LeClair", "Alexander", ""], ["Haque", "Sakib", ""], ["Wu", "Lingfei", ""], ["McMillan", "Collin", ""]]}, {"id": "2004.02857", "submitter": "Jacob Krantz", "authors": "Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, Stefan Lee", "title": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a language-guided navigation task set in a continuous 3D\nenvironment where agents must execute low-level actions to follow natural\nlanguage navigation directions. By being situated in continuous environments,\nthis setting lifts a number of assumptions implicit in prior work that\nrepresents environments as a sparse graph of panoramas with edges corresponding\nto navigability. Specifically, our setting drops the presumptions of known\nenvironment topologies, short-range oracle navigation, and perfect agent\nlocalization. To contextualize this new task, we develop models that mirror\nmany of the advances made in prior settings as well as single-modality\nbaselines. While some of these techniques transfer, we find significantly lower\nabsolute performance in the continuous setting -- suggesting that performance\nin prior `navigation-graph' settings may be inflated by the strong implicit\nassumptions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:49:12 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 18:06:55 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Krantz", "Jacob", ""], ["Wijmans", "Erik", ""], ["Majumdar", "Arjun", ""], ["Batra", "Dhruv", ""], ["Lee", "Stefan", ""]]}, {"id": "2004.02913", "submitter": "Guokan Shang", "authors": "Guokan Shang (1 and 2), Antoine Jean-Pierre Tixier (1), Michalis\n  Vazirgiannis (1 and 3), Jean-Pierre Lorr\\'e (2) ((1) \\'Ecole Polytechnique,\n  (2) Linagora, (3) AUEB)", "title": "Speaker-change Aware CRF for Dialogue Act Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in Dialogue Act (DA) classification approaches the task as a\nsequence labeling problem, using neural network models coupled with a\nConditional Random Field (CRF) as the last layer. CRF models the conditional\nprobability of the target DA label sequence given the input utterance sequence.\nHowever, the task involves another important input sequence, that of speakers,\nwhich is ignored by previous work. To address this limitation, this paper\nproposes a simple modification of the CRF layer that takes speaker-change into\naccount. Experiments on the SwDA corpus show that our modified CRF layer\noutperforms the original one, with very wide margins for some DA labels.\nFurther, visualizations demonstrate that our CRF layer can learn meaningful,\nsophisticated transition patterns between DA label pairs conditioned on\nspeaker-change in an end-to-end way. Code is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 18:03:06 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 01:58:52 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Shang", "Guokan", "", "1 and 2"], ["Tixier", "Antoine Jean-Pierre", "", "1 and 3"], ["Vazirgiannis", "Michalis", "", "1 and 3"], ["Lorr\u00e9", "Jean-Pierre", ""]]}, {"id": "2004.02929", "submitter": "Elena Alvarez Mellado", "authors": "Elena \\'Alvarez-Mellado", "title": "An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper\n  Headlines", "comments": "Accepted at the 4th Workshop on Computational Approaches to\n  Linguistic Code-Switching, LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of anglicisms (lexical borrowings from English) is relevant\nboth for lexicographic purposes and for NLP downstream tasks. We introduce a\ncorpus of European Spanish newspaper headlines annotated with anglicisms and a\nbaseline model for anglicism extraction. In this paper we present: (1) a corpus\nof 21,570 newspaper headlines written in European Spanish annotated with\nemergent anglicisms and (2) a conditional random field baseline model with\nhandcrafted features for anglicism extraction. We present the newspaper\nheadlines corpus, describe the annotation tagset and guidelines and introduce a\nCRF model that can serve as baseline for the task of detecting anglicisms. The\npresented work is a first step towards the creation of an anglicism extractor\nfor Spanish newswire.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 18:41:43 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["\u00c1lvarez-Mellado", "Elena", ""]]}, {"id": "2004.02973", "submitter": "Omer Ben-Porat", "authors": "Omer Ben-Porat, Sharon Hirsch, Lital Kuchy, Guy Elad, Roi Reichart,\n  Moshe Tennenholtz", "title": "Predicting Strategic Behavior from Free Text", "comments": "Accepted to Journal of Artificial Intelligence Research (JAIR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connection between messaging and action is fundamental both to web\napplications, such as web search and sentiment analysis, and to economics.\nHowever, while prominent online applications exploit messaging in natural\n(human) language in order to predict non-strategic action selection, the\neconomics literature focuses on the connection between structured stylized\nmessaging to strategic decisions in games and multi-agent encounters. This\npaper aims to connect these two strands of research, which we consider highly\ntimely and important due to the vast online textual communication on the web.\nParticularly, we introduce the following question: can free text expressed in\nnatural language serve for the prediction of action selection in an economic\ncontext, modeled as a game?\n  In order to initiate the research on this question, we introduce the study of\nan individual's action prediction in a one-shot game based on free text he/she\nprovides, while being unaware of the game to be played. We approach the problem\nby attributing commonsensical personality attributes via crowd-sourcing to free\ntexts written by individuals, and employing transductive learning to predict\nactions taken by these individuals in one-shot games based on these attributes.\nOur approach allows us to train a single classifier that can make predictions\nwith respect to actions taken in multiple games. In experiments with three\nwell-studied games, our algorithm compares favorably with strong alternative\napproaches. In ablation analysis, we demonstrate the importance of our modeling\nchoices---the representation of the text with the commonsensical personality\nattributes and our classifier---to the predictive power of our model.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 20:05:30 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 08:17:52 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ben-Porat", "Omer", ""], ["Hirsch", "Sharon", ""], ["Kuchy", "Lital", ""], ["Elad", "Guy", ""], ["Reichart", "Roi", ""], ["Tennenholtz", "Moshe", ""]]}, {"id": "2004.02984", "submitter": "Zhiqing Sun", "authors": "Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny\n  Zhou", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Processing (NLP) has recently achieved great success by\nusing huge pre-trained models with hundreds of millions of parameters. However,\nthese models suffer from heavy model sizes and high latency such that they\ncannot be deployed to resource-limited mobile devices. In this paper, we\npropose MobileBERT for compressing and accelerating the popular BERT model.\nLike the original BERT, MobileBERT is task-agnostic, that is, it can be\ngenerically applied to various downstream NLP tasks via simple fine-tuning.\nBasically, MobileBERT is a thin version of BERT_LARGE, while equipped with\nbottleneck structures and a carefully designed balance between self-attentions\nand feed-forward networks. To train MobileBERT, we first train a specially\ndesigned teacher model, an inverted-bottleneck incorporated BERT_LARGE model.\nThen, we conduct knowledge transfer from this teacher to MobileBERT. Empirical\nstudies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE\nwhile achieving competitive results on well-known benchmarks. On the natural\nlanguage inference tasks of GLUE, MobileBERT achieves a GLUEscore o 77.7 (0.6\nlower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD\nv1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of\n90.0/79.2 (1.5/2.1 higher than BERT_BASE).\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 20:20:58 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 23:54:36 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Sun", "Zhiqing", ""], ["Yu", "Hongkun", ""], ["Song", "Xiaodan", ""], ["Liu", "Renjie", ""], ["Yang", "Yiming", ""], ["Zhou", "Denny", ""]]}, {"id": "2004.02986", "submitter": "Xusen Yin", "authors": "Xusen Yin and Jonathan May", "title": "Zero-Shot Learning of Text Adventure Games with Sentence-Level Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms such as Q-learning have shown great promise\nin training models to learn the optimal action to take for a given system\nstate; a goal in applications with an exploratory or adversarial nature such as\ntask-oriented dialogues or games. However, models that do not have direct\naccess to their state are harder to train; when the only state access is via\nthe medium of language, this can be particularly pronounced. We introduce a new\nmodel amenable to deep Q-learning that incorporates a Siamese neural network\narchitecture and a novel refactoring of the Q-value function in order to better\nrepresent system state given its approximation over a language channel. We\nevaluate the model in the context of zero-shot text-based adventure game\nlearning. Extrinsically, our model reaches the baseline's convergence\nperformance point needing only 15% of its iterations, reaches a convergence\nperformance point 15% higher than the baseline's, and is able to play unseen,\nunrelated games with no fine-tuning. We probe our new model's representation\nspace to determine that intrinsically, this is due to the appropriate\nclustering of different linguistic mediation into the same state.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 20:24:33 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Yin", "Xusen", ""], ["May", "Jonathan", ""]]}, {"id": "2004.02990", "submitter": "Guy Tevet", "authors": "Guy Tevet, Jonathan Berant", "title": "Evaluating the Evaluation of Diversity in Natural Language Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite growing interest in natural language generation (NLG) models that\nproduce diverse outputs, there is currently no principled method for evaluating\nthe diversity of an NLG system. In this work, we propose a framework for\nevaluating diversity metrics. The framework measures the correlation between a\nproposed diversity metric and a diversity parameter, a single parameter that\ncontrols some aspect of diversity in generated text. For example, a diversity\nparameter might be a binary variable used to instruct crowdsourcing workers to\ngenerate text with either low or high content diversity. We demonstrate the\nutility of our framework by: (a) establishing best practices for eliciting\ndiversity judgments from humans, (b) showing that humans substantially\noutperform automatic metrics in estimating content diversity, and (c)\ndemonstrating that existing methods for controlling diversity by tuning a\n\"decoding parameter\" mostly affect form but not meaning. Our framework can\nadvance the understanding of different diversity metrics, an essential step on\nthe road towards better NLG systems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 20:44:10 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 10:41:56 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 09:49:19 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tevet", "Guy", ""], ["Berant", "Jonathan", ""]]}, {"id": "2004.02995", "submitter": "Jiangming Liu", "authors": "Jiangming Liu, Matt Gardner, Shay B. Cohen, Mirella Lapata", "title": "Multi-Step Inference for Reasoning Over Paragraphs", "comments": "accepted by EMNLP 2020", "journal-ref": null, "doi": "10.18653/v1/2020.emnlp-main.245", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex reasoning over text requires understanding and chaining together\nfree-form predicates and logical connectives. Prior work has largely tried to\ndo this either symbolically or with black-box transformers. We present a middle\nground between these two extremes: a compositional model reminiscent of neural\nmodule networks that can perform chained logical reasoning. This model first\nfinds relevant sentences in the context and then chains them together using\nneural modules. Our model gives significant performance improvements (up to\n29\\% relative error reduction when comfibined with a reranker) on ROPES, a\nrecently introduced complex reasoning dataset.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 21:12:53 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 04:09:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Jiangming", ""], ["Gardner", "Matt", ""], ["Cohen", "Shay B.", ""], ["Lapata", "Mirella", ""]]}, {"id": "2004.03012", "submitter": "Vered Shwartz", "authors": "Vered Shwartz, Rachel Rudinger, and Oyvind Tafjord", "title": "\"You are grounded!\": Latent Name Artifacts in Pre-trained Language\n  Models", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (LMs) may perpetuate biases originating in their\ntraining corpus to downstream models. We focus on artifacts associated with the\nrepresentation of given names (e.g., Donald), which, depending on the corpus,\nmay be associated with specific entities, as indicated by next token prediction\n(e.g., Trump). While helpful in some contexts, grounding happens also in\nunder-specified or inappropriate contexts. For example, endings generated for\n`Donald is a' substantially differ from those of other names, and often have\nmore-than-average negative sentiment. We demonstrate the potential effect on\ndownstream tasks with reading comprehension probes where name perturbation\nchanges the model answers. As a silver lining, our experiments suggest that\nadditional pre-training on different corpora may mitigate this bias.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 21:48:39 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 18:45:17 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Shwartz", "Vered", ""], ["Rudinger", "Rachel", ""], ["Tafjord", "Oyvind", ""]]}, {"id": "2004.03020", "submitter": "Chen Chen", "authors": "Aaron Traylor, Chen Chen, Behzad Golshan, Xiaolan Wang, Yuliang Li,\n  Yoshihiko Suhara, Jinfeng Li, Cagatay Demiralp and Wang-Chiew Tan", "title": "Enhancing Review Comprehension with Domain-Specific Commonsense", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Review comprehension has played an increasingly important role in improving\nthe quality of online services and products and commonsense knowledge can\nfurther enhance review comprehension. However, existing general-purpose\ncommonsense knowledge bases lack sufficient coverage and precision to\nmeaningfully improve the comprehension of domain-specific reviews. In this\npaper, we introduce xSense, an effective system for review comprehension using\ndomain-specific commonsense knowledge bases (xSense KBs). We show that xSense\nKBs can be constructed inexpensively and present a knowledge distillation\nmethod that enables us to use xSense KBs along with BERT to boost the\nperformance of various review comprehension tasks. We evaluate xSense over\nthree review comprehension tasks: aspect extraction, aspect sentiment\nclassification, and question answering. We find that xSense outperforms the\nstate-of-the-art models for the first two tasks and improves the baseline BERT\nQA model significantly, demonstrating the usefulness of incorporating\ncommonsense into review comprehension pipelines. To facilitate future research\nand applications, we publicly release three domain-specific knowledge bases and\na domain-specific question answering benchmark along with this paper.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 22:11:30 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Traylor", "Aaron", ""], ["Chen", "Chen", ""], ["Golshan", "Behzad", ""], ["Wang", "Xiaolan", ""], ["Li", "Yuliang", ""], ["Suhara", "Yoshihiko", ""], ["Li", "Jinfeng", ""], ["Demiralp", "Cagatay", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2004.03027", "submitter": "Yumo Xu", "authors": "Yumo Xu and Mirella Lapata", "title": "Query Focused Multi-Document Summarization with Distant Supervision", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of better modeling query-cluster interactions to\nfacilitate query focused multi-document summarization (QFS). Due to the lack of\ntraining data, existing work relies heavily on retrieval-style methods for\nestimating the relevance between queries and text segments. In this work, we\nleverage distant supervision from question answering where various resources\nare available to more explicitly capture the relationship between queries and\ndocuments. We propose a coarse-to-fine modeling framework which introduces\nseparate modules for estimating whether segments are relevant to the query,\nlikely to contain an answer, and central. Under this framework, a trained\nevidence estimator further discerns which retrieved segments might answer the\nquery for final selection in the summary. We demonstrate that our framework\noutperforms strong comparison systems on standard QFS benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 22:35:19 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Xu", "Yumo", ""], ["Lapata", "Mirella", ""]]}, {"id": "2004.03032", "submitter": "Daniel Edmiston", "authors": "Daniel Edmiston", "title": "A Systematic Analysis of Morphological Content in BERT Models for\n  Multiple Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes experiments which probe the hidden representations of\nseveral BERT-style models for morphological content. The goal is to examine the\nextent to which discrete linguistic structure, in the form of morphological\nfeatures and feature values, presents itself in the vector representations and\nattention distributions of pre-trained language models for five European\nlanguages. The experiments contained herein show that (i) Transformer\narchitectures largely partition their embedding space into convex sub-regions\nhighly correlated with morphological feature value, (ii) the contextualized\nnature of transformer embeddings allows models to distinguish ambiguous\nmorphological forms in many, but not all cases, and (iii) very specific\nattention head/layer combinations appear to hone in on subject-verb agreement.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 22:50:27 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Edmiston", "Daniel", ""]]}, {"id": "2004.03034", "submitter": "Esin Durmus", "authors": "Esin Durmus, Faisal Ladhak, Claire Cardie", "title": "The Role of Pragmatic and Discourse Context in Determining Argument\n  Impact", "comments": "EMNLP 2019", "journal-ref": null, "doi": "10.18653/v1/D19-1568", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research in the social sciences and psychology has shown that the\npersuasiveness of an argument depends not only the language employed, but also\non attributes of the source/communicator, the audience, and the appropriateness\nand strength of the argument's claims given the pragmatic and discourse context\nof the argument. Among these characteristics of persuasive arguments, prior\nwork in NLP does not explicitly investigate the effect of the pragmatic and\ndiscourse context when determining argument quality. This paper presents a new\ndataset to initiate the study of this aspect of argumentation: it consists of a\ndiverse collection of arguments covering 741 controversial topics and\ncomprising over 47,000 claims. We further propose predictive models that\nincorporate the pragmatic and discourse context of argumentative claims and\nshow that they outperform models that rely only on claim-specific linguistic\nfeatures for predicting the perceived impact of individual claims within a\nparticular line of argument.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 23:00:37 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Durmus", "Esin", ""], ["Ladhak", "Faisal", ""], ["Cardie", "Claire", ""]]}, {"id": "2004.03061", "submitter": "Tiago Pimentel", "authors": "Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina\n  Williams, Ryan Cotterell", "title": "Information-Theoretic Probing for Linguistic Structure", "comments": "Accepted for publication at ACL 2020. This is the camera ready\n  version. Code available in https://github.com/rycolab/info-theoretic-probing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of neural networks on a diverse set of NLP tasks has led\nresearchers to question how much these networks actually ``know'' about natural\nlanguage. Probes are a natural way of assessing this. When probing, a\nresearcher chooses a linguistic task and trains a supervised model to predict\nannotations in that linguistic task from the network's learned representations.\nIf the probe does well, the researcher may conclude that the representations\nencode knowledge related to the task. A commonly held belief is that using\nsimpler models as probes is better; the logic is that simpler models will\nidentify linguistic structure, but not learn the task itself. We propose an\ninformation-theoretic operationalization of probing as estimating mutual\ninformation that contradicts this received wisdom: one should always select the\nhighest performing probe one can, even if it is more complex, since it will\nresult in a tighter estimate, and thus reveal more of the linguistic\ninformation inherent in the representation. The experimental portion of our\npaper focuses on empirically estimating the mutual information between a\nlinguistic property and BERT, comparing these estimates to several baselines.\nWe evaluate on a set of ten typologically diverse languages often\nunderrepresented in NLP research---plus English---totalling eleven languages.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 01:06:36 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 21:58:58 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Pimentel", "Tiago", ""], ["Valvoda", "Josef", ""], ["Maudslay", "Rowan Hall", ""], ["Zmigrod", "Ran", ""], ["Williams", "Adina", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2004.03066", "submitter": "Alex Warstadt", "authors": "Paloma Jeretic, Alex Warstadt, Suvrat Bhooshan, Adina Williams", "title": "Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature\n  and PRESupposition", "comments": "to appear in Proceedings of ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language inference (NLI) is an increasingly important task for\nnatural language understanding, which requires one to infer whether a sentence\nentails another. However, the ability of NLI models to make pragmatic\ninferences remains understudied. We create an IMPlicature and PRESupposition\ndiagnostic dataset (IMPPRES), consisting of >25k semiautomatically generated\nsentence pairs illustrating well-studied pragmatic inference types. We use\nIMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on\nMultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although\nMultiNLI appears to contain very few pairs illustrating these inference types,\nwe find that BERT learns to draw pragmatic inferences. It reliably treats\nscalar implicatures triggered by \"some\" as entailments. For some presupposition\ntriggers like \"only\", BERT reliably recognizes the presupposition as an\nentailment, even when the trigger is embedded under an entailment canceling\noperator like negation. BOW and InferSent show weaker evidence of pragmatic\nreasoning. We conclude that NLI training encourages models to learn some, but\nnot all, pragmatic inferences.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 01:20:55 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 01:17:37 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Jeretic", "Paloma", ""], ["Warstadt", "Alex", ""], ["Bhooshan", "Suvrat", ""], ["Williams", "Adina", ""]]}, {"id": "2004.03070", "submitter": "Duyu Tang", "authors": "Daya Guo, Akari Asai, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou,\n  Daxin Jiang, Jian Yin and Ming Zhou", "title": "Inferential Text Generation with Multiple Knowledge Sources and\n  Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of generating inferential texts of events for a variety\nof commonsense like \\textit{if-else} relations. Existing approaches typically\nuse limited evidence from training examples and learn for each relation\nindividually. In this work, we use multiple knowledge sources as fuels for the\nmodel. Existing commonsense knowledge bases like ConceptNet are dominated by\ntaxonomic knowledge (e.g., \\textit{isA} and \\textit{relatedTo} relations),\nhaving a limited number of inferential knowledge. We use not only structured\ncommonsense knowledge bases, but also natural language snippets from\nsearch-engine results. These sources are incorporated into a generative base\nmodel via key-value memory network. In addition, we introduce a meta-learning\nbased multi-task learning algorithm. For each targeted commonsense relation, we\nregard the learning of examples from other relations as the meta-training\nprocess, and the evaluation on examples from the targeted relation as the\nmeta-test process. We conduct experiments on Event2Mind and ATOMIC datasets.\nResults show that both the integration of multiple knowledge sources and the\nuse of the meta-learning algorithm improve the performance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 01:49:18 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 05:02:26 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Guo", "Daya", ""], ["Asai", "Akari", ""], ["Tang", "Duyu", ""], ["Duan", "Nan", ""], ["Gong", "Ming", ""], ["Shou", "Linjun", ""], ["Jiang", "Daxin", ""], ["Yin", "Jian", ""], ["Zhou", "Ming", ""]]}, {"id": "2004.03090", "submitter": "Bodhisattwa Prasad Majumder", "authors": "Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, Julian McAuley", "title": "Interview: A Large-Scale Open-Source Corpus of Media Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing conversational datasets consist either of written proxies for dialog\nor small-scale transcriptions of natural speech. We introduce 'Interview': a\nlarge-scale (105K conversations) media dialog dataset collected from news\ninterview transcripts. Compared to existing large-scale proxies for\nconversational data, language models trained on our dataset exhibit better\nzero-shot out-of-domain performance on existing spoken dialog datasets,\ndemonstrating its usefulness in modeling real-world conversations. 'Interview'\ncontains speaker role annotations for each turn, facilitating the development\nof engaging, responsive dialog systems. In fact, experiments on two dialog\ntasks show that leveraging such labels improves performance over strong\nspeaker-agnostic baselines, and enabling models to generate more specific and\ninquisitive responses in interview-style conversations.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 02:44:50 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Majumder", "Bodhisattwa Prasad", ""], ["Li", "Shuyang", ""], ["Ni", "Jianmo", ""], ["McAuley", "Julian", ""]]}, {"id": "2004.03093", "submitter": "Allen Schmaltz", "authors": "Allen Schmaltz and Andrew Beam", "title": "Exemplar Auditing for Multi-Label Biomedical Text Classification", "comments": "22 pages, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical applications of AI in medicine consist of semi-supervised\ndiscovery: The investigator aims to identify features of interest at a\nresolution more fine-grained than that of the available human labels. This is\noften the scenario faced in healthcare applications as coarse, high-level\nlabels (e.g., billing codes) are often the only sources that are readily\navailable. These challenges are compounded for modalities such as text, where\nthe feature space is very high-dimensional, and often contains considerable\namounts of noise.\n  In this work, we generalize a recently proposed zero-shot sequence labeling\nmethod, \"binary labeling via a convolutional decomposition\", to the case where\nthe available document-level human labels are themselves relatively\nhigh-dimensional. The approach yields classification with \"introspection\",\nrelating the fine-grained features of an inference-time prediction to their\nnearest neighbors from the training set, under the model. The approach is\neffective, yet parsimonious, as demonstrated on a well-studied MIMIC-III\nmulti-label classification task of electronic health record data, and is useful\nas a tool for organizing the analysis of neural model predictions and\nhigh-dimensional datasets. Our proposed approach yields both a competitively\neffective classification model and an interrogation mechanism to aid healthcare\nworkers in understanding the salient features that drive the model's\npredictions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 02:54:20 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Schmaltz", "Allen", ""], ["Beam", "Andrew", ""]]}, {"id": "2004.03096", "submitter": "Yiming Cui", "authors": "Nan Shao, Yiming Cui, Ting Liu, Shijin Wang, Guoping Hu", "title": "Is Graph Structure Necessary for Multi-hop Question Answering?", "comments": "6 pages, to appear at EMNLP 2020", "journal-ref": null, "doi": "10.18653/v1/2020.emnlp-main.583", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, attempting to model texts as graph structure and introducing graph\nneural networks to deal with it has become a trend in many NLP research areas.\nIn this paper, we investigate whether the graph structure is necessary for\nmulti-hop question answering. Our analysis is centered on HotpotQA. We\nconstruct a strong baseline model to establish that, with the proper use of\npre-trained models, graph structure may not be necessary for multi-hop question\nanswering. We point out that both graph structure and adjacency matrix are\ntask-related prior knowledge, and graph-attention can be considered as a\nspecial case of self-attention. Experiments and visualized analysis demonstrate\nthat graph-attention or the entire graph structure can be replaced by\nself-attention or Transformers.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 02:59:42 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 09:29:19 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Shao", "Nan", ""], ["Cui", "Yiming", ""], ["Liu", "Ting", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "2004.03097", "submitter": "Bowen Wu", "authors": "Bowen Wu, Huan Zhang, Mengyuan Li, Zongsheng Wang, Qihang Feng,\n  Junhong Huang, Baoxun Wang", "title": "Towards Non-task-specific Distillation of BERT via Sentence\n  Representation Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, BERT has become an essential ingredient of various NLP deep models\ndue to its effectiveness and universal-usability. However, the online\ndeployment of BERT is often blocked by its large-scale parameters and high\ncomputational cost. There are plenty of studies showing that the knowledge\ndistillation is efficient in transferring the knowledge from BERT into the\nmodel with a smaller size of parameters. Nevertheless, current BERT\ndistillation approaches mainly focus on task-specified distillation, such\nmethodologies lead to the loss of the general semantic knowledge of BERT for\nuniversal-usability. In this paper, we propose a sentence representation\napproximating oriented distillation framework that can distill the pre-trained\nBERT into a simple LSTM based model without specifying tasks. Consistent with\nBERT, our distilled model is able to perform transfer learning via fine-tuning\nto adapt to any sentence-level downstream task. Besides, our model can further\ncooperate with task-specific distillation procedures. The experimental results\non multiple NLP tasks from the GLUE benchmark show that our approach\noutperforms other task-specific distillation methods or even much larger\nmodels, i.e., ELMO, with efficiency well-improved.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 03:03:00 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Wu", "Bowen", ""], ["Zhang", "Huan", ""], ["Li", "Mengyuan", ""], ["Wang", "Zongsheng", ""], ["Feng", "Qihang", ""], ["Huang", "Junhong", ""], ["Wang", "Baoxun", ""]]}, {"id": "2004.03101", "submitter": "Pratyay Banerjee", "authors": "Pratyay Banerjee and Chitta Baral", "title": "Knowledge Fusion and Semantic Knowledge Ranking for Open Domain Question\n  Answering", "comments": "9 pages. 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Domain Question Answering requires systems to retrieve external\nknowledge and perform multi-hop reasoning by composing knowledge spread over\nmultiple sentences. In the recently introduced open domain question answering\nchallenge datasets, QASC and OpenBookQA, we need to perform retrieval of facts\nand compose facts to correctly answer questions. In our work, we learn a\nsemantic knowledge ranking model to re-rank knowledge retrieved through Lucene\nbased information retrieval systems. We further propose a \"knowledge fusion\nmodel\" which leverages knowledge in BERT-based language models with externally\nretrieved knowledge and improves the knowledge understanding of the BERT-based\nlanguage models. On both OpenBookQA and QASC datasets, the knowledge fusion\nmodel with semantically re-ranked knowledge outperforms previous attempts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 03:16:47 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 06:46:36 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Banerjee", "Pratyay", ""], ["Baral", "Chitta", ""]]}, {"id": "2004.03116", "submitter": "Yiming Cui", "authors": "Yiming Cui, Ting Liu, Ziqing Yang, Zhipeng Chen, Wentao Ma, Wanxiang\n  Che, Shijin Wang, Guoping Hu", "title": "A Sentence Cloze Dataset for Chinese Machine Reading Comprehension", "comments": "7 pages, to appear at COLING 2020", "journal-ref": null, "doi": "10.18653/v1/2020.coling-main.589", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the continuous efforts by the Chinese NLP community, more and more\nChinese machine reading comprehension datasets become available. To add\ndiversity in this area, in this paper, we propose a new task called Sentence\nCloze-style Machine Reading Comprehension (SC-MRC). The proposed task aims to\nfill the right candidate sentence into the passage that has several blanks. We\nbuilt a Chinese dataset called CMRC 2019 to evaluate the difficulty of the\nSC-MRC task. Moreover, to add more difficulties, we also made fake candidates\nthat are similar to the correct ones, which requires the machine to judge their\ncorrectness in the context. The proposed dataset contains over 100K blanks\n(questions) within over 10K passages, which was originated from Chinese\nnarrative stories. To evaluate the dataset, we implement several baseline\nsystems based on the pre-trained models, and the results show that the\nstate-of-the-art model still underperforms human performance by a large margin.\nWe release the dataset and baseline system to further facilitate our community.\nResources available through https://github.com/ymcui/cmrc2019\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 04:09:00 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 06:41:34 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Cui", "Yiming", ""], ["Liu", "Ting", ""], ["Yang", "Ziqing", ""], ["Chen", "Zhipeng", ""], ["Ma", "Wentao", ""], ["Che", "Wanxiang", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "2004.03125", "submitter": "DongHyun Choi", "authors": "DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin", "title": "RYANSQL: Recursively Applying Sketch-based Slot Fillings for Complex\n  Text-to-SQL in Cross-Domain Databases", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-SQL is the problem of converting a user question into an SQL query,\nwhen the question and database are given. In this paper, we present a neural\nnetwork approach called RYANSQL (Recursively Yielding Annotation Network for\nSQL) to solve complex Text-to-SQL tasks for cross-domain databases. State-ment\nPosition Code (SPC) is defined to trans-form a nested SQL query into a set of\nnon-nested SELECT statements; a sketch-based slot filling approach is proposed\nto synthesize each SELECT statement for its corresponding SPC. Additionally,\ntwo input manipulation methods are presented to improve generation performance\nfurther. RYANSQL achieved 58.2% accuracy on the challenging Spider benchmark,\nwhich is a 3.2%p improvement over previous state-of-the-art approaches. At the\ntime of writing, RYANSQL achieves the first position on the Spider leaderboard.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 04:51:04 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Choi", "DongHyun", ""], ["Shin", "Myeong Cheol", ""], ["Kim", "EungGyun", ""], ["Shin", "Dong Ryeol", ""]]}, {"id": "2004.03133", "submitter": "Seungjae Shin", "authors": "Seungjae Shin, Kyungwoo Song, JoonHo Jang, Hyemi Kim, Weonyoung Joo,\n  Il-Chul Moon", "title": "Neutralizing Gender Bias in Word Embedding with Latent Disentanglement\n  and Counterfactual Generation", "comments": "Findings of EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research demonstrates that word embeddings, trained on the\nhuman-generated corpus, have strong gender biases in embedding spaces, and\nthese biases can result in the discriminative results from the various\ndownstream tasks. Whereas the previous methods project word embeddings into a\nlinear subspace for debiasing, we introduce a \\textit{Latent Disentanglement}\nmethod with a siamese auto-encoder structure with an adapted gradient reversal\nlayer. Our structure enables the separation of the semantic latent information\nand gender latent information of given word into the disjoint latent\ndimensions. Afterwards, we introduce a \\textit{Counterfactual Generation} to\nconvert the gender information of words, so the original and the modified\nembeddings can produce a gender-neutralized word embedding after geometric\nalignment regularization, without loss of semantic information. From the\nvarious quantitative and qualitative debiasing experiments, our method shows to\nbe better than existing debiasing methods in debiasing word embeddings. In\naddition, Our method shows the ability to preserve semantic information during\ndebiasing by minimizing the semantic information losses for extrinsic NLP\ndownstream tasks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 05:16:48 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 05:06:41 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Shin", "Seungjae", ""], ["Song", "Kyungwoo", ""], ["Jang", "JoonHo", ""], ["Kim", "Hyemi", ""], ["Joo", "Weonyoung", ""], ["Moon", "Il-Chul", ""]]}, {"id": "2004.03136", "submitter": "Seanie Lee", "authors": "Kyubyong Park, Seanie Lee", "title": "g2pM: A Neural Grapheme-to-Phoneme Conversion Package for Mandarin\n  Chinese Based on a New Open Benchmark Dataset", "comments": "INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversion of Chinese graphemes to phonemes (G2P) is an essential component\nin Mandarin Chinese Text-To-Speech (TTS) systems. One of the biggest challenges\nin Chinese G2P conversion is how to disambiguate the pronunciation of\npolyphones - characters having multiple pronunciations. Although many academic\nefforts have been made to address it, there has been no open dataset that can\nserve as a standard benchmark for fair comparison to date. In addition, most of\nthe reported systems are hard to employ for researchers or practitioners who\nwant to convert Chinese text into pinyin at their convenience. Motivated by\nthese, in this work, we introduce a new benchmark dataset that consists of\n99,000+ sentences for Chinese polyphone disambiguation. We train a simple\nneural network model on it, and find that it outperforms other preexisting G2P\nsystems. Finally, we package our project and share it on PyPi.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 05:44:58 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 14:48:27 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 11:13:03 GMT"}, {"version": "v4", "created": "Thu, 30 Jul 2020 02:54:26 GMT"}, {"version": "v5", "created": "Thu, 17 Sep 2020 10:06:25 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Park", "Kyubyong", ""], ["Lee", "Seanie", ""]]}, {"id": "2004.03137", "submitter": "Mingxuan Wang", "authors": "Mingxuan Wang, Hongxiao Bai, Hai Zhao, Lei Li", "title": "Cross-lingual Supervision Improves Unsupervised Neural Machine\n  Translation", "comments": "NAACL Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation~(NMT) is ineffective for zero-resource languages.\nRecent works exploring the possibility of unsupervised neural machine\ntranslation (UNMT) with only monolingual data can achieve promising results.\nHowever, there are still big gaps between UNMT and NMT with parallel\nsupervision. In this work, we introduce a multilingual unsupervised NMT\n(\\method) framework to leverage weakly supervised signals from high-resource\nlanguage pairs to zero-resource translation directions. More specifically, for\nunsupervised language pairs \\texttt{En-De}, we can make full use of the\ninformation from parallel dataset \\texttt{En-Fr} to jointly train the\nunsupervised translation directions all in one model. \\method is based on\nmultilingual models which require no changes to the standard unsupervised NMT.\nEmpirical results demonstrate that \\method significantly improves the\ntranslation quality by more than 3 BLEU score on six benchmark unsupervised\ntranslation directions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 05:46:49 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 03:44:16 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 03:52:50 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Mingxuan", ""], ["Bai", "Hongxiao", ""], ["Zhao", "Hai", ""], ["Li", "Lei", ""]]}, {"id": "2004.03151", "submitter": "Dana Ruiter", "authors": "Dana Ruiter, Josef van Genabith and Cristina Espa\\~na-Bonet", "title": "Self-Induced Curriculum Learning in Self-Supervised Neural Machine\n  Translation", "comments": "12 pages, 5 images, to be published at EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised neural machine translation (SSNMT) jointly learns to identify\nand select suitable training data from comparable (rather than parallel)\ncorpora and to translate, in a way that the two tasks support each other in a\nvirtuous circle. In this study, we provide an in-depth analysis of the sampling\nchoices the SSNMT model makes during training. We show how, without it having\nbeen told to do so, the model self-selects samples of increasing (i) complexity\nand (ii) task-relevance in combination with (iii) performing a denoising\ncurriculum. We observe that the dynamics of the mutual-supervision signals of\nboth system internal representation types are vital for the extraction and\ntranslation performance. We show that in terms of the Gunning-Fog Readability\nindex, SSNMT starts extracting and learning from Wikipedia data suitable for\nhigh school students and quickly moves towards content suitable for first year\nundergraduate students.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 06:45:45 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 14:13:45 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ruiter", "Dana", ""], ["van Genabith", "Josef", ""], ["Espa\u00f1a-Bonet", "Cristina", ""]]}, {"id": "2004.03176", "submitter": "Jan Niehues", "authors": "Jan Niehues", "title": "Machine Translation with Unsupervised Length-Constraints", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have seen significant improvements in machine translation due to the usage\nof deep learning. While the improvements in translation quality are impressive,\nthe encoder-decoder architecture enables many more possibilities. In this\npaper, we explore one of these, the generation of constraint translation. We\nfocus on length constraints, which are essential if the translation should be\ndisplayed in a given format. In this work, we propose an end-to-end approach\nfor this task. Compared to a traditional method that first translates and then\nperforms sentence compression, the text compression is learned completely\nunsupervised. By combining the idea with zero-shot multilingual machine\ntranslation, we are also able to perform unsupervised monolingual sentence\ncompression. In order to fulfill the length constraints, we investigated\nseveral methods to integrate the constraints into the model. Using the\npresented technique, we are able to significantly improve the translation\nquality under constraints. Furthermore, we are able to perform unsupervised\nmonolingual sentence compression.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 07:55:41 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Niehues", "Jan", ""]]}, {"id": "2004.03180", "submitter": "Mamoru Komachi", "authors": "Aizhan Imankulova, Masahiro Kaneko, Tosho Hirasawa and Mamoru Komachi", "title": "Towards Multimodal Simultaneous Neural Machine Translation", "comments": "10 pages; WMT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simultaneous translation involves translating a sentence before the speaker's\nutterance is completed in order to realize real-time understanding in multiple\nlanguages. This task is significantly more challenging than the general full\nsentence translation because of the shortage of input information during\ndecoding. To alleviate this shortage, we propose multimodal simultaneous neural\nmachine translation (MSNMT), which leverages visual information as an\nadditional modality. Our experiments with the Multi30k dataset showed that\nMSNMT significantly outperforms its text-only counterpart in more timely\ntranslation situations with low latency. Furthermore, we verified the\nimportance of visual information during decoding by performing an adversarial\nevaluation of MSNMT, where we studied how models behaved with incongruent input\nmodality and analyzed the effect of different word order between source and\ntarget languages.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:02:21 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 04:38:38 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Imankulova", "Aizhan", ""], ["Kaneko", "Masahiro", ""], ["Hirasawa", "Tosho", ""], ["Komachi", "Mamoru", ""]]}, {"id": "2004.03181", "submitter": "Leo Bouscarrat", "authors": "L\\'eo Bouscarrat (QARMA, TALEP), Antoine Bonnefoy, C\\'ecile Capponi\n  (LIF, QARMA), Carlos Ramisch (TALEP)", "title": "Multilingual enrichment of disease biomedical ontologies", "comments": null, "journal-ref": "2nd workshop on MultilingualBIO: Multilingual Biomedical Text\n  Processing, May 2020, Marseille, France", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating biomedical ontologies is an important challenge, but doing it\nmanually requires much time and money. We study the possibility to use\nopen-source knowledge bases to translate biomedical ontologies. We focus on two\naspects: coverage and quality. We look at the coverage of two biomedical\nontologies focusing on diseases with respect to Wikidata for 9 European\nlanguages (Czech, Dutch, English, French, German, Italian, Polish, Portuguese\nand Spanish) for both ontologies, plus Arabic, Chinese and Russian for the\nsecond one. We first use direct links between Wikidata and the studied\nontologies and then use second-order links by going through other intermediate\nontologies. We then compare the quality of the translations obtained thanks to\nWikidata with a commercial machine translation tool, here Google Cloud\nTranslation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:04:21 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bouscarrat", "L\u00e9o", "", "QARMA, TALEP"], ["Bonnefoy", "Antoine", "", "LIF, QARMA"], ["Capponi", "C\u00e9cile", "", "LIF, QARMA"], ["Ramisch", "Carlos", "", "TALEP"]]}, {"id": "2004.03186", "submitter": "Han Xu", "authors": "Xu Han, Tianyu Gao, Yankai Lin, Hao Peng, Yaoliang Yang, Chaojun Xiao,\n  Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou", "title": "More Data, More Relations, More Context and More Openness: A Review and\n  Outlook for Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational facts are an important component of human knowledge, which are\nhidden in vast amounts of text. In order to extract these facts from text,\npeople have been working on relation extraction (RE) for years. From early\npattern matching to current neural networks, existing RE methods have achieved\nsignificant progress. Yet with explosion of Web text and emergence of new\nrelations, human knowledge is increasing drastically, and we thus require\n\"more\" from RE: a more powerful RE system that can robustly utilize more data,\nefficiently learn more relations, easily handle more complicated context, and\nflexibly generalize to more open domains. In this paper, we look back at\nexisting RE methods, analyze key challenges we are facing nowadays, and show\npromising directions towards more powerful RE. We hope our view can advance\nthis field and inspire more efforts in the community.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:15:21 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 11:56:52 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 09:15:29 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Han", "Xu", ""], ["Gao", "Tianyu", ""], ["Lin", "Yankai", ""], ["Peng", "Hao", ""], ["Yang", "Yaoliang", ""], ["Xiao", "Chaojun", ""], ["Liu", "Zhiyuan", ""], ["Li", "Peng", ""], ["Sun", "Maosong", ""], ["Zhou", "Jie", ""]]}, {"id": "2004.03194", "submitter": "Youngmoon Jung", "authors": "Youngmoon Jung, Seong Min Kye, Yeunju Choi, Myunghun Jung, Hoirin Kim", "title": "Improving Multi-Scale Aggregation Using Feature Pyramid Module for\n  Robust Speaker Verification of Variable-Duration Utterances", "comments": "Accepted to Interspeech 2020", "journal-ref": "Proc. Interspeech 2020, pp. 1501-1505", "doi": "10.21437/Interspeech.2020-1025", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the most widely used approach for speaker verification is the deep\nspeaker embedding learning. In this approach, we obtain a speaker embedding\nvector by pooling single-scale features that are extracted from the last layer\nof a speaker feature extractor. Multi-scale aggregation (MSA), which utilizes\nmulti-scale features from different layers of the feature extractor, has\nrecently been introduced and shows superior performance for variable-duration\nutterances. To increase the robustness dealing with utterances of arbitrary\nduration, this paper improves the MSA by using a feature pyramid module. The\nmodule enhances speaker-discriminative information of features from multiple\nlayers via a top-down pathway and lateral connections. We extract speaker\nembeddings using the enhanced features that contain rich speaker information\nwith different time scales. Experiments on the VoxCeleb dataset show that the\nproposed module improves previous MSA methods with a smaller number of\nparameters. It also achieves better performance than state-of-the-art\napproaches for both short and long utterances.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:35:05 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 04:28:40 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 09:39:43 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 06:09:29 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Jung", "Youngmoon", ""], ["Kye", "Seong Min", ""], ["Choi", "Yeunju", ""], ["Jung", "Myunghun", ""], ["Kim", "Hoirin", ""]]}, {"id": "2004.03212", "submitter": "Lisai Zhang", "authors": "Lisai Zhang, Qingcai Chen, Baotian Hu, and Shuoran Jiang", "title": "Text-Guided Neural Image Inpainting", "comments": "ACM MM'2020 (Oral). 9 pages, 4 tables, 7 figures", "journal-ref": null, "doi": "10.1145/3394171.3414017", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting task requires filling the corrupted image with contents\ncoherent with the context. This research field has achieved promising progress\nby using neural image inpainting methods. Nevertheless, there is still a\ncritical challenge in guessing the missed content with only the context pixels.\nThe goal of this paper is to fill the semantic information in corrupted images\naccording to the provided descriptive text. Unique from existing text-guided\nimage generation works, the inpainting models are required to compare the\nsemantic content of the given text and the remaining part of the image, then\nfind out the semantic content that should be filled for missing part. To\nfulfill such a task, we propose a novel inpainting model named Text-Guided Dual\nAttention Inpainting Network (TDANet). Firstly, a dual multimodal attention\nmechanism is designed to extract the explicit semantic information about the\ncorrupted regions, which is done by comparing the descriptive text and\ncomplementary image areas through reciprocal attention. Secondly, an image-text\nmatching loss is applied to maximize the semantic similarity of the generated\nimage and the text. Experiments are conducted on two open datasets. Results\nshow that the proposed TDANet model reaches new state-of-the-art on both\nquantitative and qualitative measures. Result analysis suggests that the\ngenerated images are consistent with the guidance text, enabling the generation\nof various results by providing different descriptions. Codes are available at\nhttps://github.com/idealwhite/TDANet\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 09:04:43 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 03:40:11 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 12:23:53 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 08:12:02 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Lisai", ""], ["Chen", "Qingcai", ""], ["Hu", "Baotian", ""], ["Jiang", "Shuoran", ""]]}, {"id": "2004.03227", "submitter": "Jind\\v{r}ich Helcl", "authors": "Zden\\v{e}k Kasner, Jind\\v{r}ich Libovick\\'y, Jind\\v{r}ich Helcl", "title": "Improving Fluency of Non-Autoregressive Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-autoregressive (nAR) models for machine translation (MT) manifest\nsuperior decoding speed when compared to autoregressive (AR) models, at the\nexpense of impaired fluency of their outputs. We improve the fluency of a nAR\nmodel with connectionist temporal classification (CTC) by employing additional\nfeatures in the scoring model used during beam search decoding. Since the beam\nsearch decoding in our model only requires to run the network in a single\nforward pass, the decoding speed is still notably higher than in standard AR\nmodels. We train models for three language pairs: German, Czech, and Romanian\nfrom and into English. The results show that our proposed models can be more\nefficient in terms of decoding speed and still achieve a competitive BLEU score\nrelative to AR models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 09:40:54 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Kasner", "Zden\u011bk", ""], ["Libovick\u00fd", "Jind\u0159ich", ""], ["Helcl", "Jind\u0159ich", ""]]}, {"id": "2004.03238", "submitter": "Kazutoshi Shinoda", "authors": "Kazutoshi Shinoda, Saku Sugawara, Akiko Aizawa", "title": "Improving the Robustness of QA Models to Challenge Sets with Variational\n  Question-Answer Pair Generation", "comments": "ACL-IJCNLP 2021 SRW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) models for reading comprehension have achieved\nhuman-level accuracy on in-distribution test sets. However, they have been\ndemonstrated to lack robustness to challenge sets, whose distribution is\ndifferent from that of training sets. Existing data augmentation methods\nmitigate this problem by simply augmenting training sets with synthetic\nexamples sampled from the same distribution as the challenge sets. However,\nthese methods assume that the distribution of a challenge set is known a\npriori, making them less applicable to unseen challenge sets. In this study, we\nfocus on question-answer pair generation (QAG) to mitigate this problem. While\nmost existing QAG methods aim to improve the quality of synthetic examples, we\nconjecture that diversity-promoting QAG can mitigate the sparsity of training\nsets and lead to better robustness. We present a variational QAG model that\ngenerates multiple diverse QA pairs from a paragraph. Our experiments show that\nour method can improve the accuracy of 12 challenge sets, as well as the\nin-distribution accuracy. Our code and data are available at\nhttps://github.com/KazutoshiShinoda/VQAG.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 10:15:19 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 01:21:14 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Shinoda", "Kazutoshi", ""], ["Sugawara", "Saku", ""], ["Aizawa", "Akiko", ""]]}, {"id": "2004.03283", "submitter": "Leonhard Hennig", "authors": "Martin Schiersch, Veselina Mironova, Maximilian Schmitt, Philippe\n  Thomas, Aleksandra Gabryszak, Leonhard Hennig", "title": "A German Corpus for Fine-Grained Named Entity Recognition and Relation\n  Extraction of Traffic and Industry Events", "comments": "Published in LREC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring mobility- and industry-relevant events is important in areas such\nas personal travel planning and supply chain management, but extracting events\npertaining to specific companies, transit routes and locations from\nheterogeneous, high-volume text streams remains a significant challenge. This\nwork describes a corpus of German-language documents which has been annotated\nwith fine-grained geo-entities, such as streets, stops and routes, as well as\nstandard named entity types. It has also been annotated with a set of 15\ntraffic- and industry-related n-ary relations and events, such as accidents,\ntraffic jams, acquisitions, and strikes. The corpus consists of newswire texts,\nTwitter messages, and traffic reports from radio stations, police and railway\ncompanies. It allows for training and evaluating both named entity recognition\nalgorithms that aim for fine-grained typing of geo-entities, as well as n-ary\nrelation extraction systems.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:39:50 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Schiersch", "Martin", ""], ["Mironova", "Veselina", ""], ["Schmitt", "Maximilian", ""], ["Thomas", "Philippe", ""], ["Gabryszak", "Aleksandra", ""], ["Hennig", "Leonhard", ""]]}, {"id": "2004.03287", "submitter": "Leonhard Hennig", "authors": "Saskia Sch\\\"on, Veselina Mironova, Aleksandra Gabryszak, Leonhard\n  Hennig", "title": "A Corpus Study and Annotation Schema for Named Entity Recognition and\n  Relation Extraction of Business Products", "comments": "Published in LREC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing non-standard entity types and relations, such as B2B products,\nproduct classes and their producers, in news and forum texts is important in\napplication areas such as supply chain monitoring and market research. However,\nthere is a decided lack of annotated corpora and annotation guidelines in this\ndomain. In this work, we present a corpus study, an annotation schema and\nassociated guidelines, for the annotation of product entity and company-product\nrelation mentions. We find that although product mentions are often realized as\nnoun phrases, defining their exact extent is difficult due to high boundary\nambiguity and the broad syntactic and semantic variety of their surface\nrealizations. We also describe our ongoing annotation effort, and present a\npreliminary corpus of English web and social media documents annotated\naccording to the proposed guidelines.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:45:22 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Sch\u00f6n", "Saskia", ""], ["Mironova", "Veselina", ""], ["Gabryszak", "Aleksandra", ""], ["Hennig", "Leonhard", ""]]}, {"id": "2004.03289", "submitter": "Yo Joong Choe", "authors": "Jiyeon Ham, Yo Joong Choe, Kyubyong Park, Ilji Choi, Hyungjoon Soh", "title": "KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language\n  Understanding", "comments": "Findings of EMNLP 2020. Datasets available at\n  https://github.com/kakaobrain/KorNLUDatasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Natural language inference (NLI) and semantic textual similarity (STS) are\nkey tasks in natural language understanding (NLU). Although several benchmark\ndatasets for those tasks have been released in English and a few other\nlanguages, there are no publicly available NLI or STS datasets in the Korean\nlanguage. Motivated by this, we construct and release new datasets for Korean\nNLI and STS, dubbed KorNLI and KorSTS, respectively. Following previous\napproaches, we machine-translate existing English training sets and manually\ntranslate development and test sets into Korean. To accelerate research on\nKorean NLU, we also establish baselines on KorNLI and KorSTS. Our datasets are\npublicly available at https://github.com/kakaobrain/KorNLUDatasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:49:15 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 04:15:35 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 09:28:51 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ham", "Jiyeon", ""], ["Choe", "Yo Joong", ""], ["Park", "Kyubyong", ""], ["Choi", "Ilji", ""], ["Soh", "Hyungjoon", ""]]}, {"id": "2004.03324", "submitter": "Goran Glava\\v{s}", "authors": "Leon Sch\\\"uller and Florian Wilhelm and Nico Kreiling and Goran\n  Glava\\v{s}", "title": "Windowing Models for Abstractive Summarization of Long Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural summarization models suffer from the fixed-size input limitation: if\ntext length surpasses the model's maximal number of input tokens, some document\ncontent (possibly summary-relevant) gets truncated Independently summarizing\nwindows of maximal input size disallows for information flow between windows\nand leads to incoherent summaries. We propose windowing models for neural\nabstractive summarization of (arbitrarily) long texts. We extend the\nsequence-to-sequence model augmented with pointer generator network by (1)\nallowing the encoder to slide over different windows of the input document and\n(2) sharing the decoder and retaining its state across different input windows.\nWe explore two windowing variants: Static Windowing precomputes the number of\ntokens the decoder should generate from each window (based on training corpus\nstatistics); in Dynamic Windowing the decoder learns to emit a token that\nsignals encoder's shift to the next input window. Empirical results render our\nmodels effective in their intended use-case: summarizing long texts with\nrelevant content not bound to the very document beginning.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:01:26 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Sch\u00fcller", "Leon", ""], ["Wilhelm", "Florian", ""], ["Kreiling", "Nico", ""], ["Glava\u0161", "Goran", ""]]}, {"id": "2004.03329", "submitter": "Pengtao Xie", "authors": "Xuehai He, Shu Chen, Zeqian Ju, Xiangyu Dong, Hongchao Fang, Sicheng\n  Wang, Yue Yang, Jiaqi Zeng, Ruisi Zhang, Ruoyu Zhang, Meng Zhou, Penghui Zhu,\n  Pengtao Xie", "title": "MedDialog: Two Large-scale Medical Dialogue Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical dialogue systems are promising in assisting in telemedicine to\nincrease access to healthcare services, improve the quality of patient care,\nand reduce medical costs. To facilitate the research and development of medical\ndialogue systems, we build two large-scale medical dialogue datasets:\nMedDialog-EN and MedDialog-CN. MedDialog-EN is an English dataset containing\n0.3 million conversations between patients and doctors and 0.5 million\nutterances. MedDialog-CN is an Chinese dataset containing 1.1 million\nconversations and 4 million utterances. To our best knowledge,\nMedDialog-(EN,CN) are the largest medical dialogue datasets to date. The\ndataset is available at https://github.com/UCSD-AI4H/Medical-Dialogue-System\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:07:09 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 22:15:10 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["He", "Xuehai", ""], ["Chen", "Shu", ""], ["Ju", "Zeqian", ""], ["Dong", "Xiangyu", ""], ["Fang", "Hongchao", ""], ["Wang", "Sicheng", ""], ["Yang", "Yue", ""], ["Zeng", "Jiaqi", ""], ["Zhang", "Ruisi", ""], ["Zhang", "Ruoyu", ""], ["Zhou", "Meng", ""], ["Zhu", "Penghui", ""], ["Xie", "Pengtao", ""]]}, {"id": "2004.03340", "submitter": "Germ\\'an Kruszewski", "authors": "Germ\\'an Kruszewski, Ionut-Teodor Sorodoc, Tomas Mikolov", "title": "Evaluating Online Continual Learning with CALM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Continual Learning (OCL) studies learning over a continuous data\nstream without observing any single example more than once, a setting that is\ncloser to the experience of humans and systems that must learn \"on-the-wild\".\nYet, commonly available benchmarks are far from these real-world conditions,\nbecause they explicitly signal different tasks, lack latent similarity\nstructure or assume temporal independence between different examples. Here, we\npropose a new benchmark for OCL based on language modelling in which input\nalternates between different languages and domains without any explicit\ndelimitation. Additionally, we propose new metrics to study catastrophic\nforgetting in this setting and evaluate multiple baseline models based on\ncompositions of experts. Finally, we introduce a simple gating technique that\nlearns the latent similarities between different inputs, improving the\nperformance of a Products of Experts model.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:17:05 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 12:20:27 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kruszewski", "Germ\u00e1n", ""], ["Sorodoc", "Ionut-Teodor", ""], ["Mikolov", "Tomas", ""]]}, {"id": "2004.03354", "submitter": "Nina Poerner", "authors": "Nina Poerner, Ulli Waltinger and Hinrich Sch\\\"utze", "title": "Inexpensive Domain Adaptation of Pretrained Language Models: Case\n  Studies on Biomedical NER and Covid-19 QA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved\nby unsupervised pretraining on target-domain text. While successful, this\napproach is expensive in terms of hardware, runtime and CO_2 emissions. Here,\nwe propose a cheaper alternative: We train Word2Vec on target-domain text and\nalign the resulting word vectors with the wordpiece vectors of a general-domain\nPTLM. We evaluate on eight biomedical Named Entity Recognition (NER) tasks and\ncompare against the recently proposed BioBERT model. We cover over 60% of the\nBioBERT-BERT F1 delta, at 5% of BioBERT's CO_2 footprint and 2% of its cloud\ncompute cost. We also show how to quickly adapt an existing general-domain\nQuestion Answering (QA) model to an emerging domain: the Covid-19 pandemic.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:31:06 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 18:52:15 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 20:23:10 GMT"}, {"version": "v4", "created": "Sat, 27 Jun 2020 14:27:19 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Poerner", "Nina", ""], ["Waltinger", "Ulli", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2004.03386", "submitter": "Su Zhu", "authors": "Su Zhu, Jieyu Li, Lu Chen, and Kai Yu", "title": "Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue\n  State Tracking", "comments": "16 pages, 4 figures, 11 tables. Accepted to EMNLP 2020 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue state tracking (DST) aims at estimating the current dialogue state\ngiven all the preceding conversation. For multi-domain DST, the data sparsity\nproblem is a major obstacle due to increased numbers of state candidates and\ndialogue lengths. To encode the dialogue context efficiently, we utilize the\nprevious dialogue state (predicted) and the current dialogue utterance as the\ninput for DST. To consider relations among different domain-slots, the schema\ngraph involving prior knowledge is exploited. In this paper, a novel context\nand schema fusion network is proposed to encode the dialogue context and schema\ngraph by using internal and external attention mechanisms. Experiment results\nshow that our approach can obtain new state-of-the-art performance of the\nopen-vocabulary DST on both MultiWOZ 2.0 and MultiWOZ 2.1 benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:46:39 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 13:08:17 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 10:31:51 GMT"}, {"version": "v4", "created": "Wed, 7 Oct 2020 11:19:57 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhu", "Su", ""], ["Li", "Jieyu", ""], ["Chen", "Lu", ""], ["Yu", "Kai", ""]]}, {"id": "2004.03420", "submitter": "Eugene Kharitonov", "authors": "Eugene Kharitonov and Marco Baroni", "title": "Emergent Language Generalization and Acquisition Speed are not tied to\n  Compositionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies of discrete languages emerging when neural agents communicate to\nsolve a joint task often look for evidence of compositional structure. This\nstems for the expectation that such a structure would allow languages to be\nacquired faster by the agents and enable them to generalize better. We argue\nthat these beneficial properties are only loosely connected to\ncompositionality. In two experiments, we demonstrate that, depending on the\ntask, non-compositional languages might show equal, or better, generalization\nperformance and acquisition speed than compositional ones. Further research in\nthe area should be clearer about what benefits are expected from\ncompositionality, and how the latter would lead to them.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:10:27 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 14:46:24 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kharitonov", "Eugene", ""], ["Baroni", "Marco", ""]]}, {"id": "2004.03422", "submitter": "Frederike Zufall", "authors": "Frederike Zufall, Huangpan Zhang, Katharina Kloppenborg, Torsten Zesch", "title": "Operationalizing the legal concept of 'Incitement to Hatred' as an NLP\n  task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech detection or offensive language detection are well-established\nbut controversial NLP tasks. With 'hate speech' not being a legal term, these\ntasks often elaborate on the question of which statements are perceived as\nbeing 'appropriate' or 'offensive'. Looking beyond this cursory understanding,\nthis article proposes a way to operationalize the legal concept of incitement\nto hatred as an NLP task. We pursue this task based on the criminal offense of\nincitement to hatred in {\\S} 130 of the German Criminal Code along with the\nunderlying EU Framework. Under the German Network Enforcement Act, social media\nproviders are subject to a direct obligation to delete postings violating this\noffense. We take this as a use case to study the transition from the\nill-defined concepts of hate speech or offensive language which are usually\nused in NLP to an operationalization of an actual legally binding obligation.\nWe first translate the legal assessment into a series of binary decisions and\nthen collect, annotate, and analyze a dataset according to our annotation\nscheme. Finally, we translate each of the legal decisions into an NLP task\nbased on the annotated data. The two subtasks of target group detection and\ntargeting act detection can be considered crucial from a legal viewpoint. We\nshow that both can be annotated by non-legally trained persons with sufficient\nreliability.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:13:38 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 10:28:19 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zufall", "Frederike", ""], ["Zhang", "Huangpan", ""], ["Kloppenborg", "Katharina", ""], ["Zesch", "Torsten", ""]]}, {"id": "2004.03437", "submitter": "Yi Zheng", "authors": "Yi Zheng, Xianjie Yang, Xuyong Dang", "title": "Homophone-based Label Smoothing in End-to-End Automatic Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new label smoothing method that makes use of prior knowledge of a language\nat human level, homophone, is proposed in this paper for automatic speech\nrecognition (ASR). Compared with its forerunners, the proposed method uses\npronunciation knowledge of homophones in a more complex way. End-to-end ASR\nmodels that learn acoustic model and language model jointly and modelling units\nof characters are necessary conditions for this method. Experiments with hybrid\nCTC sequence-to-sequence model show that the new method can reduce character\nerror rate (CER) by 0.4% absolutely.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:37:30 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 07:13:13 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Zheng", "Yi", ""], ["Yang", "Xianjie", ""], ["Dang", "Xuyong", ""]]}, {"id": "2004.03461", "submitter": "Mantas Luko\\v{s}evi\\v{c}ius", "authors": "Lukas Stankevi\\v{c}ius and Mantas Luko\\v{s}evi\\v{c}ius", "title": "Testing pre-trained Transformer models for Lithuanian news clustering", "comments": "Submission accepted at https://ivus.ktu.edu/", "journal-ref": "Proceedings of the Information Society and University Studies\n  2020, pp. 46-53, vol. 2698, CEUR, Kaunas, 2020, ISSN: 1613-0073", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent introduction of Transformer deep learning architecture made\nbreakthroughs in various natural language processing tasks. However,\nnon-English languages could not leverage such new opportunities with the\nEnglish text pre-trained models. This changed with research focusing on\nmultilingual models, where less-spoken languages are the main beneficiaries. We\ncompare pre-trained multilingual BERT, XLM-R, and older learned text\nrepresentation methods as encodings for the task of Lithuanian news clustering.\nOur results indicate that publicly available pre-trained multilingual\nTransformer models can be fine-tuned to surpass word vectors but still score\nmuch lower than specially trained doc2vec embeddings.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 14:41:54 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Stankevi\u010dius", "Lukas", ""], ["Luko\u0161evi\u010dius", "Mantas", ""]]}, {"id": "2004.03484", "submitter": "Soham Parikh", "authors": "Soham Parikh, Quaizar Vohra, Mitul Tiwari", "title": "Automated Utterance Generation", "comments": "AAAI/IAAI-20, Emerging Application Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational AI assistants are becoming popular and question-answering is\nan important part of any conversational assistant. Using relevant utterances as\nfeatures in question-answering has shown to improve both the precision and\nrecall for retrieving the right answer by a conversational assistant. Hence,\nutterance generation has become an important problem with the goal of\ngenerating relevant utterances (sentences or phrases) from a knowledge base\narticle that consists of a title and a description. However, generating good\nutterances usually requires a lot of manual effort, creating the need for an\nautomated utterance generation. In this paper, we propose an utterance\ngeneration system which 1) uses extractive summarization to extract important\nsentences from the description, 2) uses multiple paraphrasing techniques to\ngenerate a diverse set of paraphrases of the title and summary sentences, and\n3) selects good candidate paraphrases with the help of a novel candidate\nselection algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:35:54 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 01:27:09 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Parikh", "Soham", ""], ["Vohra", "Quaizar", ""], ["Tiwari", "Mitul", ""]]}, {"id": "2004.03485", "submitter": "Kareem Darwish", "authors": "Younes Samih and Kareem Darwish", "title": "A Few Topical Tweets are Enough for Effective User-Level Stance\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stance detection entails ascertaining the position of a user towards a\ntarget, such as an entity, topic, or claim. Recent work that employs\nunsupervised classification has shown that performing stance detection on vocal\nTwitter users, who have many tweets on a target, can yield very high accuracy\n(+98%). However, such methods perform poorly or fail completely for less vocal\nusers, who may have authored only a few tweets about a target. In this paper,\nwe tackle stance detection for such users using two approaches. In the first\napproach, we improve user-level stance detection by representing tweets using\ncontextualized embeddings, which capture latent meanings of words in context.\nWe show that this approach outperforms two strong baselines and achieves 89.6%\naccuracy and 91.3% macro F-measure on eight controversial topics. In the second\napproach, we expand the tweets of a given user using their Twitter timeline\ntweets, and then we perform unsupervised classification of the user, which\nentails clustering a user with other users in the training set. This approach\nachieves 95.6% accuracy and 93.1% macro F-measure.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:35:55 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Samih", "Younes", ""], ["Darwish", "Kareem", ""]]}, {"id": "2004.03490", "submitter": "Priyanka Sen", "authors": "Priyanka Sen and Amir Saffari", "title": "What do Models Learn from Question Answering Datasets?", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While models have reached superhuman performance on popular question\nanswering (QA) datasets such as SQuAD, they have yet to outperform humans on\nthe task of question answering itself. In this paper, we investigate if models\nare learning reading comprehension from QA datasets by evaluating BERT-based\nmodels across five datasets. We evaluate models on their generalizability to\nout-of-domain examples, responses to missing or incorrect data, and ability to\nhandle question variations. We find that no single dataset is robust to all of\nour experiments and identify shortcomings in both datasets and evaluation\nmethods. Following our analysis, we make recommendations for building future QA\ndatasets that better evaluate the task of question answering through reading\ncomprehension. We also release code to convert QA datasets to a shared format\nfor easier experimentation at\nhttps://github.com/amazon-research/qa-dataset-converter.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:41:55 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 13:02:44 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Sen", "Priyanka", ""], ["Saffari", "Amir", ""]]}, {"id": "2004.03554", "submitter": "Muhammad Asif Ali", "authors": "Muhammad Asif Ali, Yifang Sun, Bing Li, Wei Wang", "title": "Fine-Grained Named Entity Typing over Distantly Supervised Data Based on\n  Refined Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-Grained Named Entity Typing (FG-NET) is a key component in Natural\nLanguage Processing (NLP). It aims at classifying an entity mention into a wide\nrange of entity types. Due to a large number of entity types, distant\nsupervision is used to collect training data for this task, which noisily\nassigns type labels to entity mentions irrespective of the context. In order to\nalleviate the noisy labels, existing approaches on FGNET analyze the entity\nmentions entirely independent of each other and assign type labels solely based\non mention sentence-specific context. This is inadequate for highly overlapping\nand noisy type labels as it hinders information passing across sentence\nboundaries. For this, we propose an edge-weighted attentive graph convolution\nnetwork that refines the noisy mention representations by attending over\ncorpus-level contextual clues prior to the end classification. Experimental\nevaluation shows that the proposed model outperforms the existing research by a\nrelative score of upto 10.2% and 8.3% for macro f1 and micro f1 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:26:36 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Ali", "Muhammad Asif", ""], ["Sun", "Yifang", ""], ["Li", "Bing", ""], ["Wang", "Wei", ""]]}, {"id": "2004.03555", "submitter": "Oshin Agarwal", "authors": "Oshin Agarwal, Daniel M. Bikel", "title": "Entity Linking via Dual and Cross-Attention Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Linking has two main open areas of research: 1) generate candidate\nentities without using alias tables and 2) generate more contextual\nrepresentations for both mentions and entities. Recently, a solution has been\nproposed for the former as a dual-encoder entity retrieval system (Gillick et\nal., 2019) that learns mention and entity representations in the same space,\nand performs linking by selecting the nearest entity to the mention in this\nspace. In this work, we use this retrieval system solely for generating\ncandidate entities. We then rerank the entities by using a cross-attention\nencoder over the target mention and each of the candidate entities. Whereas a\ndual encoder approach forces all information to be contained in the small,\nfixed set of vector dimensions used to represent mentions and entities, a\ncrossattention model allows for the use of detailed information (read:\nfeatures) from the entirety of each <mention, context, candidate entity> tuple.\nWe experiment with features used in the reranker including different ways of\nincorporating document-level context. We achieve state-of-the-art results on\nTACKBP-2010 dataset, with 92.05% accuracy. Furthermore, we show how the\nrescoring model generalizes well when trained on the larger CoNLL-2003 dataset\nand evaluated on TACKBP-2010.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:28:28 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Agarwal", "Oshin", ""], ["Bikel", "Daniel M.", ""]]}, {"id": "2004.03561", "submitter": "Changmao Li", "authors": "Changmao Li, Jinho D. Choi", "title": "Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for\n  Span-based Question Answering", "comments": "Accepted by the Annual Conference of the Association for\n  Computational Linguistics, ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to transformers that learns hierarchical\nrepresentations in multiparty dialogue. First, three language modeling tasks\nare used to pre-train the transformers, token- and utterance-level language\nmodeling and utterance order prediction, that learn both token and utterance\nembeddings for better understanding in dialogue contexts. Then, multi-task\nlearning between the utterance prediction and the token span prediction is\napplied to fine-tune for span-based question answering (QA). Our approach is\nevaluated on the FriendsQA dataset and shows improvements of 3.8% and 1.4% over\nthe two state-of-the-art transformer models, BERT and RoBERTa, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:36:33 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 04:35:45 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Li", "Changmao", ""], ["Choi", "Jinho D.", ""]]}, {"id": "2004.03588", "submitter": "Jia-Chen Gu", "authors": "Jia-Chen Gu, Tianda Li, Quan Liu, Zhen-Hua Ling, Zhiming Su, Si Wei,\n  Xiaodan Zhu", "title": "Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based\n  Chatbots", "comments": "Accepted by CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of employing pre-trained language models\nfor multi-turn response selection in retrieval-based chatbots. A new model,\nnamed Speaker-Aware BERT (SA-BERT), is proposed in order to make the model\naware of the speaker change information, which is an important and intrinsic\nproperty of multi-turn dialogues. Furthermore, a speaker-aware disentanglement\nstrategy is proposed to tackle the entangled dialogues. This strategy selects a\nsmall number of most important utterances as the filtered context according to\nthe speakers' information in them. Finally, domain adaptation is performed to\nincorporate the in-domain knowledge into pre-trained language models.\nExperiments on five public datasets show that our proposed model outperforms\nthe present models on all metrics by large margins and achieves new\nstate-of-the-art performances for multi-turn response selection.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 02:08:04 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 01:27:11 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Gu", "Jia-Chen", ""], ["Li", "Tianda", ""], ["Liu", "Quan", ""], ["Ling", "Zhen-Hua", ""], ["Su", "Zhiming", ""], ["Wei", "Si", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2004.03589", "submitter": "Piji Li", "authors": "Piji Li, Lidong Bing, Zhongyu Wei, Wai Lam", "title": "Salience Estimation with Multi-Attention Learning for Abstractive Text\n  Summarization", "comments": "11 pages, @CUHK. arXiv admin note: text overlap with\n  arXiv:1803.11070, arXiv:1708.00625", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanism plays a dominant role in the sequence generation models\nand has been used to improve the performance of machine translation and\nabstractive text summarization. Different from neural machine translation, in\nthe task of text summarization, salience estimation for words, phrases or\nsentences is a critical component, since the output summary is a distillation\nof the input text. Although the typical attention mechanism can conduct text\nfragment selection from the input text conditioned on the decoder states, there\nis still a gap to conduct direct and effective salience detection. To bring\nback direct salience estimation for summarization with neural networks, we\npropose a Multi-Attention Learning framework which contains two new attention\nlearning components for salience estimation: supervised attention learning and\nunsupervised attention learning. We regard the attention weights as the\nsalience information, which means that the semantic units with large attention\nvalue will be more important. The context information obtained based on the\nestimated salience is incorporated with the typical attention mechanism in the\ndecoder to conduct summary generation. Extensive experiments on some benchmark\ndatasets in different languages demonstrate the effectiveness of the proposed\nframework for the task of abstractive summarization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 02:38:56 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Li", "Piji", ""], ["Bing", "Lidong", ""], ["Wei", "Zhongyu", ""], ["Lam", "Wai", ""]]}, {"id": "2004.03607", "submitter": "Rowan Zellers", "authors": "Rowan Zellers, Ari Holtzman, Elizabeth Clark, Lianhui Qin, Ali\n  Farhadi, Yejin Choi", "title": "TuringAdvice: A Generative and Dynamic Evaluation of Language Use", "comments": "NAACL 2021 camera ready. Project page at\n  https://rowanzellers.com/advice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose TuringAdvice, a new challenge task and dataset for language\nunderstanding models. Given a written situation that a real person is currently\nfacing, a model must generate helpful advice in natural language. Our\nevaluation framework tests a fundamental aspect of human language\nunderstanding: our ability to use language to resolve open-ended situations by\ncommunicating with each other.\n  Empirical results show that today's models struggle at TuringAdvice, even\nmultibillion parameter models finetuned on 600k in-domain training examples.\nThe best model, a finetuned T5, writes advice that is at least as helpful as\nhuman-written advice in only 14% of cases; a much larger non-finetunable GPT3\nmodel does even worse at 4%. This low performance reveals language\nunderstanding errors that are hard to spot outside of a generative setting,\nshowing much room for progress.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 18:00:03 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 01:05:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zellers", "Rowan", ""], ["Holtzman", "Ari", ""], ["Clark", "Elizabeth", ""], ["Qin", "Lianhui", ""], ["Farhadi", "Ali", ""], ["Choi", "Yejin", ""]]}, {"id": "2004.03636", "submitter": "Jun Chen", "authors": "Jun Chen, Robert Hoehndorf, Mohamed Elhoseiny and Xiangliang Zhang", "title": "Efficient long-distance relation extraction with DG-SpanBERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In natural language processing, relation extraction seeks to rationally\nunderstand unstructured text. Here, we propose a novel SpanBERT-based graph\nconvolutional network (DG-SpanBERT) that extracts semantic features from a raw\nsentence using the pre-trained language model SpanBERT and a graph\nconvolutional network to pool latent features. Our DG-SpanBERT model inherits\nthe advantage of SpanBERT on learning rich lexical features from large-scale\ncorpus. It also has the ability to capture long-range relations between\nentities due to the usage of GCN on dependency tree. The experimental results\nshow that our model outperforms other existing dependency-based and\nsequence-based models and achieves a state-of-the-art performance on the TACRED\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 18:21:47 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Chen", "Jun", ""], ["Hoehndorf", "Robert", ""], ["Elhoseiny", "Mohamed", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2004.03643", "submitter": "Colin Cherry", "authors": "Naveen Arivazhagan, Colin Cherry, Wolfgang Macherey and George Foster", "title": "Re-translation versus Streaming for Simultaneous Translation", "comments": "IWSLT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been great progress in improving streaming machine translation, a\nsimultaneous paradigm where the system appends to a growing hypothesis as more\nsource content becomes available. We study a related problem in which revisions\nto the hypothesis beyond strictly appending words are permitted. This is\nsuitable for applications such as live captioning an audio feed. In this\nsetting, we compare custom streaming approaches to re-translation, a\nstraightforward strategy where each new source token triggers a distinct\ntranslation from scratch. We find re-translation to be as good or better than\nstate-of-the-art streaming systems, even when operating under constraints that\nallow very few revisions. We attribute much of this success to a previously\nproposed data-augmentation technique that adds prefix-pairs to the training\ndata, which alongside wait-k inference forms a strong baseline for streaming\ntranslation. We also highlight re-translation's ability to wrap arbitrarily\npowerful MT systems with an experiment showing large improvements from an\nupgrade to its base model.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 18:27:32 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 17:06:21 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 23:36:13 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Arivazhagan", "Naveen", ""], ["Cherry", "Colin", ""], ["Macherey", "Wolfgang", ""], ["Foster", "George", ""]]}, {"id": "2004.03658", "submitter": "Haitian Sun", "authors": "Haitian Sun, Andrew O. Arnold, Tania Bedrax-Weiss, Fernando Pereira,\n  William W. Cohen", "title": "Faithful Embeddings for Knowledge Base Queries", "comments": "Published at 34th Conference on Neural Information Processing Systems\n  (NeurIPS 2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deductive closure of an ideal knowledge base (KB) contains exactly the\nlogical queries that the KB can answer. However, in practice KBs are both\nincomplete and over-specified, failing to answer some queries that have\nreal-world answers. \\emph{Query embedding} (QE) techniques have been recently\nproposed where KB entities and KB queries are represented jointly in an\nembedding space, supporting relaxation and generalization in KB inference.\nHowever, experiments in this paper show that QE systems may disagree with\ndeductive reasoning on answers that do not require generalization or\nrelaxation. We address this problem with a novel QE method that is more\nfaithful to deductive reasoning, and show that this leads to better performance\non complex queries to incomplete KBs. Finally we show that inserting this new\nQE module into a neural question-answering system leads to substantial\nimprovements over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 19:25:16 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 21:19:36 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 03:46:25 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Sun", "Haitian", ""], ["Arnold", "Andrew O.", ""], ["Bedrax-Weiss", "Tania", ""], ["Pereira", "Fernando", ""], ["Cohen", "William W.", ""]]}, {"id": "2004.03659", "submitter": "Elena Tutubalina Dr.", "authors": "Elena Tutubalina, Ilseyar Alimova, Zulfat Miftahutdinov, Andrey\n  Sakhovskiy, Valentin Malykh and Sergey Nikolenko", "title": "The Russian Drug Reaction Corpus and Neural Models for Drug Reactions\n  and Effectiveness Detection in User Reviews", "comments": "9 pages, 9 tables, 4 figures", "journal-ref": "Bioinformatics, 2020", "doi": "10.1093/bioinformatics/btaa675", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Russian Drug Reaction Corpus (RuDReC) is a new partially annotated corpus\nof consumer reviews in Russian about pharmaceutical products for the detection\nof health-related named entities and the effectiveness of pharmaceutical\nproducts. The corpus itself consists of two parts, the raw one and the labelled\none. The raw part includes 1.4 million health-related user-generated texts\ncollected from various Internet sources, including social media. The labelled\npart contains 500 consumer reviews about drug therapy with drug- and\ndisease-related information. Labels for sentences include health-related issues\nor their absence. The sentences with one are additionally labelled at the\nexpression level for identification of fine-grained subtypes such as drug\nclasses and drug forms, drug indications, and drug reactions. Further, we\npresent a baseline model for named entity recognition (NER) and multi-label\nsentence classification tasks on this corpus. The macro F1 score of 74.85% in\nthe NER task was achieved by our RuDR-BERT model. For the sentence\nclassification task, our model achieves the macro F1 score of 68.82% gaining\n7.47% over the score of BERT model trained on Russian data. We make the RuDReC\ncorpus and pretrained weights of domain-specific BERT models freely available\nat https://github.com/cimm-kzn/RuDReC\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 19:26:13 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Tutubalina", "Elena", ""], ["Alimova", "Ilseyar", ""], ["Miftahutdinov", "Zulfat", ""], ["Sakhovskiy", "Andrey", ""], ["Malykh", "Valentin", ""], ["Nikolenko", "Sergey", ""]]}, {"id": "2004.03661", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang and Marcel Worring", "title": "Query-controllable Video Summarization", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When video collections become huge, how to explore both within and across\nvideos efficiently is challenging. Video summarization is one of the ways to\ntackle this issue. Traditional summarization approaches limit the effectiveness\nof video exploration because they only generate one fixed video summary for a\ngiven input video independent of the information need of the user. In this\nwork, we introduce a method which takes a text-based query as input and\ngenerates a video summary corresponding to it. We do so by modeling video\nsummarization as a supervised learning problem and propose an end-to-end deep\nlearning based method for query-controllable video summarization to generate a\nquery-dependent video summary. Our proposed method consists of a video summary\ncontroller, video summary generator, and video summary output module. To foster\nthe research of query-controllable video summarization and conduct our\nexperiments, we introduce a dataset that contains frame-based relevance score\nlabels. Based on our experimental result, it shows that the text-based query\nhelps control the video summary. It also shows the text-based query improves\nour model performance. Our code and dataset:\nhttps://github.com/Jhhuangkay/Query-controllable-Video-Summarization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 19:35:04 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Worring", "Marcel", ""]]}, {"id": "2004.03672", "submitter": "Zi-Yi Dou", "authors": "Zi-Yi Dou, Antonios Anastasopoulos, Graham Neubig", "title": "Dynamic Data Selection and Weighting for Iterative Back-Translation", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Back-translation has proven to be an effective method to utilize monolingual\ndata in neural machine translation (NMT), and iteratively conducting\nback-translation can further improve the model performance. Selecting which\nmonolingual data to back-translate is crucial, as we require that the resulting\nsynthetic data are of high quality and reflect the target domain. To achieve\nthese two goals, data selection and weighting strategies have been proposed,\nwith a common practice being to select samples close to the target domain but\nalso dissimilar to the average general-domain text. In this paper, we provide\ninsights into this commonly used approach and generalize it to a dynamic\ncurriculum learning strategy, which is applied to iterative back-translation\nmodels. In addition, we propose weighting strategies based on both the current\nquality of the sentence and its improvement over the previous iteration. We\nevaluate our models on domain adaptation, low-resource, and high-resource MT\nsettings and on two language pairs. Experimental results demonstrate that our\nmethods achieve improvements of up to 1.8 BLEU points over competitive\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 19:49:58 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 22:00:22 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Dou", "Zi-Yi", ""], ["Anastasopoulos", "Antonios", ""], ["Neubig", "Graham", ""]]}, {"id": "2004.03685", "submitter": "Alon Jacovi", "authors": "Alon Jacovi, Yoav Goldberg", "title": "Towards Faithfully Interpretable NLP Systems: How should we define and\n  evaluate faithfulness?", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing popularity of deep-learning based NLP models, comes a need\nfor interpretable systems. But what is interpretability, and what constitutes a\nhigh-quality interpretation? In this opinion piece we reflect on the current\nstate of interpretability evaluation research. We call for more clearly\ndifferentiating between different desired criteria an interpretation should\nsatisfy, and focus on the faithfulness criteria. We survey the literature with\nrespect to faithfulness evaluation, and arrange the current approaches around\nthree assumptions, providing an explicit form to how faithfulness is \"defined\"\nby the community. We provide concrete guidelines on how evaluation of\ninterpretation methods should and should not be conducted. Finally, we claim\nthat the current binary definition for faithfulness sets a potentially\nunrealistic bar for being considered faithful. We call for discarding the\nbinary notion of faithfulness in favor of a more graded one, which we believe\nwill be of greater practical utility.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:15:28 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 01:18:49 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 20:44:37 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Jacovi", "Alon", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2004.03705", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam\n  Chenaghlu, Jianfeng Gao", "title": "Deep Learning Based Text Classification: A Comprehensive Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based models have surpassed classical machine learning based\napproaches in various text classification tasks, including sentiment analysis,\nnews categorization, question answering, and natural language inference. In\nthis paper, we provide a comprehensive review of more than 150 deep learning\nbased models for text classification developed in recent years, and discuss\ntheir technical contributions, similarities, and strengths. We also provide a\nsummary of more than 40 popular datasets widely used for text classification.\nFinally, we provide a quantitative analysis of the performance of different\ndeep learning models on popular benchmarks, and discuss future research\ndirections.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 02:00:30 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 20:53:58 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 07:41:46 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Minaee", "Shervin", ""], ["Kalchbrenner", "Nal", ""], ["Cambria", "Erik", ""], ["Nikzad", "Narjes", ""], ["Chenaghlu", "Meysam", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2004.03720", "submitter": "Kaj Bostrom", "authors": "Kaj Bostrom and Greg Durrett", "title": "Byte Pair Encoding is Suboptimal for Language Model Pretraining", "comments": "5 pages, 3 figures. To be published in Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of pretrained transformer language models (LMs) in natural\nlanguage processing has led to a wide range of pretraining setups. In\nparticular, these models employ a variety of subword tokenization methods, most\nnotably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the\nWordPiece method (Schuster and Nakajima, 2012), and unigram language modeling\n(Kudo, 2018), to segment text. However, to the best of our knowledge, the\nliterature does not contain a direct evaluation of the impact of tokenization\non language model pretraining. We analyze differences between BPE and unigram\nLM tokenization, finding that the latter method recovers subword units that\nalign more closely with morphology and avoids problems stemming from BPE's\ngreedy construction procedure. We then compare the fine-tuned task performance\nof identical transformer masked language models pretrained with these\ntokenizations. Across downstream tasks and two languages (English and\nJapanese), we find that the unigram LM tokenization method matches or\noutperforms BPE. We hope that developers of future pretrained LMs will consider\nadopting the unigram LM method over the more prevalent BPE.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 21:21:06 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 17:35:44 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Bostrom", "Kaj", ""], ["Durrett", "Greg", ""]]}, {"id": "2004.03734", "submitter": "Ashwinkumar Ganesan", "authors": "Ashwinkumar Ganesan, Francis Ferraro, Tim Oates", "title": "Locality Preserving Loss: Neighbors that Live together, Align together", "comments": null, "journal-ref": "Adapt-NLP 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a locality preserving loss (LPL) that improves the alignment\nbetween vector space embeddings while separating uncorrelated representations.\nGiven two pretrained embedding manifolds, LPL optimizes a model to project an\nembedding and maintain its local neighborhood while aligning one manifold to\nanother. This reduces the overall size of the dataset required to align the two\nin tasks such as cross-lingual word alignment. We show that the LPL-based\nalignment between input vector spaces acts as a regularizer, leading to better\nand consistent accuracy than the baseline, especially when the size of the\ntraining set is small. We demonstrate the effectiveness of LPL optimized\nalignment on semantic text similarity (STS), natural language inference (SNLI),\nmulti-genre language inference (MNLI) and cross-lingual word alignment(CLA)\nshowing consistent improvements, finding up to 16% improvement over our\nbaseline in lower resource settings.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 22:26:09 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 04:56:20 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ganesan", "Ashwinkumar", ""], ["Ferraro", "Francis", ""], ["Oates", "Tim", ""]]}, {"id": "2004.03742", "submitter": "Boxin Wang", "authors": "Boxin Wang, Boyuan Pan, Xin Li, Bo Li", "title": "Towards Evaluating the Robustness of Chinese BERT Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in large-scale language representation models such as BERT\nhave improved the state-of-the-art performances in many NLP tasks. Meanwhile,\ncharacter-level Chinese NLP models, including BERT for Chinese, have also\ndemonstrated that they can outperform the existing models. In this paper, we\nshow that, however, such BERT-based models are vulnerable under character-level\nadversarial attacks. We propose a novel Chinese char-level attack method\nagainst BERT-based classifiers. Essentially, we generate \"small\" perturbation\non the character level in the embedding space and guide the character\nsubstitution procedure. Extensive experiments show that the classification\naccuracy on a Chinese news dataset drops from 91.8% to 0% by manipulating less\nthan 2 characters on average based on the proposed attack. Human evaluations\nalso confirm that our generated Chinese adversarial examples barely affect\nhuman performance on these NLP tasks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 23:02:37 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wang", "Boxin", ""], ["Pan", "Boyuan", ""], ["Li", "Xin", ""], ["Li", "Bo", ""]]}, {"id": "2004.03744", "submitter": "Virginie Do", "authors": "Virginie Do, Oana-Maria Camburu, Zeynep Akata and Thomas Lukasiewicz", "title": "e-SNLI-VE-2.0: Corrected Visual-Textual Entailment with Natural Language\n  Explanations", "comments": null, "journal-ref": "IEEE CVPR Workshop on Fair, Data Efficient and Trusted Computer\n  Vision, 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed SNLI-VE corpus for recognising visual-textual\nentailment is a large, real-world dataset for fine-grained multimodal\nreasoning. However, the automatic way in which SNLI-VE has been assembled (via\ncombining parts of two related datasets) gives rise to a large number of errors\nin the labels of this corpus. In this paper, we first present a data collection\neffort to correct the class with the highest error rate in SNLI-VE. Secondly,\nwe re-evaluate an existing model on the corrected corpus, which we call\nSNLI-VE-2.0, and provide a quantitative comparison with its performance on the\nnon-corrected corpus. Thirdly, we introduce e-SNLI-VE-2.0, which appends\nhuman-written natural language explanations to SNLI-VE-2.0. Finally, we train\nmodels that learn from these explanations at training time, and output such\nexplanations at testing time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 23:12:51 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 07:26:19 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Do", "Virginie", ""], ["Camburu", "Oana-Maria", ""], ["Akata", "Zeynep", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2004.03755", "submitter": "Goonmeet Bajaj", "authors": "Goonmeet Bajaj, Bortik Bandyopadhyay, Daniel Schmidt, Pranav\n  Maneriker, Christopher Myers, Srinivasan Parthasarathy", "title": "Understanding Knowledge Gaps in Visual Question Answering: Implications\n  for Gap Identification and Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) systems are tasked with answering natural\nlanguage questions corresponding to a presented image. Traditional VQA datasets\ntypically contain questions related to the spatial information of objects,\nobject attributes, or general scene questions. Recently, researchers have\nrecognized the need to improve the balance of such datasets to reduce the\nsystem's dependency on memorized linguistic features and statistical biases,\nwhile aiming for enhanced visual understanding. However, it is unclear whether\nany latent patterns exist to quantify and explain these failures. As an initial\nstep towards better quantifying our understanding of the performance of VQA\nmodels, we use a taxonomy of Knowledge Gaps (KGs) to tag questions with one or\nmore types of KGs. Each Knowledge Gap (KG) describes the reasoning abilities\nneeded to arrive at a resolution. After identifying KGs for each question, we\nexamine the skew in the distribution of questions for each KG. We then\nintroduce a targeted question generation model to reduce this skew, which\nallows us to generate new types of questions for an image. These new questions\ncan be added to existing VQA datasets to increase the diversity of questions\nand reduce the skew.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 00:27:43 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 21:53:59 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Bajaj", "Goonmeet", ""], ["Bandyopadhyay", "Bortik", ""], ["Schmidt", "Daniel", ""], ["Maneriker", "Pranav", ""], ["Myers", "Christopher", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "2004.03760", "submitter": "Tianda Li", "authors": "Tianda Li, Jia-Chen Gu, Xiaodan Zhu, Quan Liu, Zhen-Hua Ling, Zhiming\n  Su, Si Wei", "title": "DialBERT: A Hierarchical Pre-Trained Model for Conversation\n  Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentanglement is a problem in which multiple conversations occur in the\nsame channel simultaneously, and the listener should decide which utterance is\npart of the conversation he will respond to. We propose a new model, named\nDialogue BERT (DialBERT), which integrates local and global semantics in a\nsingle stream of messages to disentangle the conversations that mixed together.\nWe employ BERT to capture the matching information in each utterance pair at\nthe utterance-level, and use a BiLSTM to aggregate and incorporate the\ncontext-level information. With only a 3% increase in parameters, a 12%\nimprovement has been attained in comparison to BERT, based on the F1-Score. The\nmodel achieves a state-of-the-art result on the a new dataset proposed by IBM\nand surpasses previous work by a substantial margin.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 00:54:01 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Li", "Tianda", ""], ["Gu", "Jia-Chen", ""], ["Zhu", "Xiaodan", ""], ["Liu", "Quan", ""], ["Ling", "Zhen-Hua", ""], ["Su", "Zhiming", ""], ["Wei", "Si", ""]]}, {"id": "2004.03762", "submitter": "Noah Weber", "authors": "Noah Weber, Leena Shekhar, Heeyoung Kwon, Niranjan Balasubramanian,\n  Nathanael Chambers", "title": "Generating Narrative Text in a Switching Dynamical System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early work on narrative modeling used explicit plans and goals to generate\nstories, but the language generation itself was restricted and inflexible.\nModern methods use language models for more robust generation, but often lack\nan explicit representation of the scaffolding and dynamics that guide a\ncoherent narrative. This paper introduces a new model that integrates explicit\nnarrative structure with neural language models, formalizing narrative modeling\nas a Switching Linear Dynamical System (SLDS). A SLDS is a dynamical system in\nwhich the latent dynamics of the system (i.e. how the state vector transforms\nover time) is controlled by top-level discrete switching variables. The\nswitching variables represent narrative structure (e.g., sentiment or discourse\nstates), while the latent state vector encodes information on the current state\nof the narrative. This probabilistic formulation allows us to control\ngeneration, and can be learned in a semi-supervised fashion using both labeled\nand unlabeled data. Additionally, we derive a Gibbs sampler for our model that\ncan fill in arbitrary parts of the narrative, guided by the switching\nvariables. Our filled-in (English language) narratives outperform several\nbaselines on both automatic and human evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 01:05:19 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Weber", "Noah", ""], ["Shekhar", "Leena", ""], ["Kwon", "Heeyoung", ""], ["Balasubramanian", "Niranjan", ""], ["Chambers", "Nathanael", ""]]}, {"id": "2004.03786", "submitter": "Ye Tian", "authors": "Cheng Li, Ye Tian", "title": "Downstream Model Design of Pre-trained Language Model for Relation\n  Extraction Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised relation extraction methods based on deep neural network play an\nimportant role in the recent information extraction field. However, at present,\ntheir performance still fails to reach a good level due to the existence of\ncomplicated relations. On the other hand, recently proposed pre-trained\nlanguage models (PLMs) have achieved great success in multiple tasks of natural\nlanguage processing through fine-tuning when combined with the model of\ndownstream tasks. However, original standard tasks of PLM do not include the\nrelation extraction task yet. We believe that PLMs can also be used to solve\nthe relation extraction problem, but it is necessary to establish a specially\ndesigned downstream task model or even loss function for dealing with\ncomplicated relations. In this paper, a new network architecture with a special\nloss function is designed to serve as a downstream model of PLMs for supervised\nrelation extraction. Experiments have shown that our method significantly\nexceeded the current optimal baseline models across multiple public datasets of\nrelation extraction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 03:16:06 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Li", "Cheng", ""], ["Tian", "Ye", ""]]}, {"id": "2004.03788", "submitter": "Yue Zhou", "authors": "Yue Zhou, Yan Zhang, JingTao Yao", "title": "Satirical News Detection with Semantic Feature Extraction and\n  Game-theoretic Rough Sets", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satirical news detection is an important yet challenging task to prevent\nspread of misinformation. Many feature based and end-to-end neural nets based\nsatirical news detection systems have been proposed and delivered promising\nresults. Existing approaches explore comprehensive word features from satirical\nnews articles, but lack semantic metrics using word vectors for tweet form\nsatirical news. Moreover, the vagueness of satire and news parody determines\nthat a news tweet can hardly be classified with a binary decision, that is,\nsatirical or legitimate. To address these issues, we collect satirical and\nlegitimate news tweets, and propose a semantic feature based approach. Features\nare extracted by exploring inconsistencies in phrases, entities, and between\nmain and relative clauses. We apply game-theoretic rough set model to detect\nsatirical news, in which probabilistic thresholds are derived by game\nequilibrium and repetition learning mechanism. Experimental results on the\ncollected dataset show the robustness and improvement of the proposed approach\ncompared with Pawlak rough set model and SVM.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 03:22:21 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Zhou", "Yue", ""], ["Zhang", "Yan", ""], ["Yao", "JingTao", ""]]}, {"id": "2004.03794", "submitter": "Kristjan Arumae", "authors": "Kristjan Arumae and Parminder Bhatia", "title": "CALM: Continuous Adaptive Learning for Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large language representation models has become a standard in the\nnatural language processing community. This allows for fine tuning on any\nnumber of specific tasks, however, these large high capacity models can\ncontinue to train on domain specific unlabeled data to make initialization even\nmore robust for supervised tasks. We demonstrate that in practice these\npre-trained models present performance deterioration in the form of\ncatastrophic forgetting when evaluated on tasks from a general domain such as\nGLUE. In this work we propose CALM, Continuous Adaptive Learning for Language\nModeling: techniques to render models which retain knowledge across multiple\ndomains. With these methods, we are able to reduce the performance gap across\nsupervised tasks introduced by task specific models which we demonstrate using\na continual learning setting in biomedical and clinical domains.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 03:51:17 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Arumae", "Kristjan", ""], ["Bhatia", "Parminder", ""]]}, {"id": "2004.03807", "submitter": "Abhinav Ramesh Kashyap", "authors": "Abhinav Ramesh Kashyap, Min-Yen Kan", "title": "SciWING -- A Software Toolkit for Scientific Document Processing", "comments": "6 pages, 3 figures, First Workshop on Scholarly Document Processing -\n  SDP@EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SciWING, an open-source software toolkit which provides access\nto pre-trained models for scientific document processing tasks, inclusive of\ncitation string parsing and logical structure recovery. SciWING enables\nresearchers to rapidly experiment with different models by swapping and\nstacking different modules. It also enables them declare and run models from a\nconfiguration file. It enables researchers to perform production-ready transfer\nlearning from general, pre-trained transformers (i.e., BERT, SciBERT etc), and\naids development of end-user applications. It includes ready-to-use web and\nterminal-based applications and demonstrations (Available from\nhttp://sciwing.io).\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 04:43:37 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 07:27:01 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Kashyap", "Abhinav Ramesh", ""], ["Kan", "Min-Yen", ""]]}, {"id": "2004.03808", "submitter": "Xiaoyu Kou", "authors": "Xiaoyu Kou, Yaming Yang, Yujing Wang, Ce Zhang, Yiren Chen, Yunhai\n  Tong, Yan Zhang, and Jing Bai", "title": "Improving BERT with Self-Supervised Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most popular paradigms of applying large, pre-trained NLP models\nsuch as BERT is to fine-tune it on a smaller dataset. However, one challenge\nremains as the fine-tuned model often overfits on smaller datasets. A symptom\nof this phenomenon is that irrelevant words in the sentences, even when they\nare obvious to humans, can substantially degrade the performance of these\nfine-tuned BERT models. In this paper, we propose a novel technique, called\nSelf-Supervised Attention (SSA) to help facilitate this generalization\nchallenge. Specifically, SSA automatically generates weak, token-level\nattention labels iteratively by \"probing\" the fine-tuned model from the\nprevious iteration. We investigate two different ways of integrating SSA into\nBERT and propose a hybrid approach to combine their benefits. Empirically, on a\nvariety of public datasets, we illustrate significant performance improvement\nusing our SSA-enhanced BERT model.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 04:48:44 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:17:12 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 03:49:47 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Kou", "Xiaoyu", ""], ["Yang", "Yaming", ""], ["Wang", "Yujing", ""], ["Zhang", "Ce", ""], ["Chen", "Yiren", ""], ["Tong", "Yunhai", ""], ["Zhang", "Yan", ""], ["Bai", "Jing", ""]]}, {"id": "2004.03809", "submitter": "Ryuichi Takanobu", "authors": "Ryuichi Takanobu, Runze Liang, Minlie Huang", "title": "Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward\n  Decomposition", "comments": "ACL 2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have applied reinforcement learning to train a dialog policy and\nshow great promise these years. One common approach is to employ a user\nsimulator to obtain a large number of simulated user experiences for\nreinforcement learning algorithms. However, modeling a realistic user simulator\nis challenging. A rule-based simulator requires heavy domain expertise for\ncomplex tasks, and a data-driven simulator requires considerable data and it is\neven unclear how to evaluate a simulator. To avoid explicitly building a user\nsimulator beforehand, we propose Multi-Agent Dialog Policy Learning, which\nregards both the system and the user as the dialog agents. Two agents interact\nwith each other and are jointly learned simultaneously. The method uses the\nactor-critic framework to facilitate pretraining and improve scalability. We\nalso propose Hybrid Value Network for the role-aware reward decomposition to\nintegrate role-specific domain knowledge of each agent in the task-oriented\ndialog. Results show that our method can successfully build a system policy and\na user policy simultaneously, and two agents can achieve a high task success\nrate through conversational interaction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 04:51:40 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 02:34:16 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Takanobu", "Ryuichi", ""], ["Liang", "Runze", ""], ["Huang", "Minlie", ""]]}, {"id": "2004.03818", "submitter": "Kehai Chen", "authors": "Kehai Chen, Rui Wang, Masao Utiyama, and Eiichiro Sumita", "title": "Explicit Reordering for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Transformer-based neural machine translation (NMT), the positional\nencoding mechanism helps the self-attention networks to learn the source\nrepresentation with order dependency, which makes the Transformer-based NMT\nachieve state-of-the-art results for various translation tasks. However,\nTransformer-based NMT only adds representations of positions sequentially to\nword vectors in the input sentence and does not explicitly consider reordering\ninformation in this sentence. In this paper, we first empirically investigate\nthe relationship between source reordering information and translation\nperformance. The empirical findings show that the source input with the target\norder learned from the bilingual parallel dataset can substantially improve\ntranslation performance. Thus, we propose a novel reordering method to\nexplicitly model this reordering information for the Transformer-based NMT. The\nempirical results on the WMT14 English-to-German, WAT ASPEC\nJapanese-to-English, and WMT17 Chinese-to-English translation tasks show the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 05:28:46 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Chen", "Kehai", ""], ["Wang", "Rui", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""]]}, {"id": "2004.03822", "submitter": "Leonhard Hennig", "authors": "Johannes Kirschnick, Philippe Thomas, Roland Roller, and Leonhard\n  Hennig", "title": "SIA: A Scalable Interoperable Annotation Server for Biomedical Named\n  Entities", "comments": "11 pages, 2 figures, published in Journal of Cheminformatics", "journal-ref": "J Cheminform 10, 63 (2018)", "doi": "10.1186/s13321-018-0319-2", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years showed a strong increase in biomedical sciences and an inherent\nincrease in publication volume. Extraction of specific information from these\nsources requires highly sophisticated text mining and information extraction\ntools. However, the integration of freely available tools into customized\nworkflows is often cumbersome and difficult. We describe SIA (Scalable\nInteroperable Annotation Server), our contribution to the BeCalm-Technical\ninteroperability and performance of annotation servers (BeCalm-TIPS) task, a\nscalable, extensible, and robust annotation service. The system currently\ncovers six named entity types (i.e., Chemicals, Diseases, Genes, miRNA,\nMutations, and Organisms) and is freely available under Apache 2.0 license at\nhttps://github.com/Erechtheus/sia.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 05:44:55 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Kirschnick", "Johannes", ""], ["Thomas", "Philippe", ""], ["Roller", "Roland", ""], ["Hennig", "Leonhard", ""]]}, {"id": "2004.03829", "submitter": "Andrea Madotto Mr", "authors": "Zhaojiang Lin, Andrea Madotto, Pascale Fung", "title": "Exploring Versatile Generative Language Model Via Parameter-Efficient\n  Transfer Learning", "comments": "Accepted as Findings of EMNLP 2020, Zhaojiang Lin and Andrea Madotto\n  contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning pre-trained generative language models to down-stream language\ngeneration tasks has shown promising results. However, this comes with the cost\nof having a single, large model for each task, which is not ideal in\nlow-memory/power scenarios (e.g., mobile). In this paper, we propose an\neffective way to fine-tune multiple down-stream generation tasks simultaneously\nusing a single, large pre-trained model. The experiments on five diverse\nlanguage generation tasks show that by just using an additional 2-3% parameters\nfor each task, our model can maintain or even improve the performance of\nfine-tuning the whole model.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 06:18:44 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 05:09:59 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Lin", "Zhaojiang", ""], ["Madotto", "Andrea", ""], ["Fung", "Pascale", ""]]}, {"id": "2004.03844", "submitter": "Hassan Sajjad", "authors": "Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov", "title": "On the Effect of Dropping Layers of Pre-trained Transformer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based NLP models are trained using hundreds of millions or even\nbillions of parameters, limiting their applicability in computationally\nconstrained environments. While the number of parameters generally correlates\nwith performance, it is not clear whether the entire network is required for a\ndownstream task. Motivated by the recent work on pruning and distilling\npre-trained models, we explore strategies to drop layers in pre-trained models,\nand observe the effect of pruning on downstream GLUE tasks. We were able to\nprune BERT, RoBERTa and XLNet models up to 40%, while maintaining up to 98% of\ntheir original performance. Additionally we show that our pruned models are on\npar with those built using knowledge distillation, both in terms of size and\nperformance. Our experiments yield interesting observations such as, (i) the\nlower layers are most critical to maintain downstream task performance, (ii)\nsome tasks such as paraphrase detection and sentence similarity are more robust\nto the dropping of layers, and (iii) models trained using a different objective\nfunction exhibit different learning patterns and w.r.t the layer dropping.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 07:09:59 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 11:05:23 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sajjad", "Hassan", ""], ["Dalvi", "Fahim", ""], ["Durrani", "Nadir", ""], ["Nakov", "Preslav", ""]]}, {"id": "2004.03846", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Fei Huang, Kewei Tu", "title": "Structure-Level Knowledge Distillation For Multilingual Sequence\n  Labeling", "comments": "Accepted to ACL 2020, camera-ready. 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual sequence labeling is a task of predicting label sequences using\na single unified model for multiple languages. Compared with relying on\nmultiple monolingual models, using a multilingual model has the benefit of a\nsmaller model size, easier in online serving, and generalizability to\nlow-resource languages. However, current multilingual models still underperform\nindividual monolingual models significantly due to model capacity limitations.\nIn this paper, we propose to reduce the gap between monolingual models and the\nunified multilingual model by distilling the structural knowledge of several\nmonolingual models (teachers) to the unified multilingual model (student). We\npropose two novel KD methods based on structure-level information: (1)\napproximately minimizes the distance between the student's and the teachers'\nstructure level probability distributions, (2) aggregates the structure-level\nknowledge to local distributions and minimizes the distance between two local\nprobability distributions. Our experiments on 4 multilingual tasks with 25\ndatasets show that our approaches outperform several strong baselines and have\nstronger zero-shot generalizability than both the baseline model and teacher\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 07:14:01 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 15:10:38 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 09:28:07 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wang", "Xinyu", ""], ["Jiang", "Yong", ""], ["Bach", "Nguyen", ""], ["Wang", "Tao", ""], ["Huang", "Fei", ""], ["Tu", "Kewei", ""]]}, {"id": "2004.03849", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Yixian Liu, Zixia Jia, Chengyue Jiang, Kewei Tu", "title": "ShanghaiTech at MRP 2019: Sequence-to-Graph Transduction with\n  Second-Order Edge Inference for Cross-Framework Meaning Representation\n  Parsing", "comments": "Accepted to CoNLL 2019 Shared Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the system used in our submission to the \\textit{CoNLL\n2019 shared task: Cross-Framework Meaning Representation Parsing}. Our system\nis a graph-based parser which combines an extended pointer-generator network\nthat generates nodes and a second-order mean field variational inference module\nthat predicts edges. Our system achieved \\nth{1} and \\nth{2} place for the DM\nand PSD frameworks respectively on the in-framework ranks and achieved \\nth{3}\nplace for the DM framework on the cross-framework ranks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 07:19:18 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wang", "Xinyu", ""], ["Liu", "Yixian", ""], ["Jia", "Zixia", ""], ["Jiang", "Chengyue", ""], ["Tu", "Kewei", ""]]}, {"id": "2004.03868", "submitter": "Dieuwke Hupkes", "authors": "Diana Rodr\\'iguez Luna, Edoardo Maria Ponti, Dieuwke Hupkes, Elia\n  Bruni", "title": "Internal and external pressures on language emergence: least effort,\n  object constancy and frequency", "comments": "Accepted for EMNLP-findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work, artificial agents were shown to achieve almost perfect\naccuracy in referential games where they have to communicate to identify\nimages. Nevertheless, the resulting communication protocols rarely display\nsalient features of natural languages, such as compositionality. In this paper,\nwe propose some realistic sources of pressure on communication that avert this\noutcome. More specifically, we formalise the principle of least effort through\nan auxiliary objective. Moreover, we explore several game variants, inspired by\nthe principle of object constancy, in which we alter the frequency, position,\nand luminosity of the objects in the images. We perform an extensive analysis\non their effect through compositionality metrics, diagnostic classifiers, and\nzero-shot evaluation. Our findings reveal that the proposed sources of pressure\nresult in emerging languages with less redundancy, more focus on high-level\nconceptual information, and better abilities of generalisation. Overall, our\ncontributions reduce the gap between emergent and natural languages.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 08:12:41 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 17:48:06 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 09:29:44 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Luna", "Diana Rodr\u00edguez", ""], ["Ponti", "Edoardo Maria", ""], ["Hupkes", "Dieuwke", ""], ["Bruni", "Elia", ""]]}, {"id": "2004.03875", "submitter": "Dayiheng Liu", "authors": "Dayiheng Liu, Yeyun Gong, Jie Fu, Wei Liu, Yu Yan, Bo Shao, Daxin\n  Jiang, Jiancheng Lv, Nan Duan", "title": "Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News\n  Multi-Headline Generation", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News headline generation aims to produce a short sentence to attract readers\nto read the news. One news article often contains multiple keyphrases that are\nof interest to different users, which can naturally have multiple reasonable\nheadlines. However, most existing methods focus on the single headline\ngeneration. In this paper, we propose generating multiple headlines with\nkeyphrases of user interests, whose main idea is to generate multiple\nkeyphrases of interest to users for the news first, and then generate multiple\nkeyphrase-relevant headlines. We propose a multi-source Transformer decoder,\nwhich takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered\narticle, and (c) original article to generate keyphrase-relevant, high-quality,\nand diverse headlines. Furthermore, we propose a simple and effective method to\nmine the keyphrases of interest in the news article and build a first\nlarge-scale keyphrase-aware news headline corpus, which contains over 180K\naligned triples of $<$news article, headline, keyphrase$>$. Extensive\nexperimental comparisons on the real-world dataset show that the proposed\nmethod achieves state-of-the-art results in terms of quality and diversity\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 08:30:05 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 03:02:07 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Liu", "Dayiheng", ""], ["Gong", "Yeyun", ""], ["Fu", "Jie", ""], ["Liu", "Wei", ""], ["Yan", "Yu", ""], ["Shao", "Bo", ""], ["Jiang", "Daxin", ""], ["Lv", "Jiancheng", ""], ["Duan", "Nan", ""]]}, {"id": "2004.03902", "submitter": "Thomas Brochhagen", "authors": "Kristina Gulordava, Thomas Brochhagen, Gemma Boleda", "title": "Deep daxes: Mutual exclusivity arises through both learning biases and\n  pragmatic strategies in neural networks", "comments": null, "journal-ref": "Proceedings of the Annual Conference of the Cognitive Science\n  Society, 42, 2020,2089-2095", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Children's tendency to associate novel words with novel referents has been\ntaken to reflect a bias toward mutual exclusivity. This tendency may be\nadvantageous both as (1) an ad-hoc referent selection heuristic to single out\nreferents lacking a label and as (2) an organizing principle of lexical\nacquisition. This paper investigates under which circumstances\ncross-situational neural models can come to exhibit analogous behavior to\nchildren, focusing on these two possibilities and their interaction. To this\nend, we evaluate neural networks' on both symbolic data and, as a first, on\nlarge-scale image data. We find that constraints in both learning and selection\ncan foster mutual exclusivity, as long as they put words in competition for\nlexical meaning. For computational models, these findings clarify the role of\navailable options for better performance in tasks where mutual exclusivity is\nadvantageous. For cognitive research, they highlight latent interactions\nbetween word learning, referent selection mechanisms, and the structure of\nstimuli of varying complexity: symbolic and visual.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 09:34:24 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 09:02:07 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gulordava", "Kristina", ""], ["Brochhagen", "Thomas", ""], ["Boleda", "Gemma", ""]]}, {"id": "2004.03925", "submitter": "Bhavya Ahuja Grover Ms.", "authors": "Nikhil Kumar Rajput, Bhavya Ahuja Grover and Vipin Kumar Rathi", "title": "Word frequency and sentiment analysis of twitter messages during\n  Coronavirus pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coronavirus pandemic has taken the world by storm as also the social\nmedia. As the awareness about the ailment increased, so did messages, videos\nand posts acknowledging its presence. The social networking site, Twitter,\ndemonstrated similar effect with the number of posts related to coronavirus\nshowing an unprecedented growth in a very short span of time. This paper\npresents a statistical analysis of the twitter messages related to this disease\nposted since January 2020. Two types of empirical studies have been performed.\nThe first is on word frequency and the second on sentiments of the individual\ntweet messages. Inspection of the word frequency is useful in characterizing\nthe patterns or trends in the words used on the site. This would also reflect\non the psychology of the twitter users at this critical juncture. Unigram,\nbigram and trigram frequencies have been modeled by power law distribution. The\nresults have been validated by Sum of Square Error (SSE), R2 and Root Mean\nSquare Error (RMSE). High values of R2 and low values of SSE and RMSE lay the\ngrounds for the goodness of fit of this model. Sentiment analysis has been\nconducted to understand the general attitudes of the twitter users at this\ntime. Both tweets by general public and WHO were part of the corpus. The\nresults showed that the majority of the tweets had a positive polarity and only\nabout 15% were negative.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 10:45:08 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Rajput", "Nikhil Kumar", ""], ["Grover", "Bhavya Ahuja", ""], ["Rathi", "Vipin Kumar", ""]]}, {"id": "2004.03965", "submitter": "Nikola Nikolov", "authors": "Nikola I. Nikolov, Eric Malmi, Curtis G. Northcutt, Loreto Parisi", "title": "Rapformer: Conditional Rap Lyrics Generation with Denoising Autoencoders", "comments": "Accepted at INLG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to combine symbols to generate language is a defining\ncharacteristic of human intelligence, particularly in the context of artistic\nstory-telling through lyrics. We develop a method for synthesizing a rap verse\nbased on the content of any text (e.g., a news article), or for augmenting\npre-existing rap lyrics. Our method, called Rapformer, is based on training a\nTransformer-based denoising autoencoder to reconstruct rap lyrics from content\nwords extracted from the lyrics, trying to preserve the essential meaning,\nwhile matching the target style. Rapformer features a novel BERT-based\nparaphrasing scheme for rhyme enhancement which increases the average rhyme\ndensity of output lyrics by 10%. Experimental results on three diverse input\ndomains show that Rapformer is capable of generating technically fluent verses\nthat offer a good trade-off between content preservation and style transfer.\nFurthermore, a Turing-test-like experiment reveals that Rapformer fools human\nlyrics experts 25% of the time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 12:24:10 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 20:44:28 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Nikolov", "Nikola I.", ""], ["Malmi", "Eric", ""], ["Northcutt", "Curtis G.", ""], ["Parisi", "Loreto", ""]]}, {"id": "2004.03974", "submitter": "Federico Bianchi", "authors": "Federico Bianchi, Silvia Terragni, and Dirk Hovy", "title": "Pre-training is a Hot Topic: Contextualized Document Embeddings Improve\n  Topic Coherence", "comments": "Updated version. Published as a conference paper at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models extract groups of words from documents, whose interpretation as\na topic hopefully allows for a better understanding of the data. However, the\nresulting word groups are often not coherent, making them harder to interpret.\nRecently, neural topic models have shown improvements in overall coherence.\nConcurrently, contextual embeddings have advanced the state of the art of\nneural models in general. In this paper, we combine contextualized\nrepresentations with neural topic models. We find that our approach produces\nmore meaningful and coherent topics than traditional bag-of-words topic models\nand recent neural models. Our results indicate that future improvements in\nlanguage models will translate into better topic models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 12:37:51 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 11:06:11 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Bianchi", "Federico", ""], ["Terragni", "Silvia", ""], ["Hovy", "Dirk", ""]]}, {"id": "2004.04002", "submitter": "Stig-Arne Gr\\\"onroos", "authors": "Stig-Arne Gr\\\"onroos and Sami Virpioja and Mikko Kurimo", "title": "Transfer learning and subword sampling for asymmetric-resource\n  one-to-many neural translation", "comments": "24 pages, 12 tables, 7 figures. Accepted (Nov 2020) for publication\n  in the Machine Translation journal Special Issue on Machine Translation for\n  Low-Resource Languages (Springer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several approaches for improving neural machine translation for\nlow-resource languages: Monolingual data can be exploited via pretraining or\ndata augmentation; Parallel corpora on related language pairs can be used via\nparameter sharing or transfer learning in multilingual models; Subword\nsegmentation and regularization techniques can be applied to ensure high\ncoverage of the vocabulary. We review these approaches in the context of an\nasymmetric-resource one-to-many translation task, in which the pair of target\nlanguages are related, with one being a very low-resource and the other a\nhigher-resource language. We test various methods on three artificially\nrestricted translation tasks -- English to Estonian (low-resource) and Finnish\n(high-resource), English to Slovak and Czech, English to Danish and Swedish --\nand one real-world task, Norwegian to North S\\'ami and Finnish. The experiments\nshow positive effects especially for scheduled multi-task learning, denoising\nautoencoder, and subword sampling.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 14:19:05 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 08:03:58 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Gr\u00f6nroos", "Stig-Arne", ""], ["Virpioja", "Sami", ""], ["Kurimo", "Mikko", ""]]}, {"id": "2004.04010", "submitter": "Fahim Dalvi", "authors": "Fahim Dalvi, Hassan Sajjad, Nadir Durrani and Yonatan Belinkov", "title": "Analyzing Redundancy in Pretrained Transformer Models", "comments": "19 Pages, 14 figures, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based deep NLP models are trained using hundreds of millions of\nparameters, limiting their applicability in computationally constrained\nenvironments. In this paper, we study the cause of these limitations by\ndefining a notion of Redundancy, which we categorize into two classes: General\nRedundancy and Task-specific Redundancy. We dissect two popular pretrained\nmodels, BERT and XLNet, studying how much redundancy they exhibit at a\nrepresentation-level and at a more fine-grained neuron-level. Our analysis\nreveals interesting insights, such as: i) 85% of the neurons across the network\nare redundant and ii) at least 92% of them can be removed when optimizing\ntowards a downstream task. Based on our analysis, we present an efficient\nfeature-based transfer learning procedure, which maintains 97% performance\nwhile using at-most 10% of the original neurons.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 14:29:23 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 11:45:07 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Dalvi", "Fahim", ""], ["Sajjad", "Hassan", ""], ["Durrani", "Nadir", ""], ["Belinkov", "Yonatan", ""]]}, {"id": "2004.04037", "submitter": "Lu Hou", "authors": "Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth", "comments": "NeurIPS-2020 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pre-trained language models like BERT, though powerful in many natural\nlanguage processing tasks, are both computation and memory expensive. To\nalleviate this problem, one approach is to compress them for specific tasks\nbefore deployment. However, recent works on BERT compression usually compress\nthe large BERT model to a fixed smaller size. They can not fully satisfy the\nrequirements of different edge devices with various hardware performances. In\nthis paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT),\nwhich can flexibly adjust the size and latency by selecting adaptive width and\ndepth. The training process of DynaBERT includes first training a\nwidth-adaptive BERT and then allowing both adaptive width and depth, by\ndistilling knowledge from the full-sized model to small sub-networks. Network\nrewiring is also used to keep the more important attention heads and neurons\nshared by more sub-networks. Comprehensive experiments under various efficiency\nconstraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its\nlargest size has comparable performance as BERT-base (or RoBERTa-base), while\nat smaller widths and depths consistently outperforms existing BERT compression\nmethods. Code is available at\nhttps://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 15:06:28 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 08:51:37 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Hou", "Lu", ""], ["Huang", "Zhiqi", ""], ["Shang", "Lifeng", ""], ["Jiang", "Xin", ""], ["Chen", "Xiao", ""], ["Liu", "Qun", ""]]}, {"id": "2004.04060", "submitter": "Stanislav Peshterliev", "authors": "Stanislav Peshterliev, Christophe Dupuy, Imre Kiss", "title": "Self-Attention Gazetteer Embeddings for Named-Entity Recognition", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent attempts to ingest external knowledge into neural models for\nnamed-entity recognition (NER) have exhibited mixed results. In this work, we\npresent GazSelfAttn, a novel gazetteer embedding approach that uses\nself-attention and match span encoding to build enhanced gazetteer embeddings.\nIn addition, we demonstrate how to build gazetteer resources from the open\nsource Wikidata knowledge base. Evaluations on CoNLL-03 and Ontonotes 5\ndatasets, show F1 improvements over baseline model from 92.34 to 92.86 and\n89.11 to 89.32 respectively, achieving performance comparable to large\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 15:31:26 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 14:16:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Peshterliev", "Stanislav", ""], ["Dupuy", "Christophe", ""], ["Kiss", "Imre", ""]]}, {"id": "2004.04070", "submitter": "Ivan Vuli\\'c", "authors": "Ivan Vuli\\'c, Sebastian Ruder, and Anders S{\\o}gaard", "title": "Are All Good Word Vector Spaces Isomorphic?", "comments": "EMNLP 2020: Long paper. Equal contribution from all three authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing algorithms for aligning cross-lingual word vector spaces assume that\nvector spaces are approximately isomorphic. As a result, they perform poorly or\nfail completely on non-isomorphic spaces. Such non-isomorphism has been\nhypothesised to result from typological differences between languages. In this\nwork, we ask whether non-isomorphism is also crucially a sign of degenerate\nword vector spaces. We present a series of experiments across diverse languages\nwhich show that variance in performance across language pairs is not only due\nto typological differences, but can mostly be attributed to the size of the\nmonolingual resources available, and to the properties and duration of\nmonolingual training (e.g. \"under-training\").\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 15:49:19 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 17:22:02 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Vuli\u0107", "Ivan", ""], ["Ruder", "Sebastian", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2004.04092", "submitter": "Chunyuan Li", "authors": "Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang,\n  Jianfeng Gao", "title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space", "comments": "Accepted in EMNLP 2020; Code: https://github.com/ChunyuanLI/Optimus\n  Demo: http://aka.ms/optimus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When trained effectively, the Variational Autoencoder (VAE) can be both a\npowerful generative model and an effective representation learning framework\nfor natural language. In this paper, we propose the first large-scale language\nVAE model, Optimus. A universal latent embedding space for sentences is first\npre-trained on large text corpus, and then fine-tuned for various language\ngeneration and understanding tasks. Compared with GPT-2, Optimus enables guided\nlanguage generation from an abstract level using the latent vectors. Compared\nwith BERT, Optimus can generalize better on low-resource language understanding\ntasks due to the smooth latent space structure. Extensive experimental results\non a wide range of language tasks demonstrate the effectiveness of Optimus. It\nachieves new state-of-the-art on VAE language modeling benchmarks. We hope that\nour first pre-trained big VAE language model itself and results can help the\nNLP community renew the interests of deep generative models in the era of\nlarge-scale pre-training, and make these principled methods more practical.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 06:20:18 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 19:11:42 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 00:41:43 GMT"}, {"version": "v4", "created": "Sun, 11 Oct 2020 23:33:10 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Chunyuan", ""], ["Gao", "Xiang", ""], ["Li", "Yuan", ""], ["Peng", "Baolin", ""], ["Li", "Xiujun", ""], ["Zhang", "Yizhe", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2004.04100", "submitter": "Hao Zhou", "authors": "Hao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang, Xiaoyan Zhu", "title": "KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn\n  Knowledge-driven Conversation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research of knowledge-driven conversational systems is largely limited\ndue to the lack of dialog data which consist of multi-turn conversations on\nmultiple topics and with knowledge annotations. In this paper, we propose a\nChinese multi-domain knowledge-driven conversation dataset, KdConv, which\ngrounds the topics in multi-turn conversations to knowledge graphs. Our corpus\ncontains 4.5K conversations from three domains (film, music, and travel), and\n86K utterances with an average turn number of 19.0. These conversations contain\nin-depth discussions on related topics and natural transition between multiple\ntopics. To facilitate the following research on this corpus, we provide several\nbenchmark models. Comparative results show that the models can be enhanced by\nintroducing background knowledge, yet there is still a large space for\nleveraging knowledge to model multi-turn conversations for further research.\nResults also show that there are obvious performance differences between\ndifferent domains, indicating that it is worth to further explore transfer\nlearning and domain adaptation. The corpus and benchmark models are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:25:39 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Zhou", "Hao", ""], ["Zheng", "Chujie", ""], ["Huang", "Kaili", ""], ["Huang", "Minlie", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "2004.04103", "submitter": "Jeremy Barnes", "authors": "Irean Navas Alejo, Toni Badia, and Jeremy Barnes", "title": "Cross-lingual Emotion Intensity Prediction", "comments": "Accepted in PEOPLES 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Emotion intensity prediction determines the degree or intensity of an emotion\nthat the author expresses in a text, extending previous categorical approaches\nto emotion detection. While most previous work on this topic has concentrated\non English texts, other languages would also benefit from fine-grained emotion\nclassification, preferably without having to recreate the amount of annotated\ndata available in English in each new language. Consequently, we explore\ncross-lingual transfer approaches for fine-grained emotion detection in Spanish\nand Catalan tweets. To this end we annotate a test set of Spanish and Catalan\ntweets using Best-Worst scaling. We compare six cross-lingual approaches, e.g.,\nmachine translation and cross-lingual embeddings, which have varying\nrequirements for parallel data -- from millions of parallel sentences to\ncompletely unsupervised. The results show that on this data, methods with low\nparallel-data requirements perform surprisingly better than methods that use\nmore parallel data, which we explain through an in-depth error analysis. We\nmake the dataset and the code available at\n\\url{https://github.com/jerbarnes/fine-grained_cross-lingual_emotion}\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:28:16 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 18:48:50 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Alejo", "Irean Navas", ""], ["Badia", "Toni", ""], ["Barnes", "Jeremy", ""]]}, {"id": "2004.04106", "submitter": "Aaron Steven White", "authors": "Aaron Steven White, Kyle Rawlins", "title": "Frequency, Acceptability, and Selection: A case study of\n  clause-embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the relationship between the frequency with which verbs are\nfound in particular subcategorization frames and the acceptability of those\nverbs in those frames, focusing in particular on subordinate clause-taking\nverbs, such as \"think\", \"want\", and \"tell\". We show that verbs'\nsubcategorization frame frequency distributions are poor predictors of their\nacceptability in those frames---explaining, at best, less than 1/3 of the total\ninformation about acceptability across the lexicon---and, further, that common\nmatrix factorization techniques used to model the acquisition of verbs'\nacceptability in subcategorization frames fare only marginally better. All data\nand code are available at http://megaattitude.io.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:34:19 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["White", "Aaron Steven", ""], ["Rawlins", "Kyle", ""]]}, {"id": "2004.04123", "submitter": "Oshin Agarwal", "authors": "Oshin Agarwal, Yinfei Yang, Byron C. Wallace, Ani Nenkova", "title": "Entity-Switched Datasets: An Approach to Auditing the In-Domain\n  Robustness of Named Entity Recognition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition systems perform well on standard datasets comprising\nEnglish news. But given the paucity of data, it is difficult to draw\nconclusions about the robustness of systems with respect to recognizing a\ndiverse set of entities. We propose a method for auditing the in-domain\nrobustness of systems, focusing specifically on differences in performance due\nto the national origin of entities. We create entity-switched datasets, in\nwhich named entities in the original texts are replaced by plausible named\nentities of the same type but of different national origin. We find that\nstate-of-the-art systems' performance vary widely even in-domain: In the same\ncontext, entities from certain origins are more reliably recognized than\nentities from elsewhere. Systems perform best on American and Indian entities,\nand worst on Vietnamese and Indonesian entities. This auditing approach can\nfacilitate the development of more robust named entity recognition systems, and\nwill allow research in this area to consider fairness criteria that have\nreceived heightened attention in other predictive technology work.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:11:31 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 18:50:37 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Agarwal", "Oshin", ""], ["Yang", "Yinfei", ""], ["Wallace", "Byron C.", ""], ["Nenkova", "Ani", ""]]}, {"id": "2004.04124", "submitter": "Yihuan Mao", "authors": "Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming\n  Yang, Quanlu Zhang, Yunhai Tong, Jing Bai", "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model\n  Compression", "comments": "COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT is a cutting-edge language representation model pre-trained by a large\ncorpus, which achieves superior performances on various natural language\nunderstanding tasks. However, a major blocking issue of applying BERT to online\nservices is that it is memory-intensive and leads to unsatisfactory latency of\nuser requests, raising the necessity of model compression. Existing solutions\nleverage the knowledge distillation framework to learn a smaller model that\nimitates the behaviors of BERT. However, the training procedure of knowledge\ndistillation is expensive itself as it requires sufficient training data to\nimitate the teacher model. In this paper, we address this issue by proposing a\nhybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid\nmodel compression), which combines the advantages of different model\ncompression methods, including weight pruning, matrix factorization and\nknowledge distillation. LadaBERT achieves state-of-the-art accuracy on various\npublic datasets while the training overheads can be reduced by an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:18:56 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 15:15:11 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Mao", "Yihuan", ""], ["Wang", "Yujing", ""], ["Wu", "Chufan", ""], ["Zhang", "Chen", ""], ["Wang", "Yang", ""], ["Yang", "Yaming", ""], ["Zhang", "Quanlu", ""], ["Tong", "Yunhai", ""], ["Bai", "Jing", ""]]}, {"id": "2004.04128", "submitter": "Adriana Correia", "authors": "A. D. Correia, H. T. C. Stoof, M. Moortgat", "title": "Putting a Spin on Language: A Quantum Interpretation of Unary\n  Connectives for Linguistic Applications", "comments": "24 pages, 4 figures, QPL20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extended versions of the Lambek Calculus currently used in computational\nlinguistics rely on unary modalities to allow for the controlled application of\nstructural rules affecting word order and phrase structure. These controlled\nstructural operations give rise to derivational ambiguities that are missed by\nthe original Lambek Calculus or its pregroup simplification. Proposals for\ncompositional interpretation of extended Lambek Calculus in the compact closed\ncategory of FVect and linear maps have been made, but in these proposals the\nsyntax-semantics mapping ignores the control modalities, effectively\nrestricting their role to the syntax. Our aim is to turn the modalities into\nfirst-class citizens of the vectorial interpretation. Building on the\ndirectional density matrix semantics, we extend the interpretation of the type\nsystem with an extra spin density matrix space. The interpretation of proofs\nthen results in ambiguous derivations being tensored with orthogonal spin\nstates. Our method introduces a way of simultaneously representing co existing\ninterpretations of ambiguous utterances, and provides a uniform framework for\nthe integration of lexical and derivational ambiguity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:25:11 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 14:47:12 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Correia", "A. D.", ""], ["Stoof", "H. T. C.", ""], ["Moortgat", "M.", ""]]}, {"id": "2004.04216", "submitter": "Marco Guerini", "authors": "Serra Sinem Tekiroglu, Yi-Ling Chung, Marco Guerini", "title": "Generating Counter Narratives against Online Hate Speech: Data and\n  Strategies", "comments": "To appear at ACL 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently research has started focusing on avoiding undesired effects that\ncome with content moderation, such as censorship and overblocking, when dealing\nwith hatred online. The core idea is to directly intervene in the discussion\nwith textual responses that are meant to counter the hate content and prevent\nit from further spreading. Accordingly, automation strategies, such as natural\nlanguage generation, are beginning to be investigated. Still, they suffer from\nthe lack of sufficient amount of quality data and tend to produce\ngeneric/repetitive responses. Being aware of the aforementioned limitations, we\npresent a study on how to collect responses to hate effectively, employing\nlarge scale unsupervised language models such as GPT-2 for the generation of\nsilver data, and the best annotation strategies/neural architectures that can\nbe used for data filtering before expert validation/post-editing.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 19:35:00 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Tekiroglu", "Serra Sinem", ""], ["Chung", "Yi-Ling", ""], ["Guerini", "Marco", ""]]}, {"id": "2004.04225", "submitter": "Bennett Kleinberg", "authors": "Bennett Kleinberg, Isabelle van der Vegt, Maximilian Mozes", "title": "Measuring Emotions in the COVID-19 Real World Worry Dataset", "comments": "Accepted to ACL 2020 COVID-19 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic is having a dramatic impact on societies and economies\naround the world. With various measures of lockdowns and social distancing in\nplace, it becomes important to understand emotional responses on a large scale.\nIn this paper, we present the first ground truth dataset of emotional responses\nto COVID-19. We asked participants to indicate their emotions and express these\nin text. This resulted in the Real World Worry Dataset of 5,000 texts (2,500\nshort + 2,500 long texts). Our analyses suggest that emotional responses\ncorrelated with linguistic measures. Topic modeling further revealed that\npeople in the UK worry about their family and the economic situation.\nTweet-sized texts functioned as a call for solidarity, while longer texts shed\nlight on worries and concerns. Using predictive modeling approaches, we were\nable to approximate the emotional responses of participants from text within\n14% of their actual value. We encourage others to use the dataset and improve\nhow we can use automated methods to learn about emotional responses and worries\nabout an urgent problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 19:52:14 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 17:57:23 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Kleinberg", "Bennett", ""], ["van der Vegt", "Isabelle", ""], ["Mozes", "Maximilian", ""]]}, {"id": "2004.04228", "submitter": "Alex Wang", "authors": "Alex Wang, Kyunghyun Cho, and Mike Lewis", "title": "Asking and Answering Questions to Evaluate the Factual Consistency of\n  Summaries", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical applications of abstractive summarization models are limited by\nfrequent factual inconsistencies with respect to their input. Existing\nautomatic evaluation metrics for summarization are largely insensitive to such\nerrors. We propose an automatic evaluation protocol called QAGS (pronounced\n\"kags\") that is designed to identify factual inconsistencies in a generated\nsummary. QAGS is based on the intuition that if we ask questions about a\nsummary and its source, we will receive similar answers if the summary is\nfactually consistent with the source. To evaluate QAGS, we collect human\njudgments of factual consistency on model-generated summaries for the\nCNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018)\nsummarization datasets. QAGS has substantially higher correlations with these\njudgments than other automatic evaluation metrics. Also, QAGS offers a natural\nform of interpretability: The answers and questions generated while computing\nQAGS indicate which tokens of a summary are inconsistent and why. We believe\nQAGS is a promising tool in automatically generating usable and factually\nconsistent text.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 20:01:09 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Wang", "Alex", ""], ["Cho", "Kyunghyun", ""], ["Lewis", "Mike", ""]]}, {"id": "2004.04243", "submitter": "Stefan Constantin", "authors": "Stefan Constantin and Alex Waibel", "title": "Error-correction and extraction in request dialogs", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a component that gets a request and a correction and outputs a\ncorrected request. To get this corrected request, the entities in the\ncorrection phrase replace their corresponding entities in the request. In\naddition, the proposed component outputs these pairs of corresponding\nreparandum and repair entity. These entity pairs can be used, for example, for\nlearning in a life-long learning component of a dialog system to reduce the\nneed for correction in future dialogs. For the approach described in this work,\nwe fine-tune BERT for sequence labeling. We created a dataset to evaluate our\ncomponent; for which we got an accuracy of 93.28 %. An accuracy of 88.58 % has\nbeen achieved for out-of-domain data. This accuracy shows that the proposed\ncomponent is learning the concept of corrections and can be developed to be\nused as an upstream component to avoid the need for collecting data for request\ncorrections for every new domain.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 20:49:10 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Constantin", "Stefan", ""], ["Waibel", "Alex", ""]]}, {"id": "2004.04270", "submitter": "Sravana Reddy", "authors": "Ann Clifton, Aasish Pappu, Sravana Reddy, Yongze Yu, Jussi Karlgren,\n  Ben Carterette, Rosie Jones", "title": "The Spotify Podcast Dataset", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Podcasts are a relatively new form of audio media. Episodes appear on a\nregular cadence, and come in many different formats and levels of formality.\nThey can be formal news journalism or conversational chat; fiction or\nnon-fiction. They are rapidly growing in popularity and yet have been\nrelatively little studied. As an audio format, podcasts are more varied in\nstyle and production types than, say, broadcast news, and contain many more\ngenres than typically studied in video research. The medium is therefore a rich\ndomain with many research avenues for the IR and NLP communities. We present\nthe Spotify Podcast Dataset, a set of approximately 100K podcast episodes\ncomprised of raw audio files along with accompanying ASR transcripts. This\nrepresents over 47,000 hours of transcribed audio, and is an order of magnitude\nlarger than previous speech-to-text corpora.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 21:25:00 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 17:58:57 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 05:50:48 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Clifton", "Ann", ""], ["Pappu", "Aasish", ""], ["Reddy", "Sravana", ""], ["Yu", "Yongze", ""], ["Karlgren", "Jussi", ""], ["Carterette", "Ben", ""], ["Jones", "Rosie", ""]]}, {"id": "2004.04295", "submitter": "Miguel Ballesteros", "authors": "Miguel Ballesteros, Rishita Anubhai, Shuai Wang, Nima Pourdamghani,\n  Yogarshi Vyas, Jie Ma, Parminder Bhatia, Kathleen McKeown, and Yaser\n  Al-Onaizan", "title": "Severing the Edge Between Before and After: Neural Architectures for\n  Temporal Ordering of Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a neural architecture and a set of training methods\nfor ordering events by predicting temporal relations. Our proposed models\nreceive a pair of events within a span of text as input and they identify\ntemporal relations (Before, After, Equal, Vague) between them. Given that a key\nchallenge with this task is the scarcity of annotated data, our models rely on\neither pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and\nmulti-task learning (by leveraging complementary datasets), and self-training\ntechniques. Experiments on the MATRES dataset of English documents establish a\nnew state-of-the-art on this task.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 23:17:10 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Ballesteros", "Miguel", ""], ["Anubhai", "Rishita", ""], ["Wang", "Shuai", ""], ["Pourdamghani", "Nima", ""], ["Vyas", "Yogarshi", ""], ["Ma", "Jie", ""], ["Bhatia", "Parminder", ""], ["McKeown", "Kathleen", ""], ["Al-Onaizan", "Yaser", ""]]}, {"id": "2004.04305", "submitter": "Shahin Shayandeh", "authors": "Swadheen Shukla, Lars Liden, Shahin Shayandeh, Eslam Kamal, Jinchao\n  Li, Matt Mazzola, Thomas Park, Baolin Peng, Jianfeng Gao", "title": "Conversation Learner -- A Machine Teaching Tool for Building Dialog\n  Managers for Task-Oriented Dialog Systems", "comments": "Accepted to ACL 2020 Demonstration Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, industry solutions for building a task-oriented dialog system\nhave relied on helping dialog authors define rule-based dialog managers,\nrepresented as dialog flows. While dialog flows are intuitively interpretable\nand good for simple scenarios, they fall short of performance in terms of the\nflexibility needed to handle complex dialogs. On the other hand, purely\nmachine-learned models can handle complex dialogs, but they are considered to\nbe black boxes and require large amounts of training data. In this\ndemonstration, we showcase Conversation Learner, a machine teaching tool for\nbuilding dialog managers. It combines the best of both approaches by enabling\ndialog authors to create a dialog flow using familiar tools, converting the\ndialog flow into a parametric model (e.g., neural networks), and allowing\ndialog authors to improve the dialog manager (i.e., the parametric model) over\ntime by leveraging user-system dialog logs as training data through a machine\nteaching interface.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 00:10:54 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 20:14:05 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Shukla", "Swadheen", ""], ["Liden", "Lars", ""], ["Shayandeh", "Shahin", ""], ["Kamal", "Eslam", ""], ["Li", "Jinchao", ""], ["Mazzola", "Matt", ""], ["Park", "Thomas", ""], ["Peng", "Baolin", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2004.04312", "submitter": "Andrea Burns", "authors": "Andrea Burns, Donghyun Kim, Derry Wijaya, Kate Saenko, Bryan A.\n  Plummer", "title": "Learning to Scale Multilingual Representations for Vision-Language Tasks", "comments": "ECCV 2020 accepted spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current multilingual vision-language models either require a large number of\nadditional parameters for each supported language, or suffer performance\ndegradation as languages are added. In this paper, we propose a Scalable\nMultilingual Aligned Language Representation (SMALR) that supports many\nlanguages with few model parameters without sacrificing downstream task\nperformance. SMALR learns a fixed size language-agnostic representation for\nmost words in a multilingual vocabulary, keeping language-specific features for\njust a few. We use a masked cross-language modeling loss to align features with\ncontext from other languages. Additionally, we propose a cross-lingual\nconsistency module that ensures predictions made for a query and its machine\ntranslation are comparable. The effectiveness of SMALR is demonstrated with ten\ndiverse languages, over twice the number supported in vision-language tasks to\ndate. We evaluate on multilingual image-sentence retrieval and outperform prior\nwork by 3-4% with less than 1/5th the training parameters compared to other\nword embedding methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 01:03:44 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 19:01:28 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Burns", "Andrea", ""], ["Kim", "Donghyun", ""], ["Wijaya", "Derry", ""], ["Saenko", "Kate", ""], ["Plummer", "Bryan A.", ""]]}, {"id": "2004.04315", "submitter": "Eisa Alanazi", "authors": "Sarah Alqurashi, Ahmad Alhindi, Eisa Alanazi", "title": "Large Arabic Twitter Dataset on COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2019 coronavirus disease (COVID-19), emerged late December 2019 in China,\nis now rapidly spreading across the globe. At the time of writing this paper,\nthe number of global confirmed cases has passed two millions and half with over\n180,000 fatalities. Many countries have enforced strict social distancing\npolicies to contain the spread of the virus. This have changed the daily life\nof tens of millions of people, and urged people to turn their discussions\nonline, e.g., via online social media sites like Twitter. In this work, we\ndescribe the first Arabic tweets dataset on COVID-19 that we have been\ncollecting since January 1st, 2020. The dataset would help researchers and\npolicy makers in studying different societal issues related to the pandemic.\nMany other tasks related to behavioral change, information sharing,\nmisinformation and rumors spreading can also be analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 01:07:12 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 22:38:15 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Alqurashi", "Sarah", ""], ["Alhindi", "Ahmad", ""], ["Alanazi", "Eisa", ""]]}, {"id": "2004.04343", "submitter": "Jo\\~ao Ribeiro", "authors": "Jo\\~ao G. Ribeiro, Frederico S. Felisberto and Isabel C. Neto", "title": "Pruning and Sparsemax Methods for Hierarchical Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and evaluates two novel Hierarchical Attention Network\nmodels [Yang et al., 2016] - i) Hierarchical Pruned Attention Networks, which\nremove the irrelevant words and sentences from the classification process in\norder to reduce potential noise in the document classification accuracy and ii)\nHierarchical Sparsemax Attention Networks, which replace the Softmax function\nused in the attention mechanism with the Sparsemax [Martins and Astudillo,\n2016], capable of better handling importance distributions where a lot of words\nor sentences have very low probabilities. Our empirical evaluation on the IMDB\nReview for sentiment analysis datasets shows both approaches to be able to\nmatch the results obtained by the current state-of-the-art (without, however,\nany significant benefits). All our source code is made available\nathttps://github.com/jmribeiro/dsl-project.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:56:58 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Ribeiro", "Jo\u00e3o G.", ""], ["Felisberto", "Frederico S.", ""], ["Neto", "Isabel C.", ""]]}, {"id": "2004.04361", "submitter": "Abhyuday Jagannatha", "authors": "Abhyuday Jagannatha, Hong Yu", "title": "Calibrating Structured Output Predictors for Natural Language Processing", "comments": "ACL 2020; 9 pages + 4 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of calibrating prediction confidence for output\nentities of interest in natural language processing (NLP) applications. It is\nimportant that NLP applications such as named entity recognition and question\nanswering produce calibrated confidence scores for their predictions,\nespecially if the system is to be deployed in a safety-critical domain such as\nhealthcare. However, the output space of such structured prediction models is\noften too large to adapt binary or multi-class calibration methods directly. In\nthis study, we propose a general calibration scheme for output entities of\ninterest in neural-network based structured prediction models. Our proposed\nmethod can be used with any binary class calibration scheme and a neural\nnetwork model. Additionally, we show that our calibration method can also be\nused as an uncertainty-aware, entity-specific decoding step to improve the\nperformance of the underlying model at no additional training cost or data\nrequirements. We show that our method outperforms current calibration\ntechniques for named-entity-recognition, part-of-speech and question answering.\nWe also improve our model's performance from our decoding step across several\ntasks and benchmark datasets. Our method improves the calibration and model\nperformance on out-of-domain test scenarios as well.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 04:14:46 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 21:28:08 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Jagannatha", "Abhyuday", ""], ["Yu", "Hong", ""]]}, {"id": "2004.04418", "submitter": "Elan van Biljon", "authors": "Elan van Biljon, Arnu Pretorius and Julia Kreutzer", "title": "On Optimal Transformer Depth for Low-Resource Language Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have shown great promise as an approach to Neural Machine\nTranslation (NMT) for low-resource languages. However, at the same time,\ntransformer models remain difficult to optimize and require careful tuning of\nhyper-parameters to be useful in this setting. Many NMT toolkits come with a\nset of default hyper-parameters, which researchers and practitioners often\nadopt for the sake of convenience and avoiding tuning. These configurations,\nhowever, have been optimized for large-scale machine translation data sets with\nseveral millions of parallel sentences for European languages like English and\nFrench. In this work, we find that the current trend in the field to use very\nlarge models is detrimental for low-resource languages, since it makes training\nmore difficult and hurts overall performance, confirming previous observations.\nWe see our work as complementary to the Masakhane project (\"Masakhane\" means\n\"We Build Together\" in isiZulu.) In this spirit, low-resource NMT systems are\nnow being built by the community who needs them the most. However, many in the\ncommunity still have very limited access to the type of computational resources\nrequired for building extremely large models promoted by industrial research.\nTherefore, by showing that transformer models perform well (and often best) at\nlow-to-moderate depth, we hope to convince fellow researchers to devote less\ncomputational resources, as well as time, to exploring overly large models\nduring the development of these systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 08:25:02 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 19:42:41 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["van Biljon", "Elan", ""], ["Pretorius", "Arnu", ""], ["Kreutzer", "Julia", ""]]}, {"id": "2004.04423", "submitter": "Heiko Paulheim", "authors": "Ahmad Al Taweel and Heiko Paulheim", "title": "Towards Exploiting Implicit Human Feedback for Improving RDF2vec\n  Embeddings", "comments": "Workshop paper accepted at Deep Learning for Knowledge Graphs\n  Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  RDF2vec is a technique for creating vector space embeddings from an RDF\nknowledge graph, i.e., representing each entity in the graph as a vector. It\nfirst creates sequences of nodes by performing random walks on the graph. In a\nsecond step, those sequences are processed by the word2vec algorithm for\ncreating the actual embeddings. In this paper, we explore the use of external\nedge weights for guiding the random walks. As edge weights, transition\nprobabilities between pages in Wikipedia are used as a proxy for the human\nfeedback for the importance of an edge. We show that in some scenarios, RDF2vec\nutilizing those transition probabilities can outperform both RDF2vec based on\nrandom walks as well as the usage of graph internal edge weights.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 08:39:19 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Taweel", "Ahmad Al", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2004.04435", "submitter": "Oksana Shadura", "authors": "Vassil Vassilev (1), Aleksandr Efremov (1) and Oksana Shadura (2) ((1)\n  Princeton University, (2) University of Nebraska Lincoln)", "title": "Automatic Differentiation in ROOT", "comments": "Submitted as a proceeding for CHEP 2019", "journal-ref": null, "doi": "10.1051/epjconf/202024502015", "report-no": null, "categories": "cs.MS cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mathematics and computer algebra, automatic differentiation (AD) is a set\nof techniques to evaluate the derivative of a function specified by a computer\nprogram. AD exploits the fact that every computer program, no matter how\ncomplicated, executes a sequence of elementary arithmetic operations (addition,\nsubtraction, multiplication, division, etc.), elementary functions (exp, log,\nsin, cos, etc.) and control flow statements. AD takes source code of a function\nas input and produces source code of the derived function. By applying the\nchain rule repeatedly to these operations, derivatives of arbitrary order can\nbe computed automatically, accurately to working precision, and using at most a\nsmall constant factor more arithmetic operations than the original program.\n  This paper presents AD techniques available in ROOT, supported by Cling, to\nproduce derivatives of arbitrary C/C++ functions through implementing source\ncode transformation and employing the chain rule of differential calculus in\nboth forward mode and reverse mode. We explain its current integration for\ngradient computation in TFormula. We demonstrate the correctness and\nperformance improvements in ROOT's fitting algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 09:18:50 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Vassilev", "Vassil", ""], ["Efremov", "Aleksandr", ""], ["Shadura", "Oksana", ""]]}, {"id": "2004.04438", "submitter": "Junwei Liao", "authors": "Junwei Liao, Sefik Emre Eskimez, Liyang Lu, Yu Shi, Ming Gong, Linjun\n  Shou, Hong Qu, Michael Zeng", "title": "Improving Readability for Automatic Speech Recognition Transcription", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Automatic Speech Recognition (ASR) systems can achieve high\nperformance in terms of recognition accuracy. However, a perfectly accurate\ntranscript still can be challenging to read due to grammatical errors,\ndisfluency, and other errata common in spoken communication. Many downstream\ntasks and human readers rely on the output of the ASR system; therefore, errors\nintroduced by the speaker and ASR system alike will be propagated to the next\ntask in the pipeline. In this work, we propose a novel NLP task called ASR\npost-processing for readability (APR) that aims to transform the noisy ASR\noutput into a readable text for humans and downstream tasks while maintaining\nthe semantic meaning of the speaker. In addition, we describe a method to\naddress the lack of task-specific data by synthesizing examples for the APR\ntask using the datasets collected for Grammatical Error Correction (GEC)\nfollowed by text-to-speech (TTS) and ASR. Furthermore, we propose metrics\nborrowed from similar tasks to evaluate performance on the APR task. We compare\nfine-tuned models based on several open-sourced and adapted pre-trained models\nwith the traditional pipeline method. Our results suggest that finetuned models\nimprove the performance on the APR task significantly, hinting at the potential\nbenefits of using APR systems. We hope that the read, understand, and rewrite\napproach of our work can serve as a basis that many NLP tasks and human readers\ncan benefit from.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 09:26:42 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Liao", "Junwei", ""], ["Eskimez", "Sefik Emre", ""], ["Lu", "Liyang", ""], ["Shi", "Yu", ""], ["Gong", "Ming", ""], ["Shou", "Linjun", ""], ["Qu", "Hong", ""], ["Zeng", "Michael", ""]]}, {"id": "2004.04460", "submitter": "Matej Gjurkovic", "authors": "Matej Gjurkovi\\'c, Mladen Karan, Iva Vukojevi\\'c, Mihaela Bo\\v{s}njak,\n  Jan \\v{S}najder", "title": "PANDORA Talks: Personality and Demographics on Reddit", "comments": "Proceedings of the Ninth International Workshop on Natural Language\n  Processing for Social Media, NAACL 2021,\n  https://www.aclweb.org/anthology/2021.socialnlp-1.12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personality and demographics are important variables in social sciences,\nwhile in NLP they can aid in interpretability and removal of societal biases.\nHowever, datasets with both personality and demographic labels are scarce. To\naddress this, we present PANDORA, the first large-scale dataset of Reddit\ncomments labeled with three personality models (including the well-established\nBig 5 model) and demographics (age, gender, and location) for more than 10k\nusers. We showcase the usefulness of this dataset on three experiments, where\nwe leverage the more readily available data from other personality models to\npredict the Big 5 traits, analyze gender classification biases arising from\npsycho-demographic variables, and carry out a confirmatory and exploratory\nanalysis based on psychological theories. Finally, we present benchmark\nprediction models for all personality and demographic variables.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 10:08:05 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 11:04:44 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 13:22:41 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gjurkovi\u0107", "Matej", ""], ["Karan", "Mladen", ""], ["Vukojevi\u0107", "Iva", ""], ["Bo\u0161njak", "Mihaela", ""], ["\u0160najder", "Jan", ""]]}, {"id": "2004.04468", "submitter": "Elvys Linhares Pontes", "authors": "Elvys Linhares Pontes, St\\'ephane Huet, Juan-Manuel Torres-Moreno,\n  Thiago G. da Silva, and Andr\\'ea Carneiro Linhares", "title": "A Multilingual Study of Multi-Sentence Compression using Word\n  Vertex-Labeled Graphs and Integer Linear Programming", "comments": "Preprint version", "journal-ref": "Computaci\\'on y Sistemas Vo. 24, No. 2, 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Sentence Compression (MSC) aims to generate a short sentence with the\nkey information from a cluster of similar sentences. MSC enables summarization\nand question-answering systems to generate outputs combining fully formed\nsentences from one or several documents. This paper describes an Integer Linear\nProgramming method for MSC using a vertex-labeled graph to select different\nkeywords, with the goal of generating more informative sentences while\nmaintaining their grammaticality. Our system is of good quality and outperforms\nthe state of the art for evaluations led on news datasets in three languages:\nFrench, Portuguese and Spanish. We led both automatic and manual evaluations to\ndetermine the informativeness and the grammaticality of compressions for each\ndataset. In additional tests, which take advantage of the fact that the length\nof compressions can be modulated, we still improve ROUGE scores with shorter\noutput sentences.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 10:35:16 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Pontes", "Elvys Linhares", ""], ["Huet", "St\u00e9phane", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["da Silva", "Thiago G.", ""], ["Linhares", "Andr\u00e9a Carneiro", ""]]}, {"id": "2004.04478", "submitter": "Akash Sheoran", "authors": "Akash Sheoran, Diptesh Kanojia, Aditya Joshi, Pushpak Bhattacharyya", "title": "Recommendation Chart of Domains for Cross-Domain Sentiment\n  Analysis:Findings of A 20 Domain Study", "comments": "12th Edition of Language Resources and Evaluation Conference (LREC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain sentiment analysis (CDSA) helps to address the problem of data\nscarcity in scenarios where labelled data for a domain (known as the target\ndomain) is unavailable or insufficient. However, the decision to choose a\ndomain (known as the source domain) to leverage from is, at best, intuitive. In\nthis paper, we investigate text similarity metrics to facilitate source domain\nselection for CDSA. We report results on 20 domains (all possible pairs) using\n11 similarity metrics. Specifically, we compare CDSA performance with these\nmetrics for different domain-pairs to enable the selection of a suitable source\ndomain, given a target domain. These metrics include two novel metrics for\nevaluating domain adaptability to help source domain selection of labelled data\nand utilize word and sentence-based embeddings as metrics for unlabelled data.\nThe goal of our experiments is a recommendation chart that gives the K best\nsource domains for CDSA for a given target domain. We show that the best K\nsource domains returned by our similarity metrics have a precision of over 50%,\nfor varying values of K.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 10:55:01 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Sheoran", "Akash", ""], ["Kanojia", "Diptesh", ""], ["Joshi", "Aditya", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "2004.04487", "submitter": "Mor Geva", "authors": "Mor Geva, Ankit Gupta, Jonathan Berant", "title": "Injecting Numerical Reasoning Skills into Language Models", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained language models (LMs) are known to encode substantial\namounts of linguistic information. However, high-level reasoning skills, such\nas numerical reasoning, are difficult to learn from a language-modeling\nobjective only. Consequently, existing models for numerical reasoning have used\nspecialized architectures with limited flexibility. In this work, we show that\nnumerical reasoning is amenable to automatic data generation, and thus one can\ninject this skill into pre-trained LMs, by generating large amounts of data,\nand training in a multi-task setup. We show that pre-training our model,\nGenBERT, on this data, dramatically improves performance on DROP (49.3\n$\\rightarrow$ 72.3 F1), reaching performance that matches state-of-the-art\nmodels of comparable size, while using a simple and general-purpose\nencoder-decoder architecture. Moreover, GenBERT generalizes well to math word\nproblem datasets, while maintaining high performance on standard RC tasks. Our\napproach provides a general recipe for injecting skills into large pre-trained\nLMs, whenever the skill is amenable to automatic data augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 11:14:56 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Geva", "Mor", ""], ["Gupta", "Ankit", ""], ["Berant", "Jonathan", ""]]}, {"id": "2004.04494", "submitter": "Leyang Cui", "authors": "Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, Ming Zhou", "title": "MuTual: A Dataset for Multi-Turn Dialogue Reasoning", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-task oriented dialogue systems have achieved great success in recent\nyears due to largely accessible conversation data and the development of deep\nlearning techniques. Given a context, current systems are able to yield a\nrelevant and fluent response, but sometimes make logical mistakes because of\nweak reasoning capabilities. To facilitate the conversation reasoning research,\nwe introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning,\nconsisting of 8,860 manually annotated dialogues based on Chinese student\nEnglish listening comprehension exams. Compared to previous benchmarks for\nnon-task oriented dialogue systems, MuTual is much more challenging since it\nrequires a model that can handle various reasoning problems. Empirical results\nshow that state-of-the-art methods only reach 71%, which is far behind the\nhuman performance of 94%, indicating that there is ample room for improving\nreasoning ability. MuTual is available at https://github.com/Nealcly/MuTual.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 11:42:33 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Cui", "Leyang", ""], ["Wu", "Yu", ""], ["Liu", "Shujie", ""], ["Zhang", "Yue", ""], ["Zhou", "Ming", ""]]}, {"id": "2004.04498", "submitter": "Danielle Saunders", "authors": "Danielle Saunders and Bill Byrne", "title": "Reducing Gender Bias in Neural Machine Translation as a Domain\n  Adaptation Problem", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training data for NLP tasks often exhibits gender bias in that fewer\nsentences refer to women than to men. In Neural Machine Translation (NMT)\ngender bias has been shown to reduce translation quality, particularly when the\ntarget language has grammatical gender. The recent WinoMT challenge set allows\nus to measure this effect directly (Stanovsky et al, 2019).\n  Ideally we would reduce system bias by simply debiasing all data prior to\ntraining, but achieving this effectively is itself a challenge. Rather than\nattempt to create a `balanced' dataset, we use transfer learning on a small set\nof trusted, gender-balanced examples. This approach gives strong and consistent\nimprovements in gender debiasing with much less computational cost than\ntraining from scratch.\n  A known pitfall of transfer learning on new domains is `catastrophic\nforgetting', which we address both in adaptation and in inference. During\nadaptation we show that Elastic Weight Consolidation allows a performance\ntrade-off between general translation quality and bias reduction. During\ninference we propose a lattice-rescoring scheme which outperforms all systems\nevaluated in Stanovsky et al (2019) on WinoMT with no degradation of general\ntest set BLEU, and we show this scheme can be applied to remove gender bias in\nthe output of `black box` online commercial MT systems. We demonstrate our\napproach translating from English into three languages with varied linguistic\nproperties and data availability.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 11:55:13 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 10:33:16 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 14:20:10 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Saunders", "Danielle", ""], ["Byrne", "Bill", ""]]}, {"id": "2004.04507", "submitter": "Haipeng Sun", "authors": "Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, and\n  Tiejun Zhao", "title": "Self-Training for Unsupervised Neural Machine Translation in Unbalanced\n  Training Data Scenarios", "comments": "Accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised neural machine translation (UNMT) that relies solely on massive\nmonolingual corpora has achieved remarkable results in several translation\ntasks. However, in real-world scenarios, massive monolingual corpora do not\nexist for some extremely low-resource languages such as Estonian, and UNMT\nsystems usually perform poorly when there is not adequate training corpus for\none language. In this paper, we first define and analyze the unbalanced\ntraining data scenario for UNMT. Based on this scenario, we propose UNMT\nself-training mechanisms to train a robust UNMT system and improve its\nperformance in this case. Experimental results on several language pairs show\nthat the proposed methods substantially outperform conventional UNMT systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 12:07:17 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 01:41:38 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sun", "Haipeng", ""], ["Wang", "Rui", ""], ["Chen", "Kehai", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""], ["Zhao", "Tiejun", ""]]}, {"id": "2004.04564", "submitter": "Oshin Agarwal", "authors": "Oshin Agarwal, Yinfei Yang, Byron C. Wallace, Ani Nenkova", "title": "Interpretability Analysis for Named Entity Recognition to Understand\n  System Predictions and How They Can Improve", "comments": "Computational Linguistics Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition systems achieve remarkable performance on domains\nsuch as English news. It is natural to ask: What are these models actually\nlearning to achieve this? Are they merely memorizing the names themselves? Or\nare they capable of interpreting the text and inferring the correct entity type\nfrom the linguistic context? We examine these questions by contrasting the\nperformance of several variants of LSTM-CRF architectures for named entity\nrecognition, with some provided only representations of the context as\nfeatures. We also perform similar experiments for BERT. We find that context\nrepresentations do contribute to system performance, but that the main factor\ndriving high performance is learning the name tokens themselves. We enlist\nhuman annotators to evaluate the feasibility of inferring entity types from the\ncontext alone and find that, while people are not able to infer the entity type\neither for the majority of the errors made by the context-only system, there is\nsome room for improvement. A system should be able to recognize any name in a\npredictive context correctly and our experiments indicate that current systems\nmay be further improved by such capability.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 14:37:12 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 16:15:13 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Agarwal", "Oshin", ""], ["Yang", "Yinfei", ""], ["Wallace", "Byron C.", ""], ["Nenkova", "Ani", ""]]}, {"id": "2004.04596", "submitter": "Berry De Bruijn", "authors": "Dave Carter, Marta Stojanovic, Philip Hachey, Kevin Fournier, Simon\n  Rodier, Yunli Wang, Berry de Bruijn", "title": "Global Public Health Surveillance using Media Reports: Redesigning GPHIN", "comments": "5 pages, 1 figure. To be published in \"Ebook Series: Studies in\n  Health Technology and Informatics -- Proceedings of Medical Informatics\n  Europe 2020 (MIE 2020) -- IOS Press\"", "journal-ref": null, "doi": "10.3233/SHTI200280", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global public health surveillance relies on reporting structures and\ntransmission of trustworthy health reports. But in practice, these processes\nmay not always be fast enough, or are hindered by procedural, technical, or\npolitical barriers. GPHIN, the Global Public Health Intelligence Network, was\ndesigned in the late 1990s to scour mainstream news for health events, as that\ntravels faster and more freely. This paper outlines the next generation of\nGPHIN, which went live in 2017, and reports on design decisions underpinning\nits new functions and innovations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 15:34:51 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Carter", "Dave", ""], ["Stojanovic", "Marta", ""], ["Hachey", "Philip", ""], ["Fournier", "Kevin", ""], ["Rodier", "Simon", ""], ["Wang", "Yunli", ""], ["de Bruijn", "Berry", ""]]}, {"id": "2004.04598", "submitter": "Bennett Kleinberg", "authors": "Bennett Kleinberg and Paul McFarlane", "title": "Violent music vs violence and music: Drill rap and violent crime in\n  London", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current policy of removing drill music videos from social media platforms\nsuch as YouTube remains controversial because it risks conflating the\nco-occurrence of drill rap and violence with a causal chain of the two.\nEmpirically, we revisit the question of whether there is evidence to support\nthe conjecture that drill music and gang violence are linked. We provide new\nempirical insights suggesting that: i) drill music lyrics have not become more\nnegative over time if anything they have become more positive; ii) individual\ndrill artists have similar sentiment trajectories to other artists in the drill\ngenre, and iii) there is no meaningful relationship between drill music and\nreal-life violence when compared to three kinds of police-recorded violent\ncrime data in London. We suggest ideas for new work that can help build a\nmuch-needed evidence base around the problem.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 15:35:26 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Kleinberg", "Bennett", ""], ["McFarlane", "Paul", ""]]}, {"id": "2004.04696", "submitter": "Thibault Sellam", "authors": "Thibault Sellam, Dipanjan Das, Ankur P. Parikh", "title": "BLEURT: Learning Robust Metrics for Text Generation", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generation has made significant advances in the last few years. Yet,\nevaluation metrics have lagged behind, as the most popular choices (e.g., BLEU\nand ROUGE) may correlate poorly with human judgments. We propose BLEURT, a\nlearned evaluation metric based on BERT that can model human judgments with a\nfew thousand possibly biased training examples. A key aspect of our approach is\na novel pre-training scheme that uses millions of synthetic examples to help\nthe model generalize. BLEURT provides state-of-the-art results on the last\nthree years of the WMT Metrics shared task and the WebNLG Competition dataset.\nIn contrast to a vanilla BERT-based approach, it yields superior results even\nwhen the training data is scarce and out-of-distribution.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:26:52 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 17:55:15 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 16:05:48 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 17:08:18 GMT"}, {"version": "v5", "created": "Thu, 21 May 2020 16:53:47 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Sellam", "Thibault", ""], ["Das", "Dipanjan", ""], ["Parikh", "Ankur P.", ""]]}, {"id": "2004.04721", "submitter": "Mikel Artetxe", "authors": "Mikel Artetxe, Gorka Labaka, Eneko Agirre", "title": "Translation Artifacts in Cross-lingual Transfer Learning", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both human and machine translation play a central role in cross-lingual\ntransfer learning: many multilingual datasets have been created through\nprofessional translation services, and using machine translation to translate\neither the test set or the training set is a widely used transfer technique. In\nthis paper, we show that such translation process can introduce subtle\nartifacts that have a notable impact in existing cross-lingual models. For\ninstance, in natural language inference, translating the premise and the\nhypothesis independently can reduce the lexical overlap between them, which\ncurrent models are highly sensitive to. We show that some previous findings in\ncross-lingual transfer learning need to be reconsidered in the light of this\nphenomenon. Based on the gained insights, we also improve the state-of-the-art\nin XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:54:30 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 16:06:53 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 00:42:41 GMT"}, {"version": "v4", "created": "Mon, 14 Dec 2020 22:26:49 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Artetxe", "Mikel", ""], ["Labaka", "Gorka", ""], ["Agirre", "Eneko", ""]]}, {"id": "2004.04722", "submitter": "Paul Van Eecke", "authors": "Paul Van Eecke (1 and 2), Katrien Beuls (1) ((1) Artificial\n  Intelligence Laboratory, Vrije Universiteit Brussel, Brussels, Belgium, (2)\n  ITEC, imec research group at KU Leuven, Kortrijk, Belgium)", "title": "Re-conceptualising the Language Game Paradigm in the Framework of\n  Multi-Agent Reinforcement Learning", "comments": "This paper was accepted for presentation at the 2020 AAAI Spring\n  Symposium `Challenges and Opportunities for Multi-Agent Reinforcement\n  Learning' after a double-blind reviewing process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formulate the challenge of re-conceptualising the language\ngame experimental paradigm in the framework of multi-agent reinforcement\nlearning (MARL). If successful, future language game experiments will benefit\nfrom the rapid and promising methodological advances in the MARL community,\nwhile future MARL experiments on learning emergent communication will benefit\nfrom the insights and results gained from language game experiments. We\nstrongly believe that this cross-pollination has the potential to lead to major\nbreakthroughs in the modelling of how human-like languages can emerge and\nevolve in multi-agent systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:55:15 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Van Eecke", "Paul", "", "1 and 2"], ["Beuls", "Katrien", ""]]}, {"id": "2004.04733", "submitter": "Denny Vrande\\v{c}i\\'c", "authors": "Denny Vrande\\v{c}i\\'c", "title": "Architecture for a multilingual Wikipedia", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia's vision is a world in which everyone can share in the sum of all\nknowledge. In its first two decades, this vision has been very unevenly\nachieved. One of the largest hindrances is the sheer number of languages\nWikipedia needs to cover in order to achieve that goal. We argue that we need a\nnew approach to tackle this problem more effectively, a multilingual Wikipedia\nwhere content can be shared between language editions. This paper proposes an\narchitecture for a system that fulfills this goal. It separates the goal in two\nparts: creating and maintaining content in an abstract notation within a\nproject called Abstract Wikipedia, and creating an infrastructure called\nWikilambda that can translate this notation to natural language. Both parts are\nfully owned and maintained by the community, as is the integration of the\nresults in the existing Wikipedia editions. This architecture will make more\nencyclopedic content available to more people in their own language, and at the\nsame time allow more people to contribute knowledge and reach more people with\ntheir contributions, no matter what their respective language backgrounds.\nAdditionally, Wikilambda will unlock a new type of knowledge asset people can\nshare in through the Wikimedia projects, functions, which will vastly expand\nwhat people can do with knowledge from Wikimedia, and provide a new venue to\ncollaborate and to engage the creativity of contributors from all around the\nworld. These two projects will considerably expand the capabilities of the\nWikimedia platform to enable every single human being to freely share in the\nsum of all knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 22:25:10 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Vrande\u010di\u0107", "Denny", ""]]}, {"id": "2004.04768", "submitter": "Zhibo Yang", "authors": "Jianyuan Deng, Zhibo Yang, Yao Li, Dimitris Samaras, Fusheng Wang", "title": "Towards Better Opioid Antagonists Using Deep Reinforcement Learning", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naloxone, an opioid antagonist, has been widely used to save lives from\nopioid overdose, a leading cause for death in the opioid epidemic. However,\nnaloxone has short brain retention ability, which limits its therapeutic\nefficacy. Developing better opioid antagonists is critical in combating the\nopioid epidemic.Instead of exhaustively searching in a huge chemical space for\nbetter opioid antagonists, we adopt reinforcement learning which allows\nefficient gradient-based search towards molecules with desired physicochemical\nand/or biological properties. Specifically, we implement a deep reinforcement\nlearning framework to discover potential lead compounds as better opioid\nantagonists with enhanced brain retention ability. A customized multi-objective\nreward function is designed to bias the generation towards molecules with both\nsufficient opioid antagonistic effect and enhanced brain retention ability.\nThorough evaluation demonstrates that with this framework, we are able to\nidentify valid, novel and feasible molecules with multiple desired properties,\nwhich has high potential in drug discovery.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 15:28:50 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Deng", "Jianyuan", ""], ["Yang", "Zhibo", ""], ["Li", "Yao", ""], ["Samaras", "Dimitris", ""], ["Wang", "Fusheng", ""]]}, {"id": "2004.04803", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Jack Rueter and Mika H\\\"am\\\"al\\\"ainen", "title": "FST Morphology for the Endangered Skolt Sami Language", "comments": "Accepted to The 1st Joint SLTU and CCURL Workshop (SLTU-CCURL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present advances in the development of a FST-based morphological analyzer\nand generator for Skolt Sami. Like other minority Uralic languages, Skolt Sami\nexhibits a rich morphology, on the one hand, and there is little golden\nstandard material for it, on the other. This makes NLP approaches for its study\ndifficult without a solid morphological analysis. The language is severely\nendangered and the work presented in this paper forms a part of a greater whole\nin its revitalization efforts. Furthermore, we intersperse our description with\nfacilitation and description practices not well documented in the\ninfrastructure. Currently, the analyzer covers over 30,000 Skolt Sami words in\n148 inflectional paradigms and over 12 derivational forms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 20:47:15 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Rueter", "Jack", ""], ["H\u00e4m\u00e4l\u00e4inen", "Mika", ""]]}, {"id": "2004.04849", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, Tushar Khot, Ashish Sabharwal", "title": "More Bang for Your Buck: Natural Perturbation for Robust Question\n  Answering", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent models have achieved human-level scores on many NLP datasets, we\nobserve that they are considerably sensitive to small changes in input. As an\nalternative to the standard approach of addressing this issue by constructing\ntraining sets of completely new examples, we propose doing so via minimal\nperturbation of examples. Specifically, our approach involves first collecting\na set of seed examples and then applying human-driven natural perturbations (as\nopposed to rule-based machine perturbations), which often change the gold label\nas well. Local perturbations have the advantage of being relatively easier (and\nhence cheaper) to create than writing out completely new examples. To evaluate\nthe impact of this phenomenon, we consider a recent question-answering dataset\n(BoolQ) and study the benefit of our approach as a function of the perturbation\ncost ratio, the relative cost of perturbing an existing question vs. creating a\nnew one from scratch. We find that when natural perturbations are moderately\ncheaper to create, it is more effective to train models using them: such models\nexhibit higher robustness and better generalization, while retaining\nperformance on the original BoolQ dataset.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 23:12:39 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 07:10:00 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Khashabi", "Daniel", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""]]}, {"id": "2004.04877", "submitter": "Nathaniel Weir", "authors": "Nathaniel Weir, Adam Poliak, Benjamin Van Durme", "title": "Probing Neural Language Models for Human Tacit Assumptions", "comments": "To be published in CogSci 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans carry stereotypic tacit assumptions (STAs) (Prince, 1978), or\npropositional beliefs about generic concepts. Such associations are crucial for\nunderstanding natural language. We construct a diagnostic set of word\nprediction prompts to evaluate whether recent neural contextualized language\nmodels trained on large text corpora capture STAs. Our prompts are based on\nhuman responses in a psychological study of conceptual associations. We find\nmodels to be profoundly effective at retrieving concepts given associated\nproperties. Our results demonstrate empirical evidence that stereotypic\nconceptual representations are captured in neural models derived from\nsemi-supervised linguistic exposure.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 01:48:50 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 15:55:51 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Weir", "Nathaniel", ""], ["Poliak", "Adam", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2004.04902", "submitter": "Rohan Jagtap", "authors": "Rohan Jagtap, Dr. Sudhir N. Dhage", "title": "An In-depth Walkthrough on Evolution of Neural Machine Translation", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) methodologies have burgeoned from using\nsimple feed-forward architectures to the state of the art; viz. BERT model. The\nuse cases of NMT models have been broadened from just language translations to\nconversational agents (chatbots), abstractive text summarization, image\ncaptioning, etc. which have proved to be a gem in their respective\napplications. This paper aims to study the major trends in Neural Machine\nTranslation, the state of the art models in the domain and a high level\ncomparison between them.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 04:21:05 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Jagtap", "Rohan", ""], ["Dhage", "Dr. Sudhir N.", ""]]}, {"id": "2004.04906", "submitter": "Wen-Tau Yih", "authors": "Vladimir Karpukhin, Barlas O\\u{g}uz, Sewon Min, Patrick Lewis, Ledell\n  Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain question answering relies on efficient passage retrieval to\nselect candidate contexts, where traditional sparse vector space models, such\nas TF-IDF or BM25, are the de facto method. In this work, we show that\nretrieval can be practically implemented using dense representations alone,\nwhere embeddings are learned from a small number of questions and passages by a\nsimple dual-encoder framework. When evaluated on a wide range of open-domain QA\ndatasets, our dense retriever outperforms a strong Lucene-BM25 system largely\nby 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our\nend-to-end QA system establish new state-of-the-art on multiple open-domain QA\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 04:53:17 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 00:53:53 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 21:27:13 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Karpukhin", "Vladimir", ""], ["O\u011fuz", "Barlas", ""], ["Min", "Sewon", ""], ["Lewis", "Patrick", ""], ["Wu", "Ledell", ""], ["Edunov", "Sergey", ""], ["Chen", "Danqi", ""], ["Yih", "Wen-tau", ""]]}, {"id": "2004.04908", "submitter": "Tianyu Zhao", "authors": "Tianyu Zhao, Divesh Lala, Tatsuya Kawahara", "title": "Designing Precise and Robust Dialogue Response Evaluators", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic dialogue response evaluator has been proposed as an alternative to\nautomated metrics and human evaluation. However, existing automatic evaluators\nachieve only moderate correlation with human judgement and they are not robust.\nIn this work, we propose to build a reference-free evaluator and exploit the\npower of semi-supervised training and pretrained (masked) language models.\nExperimental results demonstrate that the proposed evaluator achieves a strong\ncorrelation (> 0.6) with human judgement and generalizes robustly to diverse\nresponses and corpora. We open-source the code and data in\nhttps://github.com/ZHAOTING/dialog-processing.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 04:59:37 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 04:01:55 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Zhao", "Tianyu", ""], ["Lala", "Divesh", ""], ["Kawahara", "Tatsuya", ""]]}, {"id": "2004.04917", "submitter": "Mahdi Abavisani", "authors": "Mahdi Abavisani and Liwei Wu and Shengli Hu and Joel Tetreault and\n  Alejandro Jaimes", "title": "Multimodal Categorization of Crisis Events in Social Media", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR 2020)", "journal-ref": "Conference on Computer Vision and Pattern Recognition (CVPR 2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent developments in image classification and natural language processing,\ncoupled with the rapid growth in social media usage, have enabled fundamental\nadvances in detecting breaking events around the world in real-time. Emergency\nresponse is one such area that stands to gain from these advances. By\nprocessing billions of texts and images a minute, events can be automatically\ndetected to enable emergency response workers to better assess rapidly evolving\nsituations and deploy resources accordingly. To date, most event detection\ntechniques in this area have focused on image-only or text-only approaches,\nlimiting detection performance and impacting the quality of information\ndelivered to crisis response teams. In this paper, we present a new multimodal\nfusion method that leverages both images and texts as input. In particular, we\nintroduce a cross-attention module that can filter uninformative and misleading\ncomponents from weak modalities on a sample by sample basis. In addition, we\nemploy a multimodal graph-based approach to stochastically transition between\nembeddings of different multimodal pairs during training to better regularize\nthe learning process as well as dealing with limited training data by\nconstructing new matched pairs from different samples. We show that our method\noutperforms the unimodal approaches and strong multimodal baselines by a large\nmargin on three crisis-related tasks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 06:31:30 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Abavisani", "Mahdi", ""], ["Wu", "Liwei", ""], ["Hu", "Shengli", ""], ["Tetreault", "Joel", ""], ["Jaimes", "Alejandro", ""]]}, {"id": "2004.04934", "submitter": "Alistair Conkie", "authors": "Alistair Conkie, Andrew Finch", "title": "Scalable Multilingual Frontend for TTS", "comments": "To appear in IEEE ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes progress towards making a Neural Text-to-Speech (TTS)\nFrontend that works for many languages and can be easily extended to new\nlanguages. We take a Machine Translation (MT) inspired approach to constructing\nthe frontend, and model both text normalization and pronunciation on a sentence\nlevel by building and using sequence-to-sequence (S2S) models. We experimented\nwith training normalization and pronunciation as separate S2S models and with\ntraining a single S2S model combining both functions.\n  For our language-independent approach to pronunciation we do not use a\nlexicon. Instead all pronunciations, including context-based pronunciations,\nare captured in the S2S model. We also present a language-independent chunking\nand splicing technique that allows us to process arbitrary-length sentences.\nModels for 18 languages were trained and evaluated. Many of the accuracy\nmeasurements are above 99%. We also evaluated the models in the context of\nend-to-end synthesis against our current production system.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 08:00:40 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Conkie", "Alistair", ""], ["Finch", "Andrew", ""]]}, {"id": "2004.04938", "submitter": "Yufei Tian", "authors": "Yufei Tian, Tuhin Chakrabarty, Fred Morstatter and Nanyun Peng", "title": "Identifying Distributional Perspective Differences from Colingual Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perspective differences exist among different cultures or languages. A lack\nof mutual understanding among different groups about their perspectives on\nspecific values or events may lead to uninformed decisions or biased opinions.\nAutomatically understanding the group perspectives can provide essential\nbackground for many downstream applications of natural language processing\ntechniques. In this paper, we study colingual groups and use language corpora\nas a proxy to identify their distributional perspectives. We present a novel\ncomputational approach to learn shared understandings, and benchmark our method\nby building culturally-aware models for the English, Chinese, and Japanese\nlanguages. On a held out set of diverse topics including marriage, corruption,\ndemocracy, our model achieves high correlation with human judgements regarding\nintra-group values and inter-group differences.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 08:13:07 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 19:11:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Tian", "Yufei", ""], ["Chakrabarty", "Tuhin", ""], ["Morstatter", "Fred", ""], ["Peng", "Nanyun", ""]]}, {"id": "2004.04945", "submitter": "Silviu Oprea", "authors": "Silviu Vlad Oprea, Walid Magdy", "title": "The Effect of Sociocultural Variables on Sarcasm Communication Online", "comments": "Accepted as a full paper at CSCW 2020. Please cite the CSCW version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networks (OSN) play an essential role for connecting people and\nallowing them to communicate online. OSN users share their thoughts, moments,\nand news with their network. The messages they share online can include\nsarcastic posts, where the intended meaning expressed by the written text is\ndifferent from the literal one. This could result in miscommunication. Previous\nresearch in psycholinguistics has studied the sociocultural factors the might\nlead to sarcasm misunderstanding between speakers and listeners. However, there\nis a lack of such studies in the context of OSN. In this paper we fill this gap\nby performing a quantitative analysis on the influence of sociocultural\nvariables, including gender, age, country, and English language nativeness, on\nthe effectiveness of sarcastic communication online. We collect examples of\nsarcastic tweets directly from the authors who posted them. Further, we ask\nthird-party annotators of different sociocultural backgrounds to label these\ntweets for sarcasm. Our analysis indicates that age, English language\nnativeness, and country are significantly influential and should be considered\nin the design of future social analysis tools that either study sarcasm\ndirectly, or look at related phenomena where sarcasm may have an influence. We\nalso make observations about the social ecology surrounding sarcastic exchanges\non OSNs. We conclude by suggesting ways in which our findings can be included\nin future work.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 08:30:50 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Oprea", "Silviu Vlad", ""], ["Magdy", "Walid", ""]]}, {"id": "2004.04972", "submitter": "Alistair Conkie", "authors": "Soumi Maiti, Erik Marchi, Alistair Conkie", "title": "Generating Multilingual Voices Using Speaker Space Translation Based on\n  Bilingual Speaker Data", "comments": "Accepted to IEEE ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present progress towards bilingual Text-to-Speech which is able to\ntransform a monolingual voice to speak a second language while preserving\nspeaker voice quality. We demonstrate that a bilingual speaker embedding space\ncontains a separate distribution for each language and that a simple transform\nin speaker space generated by the speaker embedding can be used to control the\ndegree of accent of a synthetic voice in a language. The same transform can be\napplied even to monolingual speakers.\n  In our experiments speaker data from an English-Spanish (Mexican) bilingual\nspeaker was used, and the goal was to enable English speakers to speak Spanish\nand Spanish speakers to speak English. We found that the simple transform was\nsufficient to convert a voice from one language to the other with a high degree\nof naturalness. In one case the transformed voice outperformed a native\nlanguage voice in listening tests. Experiments further indicated that the\ntransform preserved many of the characteristics of the original voice. The\ndegree of accent present can be controlled and naturalness is relatively\nconsistent across a range of accent values.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:01:53 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Maiti", "Soumi", ""], ["Marchi", "Erik", ""], ["Conkie", "Alistair", ""]]}, {"id": "2004.04980", "submitter": "Anastasia Funkner", "authors": "Anastasia Funkner, Ksenia Balabaeva, Sergey Kovalchuk", "title": "Negation Detection for Clinical Text Mining in Russian", "comments": "5 pages, 1 figure, 3 tables, accepted for the conference MIE 2020", "journal-ref": null, "doi": "10.3233/SHTI200179", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing predictive modeling in medicine requires additional features from\nunstructured clinical texts. In Russia, there are no instruments for natural\nlanguage processing to cope with problems of medical records. This paper is\ndevoted to a module of negation detection. The corpus-free machine learning\nmethod is based on gradient boosting classifier is used to detect whether a\ndisease is denied, not mentioned or presented in the text. The detector\nclassifies negations for five diseases and shows average F-score from 0.81 to\n0.93. The benefits of negation detection have been demonstrated by predicting\nthe presence of surgery for patients with the acute coronary syndrome.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:38:33 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Funkner", "Anastasia", ""], ["Balabaeva", "Ksenia", ""], ["Kovalchuk", "Sergey", ""]]}, {"id": "2004.04987", "submitter": "Ksenia Balabaeva", "authors": "Ksenia Balabaeva, Anastasia Funkner, Sergey Kovalchuk", "title": "Automated Spelling Correction for Clinical Text Mining in Russian", "comments": "This paper is accepted for publication to MIE 2020 Conference", "journal-ref": null, "doi": "10.3233/SHTI200119", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this paper is to develop a spell checker module for clinical\ntext in Russian. The described approach combines string distance measure\nalgorithms with technics of machine learning embedding methods. Our overall\nprecision is 0.86, lexical precision - 0.975 and error precision is 0.74. We\ndevelop spell checker as a part of medical text mining tool regarding the\nproblems of misspelling, negation, experiencer and temporality detection.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:59:44 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Balabaeva", "Ksenia", ""], ["Funkner", "Anastasia", ""], ["Kovalchuk", "Sergey", ""]]}, {"id": "2004.05001", "submitter": "Ivan P Yamshchikov", "authors": "Ivan P. Yamshchikov, Viacheslav Shibaev, Nikolay Khlebnikov, Alexey\n  Tikhonov", "title": "Style-transfer and Paraphrase: Looking for a Sensible Semantic\n  Similarity Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of such natural language processing tasks as style\ntransfer, paraphrase, and machine translation often calls for the use of\nsemantic similarity metrics. In recent years a lot of methods to measure the\nsemantic similarity of two short texts were developed. This paper provides a\ncomprehensive analysis for more than a dozen of such methods. Using a new\ndataset of fourteen thousand sentence pairs human-labeled according to their\nsemantic similarity, we demonstrate that none of the metrics widely used in the\nliterature is close enough to human judgment in these tasks. A number of\nrecently proposed metrics provide comparable results, yet Word Mover Distance\nis shown to be the most reasonable solution to measure semantic similarity in\nreformulated texts at the moment.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 11:52:06 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 14:10:24 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 21:58:57 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Yamshchikov", "Ivan P.", ""], ["Shibaev", "Viacheslav", ""], ["Khlebnikov", "Nikolay", ""], ["Tikhonov", "Alexey", ""]]}, {"id": "2004.05009", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Yashesh Gaur, Liang Lu, Jinyu Li, Yifan Gong", "title": "Minimum Latency Training Strategies for Streaming Sequence-to-Sequence\n  ASR", "comments": "Accepted at IEEE ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a few novel streaming attention-based sequence-to-sequence (S2S)\nmodels have been proposed to perform online speech recognition with linear-time\ndecoding complexity. However, in these models, the decisions to generate tokens\nare delayed compared to the actual acoustic boundaries since their\nunidirectional encoders lack future information. This leads to an inevitable\nlatency during inference. To alleviate this issue and reduce latency, we\npropose several strategies during training by leveraging external hard\nalignments extracted from the hybrid model. We investigate to utilize the\nalignments in both the encoder and the decoder. On the encoder side, (1)\nmulti-task learning and (2) pre-training with the framewise classification task\nare studied. On the decoder side, we (3) remove inappropriate alignment paths\nbeyond an acceptable latency during the alignment marginalization, and (4)\ndirectly minimize the differentiable expected latency loss. Experiments on the\nCortana voice search task demonstrate that our proposed methods can\nsignificantly reduce the latency, and even improve the recognition accuracy in\ncertain cases on the decoder side. We also present some analysis to understand\nthe behaviors of streaming S2S models.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 12:24:49 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 00:21:37 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Gaur", "Yashesh", ""], ["Lu", "Liang", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "2004.05051", "submitter": "Simran Khanuja", "authors": "Simran Khanuja, Sandipan Dandapat, Sunayana Sitaram, Monojit Choudhury", "title": "A New Dataset for Natural Language Inference from Code-mixed\n  Conversations", "comments": "To appear in CALCS, LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Inference (NLI) is the task of inferring the logical\nrelationship, typically entailment or contradiction, between a premise and\nhypothesis. Code-mixing is the use of more than one language in the same\nconversation or utterance, and is prevalent in multilingual communities all\nover the world. In this paper, we present the first dataset for code-mixed NLI,\nin which both the premises and hypotheses are in code-mixed Hindi-English. We\nuse data from Hindi movies (Bollywood) as premises, and crowd-source hypotheses\nfrom Hindi-English bilinguals. We conduct a pilot annotation study and describe\nthe final annotation protocol based on observations from the pilot. Currently,\nthe data collected consists of 400 premises in the form of code-mixed\nconversation snippets and 2240 code-mixed hypotheses. We conduct an extensive\nanalysis to infer the linguistic phenomena commonly observed in the dataset\nobtained. We evaluate the dataset using a standard mBERT-based pipeline for NLI\nand report results.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 14:32:01 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 04:34:52 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Khanuja", "Simran", ""], ["Dandapat", "Sandipan", ""], ["Sitaram", "Sunayana", ""], ["Choudhury", "Monojit", ""]]}, {"id": "2004.05067", "submitter": "Jordan Kodner", "authors": "Jordan Kodner, Nitish Gupta", "title": "Overestimation of Syntactic Representationin Neural Language Models", "comments": "Accepted for publication at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of powerful neural language models over the last few years,\nresearch attention has increasingly focused on what aspects of language they\nrepresent that make them so successful. Several testing methodologies have been\ndeveloped to probe models' syntactic representations. One popular method for\ndetermining a model's ability to induce syntactic structure trains a model on\nstrings generated according to a template then tests the model's ability to\ndistinguish such strings from superficially similar ones with different syntax.\nWe illustrate a fundamental problem with this approach by reproducing positive\nresults from a recent paper with two non-syntactic baseline language models: an\nn-gram model and an LSTM model trained on scrambled inputs.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 15:13:03 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Kodner", "Jordan", ""], ["Gupta", "Nitish", ""]]}, {"id": "2004.05080", "submitter": "Jiaqi Li", "authors": "Jiaqi Li, Ming Liu, Min-Yen Kan, Zihao Zheng, Zekun Wang, Wenqiang\n  Lei, Ting Liu and Bing Qin", "title": "Molweni: A Challenge Multiparty Dialogues-based Machine Reading\n  Comprehension Dataset with Discourse Structure", "comments": "Accepted by COLING 2020, long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into the area of multiparty dialog has grown considerably over\nrecent years. We present the Molweni dataset, a machine reading comprehension\n(MRC) dataset with discourse structure built over multiparty dialog. Molweni's\nsource samples from the Ubuntu Chat Corpus, including 10,000 dialogs comprising\n88,303 utterances. We annotate 30,066 questions on this corpus, including both\nanswerable and unanswerable questions. Molweni also uniquely contributes\ndiscourse dependency annotations in a modified Segmented Discourse\nRepresentation Theory (SDRT; Asher et al., 2016) style for all of its\nmultiparty dialogs, contributing large-scale (78,245 annotated discourse\nrelations) data to bear on the task of multiparty dialog discourse parsing. Our\nexperiments show that Molweni is a challenging dataset for current MRC models:\nBERT-wwm, a current, strong SQuAD 2.0 performer, achieves only 67.7% F1 on\nMolweni's questions, a 20+% significant drop as compared against its SQuAD 2.0\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 15:52:08 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 10:39:42 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 08:03:58 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Jiaqi", ""], ["Liu", "Ming", ""], ["Kan", "Min-Yen", ""], ["Zheng", "Zihao", ""], ["Wang", "Zekun", ""], ["Lei", "Wenqiang", ""], ["Liu", "Ting", ""], ["Qin", "Bing", ""]]}, {"id": "2004.05109", "submitter": "Shlok Mishra", "authors": "Shlok Kumar Mishra, Pranav Goel, Abhishek Sharma, Abhyuday Jagannatha,\n  David Jacobs, Hal Daum\\'e III", "title": "Towards Automatic Generation of Questions from Long Answers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic question generation (AQG) has broad applicability in domains such\nas tutoring systems, conversational agents, healthcare literacy, and\ninformation retrieval. Existing efforts at AQG have been limited to short\nanswer lengths of up to two or three sentences. However, several real-world\napplications require question generation from answers that span several\nsentences. Therefore, we propose a novel evaluation benchmark to assess the\nperformance of existing AQG systems for long-text answers. We leverage the\nlarge-scale open-source Google Natural Questions dataset to create the\naforementioned long-answer AQG benchmark. We empirically demonstrate that the\nperformance of existing AQG methods significantly degrades as the length of the\nanswer increases. Transformer-based methods outperform other existing AQG\nmethods on long answers in terms of automatic as well as human evaluation.\nHowever, we still observe degradation in the performance of our best performing\nmodels with increasing sentence length, suggesting that long answer QA is a\nchallenging benchmark task for future research.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:45:08 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 16:34:55 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 17:57:04 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Mishra", "Shlok Kumar", ""], ["Goel", "Pranav", ""], ["Sharma", "Abhishek", ""], ["Jagannatha", "Abhyuday", ""], ["Jacobs", "David", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "2004.05119", "submitter": "Siddhant Garg", "authors": "Siddhant Garg, Rohit Kumar Sharma, Yingyu Liang", "title": "Beyond Fine-tuning: Few-Sample Sentence Embedding Transfer", "comments": "Accepted at AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-tuning (FT) pre-trained sentence embedding models on small datasets has\nbeen shown to have limitations. In this paper we show that concatenating the\nembeddings from the pre-trained model with those from a simple sentence\nembedding model trained only on the target data, can improve over the\nperformance of FT for few-sample tasks. To this end, a linear classifier is\ntrained on the combined embeddings, either by freezing the embedding model\nweights or training the classifier and embedding models end-to-end. We perform\nevaluation on seven small datasets from NLP tasks and show that our approach\nwith end-to-end training outperforms FT with negligible computational overhead.\nFurther, we also show that sophisticated combination techniques like CCA and\nKCCA do not work as well in practice as concatenation. We provide theoretical\nanalysis to explain this empirical observation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:57:06 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 16:57:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Garg", "Siddhant", ""], ["Sharma", "Rohit Kumar", ""], ["Liang", "Yingyu", ""]]}, {"id": "2004.05125", "submitter": "Jimmy Lin", "authors": "Edwin Zhang, Nikhil Gupta, Rodrigo Nogueira, Kyunghyun Cho, and Jimmy\n  Lin", "title": "Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research\n  Dataset: Preliminary Thoughts and Lessons Learned", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Neural Covidex, a search engine that exploits the latest\nneural ranking architectures to provide information access to the COVID-19 Open\nResearch Dataset curated by the Allen Institute for AI. This web application\nexists as part of a suite of tools that we have developed over the past few\nweeks to help domain experts tackle the ongoing global pandemic. We hope that\nimproved information access capabilities to the scientific literature can\ninform evidence-based decision making and insight generation. This paper\ndescribes our initial efforts and offers a few thoughts about lessons we have\nlearned along the way.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 17:12:29 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhang", "Edwin", ""], ["Gupta", "Nikhil", ""], ["Nogueira", "Rodrigo", ""], ["Cho", "Kyunghyun", ""], ["Lin", "Jimmy", ""]]}, {"id": "2004.05140", "submitter": "Keunwoo Yu", "authors": "Keunwoo Peter Yu and Yi Yang", "title": "One Model to Recognize Them All: Marginal Distillation from NER Models\n  with Different Tag Sets", "comments": "9 pages, LaTeX; column header of Table 2 corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is a fundamental component in the modern\nlanguage understanding pipeline. Public NER resources such as annotated data\nand model services are available in many domains. However, given a particular\ndownstream application, there is often no single NER resource that supports all\nthe desired entity types, so users must leverage multiple resources with\ndifferent tag sets. This paper presents a marginal distillation (MARDI)\napproach for training a unified NER model from resources with disjoint or\nheterogeneous tag sets. In contrast to recent works, MARDI merely requires\naccess to pre-trained models rather than the original training datasets. This\nflexibility makes it easier to work with sensitive domains like healthcare and\nfinance. Furthermore, our approach is general enough to integrate with\ndifferent NER architectures, including local models (e.g., BiLSTM) and global\nmodels (e.g., CRF). Experiments on two benchmark datasets show that MARDI\nperforms on par with a strong marginal CRF baseline, while being more flexible\nin the form of required NER resources. MARDI also sets a new state of the art\non the progressive NER task. MARDI significantly outperforms the\nstart-of-the-art model on the task of progressive NER.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 17:36:27 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 13:55:24 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Yu", "Keunwoo Peter", ""], ["Yang", "Yi", ""]]}, {"id": "2004.05150", "submitter": "Iz Beltagy", "authors": "Iz Beltagy and Matthew E. Peters and Arman Cohan", "title": "Longformer: The Long-Document Transformer", "comments": "Version 2 introduces the Longformer-Encoder-Decoder (LED) model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based models are unable to process long sequences due to their\nself-attention operation, which scales quadratically with the sequence length.\nTo address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process\ndocuments of thousands of tokens or longer. Longformer's attention mechanism is\na drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work\non long-sequence transformers, we evaluate Longformer on character-level\nlanguage modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a\nvariety of downstream tasks. Our pretrained Longformer consistently outperforms\nRoBERTa on long document tasks and sets new state-of-the-art results on WikiHop\nand TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a\nLongformer variant for supporting long document generative sequence-to-sequence\ntasks, and demonstrate its effectiveness on the arXiv summarization dataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 17:54:09 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 17:52:35 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Beltagy", "Iz", ""], ["Peters", "Matthew E.", ""], ["Cohan", "Arman", ""]]}, {"id": "2004.05160", "submitter": "Jind\\v{r}ich Libovick\\'y", "authors": "Jind\\v{r}ich Libovick\\'y, Rudolf Rosa, Alexander Fraser", "title": "On the Language Neutrality of Pre-trained Multilingual Representations", "comments": "12 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1911.03310. Accepted to Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual contextual embeddings, such as multilingual BERT and\nXLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work\nprobed the cross-linguality of the representations indirectly using zero-shot\ntransfer learning on morphological and syntactic tasks. We instead investigate\nthe language-neutrality of multilingual contextual embeddings directly and with\nrespect to lexical semantics. Our results show that contextual embeddings are\nmore language-neutral and, in general, more informative than aligned static\nword-type embeddings, which are explicitly trained for language neutrality.\nContextual embeddings are still only moderately language-neutral by default, so\nwe propose two simple methods for achieving stronger language neutrality:\nfirst, by unsupervised centering of the representation for each language and\nsecond, by fitting an explicit projection on small parallel data. Besides, we\nshow how to reach state-of-the-art accuracy on language identification and\nmatch the performance of statistical methods for word alignment of parallel\nsentences without using parallel data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 19:50:32 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 11:44:10 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 16:10:07 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 18:48:19 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Libovick\u00fd", "Jind\u0159ich", ""], ["Rosa", "Rudolf", ""], ["Fraser", "Alexander", ""]]}, {"id": "2004.05219", "submitter": "Georgiana Dinu", "authors": "Georgiana Dinu, Prashant Mathur, Marcello Federico, Stanislas Lauly,\n  Yaser Al-Onaizan", "title": "Joint translation and unit conversion for end-to-end localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of natural language tasks require processing of textual data which\ncontains a mix of natural language and formal languages such as mathematical\nexpressions. In this paper, we take unit conversions as an example and propose\na data augmentation technique which leads to models learning both translation\nand conversion tasks as well as how to adequately switch between them for\nend-to-end localization.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 20:18:43 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Dinu", "Georgiana", ""], ["Mathur", "Prashant", ""], ["Federico", "Marcello", ""], ["Lauly", "Stanislas", ""], ["Al-Onaizan", "Yaser", ""]]}, {"id": "2004.05274", "submitter": "Yu-An Chung", "authors": "Yu-An Chung, James Glass", "title": "Improved Speech Representations with Multi-Target Autoregressive\n  Predictive Coding", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training objectives based on predictive coding have recently been shown to be\nvery effective at learning meaningful representations from unlabeled speech.\nOne example is Autoregressive Predictive Coding (Chung et al., 2019), which\ntrains an autoregressive RNN to generate an unseen future frame given a context\nsuch as recent past frames. The basic hypothesis of these approaches is that\nhidden states that can accurately predict future frames are a useful\nrepresentation for many downstream tasks. In this paper we extend this\nhypothesis and aim to enrich the information encoded in the hidden states by\ntraining the model to make more accurate future predictions. We propose an\nauxiliary objective that serves as a regularization to improve generalization\nof the future frame prediction task. Experimental results on phonetic\nclassification, speech recognition, and speech translation not only support the\nhypothesis, but also demonstrate the effectiveness of our approach in learning\nrepresentations that contain richer phonetic content.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 01:09:36 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Chung", "Yu-An", ""], ["Glass", "James", ""]]}, {"id": "2004.05323", "submitter": "Paria Jamshid Lou", "authors": "Paria Jamshid Lou, Mark Johnson", "title": "Improving Disfluency Detection by Self-Training a Self-Attentive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attentive neural syntactic parsers using contextualized word embeddings\n(e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing\nand disfluency detection in speech transcripts. Since the contextualized word\nembeddings are pre-trained on a large amount of unlabeled data, using\nadditional unlabeled data to train a neural model might seem redundant.\nHowever, we show that self-training - a semi-supervised technique for\nincorporating unlabeled data - sets a new state-of-the-art for the\nself-attentive parser on disfluency detection, demonstrating that self-training\nprovides benefits orthogonal to the pre-trained contextualized word\nrepresentations. We also show that ensembling self-trained parsers provides\nfurther gains for disfluency detection.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 06:53:08 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 06:44:14 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Lou", "Paria Jamshid", ""], ["Johnson", "Mark", ""]]}, {"id": "2004.05328", "submitter": "Javad PourMostafa Roshan Sharami", "authors": "Javad PourMostafa Roshan Sharami, Parsa Abbasi Sarabestani, Seyed\n  Abolghasem Mirroshandel", "title": "DeepSentiPers: Novel Deep Learning Models Trained Over Proposed\n  Augmented Persian Sentiment Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on how to extract opinions over each Persian\nsentence-level text. Deep learning models provided a new way to boost the\nquality of the output. However, these architectures need to feed on big\nannotated data as well as an accurate design. To best of our knowledge, we do\nnot merely suffer from lack of well-annotated Persian sentiment corpus, but\nalso a novel model to classify the Persian opinions in terms of both multiple\nand binary classification. So in this work, first we propose two novel deep\nlearning architectures comprises of bidirectional LSTM and CNN. They are a part\nof a deep hierarchy designed precisely and also able to classify sentences in\nboth cases. Second, we suggested three data augmentation techniques for the\nlow-resources Persian sentiment corpus. Our comprehensive experiments on three\nbaselines and two different neural word embedding methods show that our data\naugmentation methods and intended models successfully address the aims of the\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 07:45:35 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Sharami", "Javad PourMostafa Roshan", ""], ["Sarabestani", "Parsa Abbasi", ""], ["Mirroshandel", "Seyed Abolghasem", ""]]}, {"id": "2004.05388", "submitter": "Qian Liu", "authors": "Qian Liu, Yihong Chen, Bei Chen, Jian-Guang Lou, Zixuan Chen, Bin\n  Zhou, Dongmei Zhang", "title": "You Impress Me: Dialogue Generation via Mutual Persona Perception", "comments": "Accepted by ACL 2020, code is avaiable at\n  https://github.com/SivilTaram/Persona-Dialogue-Generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the continuing efforts to improve the engagingness and consistency of\nchit-chat dialogue systems, the majority of current work simply focus on\nmimicking human-like responses, leaving understudied the aspects of modeling\nunderstanding between interlocutors. The research in cognitive science,\ninstead, suggests that understanding is an essential signal for a high-quality\nchit-chat conversation. Motivated by this, we propose P^2 Bot, a\ntransmitter-receiver based framework with the aim of explicitly modeling\nunderstanding. Specifically, P^2 Bot incorporates mutual persona perception to\nenhance the quality of personalized dialogue generation. Experiments on a large\npublic dataset, Persona-Chat, demonstrate the effectiveness of our approach,\nwith a considerable boost over the state-of-the-art baselines across both\nautomatic metrics and human evaluations.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 12:51:07 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Liu", "Qian", ""], ["Chen", "Yihong", ""], ["Chen", "Bei", ""], ["Lou", "Jian-Guang", ""], ["Chen", "Zixuan", ""], ["Zhou", "Bin", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2004.05438", "submitter": "Kevin Lybarger", "authors": "Kevin Lybarger, Mari Ostendorf, Meliha Yetisgen", "title": "Annotating Social Determinants of Health Using Active Learning, and\n  Characterizing Determinants Using Neural Event Extraction", "comments": null, "journal-ref": "Journal of Biomedical Informatics 113 (2021) 103631", "doi": "10.1016/j.jbi.2020.103631", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social determinants of health (SDOH) affect health outcomes, and knowledge of\nSDOH can inform clinical decision-making. Automatically extracting SDOH\ninformation from clinical text requires data-driven information extraction\nmodels trained on annotated corpora that are heterogeneous and frequently\ninclude critical SDOH. This work presents a new corpus with SDOH annotations, a\nnovel active learning framework, and the first extraction results on the new\ncorpus. The Social History Annotation Corpus (SHAC) includes 4,480 social\nhistory sections with detailed annotation for 12 SDOH characterizing the\nstatus, extent, and temporal information of 18K distinct events. We introduce a\nnovel active learning framework that selects samples for annotation using a\nsurrogate text classification task as a proxy for a more complex event\nextraction task. The active learning framework successfully increases the\nfrequency of health risk factors and improves automatic extraction of these\nevents over undirected annotation. An event extraction model trained on SHAC\nachieves high extraction performance for substance use status (0.82-0.93 F1),\nemployment status (0.81-0.86 F1), and living status type (0.81-0.93 F1) on data\nfrom three institutions.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 16:19:02 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 05:54:50 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lybarger", "Kevin", ""], ["Ostendorf", "Mari", ""], ["Yetisgen", "Meliha", ""]]}, {"id": "2004.05456", "submitter": "Yijiang Liu", "authors": "Yijiang Liu, Meishan Zhang, Donghong Ji", "title": "End to End Chinese Lexical Fusion Recognition with Sememe Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Chinese lexical fusion recognition, a new task\nwhich could be regarded as one kind of coreference recognition. First, we\nintroduce the task in detail, showing the relationship with coreference\nrecognition and differences from the existing tasks. Second, we propose an\nend-to-end joint model for the task, which exploits the state-of-the-art BERT\nrepresentations as encoder, and is further enhanced with the sememe knowledge\nfrom HowNet by graph attention networks. We manually annotate a benchmark\ndataset for the task and then conduct experiments on it. Results demonstrate\nthat our joint model is effective and competitive for the task. Detailed\nanalysis is offered for comprehensively understanding the new task and our\nproposed model.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 18:17:18 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Liu", "Yijiang", ""], ["Zhang", "Meishan", ""], ["Ji", "Donghong", ""]]}, {"id": "2004.05476", "submitter": "Maite Taboada", "authors": "Varada Kolhatkar, Nithum Thain, Jeffrey Sorensen, Lucas Dixon and\n  Maite Taboada", "title": "Classifying Constructive Comments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the Constructive Comments Corpus (C3), comprised of 12,000\nannotated news comments, intended to help build new tools for online\ncommunities to improve the quality of their discussions. We define constructive\ncomments as high-quality comments that make a contribution to the conversation.\nWe explain the crowd worker annotation scheme and define a taxonomy of\nsub-characteristics of constructiveness. The quality of the annotation scheme\nand the resulting dataset is evaluated using measurements of inter-annotator\nagreement, expert assessment of a sample, and by the constructiveness\nsub-characteristics, which we show provide a proxy for the general\nconstructiveness concept. We provide models for constructiveness trained on C3\nusing both feature-based and a variety of deep learning approaches and\ndemonstrate that these models capture general rather than topic- or\ndomain-specific characteristics of constructiveness, through domain adaptation\nexperiments. We examine the role that length plays in our models, as comment\nlength could be easily gamed if models depend heavily upon this feature. By\nexamining the errors made by each model and their distribution by length, we\nshow that the best performing models are less correlated with comment\nlength.The constructiveness corpus and our experiments pave the way for a\nmoderation tool focused on promoting comments that make a contribution, rather\nthan only filtering out undesirable content.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 20:05:52 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 22:23:15 GMT"}, {"version": "v3", "created": "Sat, 25 Jul 2020 04:28:42 GMT"}, {"version": "v4", "created": "Wed, 5 Aug 2020 03:14:04 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Kolhatkar", "Varada", ""], ["Thain", "Nithum", ""], ["Sorensen", "Jeffrey", ""], ["Dixon", "Lucas", ""], ["Taboada", "Maite", ""]]}, {"id": "2004.05483", "submitter": "Vered Shwartz", "authors": "Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and\n  Yejin Choi", "title": "Unsupervised Commonsense Question Answering with Self-Talk", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language understanding involves reading between the lines with\nimplicit background knowledge. Current systems either rely on pre-trained\nlanguage models as the sole implicit source of world knowledge, or resort to\nexternal knowledge bases (KBs) to incorporate additional relevant knowledge. We\npropose an unsupervised framework based on self-talk as a novel alternative to\nmultiple-choice commonsense tasks. Inspired by inquiry-based discovery learning\n(Bruner, 1961), our approach inquires language models with a number of\ninformation seeking questions such as \"$\\textit{what is the definition of\n...}$\" to discover additional background knowledge. Empirical results\ndemonstrate that the self-talk procedure substantially improves the performance\nof zero-shot language model baselines on four out of six commonsense\nbenchmarks, and competes with models that obtain knowledge from external KBs.\nWhile our approach improves performance on several benchmarks, the self-talk\ninduced knowledge even when leading to correct answers is not always seen as\nuseful by human judges, raising interesting questions about the inner-workings\nof pre-trained language models for commonsense reasoning.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 20:43:37 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 18:55:05 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Shwartz", "Vered", ""], ["West", "Peter", ""], ["Bras", "Ronan Le", ""], ["Bhagavatula", "Chandra", ""], ["Choi", "Yejin", ""]]}, {"id": "2004.05484", "submitter": "Rami Al-Rfou", "authors": "Uma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips,\n  Yinfei Yang", "title": "LAReQA: Language-agnostic answer retrieval from a multilingual pool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LAReQA, a challenging new benchmark for language-agnostic answer\nretrieval from a multilingual candidate pool. Unlike previous cross-lingual\ntasks, LAReQA tests for \"strong\" cross-lingual alignment, requiring\nsemantically related cross-language pairs to be closer in representation space\nthan unrelated same-language pairs. Building on multilingual BERT (mBERT), we\nstudy different strategies for achieving strong alignment. We find that\naugmenting training data via machine translation is effective, and improves\nsignificantly over using mBERT out-of-the-box. Interestingly, the embedding\nbaseline that performs the best on LAReQA falls short of competing baselines on\nzero-shot variants of our task that only target \"weak\" alignment. This finding\nunderscores our claim that languageagnostic retrieval is a substantively new\nkind of cross-lingual evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 20:51:11 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Roy", "Uma", ""], ["Constant", "Noah", ""], ["Al-Rfou", "Rami", ""], ["Barua", "Aditya", ""], ["Phillips", "Aaron", ""], ["Yang", "Yinfei", ""]]}, {"id": "2004.05516", "submitter": "Kelly Marchisio", "authors": "Kelly Marchisio, Kevin Duh, and Philipp Koehn", "title": "When Does Unsupervised Machine Translation Work?", "comments": "WMT20 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the reported success of unsupervised machine translation (MT), the\nfield has yet to examine the conditions under which these methods succeed, and\nwhere they fail. We conduct an extensive empirical evaluation of unsupervised\nMT using dissimilar language pairs, dissimilar domains, diverse datasets, and\nauthentic low-resource languages. We find that performance rapidly deteriorates\nwhen source and target corpora are from different domains, and that random word\nembedding initialization can dramatically affect downstream translation\nperformance. We additionally find that unsupervised MT performance declines\nwhen source and target languages use different scripts, and observe very poor\nperformance on authentic low-resource language pairs. We advocate for extensive\nempirical evaluation of unsupervised MT systems to highlight failure points and\nencourage continued research on the most promising paradigms.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 00:57:47 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 16:12:43 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 02:48:08 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Marchisio", "Kelly", ""], ["Duh", "Kevin", ""], ["Koehn", "Philipp", ""]]}, {"id": "2004.05568", "submitter": "Shangwen  Lv", "authors": "Shangwen Lv, Yuechen Wang, Daya Guo, Duyu Tang, Nan Duan, Fuqing Zhu,\n  Ming Gong, Linjun Shou, Ryan Ma, Daxin Jiang, Guihong Cao, Ming Zhou, Songlin\n  Hu", "title": "Pre-training Text Representations as Meta Learning", "comments": "2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training text representations has recently been shown to significantly\nimprove the state-of-the-art in many natural language processing tasks. The\ncentral goal of pre-training is to learn text representations that are useful\nfor subsequent tasks. However, existing approaches are optimized by minimizing\na proxy objective, such as the negative log likelihood of language modeling. In\nthis work, we introduce a learning algorithm which directly optimizes model's\nability to learn text representations for effective learning of downstream\ntasks. We show that there is an intrinsic connection between multi-task\npre-training and model-agnostic meta-learning with a sequence of meta-train\nsteps. The standard multi-task learning objective adopted in BERT is a special\ncase of our learning algorithm where the depth of meta-train is zero. We study\nthe problem in two settings: unsupervised pre-training and supervised\npre-training with different pre-training objects to verify the generality of\nour approach.Experimental results show that our algorithm brings improvements\nand learns better initializations for a variety of downstream tasks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 09:05:47 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Lv", "Shangwen", ""], ["Wang", "Yuechen", ""], ["Guo", "Daya", ""], ["Tang", "Duyu", ""], ["Duan", "Nan", ""], ["Zhu", "Fuqing", ""], ["Gong", "Ming", ""], ["Shou", "Linjun", ""], ["Ma", "Ryan", ""], ["Jiang", "Daxin", ""], ["Cao", "Guihong", ""], ["Zhou", "Ming", ""], ["Hu", "Songlin", ""]]}, {"id": "2004.05569", "submitter": "Veronica Latcinnik", "authors": "Veronica Latcinnik, Jonathan Berant", "title": "Explaining Question Answering Models through Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained language models (LMs) have been shown to perform\nsurprisingly well when fine-tuned on tasks that require commonsense and world\nknowledge. However, in end-to-end architectures, it is difficult to explain\nwhat is the knowledge in the LM that allows it to make a correct prediction. In\nthis work, we propose a model for multi-choice question answering, where a\nLM-based generator generates a textual hypothesis that is later used by a\nclassifier to answer the question. The hypothesis provides a window into the\ninformation used by the fine-tuned LM that can be inspected by humans. A key\nchallenge in this setup is how to constrain the model to generate hypotheses\nthat are meaningful to humans. We tackle this by (a) joint training with a\nsimple similarity classifier that encourages meaningful hypotheses, and (b) by\nadding loss functions that encourage natural text without repetitions. We show\non several tasks that our model reaches performance that is comparable to\nend-to-end architectures, while producing hypotheses that elucidate the\nknowledge used by the LM for answering the question.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 09:06:46 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Latcinnik", "Veronica", ""], ["Berant", "Jonathan", ""]]}, {"id": "2004.05572", "submitter": "Deng Cai", "authors": "Deng Cai and Wai Lam", "title": "AMR Parsing via Graph-Sequence Iterative Inference", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new end-to-end model that treats AMR parsing as a series of dual\ndecisions on the input sequence and the incrementally constructed graph. At\neach time step, our model performs multiple rounds of attention, reasoning, and\ncomposition that aim to answer two critical questions: (1) which part of the\ninput \\textit{sequence} to abstract; and (2) where in the output \\textit{graph}\nto construct the new concept. We show that the answers to these two questions\nare mutually causalities. We design a model based on iterative inference that\nhelps achieve better answers in both perspectives, leading to greatly improved\nparsing accuracy. Our experimental results significantly outperform all\npreviously reported \\textsc{Smatch} scores by large margins. Remarkably,\nwithout the help of any large-scale pre-trained language model (e.g., BERT),\nour model already surpasses previous state-of-the-art using BERT. With the help\nof BERT, we can push the state-of-the-art results to 80.2\\% on LDC2017T10 (AMR\n2.0) and 75.4\\% on LDC2014T12 (AMR 1.0).\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 09:15:21 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 04:01:44 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Cai", "Deng", ""], ["Lam", "Wai", ""]]}, {"id": "2004.05686", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Ahmed Awadallah", "title": "XtremeDistil: Multi-stage Distillation for Massive Multilingual Models", "comments": "To appear in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep and large pre-trained language models are the state-of-the-art for\nvarious natural language processing tasks. However, the huge size of these\nmodels could be a deterrent to use them in practice. Some recent and concurrent\nworks use knowledge distillation to compress these huge models into shallow\nones. In this work we study knowledge distillation with a focus on\nmulti-lingual Named Entity Recognition (NER). In particular, we study several\ndistillation strategies and propose a stage-wise optimization scheme leveraging\nteacher internal representations that is agnostic of teacher architecture and\nshow that it outperforms strategies employed in prior works. Additionally, we\ninvestigate the role of several factors like the amount of unlabeled data,\nannotation resources, model architecture and inference latency to name a few.\nWe show that our approach leads to massive compression of MBERT-like teacher\nmodels by upto 35x in terms of parameters and 51x in terms of latency for batch\ninference while retaining 95% of its F1-score for NER over 41 languages.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 19:49:27 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 00:20:48 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Awadallah", "Ahmed", ""]]}, {"id": "2004.05704", "submitter": "Robik Shrestha", "authors": "Robik Shrestha, Kushal Kafle, Christopher Kanan", "title": "A negative case analysis of visual grounding methods for VQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Visual Question Answering (VQA) methods tend to exploit dataset\nbiases and spurious statistical correlations, instead of producing right\nanswers for the right reasons. To address this issue, recent bias mitigation\nmethods for VQA propose to incorporate visual cues (e.g., human attention maps)\nto better ground the VQA models, showcasing impressive gains. However, we show\nthat the performance improvements are not a result of improved visual\ngrounding, but a regularization effect which prevents over-fitting to\nlinguistic priors. For instance, we find that it is not actually necessary to\nprovide proper, human-based cues; random, insensible cues also result in\nsimilar improvements. Based on this observation, we propose a simpler\nregularization scheme that does not require any external annotations and yet\nachieves near state-of-the-art performance on VQA-CPv2.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 21:45:23 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 17:38:04 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Shrestha", "Robik", ""], ["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "2004.05707", "submitter": "Zhibin Lu", "authors": "Zhibin Lu, Pan Du, Jian-Yun Nie", "title": "VGCN-BERT: Augmenting BERT with Graph Embedding for Text Classification", "comments": "12 pages, 2 figures", "journal-ref": "in J. M. Jose et al. (Eds.): ECIR 2020, LNCS 12035, pp.369-382,\n  2020", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much progress has been made recently on text classification with methods\nbased on neural networks. In particular, models using attention mechanism such\nas BERT have shown to have the capability of capturing the contextual\ninformation within a sentence or document. However, their ability of capturing\nthe global information about the vocabulary of a language is more limited. This\nlatter is the strength of Graph Convolutional Networks (GCN). In this paper, we\npropose VGCN-BERT model which combines the capability of BERT with a Vocabulary\nGraph Convolutional Network (VGCN). Local information and global information\ninteract through different layers of BERT, allowing them to influence mutually\nand to build together a final representation for classification. In our\nexperiments on several text classification datasets, our approach outperforms\nBERT and GCN alone, and achieve higher effectiveness than that reported in\nprevious studies.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 22:02:33 GMT"}], "update_date": "2020-06-14", "authors_parsed": [["Lu", "Zhibin", ""], ["Du", "Pan", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "2004.05744", "submitter": "DongHyun Choi", "authors": "DongHyun Choi and IlNam Park and Myeong Cheol Shin and EungGyun Kim\n  and Dong Ryeol Shin", "title": "Integrated Eojeol Embedding for Erroneous Sentence Classification in\n  Korean Chatbots", "comments": "9 pages, 2 figures", "journal-ref": "IEEE Access, 2021", "doi": "10.1109/ACCESS.2021.3085864", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts to analyze the Korean sentence classification system for\na chatbot. Sentence classification is the task of classifying an input sentence\nbased on predefined categories. However, spelling or space error contained in\nthe input sentence causes problems in morphological analysis and tokenization.\nThis paper proposes a novel approach of Integrated Eojeol (Korean syntactic\nword separated by space) Embedding to reduce the effect that poorly analyzed\nmorphemes may make on sentence classification. It also proposes two noise\ninsertion methods that further improve classification performance. Our\nevaluation results indicate that the proposed system classifies erroneous\nsentences more accurately than the baseline system by 17%p.0\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 02:11:19 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Choi", "DongHyun", ""], ["Park", "IlNam", ""], ["Shin", "Myeong Cheol", ""], ["Kim", "EungGyun", ""], ["Shin", "Dong Ryeol", ""]]}, {"id": "2004.05755", "submitter": "Yufei Tian", "authors": "Yufei Tian, Jianfei Yu, Jing Jiang", "title": "Aspect and Opinion Aware Abstractive Review Summarization with\n  Reinforced Hard Typed Decoder", "comments": null, "journal-ref": null, "doi": "10.1145/3357384.3358142", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study abstractive review summarization.Observing that\nreview summaries often consist of aspect words, opinion words and context\nwords, we propose a two-stage reinforcement learning approach, which first\npredicts the output word type from the three types, and then leverages the\npredicted word type to generate the final word distribution.Experimental\nresults on two Amazon product review datasets demonstrate that our method can\nconsistently outperform several strong baseline approaches based on ROUGE\nscores.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 03:35:29 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Tian", "Yufei", ""], ["Yu", "Jianfei", ""], ["Jiang", "Jing", ""]]}, {"id": "2004.05757", "submitter": "Mingjun Zhao", "authors": "Mingjun Zhao, Haijiang Wu, Di Niu and Xiaoli Wang", "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation\n  Models", "comments": "Accepted as full paper by AAAI-2020 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The competitive performance of neural machine translation (NMT) critically\nrelies on large amounts of training data. However, acquiring high-quality\ntranslation pairs requires expert knowledge and is costly. Therefore, how to\nbest utilize a given dataset of samples with diverse quality and\ncharacteristics becomes an important yet understudied question in NMT.\nCurriculum learning methods have been introduced to NMT to optimize a model's\nperformance by prescribing the data input order, based on heuristics such as\nthe assessment of noise and difficulty levels. However, existing methods\nrequire training from scratch, while in practice most NMT models are\npre-trained on big data already. Moreover, as heuristics, they do not\ngeneralize well. In this paper, we aim to learn a curriculum for improving a\npre-trained NMT model by re-selecting influential data samples from the\noriginal training set and formulate this task as a reinforcement learning\nproblem. Specifically, we propose a data selection framework based on\nDeterministic Actor-Critic, in which a critic network predicts the expected\nchange of model performance due to a certain sample, while an actor network\nlearns to select the best sample out of a random batch of samples presented to\nit. Experiments on several translation datasets show that our method can\nfurther improve the performance of NMT when original batch training reaches its\nceiling, without using additional new training data, and significantly\noutperforms several strong baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 03:40:44 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhao", "Mingjun", ""], ["Wu", "Haijiang", ""], ["Niu", "Di", ""], ["Wang", "Xiaoli", ""]]}, {"id": "2004.05773", "submitter": "Isabelle Augenstein", "authors": "Pepa Atanasova and Jakob Grue Simonsen and Christina Lioma and\n  Isabelle Augenstein", "title": "Generating Fact Checking Explanations", "comments": "In Proceedings of the 2020 Annual Conference of the Association for\n  Computational Linguistics (ACL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing work on automated fact checking is concerned with predicting\nthe veracity of claims based on metadata, social network spread, language used\nin claims, and, more recently, evidence supporting or denying claims. A crucial\npiece of the puzzle that is still missing is to understand how to automate the\nmost elaborate part of the process -- generating justifications for verdicts on\nclaims. This paper provides the first study of how these explanations can be\ngenerated automatically based on available claim context, and how this task can\nbe modelled jointly with veracity prediction. Our results indicate that\noptimising both objectives at the same time, rather than training them\nseparately, improves the performance of a fact checking system. The results of\na manual evaluation further suggest that the informativeness, coverage and\noverall quality of the generated explanations are also improved in the\nmulti-task model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 05:23:25 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Atanasova", "Pepa", ""], ["Simonsen", "Jakob Grue", ""], ["Lioma", "Christina", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2004.05801", "submitter": "Chinnadhurai Sankar", "authors": "Chinnadhurai Sankar, Sujith Ravi, Zornitsa Kozareva", "title": "ProFormer: Towards On-Device LSH Projection Based Transformers", "comments": "EACL 2021 - BEST PAPER AWARD, Honorable Mention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the heart of text based neural models lay word representations, which are\npowerful but occupy a lot of memory making it challenging to deploy to devices\nwith memory constraints such as mobile phones, watches and IoT. To surmount\nthese challenges, we introduce ProFormer -- a projection based transformer\narchitecture that is faster and lighter making it suitable to deploy to memory\nconstraint devices and preserve user privacy. We use LSH projection layer to\ndynamically generate word representations on-the-fly without embedding lookup\ntables leading to significant memory footprint reduction from O(V.d) to O(T),\nwhere V is the vocabulary size, d is the embedding dimension size and T is the\ndimension of the LSH projection representation.\n  We also propose a local projection attention (LPA) layer, which uses\nself-attention to transform the input sequence of N LSH word projections into a\nsequence of N/K representations reducing the computations quadratically by\nO(K^2). We evaluate ProFormer on multiple text classification tasks and\nobserved improvements over prior state-of-the-art on-device approaches for\nshort text classification and comparable performance for long text\nclassification tasks. In comparison with a 2-layer BERT model, ProFormer\nreduced the embedding memory footprint from 92.16 MB to 1.3 KB and requires 16\ntimes less computation overhead, which is very impressive making it the fastest\nand smallest on-device model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 07:31:31 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 00:27:50 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sankar", "Chinnadhurai", ""], ["Ravi", "Sujith", ""], ["Kozareva", "Zornitsa", ""]]}, {"id": "2004.05808", "submitter": "Zhen Ke", "authors": "Zhen Ke, Liang Shi, Erli Meng, Bin Wang, Xipeng Qiu, Xuanjing Huang", "title": "Unified Multi-Criteria Chinese Word Segmentation with BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Criteria Chinese Word Segmentation (MCCWS) aims at finding word\nboundaries in a Chinese sentence composed of continuous characters while\nmultiple segmentation criteria exist. The unified framework has been widely\nused in MCCWS and shows its effectiveness. Besides, the pre-trained BERT\nlanguage model has been also introduced into the MCCWS task in a multi-task\nlearning framework. In this paper, we combine the superiority of the unified\nframework and pretrained language model, and propose a unified MCCWS model\nbased on BERT. Moreover, we augment the unified BERT-based MCCWS model with the\nbigram features and an auxiliary criterion classification task. Experiments on\neight datasets with diverse criteria demonstrate that our methods could achieve\nnew state-of-the-art results for MCCWS.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 07:50:04 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ke", "Zhen", ""], ["Shi", "Liang", ""], ["Meng", "Erli", ""], ["Wang", "Bin", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2004.05809", "submitter": "Jiajun Zhang", "authors": "Jiajun Zhang and Chengqing Zong", "title": "Neural Machine Translation: Challenges, Progress and Future", "comments": "Invited Review of Science China Technological Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation (MT) is a technique that leverages computers to translate\nhuman languages automatically. Nowadays, neural machine translation (NMT) which\nmodels direct mapping between source and target languages with deep neural\nnetworks has achieved a big breakthrough in translation performance and become\nthe de facto paradigm of MT. This article makes a review of NMT framework,\ndiscusses the challenges in NMT, introduces some exciting recent progresses and\nfinally looks forward to some potential future research trends. In addition, we\nmaintain the state-of-the-art methods for various NMT tasks at the website\nhttps://github.com/ZNLP/SOTA-MT.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 07:53:57 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "2004.05812", "submitter": "Shuangyong Song", "authors": "Shuangyong Song, Chao Wang, Qianqian Xie, Xinxing Zu, Huan Chen,\n  Haiqing Chen", "title": "MLR: A Two-stage Conversational Query Rewriting Model with Multi-task\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational context understanding aims to recognize the real intention of\nuser from the conversation history, which is critical for building the dialogue\nsystem. However, the multi-turn conversation understanding in open domain is\nstill quite challenging, which requires the system extracting the important\ninformation and resolving the dependencies in contexts among a variety of open\ntopics. In this paper, we propose the conversational query rewriting model -\nMLR, which is a Multi-task model on sequence Labeling and query Rewriting. MLR\nreformulates the multi-turn conversational queries into a single turn query,\nwhich conveys the true intention of users concisely and alleviates the\ndifficulty of the multi-turn dialogue modeling. In the model, we formulate the\nquery rewriting as a sequence generation problem and introduce word category\ninformation via the auxiliary word category label predicting task. To train our\nmodel, we construct a new Chinese query rewriting dataset and conduct\nexperiments on it. The experimental results show that our model outperforms\ncompared models, and prove the effectiveness of the word category information\nin improving the rewriting performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 08:04:49 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Song", "Shuangyong", ""], ["Wang", "Chao", ""], ["Xie", "Qianqian", ""], ["Zu", "Xinxing", ""], ["Chen", "Huan", ""], ["Chen", "Haiqing", ""]]}, {"id": "2004.05816", "submitter": "Hyunwoo Kim", "authors": "Hyunwoo Kim, Byeongchang Kim, Gunhee Kim", "title": "Will I Sound Like Me? Improving Persona Consistency in Dialogues through\n  Pragmatic Self-Consciousness", "comments": "Accepted paper at EMNLP 2020 and ICLR 2020 BAICS workshop (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the task of improving persona consistency of dialogue agents.\nRecent models tackling consistency often train with additional Natural Language\nInference (NLI) labels or attach trained extra modules to the generative agent\nfor maintaining consistency. However, such additional labels and training can\nbe demanding. Also, we find even the best-performing persona-based agents are\ninsensitive to contradictory words. Inspired by social cognition and\npragmatics, we endow existing dialogue agents with public self-consciousness on\nthe fly through an imaginary listener. Our approach, based on the Rational\nSpeech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to\nrefrain from uttering contradiction. We further extend the framework by\nlearning the distractor selection, which has been usually done manually or\nrandomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang\net al., 2018) dataset show that our approach reduces contradiction and improves\nconsistency of existing dialogue models. Moreover, we show that it can be\ngeneralized to improve context-consistency beyond persona in dialogues.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 08:16:16 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 08:20:22 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Kim", "Hyunwoo", ""], ["Kim", "Byeongchang", ""], ["Kim", "Gunhee", ""]]}, {"id": "2004.05827", "submitter": "Shuyang Gao", "authors": "Shuyang Gao, Sanchit Agarwal, Tagyoung Chung, Di Jin, Dilek\n  Hakkani-Tur", "title": "From Machine Reading Comprehension to Dialogue State Tracking: Bridging\n  the Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue state tracking (DST) is at the heart of task-oriented dialogue\nsystems. However, the scarcity of labeled data is an obstacle to building\naccurate and robust state tracking systems that work across a variety of\ndomains. Existing approaches generally require some dialogue data with state\ninformation and their ability to generalize to unknown domains is limited. In\nthis paper, we propose using machine reading comprehension (RC) in state\ntracking from two perspectives: model architectures and datasets. We divide the\nslot types in dialogue state into categorical or extractive to borrow the\nadvantages from both multiple-choice and span-based reading comprehension\nmodels. Our method achieves near the current state-of-the-art in joint goal\naccuracy on MultiWOZ 2.1 given full training data. More importantly, by\nleveraging machine reading comprehension datasets, our method outperforms the\nexisting approaches by many a large margin in few-shot scenarios when the\navailability of in-domain data is limited. Lastly, even without any state\ntracking data, i.e., zero-shot scenario, our proposed approach achieves greater\nthan 90% average slot accuracy in 12 out of 30 slots in MultiWOZ 2.1.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 09:00:03 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Gao", "Shuyang", ""], ["Agarwal", "Sanchit", ""], ["Chung", "Tagyoung", ""], ["Jin", "Di", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2004.05861", "submitter": "Maram Hasanain", "authors": "Fatima Haouari, Maram Hasanain, Reem Suwaileh, Tamer Elsayed", "title": "ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation\n  Networks", "comments": "This work was accepted at the Sixth Arabic Natural Language\n  Processing Workshop (EACL/WANLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present ArCOV-19, an Arabic COVID-19 Twitter dataset that\nspans one year, covering the period from 27th of January 2020 till 31st of\nJanuary 2021. ArCOV-19 is the first publicly-available Arabic Twitter dataset\ncovering COVID-19 pandemic that includes about 2.7M tweets alongside the\npropagation networks of the most-popular subset of them (i.e., most-retweeted\nand -liked). The propagation networks include both retweets and conversational\nthreads (i.e., threads of replies). ArCOV-19 is designed to enable research\nunder several domains including natural language processing, information\nretrieval, and social computing. Preliminary analysis shows that ArCOV-19\ncaptures rising discussions associated with the first reported cases of the\ndisease as they appeared in the Arab world. In addition to the source tweets\nand propagation networks, we also release the search queries and\nlanguage-independent crawler used to collect the tweets to encourage the\ncuration of similar datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 10:49:53 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 07:09:53 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2020 16:14:53 GMT"}, {"version": "v4", "created": "Sat, 13 Mar 2021 23:14:06 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Haouari", "Fatima", ""], ["Hasanain", "Maram", ""], ["Suwaileh", "Reem", ""], ["Elsayed", "Tamer", ""]]}, {"id": "2004.05887", "submitter": "Maximilian Mozes", "authors": "Maximilian Mozes, Pontus Stenetorp, Bennett Kleinberg, Lewis D.\n  Griffin", "title": "Frequency-Guided Word Substitutions for Detecting Textual Adversarial\n  Examples", "comments": "EACL 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent efforts have shown that neural text processing models are vulnerable\nto adversarial examples, but the nature of these examples is poorly understood.\nIn this work, we show that adversarial attacks against CNN, LSTM and\nTransformer-based classification models perform word substitutions that are\nidentifiable through frequency differences between replaced words and their\ncorresponding substitutions. Based on these findings, we propose\nfrequency-guided word substitutions (FGWS), a simple algorithm exploiting the\nfrequency properties of adversarial word substitutions for the detection of\nadversarial examples. FGWS achieves strong performance by accurately detecting\nadversarial examples on the SST-2 and IMDb sentiment datasets, with F1\ndetection scores of up to 91.4% against RoBERTa-based classification models. We\ncompare our approach against a recently proposed perturbation discrimination\nframework and show that we outperform it by up to 13.0% F1.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 12:11:36 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 09:55:19 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Mozes", "Maximilian", ""], ["Stenetorp", "Pontus", ""], ["Kleinberg", "Bennett", ""], ["Griffin", "Lewis D.", ""]]}, {"id": "2004.05916", "submitter": "Damian Pascual", "authors": "Damian Pascual, Gino Brunner and Roger Wattenhofer", "title": "Telling BERT's full story: from Local Attention to Global Aggregation", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a deep look into the behavior of self-attention heads in the\ntransformer architecture. In light of recent work discouraging the use of\nattention distributions for explaining a model's behavior, we show that\nattention distributions can nevertheless provide insights into the local\nbehavior of attention heads. This way, we propose a distinction between local\npatterns revealed by attention and global patterns that refer back to the\ninput, and analyze BERT from both angles. We use gradient attribution to\nanalyze how the output of an attention attention head depends on the input\ntokens, effectively extending the local attention-based analysis to account for\nthe mixing of information throughout the transformer layers. We find that there\nis a significant discrepancy between attention and attribution distributions,\ncaused by the mixing of context inside the model. We quantify this discrepancy\nand observe that interestingly, there are some patterns that persist across all\nlayers despite the mixing.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 01:36:41 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 21:48:04 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Pascual", "Damian", ""], ["Brunner", "Gino", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2004.05964", "submitter": "Shusei Eshima", "authors": "Shusei Eshima, Kosuke Imai and Tomoya Sasaki", "title": "Keyword Assisted Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, fully automated content analysis based on probabilistic\ntopic models has become popular among social scientists because of their\nscalability. The unsupervised nature of the models makes them suitable for\nexploring topics in a corpus without prior knowledge. However, researchers find\nthat these models often fail to measure specific concepts of substantive\ninterest by inadvertently creating multiple topics with similar content and\ncombining distinct themes into a single topic. In this paper, we empirically\ndemonstrate that providing a small number of keywords can substantially enhance\nthe measurement performance of topic models. An important advantage of the\nproposed keyword assisted topic model (keyATM) is that the specification of\nkeywords requires researchers to label topics prior to fitting a model to the\ndata. This contrasts with a widespread practice of post-hoc topic\ninterpretation and adjustments that compromises the objectivity of empirical\nfindings. In our application, we find that keyATM provides more interpretable\nresults, has better document classification performance, and is less sensitive\nto the number of topics than the standard topic models. Finally, we show that\nkeyATM can also incorporate covariates and model time trends. An open-source\nsoftware package is available for implementing the proposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:35:28 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:24:52 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Eshima", "Shusei", ""], ["Imai", "Kosuke", ""], ["Sasaki", "Tomoya", ""]]}, {"id": "2004.05985", "submitter": "Lukasz Augustyniak", "authors": "{\\L}ukasz Augustyniak, Piotr Szymanski, Miko{\\l}aj Morzy, Piotr\n  Zelasko, Adrian Szymczak, Jan Mizgajski, Yishay Carmiel, Najim Dehak", "title": "Punctuation Prediction in Spontaneous Conversations: Can We Mitigate ASR\n  Errors with Retrofitted Word Embeddings?", "comments": "submitted to INTERSPEECH'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Speech Recognition (ASR) systems introduce word errors, which often\nconfuse punctuation prediction models, turning punctuation restoration into a\nchallenging task. These errors usually take the form of homonyms. We show how\nretrofitting of the word embeddings on the domain-specific data can mitigate\nASR errors. Our main contribution is a method for better alignment of homonym\nembeddings and the validation of the presented method on the punctuation\nprediction task. We record the absolute improvement in punctuation prediction\naccuracy between 6.2% (for question marks) to 9% (for periods) when compared\nwith the state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:02:28 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Augustyniak", "\u0141ukasz", ""], ["Szymanski", "Piotr", ""], ["Morzy", "Miko\u0142aj", ""], ["Zelasko", "Piotr", ""], ["Szymczak", "Adrian", ""], ["Mizgajski", "Jan", ""], ["Carmiel", "Yishay", ""], ["Dehak", "Najim", ""]]}, {"id": "2004.05986", "submitter": "Liang  Xu", "authors": "Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen\n  Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\n  Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina\n  Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng\n  Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson and Zhenzhong\n  Lan", "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark", "comments": "Accepted by COLING2020; 10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The advent of natural language understanding (NLU) benchmarks for English,\nsuch as GLUE and SuperGLUE allows new NLU models to be evaluated across a\ndiverse set of tasks. These comprehensive benchmarks have facilitated a broad\nrange of research and applications in natural language processing (NLP). The\nproblem, however, is that most such benchmarks are limited to English, which\nhas made it difficult to replicate many of the successes in English NLU for\nother languages. To help remedy this issue, we introduce the first large-scale\nChinese Language Understanding Evaluation (CLUE) benchmark. CLUE is an\nopen-ended, community-driven project that brings together 9 tasks spanning\nseveral well-established single-sentence/sentence-pair classification tasks, as\nwell as machine reading comprehension, all on original Chinese text. To\nestablish results on these tasks, we report scores using an exhaustive set of\ncurrent state-of-the-art pre-trained Chinese models (9 in total). We also\nintroduce a number of supplementary datasets and additional tools to help\nfacilitate further progress on Chinese NLU. Our benchmark is released at\nhttps://www.CLUEbenchmarks.com\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:02:29 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 11:41:07 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 14:46:45 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Xu", "Liang", ""], ["Hu", "Hai", ""], ["Zhang", "Xuanwei", ""], ["Li", "Lu", ""], ["Cao", "Chenjie", ""], ["Li", "Yudong", ""], ["Xu", "Yechen", ""], ["Sun", "Kai", ""], ["Yu", "Dian", ""], ["Yu", "Cong", ""], ["Tian", "Yin", ""], ["Dong", "Qianqian", ""], ["Liu", "Weitang", ""], ["Shi", "Bo", ""], ["Cui", "Yiming", ""], ["Li", "Junyi", ""], ["Zeng", "Jun", ""], ["Wang", "Rongzhao", ""], ["Xie", "Weijian", ""], ["Li", "Yanting", ""], ["Patterson", "Yina", ""], ["Tian", "Zuoyu", ""], ["Zhang", "Yiwen", ""], ["Zhou", "He", ""], ["Liu", "Shaoweihua", ""], ["Zhao", "Zhe", ""], ["Zhao", "Qipeng", ""], ["Yue", "Cong", ""], ["Zhang", "Xinrui", ""], ["Yang", "Zhengliang", ""], ["Richardson", "Kyle", ""], ["Lan", "Zhenzhong", ""]]}, {"id": "2004.05989", "submitter": "Bahman Mirheidari", "authors": "Bahman Mirheidari, Yilin Pan, Daniel Blackburn, Ronan O'Malley, Traci\n  Walker, Annalena Venneri, Markus Reuber, Heidi Christensen", "title": "Data augmentation using generative networks to identify dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data limitation is one of the most common issues in training machine learning\nclassifiers for medical applications. Due to ethical concerns and data privacy,\nthe number of people that can be recruited to such experiments is generally\nsmaller than the number of participants contributing to non-healthcare\ndatasets. Recent research showed that generative models can be used as an\neffective approach for data augmentation, which can ultimately help to train\nmore robust classifiers sparse data domains. A number of studies proved that\nthis data augmentation technique works for image and audio data sets. In this\npaper, we investigate the application of a similar approach to different types\nof speech and audio-based features extracted from interactions recorded with\nour automatic dementia detection system. Using two generative models we show\nhow the generated synthesized samples can improve the performance of a DNN\nbased classifier. The variational autoencoder increased the F-score of a\nfour-way classifier distinguishing the typical patient groups seen in memory\nclinics from 58% to around 74%, a 16% improvement\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:05:24 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Mirheidari", "Bahman", ""], ["Pan", "Yilin", ""], ["Blackburn", "Daniel", ""], ["O'Malley", "Ronan", ""], ["Walker", "Traci", ""], ["Venneri", "Annalena", ""], ["Reuber", "Markus", ""], ["Christensen", "Heidi", ""]]}, {"id": "2004.05991", "submitter": "Pratik Jawanpuria", "authors": "Pratik Jawanpuria, Mayank Meghwanshi, Bamdev Mishra", "title": "A Simple Approach to Learning Unsupervised Multilingual Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on unsupervised learning of cross-lingual embeddings in\nbilingual setting has given impetus to learning a shared embedding space for\nseveral languages without any supervision. A popular framework to solve the\nlatter problem is to jointly solve the following two sub-problems: 1) learning\nunsupervised word alignment between several pairs of languages, and 2) learning\nhow to map the monolingual embeddings of every language to a shared\nmultilingual space. In contrast, we propose a simple, two-stage framework in\nwhich we decouple the above two sub-problems and solve them separately using\nexisting techniques. The proposed approach obtains surprisingly good\nperformance in various tasks such as bilingual lexicon induction, cross-lingual\nword similarity, multilingual document classification, and multilingual\ndependency parsing. When distant languages are involved, the proposed solution\nillustrates robustness and outperforms existing unsupervised multilingual word\nembedding approaches. Overall, our experimental results encourage development\nof multi-stage models for such challenging problems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 05:54:10 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 15:17:01 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Jawanpuria", "Pratik", ""], ["Meghwanshi", "Mayank", ""], ["Mishra", "Bamdev", ""]]}, {"id": "2004.06015", "submitter": "Yu Chen", "authors": "Yu Chen, Lingfei Wu and Mohammed J. Zaki", "title": "Toward Subgraph Guided Knowledge Graph Question Generation with Graph\n  Neural Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph (KG) question generation (QG) aims to generate natural\nlanguage questions from KGs and target answers. Previous works mostly focus on\na simple setting which is to generate questions from a single KG triple. In\nthis work, we focus on a more realistic setting where we aim to generate\nquestions from a KG subgraph and target answers. In addition, most of previous\nworks built on either RNN-based or Transformer-based models to encode a\nlinearized KG sugraph, which totally discards the explicit structure\ninformation of a KG subgraph. To address this issue, we propose to apply a\nbidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we\nenhance our RNN decoder with node-level copying mechanism to allow directly\ncopying node attributes from the KG subgraph to the output question. Both\nautomatic and human evaluation results demonstrate that our model achieves new\nstate-of-the-art scores, outperforming existing methods by a significant margin\non two QG benchmarks. Experimental results also show that our QG model can\nconsistently benefit the Question Answering (QA) task as a mean of data\naugmentation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:43:22 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 04:19:32 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Chen", "Yu", ""], ["Wu", "Lingfei", ""], ["Zaki", "Mohammed J.", ""]]}, {"id": "2004.06025", "submitter": "Abhijeet Awasthi", "authors": "Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, Sunita Sarawagi", "title": "Learning from Rules Generalizing Labeled Exemplars", "comments": "ICLR 2020 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications labeled data is not readily available, and needs to be\ncollected via pain-staking human supervision. We propose a rule-exemplar method\nfor collecting human supervision to combine the efficiency of rules with the\nquality of instance labels. The supervision is coupled such that it is both\nnatural for humans and synergistic for learning. We propose a training\nalgorithm that jointly denoises rules via latent coverage variables, and trains\nthe model through a soft implication loss over the coverage and label\nvariables. The denoised rules and trained model are used jointly for inference.\nEmpirical evaluation on five different tasks shows that (1) our algorithm is\nmore accurate than several existing methods of learning from a mix of clean and\nnoisy supervision, and (2) the coupled rule-exemplar supervision is effective\nin denoising rules.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:57:54 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 15:56:59 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Awasthi", "Abhijeet", ""], ["Ghosh", "Sabyasachi", ""], ["Goyal", "Rasna", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "2004.06063", "submitter": "Markus Freitag", "authors": "Markus Freitag, David Grangier, Isaac Caswell", "title": "BLEU might be Guilty but References are not Innocent", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of automatic metrics for machine translation has been\nincreasingly called into question, especially for high-quality systems. This\npaper demonstrates that, while choice of metric is important, the nature of the\nreferences is also critical. We study different methods to collect references\nand compare their value in automated evaluation by reporting correlation with\nhuman evaluation for a variety of systems and metrics. Motivated by the finding\nthat typical references exhibit poor diversity, concentrating around\ntranslationese language, we develop a paraphrasing task for linguists to\nperform on existing reference translations, which counteracts this bias. Our\nmethod yields higher correlation with human judgment not only for the\nsubmissions of WMT 2019 English to German, but also for Back-translation and\nAPE augmented MT output, which have been shown to have low correlation with\nautomatic metrics using standard references. We demonstrate that our\nmethodology improves correlation with all modern evaluation metrics we look at,\nincluding embedding-based methods. To complete this picture, we reveal that\nmulti-reference BLEU does not improve the correlation for high quality output,\nand present an alternative multi-reference formulation that is more effective.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 16:49:09 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 13:02:12 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Freitag", "Markus", ""], ["Grangier", "David", ""], ["Caswell", "Isaac", ""]]}, {"id": "2004.06076", "submitter": "Adyasha Maharana", "authors": "Adyasha Maharana, Mohit Bansal", "title": "Adversarial Augmentation Policy Search for Domain and Cross-Lingual\n  Generalization in Reading Comprehension", "comments": "Findings of EMNLP, 2020 (16 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading comprehension models often overfit to nuances of training datasets\nand fail at adversarial evaluation. Training with adversarially augmented\ndataset improves robustness against those adversarial attacks but hurts\ngeneralization of the models. In this work, we present several effective\nadversaries and automated data augmentation policy search methods with the goal\nof making reading comprehension models more robust to adversarial evaluation,\nbut also improving generalization to the source domain as well as new domains\nand languages. We first propose three new methods for generating QA\nadversaries, that introduce multiple points of confusion within the context,\nshow dependence on insertion location of the distractor, and reveal the\ncompounding effect of mixing adversarial strategies with syntactic and semantic\nparaphrasing methods. Next, we find that augmenting the training datasets with\nuniformly sampled adversaries improves robustness to the adversarial attacks\nbut leads to decline in performance on the original unaugmented dataset. We\naddress this issue via RL and more efficient Bayesian policy search methods for\nautomatically learning the best augmentation policy combinations of the\ntransformation probability for each adversary in a large search space. Using\nthese learned policies, we show that adversarial training can lead to\nsignificant improvements in in-domain, out-of-domain, and cross-lingual\n(German, Russian, Turkish) generalization.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 17:20:08 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 15:30:48 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 01:38:28 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2020 16:43:56 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Maharana", "Adyasha", ""], ["Bansal", "Mohit", ""]]}, {"id": "2004.06100", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh\n  Krishnan, and Dawn Song", "title": "Pretrained Transformers Improve Out-of-Distribution Robustness", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although pretrained Transformers such as BERT achieve high accuracy on\nin-distribution examples, do they generalize to new distributions? We\nsystematically measure out-of-distribution (OOD) generalization for seven NLP\ndatasets by constructing a new robustness benchmark with realistic distribution\nshifts. We measure the generalization of previous models including bag-of-words\nmodels, ConvNets, and LSTMs, and we show that pretrained Transformers'\nperformance declines are substantially smaller. Pretrained transformers are\nalso more effective at detecting anomalous or OOD examples, while many previous\nmodels are frequently worse than chance. We examine which factors affect\nrobustness, finding that larger models are not necessarily more robust,\ndistillation can be harmful, and more diverse pretraining data can enhance\nrobustness. Finally, we show where future work can improve OOD robustness.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 17:58:56 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 05:01:33 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Hendrycks", "Dan", ""], ["Liu", "Xiaoyuan", ""], ["Wallace", "Eric", ""], ["Dziedzic", "Adam", ""], ["Krishnan", "Rishabh", ""], ["Song", "Dawn", ""]]}, {"id": "2004.06153", "submitter": "Ming Jiang", "authors": "Ming Jiang, Jennifer D'Souza, S\\\"oren Auer, J. Stephen Downie", "title": "Improving Scholarly Knowledge Representation: Evaluating BERT-based\n  Models for Scientific Relation Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of research publications, there is a vast amount of\nscholarly knowledge that needs to be organized in digital libraries. To deal\nwith this challenge, techniques relying on knowledge-graph structures are being\nadvocated. Within such graph-based pipelines, inferring relation types between\nrelated scientific concepts is a crucial step. Recently, advanced techniques\nrelying on language models pre-trained on the large corpus have been popularly\nexplored for automatic relation classification. Despite remarkable\ncontributions that have been made, many of these methods were evaluated under\ndifferent scenarios, which limits their comparability. To this end, we present\na thorough empirical evaluation on eight Bert-based classification models by\nfocusing on two key factors: 1) Bert model variants, and 2) classification\nstrategies. Experiments on three corpora show that domain-specific pre-training\ncorpus benefits the Bert-based classification model to identify the type of\nscientific relations. Although the strategy of predicting a single relation\neach time achieves a higher classification accuracy than the strategy of\nidentifying multiple relation types simultaneously in general, the latter\nstrategy demonstrates a more consistent performance in the corpus with either a\nlarge or small size of annotations. Our study aims to offer recommendations to\nthe stakeholders of digital libraries for selecting the appropriate technique\nto build knowledge-graph-based systems for enhanced scholarly information\norganization.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 18:46:55 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 14:30:03 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Jiang", "Ming", ""], ["D'Souza", "Jennifer", ""], ["Auer", "S\u00f6ren", ""], ["Downie", "J. Stephen", ""]]}, {"id": "2004.06165", "submitter": "Xiujun Li", "authors": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei\n  Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "comments": "ECCV 2020, Code and pre-trained models are released:\n  https://github.com/microsoft/Oscar", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pre-training methods of learning cross-modal representations on\nimage-text pairs are becoming popular for vision-language tasks. While existing\nmethods simply concatenate image region features and text features as input to\nthe model to be pre-trained and use self-attention to learn image-text semantic\nalignments in a brute force manner, in this paper, we propose a new learning\nmethod Oscar (Object-Semantics Aligned Pre-training), which uses object tags\ndetected in images as anchor points to significantly ease the learning of\nalignments. Our method is motivated by the observation that the salient objects\nin an image can be accurately detected, and are often mentioned in the paired\ntext. We pre-train an Oscar model on the public corpus of 6.5 million\ntext-image pairs, and fine-tune it on downstream tasks, creating new\nstate-of-the-arts on six well-established vision-language understanding and\ngeneration tasks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:18:10 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 03:29:46 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 04:57:31 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 01:18:25 GMT"}, {"version": "v5", "created": "Sun, 26 Jul 2020 00:46:46 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Xiujun", ""], ["Yin", "Xi", ""], ["Li", "Chunyuan", ""], ["Zhang", "Pengchuan", ""], ["Hu", "Xiaowei", ""], ["Zhang", "Lei", ""], ["Wang", "Lijuan", ""], ["Hu", "Houdong", ""], ["Dong", "Li", ""], ["Wei", "Furu", ""], ["Choi", "Yejin", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2004.06176", "submitter": "Keping Bi", "authors": "Keping Bi, Rahul Jha, W. Bruce Croft, Asli Celikyilmaz", "title": "AREDSUM: Adaptive Redundancy-Aware Iterative Sentence Ranking for\n  Extractive Document Summarization", "comments": "In proceedings of EACL'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancy-aware extractive summarization systems score the redundancy of the\nsentences to be included in a summary either jointly with their salience\ninformation or separately as an additional sentence scoring step. Previous work\nshows the efficacy of jointly scoring and selecting sentences with neural\nsequence generation models. It is, however, not well-understood if the gain is\ndue to better encoding techniques or better redundancy reduction approaches.\nSimilarly, the contribution of salience versus diversity components on the\ncreated summary is not studied well. Building on the state-of-the-art encoding\nmethods for summarization, we present two adaptive learning models: AREDSUM-SEQ\nthat jointly considers salience and novelty during sentence selection; and a\ntwo-step AREDSUM-CTX that scores salience first, then learns to balance\nsalience and redundancy, enabling the measurement of the impact of each aspect.\nEmpirical results on CNN/DailyMail and NYT50 datasets show that by modeling\ndiversity explicitly in a separate step, AREDSUM-CTX achieves significantly\nbetter performance than AREDSUM-SEQ as well as state-of-the-art extractive\nsummarization baselines.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 20:02:03 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 02:44:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bi", "Keping", ""], ["Jha", "Rahul", ""], ["Croft", "W. Bruce", ""], ["Celikyilmaz", "Asli", ""]]}, {"id": "2004.06188", "submitter": "Saif Mohammad Dr.", "authors": "Will E. Hipson and Saif M. Mohammad", "title": "PoKi: A Large Dataset of Poems by Children", "comments": null, "journal-ref": "Proceedings of the 12th Language Resources and Evaluation\n  Conference (LREC-2020), May 2020, Marseille, France", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Child language studies are crucial in improving our understanding of child\nwell-being; especially in determining the factors that impact happiness, the\nsources of anxiety, techniques of emotion regulation, and the mechanisms to\ncope with stress. However, much of this research is stymied by the lack of\navailability of large child-written texts. We present a new corpus of\nchild-written text, PoKi, which includes about 62 thousand poems written by\nchildren from grades 1 to 12. PoKi is especially useful in studying child\nlanguage because it comes with information about the age of the child authors\n(their grade). We analyze the words in PoKi along several emotion dimensions\n(valence, arousal, dominance) and discrete emotions (anger, fear, sadness,\njoy). We use non-parametric regressions to model developmental differences from\nearly childhood to late-adolescence. Results show decreases in valence that are\nespecially pronounced during mid-adolescence, while arousal and dominance\npeaked during adolescence. Gender differences in the developmental trajectory\nof emotions are also observed. Our results support and extend the current state\nof emotion development research.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 20:36:57 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 13:22:52 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 00:55:37 GMT"}, {"version": "v4", "created": "Sun, 3 May 2020 01:48:31 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hipson", "Will E.", ""], ["Mohammad", "Saif M.", ""]]}, {"id": "2004.06190", "submitter": "Alexios Gidiotis", "authors": "Alexios Gidiotis and Grigorios Tsoumakas", "title": "A Divide-and-Conquer Approach to the Summarization of Long Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel divide-and-conquer method for the neural summarization of\nlong documents. Our method exploits the discourse structure of the document and\nuses sentence similarity to split the problem into an ensemble of smaller\nsummarization problems. In particular, we break a long document and its summary\ninto multiple source-target pairs, which are used for training a model that\nlearns to summarize each part of the document separately. These partial\nsummaries are then combined in order to produce a final complete summary. With\nthis approach we can decompose the problem of long document summarization into\nsmaller and simpler problems, reducing computational complexity and creating\nmore training examples, which at the same time contain less noise in the target\nsummaries compared to the standard approach. We demonstrate that this approach\npaired with different summarization models, including sequence-to-sequence RNNs\nand Transformers, can lead to improved summarization performance. Our best\nmodels achieve results that are on par with the state-of-the-art in two two\npublicly available datasets of academic articles.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 20:38:49 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 08:45:45 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 14:10:54 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Gidiotis", "Alexios", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2004.06201", "submitter": "Yi Tay", "authors": "Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, Andrew\n  Tomkins", "title": "Reverse Engineering Configurations of Neural Text Generation Models", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper seeks to develop a deeper understanding of the fundamental\nproperties of neural text generations models. The study of artifacts that\nemerge in machine generated text as a result of modeling choices is a nascent\nresearch area. Previously, the extent and degree to which these artifacts\nsurface in generated text has not been well studied. In the spirit of better\nunderstanding generative text models and their artifacts, we propose the new\ntask of distinguishing which of several variants of a given model generated a\npiece of text, and we conduct an extensive suite of diagnostic tests to observe\nwhether modeling choices (e.g., sampling methods, top-$k$ probabilities, model\narchitectures, etc.) leave detectable artifacts in the text they generate. Our\nkey finding, which is backed by a rigorous set of experiments, is that such\nartifacts are present and that different modeling choices can be inferred by\nobserving the generated text alone. This suggests that neural text generators\nmay be more sensitive to various modeling choices than previously thought.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 21:02:44 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Tay", "Yi", ""], ["Bahri", "Dara", ""], ["Zheng", "Che", ""], ["Brunk", "Clifford", ""], ["Metzler", "Donald", ""], ["Tomkins", "Andrew", ""]]}, {"id": "2004.06216", "submitter": "Murthy Devarakonda", "authors": "Hong Guan, Jianfu Li, Hua Xu, Murthy Devarakonda", "title": "Robustly Pre-trained Neural Model for Direct Temporal Relation\n  Extraction", "comments": "10 pages, 1 Figure, 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Identifying relationships between clinical events and temporal\nexpressions is a key challenge in meaningfully analyzing clinical text for use\nin advanced AI applications. While previous studies exist, the state-of-the-art\nperformance has significant room for improvement.\n  Methods: We studied several variants of BERT (Bidirectional Encoder\nRepresentations using Transformers) some involving clinical domain\ncustomization and the others involving improved architecture and/or training\nstrategies. We evaluated these methods using a direct temporal relations\ndataset which is a semantically focused subset of the 2012 i2b2 temporal\nrelations challenge dataset.\n  Results: Our results show that RoBERTa, which employs better pre-training\nstrategies including using 10x larger corpus, has improved overall F measure by\n0.0864 absolute score (on the 1.00 scale) and thus reducing the error rate by\n24% relative to the previous state-of-the-art performance achieved with an SVM\n(support vector machine) model.\n  Conclusion: Modern contextual language modeling neural networks, pre-trained\non a large corpus, achieve impressive performance even on highly-nuanced\nclinical temporal relation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 22:01:38 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Guan", "Hong", ""], ["Li", "Jianfu", ""], ["Xu", "Hua", ""], ["Devarakonda", "Murthy", ""]]}, {"id": "2004.06222", "submitter": "Murthy Devarakonda", "authors": "Ashwin Karthik Ambalavanan, Murthy Devarakonda", "title": "Cascade Neural Ensemble for Identifying Scientifically Sound Articles", "comments": "11 pages, 4 figures, and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: A significant barrier to conducting systematic reviews and\nmeta-analysis is efficiently finding scientifically sound relevant articles.\nTypically, less than 1% of articles match this requirement which leads to a\nhighly imbalanced task. Although feature-engineered and early neural networks\nmodels were studied for this task, there is an opportunity to improve the\nresults.\n  Methods: We framed the problem of filtering articles as a classification\ntask, and trained and tested several ensemble architectures of SciBERT, a\nvariant of BERT pre-trained on scientific articles, on a manually annotated\ndataset of about 50K articles from MEDLINE. Since scientifically sound articles\nare identified through a multi-step process we proposed a novel cascade\nensemble analogous to the selection process. We compared the performance of the\ncascade ensemble with a single integrated model and other types of ensembles as\nwell as with results from previous studies.\n  Results: The cascade ensemble architecture achieved 0.7505 F measure, an\nimpressive 49.1% error rate reduction, compared to a CNN model that was\npreviously proposed and evaluated on a selected subset of the 50K articles. On\nthe full dataset, the cascade ensemble achieved 0.7639 F measure, resulting in\nan error rate reduction of 19.7% compared to the best performance reported in a\nprevious study that used the full dataset.\n  Conclusion: Pre-trained contextual encoder neural networks (e.g. SciBERT)\nperform better than the models studied previously and manually created search\nfilters in filtering for scientifically sound relevant articles. The superior\nperformance achieved by the cascade ensemble is a significant result that\ngeneralizes beyond this task and the dataset, and is analogous to query\noptimization in IR and databases.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 22:23:04 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ambalavanan", "Ashwin Karthik", ""], ["Devarakonda", "Murthy", ""]]}, {"id": "2004.06295", "submitter": "Hao Fei", "authors": "Hao Fei and Meishan Zhang and Donghong Ji", "title": "Cross-Lingual Semantic Role Labeling with High-Quality Translated\n  Training Corpus", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many efforts of research are devoted to semantic role labeling (SRL) which is\ncrucial for natural language understanding. Supervised approaches have achieved\nimpressing performances when large-scale corpora are available for\nresource-rich languages such as English. While for the low-resource languages\nwith no annotated SRL dataset, it is still challenging to obtain competitive\nperformances. Cross-lingual SRL is one promising way to address the problem,\nwhich has achieved great advances with the help of model transferring and\nannotation projection. In this paper, we propose a novel alternative based on\ncorpus translation, constructing high-quality training datasets for the target\nlanguages from the source gold-standard SRL annotations. Experimental results\non Universal Proposition Bank show that the translation-based method is highly\neffective, and the automatic pseudo datasets can improve the target-language\nSRL performances significantly.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 04:16:43 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 03:43:42 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Fei", "Hao", ""], ["Zhang", "Meishan", ""], ["Ji", "Donghong", ""]]}, {"id": "2004.06303", "submitter": "Rediet Abebe", "authors": "Rediet Abebe, Salvatore Giorgi, Anna Tedijanto, Anneke Buffone, H.\n  Andrew Schwartz", "title": "Quantifying Community Characteristics of Maternal Mortality Using Social\n  Media", "comments": "In Proceedings of The Web Conference 2020(WWW '20)", "journal-ref": null, "doi": "10.1145/3366423.3380066", "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most mortality rates have decreased in the US, maternal mortality has\nincreased and is among the highest of any OECD nation. Extensive public health\nresearch is ongoing to better understand the characteristics of communities\nwith relatively high or low rates. In this work, we explore the role that\nsocial media language can play in providing insights into such community\ncharacteristics. Analyzing pregnancy-related tweets generated in US counties,\nwe reveal a diverse set of latent topics including Morning Sickness, Celebrity\nPregnancies, and Abortion Rights. We find that rates of mentioning these topics\non Twitter predicts maternal mortality rates with higher accuracy than standard\nsocioeconomic and risk variables such as income, race, and access to\nhealth-care, holding even after reducing the analysis to six topics chosen for\ntheir interpretability and connections to known risk factors. We then\ninvestigate psychological dimensions of community language, finding the use of\nless trustful, more stressed, and more negative affective language is\nsignificantly associated with higher mortality rates, while trust and negative\naffect also explain a significant portion of racial disparities in maternal\nmortality. We discuss the potential for these insights to inform actionable\nhealth interventions at the community-level.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 04:57:51 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Abebe", "Rediet", ""], ["Giorgi", "Salvatore", ""], ["Tedijanto", "Anna", ""], ["Buffone", "Anneke", ""], ["Schwartz", "H. Andrew", ""]]}, {"id": "2004.06338", "submitter": "Sevinj Yolchuyeva", "authors": "Sevinj Yolchuyeva, G\\'eza N\\'emeth, B\\'alint Gyires-T\\'oth", "title": "Transformer based Grapheme-to-Phoneme Conversion", "comments": "INTERSPEECH 2019", "journal-ref": null, "doi": "10.21437/Interspeech.2019-1954", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanism is one of the most successful techniques in deep learning\nbased Natural Language Processing (NLP). The transformer network architecture\nis completely based on attention mechanisms, and it outperforms\nsequence-to-sequence models in neural machine translation without recurrent and\nconvolutional layers. Grapheme-to-phoneme (G2P) conversion is a task of\nconverting letters (grapheme sequence) to their pronunciations (phoneme\nsequence). It plays a significant role in text-to-speech (TTS) and automatic\nspeech recognition (ASR) systems. In this paper, we investigate the application\nof transformer architecture to G2P conversion and compare its performance with\nrecurrent and convolutional neural network based approaches. Phoneme and word\nerror rates are evaluated on the CMUDict dataset for US English and the NetTalk\ndataset. The results show that transformer based G2P outperforms the\nconvolutional-based approach in terms of word error rate and our results\nsignificantly exceeded previous recurrent approaches (without attention)\nregarding word and phoneme error rates on both datasets. Furthermore, the size\nof the proposed model is much smaller than the size of the previous approaches.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 07:48:15 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 21:09:53 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Yolchuyeva", "Sevinj", ""], ["N\u00e9meth", "G\u00e9za", ""], ["Gyires-T\u00f3th", "B\u00e1lint", ""]]}, {"id": "2004.06343", "submitter": "Nikhil Saldanha", "authors": "Youri Arkesteijn, Nikhil Saldanha, Bastijn Kostense", "title": "Code Completion using Neural Attention and Byte Pair Encoding", "comments": "4 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to do code completion based on implementing a Neural\nNetwork from Li et. al.. Our contribution is that we use an encoding that is\nin-between character and word encoding called Byte Pair Encoding (BPE). We use\nthis on the source code files treating them as natural text without first going\nthrough the abstract syntax tree (AST). We have implemented two models: an\nattention-enhanced LSTM and a pointer network, where the pointer network was\noriginally introduced to solve out of vocabulary problems. We are interested to\nsee if BPE can replace the need for the pointer network for code completion.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 08:00:40 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Arkesteijn", "Youri", ""], ["Saldanha", "Nikhil", ""], ["Kostense", "Bastijn", ""]]}, {"id": "2004.06358", "submitter": "Matthias Sperber", "authors": "Matthias Sperber, Matthias Paulik", "title": "Speech Translation and the End-to-End Promise: Taking Stock of Where We\n  Are", "comments": "ACL 2020 theme track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over its three decade history, speech translation has experienced several\nshifts in its primary research themes; moving from loosely coupled cascades of\nspeech recognition and machine translation, to exploring questions of tight\ncoupling, and finally to end-to-end models that have recently attracted much\nattention. This paper provides a brief survey of these developments, along with\na discussion of the main challenges of traditional approaches which stem from\ncommitting to intermediate representations from the speech recognizer, and from\ntraining cascaded models separately towards different objectives.\n  Recent end-to-end modeling techniques promise a principled way of overcoming\nthese issues by allowing joint training of all model components and removing\nthe need for explicit intermediate representations. However, a closer look\nreveals that many end-to-end models fall short of solving these issues, due to\ncompromises made to address data scarcity. This paper provides a unifying\ncategorization and nomenclature that covers both traditional and recent\napproaches and that may help researchers by highlighting both trade-offs and\nopen research questions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 08:43:51 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Sperber", "Matthias", ""], ["Paulik", "Matthias", ""]]}, {"id": "2004.06384", "submitter": "Shengbin Jia", "authors": "Shengbin Jia, Ling Ding, Xiaojun Chen, Shijia E, Yang Xiang", "title": "Incorporating Uncertain Segmentation Information into Chinese NER for\n  Social Media Text", "comments": "SocialNLP@ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese word segmentation is necessary to provide word-level information for\nChinese named entity recognition (NER) systems. However, segmentation error\npropagation is a challenge for Chinese NER while processing colloquial data\nlike social media text. In this paper, we propose a model (UIcwsNN) that\nspecializes in identifying entities from Chinese social media text, especially\nby leveraging ambiguous information of word segmentation. Such uncertain\ninformation contains all the potential segmentation states of a sentence that\nprovides a channel for the model to infer deep word-level characteristics. We\npropose a trilogy (i.e., candidate position embedding -> position selective\nattention -> adaptive word convolution) to encode uncertain word segmentation\ninformation and acquire appropriate word-level representation. Experiments\nresults on the social media corpus show that our model alleviates the\nsegmentation error cascading trouble effectively, and achieves a significant\nperformance improvement of more than 2% over previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 09:39:35 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 09:10:35 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Jia", "Shengbin", ""], ["Ding", "Ling", ""], ["Chen", "Xiaojun", ""], ["E", "Shijia", ""], ["Xiang", "Yang", ""]]}, {"id": "2004.06427", "submitter": "Shu Liu", "authors": "Shu Liu, Wei Li, Yunfang Wu, Qi Su, Xu Sun", "title": "Jointly Modeling Aspect and Sentiment with Dynamic Heterogeneous Graph\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target-Based Sentiment Analysis aims to detect the opinion aspects (aspect\nextraction) and the sentiment polarities (sentiment detection) towards them.\nBoth the previous pipeline and integrated methods fail to precisely model the\ninnate connection between these two objectives. In this paper, we propose a\nnovel dynamic heterogeneous graph to jointly model the two objectives in an\nexplicit way. Both the ordinary words and sentiment labels are treated as nodes\nin the heterogeneous graph, so that the aspect words can interact with the\nsentiment information. The graph is initialized with multiple types of\ndependencies, and dynamically modified during real-time prediction. Experiments\non the benchmark datasets show that our model outperforms the state-of-the-art\nmodels. Further analysis demonstrates that our model obtains significant\nperformance gain on the challenging instances under multiple-opinion aspects\nand no-opinion aspect situations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 11:27:30 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Liu", "Shu", ""], ["Li", "Wei", ""], ["Wu", "Yunfang", ""], ["Su", "Qi", ""], ["Sun", "Xu", ""]]}, {"id": "2004.06438", "submitter": "Wei Li", "authors": "Siyu Duan, Wei Li, Cai Jing, Yancheng He, Yunfang Wu, Xu Sun", "title": "Query-Variant Advertisement Text Generation with Association Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advertising is an important revenue source for many companies. However, it is\nexpensive to manually create advertisements that meet the needs of various\nqueries for massive items. In this paper, we propose the query-variant\nadvertisement text generation task that aims to generate candidate\nadvertisements for different queries with various needs given the item\nkeywords. In this task, for many different queries there is only one general\npurposed advertisement with no predefined query-advertisement pair, which would\ndiscourage traditional End-to-End models from generating query-variant\nadvertisements for different queries with different needs. To deal with the\nproblem, we propose a query-variant advertisement text generation model that\ntakes keywords and associated external knowledge as input during training and\nadds different queries during inference. Adding external knowledge helps the\nmodel adapted to the information besides the item keywords during training,\nwhich makes the transition between training and inference more smoothing when\nthe query is added during inference. Both automatic and human evaluation show\nthat our model can generate more attractive and query-focused advertisements\nthan the strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 12:04:28 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Duan", "Siyu", ""], ["Li", "Wei", ""], ["Jing", "Cai", ""], ["He", "Yancheng", ""], ["Wu", "Yunfang", ""], ["Sun", "Xu", ""]]}, {"id": "2004.06465", "submitter": "Binny Mathew", "authors": "Sai Saketh Aluru, Binny Mathew, Punyajoy Saha, and Animesh Mukherjee", "title": "Deep Learning Models for Multilingual Hate Speech Detection", "comments": "16 pages, Accepted at ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech detection is a challenging problem with most of the datasets\navailable in only one language: English. In this paper, we conduct a large\nscale analysis of multilingual hate speech in 9 languages from 16 different\nsources. We observe that in low resource setting, simple models such as LASER\nembedding with logistic regression performs the best, while in high resource\nsetting BERT based models perform better. In case of zero-shot classification,\nlanguages such as Italian and Portuguese achieve good results. Our proposed\nframework could be used as an efficient solution for low-resource languages.\nThese models could also act as good baselines for future multilingual hate\nspeech detection tasks. We have made our code and experimental settings public\nfor other researchers at https://github.com/punyajoy/DE-LIMIT.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 13:14:27 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 15:28:29 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 05:48:56 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Aluru", "Sai Saketh", ""], ["Mathew", "Binny", ""], ["Saha", "Punyajoy", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "2004.06474", "submitter": "Armen Allahverdyan", "authors": "Weibing Deng, R. Xie, S. Deng, and Armen E. Allahverdyan", "title": "Two halves of a meaningful text are statistically different", "comments": "15 pages and 14 tables", "journal-ref": null, "doi": "10.1088/1742-5468/abe947", "report-no": null, "categories": "cs.CL cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which statistical features distinguish a meaningful text (possibly written in\nan unknown system) from a meaningless set of symbols? Here we answer this\nquestion by comparing features of the first half of a text to its second half.\nThis comparison can uncover hidden effects, because the halves have the same\nvalues of many parameters (style, genre {\\it etc}). We found that the first\nhalf has more different words and more rare words than the second half. Also,\nwords in the first half are distributed less homogeneously over the text in the\nsense of of the difference between the frequency and the inverse spatial\nperiod. These differences hold for the significant majority of several hundred\nrelatively short texts we studied. The statistical significance is confirmed\nvia the Wilcoxon test. Differences disappear after random permutation of words\nthat destroys the linear structure of the text. The differences reveal a\ntemporal asymmetry in meaningful texts, which is confirmed by showing that\ntexts are much better compressible in their natural way (i.e. along the\nnarrative) than in the word-inverted form. We conjecture that these results\nconnect the semantic organization of a text (defined by the flow of its\nnarrative) to its statistical features.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 20:00:12 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Deng", "Weibing", ""], ["Xie", "R.", ""], ["Deng", "S.", ""], ["Allahverdyan", "Armen E.", ""]]}, {"id": "2004.06499", "submitter": "Wietse de Vries", "authors": "Wietse de Vries, Andreas van Cranenburgh and Malvina Nissim", "title": "What's so special about BERT's layers? A closer look at the NLP pipeline\n  in monolingual and multilingual models", "comments": "Accepted at Findings of EMNLP 2020 (camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Peeking into the inner workings of BERT has shown that its layers resemble\nthe classical NLP pipeline, with progressively more complex tasks being\nconcentrated in later layers. To investigate to what extent these results also\nhold for a language other than English, we probe a Dutch BERT-based model and\nthe multilingual BERT model for Dutch NLP tasks. In addition, through a deeper\nanalysis of part-of-speech tagging, we show that also within a given task,\ninformation is spread over different parts of the network and the pipeline\nmight not be as neat as it seems. Each layer has different specialisations, so\nthat it may be more useful to combine information from different layers,\ninstead of selecting a single one based on the best overall performance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 13:41:48 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 11:51:34 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["de Vries", "Wietse", ""], ["van Cranenburgh", "Andreas", ""], ["Nissim", "Malvina", ""]]}, {"id": "2004.06518", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Mojtaba Heidarysafa, Tolu Odukoya, Philip Potter,\n  Laura E. Barnes, Donald E. Brown", "title": "Gender Detection on Social Networks using Ensemble Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-63128-4_26", "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the ever-increasing volume of posts on social media sites such as\nFacebook and Twitter requires improved information processing methods for\nprofiling authorship. Document classification is central to this task, but the\nperformance of traditional supervised classifiers has degraded as the volume of\nsocial media has increased. This paper addresses this problem in the context of\ngender detection through ensemble classification that employs multi-model deep\nlearning architectures to generate specialized understanding from different\nfeature spaces.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:08:49 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 17:25:00 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 21:54:34 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Kowsari", "Kamran", ""], ["Heidarysafa", "Mojtaba", ""], ["Odukoya", "Tolu", ""], ["Potter", "Philip", ""], ["Barnes", "Laura E.", ""], ["Brown", "Donald E.", ""]]}, {"id": "2004.06519", "submitter": "Federico Bianchi", "authors": "Federico Bianchi and Valerio Di Carlo and Paolo Nicoli and Matteo\n  Palmonari", "title": "Compass-aligned Distributional Embeddings for Studying Semantic\n  Differences across Corpora", "comments": "arXiv admin note: text overlap with arXiv:1906.02376", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2vec is one of the most used algorithms to generate word embeddings\nbecause of a good mix of efficiency, quality of the generated representations\nand cognitive grounding. However, word meaning is not static and depends on the\ncontext in which words are used. Differences in word meaning that depends on\ntime, location, topic, and other factors, can be studied by analyzing\nembeddings generated from different corpora in collections that are\nrepresentative of these factors. For example, language evolution can be studied\nusing a collection of news articles published in different time periods. In\nthis paper, we present a general framework to support cross-corpora language\nstudies with word embeddings, where embeddings generated from different corpora\ncan be compared to find correspondences and differences in meaning across the\ncorpora. CADE is the core component of our framework and solves the key problem\nof aligning the embeddings generated from different corpora. In particular, we\nfocus on providing solid evidence about the effectiveness, generality, and\nrobustness of CADE. To this end, we conduct quantitative and qualitative\nexperiments in different domains, from temporal word embeddings to language\nlocalization and topical analysis. The results of our experiments suggest that\nCADE achieves state-of-the-art or superior performance on tasks where several\ncompeting approaches are available, yet providing a general method that can be\nused in a variety of domains. Finally, our experiments shed light on the\nconditions under which the alignment is reliable, which substantially depends\non the degree of cross-corpora vocabulary overlap.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:46:47 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Bianchi", "Federico", ""], ["Di Carlo", "Valerio", ""], ["Nicoli", "Paolo", ""], ["Palmonari", "Matteo", ""]]}, {"id": "2004.06555", "submitter": "Naofumi Tomita", "authors": "Steven Jiang, Weiyi Wu, Naofumi Tomita, Craig Ganoe, Saeed Hassanpour", "title": "Multi-Ontology Refined Embeddings (MORE): A Hybrid Multi-Ontology and\n  Corpus-based Semantic Representation for Biomedical Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Currently, a major limitation for natural language processing\n(NLP) analyses in clinical applications is that a concept can be referenced in\nvarious forms across different texts. This paper introduces Multi-Ontology\nRefined Embeddings (MORE), a novel hybrid framework for incorporating domain\nknowledge from multiple ontologies into a distributional semantic model,\nlearned from a corpus of clinical text.\n  Materials and Methods: We use the RadCore and MIMIC-III free-text datasets\nfor the corpus-based component of MORE. For the ontology-based part, we use the\nMedical Subject Headings (MeSH) ontology and three state-of-the-art\nontology-based similarity measures. In our approach, we propose a new learning\nobjective, modified from the Sigmoid cross-entropy objective function.\n  Results and Discussion: We evaluate the quality of the generated word\nembeddings using two established datasets of semantic similarities among\nbiomedical concept pairs. On the first dataset with 29 concept pairs, with the\nsimilarity scores established by physicians and medical coders, MORE's\nsimilarity scores have the highest combined correlation (0.633), which is 5.0%\nhigher than that of the baseline model and 12.4% higher than that of the best\nontology-based similarity measure.On the second dataset with 449 concept pairs,\nMORE's similarity scores have a correlation of 0.481, with the average of four\nmedical residents' similarity ratings, and that outperforms the skip-gram model\nby 8.1% and the best ontology measure by 6.9%.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 14:38:41 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Jiang", "Steven", ""], ["Wu", "Weiyi", ""], ["Tomita", "Naofumi", ""], ["Ganoe", "Craig", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "2004.06575", "submitter": "Marta R. Costa-juss\\`a", "authors": "Carlos Escolano, Marta R. Costa-juss\\`a, Jos\\'e A. R. Fonollosa and\n  Mikel Artetxe", "title": "Multilingual Machine Translation: Closing the Gap between Shared and\n  Language-specific Encoder-Decoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art multilingual machine translation relies on a universal\nencoder-decoder, which requires retraining the entire system to add new\nlanguages. In this paper, we propose an alternative approach that is based on\nlanguage-specific encoder-decoders, and can thus be more easily extended to new\nlanguages by learning their corresponding modules. So as to encourage a common\ninterlingua representation, we simultaneously train the N initial languages.\nOur experiments show that the proposed approach outperforms the universal\nencoder-decoder by 3.28 BLEU points on average, and when adding new languages,\nwithout the need to retrain the rest of the modules. All in all, our work\ncloses the gap between shared and language-specific encoder-decoders, advancing\ntoward modular multilingual machine translation systems that can be flexibly\nextended in lifelong learning settings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:02:24 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Escolano", "Carlos", ""], ["Costa-juss\u00e0", "Marta R.", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""], ["Artetxe", "Mikel", ""]]}, {"id": "2004.06577", "submitter": "Hamza Harkous", "authors": "Hamza Harkous, Isabel Groves, Amir Saffari", "title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation\n  with Semantic Fidelity", "comments": "28th International Conference on Computational Linguistics (COLING\n  2020), Online, December 8-13, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  End-to-end neural data-to-text (D2T) generation has recently emerged as an\nalternative to pipeline-based architectures. However, it has faced challenges\nin generalizing to new domains and generating semantically consistent text. In\nthis work, we present DataTuner, a neural, end-to-end data-to-text generation\nsystem that makes minimal assumptions about the data representation and the\ntarget domain. We take a two-stage generation-reranking approach, combining a\nfine-tuned language model with a semantic fidelity classifier. Each of our\ncomponents is learnt end-to-end without the need for dataset-specific\nheuristics, entity delexicalization, or post-processing. We show that DataTuner\nachieves state of the art results on the automated metrics across four major\nD2T datasets (LDC2017T10, WebNLG, ViGGO, and Cleaned E2E), with a fluency\nassessed by human annotators nearing or exceeding the human-written reference\ntexts. We further demonstrate that the model-based semantic fidelity scorer in\nDataTuner is a better assessment tool compared to traditional, heuristic-based\nmeasures. Our generated text has a significantly better semantic fidelity than\nthe state of the art across all four datasets\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 11:16:53 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 19:09:46 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Harkous", "Hamza", ""], ["Groves", "Isabel", ""], ["Saffari", "Amir", ""]]}, {"id": "2004.06608", "submitter": "Danushka Bollegala", "authors": "Xia Cui and Danushka Bollegala", "title": "Multi-source Attention for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation considers the problem of generalising a model learnt using\ndata from a particular source domain to a different target domain. Often it is\ndifficult to find a suitable single source to adapt from, and one must consider\nmultiple sources. Using an unrelated source can result in sub-optimal\nperformance, known as the \\emph{negative transfer}. However, it is challenging\nto select the appropriate source(s) for classifying a given target instance in\nmulti-source unsupervised domain adaptation (UDA). We model source-selection as\nan attention-learning problem, where we learn attention over sources for a\ngiven target instance. For this purpose, we first independently learn\nsource-specific classification models, and a relatedness map between sources\nand target domains using pseudo-labelled target domain instances. Next, we\nlearn attention-weights over the sources for aggregating the predictions of the\nsource-specific models. Experimental results on cross-domain sentiment\nclassification benchmarks show that the proposed method outperforms prior\nproposals in multi-source UDA.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:51:02 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 13:48:36 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Cui", "Xia", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2004.06660", "submitter": "Paul Michel", "authors": "Keita Kurita, Paul Michel, Graham Neubig", "title": "Weight Poisoning Attacks on Pre-trained Models", "comments": "Published as a long paper at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, NLP has seen a surge in the usage of large pre-trained models.\nUsers download weights of models pre-trained on large datasets, then fine-tune\nthe weights on a task of their choice. This raises the question of whether\ndownloading untrusted pre-trained weights can pose a security threat. In this\npaper, we show that it is possible to construct ``weight poisoning'' attacks\nwhere pre-trained weights are injected with vulnerabilities that expose\n``backdoors'' after fine-tuning, enabling the attacker to manipulate the model\nprediction simply by injecting an arbitrary keyword. We show that by applying a\nregularization method, which we call RIPPLe, and an initialization procedure,\nwhich we call Embedding Surgery, such attacks are possible even with limited\nknowledge of the dataset and fine-tuning procedure. Our experiments on\nsentiment classification, toxicity detection, and spam detection show that this\nattack is widely applicable and poses a serious threat. Finally, we outline\npractical defenses against such attacks. Code to reproduce our experiments is\navailable at https://github.com/neulab/RIPPLe.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:51:42 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Kurita", "Keita", ""], ["Michel", "Paul", ""], ["Neubig", "Graham", ""]]}, {"id": "2004.06698", "submitter": "Gi-Cheon Kang", "authors": "Gi-Cheon Kang, Junseok Park, Hwaran Lee, Byoung-Tak Zhang, Jin-Hwa Kim", "title": "DialGraph: Sparse Graph Learning Networks for Visual Dialog", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual dialog is a task of answering a sequence of questions grounded in an\nimage utilizing a dialog history. Previous studies have implicitly explored the\nproblem of reasoning semantic structures among the history using softmax\nattention. However, we argue that the softmax attention yields dense structures\nthat could distract to answer the questions requiring partial or even no\ncontextual information. In this paper, we formulate the visual dialog tasks as\ngraph structure learning tasks. To tackle the problem, we propose Sparse Graph\nLearning Networks (SGLNs) consisting of a multimodal node embedding module and\na sparse graph learning module. The proposed model explicitly learn sparse\ndialog structures by incorporating binary and score edges, leveraging a new\nstructural loss function. Then, it finally outputs the answer, updating each\nnode via a message passing framework. As a result, the proposed model\noutperforms the state-of-the-art approaches on the VisDial v1.0 dataset, only\nusing 10.95% of the dialog history, as well as improves interpretability\ncompared to baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:52:41 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Kang", "Gi-Cheon", ""], ["Park", "Junseok", ""], ["Lee", "Hwaran", ""], ["Zhang", "Byoung-Tak", ""], ["Kim", "Jin-Hwa", ""]]}, {"id": "2004.06747", "submitter": "Carlos-Emiliano Gonz\\'alez-Gallardo", "authors": "Carlos-Emiliano Gonz\\'alez-Gallardo, Eric SanJuan, Juan-Manuel\n  Torres-Moreno", "title": "Extending Text Informativeness Measures to Passage Interestingness\n  Evaluation (Language Model vs. Word Embedding)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Standard informativeness measures used to evaluate Automatic Text\nSummarization mostly rely on n-gram overlapping between the automatic summary\nand the reference summaries. These measures differ from the metric they use\n(cosine, ROUGE, Kullback-Leibler, Logarithm Similarity, etc.) and the bag of\nterms they consider (single words, word n-grams, entities, nuggets, etc.).\nRecent word embedding approaches offer a continuous alternative to discrete\napproaches based on the presence/absence of a text unit. Informativeness\nmeasures have been extended to Focus Information Retrieval evaluation involving\na user's information need represented by short queries. In particular for the\ntask of CLEF-INEX Tweet Contextualization, tweet contents have been considered\nas queries. In this paper we define the concept of Interestingness as a\ngeneralization of Informativeness, whereby the information need is diverse and\nformalized as an unknown set of implicit queries. We then study the ability of\nstate of the art Informativeness measures to cope with this generalization.\nLately we show that with this new framework, standard word embeddings\noutperforms discrete measures only on uni-grams, however bi-grams seems to be a\nkey point of interestingness evaluation. Lastly we prove that the CLEF-INEX\nTweet Contextualization 2012 Logarithm Similarity measure provides best\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 18:22:48 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Gonz\u00e1lez-Gallardo", "Carlos-Emiliano", ""], ["SanJuan", "Eric", ""], ["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "2004.06748", "submitter": "Xinyi Wang", "authors": "Xinyi Wang, Yulia Tsvetkov, Graham Neubig", "title": "Balancing Training for Multilingual Neural Machine Translation", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training multilingual machine translation (MT) models that can translate\nto/from multiple languages, we are faced with imbalanced training sets: some\nlanguages have much more training data than others. Standard practice is to\nup-sample less resourced languages to increase representation, and the degree\nof up-sampling has a large effect on the overall performance. In this paper, we\npropose a method that instead automatically learns how to weight training data\nthrough a data scorer that is optimized to maximize performance on all test\nlanguages. Experiments on two sets of languages under both one-to-many and\nmany-to-one MT settings show our method not only consistently outperforms\nheuristic baselines in terms of average performance, but also offers flexible\ncontrol over the performance of which languages are optimized.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 18:23:28 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 14:00:42 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 19:24:34 GMT"}, {"version": "v4", "created": "Sat, 5 Sep 2020 22:55:01 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wang", "Xinyi", ""], ["Tsvetkov", "Yulia", ""], ["Neubig", "Graham", ""]]}, {"id": "2004.06753", "submitter": "Dirk Groeneveld", "authors": "Dirk Groeneveld, Tushar Khot, Mausam, and Ashish Sabharwal", "title": "A Simple Yet Strong Pipeline for HotpotQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art models for multi-hop question answering typically augment\nlarge-scale language models like BERT with additional, intuitively useful\ncapabilities such as named entity recognition, graph-based reasoning, and\nquestion decomposition. However, does their strong performance on popular\nmulti-hop datasets really justify this added design complexity? Our results\nsuggest that the answer may be no, because even our simple pipeline based on\nBERT, named Quark, performs surprisingly well. Specifically, on HotpotQA, Quark\noutperforms these models on both question answering and support identification\n(and achieves performance very close to a RoBERTa model). Our pipeline has\nthree steps: 1) use BERT to identify potentially relevant sentences\nindependently of each other; 2) feed the set of selected sentences as context\ninto a standard BERT span prediction model to choose an answer; and 3) use the\nsentence selection model, now with the chosen answer, to produce supporting\nsentences. The strong performance of Quark resurfaces the importance of\ncarefully exploring simple model designs before using popular benchmarks to\njustify the value of complex techniques.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 18:48:57 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Groeneveld", "Dirk", ""], ["Khot", "Tushar", ""], ["Mausam", "", ""], ["Sabharwal", "Ashish", ""]]}, {"id": "2004.06756", "submitter": "Taejin Park", "authors": "Tae Jin Park, Kyu J. Han, Jing Huang, Xiaodong He, Bowen Zhou,\n  Panayiotis Georgiou and Shrikanth Narayanan", "title": "Speaker Diarization with Lexical Information", "comments": null, "journal-ref": "Interspeech 2019, 391-395", "doi": "10.21437/Interspeech.2019-1947", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel approach for speaker diarization to leverage\nlexical information provided by automatic speech recognition. We propose a\nspeaker diarization system that can incorporate word-level speaker turn\nprobabilities with speaker embeddings into a speaker clustering process to\nimprove the overall diarization accuracy. To integrate lexical and acoustic\ninformation in a comprehensive way during clustering, we introduce an adjacency\nmatrix integration for spectral clustering. Since words and word boundary\ninformation for word-level speaker turn probability estimation are provided by\na speech recognition system, our proposed method works without any human\nintervention for manual transcriptions. We show that the proposed method\nimproves diarization performance on various evaluation datasets compared to the\nbaseline diarization system using acoustic information only in speaker\nembeddings.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 17:16:56 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Park", "Tae Jin", ""], ["Han", "Kyu J.", ""], ["Huang", "Jing", ""], ["He", "Xiaodong", ""], ["Zhou", "Bowen", ""], ["Georgiou", "Panayiotis", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2004.06778", "submitter": "Negin Karisani", "authors": "Negin Karisani, Payam Karisani", "title": "Mining Coronavirus (COVID-19) Posts in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  World Health Organization (WHO) characterized the novel coronavirus\n(COVID-19) as a global pandemic on March 11th, 2020. Before this and in late\nJanuary, more specifically on January 27th, while the majority of the infection\ncases were still reported in China and a few cruise ships, we began crawling\nsocial media user postings using the Twitter search API. Our goal was to\nleverage machine learning and linguistic tools to better understand the impact\nof the outbreak in China. Unlike our initial expectation to monitor a local\noutbreak, COVID-19 rapidly spread across the globe. In this short article we\nreport the preliminary results of our study on automatically detecting the\npositive reports of COVID-19 from social media user postings using\nstate-of-the-art machine learning models.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 23:38:50 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Karisani", "Negin", ""], ["Karisani", "Payam", ""]]}, {"id": "2004.06793", "submitter": "Toktam Amanzadeh Oghaz", "authors": "Toktam A. Oghaz, Ece C. Mutlu, Jasser Jasser, Niloofar Yousefi, Ivan\n  Garibay", "title": "Probabilistic Model of Narratives Over Topical Trends in Social Media: A\n  Discrete Time Model", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": "10.1145/3372923.3404790", "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social media platforms are turning into the prime source of news and\nnarratives about worldwide events. However,a systematic summarization-based\nnarrative extraction that can facilitate communicating the main underlying\nevents is lacking. To address this issue, we propose a novel event-based\nnarrative summary extraction framework. Our proposed framework is designed as a\nprobabilistic topic model, with categorical time distribution, followed by\nextractive text summarization. Our topic model identifies topics' recurrence\nover time with a varying time resolution. This framework not only captures the\ntopic distributions from the data, but also approximates the user activity\nfluctuations over time. Furthermore, we define significance-dispersity\ntrade-off (SDT) as a comparison measure to identify the topic with the highest\nlifetime attractiveness in a timestamped corpus. We evaluate our model on a\nlarge corpus of Twitter data, including more than one million tweets in the\ndomain of the disinformation campaigns conducted against the White Helmets of\nSyria. Our results indicate that the proposed framework is effective in\nidentifying topical trends, as well as extracting narrative summaries from text\ncorpus with timestamped data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 20:18:21 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Oghaz", "Toktam A.", ""], ["Mutlu", "Ece C.", ""], ["Jasser", "Jasser", ""], ["Yousefi", "Niloofar", ""], ["Garibay", "Ivan", ""]]}, {"id": "2004.06800", "submitter": "Lee James O'Riordan", "authors": "Lee J. O'Riordan, Myles Doyle, Fabio Baruffa, Venkatesh Kannan", "title": "A hybrid classical-quantum workflow for natural language processing", "comments": "For associated code, see https://github.com/ICHEC/QNLP", "journal-ref": null, "doi": "10.1088/2632-2153/abbd2e", "report-no": null, "categories": "quant-ph cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing (NLP) problems are ubiquitous in classical\ncomputing, where they often require significant computational resources to\ninfer sentence meanings. With the appearance of quantum computing hardware and\nsimulators, it is worth developing methods to examine such problems on these\nplatforms. In this manuscript we demonstrate the use of quantum computing\nmodels to perform NLP tasks, where we represent corpus meanings, and perform\ncomparisons between sentences of a given structure. We develop a hybrid\nworkflow for representing small and large scale corpus data sets to be encoded,\nprocessed, and decoded using a quantum circuit model. In addition, we provide\nour results showing the efficacy of the method, and release our developed\ntoolkit as an open software suite.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 12:19:17 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["O'Riordan", "Lee J.", ""], ["Doyle", "Myles", ""], ["Baruffa", "Fabio", ""], ["Kannan", "Venkatesh", ""]]}, {"id": "2004.06814", "submitter": "Emma Manning", "authors": "Emma Manning, Shira Wein, Nathan Schneider", "title": "A Human Evaluation of AMR-to-English Generation Systems", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current state-of-the art systems for generating English text from\nAbstract Meaning Representation (AMR) have been evaluated only using automated\nmetrics, such as BLEU, which are known to be problematic for natural language\ngeneration. In this work, we present the results of a new human evaluation\nwhich collects fluency and adequacy scores, as well as categorization of error\ntypes, for several recent AMR generation systems. We discuss the relative\nquality of these systems and how our results compare to those of automatic\nmetrics, finding that while the metrics are mostly successful in ranking\nsystems overall, collecting human judgments allows for more nuanced\ncomparisons. We also analyze common errors made by these systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 21:41:30 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 17:18:27 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Manning", "Emma", ""], ["Wein", "Shira", ""], ["Schneider", "Nathan", ""]]}, {"id": "2004.06866", "submitter": "William Merrill", "authors": "William Merrill", "title": "On the Linguistic Capacity of Real-Time Counter Automata", "comments": "Unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counter machines have achieved a newfound relevance to the field of natural\nlanguage processing (NLP): recent work suggests some strong-performing\nrecurrent neural networks utilize their memory as counters. Thus, one potential\nway to understand the success of these networks is to revisit the theory of\ncounter computation. Therefore, we study the abilities of real-time counter\nmachines as formal grammars, focusing on formal properties that are relevant\nfor NLP models. We first show that several variants of the counter machine\nconverge to express the same class of formal languages. We also prove that\ncounter languages are closed under complement, union, intersection, and many\nother common set operations. Next, we show that counter machines cannot\nevaluate boolean expressions, even though they can weakly validate their\nsyntax. This has implications for the interpretability and evaluation of neural\nnetwork systems: successfully matching syntactic patterns does not guarantee\nthat counter memory accurately encodes compositional semantics. Finally, we\nconsider whether counter languages are semilinear. This work makes general\ncontributions to the theory of formal languages that are of potential interest\nfor understanding recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 03:37:47 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Merrill", "William", ""]]}, {"id": "2004.06870", "submitter": "Deming Ye", "authors": "Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun,\n  Zhiyuan Liu", "title": "Coreferential Reasoning Learning for Language Representation", "comments": "Accepted by EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language representation models such as BERT could effectively capture\ncontextual semantic information from plain text, and have been proved to\nachieve promising results in lots of downstream NLP tasks with appropriate\nfine-tuning. However, most existing language representation models cannot\nexplicitly handle coreference, which is essential to the coherent understanding\nof the whole discourse. To address this issue, we present CorefBERT, a novel\nlanguage representation model that can capture the coreferential relations in\ncontext. The experimental results show that, compared with existing baseline\nmodels, CorefBERT can achieve significant improvements consistently on various\ndownstream NLP tasks that require coreferential reasoning, while maintaining\ncomparable performance to previous models on other common NLP tasks. The source\ncode and experiment details of this paper can be obtained from\nhttps://github.com/thunlp/CorefBERT.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 03:57:45 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 12:37:53 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ye", "Deming", ""], ["Lin", "Yankai", ""], ["Du", "Jiaju", ""], ["Liu", "Zhenghao", ""], ["Li", "Peng", ""], ["Sun", "Maosong", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "2004.06871", "submitter": "Chien-Sheng Wu", "authors": "Chien-Sheng Wu, Steven Hoi, Richard Socher, and Caiming Xiong", "title": "TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented\n  Dialogue", "comments": "EMNLP 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The underlying difference of linguistic patterns between general text and\ntask-oriented dialogue makes existing pre-trained language models less useful\nin practice. In this work, we unify nine human-human and multi-turn\ntask-oriented dialogue datasets for language modeling. To better model dialogue\nbehavior during pre-training, we incorporate user and system tokens into the\nmasked language modeling. We propose a contrastive objective function to\nsimulate the response selection task. Our pre-trained task-oriented dialogue\nBERT (TOD-BERT) outperforms strong baselines like BERT on four downstream\ntask-oriented dialogue applications, including intention recognition, dialogue\nstate tracking, dialogue act prediction, and response selection. We also show\nthat TOD-BERT has a stronger few-shot ability that can mitigate the data\nscarcity problem for task-oriented dialogue.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 04:09:05 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 18:10:32 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 16:34:52 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Wu", "Chien-Sheng", ""], ["Hoi", "Steven", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "2004.06986", "submitter": "Philipp Wicke", "authors": "Philipp Wicke and Marianna M. Bolognesi", "title": "Framing COVID-19: How we conceptualize and discuss the pandemic on\n  Twitter", "comments": "41 pages, 6 figures", "journal-ref": "PLOS ONE 2020", "doi": "10.1371/journal.pone.0240010", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Doctors and nurses in these weeks are busy in the trenches, fighting against\na new invisible enemy: Covid-19. Cities are locked down and civilians are\nbesieged in their own homes, to prevent the spreading of the virus. War-related\nterminology is commonly used to frame the discourse around epidemics and\ndiseases. Arguably the discourse around the current epidemic will make use of\nwar-related metaphors too,not only in public discourse and the media, but also\nin the tweets written by non-experts of mass communication. We hereby present\nan analysis of the discourse around #Covid-19, based on a corpus of 200k tweets\nposted on Twitter during March and April 2020. Using topic modelling we first\nanalyze the topics around which the discourse can be classified. Then, we show\nthat the WAR framing is used to talk about specific topics, such as the virus\ntreatment, but not others, such as the effects of social distancing on the\npopulation. We then measure and compare the popularity of the WAR frame to\nthree alternative figurative frames (MONSTER, STORM and TSUNAMI) and a literal\nframe used as control (FAMILY). The results show that while the FAMILY literal\nframe covers a wider portion of the corpus, among the figurative framings WAR\nis the most frequently used, and thus arguably the most conventional one.\nHowever, we conclude, this frame is not apt to elaborate the discourse around\nmany aspects involved in the current situation. Therefore, we conclude, in line\nwith previous suggestions, a plethora of framing options, or a metaphor menu,\nmay facilitate the communication of various aspects involved in the\nCovid-19-related discourse on the social media, and thus support civilians in\nthe expression of their feelings, opinions and ideas during the current\npandemic.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 10:14:15 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 10:26:50 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Wicke", "Philipp", ""], ["Bolognesi", "Marianna M.", ""]]}, {"id": "2004.07000", "submitter": "Johannes Dellert", "authors": "Johannes Dellert", "title": "Exploring Probabilistic Soft Logic as a framework for integrating\n  top-down and bottom-up processing of language in a task context", "comments": "32 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report describes a new prototype architecture designed to\nintegrate top-down and bottom-up analysis of non-standard linguistic input,\nwhere a semantic model of the context of an utterance is used to guide the\nanalysis of the non-standard surface forms, including their automated\nnormalization in context. While the architecture is generally applicable, as a\nconcrete use case of the architecture we target the generation of\nsemantically-informed target hypotheses for answers written by German learners\nin response to reading comprehension questions, where the reading context and\npossible target answers are given.\n  The architecture integrates existing NLP components to produce candidate\nanalyses on eight levels of linguistic modeling, all of which are broken down\ninto atomic statements and connected into a large graphical model using\nProbabilistic Soft Logic (PSL) as a framework. Maximum a posteriori inference\non the resulting graphical model then assigns a belief distribution to\ncandidate target hypotheses. The current version of the architecture builds on\nUniversal Dependencies (UD) as its representation formalism on the form level\nand on Abstract Meaning Representations (AMRs) to represent semantic analyses\nof learner answers and the context information provided by the target answers.\nThese general choices will make it comparatively straightforward to apply the\narchitecture to other tasks and other languages.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 11:00:07 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Dellert", "Johannes", ""]]}, {"id": "2004.07067", "submitter": "Mohamed El-Geish", "authors": "Mohamed El-Geish", "title": "Gestalt: a Stacking Ensemble for SQuAD2.0", "comments": "11 pages, 7 figures, Stanford CS224n Natural Language Processing with\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep-learning system -- for the SQuAD2.0 task -- that finds, or\nindicates the lack of, a correct answer to a question in a context paragraph.\nOur goal is to learn an ensemble of heterogeneous SQuAD2.0 models that, when\nblended properly, outperforms the best model in the ensemble per se. We created\na stacking ensemble that combines top-N predictions from two models, based on\nALBERT and RoBERTa, into a multiclass classification task to pick the best\nanswer out of their predictions. We explored various ensemble configurations,\ninput representations, and model architectures. For evaluation, we examined\ntest-set EM and F1 scores; our best-performing ensemble incorporated a\nCNN-based meta-model and scored 87.117 and 90.306, respectively -- a relative\nimprovement of 0.55% for EM and 0.61% for F1 scores, compared to the baseline\nperformance of the best model in the ensemble, an ALBERT-based model, at 86.644\nfor EM and 89.760 for F1.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 08:09:22 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["El-Geish", "Mohamed", ""]]}, {"id": "2004.07070", "submitter": "Grzegorz Chrupa{\\l}a", "authors": "Grzegorz Chrupa{\\l}a, Bertrand Higy, Afra Alishahi", "title": "Analyzing analytical methods: The case of phonology in neural models of\n  spoken language", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the fast development of analysis techniques for NLP and speech\nprocessing systems, few systematic studies have been conducted to compare the\nstrengths and weaknesses of each method. As a step in this direction we study\nthe case of representations of phonology in neural network models of spoken\nlanguage. We use two commonly applied analytical techniques, diagnostic\nclassifiers and representational similarity analysis, to quantify to what\nextent neural activation patterns encode phonemes and phoneme sequences. We\nmanipulate two factors that can affect the outcome of analysis. First, we\ninvestigate the role of learning by comparing neural activations extracted from\ntrained versus randomly-initialized models. Second, we examine the temporal\nscope of the activations by probing both local activations corresponding to a\nfew milliseconds of the speech signal, and global activations pooled over the\nwhole utterance. We conclude that reporting analysis results with randomly\ninitialized models is crucial, and that global-scope methods tend to yield more\nconsistent results and we recommend their use as a complement to local-scope\ndiagnostic methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 13:04:15 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 07:59:40 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Chrupa\u0142a", "Grzegorz", ""], ["Higy", "Bertrand", ""], ["Alishahi", "Afra", ""]]}, {"id": "2004.07093", "submitter": "Kazuki Miyazawa", "authors": "Kazuki Miyazawa, Tatsuya Aoki, Takato Horii, and Takayuki Nagai", "title": "lamBERT: Language and Action Learning Using Multimodal BERT", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the bidirectional encoder representations from transformers (BERT)\nmodel has attracted much attention in the field of natural language processing,\nowing to its high performance in language understanding-related tasks. The BERT\nmodel learns language representation that can be adapted to various tasks via\npre-training using a large corpus in an unsupervised manner. This study\nproposes the language and action learning using multimodal BERT (lamBERT) model\nthat enables the learning of language and actions by 1) extending the BERT\nmodel to multimodal representation and 2) integrating it with reinforcement\nlearning. To verify the proposed model, an experiment is conducted in a grid\nenvironment that requires language understanding for the agent to act properly.\nAs a result, the lamBERT model obtained higher rewards in multitask settings\nand transfer settings when compared to other models, such as the convolutional\nneural network-based model and the lamBERT model without pre-training.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 13:54:55 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Miyazawa", "Kazuki", ""], ["Aoki", "Tatsuya", ""], ["Horii", "Takato", ""], ["Nagai", "Takayuki", ""]]}, {"id": "2004.07126", "submitter": "Avi Caciularu", "authors": "Oren Barkan, Idan Rejwan, Avi Caciularu, Noam Koenigstein", "title": "Bayesian Hierarchical Words Representation Learning", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Bayesian Hierarchical Words Representation (BHWR)\nlearning algorithm. BHWR facilitates Variational Bayes word representation\nlearning combined with semantic taxonomy modeling via hierarchical priors. By\npropagating relevant information between related words, BHWR utilizes the\ntaxonomy to improve the quality of such representations. Evaluation of several\nlinguistic datasets demonstrates the advantages of BHWR over suitable\nalternatives that facilitate Bayesian modeling with or without semantic priors.\nFinally, we further show that BHWR produces better representations for rare\nwords.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 13:39:52 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Barkan", "Oren", ""], ["Rejwan", "Idan", ""], ["Caciularu", "Avi", ""], ["Koenigstein", "Noam", ""]]}, {"id": "2004.07159", "submitter": "Bin Bi", "authors": "Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei\n  Huang, Luo Si", "title": "PALM: Pre-training an Autoencoding&Autoregressive Language Model for\n  Context-conditioned Generation", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a\npowerful technique for natural language understanding and generation. Existing\npre-training techniques employ autoencoding and/or autoregressive objectives to\ntrain Transformer-based models by recovering original word tokens from\ncorrupted text with some masked tokens. The training goals of existing\ntechniques are often inconsistent with the goals of many language generation\ntasks, such as generative question answering and conversational response\ngeneration, for producing new text given context.\n  This work presents PALM with a novel scheme that jointly pre-trains an\nautoencoding and autoregressive language model on a large unlabeled corpus,\nspecifically designed for generating new text conditioned on context. The new\nscheme alleviates the mismatch introduced by the existing denoising scheme\nbetween pre-training and fine-tuning where generation is more than\nreconstructing original text. An extensive set of experiments show that PALM\nachieves new state-of-the-art results on a variety of language generation\nbenchmarks covering generative question answering (Rank 1 on the official MARCO\nleaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword,\nquestion generation on SQuAD, and conversational response generation on Cornell\nMovie Dialogues.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 06:25:36 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 23:58:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Bi", "Bin", ""], ["Li", "Chenliang", ""], ["Wu", "Chen", ""], ["Yan", "Ming", ""], ["Wang", "Wei", ""], ["Huang", "Songfang", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""]]}, {"id": "2004.07180", "submitter": "Arman Cohan", "authors": "Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S. Weld", "title": "SPECTER: Document-level Representation Learning using Citation-informed\n  Transformers", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Representation learning is a critical ingredient for natural language\nprocessing systems. Recent Transformer language models like BERT learn powerful\ntextual representations, but these models are targeted towards token- and\nsentence-level training objectives and do not leverage information on\ninter-document relatedness, which limits their document-level representation\npower. For applications on scientific documents, such as classification and\nrecommendation, the embeddings power strong performance on end tasks. We\npropose SPECTER, a new method to generate document-level embedding of\nscientific documents based on pretraining a Transformer language model on a\npowerful signal of document-level relatedness: the citation graph. Unlike\nexisting pretrained language models, SPECTER can be easily applied to\ndownstream applications without task-specific fine-tuning. Additionally, to\nencourage further research on document-level models, we introduce SciDocs, a\nnew evaluation benchmark consisting of seven document-level tasks ranging from\ncitation prediction, to document classification and recommendation. We show\nthat SPECTER outperforms a variety of competitive baselines on the benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 16:05:51 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 16:44:20 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 18:59:17 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 17:39:52 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Cohan", "Arman", ""], ["Feldman", "Sergey", ""], ["Beltagy", "Iz", ""], ["Downey", "Doug", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2004.07202", "submitter": "Livio Baldini Soares", "authors": "Thibault F\\'evry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol\n  Choi, Tom Kwiatkowski", "title": "Entities as Experts: Sparse Memory Access with Entity Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We focus on the problem of capturing declarative knowledge about entities in\nthe learned parameters of a language model. We introduce a new model - Entities\nas Experts (EAE) - that can access distinct memories of the entities mentioned\nin a piece of text. Unlike previous efforts to integrate entity knowledge into\nsequence models, EAE's entity representations are learned directly from text.\nWe show that EAE's learned representations capture sufficient knowledge to\nanswer TriviaQA questions such as \"Which Dr. Who villain has been played by\nRoger Delgado, Anthony Ainley, Eric Roberts?\", outperforming an\nencoder-generator Transformer model with 10x the parameters. According to the\nLAMA knowledge probes, EAE contains more factual knowledge than a similarly\nsized BERT, as well as previous approaches that integrate external sources of\nentity knowledge. Because EAE associates parameters with specific entities, it\nonly needs to access a fraction of its parameters at inference time, and we\nshow that the correct identification and representation of entities is\nessential to EAE's performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:00:05 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 19:00:27 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["F\u00e9vry", "Thibault", ""], ["Soares", "Livio Baldini", ""], ["FitzGerald", "Nicholas", ""], ["Choi", "Eunsol", ""], ["Kwiatkowski", "Tom", ""]]}, {"id": "2004.07265", "submitter": "Jiehang Zeng", "authors": "Jiehang Zeng, Lu Liu and Xiaoqing Zheng", "title": "Learning Structured Embeddings of Knowledge Graphs with Adversarial\n  Learning Framework", "comments": "7 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale knowledge graphs are now available and ready to provide\nsemantically structured information that is regarded as an important resource\nfor question answering and decision support tasks. However, they are built on\nrigid symbolic frameworks which makes them hard to be used in other intelligent\nsystems. We present a learning method using generative adversarial architecture\ndesigned to embed the entities and relations of the knowledge graphs into a\ncontinuous vector space. A generative network (GN) takes two elements of a\n(subject, predicate, object) triple as input and generates the vector\nrepresentation of the missing element. A discriminative network (DN) scores a\ntriple to distinguish a positive triple from those generated by GN. The\ntraining goal for GN is to deceive DN to make wrong classification. When\narriving at a convergence, GN recovers the training data and can be used for\nknowledge graph completion, while DN is trained to be a good triple classifier.\nUnlike few previous studies based on generative adversarial architectures, our\nGN is able to generate unseen instances while they just use GN to better choose\nnegative samples (already existed) for DN. Experiments demonstrate our method\ncan improve classical relational learning models (e.g.TransE) with a\nsignificant margin on both the link prediction and triple classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 18:01:36 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Zeng", "Jiehang", ""], ["Liu", "Lu", ""], ["Zheng", "Xiaoqing", ""]]}, {"id": "2004.07324", "submitter": "Pirashanth Ratnamogan", "authors": "Idriss Mghabbar, Pirashanth Ratnamogan", "title": "Building a Multi-domain Neural Machine Translation Model using Knowledge\n  Distillation", "comments": null, "journal-ref": "24th European Conference on Artificial Intelligence (ECAI), 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lack of specialized data makes building a multi-domain neural machine\ntranslation tool challenging. Although emerging literature dealing with low\nresource languages starts to show promising results, most state-of-the-art\nmodels used millions of sentences. Today, the majority of multi-domain\nadaptation techniques are based on complex and sophisticated architectures that\nare not adapted for real-world applications. So far, no scalable method is\nperforming better than the simple yet effective mixed-finetuning, i.e\nfinetuning a generic model with a mix of all specialized data and generic data.\nIn this paper, we propose a new training pipeline where knowledge distillation\nand multiple specialized teachers allow us to efficiently finetune a model\nwithout adding new costs at inference time. Our experiments demonstrated that\nour training pipeline allows improving the performance of multi-domain\ntranslation over finetuning in configurations with 2, 3, and 4 domains by up to\n2 points in BLEU.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 20:21:19 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Mghabbar", "Idriss", ""], ["Ratnamogan", "Pirashanth", ""]]}, {"id": "2004.07347", "submitter": "Wenhu Chen", "authors": "Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William\n  Wang", "title": "HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and\n  Textual Data", "comments": "Accepted to Proceedings of EMNLP 2020 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing question answering datasets focus on dealing with homogeneous\ninformation, based either only on text or KB/Table information alone. However,\nas human knowledge is distributed over heterogeneous forms, using homogeneous\ninformation alone might lead to severe coverage problems. To fill in the gap,\nwe present HybridQA https://github.com/wenhuchen/HybridQA, a new large-scale\nquestion-answering dataset that requires reasoning on heterogeneous\ninformation. Each question is aligned with a Wikipedia table and multiple\nfree-form corpora linked with the entities in the table. The questions are\ndesigned to aggregate both tabular information and text information, i.e., lack\nof either form would render the question unanswerable. We test with three\ndifferent models: 1) a table-only model. 2) text-only model. 3) a hybrid model\nthat combines heterogeneous information to find the answer. The experimental\nresults show that the EM scores obtained by two baselines are below 20\\%, while\nthe hybrid model can achieve an EM over 40\\%. This gap suggests the necessity\nto aggregate heterogeneous information in HybridQA. However, the hybrid model's\nscore is still far behind human performance. Hence, HybridQA can serve as a\nchallenging benchmark to study question answering with heterogeneous\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 21:18:15 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 06:52:11 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 23:29:14 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Chen", "Wenhu", ""], ["Zha", "Hanwen", ""], ["Chen", "Zhiyu", ""], ["Xiong", "Wenhan", ""], ["Wang", "Hong", ""], ["Wang", "William", ""]]}, {"id": "2004.07390", "submitter": "Dominique Larchey-Wendling", "authors": "Dominik Kirst and Dominique Larchey-Wendling", "title": "Trakhtenbrot's Theorem in Coq, A Constructive Approach to Finite Model\n  Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study finite first-order satisfiability (FSAT) in the constructive setting\nof dependent type theory. Employing synthetic accounts of enumerability and\ndecidability, we give a full classification of FSAT depending on the\nfirst-order signature of non-logical symbols. On the one hand, our development\nfocuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as\nthe signature contains an at least binary relation symbol. Our proof proceeds\nby a many-one reduction chain starting from the Post correspondence problem. On\nthe other hand, we establish the decidability of FSAT for monadic first-order\nlogic, i.e. where the signature only contains at most unary function and\nrelation symbols, as well as the enumerability of FSAT for arbitrary enumerable\nsignatures. All our results are mechanised in the framework of a growing Coq\nlibrary of synthetic undecidability proofs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 23:26:04 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Kirst", "Dominik", ""], ["Larchey-Wendling", "Dominique", ""]]}, {"id": "2004.07426", "submitter": "Kai Chen", "authors": "Kai Chen, Fayuan Li, Baotian Hu, Weihua Peng, Qingcai Chen and Hong Yu", "title": "Neural Data-to-Text Generation with Dynamic Content Planning", "comments": "25 pages, 1 figure and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural data-to-text generation models have achieved significant advancement\nin recent years. However, these models have two shortcomings: the generated\ntexts tend to miss some vital information, and they often generate descriptions\nthat are not consistent with the structured input data. To alleviate these\nproblems, we propose a Neural data-to-text generation model with Dynamic\ncontent Planning, named NDP for abbreviation. The NDP can utilize the\npreviously generated text to dynamically select the appropriate entry from the\ngiven structured data. We further design a reconstruction mechanism with a\nnovel objective function that can reconstruct the whole entry of the used data\nsequentially from the hidden states of the decoder, which aids the accuracy of\nthe generated text. Empirical results show that the NDP achieves superior\nperformance over the state-of-the-art on ROTOWIRE dataset, in terms of relation\ngeneration (RG), content selection (CS), content ordering (CO) and BLEU\nmetrics. The human evaluation result shows that the texts generated by the\nproposed NDP are better than the corresponding ones generated by NCP in most of\ntime. And using the proposed reconstruction mechanism, the fidelity of the\ngenerated text can be further improved significantly.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 02:50:51 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 03:28:46 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Chen", "Kai", ""], ["Li", "Fayuan", ""], ["Hu", "Baotian", ""], ["Peng", "Weihua", ""], ["Chen", "Qingcai", ""], ["Yu", "Hong", ""]]}, {"id": "2004.07437", "submitter": "Chitwan Saharia", "authors": "Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi", "title": "Non-Autoregressive Machine Translation with Latent Alignments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two strong methods, CTC and Imputer, for\nnon-autoregressive machine translation that model latent alignments with\ndynamic programming. We revisit CTC for machine translation and demonstrate\nthat a simple CTC model can achieve state-of-the-art for single-step\nnon-autoregressive machine translation, contrary to what prior work indicates.\nIn addition, we adapt the Imputer model for non-autoregressive machine\ntranslation and demonstrate that Imputer with just 4 generation steps can match\nthe performance of an autoregressive Transformer baseline. Our latent alignment\nmodels are simpler than many existing non-autoregressive translation baselines;\nfor example, we do not require target length prediction or re-scoring with an\nautoregressive model. On the competitive WMT'14 En$\\rightarrow$De task, our CTC\nmodel achieves 25.7 BLEU with a single generation step, while Imputer achieves\n27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This\ncompares favourably to the autoregressive Transformer baseline at 27.8 BLEU.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 03:45:56 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 17:34:48 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 13:08:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Saharia", "Chitwan", ""], ["Chan", "William", ""], ["Saxena", "Saurabh", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "2004.07453", "submitter": "Gabriel Stanovsky", "authors": "Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge and\n  Noah A. Smith", "title": "The Right Tool for the Job: Matching Model and Instance Complexities", "comments": "ACL 2020; 12 pages; code available in\n  https://github.com/allenai/sledgehammer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As NLP models become larger, executing a trained model requires significant\ncomputational resources incurring monetary and environmental costs. To better\nrespect a given inference budget, we propose a modification to contextual\nrepresentation fine-tuning which, during inference, allows for an early (and\nfast) \"exit\" from neural network calculations for simple instances, and late\n(and accurate) exit for hard instances. To achieve this, we add classifiers to\ndifferent layers of BERT and use their calibrated confidence scores to make\nearly exit decisions. We test our proposed modification on five different\ndatasets in two tasks: three text classification datasets and two natural\nlanguage inference benchmarks. Our method presents a favorable speed/accuracy\ntradeoff in almost all cases, producing models which are up to five times\nfaster than the state of the art, while preserving their accuracy. Our method\nalso requires almost no additional training resources (in either time or\nparameters) compared to the baseline BERT model. Finally, our method alleviates\nthe need for costly retraining of multiple models at different levels of\nefficiency; we allow users to control the inference speed/accuracy tradeoff\nusing a single trained model, by setting a single variable at inference time.\nWe publicly release our code.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 04:28:08 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 03:45:10 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Schwartz", "Roy", ""], ["Stanovsky", "Gabriel", ""], ["Swayamdipta", "Swabha", ""], ["Dodge", "Jesse", ""], ["Smith", "Noah A.", ""]]}, {"id": "2004.07462", "submitter": "Silin Gao", "authors": "Silin Gao, Yichi Zhang, Zhijian Ou and Zhou Yu", "title": "Paraphrase Augmented Task-Oriented Dialog Generation", "comments": "Accepted to ACL 2020, 10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural generative models have achieved promising performance on dialog\ngeneration tasks if given a huge data set. However, the lack of high-quality\ndialog data and the expensive data annotation process greatly limit their\napplication in real-world settings. We propose a paraphrase augmented response\ngeneration (PARG) framework that jointly trains a paraphrase model and a\nresponse generation model to improve the dialog generation performance. We also\ndesign a method to automatically construct paraphrase training data set based\non dialog state and dialog act labels. PARG is applicable to various dialog\ngeneration models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al.,\n2019). Experimental results show that the proposed framework improves these\nstate-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also\nsignificantly outperforms other data augmentation methods in dialog generation\ntasks, especially under low resource settings.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 05:12:36 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 12:26:36 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Gao", "Silin", ""], ["Zhang", "Yichi", ""], ["Ou", "Zhijian", ""], ["Yu", "Zhou", ""]]}, {"id": "2004.07493", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Dong-Ho Lee, Ming Shen, Ryan Moreno, Xiao Huang,\n  Prashant Shiralkar, Xiang Ren", "title": "TriggerNER: Learning with Entity Triggers as Explanations for Named\n  Entity Recognition", "comments": "Accepted to the ACL 2020. Project page:\n  https://inklab.usc.edu/TriggerNER/ (Fixed a few typos and added a new\n  figure.)", "journal-ref": "Proc. of ACL 2020, page 8503--8511", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural models for named entity recognition (NER) in a new domain\noften requires additional human annotations (e.g., tens of thousands of labeled\ninstances) that are usually expensive and time-consuming to collect. Thus, a\ncrucial research question is how to obtain supervision in a cost-effective way.\nIn this paper, we introduce \"entity triggers,\" an effective proxy of human\nexplanations for facilitating label-efficient learning of NER models. An entity\ntrigger is defined as a group of words in a sentence that helps to explain why\nhumans would recognize an entity in the sentence.\n  We crowd-sourced 14k entity triggers for two well-studied NER datasets. Our\nproposed model, Trigger Matching Network, jointly learns trigger\nrepresentations and soft matching module with self-attention such that can\ngeneralize to unseen sentences easily for tagging. Our framework is\nsignificantly more cost-effective than the traditional neural NER frameworks.\nExperiments show that using only 20% of the trigger-annotated sentences results\nin a comparable performance as using 70% of conventional annotated sentences.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 07:27:43 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 10:06:37 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 07:43:25 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 01:10:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["Lee", "Dong-Ho", ""], ["Shen", "Ming", ""], ["Moreno", "Ryan", ""], ["Huang", "Xiao", ""], ["Shiralkar", "Prashant", ""], ["Ren", "Xiang", ""]]}, {"id": "2004.07499", "submitter": "Bill Yuchen Lin", "authors": "Dong-Ho Lee, Rahul Khanna, Bill Yuchen Lin, Jamin Chen, Seyeon Lee,\n  Qinyuan Ye, Elizabeth Boschee, Leonardo Neves, Xiang Ren", "title": "LEAN-LIFE: A Label-Efficient Annotation Framework Towards Learning from\n  Explanation", "comments": "Accepted to the ACL 2020 (demo). The first two authors contributed\n  equally. Project page: http://inklab.usc.edu/leanlife/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successfully training a deep neural network demands a huge corpus of labeled\ndata. However, each label only provides limited information to learn from and\ncollecting the requisite number of labels involves massive human effort. In\nthis work, we introduce LEAN-LIFE, a web-based, Label-Efficient AnnotatioN\nframework for sequence labeling and classification tasks, with an easy-to-use\nUI that not only allows an annotator to provide the needed labels for a task,\nbut also enables LearnIng From Explanations for each labeling decision. Such\nexplanations enable us to generate useful additional labeled data from\nunlabeled instances, bolstering the pool of available training data. On three\npopular NLP tasks (named entity recognition, relation extraction, sentiment\nanalysis), we find that using this enhanced supervision allows our models to\nsurpass competitive baseline F1 scores by more than 5-10 percentage points,\nwhile using 2X times fewer labeled instances. Our framework is the first to\nutilize this enhanced supervision technique and does so for three important\ntasks -- thus providing improved annotation recommendations to users and an\nability to build datasets of (data, label, explanation) triples instead of the\nregular (data, label) pair.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 07:38:07 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Lee", "Dong-Ho", ""], ["Khanna", "Rahul", ""], ["Lin", "Bill Yuchen", ""], ["Chen", "Jamin", ""], ["Lee", "Seyeon", ""], ["Ye", "Qinyuan", ""], ["Boschee", "Elizabeth", ""], ["Neves", "Leonardo", ""], ["Ren", "Xiang", ""]]}, {"id": "2004.07601", "submitter": "Shaoxiong Ji", "authors": "Shaoxiong Ji, Xue Li, Zi Huang, and Erik Cambria", "title": "Suicidal Ideation and Mental Disorder Detection with Attentive Relation\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health is a critical issue in modern society, and mental disorders\ncould sometimes turn to suicidal ideation without effective treatment. Early\ndetection of mental disorders and suicidal ideation from social content\nprovides a potential way for effective social intervention. However,\nclassifying suicidal ideation and other mental disorders is challenging as they\nshare similar patterns in language usage and sentimental polarity. This paper\nenhances text representation with lexicon-based sentiment scores and latent\ntopics and proposes using relation networks to detect suicidal ideation and\nmental disorders with related risk indicators. The relation module is further\nequipped with the attention mechanism to prioritize more critical relational\nfeatures. Through experiments on three real-world datasets, our model\noutperforms most of its counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 11:18:55 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 08:42:30 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 17:54:28 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ji", "Shaoxiong", ""], ["Li", "Xue", ""], ["Huang", "Zi", ""], ["Cambria", "Erik", ""]]}, {"id": "2004.07623", "submitter": "Ankur Mali", "authors": "Ankur Mali, Alexander Ororbia, Daniel Kifer, Clyde Lee Giles", "title": "Recognizing Long Grammatical Sequences Using Recurrent Networks\n  Augmented With An External Differentiable Stack", "comments": "14 pages, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent neural networks (RNNs) are a widely used deep architecture for\nsequence modeling, generation, and prediction. Despite success in applications\nsuch as machine translation and voice recognition, these stateful models have\nseveral critical shortcomings. Specifically, RNNs generalize poorly over very\nlong sequences, which limits their applicability to many important temporal\nprocessing and time series forecasting problems. For example, RNNs struggle in\nrecognizing complex context free languages (CFLs), never reaching 100% accuracy\non training. One way to address these shortcomings is to couple an RNN with an\nexternal, differentiable memory structure, such as a stack. However,\ndifferentiable memories in prior work have neither been extensively studied on\nCFLs nor tested on sequences longer than those seen in training. The few\nefforts that have studied them have shown that continuous differentiable memory\nstructures yield poor generalization for complex CFLs, making the RNN less\ninterpretable. In this paper, we improve the memory-augmented RNN with\nimportant architectural and state updating mechanisms that ensure that the\nmodel learns to properly balance the use of its latent states with external\nmemory. Our improved RNN models exhibit better generalization performance and\nare able to classify long strings generated by complex hierarchical context\nfree grammars (CFGs). We evaluate our models on CGGs, including the Dyck\nlanguages, as well as on the Penn Treebank language modelling task, and achieve\nstable, robust performance across these benchmarks. Furthermore, we show that\nonly our memory-augmented networks are capable of retaining memory for a longer\nduration up to strings of length 160.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 14:19:15 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 15:36:26 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Mali", "Ankur", ""], ["Ororbia", "Alexander", ""], ["Kifer", "Daniel", ""], ["Giles", "Clyde Lee", ""]]}, {"id": "2004.07633", "submitter": "Jan Deriu", "authors": "Jan Deriu, Katsiaryna Mlynchyk, Philippe Schl\\\"apfer, Alvaro Rodrigo,\n  Dirk von Gr\\\"unigen, Nicolas Kaiser, Kurt Stockinger, Eneko Agirre, and Mark\n  Cieliebak", "title": "A Methodology for Creating Question Answering Corpora Using Inverse Data\n  Annotation", "comments": null, "journal-ref": "Proceedings of the 58th Annual Meeting of the Association for\n  Computational Linguistics. 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel methodology to efficiently construct a\ncorpus for question answering over structured data. For this, we introduce an\nintermediate representation that is based on the logical query plan in a\ndatabase called Operation Trees (OT). This representation allows us to invert\nthe annotation process without losing flexibility in the types of queries that\nwe generate. Furthermore, it allows for fine-grained alignment of query tokens\nto OT operations. In our method, we randomly generate OTs from a context-free\ngrammar. Afterwards, annotators have to write the appropriate natural language\nquestion that is represented by the OT. Finally, the annotators assign the\ntokens to the OT operations. We apply the method to create a new corpus OTTA\n(Operation Trees and Token Assignment), a large semantic parsing corpus for\nevaluating natural language interfaces to databases. We compare OTTA to Spider\nand LC-QuaD 2.0 and show that our methodology more than triples the annotation\nspeed while maintaining the complexity of the queries. Finally, we train a\nstate-of-the-art semantic parsing model on our data and show that our corpus is\na challenging dataset and that the token alignment can be leveraged to increase\nthe performance significantly.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 12:50:01 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 08:13:32 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Deriu", "Jan", ""], ["Mlynchyk", "Katsiaryna", ""], ["Schl\u00e4pfer", "Philippe", ""], ["Rodrigo", "Alvaro", ""], ["von Gr\u00fcnigen", "Dirk", ""], ["Kaiser", "Nicolas", ""], ["Stockinger", "Kurt", ""], ["Agirre", "Eneko", ""], ["Cieliebak", "Mark", ""]]}, {"id": "2004.07642", "submitter": "Robert Litschko", "authors": "Robert Litschko, Ivan Vuli\\'c, \\v{Z}eljko Agi\\'c, Goran Glava\\v{s}", "title": "Towards Instance-Level Parser Selection for Cross-Lingual Transfer of\n  Dependency Parsers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Current methods of cross-lingual parser transfer focus on predicting the best\nparser for a low-resource target language globally, that is, \"at treebank\nlevel\". In this work, we propose and argue for a novel cross-lingual transfer\nparadigm: instance-level parser selection (ILPS), and present a\nproof-of-concept study focused on instance-level selection in the framework of\ndelexicalized parser transfer. We start from an empirical observation that\ndifferent source parsers are the best choice for different Universal POS\nsequences in the target language. We then propose to predict the best parser at\nthe instance level. To this end, we train a supervised regression model, based\non the Transformer architecture, to predict parser accuracies for individual\nPOS-sequences. We compare ILPS against two strong single-best parser selection\nbaselines (SBPS): (1) a model that compares POS n-gram distributions between\nthe source and target languages (KL) and (2) a model that selects the source\nbased on the similarity between manually created language vectors encoding\nsyntactic properties of languages (L2V). The results from our extensive\nevaluation, coupling 42 source parsers and 20 diverse low-resource test\nlanguages, show that ILPS outperforms KL and L2V on 13/20 and 14/20 test\nlanguages, respectively. Further, we show that by predicting the best parser\n\"at the treebank level\" (SBPS), using the aggregation of predictions from our\ninstance-level model, we outperform the same baselines on 17/20 and 16/20 test\nlanguages.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:18:55 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Litschko", "Robert", ""], ["Vuli\u0107", "Ivan", ""], ["Agi\u0107", "\u017deljko", ""], ["Glava\u0161", "Goran", ""]]}, {"id": "2004.07667", "submitter": "Shauli Ravfogel", "authors": "Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav\n  Goldberg", "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace\n  Projection", "comments": "Accepted as a long paper in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to control for the kinds of information encoded in neural\nrepresentation has a variety of use cases, especially in light of the challenge\nof interpreting these models. We present Iterative Null-space Projection\n(INLP), a novel method for removing information from neural representations.\nOur method is based on repeated training of linear classifiers that predict a\ncertain property we aim to remove, followed by projection of the\nrepresentations on their null-space. By doing so, the classifiers become\noblivious to that target property, making it hard to linearly separate the data\naccording to it. While applicable for multiple uses, we evaluate our method on\nbias and fairness use-cases, and show that our method is able to mitigate bias\nin word embeddings, as well as to increase fairness in a setting of multi-class\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:02:50 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 21:09:39 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Ravfogel", "Shauli", ""], ["Elazar", "Yanai", ""], ["Gonen", "Hila", ""], ["Twiton", "Michael", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2004.07672", "submitter": "Haoyu Song", "authors": "Haoyu Song, Yan Wang, Wei-Nan Zhang, Xiaojiang Liu, Ting Liu", "title": "Generate, Delete and Rewrite: A Three-Stage Framework for Improving\n  Persona Consistency of Dialogue Generation", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining a consistent personality in conversations is quite natural for\nhuman beings, but is still a non-trivial task for machines. The persona-based\ndialogue generation task is thus introduced to tackle the\npersonality-inconsistent problem by incorporating explicit persona text into\ndialogue generation models. Despite the success of existing persona-based\nmodels on generating human-like responses, their one-stage decoding framework\ncan hardly avoid the generation of inconsistent persona words. In this work, we\nintroduce a three-stage framework that employs a generate-delete-rewrite\nmechanism to delete inconsistent words from a generated response prototype and\nfurther rewrite it to a personality-consistent one. We carry out evaluations by\nboth human and automatic metrics. Experiments on the Persona-Chat dataset show\nthat our approach achieves good performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:10:24 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 05:58:21 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 03:28:19 GMT"}, {"version": "v4", "created": "Thu, 30 Apr 2020 06:53:44 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Song", "Haoyu", ""], ["Wang", "Yan", ""], ["Zhang", "Wei-Nan", ""], ["Liu", "Xiaojiang", ""], ["Liu", "Ting", ""]]}, {"id": "2004.07683", "submitter": "Tom Bosc", "authors": "Tom Bosc and Pascal Vincent", "title": "Do sequence-to-sequence VAEs learn global features of sentences?", "comments": "Camera-ready version, EMNLP2020", "journal-ref": null, "doi": "10.18653/v1/2020.emnlp-main.350", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Autoregressive language models are powerful and relatively easy to train.\nHowever, these models are usually trained without explicit conditioning labels\nand do not offer easy ways to control global aspects such as sentiment or topic\nduring generation. Bowman & al. (2016) adapted the Variational Autoencoder\n(VAE) for natural language with the sequence-to-sequence architecture and\nclaimed that the latent vector was able to capture such global features in an\nunsupervised manner. We question this claim. We measure which words benefit\nmost from the latent information by decomposing the reconstruction loss per\nposition in the sentence. Using this method, we find that VAEs are prone to\nmemorizing the first words and the sentence length, producing local features of\nlimited usefulness. To alleviate this, we investigate alternative architectures\nbased on bag-of-words assumptions and language model pretraining. These\nvariants learn latent variables that are more global, i.e., more predictive of\ntopic or sentiment labels. Moreover, using reconstructions, we observe that\nthey decrease memorization: the first word and the sentence length are not\nrecovered as accurately than with the baselines, consequently yielding more\ndiverse reconstructions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:43:27 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 18:59:31 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bosc", "Tom", ""], ["Vincent", "Pascal", ""]]}, {"id": "2004.07737", "submitter": "Federico Bianchi", "authors": "Federico Bianchi, Silvia Terragni, Dirk Hovy, Debora Nozza, and\n  Elisabetta Fersini", "title": "Cross-lingual Contextualized Topic Models with Zero-shot Learning", "comments": "Updated version. Published as a conference paper at EACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in\nmultiple languages. They all cover the same content, but the linguistic\ndifferences make it impossible to use traditional, bag-of-word-based topic\nmodels. Models have to be either single-language or suffer from a huge, but\nextremely sparse vocabulary. Both issues can be addressed by transfer learning.\nIn this paper, we introduce a zero-shot cross-lingual topic model. Our model\nlearns topics on one language (here, English), and predicts them for unseen\ndocuments in different languages (here, Italian, French, German, and\nPortuguese). We evaluate the quality of the topic predictions for the same\ndocument in different languages. Our results show that the transferred topics\nare coherent and stable across languages, which suggests exciting future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:21:17 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 16:49:18 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Bianchi", "Federico", ""], ["Terragni", "Silvia", ""], ["Hovy", "Dirk", ""], ["Nozza", "Debora", ""], ["Fersini", "Elisabetta", ""]]}, {"id": "2004.07761", "submitter": "Pengyu Nie", "authors": "Pengyu Nie, Karl Palmskog, Junyi Jessy Li, Milos Gligoric", "title": "Deep Generation of Coq Lemma Names Using Elaborated Terms", "comments": "Accepted in International Joint Conference on Automated Reasoning\n  (IJCAR 2020). With Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coding conventions for naming, spacing, and other essentially stylistic\nproperties are necessary for developers to effectively understand, review, and\nmodify source code in large software projects. Consistent conventions in\nverification projects based on proof assistants, such as Coq, increase in\nimportance as projects grow in size and scope. While conventions can be\ndocumented and enforced manually at high cost, emerging approaches\nautomatically learn and suggest idiomatic names in Java-like languages by\napplying statistical language models on large code corpora. However, due to its\npowerful language extension facilities and fusion of type checking and\ncomputation, Coq is a challenging target for automated learning techniques. We\npresent novel generation models for learning and suggesting lemma names for Coq\nprojects. Our models, based on multi-input neural networks, are the first to\nleverage syntactic and semantic information from Coq's lexer (tokens in lemma\nstatements), parser (syntax trees), and kernel (elaborated terms) for naming;\nthe key insight is that learning from elaborated terms can substantially boost\nmodel performance. We implemented our models in a toolchain, dubbed Roosterize,\nand applied it on a large corpus of code derived from the Mathematical\nComponents family of projects, known for its stringent coding conventions. Our\nresults show that Roosterize substantially outperforms baselines for suggesting\nlemma names, highlighting the importance of using multi-input models and\nelaborated terms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:54:21 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 16:50:17 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Nie", "Pengyu", ""], ["Palmskog", "Karl", ""], ["Li", "Junyi Jessy", ""], ["Gligoric", "Milos", ""]]}, {"id": "2004.07776", "submitter": "Hrafn Loftsson", "authors": "J\\'on Fri{\\dh}rik Da{\\dh}ason, David Erik Mollberg, Hrafn Loftsson,\n  Krist\\'in Bjarnad\\'ottir", "title": "Kvistur 2.0: a BiLSTM Compound Splitter for Icelandic", "comments": "Accepted at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a character-based BiLSTM model for splitting\nIcelandic compound words, and show how varying amounts of training data affects\nthe performance of the model. Compounding is highly productive in Icelandic,\nand new compounds are constantly being created. This results in a large number\nof out-of-vocabulary (OOV) words, negatively impacting the performance of many\nNLP tools. Our model is trained on a dataset of 2.9 million unique word forms\nand their constituent structures from the Database of Icelandic Morphology. The\nmodel learns how to split compound words into two parts and can be used to\nderive the constituent structure of any word form. Knowing the constituent\nstructure of a word form makes it possible to generate the optimal split for a\ngiven task, e.g., a full split for subword tokenization, or, in the case of\npart-of-speech tagging, splitting an OOV word until the largest known\nmorphological head is found. The model outperforms other previously published\nmethods when evaluated on a corpus of manually split word forms. This method\nhas been integrated into Kvistur, an Icelandic compound word analyzer.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:11:02 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Da\u00f0ason", "J\u00f3n Fri\u00f0rik", ""], ["Mollberg", "David Erik", ""], ["Loftsson", "Hrafn", ""], ["Bjarnad\u00f3ttir", "Krist\u00edn", ""]]}, {"id": "2004.07790", "submitter": "Joe Stacey", "authors": "Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Sebastian Riedel,\n  Tim Rockt\\\"aschel", "title": "Avoiding the Hypothesis-Only Bias in Natural Language Inference via\n  Ensemble Adversarial Training", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Inference (NLI) datasets contain annotation artefacts\nresulting in spurious correlations between the natural language utterances and\ntheir respective entailment classes. These artefacts are exploited by neural\nnetworks even when only considering the hypothesis and ignoring the premise,\nleading to unwanted biases. Belinkov et al. (2019b) proposed tackling this\nproblem via adversarial training, but this can lead to learned sentence\nrepresentations that still suffer from the same biases. We show that the bias\ncan be reduced in the sentence representations by using an ensemble of\nadversaries, encouraging the model to jointly decrease the accuracy of these\ndifferent adversaries while fitting the data. This approach produces more\nrobust NLI models, outperforming previous de-biasing efforts when generalised\nto 12 other datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In\naddition, we find that the optimal number of adversarial classifiers depends on\nthe dimensionality of the sentence representations, with larger sentence\nrepresentations being more difficult to de-bias while benefiting from using a\ngreater number of adversaries.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:37:15 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 17:19:14 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 18:47:32 GMT"}, {"version": "v4", "created": "Sat, 10 Oct 2020 17:12:15 GMT"}, {"version": "v5", "created": "Thu, 27 May 2021 17:14:46 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Stacey", "Joe", ""], ["Minervini", "Pasquale", ""], ["Dubossarsky", "Haim", ""], ["Riedel", "Sebastian", ""], ["Rockt\u00e4schel", "Tim", ""]]}, {"id": "2004.07807", "submitter": "Md. Rezaul Karim", "authors": "Md. Rezaul Karim and Bharathi Raja Chakravarthi and John P. McCrae and\n  Michael Cochez", "title": "Classification Benchmarks for Under-resourced Bengali Language based on\n  Multichannel Convolutional-LSTM Network", "comments": "This paper is under review in the Journal of Natural Language\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exponential growths of social media and micro-blogging sites not only provide\nplatforms for empowering freedom of expressions and individual voices but also\nenables people to express anti-social behaviour like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\nthese data for social and anti-social behaviours analysis, document\ncharacterization, and sentiment analysis by predicting the contexts mostly for\nhighly resourced languages such as English. However, there are languages that\nare under-resources, e.g., South Asian languages like Bengali, Tamil, Assamese,\nTelugu that lack of computational resources for the NLP tasks. In this paper,\nwe provide several classification benchmarks for Bengali, an under-resourced\nlanguage. We prepared three datasets of expressing hate, commonly used topics,\nand opinions for hate speech detection, document classification, and sentiment\nanalysis, respectively. We built the largest Bengali word embedding models to\ndate based on 250 million articles, which we call BengFastText. We perform\nthree different experiments, covering document classification, sentiment\nanalysis, and hate speech detection. We incorporate word embeddings into a\nMultichannel Convolutional-LSTM (MConv-LSTM) network for predicting different\ntypes of hate speech, document classification, and sentiment analysis.\nExperiments demonstrate that BengFastText can capture the semantics of words\nfrom respective contexts correctly. Evaluations against several baseline\nembedding models, e.g., Word2Vec and GloVe yield up to 92.30%, 82.25%, and\n90.45% F1-scores in case of document classification, sentiment analysis, and\nhate speech detection, respectively during 5-fold cross-validation tests.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 22:17:04 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 17:21:30 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Karim", "Md. Rezaul", ""], ["Chakravarthi", "Bharathi Raja", ""], ["McCrae", "John P.", ""], ["Cochez", "Michael", ""]]}, {"id": "2004.07820", "submitter": "Sayan Nag", "authors": "Uddalok Sarkar, Soumyadeep Pal, Sayan Nag, Chirayata Bhattacharya,\n  Shankha Sanyal, Archi Banerjee, Ranjan Sengupta and Dipak Ghosh", "title": "Speaker Recognition in Bengali Language from Nonlinear Features", "comments": "arXiv admin note: text overlap with arXiv:1612.00171,\n  arXiv:1601.07709", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  At present Automatic Speaker Recognition system is a very important issue due\nto its diverse applications. Hence, it becomes absolutely necessary to obtain\nmodels that take into consideration the speaking style of a person, vocal tract\ninformation, timbral qualities of his voice and other congenital information\nregarding his voice. The study of Bengali speech recognition and speaker\nidentification is scarce in the literature. Hence the need arises for involving\nBengali subjects in modelling our speaker identification engine. In this work,\nwe have extracted some acoustic features of speech using non linear\nmultifractal analysis. The Multifractal Detrended Fluctuation Analysis reveals\nessentially the complexity associated with the speech signals taken. The source\ncharacteristics have been quantified with the help of different techniques like\nCorrelation Matrix, skewness of MFDFA spectrum etc. The Results obtained from\nthis study gives a good recognition rate for Bengali Speakers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 22:38:54 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Sarkar", "Uddalok", ""], ["Pal", "Soumyadeep", ""], ["Nag", "Sayan", ""], ["Bhattacharya", "Chirayata", ""], ["Sanyal", "Shankha", ""], ["Banerjee", "Archi", ""], ["Sengupta", "Ranjan", ""], ["Ghosh", "Dipak", ""]]}, {"id": "2004.07898", "submitter": "Yufang Hou", "authors": "Yufang Hou", "title": "Bridging Anaphora Resolution as Question Answering", "comments": "accepted at ACL2020. This version is slightly different than the\n  ACL2020 camera-ready version. Thanks for Massimo Poesio's comments, I've made\n  two small changes to describe GNOME and the work in Poesio et al. (2004) more\n  accurately", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous studies on bridging anaphora resolution (Poesio et al., 2004;\nHou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and\nassume that the gold mention information is given. In this paper, we cast\nbridging anaphora resolution as question answering based on context. This\nallows us to find the antecedent for a given anaphor without knowing any gold\nmention information (except the anaphor itself). We present a question\nanswering framework (BARQA) for this task, which leverages the power of\ntransfer learning. Furthermore, we propose a novel method to generate a large\namount of \"quasi-bridging\" training data. We show that our model pre-trained on\nthis dataset and fine-tuned on a small amount of in-domain dataset achieves new\nstate-of-the-art results for bridging anaphora resolution on two bridging\ncorpora (ISNotes (Markert et al., 2012) and BASHI (Roesiger, 2018)).\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 19:42:43 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 17:44:54 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 22:28:10 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Hou", "Yufang", ""]]}, {"id": "2004.07922", "submitter": "Ritu Yadav", "authors": "Ritu Yadav", "title": "Light-Weighted CNN for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For management, documents are categorized into a specific category, and to do\nthese, most of the organizations use manual labor. In today's automation era,\nmanual efforts on such a task are not justified, and to avoid this, we have so\nmany software out there in the market. However, efficiency and minimal resource\nconsumption is the focal point which is also creating a competition. The\ncategorization of such documents into specified classes by machine provides\nexcellent help. One of categorization technique is text classification using a\nConvolutional neural network(TextCNN). TextCNN uses multiple sizes of filters,\nas in the case of the inception layer introduced in Googlenet. The network\nprovides good accuracy but causes high memory consumption due to a large number\nof trainable parameters. As a solution to this problem, we introduced a whole\nnew architecture based on separable convolution. The idea of separable\nconvolution already exists in the field of image classification but not yet\nintroduces to text classification tasks. With the help of this architecture, we\ncan achieve a drastic reduction in trainable parameters.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 20:23:52 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Yadav", "Ritu", ""]]}, {"id": "2004.08013", "submitter": "Niru Maheswaranathan", "authors": "Niru Maheswaranathan, David Sussillo", "title": "How recurrent networks implement contextual processing in sentiment\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have a remarkable capacity for contextual processing--using\nrecent or nearby inputs to modify processing of current input. For example, in\nnatural language, contextual processing is necessary to correctly interpret\nnegation (e.g. phrases such as \"not bad\"). However, our ability to understand\nhow networks process context is limited. Here, we propose general methods for\nreverse engineering recurrent neural networks (RNNs) to identify and elucidate\ncontextual processing. We apply these methods to understand RNNs trained on\nsentiment classification. This analysis reveals inputs that induce contextual\neffects, quantifies the strength and timescale of these effects, and identifies\nsets of these inputs with similar properties. Additionally, we analyze\ncontextual effects related to differential processing of the beginning and end\nof documents. Using the insights learned from the RNNs we improve baseline\nBag-of-Words models with simple extensions that incorporate contextual\nmodification, recovering greater than 90% of the RNN's performance increase\nover the baseline. This work yields a new understanding of how RNNs process\ncontextual information, and provides tools that should provide similar insight\nmore broadly.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 00:58:30 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Maheswaranathan", "Niru", ""], ["Sussillo", "David", ""]]}, {"id": "2004.08022", "submitter": "Piji Li", "authors": "Piji Li, Haisong Zhang, Xiaojiang Liu, Shuming Shi", "title": "SongNet: Rigid Formats Controlled Text Generation", "comments": "ACL2020, 10 pages, code: https://github.com/lipiji/SongNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural text generation has made tremendous progress in various tasks. One\ncommon characteristic of most of the tasks is that the texts are not restricted\nto some rigid formats when generating. However, we may confront some special\ntext paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi\n(classical Chinese poetry of the Song dynasty), etc. The typical\ncharacteristics of these texts are in three folds: (1) They must comply fully\nwith the rigid predefined formats. (2) They must obey some rhyming schemes. (3)\nAlthough they are restricted to some formats, the sentence integrity must be\nguaranteed. To the best of our knowledge, text generation based on the\npredefined rigid formats has not been well investigated. Therefore, we propose\na simple and elegant framework named SongNet to tackle this problem. The\nbackbone of the framework is a Transformer-based auto-regressive language\nmodel. Sets of symbols are tailor-designed to improve the modeling performance\nespecially on format, rhyme, and sentence integrity. We improve the attention\nmechanism to impel the model to capture some future information on the format.\nA pre-training and fine-tuning framework is designed to further improve the\ngeneration quality. Extensive experiments conducted on two collected corpora\ndemonstrate that our proposed framework generates significantly better results\nin terms of both automatic metrics and the human evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 01:40:18 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:49:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Piji", ""], ["Zhang", "Haisong", ""], ["Liu", "Xiaojiang", ""], ["Shi", "Shuming", ""]]}, {"id": "2004.08031", "submitter": "David R Mortensen", "authors": "David R. Mortensen, Xinjian Li, Patrick Littell, Alexis Michaud,\n  Shruti Rijhwani, Antonios Anastasopoulos, Alan W. Black, Florian Metze,\n  Graham Neubig", "title": "AlloVera: A Multilingual Allophone Database", "comments": "8 pages, LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new resource, AlloVera, which provides mappings from 218\nallophones to phonemes for 14 languages. Phonemes are contrastive phonological\nunits, and allophones are their various concrete realizations, which are\npredictable from phonological context. While phonemic representations are\nlanguage specific, phonetic representations (stated in terms of (allo)phones)\nare much closer to a universal (language-independent) transcription. AlloVera\nallows the training of speech recognition models that output phonetic\ntranscriptions in the International Phonetic Alphabet (IPA), regardless of the\ninput language. We show that a \"universal\" allophone model, Allosaurus, built\nwith AlloVera, outperforms \"universal\" phonemic models and language-specific\nmodels on a speech-transcription task. We explore the implications of this\ntechnology (and related technologies) for the documentation of endangered and\nminority languages. We further explore other applications for which AlloVera\nwill be suitable as it grows, including phonological typology.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 02:02:18 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Mortensen", "David R.", ""], ["Li", "Xinjian", ""], ["Littell", "Patrick", ""], ["Michaud", "Alexis", ""], ["Rijhwani", "Shruti", ""], ["Anastasopoulos", "Antonios", ""], ["Black", "Alan W.", ""], ["Metze", "Florian", ""], ["Neubig", "Graham", ""]]}, {"id": "2004.08046", "submitter": "Dongyu Ru", "authors": "Dongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mingxuan Wang, Weinan\n  Zhang, Yong Yu, Lei Li", "title": "Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete\n  Space", "comments": "Accepted to EMNLP 2020 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning for sentence understanding aims at discovering informative\nunlabeled data for annotation and therefore reducing the demand for labeled\ndata. We argue that the typical uncertainty sampling method for active learning\nis time-consuming and can hardly work in real-time, which may lead to\nineffective sample selection. We propose adversarial uncertainty sampling in\ndiscrete space (AUSDS) to retrieve informative unlabeled samples more\nefficiently. AUSDS maps sentences into latent space generated by the popular\npre-trained language models, and discover informative unlabeled text samples\nfor annotation via adversarial attack. The proposed approach is extremely\nefficient compared with traditional uncertainty sampling with more than 10x\nspeedup. Experimental results on five datasets show that AUSDS outperforms\nstrong baselines on effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 03:12:34 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 04:45:49 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Ru", "Dongyu", ""], ["Feng", "Jiangtao", ""], ["Qiu", "Lin", ""], ["Zhou", "Hao", ""], ["Wang", "Mingxuan", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""], ["Li", "Lei", ""]]}, {"id": "2004.08053", "submitter": "Marta R. Costa-juss\\`a", "authors": "Jordi Armengol-Estap\\'e, Marta R. Costa-juss\\`a, Carlos Escolano", "title": "Enriching the Transformer with Linguistic Factors for Low-Resource\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introducing factors, that is to say, word features such as linguistic\ninformation referring to the source tokens, is known to improve the results of\nneural machine translation systems in certain settings, typically in recurrent\narchitectures. This study proposes enhancing the current state-of-the-art\nneural machine translation architecture, the Transformer, so that it allows to\nintroduce external knowledge. In particular, our proposed modification, the\nFactored Transformer, uses linguistic factors that insert additional knowledge\ninto the machine translation system. Apart from using different kinds of\nfeatures, we study the effect of different architectural configurations.\nSpecifically, we analyze the performance of combining words and features at the\nembedding level or at the encoder level, and we experiment with two different\ncombination strategies. With the best-found configuration, we show improvements\nof 0.8 BLEU over the baseline Transformer in the IWSLT German-to-English task.\nMoreover, we experiment with the more challenging FLoRes English-to-Nepali\nbenchmark, which includes both extremely low-resourced and very distant\nlanguages, and obtain an improvement of 1.2 BLEU.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 03:40:13 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 09:06:18 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Armengol-Estap\u00e9", "Jordi", ""], ["Costa-juss\u00e0", "Marta R.", ""], ["Escolano", "Carlos", ""]]}, {"id": "2004.08056", "submitter": "Kai Sun", "authors": "Dian Yu, Kai Sun, Claire Cardie, Dong Yu", "title": "Dialogue-Based Relation Extraction", "comments": "To appear in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first human-annotated dialogue-based relation extraction (RE)\ndataset DialogRE, aiming to support the prediction of relation(s) between two\narguments that appear in a dialogue. We further offer DialogRE as a platform\nfor studying cross-sentence RE as most facts span multiple sentences. We argue\nthat speaker-related information plays a critical role in the proposed task,\nbased on an analysis of similarities and differences between dialogue-based and\ntraditional RE tasks. Considering the timeliness of communication in a\ndialogue, we design a new metric to evaluate the performance of RE methods in a\nconversational setting and investigate the performance of several\nrepresentative RE methods on DialogRE. Experimental results demonstrate that a\nspeaker-aware extension on the best-performing model leads to gains in both the\nstandard and conversational evaluation settings. DialogRE is available at\nhttps://dataset.org/dialogre/.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 03:51:57 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Yu", "Dian", ""], ["Sun", "Kai", ""], ["Cardie", "Claire", ""], ["Yu", "Dong", ""]]}, {"id": "2004.08070", "submitter": "Alasdair Tran", "authors": "Alasdair Tran, Alexander Mathews, Lexing Xie", "title": "Transform and Tell: Entity-Aware News Image Captioning", "comments": "Published in CVPR 2020. Code is available at\n  https://github.com/alasdairtran/transform-and-tell and demo is available at\n  https://transform-and-tell.ml", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 13035-13045", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end model which generates captions for images embedded\nin news articles. News images present two key challenges: they rely on\nreal-world knowledge, especially about named entities; and they typically have\nlinguistically rich captions that include uncommon words. We address the first\nchallenge by associating words in the caption with faces and objects in the\nimage, via a multi-modal, multi-head attention mechanism. We tackle the second\nchallenge with a state-of-the-art transformer language model that uses\nbyte-pair-encoding to generate captions as a sequence of word parts. On the\nGoodNews dataset, our model outperforms the previous state of the art by a\nfactor of four in CIDEr score (13 to 54). This performance gain comes from a\nunique combination of language models, word representation, image embeddings,\nface embeddings, object embeddings, and improvements in neural network design.\nWe also introduce the NYTimes800k dataset which is 70% larger than GoodNews,\nhas higher article quality, and includes the locations of images within\narticles as an additional contextual cue.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 05:44:37 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 01:21:14 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Tran", "Alasdair", ""], ["Mathews", "Alexander", ""], ["Xie", "Lexing", ""]]}, {"id": "2004.08076", "submitter": "Amrith Krishna", "authors": "Amrith Krishna, Ashim Gupta, Deepak Garasangi, Jivnesh Sandhan,\n  Pavankumar Satuluri, Pawan Goyal", "title": "Neural Approaches for Data Driven Dependency Parsing in Sanskrit", "comments": "submitted to WSC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven approaches for dependency parsing have been of great interest in\nNatural Language Processing for the past couple of decades. However, Sanskrit\nstill lacks a robust purely data-driven dependency parser, probably with an\nexception to Krishna (2019). This can primarily be attributed to the lack of\navailability of task-specific labelled data and the morphologically rich nature\nof the language. In this work, we evaluate four different data-driven machine\nlearning models, originally proposed for different languages, and compare their\nperformances on Sanskrit data. We experiment with 2 graph based and 2\ntransition based parsers. We compare the performance of each of the models in a\nlow-resource setting, with 1,500 sentences for training. Further, since our\nfocus is on the learning power of each of the models, we do not incorporate any\nSanskrit specific features explicitly into the models, and rather use the\ndefault settings in each of the paper for obtaining the feature functions. In\nthis work, we analyse the performance of the parsers using both an in-domain\nand an out-of-domain test dataset. We also investigate the impact of word\nordering in which the sentences are provided as input to these systems, by\nparsing verses and their corresponding prose order (anvaya) sentences.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 06:47:15 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Krishna", "Amrith", ""], ["Gupta", "Ashim", ""], ["Garasangi", "Deepak", ""], ["Sandhan", "Jivnesh", ""], ["Satuluri", "Pavankumar", ""], ["Goyal", "Pawan", ""]]}, {"id": "2004.08097", "submitter": "Joongbo Shin", "authors": "Joongbo Shin, Yoonhyung Lee, Seunghyun Yoon, Kyomin Jung", "title": "Fast and Accurate Deep Bidirectional Language Representations for\n  Unsupervised Learning", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though BERT achieves successful performance improvements in various\nsupervised learning tasks, applying BERT for unsupervised tasks still holds a\nlimitation that it requires repetitive inference for computing contextual\nlanguage representations. To resolve the limitation, we propose a novel deep\nbidirectional language model called Transformer-based Text Autoencoder (T-TA).\nThe T-TA computes contextual language representations without repetition and\nhas benefits of the deep bidirectional architecture like BERT. In run-time\nexperiments on CPU environments, the proposed T-TA performs over six times\nfaster than the BERT-based model in the reranking task and twelve times faster\nin the semantic similarity task. Furthermore, the T-TA shows competitive or\neven better accuracies than those of BERT on the above tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 07:43:38 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Shin", "Joongbo", ""], ["Lee", "Yoonhyung", ""], ["Yoon", "Seunghyun", ""], ["Jung", "Kyomin", ""]]}, {"id": "2004.08114", "submitter": "Philip John Gorinski", "authors": "Gabriel Gordon-Hall, Philip John Gorinski, Gerasimos Lampouras,\n  Ignacio Iacobacci", "title": "Show Us the Way: Learning to Manage Dialog from Demonstrations", "comments": "8 pages + 2 pages references, 4 figures, 4 tables, accepted to DSTC8\n  Workshop at AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our submission to the End-to-End Multi-Domain Dialog Challenge\nTrack of the Eighth Dialog System Technology Challenge. Our proposed dialog\nsystem adopts a pipeline architecture, with distinct components for Natural\nLanguage Understanding, Dialog State Tracking, Dialog Management and Natural\nLanguage Generation. At the core of our system is a reinforcement learning\nalgorithm which uses Deep Q-learning from Demonstrations to learn a dialog\npolicy with the help of expert examples. We find that demonstrations are\nessential to training an accurate dialog policy where both state and action\nspaces are large. Evaluation of our Dialog Management component shows that our\napproach is effective - beating supervised and reinforcement learning\nbaselines.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:41:54 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Gordon-Hall", "Gabriel", ""], ["Gorinski", "Philip John", ""], ["Lampouras", "Gerasimos", ""], ["Iacobacci", "Ignacio", ""]]}, {"id": "2004.08123", "submitter": "Mathis Linger", "authors": "Mathis Linger and Mhamed Hajaiej", "title": "Batch Clustering for Multilingual News Streaming", "comments": "7 pages, 2 figures", "journal-ref": "Proceedings of Text2Story - Third Workshop on Narrative Extraction\n  From Texts co-located with 42nd European Conference on Information Retrieval\n  (ECIR 2020) Lisbon, Portugal, April 14th, 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, digital news articles are widely available, published by various\neditors and often written in different languages. This large volume of diverse\nand unorganized information makes human reading very difficult or almost\nimpossible. This leads to a need for algorithms able to arrange high amount of\nmultilingual news into stories. To this purpose, we extend previous works on\nTopic Detection and Tracking, and propose a new system inspired from newsLens.\nWe process articles per batch, looking for monolingual local topics which are\nthen linked across time and languages. Here, we introduce a novel \"replaying\"\nstrategy to link monolingual local topics into stories. Besides, we propose new\nfine tuned multilingual embedding using SBERT to create crosslingual stories.\nOur system gives monolingual state-of-the-art results on dataset of Spanish and\nGerman news and crosslingual state-of-the-art results on English, Spanish and\nGerman news.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:59:13 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Linger", "Mathis", ""], ["Hajaiej", "Mhamed", ""]]}, {"id": "2004.08134", "submitter": "Leonhard Hennig", "authors": "Christoph Alt and Aleksandra Gabryszak and Leonhard Hennig", "title": "Probing Linguistic Features of Sentence-Level Representations in Neural\n  Relation Extraction", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent progress, little is known about the features captured by\nstate-of-the-art neural relation extraction (RE) models. Common methods encode\nthe source sentence, conditioned on the entity mentions, before classifying the\nrelation. However, the complexity of the task makes it difficult to understand\nhow encoder architecture and supporting linguistic knowledge affect the\nfeatures learned by the encoder. We introduce 14 probing tasks targeting\nlinguistic properties relevant to RE, and we use them to study representations\nlearned by more than 40 different encoder architecture and linguistic feature\ncombinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find\nthat the bias induced by the architecture and the inclusion of linguistic\nfeatures are clearly expressed in the probing task performance. For example,\nadding contextualized word representations greatly increases performance on\nprobing tasks with a focus on named entity and part-of-speech information, and\nyields better results in RE. In contrast, entity masking improves RE, but\nconsiderably lowers performance on entity type related probing tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 09:17:40 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Alt", "Christoph", ""], ["Gabryszak", "Aleksandra", ""], ["Hennig", "Leonhard", ""]]}, {"id": "2004.08166", "submitter": "Yavuz Selim Kartal", "authors": "Yavuz Selim Kartal, Busra Guvenen and Mucahid Kutlu", "title": "Too Many Claims to Fact-Check: Prioritizing Political Claims Based on\n  Check-Worthiness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive amount of misinformation spreading on the Internet on a daily\nbasis has enormous negative impacts on societies. Therefore, we need automated\nsystems helping fact-checkers in the combat against misinformation. In this\npaper, we propose a model prioritizing the claims based on their\ncheck-worthiness. We use BERT model with additional features including\ndomain-specific controversial topics, word embeddings, and others. In our\nexperiments, we show that our proposed model outperforms all state-of-the-art\nmodels in both test collections of CLEF Check That! Lab in 2018 and 2019. We\nalso conduct a qualitative analysis to shed light-detecting check-worthy\nclaims. We suggest requesting rationales behind judgments are needed to\nunderstand subjective nature of the task and problematic labels.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 10:55:07 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 20:33:58 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kartal", "Yavuz Selim", ""], ["Guvenen", "Busra", ""], ["Kutlu", "Mucahid", ""]]}, {"id": "2004.08178", "submitter": "Yekun Chai", "authors": "Yekun Chai, Shuo Jin, Xinwen Hou", "title": "Highway Transformer: Self-Gating Enhanced Self-Attentive Networks", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention mechanisms have made striking state-of-the-art (SOTA) progress\nin various sequence learning tasks, standing on the multi-headed dot product\nattention by attending to all the global contexts at different locations.\nThrough a pseudo information highway, we introduce a gated component\nself-dependency units (SDU) that incorporates LSTM-styled gating units to\nreplenish internal semantic importance within the multi-dimensional latent\nspace of individual representations. The subsidiary content-based SDU gates\nallow for the information flow of modulated latent embeddings through skipped\nconnections, leading to a clear margin of convergence speed with gradient\ndescent algorithms. We may unveil the role of gating mechanism to aid in the\ncontext-based Transformer modules, with hypothesizing that SDU gates,\nespecially on shallow layers, could push it faster to step towards suboptimal\npoints during the optimization process.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 11:25:07 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 08:57:04 GMT"}, {"version": "v3", "created": "Sun, 24 May 2020 08:43:45 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2020 15:09:28 GMT"}, {"version": "v5", "created": "Tue, 24 Nov 2020 16:19:49 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Chai", "Yekun", ""], ["Jin", "Shuo", ""], ["Hou", "Xinwen", ""]]}, {"id": "2004.08202", "submitter": "Bennett Kleinberg", "authors": "Isabelle van der Vegt, Bennett Kleinberg", "title": "Women worry about family, men about the economy: Gender differences in\n  emotional responses to COVID-19", "comments": "To appear in SocInfo 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among the critical challenges around the COVID-19 pandemic is dealing with\nthe potentially detrimental effects on people's mental health. Designing\nappropriate interventions and identifying the concerns of those most at risk\nrequires methods that can extract worries, concerns and emotional responses\nfrom text data. We examine gender differences and the effect of document length\non worries about the ongoing COVID-19 situation. Our findings suggest that i)\nshort texts do not offer as adequate insights into psychological processes as\nlonger texts. We further find ii) marked gender differences in topics\nconcerning emotional responses. Women worried more about their loved ones and\nsevere health concerns while men were more occupied with effects on the economy\nand society. This paper adds to the understanding of general gender differences\nin language found elsewhere, and shows that the current unique circumstances\nlikely amplified these effects. We close this paper with a call for more\nhigh-quality datasets due to the limitations of Tweet-sized data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 12:23:46 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 11:13:00 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["van der Vegt", "Isabelle", ""], ["Kleinberg", "Bennett", ""]]}, {"id": "2004.08205", "submitter": "Nikolaos Lykousas", "authors": "Nikolaos Lykousas, Constantinos Patsakis", "title": "Large-scale analysis of grooming in modern social networks", "comments": "Pre-print. Under Review. arXiv admin note: text overlap with\n  arXiv:1911.08370 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks are evolving to engage their users more by providing them\nwith more functionalities. One of the most attracting ones is streaming. Users\nmay broadcast part of their daily lives to thousands of others world-wide and\ninteract with them in real-time. Unfortunately, this feature is reportedly\nexploited for grooming. In this work, we provide the first in-depth analysis of\nthis problem for social live streaming services. More precisely, using a\ndataset that we collected, we identify predatory behaviours and grooming on\nchats that bypassed the moderation mechanisms of the LiveMe, the service under\ninvestigation. Beyond the traditional text approaches, we also investigate the\nrelevance of emojis in this context, as well as the user interactions through\nthe gift mechanisms of LiveMe. Finally, our analysis indicates the possibility\nof grooming towards minors, showing the extent of the problem in such\nplatforms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:23:13 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Lykousas", "Nikolaos", ""], ["Patsakis", "Constantinos", ""]]}, {"id": "2004.08243", "submitter": "Pratik Jawanpuria", "authors": "Pratik Jawanpuria, Mayank Meghwanshi, Bamdev Mishra", "title": "Geometry-aware Domain Adaptation for Unsupervised Alignment of Word\n  Embeddings", "comments": "Accepted as a short paper in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel manifold based geometric approach for learning\nunsupervised alignment of word embeddings between the source and the target\nlanguages. Our approach formulates the alignment learning problem as a domain\nadaptation problem over the manifold of doubly stochastic matrices. This\nviewpoint arises from the aim to align the second order information of the two\nlanguage spaces. The rich geometry of the doubly stochastic manifold allows to\nemploy efficient Riemannian conjugate gradient algorithm for the proposed\nformulation. Empirically, the proposed approach outperforms state-of-the-art\noptimal transport based approach on the bilingual lexicon induction task across\nseveral language pairs. The performance improvement is more significant for\ndistant language pairs.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 04:41:06 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 14:48:45 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Jawanpuria", "Pratik", ""], ["Meghwanshi", "Mayank", ""], ["Mishra", "Bamdev", ""]]}, {"id": "2004.08249", "submitter": "Liyuan Liu", "authors": "Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Jiawei Han", "title": "Understanding the Difficulty of Training Transformers", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have proved effective in many NLP tasks. However, their training\nrequires non-trivial efforts regarding designing cutting-edge optimizers and\nlearning rate schedulers carefully (e.g., conventional SGD fails to train\nTransformers effectively). Our objective here is to understand $\\textit{what\ncomplicates Transformer training}$ from both empirical and theoretical\nperspectives. Our analysis reveals that unbalanced gradients are not the root\ncause of the instability of training. Instead, we identify an amplification\neffect that influences training substantially -- for each layer in a\nmulti-layer Transformer model, heavy dependency on its residual branch makes\ntraining unstable, since it amplifies small parameter perturbations (e.g.,\nparameter updates) and results in significant disturbances in the model output.\nYet we observe that a light dependency limits the model potential and leads to\ninferior trained models. Inspired by our analysis, we propose Admin\n($\\textbf{Ad}$aptive $\\textbf{m}$odel $\\textbf{in}$itialization) to stabilize\nstabilize the early stage's training and unleash its full potential in the late\nstage. Extensive experiments show that Admin is more stable, converges faster,\nand leads to better performance. Implementations are released at:\nhttps://github.com/LiyuanLucasLiu/Transforemr-Clinic.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 13:59:07 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 05:05:56 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Liu", "Liyuan", ""], ["Liu", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Chen", "Weizhu", ""], ["Han", "Jiawei", ""]]}, {"id": "2004.08299", "submitter": "Hwanhee Lee", "authors": "Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung\n  Bui and Kyomin Jung", "title": "DSTC8-AVSD: Multimodal Semantic Transformer Network with Retrieval Style\n  Word Generator", "comments": "Presented at DSTC Workshop @ AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Visual Scene-aware Dialog (AVSD) is the task of generating a response\nfor a question with a given scene, video, audio, and the history of previous\nturns in the dialog. Existing systems for this task employ the transformers or\nrecurrent neural network-based architecture with the encoder-decoder framework.\nEven though these techniques show superior performance for this task, they have\nsignificant limitations: the model easily overfits only to memorize the\ngrammatical patterns; the model follows the prior distribution of the\nvocabularies in a dataset. To alleviate the problems, we propose a Multimodal\nSemantic Transformer Network. It employs a transformer-based architecture with\nan attention-based word embedding layer that generates words by querying word\nembeddings. With this design, our model keeps considering the meaning of the\nwords at the generation stage. The empirical results demonstrate the\nsuperiority of our proposed model that outperforms most of the previous works\nfor the AVSD task.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 07:10:08 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Lee", "Hwanhee", ""], ["Yoon", "Seunghyun", ""], ["Dernoncourt", "Franck", ""], ["Kim", "Doo Soon", ""], ["Bui", "Trung", ""], ["Jung", "Kyomin", ""]]}, {"id": "2004.08301", "submitter": "Koujin Takeda", "authors": "Hiroki Kitano, Koujin Takeda", "title": "Belief Propagation for Maximum Coverage on Weighted Bipartite Graph and\n  Application to Text Summarization", "comments": "4 pages, 4 figures", "journal-ref": "J. Phys. Soc. Jpn. 89, 043801 (2020)", "doi": "10.7566/JPSJ.89.043801", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study text summarization from the viewpoint of maximum coverage problem.\nIn graph theory, the task of text summarization is regarded as maximum coverage\nproblem on bipartite graph with weighted nodes. In recent study,\nbelief-propagation based algorithm for maximum coverage on unweighted graph was\nproposed using the idea of statistical mechanics. We generalize it to weighted\ngraph for text summarization. Then we apply our algorithm to weighted biregular\nrandom graph for verification of maximum coverage performance. We also apply it\nto bipartite graph representing real document in open text dataset, and check\nthe performance of text summarization. As a result, our algorithm exhibits\nbetter performance than greedy-type algorithm in some setting of text\nsummarization.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 05:50:20 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Kitano", "Hiroki", ""], ["Takeda", "Koujin", ""]]}, {"id": "2004.08326", "submitter": "Chenglin Xu", "authors": "Chenglin Xu, Wei Rao, Eng Siong Chng and Haizhou Li", "title": "SpEx: Multi-Scale Time Domain Speaker Extraction Network", "comments": "ACCEPTED in IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing (TASLP)", "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  2020", "doi": "10.1109/TASLP.2020.2987429", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker extraction aims to mimic humans' selective auditory attention by\nextracting a target speaker's voice from a multi-talker environment. It is\ncommon to perform the extraction in frequency-domain, and reconstruct the\ntime-domain signal from the extracted magnitude and estimated phase spectra.\nHowever, such an approach is adversely affected by the inherent difficulty of\nphase estimation. Inspired by Conv-TasNet, we propose a time-domain speaker\nextraction network (SpEx) that converts the mixture speech into multi-scale\nembedding coefficients instead of decomposing the speech signal into magnitude\nand phase spectra. In this way, we avoid phase estimation. The SpEx network\nconsists of four network components, namely speaker encoder, speech encoder,\nspeaker extractor, and speech decoder. Specifically, the speech encoder\nconverts the mixture speech into multi-scale embedding coefficients, the\nspeaker encoder learns to represent the target speaker with a speaker\nembedding. The speaker extractor takes the multi-scale embedding coefficients\nand target speaker embedding as input and estimates a receptive mask. Finally,\nthe speech decoder reconstructs the target speaker's speech from the masked\nembedding coefficients. We also propose a multi-task learning framework and a\nmulti-scale embedding implementation. Experimental results show that the\nproposed SpEx achieves 37.3%, 37.7% and 15.0% relative improvements over the\nbest baseline in terms of signal-to-distortion ratio (SDR), scale-invariant SDR\n(SI-SDR), and perceptual evaluation of speech quality (PESQ) under an open\nevaluation condition.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 16:13:06 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Xu", "Chenglin", ""], ["Rao", "Wei", ""], ["Chng", "Eng Siong", ""], ["Li", "Haizhou", ""]]}, {"id": "2004.08333", "submitter": "Alireza Borjali", "authors": "Alireza Borjali, Martin Magneli, David Shin, Henrik Malchau, Orhun K.\n  Muratoglu, Kartik M. Varadarajan", "title": "Natural Language Processing with Deep Learning for Medical Adverse Event\n  Detection from Free-Text Medical Narratives: A Case Study of Detecting Total\n  Hip Replacement Dislocation", "comments": null, "journal-ref": null, "doi": "10.1016/j.compbiomed.2020.104140", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and timely detection of medical adverse events (AEs) from free-text\nmedical narratives is challenging. Natural language processing (NLP) with deep\nlearning has already shown great potential for analyzing free-text data, but\nits application for medical AE detection has been limited. In this study we\nproposed deep learning based NLP (DL-NLP) models for efficient and accurate hip\ndislocation AE detection following total hip replacement from standard\n(radiology notes) and non-standard (follow-up telephone notes) free-text\nmedical narratives. We benchmarked these proposed models with a wide variety of\ntraditional machine learning based NLP (ML-NLP) models, and also assessed the\naccuracy of International Classification of Diseases (ICD) and Current\nProcedural Terminology (CPT) codes in capturing these hip dislocation AEs in a\nmulti-center orthopaedic registry. All DL-NLP models out-performed all of the\nML-NLP models, with a convolutional neural network (CNN) model achieving the\nbest overall performance (Kappa = 0.97 for radiology notes, and Kappa = 1.00\nfor follow-up telephone notes). On the other hand, the ICD/CPT codes of the\npatients who sustained a hip dislocation AE were only 75.24% accurate, showing\nthe potential of the proposed model to be used in largescale orthopaedic\nregistries for accurate and efficient hip dislocation AE detection to improve\nthe quality of care and patient outcome.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 16:25:36 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 18:54:36 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Borjali", "Alireza", ""], ["Magneli", "Martin", ""], ["Shin", "David", ""], ["Malchau", "Henrik", ""], ["Muratoglu", "Orhun K.", ""], ["Varadarajan", "Kartik M.", ""]]}, {"id": "2004.08355", "submitter": "Georg Rehm", "authors": "Georg Rehm, Dimitrios Galanis, Penny Labropoulou, Stelios Piperidis,\n  Martin Wel{\\ss}, Ricardo Usbeck, Joachim K\\\"ohler, Miltos Deligiannis,\n  Katerina Gkirtzou, Johannes Fischer, Christian Chiarcos, Nils Feldhus,\n  Juli\\'an Moreno-Schneider, Florian Kintzel, Elena Montiel, V\\'ictor\n  Rodr\\'iguez Doncel, John P. McCrae, David Laqua, Irina Patricia Theile,\n  Christian Dittmar, Kalina Bontcheva, Ian Roberts, Andrejs Vasiljevs, Andis\n  Lagzdi\\c{n}\\v{s}", "title": "Towards an Interoperable Ecosystem of AI and LT Platforms: A Roadmap for\n  the Implementation of Different Levels of Interoperability", "comments": "Proceedings of the 1st International Workshop on Language Technology\n  Platforms (IWLTP 2020). To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With regard to the wider area of AI/LT platform interoperability, we\nconcentrate on two core aspects: (1) cross-platform search and discovery of\nresources and services; (2) composition of cross-platform service workflows. We\ndevise five different levels (of increasing complexity) of platform\ninteroperability that we suggest to implement in a wider federation of AI/LT\nplatforms. We illustrate the approach using the five emerging AI/LT platforms\nAI4EU, ELG, Lynx, QURATOR and SPEAKER.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:22:52 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Rehm", "Georg", ""], ["Galanis", "Dimitrios", ""], ["Labropoulou", "Penny", ""], ["Piperidis", "Stelios", ""], ["Wel\u00df", "Martin", ""], ["Usbeck", "Ricardo", ""], ["K\u00f6hler", "Joachim", ""], ["Deligiannis", "Miltos", ""], ["Gkirtzou", "Katerina", ""], ["Fischer", "Johannes", ""], ["Chiarcos", "Christian", ""], ["Feldhus", "Nils", ""], ["Moreno-Schneider", "Juli\u00e1n", ""], ["Kintzel", "Florian", ""], ["Montiel", "Elena", ""], ["Doncel", "V\u00edctor Rodr\u00edguez", ""], ["McCrae", "John P.", ""], ["Laqua", "David", ""], ["Theile", "Irina Patricia", ""], ["Dittmar", "Christian", ""], ["Bontcheva", "Kalina", ""], ["Roberts", "Ian", ""], ["Vasiljevs", "Andrejs", ""], ["Lagzdi\u0146\u0161", "Andis", ""]]}, {"id": "2004.08361", "submitter": "Anjalie Field", "authors": "Anjalie Field, Yulia Tsvetkov", "title": "Unsupervised Discovery of Implicit Gender Bias", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their prevalence in society, social biases are difficult to identify,\nprimarily because human judgements in this domain can be unreliable. We take an\nunsupervised approach to identifying gender bias against women at a comment\nlevel and present a model that can surface text likely to contain bias. Our\nmain challenge is forcing the model to focus on signs of implicit bias, rather\nthan other artifacts in the data. Thus, our methodology involves reducing the\ninfluence of confounds through propensity matching and adversarial learning.\nOur analysis shows how biased comments directed towards female politicians\ncontain mixed criticisms, while comments directed towards other female public\nfigures focus on appearance and sexualization. Ultimately, our work offers a\nway to capture subtle biases in various domains without relying on subjective\nhuman judgements.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:36:20 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 16:43:42 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Field", "Anjalie", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2004.08371", "submitter": "Esteban Marquer", "authors": "Lea Dieudonat, Kelvin Han, Phyllicia Leavitt, Esteban Marquer", "title": "Exploring the Combination of Contextual Word Embeddings and Knowledge\n  Graph Embeddings", "comments": "pre-publication, 16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ``Classical'' word embeddings, such as Word2Vec, have been shown to capture\nthe semantics of words based on their distributional properties. However, their\nability to represent the different meanings that a word may have is limited.\nSuch approaches also do not explicitly encode relations between entities, as\ndenoted by words. Embeddings of knowledge bases (KB) capture the explicit\nrelations between entities denoted by words, but are not able to directly\ncapture the syntagmatic properties of these words. To our knowledge, recent\nresearch have focused on representation learning that augment the strengths of\none with the other. In this work, we begin exploring another approach using\ncontextual and KB embeddings jointly at the same level and propose two tasks --\nan entity typing and a relation typing task -- that evaluate the performance of\ncontextual and KB embeddings. We also evaluated a concatenated model of\ncontextual and KB embeddings with these two tasks, and obtain conclusive\nresults on the first task. We hope our work may contribute as a basis for\nmodels and datasets that develop in the direction of this approach.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:49:45 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Dieudonat", "Lea", ""], ["Han", "Kelvin", ""], ["Leavitt", "Phyllicia", ""], ["Marquer", "Esteban", ""]]}, {"id": "2004.08385", "submitter": "Noa Garcia", "authors": "Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima", "title": "Knowledge-Based Visual Question Answering in Videos", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.10706", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel video understanding task by fusing knowledge-based and\nvideo question answering. First, we introduce KnowIT VQA, a video dataset with\n24,282 human-generated question-answer pairs about a popular sitcom. The\ndataset combines visual, textual and temporal coherence reasoning together with\nknowledge-based questions, which need of the experience obtained from the\nviewing of the series to be answered. Second, we propose a video understanding\nmodel by combining the visual and textual video content with specific knowledge\nabout the show. Our main findings are: (i) the incorporation of knowledge\nproduces outstanding improvements for VQA in video, and (ii) the performance on\nKnowIT VQA still lags well behind human accuracy, indicating its usefulness for\nstudying current video modelling limitations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 02:06:26 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Garcia", "Noa", ""], ["Otani", "Mayu", ""], ["Chu", "Chenhui", ""], ["Nakashima", "Yuta", ""]]}, {"id": "2004.08449", "submitter": "Eric Smith", "authors": "Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, Y-Lan\n  Boureau", "title": "Can You Put it All Together: Evaluating Conversational Agents' Ability\n  to Blend Skills", "comments": "accepted to ACL 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being engaging, knowledgeable, and empathetic are all desirable general\nqualities in a conversational agent. Previous work has introduced tasks and\ndatasets that aim to help agents to learn those qualities in isolation and\ngauge how well they can express them. But rather than being specialized in one\nsingle quality, a good open-domain conversational agent should be able to\nseamlessly blend them all into one cohesive conversational flow. In this work,\nwe investigate several ways to combine models trained towards isolated\ncapabilities, ranging from simple model aggregation schemes that require\nminimal additional training, to various forms of multi-task training that\nencompass several skills at all training stages. We further propose a new\ndataset, BlendedSkillTalk, to analyze how these capabilities would mesh\ntogether in a natural conversation, and compare the performance of different\narchitectures and training schemes. Our experiments show that multi-tasking\nover several tasks that focus on particular capabilities results in better\nblended conversation performance compared to models trained on a single skill,\nand that both unified or two-stage approaches perform well if they are\nconstructed to avoid unwanted bias in skill selection or are fine-tuned on our\nnew task.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 20:51:40 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Smith", "Eric Michael", ""], ["Williamson", "Mary", ""], ["Shuster", "Kurt", ""], ["Weston", "Jason", ""], ["Boureau", "Y-Lan", ""]]}, {"id": "2004.08500", "submitter": "William Merrill", "authors": "William Merrill and Gail Weiss and Yoav Goldberg and Roy Schwartz and\n  Noah A. Smith and Eran Yahav", "title": "A Formal Hierarchy of RNN Architectures", "comments": "To appear at ACL 2020. Updated to include computational cost\n  estimates and updated experimental results (in an erratum appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a formal hierarchy of the expressive capacity of RNN\narchitectures. The hierarchy is based on two formal properties: space\ncomplexity, which measures the RNN's memory, and rational recurrence, defined\nas whether the recurrent update can be described by a weighted finite-state\nmachine. We place several RNN variants within this hierarchy. For example, we\nprove the LSTM is not rational, which formally separates it from the related\nQRNN (Bradbury et al., 2016). We also show how these models' expressive\ncapacity is expanded by stacking multiple layers or composing them with\ndifferent pooling functions. Our results build on the theory of \"saturated\"\nRNNs (Merrill, 2019). While formally extending these findings to unsaturated\nRNNs is left to future work, we hypothesize that the practical learnable\ncapacity of unsaturated RNNs obeys a similar hierarchy. Experimental findings\nfrom training unsaturated networks on formal languages support this conjecture.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 00:57:54 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 18:37:47 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 18:38:17 GMT"}, {"version": "v4", "created": "Sat, 19 Sep 2020 23:03:45 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Merrill", "William", ""], ["Weiss", "Gail", ""], ["Goldberg", "Yoav", ""], ["Schwartz", "Roy", ""], ["Smith", "Noah A.", ""], ["Yahav", "Eran", ""]]}, {"id": "2004.08511", "submitter": "Wang Chen", "authors": "Wang Chen, Hou Pong Chan, Piji Li, Irwin King", "title": "Exclusive Hierarchical Decoding for Deep Keyphrase Generation", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase generation (KG) aims to summarize the main ideas of a document into\na set of keyphrases. A new setting is recently introduced into this problem, in\nwhich, given a document, the model needs to predict a set of keyphrases and\nsimultaneously determine the appropriate number of keyphrases to produce.\nPrevious work in this setting employs a sequential decoding process to generate\nkeyphrases. However, such a decoding method ignores the intrinsic hierarchical\ncompositionality existing in the keyphrase set of a document. Moreover,\nprevious work tends to generate duplicated keyphrases, which wastes time and\ncomputing resources. To overcome these limitations, we propose an exclusive\nhierarchical decoding framework that includes a hierarchical decoding process\nand either a soft or a hard exclusion mechanism. The hierarchical decoding\nprocess is to explicitly model the hierarchical compositionality of a keyphrase\nset. Both the soft and the hard exclusion mechanisms keep track of\npreviously-predicted keyphrases within a window size to enhance the diversity\nof the generated keyphrases. Extensive experiments on multiple KG benchmark\ndatasets demonstrate the effectiveness of our method to generate less\nduplicated and more accurate keyphrases.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 02:58:00 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Chen", "Wang", ""], ["Chan", "Hou Pong", ""], ["Li", "Piji", ""], ["King", "Irwin", ""]]}, {"id": "2004.08526", "submitter": "Seiichi Uchida", "authors": "Masaya Ikoma, Brian Kenji Iwana, Seiichi Uchida", "title": "Effect of Text Color on Word Embeddings", "comments": "to appear at the 14th International Workshop on Document Analysis\n  Systems (DAS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural scenes and documents, we can find the correlation between a text\nand its color. For instance, the word, \"hot\", is often printed in red, while\n\"cold\" is often in blue. This correlation can be thought of as a feature that\nrepresents the semantic difference between the words. Based on this\nobservation, we propose the idea of using text color for word embeddings. While\ntext-only word embeddings (e.g. word2vec) have been extremely successful, they\noften represent antonyms as similar since they are often interchangeable in\nsentences. In this paper, we try two tasks to verify the usefulness of text\ncolor in understanding the meanings of words, especially in identifying\nsynonyms and antonyms. First, we quantify the color distribution of words from\nthe book cover images and analyze the correlation between the color and meaning\nof the word. Second, we try to retrain word embeddings with the color\ndistribution of words as a constraint. By observing the changes in the word\nembeddings of synonyms and antonyms before and after re-training, we aim to\nunderstand the kind of words that have positive or negative effects in their\nword embeddings when incorporating text color information.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 05:14:18 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Ikoma", "Masaya", ""], ["Iwana", "Brian Kenji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2004.08673", "submitter": "Maria Mihaela Trusca", "authors": "Maria Mihaela Trusca, Daan Wassenberg, Flavius Frasincar, Rommert\n  Dekker", "title": "A Hybrid Approach for Aspect-Based Sentiment Analysis Using Deep\n  Contextual Word Embeddings and Hierarchical Attention", "comments": "Accepted for publication in the 20th International Conference on Web\n  Engineering (ICWE 2020), Helsinki Finland, 9-12 June 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web has become the main platform where people express their opinions\nabout entities of interest and their associated aspects. Aspect-Based Sentiment\nAnalysis (ABSA) aims to automatically compute the sentiment towards these\naspects from opinionated text. In this paper we extend the state-of-the-art\nHybrid Approach for Aspect-Based Sentiment Analysis (HAABSA) method in two\ndirections. First we replace the non-contextual word embeddings with deep\ncontextual word embeddings in order to better cope with the word semantics in a\ngiven text. Second, we use hierarchical attention by adding an extra attention\nlayer to the HAABSA high-level representations in order to increase the method\nflexibility in modeling the input data. Using two standard datasets (SemEval\n2015 and SemEval 2016) we show that the proposed extensions improve the\naccuracy of the built model for ABSA.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 17:54:55 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Trusca", "Maria Mihaela", ""], ["Wassenberg", "Daan", ""], ["Frasincar", "Flavius", ""], ["Dekker", "Rommert", ""]]}, {"id": "2004.08694", "submitter": "Kaustubh Dhole", "authors": "Kaustubh D. Dhole and Christopher D. Manning", "title": "Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation", "comments": "Some of the results in the paper were incorrect", "journal-ref": "Association for Computational Linguistics, 2020.acl-main.69", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Generation (QG) is fundamentally a simple syntactic transformation;\nhowever, many aspects of semantics influence what questions are good to form.\nWe implement this observation by developing Syn-QG, a set of transparent\nsyntactic rules leveraging universal dependencies, shallow semantic parsing,\nlexical resources, and custom rules which transform declarative sentences into\nquestion-answer pairs. We utilize PropBank argument descriptions and VerbNet\nstate predicates to incorporate shallow semantic content, which helps generate\nquestions of a descriptive nature and produce inferential and semantically\nricher questions than existing systems. In order to improve syntactic fluency\nand eliminate grammatically incorrect questions, we employ back-translation\nover the output of these syntactic rules. A set of crowd-sourced evaluations\nshows that our system can generate a larger number of highly grammatical and\nrelevant questions than previous QG systems and that back-translation\ndrastically improves grammaticality at a slight cost of generating irrelevant\nquestions.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 19:57:39 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 18:37:33 GMT"}, {"version": "v3", "created": "Sat, 11 Jul 2020 11:36:17 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 15:51:55 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Dhole", "Kaustubh D.", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2004.08726", "submitter": "Aylin Caliskan", "authors": "Autumn Toney, Akshat Pandey, Wei Guo, David Broniatowski, Aylin\n  Caliskan", "title": "Automatically Characterizing Targeted Information Operations Through\n  Biases Present in Discourse on Twitter", "comments": "5 pages, 4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper considers the problem of automatically characterizing overall\nattitudes and biases that may be associated with emerging information\noperations via artificial intelligence. Accurate analysis of these emerging\ntopics usually requires laborious, manual analysis by experts to annotate\nmillions of tweets to identify biases in new topics. We introduce extensions of\nthe Word Embedding Association Test from Caliskan et al. to a new domain\n(Caliskan, 2017). Our practical and unsupervised method is used to quantify\nbiases promoted in information operations. We validate our method using known\ninformation operation-related tweets from Twitter's Transparency Report. We\nperform a case study on the COVID-19 pandemic to evaluate our method's\nperformance on non-labeled Twitter data, demonstrating its usability in\nemerging domains.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 23:03:14 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 20:16:21 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 02:12:53 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Toney", "Autumn", ""], ["Pandey", "Akshat", ""], ["Guo", "Wei", ""], ["Broniatowski", "David", ""], ["Caliskan", "Aylin", ""]]}, {"id": "2004.08728", "submitter": "Masoud Jalili Sabet", "authors": "Masoud Jalili Sabet, Philipp Dufter, Fran\\c{c}ois Yvon, Hinrich\n  Sch\\\"utze", "title": "SimAlign: High Quality Word Alignments without Parallel Training Data\n  using Static and Contextualized Embeddings", "comments": "EMNLP (Findings) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word alignments are useful for tasks like statistical and neural machine\ntranslation (NMT) and cross-lingual annotation projection. Statistical word\naligners perform well, as do methods that extract alignments jointly with\ntranslations in NMT. However, most approaches require parallel training data,\nand quality decreases as less training data is available. We propose word\nalignment methods that require no parallel data. The key idea is to leverage\nmultilingual word embeddings, both static and contextualized, for word\nalignment. Our multilingual embeddings are created from monolingual data only\nwithout relying on any parallel data or dictionaries. We find that alignments\ncreated from embeddings are superior for four and comparable for two language\npairs compared to those produced by traditional statistical aligners, even with\nabundant parallel data; e.g., contextualized embeddings achieve a word\nalignment F1 for English-German that is 5 percentage points higher than\neflomal, a high-quality statistical aligner, trained on 100k parallel\nsentences.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 23:10:36 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 15:36:20 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 01:55:49 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 10:18:06 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Sabet", "Masoud Jalili", ""], ["Dufter", "Philipp", ""], ["Yvon", "Fran\u00e7ois", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2004.08731", "submitter": "Brent Biseda", "authors": "Brent Biseda and Katie Mo", "title": "Enhancing Pharmacovigilance with Drug Reviews and Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores whether the use of drug reviews and social media could be\nleveraged as potential alternative sources for pharmacovigilance of adverse\ndrug reactions (ADRs). We examined the performance of BERT alongside two\nvariants that are trained on biomedical papers, BioBERT7, and clinical notes,\nClinical BERT8. A variety of 8 different BERT models were fine-tuned and\ncompared across three different tasks in order to evaluate their relative\nperformance to one another in the ADR tasks. The tasks include sentiment\nclassification of drug reviews, presence of ADR in twitter postings, and named\nentity recognition of ADRs in twitter postings. BERT demonstrates its\nflexibility with high performance across all three different pharmacovigilance\nrelated tasks.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 23:35:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Biseda", "Brent", ""], ["Mo", "Katie", ""]]}, {"id": "2004.08744", "submitter": "Amanpreet Singh", "authors": "Amanpreet Singh, Vedanuj Goswami, Devi Parikh", "title": "Are we pretraining it right? Digging deeper into visio-linguistic\n  pretraining", "comments": "23 pages, 6 figures. First two authors contributed equally. More info\n  at https://github.com/facebookresearch/pythia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous recent works have proposed pretraining generic visio-linguistic\nrepresentations and then finetuning them for downstream vision and language\ntasks. While architecture and objective function design choices have received\nattention, the choice of pretraining datasets has received little attention. In\nthis work, we question some of the default choices made in literature. For\ninstance, we systematically study how varying similarity between the\npretraining dataset domain (textual and visual) and the downstream domain\naffects performance. Surprisingly, we show that automatically generated data in\na domain closer to the downstream task (e.g., VQA v2) is a better choice for\npretraining than \"natural\" data but of a slightly different domain (e.g.,\nConceptual Captions). On the other hand, some seemingly reasonable choices of\npretraining datasets were found to be entirely ineffective for some downstream\ntasks. This suggests that despite the numerous recent efforts, vision &\nlanguage pretraining does not quite work \"out of the box\" yet. Overall, as a\nby-product of our study, we find that simple design choices in pretraining can\nhelp us achieve close to state-of-art results on downstream tasks without any\narchitectural changes.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 01:55:19 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Singh", "Amanpreet", ""], ["Goswami", "Vedanuj", ""], ["Parikh", "Devi", ""]]}, {"id": "2004.08789", "submitter": "Sudipta Kar", "authors": "Md Zobaer Hossain, Md Ashraful Rahman, Md Saiful Islam, Sudipta Kar", "title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla", "comments": "LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 07:42:22 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hossain", "Md Zobaer", ""], ["Rahman", "Md Ashraful", ""], ["Islam", "Md Saiful", ""], ["Kar", "Sudipta", ""]]}, {"id": "2004.08793", "submitter": "Maria Mihaela Trusca", "authors": "Gino V.H. Mangnoesing, Maria Mihaela Trusca, Flavius Frasincar", "title": "Pattern Learning for Detecting Defect Reports and Improvement Requests\n  in App Reviews", "comments": "Accepted for publication in the 25th International Conference on\n  Natural Language & Information Systems (NLDB 2020), DFKI Saarbr\\\"ucken\n  Germany, June 24-26 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews are an important source of feedback for understanding\ncustomers. In this study, we follow novel approaches that target this absence\nof actionable insights by classifying reviews as defect reports and requests\nfor improvement. Unlike traditional classification methods based on expert\nrules, we reduce the manual labour by employing a supervised system that is\ncapable of learning lexico-semantic patterns through genetic programming.\nAdditionally, we experiment with a distantly-supervised SVM that makes use of\nnoisy labels generated by patterns. Using a real-world dataset of app reviews,\nwe show that the automatically learned patterns outperform the manually created\nones, to be generated. Also the distantly-supervised SVM models are not far\nbehind the pattern-based solutions, showing the usefulness of this approach\nwhen the amount of annotated data is limited.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 08:13:13 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Mangnoesing", "Gino V. H.", ""], ["Trusca", "Maria Mihaela", ""], ["Frasincar", "Flavius", ""]]}, {"id": "2004.08795", "submitter": "Ming Zhong", "authors": "Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu,\n  Xuanjing Huang", "title": "Extractive Summarization as Text Matching", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper creates a paradigm shift with regard to the way we build neural\nextractive summarization systems. Instead of following the commonly used\nframework of extracting sentences individually and modeling the relationship\nbetween sentences, we formulate the extractive summarization task as a semantic\ntext matching problem, in which a source document and candidate summaries will\nbe (extracted from the original text) matched in a semantic space. Notably,\nthis paradigm shift to semantic matching framework is well-grounded in our\ncomprehensive analysis of the inherent gap between sentence-level and\nsummary-level extractors based on the property of the dataset.\n  Besides, even instantiating the framework with a simple form of a matching\nmodel, we have driven the state-of-the-art extractive result on CNN/DailyMail\nto a new level (44.41 in ROUGE-1). Experiments on the other five datasets also\nshow the effectiveness of the matching framework. We believe the power of this\nmatching-based summarization framework has not been fully exploited. To\nencourage more instantiations in the future, we have released our codes,\nprocessed dataset, as well as generated summaries in\nhttps://github.com/maszhongming/MatchSum.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 08:27:57 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zhong", "Ming", ""], ["Liu", "Pengfei", ""], ["Chen", "Yiran", ""], ["Wang", "Danqing", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2004.08798", "submitter": "Hongcai Xu", "authors": "Hongcai Xu, Junpeng Bao, Junqing Wang", "title": "Knowledge-graph based Proactive Dialogue Generation with Improved\n  Meta-Learning", "comments": "15 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph-based dialogue systems can narrow down knowledge candidates\nfor generating informative and diverse responses with the use of prior\ninformation, e.g., triple attributes or graph paths. However, most current\nknowledge graph (KG) cover incomplete domain-specific knowledge. To overcome\nthis drawback, we propose a knowledge graph based proactive dialogue generation\nmodel (KgDg) with three components, improved model-agnostic meta-learning\nalgorithm (MAML), knowledge selection in knowledge triplets embedding, and\nknowledge aware proactive response generator. For knowledge triplets embedding\nand selection, we formulate it as a problem of sentence embedding to better\ncapture semantic information. Our improved MAML algorithm is capable of\nlearning general features from a limited number of knowledge graphs, which can\nalso quickly adapt to dialogue generation with unseen knowledge triplets.\nExtensive experiments are conducted on a knowledge aware dialogue dataset\n(DuConv). The results show that KgDg adapts both fast and well to knowledge\ngraph-based dialogue generation and outperforms state-of-the-art baseline.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 08:41:12 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Xu", "Hongcai", ""], ["Bao", "Junpeng", ""], ["Wang", "Junqing", ""]]}, {"id": "2004.08814", "submitter": "Sibei Yang", "authors": "Sibei Yang, Guanbin Li, Yizhou Yu", "title": "Graph-Structured Referring Expression Reasoning in The Wild", "comments": "CVPR 2020 Accepted Oral Paper. Data and code are available at\n  https://github.com/sibeiyang/sgmn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding referring expressions aims to locate in an image an object referred\nto by a natural language expression. The linguistic structure of a referring\nexpression provides a layout of reasoning over the visual contents, and it is\noften crucial to align and jointly understand the image and the referring\nexpression. In this paper, we propose a scene graph guided modular network\n(SGMN), which performs reasoning over a semantic graph and a scene graph with\nneural modules under the guidance of the linguistic structure of the\nexpression. In particular, we model the image as a structured semantic graph,\nand parse the expression into a language scene graph. The language scene graph\nnot only decodes the linguistic structure of the expression, but also has a\nconsistent representation with the image semantic graph. In addition to\nexploring structured solutions to grounding referring expressions, we also\npropose Ref-Reasoning, a large-scale real-world dataset for structured\nreferring expression reasoning. We automatically generate referring expressions\nover the scene graphs of images using diverse expression templates and\nfunctional programs. This dataset is equipped with real-world visual contents\nas well as semantically rich expressions with different reasoning layouts.\nExperimental results show that our SGMN not only significantly outperforms\nexisting state-of-the-art algorithms on the new Ref-Reasoning dataset, but also\nsurpasses state-of-the-art structured methods on commonly used benchmark\ndatasets. It can also provide interpretable visual evidences of reasoning. Data\nand code are available at https://github.com/sibeiyang/sgmn\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 11:00:30 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Yang", "Sibei", ""], ["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "2004.08825", "submitter": "Hongliang Dai", "authors": "Chin Lee, Hongliang Dai, Yangqiu Song, Xin Li", "title": "A Chinese Corpus for Fine-grained Entity Typing", "comments": "LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained entity typing is a challenging task with wide applications.\nHowever, most existing datasets for this task are in English. In this paper, we\nintroduce a corpus for Chinese fine-grained entity typing that contains 4,800\nmentions manually labeled through crowdsourcing. Each mention is annotated with\nfree-form entity types. To make our dataset useful in more possible scenarios,\nwe also categorize all the fine-grained types into 10 general types. Finally,\nwe conduct experiments with some neural models whose structures are typical in\nfine-grained entity typing and show how well they perform on our dataset. We\nalso show the possibility of improving Chinese fine-grained entity typing\nthrough cross-lingual transfer learning.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 11:53:32 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Lee", "Chin", ""], ["Dai", "Hongliang", ""], ["Song", "Yangqiu", ""], ["Li", "Xin", ""]]}, {"id": "2004.08833", "submitter": "Hongcai Xu", "authors": "Hongcai Xu, Junpeng Bao, Gaojie Zhang", "title": "Dynamic Knowledge Graph-based Dialogue Generation with Improved\n  Adversarial Meta-Learning", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph-based dialogue systems are capable of generating more\ninformative responses and can implement sophisticated reasoning mechanisms.\nHowever, these models do not take into account the sparseness and\nincompleteness of knowledge graph (KG)and current dialogue models cannot be\napplied to dynamic KG. This paper proposes a dynamic Knowledge graph-based\ndialogue generation method with improved adversarial Meta-Learning (KDAD). KDAD\nformulates dynamic knowledge triples as a problem of adversarial attack and\nincorporates the objective of quickly adapting to dynamic knowledge-aware\ndialogue generation. We train a knowledge graph-based dialog model with\nimproved ADML using minimal training samples. The model can initialize the\nparameters and adapt to previous unseen knowledge so that training can be\nquickly completed based on only a few knowledge triples. We show that our model\nsignificantly outperforms other baselines. We evaluate and demonstrate that our\nmethod adapts extremely fast and well to dynamic knowledge graph-based dialogue\ngeneration.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 12:27:49 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Xu", "Hongcai", ""], ["Bao", "Junpeng", ""], ["Zhang", "Gaojie", ""]]}, {"id": "2004.08900", "submitter": "Or Sharir", "authors": "Or Sharir, Barak Peleg and Yoav Shoham", "title": "The Cost of Training NLP Models: A Concise Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the cost of training large-scale language models, and the drivers\nof these costs. The intended audience includes engineers and scientists\nbudgeting their model-training experiments, as well as non-practitioners trying\nto make sense of the economics of modern-day Natural Language Processing (NLP).\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 16:28:35 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Sharir", "Or", ""], ["Peleg", "Barak", ""], ["Shoham", "Yoav", ""]]}, {"id": "2004.08994", "submitter": "Xiaodong Liu", "authors": "Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung\n  Poon and Jianfeng Gao", "title": "Adversarial Training for Large Neural Language Models", "comments": "13 pages, 9 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization and robustness are both key desiderata for designing machine\nlearning methods. Adversarial training can enhance robustness, but past work\noften finds it hurts generalization. In natural language processing (NLP),\npre-training large neural language models such as BERT have demonstrated\nimpressive gain in generalization for a variety of tasks, with further\nimprovement from adversarial fine-tuning. However, these models are still\nvulnerable to adversarial attacks. In this paper, we show that adversarial\npre-training can improve both generalization and robustness. We propose a\ngeneral algorithm ALUM (Adversarial training for large neural LangUage Models),\nwhich regularizes the training objective by applying perturbations in the\nembedding space that maximizes the adversarial loss. We present the first\ncomprehensive study of adversarial training in all stages, including\npre-training from scratch, continual pre-training on a well-trained model, and\ntask-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide\nrange of NLP tasks, in both regular and adversarial scenarios. Even for models\nthat have been well trained on extremely large text corpora, such as RoBERTa,\nALUM can still produce significant gains from continual pre-training, whereas\nconventional non-adversarial methods can not. ALUM can be further combined with\ntask-specific fine-tuning to attain additional gains. The ALUM code is publicly\navailable at https://github.com/namisan/mt-dnn.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 00:07:18 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 21:16:31 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Liu", "Xiaodong", ""], ["Cheng", "Hao", ""], ["He", "Pengcheng", ""], ["Chen", "Weizhu", ""], ["Wang", "Yu", ""], ["Poon", "Hoifung", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2004.09015", "submitter": "Frank F. Xu", "authors": "Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, Graham\n  Neubig", "title": "Incorporating External Knowledge through Pre-training for Natural\n  Language to Code Generation", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain code generation aims to generate code in a general-purpose\nprogramming language (such as Python) from natural language (NL) intents.\nMotivated by the intuition that developers usually retrieve resources on the\nweb when writing code, we explore the effectiveness of incorporating two\nvarieties of external knowledge into NL-to-code generation: automatically mined\nNL-code pairs from the online programming QA forum StackOverflow and\nprogramming language API documentation. Our evaluations show that combining the\ntwo sources with data augmentation and retrieval-based data re-sampling\nimproves the current state-of-the-art by up to 2.2% absolute BLEU score on the\ncode generation testbed CoNaLa. The code and resources are available at\nhttps://github.com/neulab/external-knowledge-codegen.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 01:45:27 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Xu", "Frank F.", ""], ["Jiang", "Zhengbao", ""], ["Yin", "Pengcheng", ""], ["Vasilescu", "Bogdan", ""], ["Neubig", "Graham", ""]]}, {"id": "2004.09036", "submitter": "Yefei Zha", "authors": "Yefei Zha, Ruobing Li, Hui Lin", "title": "Gated Convolutional Bidirectional Attention-based Model for Off-topic\n  Spoken Response Detection", "comments": "ACL2020 long paper", "journal-ref": null, "doi": "10.18653/v1/2020.acl-main.56", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-topic spoken response detection, the task aiming at predicting whether a\nresponse is off-topic for the corresponding prompt, is important for an\nautomated speaking assessment system. In many real-world educational\napplications, off-topic spoken response detectors are required to achieve high\nrecall for off-topic responses not only on seen prompts but also on prompts\nthat are unseen during training. In this paper, we propose a novel approach for\noff-topic spoken response detection with high off-topic recall on both seen and\nunseen prompts. We introduce a new model, Gated Convolutional Bidirectional\nAttention-based Model (GCBiA), which applies bi-attention mechanism and\nconvolutions to extract topic words of prompts and key-phrases of responses,\nand introduces gated unit and residual connections between major layers to\nbetter represent the relevance of responses and prompts. Moreover, a new\nnegative sampling method is proposed to augment training data. Experiment\nresults demonstrate that our novel approach can achieve significant\nimprovements in detecting off-topic responses with extremely high on-topic\nrecall, for both seen and unseen prompts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 03:16:06 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 02:12:17 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 02:22:50 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2020 07:08:36 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zha", "Yefei", ""], ["Li", "Ruobing", ""], ["Lin", "Hui", ""]]}, {"id": "2004.09045", "submitter": "Longbin Lai", "authors": "Lu Qin, Longbin Lai, Kongzhang Hao, Zhongxin Zhou, Yiwei Zhao, Yuxing\n  Han, Xuemin Lin, Zhengping Qian, Jingren Zhou", "title": "Taming the Expressiveness and Programmability of Graph Analytical\n  Queries", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph database has enjoyed a boom in the last decade, and graph queries\naccordingly gain a lot of attentions from both the academia and industry. We\nfocus on analytical queries in this paper. While analyzing existing\ndomain-specific languages (DSLs) for analytical queries regarding the\nperspectives of completeness, expressiveness and programmability, we find out\nthat none of existing work has achieved a satisfactory coverage of these\nperspectives. Motivated by this, we propose the \\flash DSL, which is named\nafter the three primitive operators Filter, LocAl and PuSH. We prove that\n\\flash is Turing complete (completeness), and show that it achieves both good\nexpressiveness and programmability for analytical queries. We provide an\nimplementation of \\flash based on code generation, and compare it with native\nC++ codes and existing DSL using representative queries. The experiment results\ndemonstrate \\flash's expressiveness, and its capability of programming complex\nalgorithms that achieve satisfactory runtime.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 04:08:28 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 07:28:06 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Qin", "Lu", ""], ["Lai", "Longbin", ""], ["Hao", "Kongzhang", ""], ["Zhou", "Zhongxin", ""], ["Zhao", "Yiwei", ""], ["Han", "Yuxing", ""], ["Lin", "Xuemin", ""], ["Qian", "Zhengping", ""], ["Zhou", "Jingren", ""]]}, {"id": "2004.09050", "submitter": "Sashank Santhanam", "authors": "Archna Bhatia, Adam Dalton, Brodie Mather, Sashank Santhanam, Samira\n  Shaikh, Alan Zemel, Tomek Strzalkowski, Bonnie J. Dorr", "title": "Adaptation of a Lexical Organization for Social Engineering Detection\n  and Response Generation", "comments": "Accepted at STOC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a paradigm for extensible lexicon development based on Lexical\nConceptual Structure to support social engineering detection and response\ngeneration. We leverage the central notions of ask (elicitation of behaviors\nsuch as providing access to money) and framing (risk/reward implied by the\nask). We demonstrate improvements in ask/framing detection through refinements\nto our lexical organization and show that response generation qualitatively\nimproves as ask/framing detection performance improves. The paradigm presents a\nsystematic and efficient approach to resource adaptation for improved\ntask-specific performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 04:37:55 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bhatia", "Archna", ""], ["Dalton", "Adam", ""], ["Mather", "Brodie", ""], ["Santhanam", "Sashank", ""], ["Shaikh", "Samira", ""], ["Zemel", "Alan", ""], ["Strzalkowski", "Tomek", ""], ["Dorr", "Bonnie J.", ""]]}, {"id": "2004.09095", "submitter": "Sebastin Santy", "authors": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, Monojit\n  Choudhury", "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP\n  World", "comments": "Accepted at ACL 2020 (10 pages + 2 pages Appendix). P.J., S.S. and\n  A.B. contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language technologies contribute to promoting multilingualism and linguistic\ndiversity around the world. However, only a very small number of the over 7000\nlanguages of the world are represented in the rapidly evolving language\ntechnologies and applications. In this paper we look at the relation between\nthe types of languages, resources, and their representation in NLP conferences\nto understand the trajectory that different languages have followed over time.\nOur quantitative investigation underlines the disparity between languages,\nespecially in terms of their resources, and calls into question the \"language\nagnostic\" status of current models and systems. Through this paper, we attempt\nto convince the ACL community to prioritise the resolution of the predicaments\nhighlighted here, so that no language is left behind.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 07:19:22 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 08:11:13 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 03:39:20 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Joshi", "Pratik", ""], ["Santy", "Sebastin", ""], ["Budhiraja", "Amar", ""], ["Bali", "Kalika", ""], ["Choudhury", "Monojit", ""]]}, {"id": "2004.09124", "submitter": "Rahma Chaabouni", "authors": "Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel\n  Dupoux, Marco Baroni", "title": "Compositionality and Generalization in Emergent Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language allows us to refer to novel composite concepts by combining\nexpressions denoting their parts according to systematic rules, a property\nknown as \\emph{compositionality}. In this paper, we study whether the language\nemerging in deep multi-agent simulations possesses a similar ability to refer\nto novel primitive combinations, and whether it accomplishes this feat by\nstrategies akin to human-language compositionality. Equipped with new ways to\nmeasure compositionality in emergent languages inspired by disentanglement in\nrepresentation learning, we establish three main results. First, given\nsufficiently large input spaces, the emergent language will naturally develop\nthe ability to refer to novel composite concepts. Second, there is no\ncorrelation between the degree of compositionality of an emergent language and\nits ability to generalize. Third, while compositionality is not necessary for\ngeneralization, it provides an advantage in terms of language transmission: The\nmore compositional a language is, the more easily it will be picked up by new\nlearners, even when the latter differ in architecture from the original agents.\nWe conclude that compositionality does not arise from simple generalization\npressure, but if an emergent language does chance upon it, it will be more\nlikely to survive and thrive.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 08:30:14 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Chaabouni", "Rahma", ""], ["Kharitonov", "Eugene", ""], ["Bouchacourt", "Diane", ""], ["Dupoux", "Emmanuel", ""], ["Baroni", "Marco", ""]]}, {"id": "2004.09143", "submitter": "Edison Marrese-Taylor", "authors": "Edison Marrese-Taylor, Machel Reid, Yutaka Matsuo", "title": "Variational Inference for Learning Representations of Natural Language\n  Edits", "comments": "Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document editing has become a pervasive component of the production of\ninformation, with version control systems enabling edits to be efficiently\nstored and applied. In light of this, the task of learning distributed\nrepresentations of edits has been recently proposed. With this in mind, we\npropose a novel approach that employs variational inference to learn a\ncontinuous latent space of vector representations to capture the underlying\nsemantic information with regard to the document editing process. We achieve\nthis by introducing a latent variable to explicitly model the aforementioned\nfeatures. This latent variable is then combined with a document representation\nto guide the generation of an edited version of this document. Additionally, to\nfacilitate standardized automatic evaluation of edit representations, which has\nheavily relied on direct human input thus far, we also propose a suite of\ndownstream tasks, PEER, specifically designed to measure the quality of edit\nrepresentations in the context of natural language processing.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:08:59 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 10:49:25 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 05:45:47 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 02:26:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Marrese-Taylor", "Edison", ""], ["Reid", "Machel", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "2004.09167", "submitter": "Akshay Smit", "authors": "Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Y. Ng,\n  Matthew P. Lungren", "title": "CheXbert: Combining Automatic Labelers and Expert Annotations for\n  Accurate Radiology Report Labeling Using BERT", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of labels from radiology text reports enables large-scale\ntraining of medical imaging models. Existing approaches to report labeling\ntypically rely either on sophisticated feature engineering based on medical\ndomain knowledge or manual annotations by experts. In this work, we introduce a\nBERT-based approach to medical image report labeling that exploits both the\nscale of available rule-based systems and the quality of expert annotations. We\ndemonstrate superior performance of a biomedically pretrained BERT model first\ntrained on annotations of a rule-based labeler and then finetuned on a small\nset of expert annotations augmented with automated backtranslation. We find\nthat our final model, CheXbert, is able to outperform the previous best\nrules-based labeler with statistical significance, setting a new SOTA for\nreport labeling on one of the largest datasets of chest x-rays.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:46:40 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 05:32:06 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 20:30:22 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Smit", "Akshay", ""], ["Jain", "Saahil", ""], ["Rajpurkar", "Pranav", ""], ["Pareek", "Anuj", ""], ["Ng", "Andrew Y.", ""], ["Lungren", "Matthew P.", ""]]}, {"id": "2004.09189", "submitter": "Chen Wu", "authors": "Chen Wu, Prince Zizhuang Wang, William Yang Wang", "title": "On the Encoder-Decoder Incompatibility in Variational Text Modeling and\n  Beyond", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) combine latent variables with amortized\nvariational inference, whose optimization usually converges into a trivial\nlocal optimum termed posterior collapse, especially in text modeling. By\ntracking the optimization dynamics, we observe the encoder-decoder\nincompatibility that leads to poor parameterizations of the data manifold. We\nargue that the trivial local optimum may be avoided by improving the encoder\nand decoder parameterizations since the posterior network is part of a\ntransition map between them. To this end, we propose Coupled-VAE, which couples\na VAE model with a deterministic autoencoder with the same structure and\nimproves the encoder and decoder parameterizations via encoder weight sharing\nand decoder signal matching. We apply the proposed Coupled-VAE approach to\nvarious VAE models with different regularization, posterior family, decoder\nstructure, and optimization strategy. Experiments on benchmark datasets (i.e.,\nPTB, Yelp, and Yahoo) show consistently improved results in terms of\nprobability estimation and richness of the latent space. We also generalize our\nmethod to conditional language modeling and propose Coupled-CVAE, which largely\nimproves the diversity of dialogue generation on the Switchboard dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:34:10 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wu", "Chen", ""], ["Wang", "Prince Zizhuang", ""], ["Wang", "William Yang", ""]]}, {"id": "2004.09205", "submitter": "Yung-Sung Chuang", "authors": "Chi-Liang Liu, Tsung-Yuan Hsu, Yung-Sung Chuang, Hung-Yi Lee", "title": "A Study of Cross-Lingual Ability and Language-specific Information in\n  Multilingual BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multilingual BERT works remarkably well on cross-lingual transfer\ntasks, superior to static non-contextualized word embeddings. In this work, we\nprovide an in-depth experimental study to supplement the existing literature of\ncross-lingual ability. We compare the cross-lingual ability of\nnon-contextualized and contextualized representation model with the same data.\nWe found that datasize and context window size are crucial factors to the\ntransferability. We also observe the language-specific information in\nmultilingual BERT. By manipulating the latent representations, we can control\nthe output languages of multilingual BERT, and achieve unsupervised token\ntranslation. We further show that based on the observation, there is a\ncomputationally cheap but effective approach to improve the cross-lingual\nability of multilingual BERT.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 11:13:16 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Liu", "Chi-Liang", ""], ["Hsu", "Tsung-Yuan", ""], ["Chuang", "Yung-Sung", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "2004.09218", "submitter": "Jens Nevens", "authors": "Jens Nevens and Paul Van Eecke and Katrien Beuls", "title": "A Practical Guide to Studying Emergent Communication through Grounded\n  Language Games", "comments": "This paper was officially published at the 'Language Learning for\n  Artificial Agents (L2A2) Symposium' of the 2019 Artificial Intelligence and\n  Simulation of Behaviour (AISB) Convention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of how an effective and efficient communication system can\nemerge in a population of agents that need to solve a particular task attracts\nmore and more attention from researchers in many fields, including artificial\nintelligence, linguistics and statistical physics. A common methodology for\nstudying this question consists of carrying out multi-agent experiments in\nwhich a population of agents takes part in a series of scripted and\ntask-oriented communicative interactions, called 'language games'. While each\nindividual language game is typically played by two agents in the population, a\nlarge series of games allows the population to converge on a shared\ncommunication system. Setting up an experiment in which a rich system for\ncommunicating about the real world emerges is a major enterprise, as it\nrequires a variety of software components for running multi-agent experiments,\nfor interacting with sensors and actuators, for conceptualising and\ninterpreting semantic structures, and for mapping between these semantic\nstructures and linguistic utterances. The aim of this paper is twofold. On the\none hand, it introduces a high-level robot interface that extends the Babel\nsoftware system, presenting for the first time a toolkit that provides flexible\nmodules for dealing with each subtask involved in running advanced grounded\nlanguage game experiments. On the other hand, it provides a practical guide to\nusing the toolkit for implementing such experiments, taking a grounded colour\nnaming game experiment as a didactic example.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 11:48:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Nevens", "Jens", ""], ["Van Eecke", "Paul", ""], ["Beuls", "Katrien", ""]]}, {"id": "2004.09219", "submitter": "Pratik Jawanpuria", "authors": "Pratik Jawanpuria, N T V Satya Dev, Anoop Kunchukuttan, Bamdev Mishra", "title": "Learning Geometric Word Meta-Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometric framework for learning meta-embeddings of words from\ndifferent embedding sources. Our framework transforms the embeddings into a\ncommon latent space, where, for example, simple averaging of different\nembeddings (of a given word) is more amenable. The proposed latent space arises\nfrom two particular geometric transformations - the orthogonal rotations and\nthe Mahalanobis metric scaling. Empirical results on several word similarity\nand word analogy benchmarks illustrate the efficacy of the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 11:49:04 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Jawanpuria", "Pratik", ""], ["Dev", "N T V Satya", ""], ["Kunchukuttan", "Anoop", ""], ["Mishra", "Bamdev", ""]]}, {"id": "2004.09249", "submitter": "Michael Mandel", "authors": "Shinji Watanabe, Michael Mandel, Jon Barker, Emmanuel Vincent, Ashish\n  Arora, Xuankai Chang, Sanjeev Khudanpur, Vimal Manohar, Daniel Povey, Desh\n  Raj, David Snyder, Aswin Shanmugam Subramanian, Jan Trmal, Bar Ben Yair,\n  Christoph Boeddeker, Zhaoheng Ni, Yusuke Fujita, Shota Horiguchi, Naoyuki\n  Kanda, Takuya Yoshioka, Neville Ryant", "title": "CHiME-6 Challenge:Tackling Multispeaker Speech Recognition for\n  Unsegmented Recordings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges we\norganize the 6th CHiME Speech Separation and Recognition Challenge (CHiME-6).\nThe new challenge revisits the previous CHiME-5 challenge and further considers\nthe problem of distant multi-microphone conversational speech diarization and\nrecognition in everyday home environments. Speech material is the same as the\nprevious CHiME-5 recordings except for accurate array synchronization. The\nmaterial was elicited using a dinner party scenario with efforts taken to\ncapture data that is representative of natural conversational speech. This\npaper provides a baseline description of the CHiME-6 challenge for both\nsegmented multispeaker speech recognition (Track 1) and unsegmented\nmultispeaker speech recognition (Track 2). Of note, Track 2 is the first\nchallenge activity in the community to tackle an unsegmented multispeaker\nspeech recognition scenario with a complete set of reproducible open source\nbaselines providing speech enhancement, speaker diarization, and speech\nrecognition modules.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 12:59:07 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 11:04:49 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Watanabe", "Shinji", ""], ["Mandel", "Michael", ""], ["Barker", "Jon", ""], ["Vincent", "Emmanuel", ""], ["Arora", "Ashish", ""], ["Chang", "Xuankai", ""], ["Khudanpur", "Sanjeev", ""], ["Manohar", "Vimal", ""], ["Povey", "Daniel", ""], ["Raj", "Desh", ""], ["Snyder", "David", ""], ["Subramanian", "Aswin Shanmugam", ""], ["Trmal", "Jan", ""], ["Yair", "Bar Ben", ""], ["Boeddeker", "Christoph", ""], ["Ni", "Zhaoheng", ""], ["Fujita", "Yusuke", ""], ["Horiguchi", "Shota", ""], ["Kanda", "Naoyuki", ""], ["Yoshioka", "Takuya", ""], ["Ryant", "Neville", ""]]}, {"id": "2004.09272", "submitter": "Daniela Massiceti", "authors": "Daniela Massiceti, Viveka Kulharia, Puneet K. Dokania, N. Siddharth,\n  Philip H.S. Torr", "title": "A Revised Generative Evaluation of Visual Dialogue", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating Visual Dialogue, the task of answering a sequence of questions\nrelating to a visual input, remains an open research challenge. The current\nevaluation scheme of the VisDial dataset computes the ranks of ground-truth\nanswers in predefined candidate sets, which Massiceti et al. (2018) show can be\nsusceptible to the exploitation of dataset biases. This scheme also does little\nto account for the different ways of expressing the same answer--an aspect of\nlanguage that has been well studied in NLP. We propose a revised evaluation\nscheme for the VisDial dataset leveraging metrics from the NLP literature to\nmeasure consensus between answers generated by the model and a set of relevant\nanswers. We construct these relevant answer sets using a simple and effective\nsemi-supervised method based on correlation, which allows us to automatically\nextend and scale sparse relevance annotations from humans to the entire\ndataset. We release these sets and code for the revised evaluation scheme as\nDenseVisDial, and intend them to be an improvement to the dataset in the face\nof its existing constraints and design choices.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 13:26:45 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 08:48:25 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Massiceti", "Daniela", ""], ["Kulharia", "Viveka", ""], ["Dokania", "Puneet K.", ""], ["Siddharth", "N.", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "2004.09297", "submitter": "Kaitao Song", "authors": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu and Tie-Yan Liu", "title": "MPNet: Masked and Permuted Pre-training for Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT adopts masked language modeling (MLM) for pre-training and is one of the\nmost successful pre-training models. Since BERT neglects dependency among\npredicted tokens, XLNet introduces permuted language modeling (PLM) for\npre-training to address this problem. However, XLNet does not leverage the full\nposition information of a sentence and thus suffers from position discrepancy\nbetween pre-training and fine-tuning. In this paper, we propose MPNet, a novel\npre-training method that inherits the advantages of BERT and XLNet and avoids\ntheir limitations. MPNet leverages the dependency among predicted tokens\nthrough permuted language modeling (vs. MLM in BERT), and takes auxiliary\nposition information as input to make the model see a full sentence and thus\nreducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a\nlarge-scale dataset (over 160GB text corpora) and fine-tune on a variety of\ndown-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet\noutperforms MLM and PLM by a large margin, and achieves better results on these\ntasks compared with previous state-of-the-art pre-trained methods (e.g., BERT,\nXLNet, RoBERTa) under the same model setting. The code and the pre-trained\nmodels are available at: https://github.com/microsoft/MPNet.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 13:54:12 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 06:54:52 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Song", "Kaitao", ""], ["Tan", "Xu", ""], ["Qin", "Tao", ""], ["Lu", "Jianfeng", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2004.09347", "submitter": "Abhishek Niranjan", "authors": "Abhishek Niranjan, Mukesh Sharma, Sai Bharath Chandra Gutha, M Ali\n  Basha Shaik", "title": "End-to-End Whisper to Natural Speech Conversion using Modified\n  Transformer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine recognition of an atypical speech like whispered speech, is a\nchallenging task. We introduce whisper-to-natural-speech conversion using\nsequence-to-sequence approach by proposing enhanced transformer architecture,\nwhich uses both parallel and non-parallel data. We investigate different\nfeatures like Mel frequency cepstral coefficients and smoothed spectral\nfeatures. The proposed networks are trained end-to-end using supervised\napproach for feature-to-feature transformation. Further, we also investigate\nthe effectiveness of embedded auxillary decoder used after N encoder\nsub-layers, trained with the frame-level objective function for identifying\nsource phoneme labels. We show results on opensource wTIMIT and CHAINS datasets\nby measuring word error rate using end-to-end ASR and also BLEU scores for the\ngenerated speech. Alternatively, we also propose a novel method to measure\nspectral shape of it by measuring formant distributions w.r.t. reference\nspeech, as formant divergence metric. We have found whisper-to-natural\nconverted speech formants probability distribution is similar to the\ngroundtruth distribution. To the authors' best knowledge, this is the first\ntime enhanced transformer has been proposed, both with and without auxiliary\ndecoder for whisper-to-natural-speech conversion and vice versa.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:47:46 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 07:08:37 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 09:27:12 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Niranjan", "Abhishek", ""], ["Sharma", "Mukesh", ""], ["Gutha", "Sai Bharath Chandra", ""], ["Shaik", "M Ali Basha", ""]]}, {"id": "2004.09367", "submitter": "Jung-Woo Ha", "authors": "Jung-Woo Ha, Kihyun Nam, Jingu Kang, Sang-Woo Lee, Sohee Yang,\n  Hyunhoon Jung, Eunmi Kim, Hyeji Kim, Soojin Kim, Hyun Ah Kim, Kyoungtae Doh,\n  Chan Kyu Lee, Nako Sung, Sunghun Kim", "title": "ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic\n  Speech Recognition of Contact Centers", "comments": "5 pages, 2 figures, 4 tables, The first two authors equally\n  contributed to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) via call is essential for various\napplications, including AI for contact center (AICC) services. Despite the\nadvancement of ASR, however, most publicly available call-based speech corpora\nsuch as Switchboard are old-fashioned. Also, most existing call corpora are in\nEnglish and mainly focus on open domain dialog or general scenarios such as\naudiobooks. Here we introduce a new large-scale Korean call-based speech corpus\nunder a goal-oriented dialog scenario from more than 11,000 people, i.e.,\nClovaCall corpus. ClovaCall includes approximately 60,000 pairs of a short\nsentence and its corresponding spoken utterance in a restaurant reservation\ndomain. We validate the effectiveness of our dataset with intensive experiments\nusing two standard ASR models. Furthermore, we release our ClovaCall dataset\nand baseline source codes to be available via\nhttps://github.com/ClovaAI/ClovaCall.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 15:12:29 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 06:53:34 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ha", "Jung-Woo", ""], ["Nam", "Kihyun", ""], ["Kang", "Jingu", ""], ["Lee", "Sang-Woo", ""], ["Yang", "Sohee", ""], ["Jung", "Hyunhoon", ""], ["Kim", "Eunmi", ""], ["Kim", "Hyeji", ""], ["Kim", "Soojin", ""], ["Kim", "Hyun Ah", ""], ["Doh", "Kyoungtae", ""], ["Lee", "Chan Kyu", ""], ["Sung", "Nako", ""], ["Kim", "Sunghun", ""]]}, {"id": "2004.09447", "submitter": "Vivek Srivastava", "authors": "Vivek Srivastava and Mayank Singh", "title": "PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code-mixing is the phenomenon of using more than one language in a sentence.\nIt is a very frequently observed pattern of communication on social media\nplatforms. Flexibility to use multiple languages in one text message might help\nto communicate efficiently with the target audience. But, it adds to the\nchallenge of processing and understanding natural language to a much larger\nextent. This paper presents a parallel corpus of the 13,738 code-mixed\nEnglish-Hindi sentences and their corresponding translation in English. The\ntranslations of sentences are done manually by the annotators. We are releasing\nthe parallel corpus to facilitate future research opportunities in code-mixed\nmachine translation. The annotated corpus is available at\nhttps://doi.org/10.5281/zenodo.3605597.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:04:22 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Srivastava", "Vivek", ""], ["Singh", "Mayank", ""]]}, {"id": "2004.09456", "submitter": "Moin Nadeem", "authors": "Moin Nadeem, Anna Bethke, Siva Reddy", "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "comments": "9 pages, 6 tables, and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A stereotype is an over-generalized belief about a particular group of\npeople, e.g., Asians are good at math or Asians are bad drivers. Such beliefs\n(biases) are known to hurt target groups. Since pretrained language models are\ntrained on large real world data, they are known to capture stereotypical\nbiases. In order to assess the adverse effects of these models, it is important\nto quantify the bias captured in them. Existing literature on quantifying bias\nevaluates pretrained language models on a small set of artificially constructed\nbias-assessing sentences. We present StereoSet, a large-scale natural dataset\nin English to measure stereotypical biases in four domains: gender, profession,\nrace, and religion. We evaluate popular models like BERT, GPT-2, RoBERTa, and\nXLNet on our dataset and show that these models exhibit strong stereotypical\nbiases. We also present a leaderboard with a hidden test set to track the bias\nof future language models at https://stereoset.mit.edu\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:14:33 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Nadeem", "Moin", ""], ["Bethke", "Anna", ""], ["Reddy", "Siva", ""]]}, {"id": "2004.09544", "submitter": "Hyundong Cho", "authors": "Hyundong Cho, Jonathan May", "title": "Grounding Conversations with Improvised Dialogues", "comments": "ACL2020 Camera Ready; 9 pages + 5 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective dialogue involves grounding, the process of establishing mutual\nknowledge that is essential for communication between people. Modern dialogue\nsystems are not explicitly trained to build common ground, and therefore\noverlook this important aspect of communication. Improvisational theater\n(improv) intrinsically contains a high proportion of dialogue focused on\nbuilding common ground, and makes use of the yes-and principle, a strong\ngrounding speech act, to establish coherence and an actionable objective\nreality. We collect a corpus of more than 26,000 yes-and turns, transcribing\nthem from improv dialogues and extracting them from larger, but more sparsely\npopulated movie script dialogue corpora, via a bootstrapped classifier. We\nfine-tune chit-chat dialogue systems with our corpus to encourage more\ngrounded, relevant conversation and confirm these findings with human\nevaluations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 18:05:53 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 05:34:13 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Cho", "Hyundong", ""], ["May", "Jonathan", ""]]}, {"id": "2004.09601", "submitter": "Shadi Shahsavari", "authors": "Shadi Shahsavari, Ehsan Ebrahimzadeh, Behnam Shahbazi, Misagh Falahi,\n  Pavan Holur, Roja Bandari, Timothy R. Tangherlini, Vwani Roychowdhury", "title": "An Automated Pipeline for Character and Relationship Extraction from\n  Readers' Literary Book Reviews on Goodreads.com", "comments": null, "journal-ref": null, "doi": "10.1145/3394231", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reader reviews of literary fiction on social media, especially those in\npersistent, dedicated forums, create and are in turn driven by underlying\nnarrative frameworks. In their comments about a novel, readers generally\ninclude only a subset of characters and their relationships, thus offering a\nlimited perspective on that work. Yet in aggregate, these reviews capture an\nunderlying narrative framework comprised of different actants (people, places,\nthings), their roles, and interactions that we label the \"consensus narrative\nframework\". We represent this framework in the form of an actant-relationship\nstory graph. Extracting this graph is a challenging computational problem,\nwhich we pose as a latent graphical model estimation problem. Posts and reviews\nare viewed as samples of sub graphs/networks of the hidden narrative framework.\nInspired by the qualitative narrative theory of Greimas, we formulate a\ngraphical generative Machine Learning (ML) model where nodes represent actants,\nand multi-edges and self-loops among nodes capture context-specific\nrelationships. We develop a pipeline of interlocking automated methods to\nextract key actants and their relationships, and apply it to thousands of\nreviews and comments posted on Goodreads.com. We manually derive the ground\ntruth narrative framework from SparkNotes, and then use word embedding tools to\ncompare relationships in ground truth networks with our extracted networks. We\nfind that our automated methodology generates highly accurate consensus\nnarrative frameworks: for our four target novels, with approximately 2900\nreviews per novel, we report average coverage/recall of important relationships\nof > 80% and an average edge detection rate of >89\\%. These extracted narrative\nframeworks can generate insight into how people (or classes of people) read and\nhow they recount what they have read to others.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 19:57:37 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Shahsavari", "Shadi", ""], ["Ebrahimzadeh", "Ehsan", ""], ["Shahbazi", "Behnam", ""], ["Falahi", "Misagh", ""], ["Holur", "Pavan", ""], ["Bandari", "Roja", ""], ["Tangherlini", "Timothy R.", ""], ["Roychowdhury", "Vwani", ""]]}, {"id": "2004.09662", "submitter": "Sashank Santhanam", "authors": "Adam Dalton, Ehsan Aghaei, Ehab Al-Shaer, Archna Bhatia, Esteban\n  Castillo, Zhuo Cheng, Sreekar Dhaduvai, Qi Duan, Md Mazharul Islam, Younes\n  Karimi, Amir Masoumzadeh, Brodie Mather, Sashank Santhanam, Samira Shaikh,\n  Tomek Strzalkowski, Bonnie J. Dorr", "title": "The Panacea Threat Intelligence and Active Defense Platform", "comments": "Accepted at STOC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe Panacea, a system that supports natural language processing (NLP)\ncomponents for active defenses against social engineering attacks. We deploy a\npipeline of human language technology, including Ask and Framing Detection,\nNamed Entity Recognition, Dialogue Engineering, and Stylometry. Panacea\nprocesses modern message formats through a plug-in architecture to accommodate\ninnovative approaches for message analysis, knowledge representation and\ndialogue generation. The novelty of the Panacea system is that uses NLP for\ncyber defense and engages the attacker using bots to elicit evidence to\nattribute to the attacker and to waste the attacker's time and resources.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 22:08:08 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Dalton", "Adam", ""], ["Aghaei", "Ehsan", ""], ["Al-Shaer", "Ehab", ""], ["Bhatia", "Archna", ""], ["Castillo", "Esteban", ""], ["Cheng", "Zhuo", ""], ["Dhaduvai", "Sreekar", ""], ["Duan", "Qi", ""], ["Islam", "Md Mazharul", ""], ["Karimi", "Younes", ""], ["Masoumzadeh", "Amir", ""], ["Mather", "Brodie", ""], ["Santhanam", "Sashank", ""], ["Shaikh", "Samira", ""], ["Strzalkowski", "Tomek", ""], ["Dorr", "Bonnie J.", ""]]}, {"id": "2004.09719", "submitter": "Xiangpeng Wan", "authors": "Xiangpeng Wan, Hakim Ghazzai, and Yehia Massoud", "title": "Word Embedding-based Text Processing for Comprehensive Summarization and\n  Distinct Information Extraction", "comments": "This paper is accepted for publication in IEEE Technology Engineering\n  Management Society International Conference (TEMSCON'20), Metro Detroit,\n  Michigan (USA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two automated text processing frameworks\nspecifically designed to analyze online reviews. The objective of the first\nframework is to summarize the reviews dataset by extracting essential sentence.\nThis is performed by converting sentences into numerical vectors and clustering\nthem using a community detection algorithm based on their similarity levels.\nAfterwards, a correlation score is measured for each sentence to determine its\nimportance level in each cluster and assign it as a tag for that community. The\nsecond framework is based on a question-answering neural network model trained\nto extract answers to multiple different questions. The collected answers are\neffectively clustered to find multiple distinct answers to a single question\nthat might be asked by a customer. The proposed frameworks are shown to be more\ncomprehensive than existing reviews processing solutions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 02:43:31 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Wan", "Xiangpeng", ""], ["Ghazzai", "Hakim", ""], ["Massoud", "Yehia", ""]]}, {"id": "2004.09731", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Lizi Liao, Xiaoyan Zhu, Tat-Seng Chua, Zitao Liu, Yan\n  Huang, Minlie Huang", "title": "Learning Goal-oriented Dialogue Policy with Opposite Agent Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches for goal-oriented dialogue policy learning used\nreinforcement learning, which focuses on the target agent policy and simply\ntreat the opposite agent policy as part of the environment. While in real-world\nscenarios, the behavior of an opposite agent often exhibits certain patterns or\nunderlies hidden policies, which can be inferred and utilized by the target\nagent to facilitate its own decision making. This strategy is common in human\nmental simulation by first imaging a specific action and the probable results\nbefore really acting it. We therefore propose an opposite behavior aware\nframework for policy learning in goal-oriented dialogues. We estimate the\nopposite agent's policy from its behavior and use this estimation to improve\nthe target agent by regarding it as part of the target policy. We evaluate our\nmodel on both cooperative and competitive dialogue tasks, showing superior\nperformance over state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 03:13:44 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Zhang", "Zheng", ""], ["Liao", "Lizi", ""], ["Zhu", "Xiaoyan", ""], ["Chua", "Tat-Seng", ""], ["Liu", "Zitao", ""], ["Huang", "Yan", ""], ["Huang", "Minlie", ""]]}, {"id": "2004.09733", "submitter": "Yuxian Gu", "authors": "Yuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan Liu, Maosong Sun", "title": "Train No Evil: Selective Masking for Task-Guided Pre-Training", "comments": "Accepted by EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pre-trained language models mostly follow the\npre-train-then-fine-tuning paradigm and have achieved great performance on\nvarious downstream tasks. However, since the pre-training stage is typically\ntask-agnostic and the fine-tuning stage usually suffers from insufficient\nsupervised data, the models cannot always well capture the domain-specific and\ntask-specific patterns. In this paper, we propose a three-stage framework by\nadding a task-guided pre-training stage with selective masking between general\npre-training and fine-tuning. In this stage, the model is trained by masked\nlanguage modeling on in-domain unsupervised data to learn domain-specific\npatterns and we propose a novel selective masking strategy to learn\ntask-specific patterns. Specifically, we design a method to measure the\nimportance of each token in sequences and selectively mask the important\ntokens. Experimental results on two sentiment analysis tasks show that our\nmethod can achieve comparable or even better performance with less than 50% of\ncomputation cost, which indicates our method is both effective and efficient.\nThe source code of this paper can be obtained from\nhttps://github.com/thunlp/SelectiveMasking.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 03:14:22 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 09:47:41 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Gu", "Yuxian", ""], ["Zhang", "Zhengyan", ""], ["Wang", "Xiaozhi", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "2004.09739", "submitter": "Tanmoy Chakraborty", "authors": "Tanya Chowdhury, Sachin Kumar, Tanmoy Chakraborty", "title": "Neural Abstractive Summarization with Structural Attention", "comments": "7 pages, 4 tables, 2 figures, IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attentional, RNN-based encoder-decoder architectures have achieved impressive\nperformance on abstractive summarization of news articles. However, these\nmethods fail to account for long term dependencies within the sentences of a\ndocument. This problem is exacerbated in multi-document summarization tasks\nsuch as summarizing the popular opinion in threads present in community\nquestion answering (CQA) websites such as Yahoo! Answers and Quora. These\nthreads contain answers which often overlap or contradict each other. In this\nwork, we present a hierarchical encoder based on structural attention to model\nsuch inter-sentence and inter-document dependencies. We set the popular\npointer-generator architecture and some of the architectures derived from it as\nour baselines and show that they fail to generate good summaries in a\nmulti-document setting. We further illustrate that our proposed model achieves\nsignificant improvement over the baselines in both single and multi-document\nsummarization settings -- in the former setting, it beats the best baseline by\n1.31 and 7.8 ROUGE-1 points on CNN and CQA datasets, respectively; in the\nlatter setting, the performance is further improved by 1.6 ROUGE-1 points on\nthe CQA dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 03:39:15 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 05:32:59 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chowdhury", "Tanya", ""], ["Kumar", "Sachin", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2004.09764", "submitter": "Fang Xianghong", "authors": "Xianghong Fang and Haoli Bai and Zenglin Xu and Michael Lyu and Irwin\n  King", "title": "Discrete Variational Attention Models for Language Generation", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Variational autoencoders have been widely applied for natural language\ngeneration, however, there are two long-standing problems: information\nunder-representation and posterior collapse. The former arises from the fact\nthat only the last hidden state from the encoder is transformed to the latent\nspace, which is insufficient to summarize data. The latter comes as a result of\nthe imbalanced scale between the reconstruction loss and the KL divergence in\nthe objective function. To tackle these issues, in this paper we propose the\ndiscrete variational attention model with categorical distribution over the\nattention mechanism owing to the discrete nature in languages. Our approach is\ncombined with an auto-regressive prior to capture the sequential dependency\nfrom observations, which can enhance the latent space for language generation.\nMoreover, thanks to the property of discreteness, the training of our proposed\napproach does not suffer from posterior collapse. Furthermore, we carefully\nanalyze the superiority of discrete latent space over the continuous space with\nthe common Gaussian distribution. Extensive experiments on language generation\ndemonstrate superior advantages of our proposed approach in comparison with the\nstate-of-the-art counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 05:49:04 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 08:02:24 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 15:18:57 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 06:35:02 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Fang", "Xianghong", ""], ["Bai", "Haoli", ""], ["Xu", "Zenglin", ""], ["Lyu", "Michael", ""], ["King", "Irwin", ""]]}, {"id": "2004.09800", "submitter": "Shizhe Diao", "authors": "Shizhe Diao, Yan Song, Tong Zhang", "title": "Keyphrase Generation with Cross-Document Attention", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase generation aims to produce a set of phrases summarizing the\nessentials of a given document. Conventional methods normally apply an\nencoder-decoder architecture to generate the output keyphrases for an input\ndocument, where they are designed to focus on each current document so they\ninevitably omit crucial corpus-level information carried by other similar\ndocuments, i.e., the cross-document dependency and latent topics. In this\npaper, we propose CDKGen, a Transformer-based keyphrase generator, which\nexpands the Transformer to global attention with cross-document attention\nnetworks to incorporate available documents as references so as to generate\nbetter keyphrases with the guidance of topic information. On top of the\nproposed Transformer + cross-document attention architecture, we also adopt a\ncopy mechanism to enhance our model via selecting appropriate words from\ndocuments to deal with out-of-vocabulary words in keyphrases. Experiment\nresults on five benchmark datasets illustrate the validity and effectiveness of\nour model, which achieves the state-of-the-art performance on all datasets.\nFurther analyses confirm that the proposed model is able to generate keyphrases\nconsistent with references while keeping sufficient diversity. The code of\nCDKGen is available at https://github.com/SVAIGBA/CDKGen.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 07:58:27 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Diao", "Shizhe", ""], ["Song", "Yan", ""], ["Zhang", "Tong", ""]]}, {"id": "2004.09813", "submitter": "Nils Reimers", "authors": "Nils Reimers, Iryna Gurevych", "title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge\n  Distillation", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present an easy and efficient method to extend existing sentence embedding\nmodels to new languages. This allows to create multilingual versions from\npreviously monolingual models. The training is based on the idea that a\ntranslated sentence should be mapped to the same location in the vector space\nas the original sentence. We use the original (monolingual) model to generate\nsentence embeddings for the source language and then train a new system on\ntranslated sentences to mimic the original model. Compared to other methods for\ntraining multilingual sentence embeddings, this approach has several\nadvantages: It is easy to extend existing models with relatively few samples to\nnew languages, it is easier to ensure desired properties for the vector space,\nand the hardware requirements for training is lower. We demonstrate the\neffectiveness of our approach for 50+ languages from various language families.\nCode to extend sentence embeddings models to more than 400 languages is\npublicly available.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 08:20:25 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 06:30:56 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Reimers", "Nils", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2004.09853", "submitter": "Siyu Ren", "authors": "Siyu Ren, Kenny Q. Zhu", "title": "Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice\n  Questions", "comments": "To appear at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel configurable framework to automatically\ngenerate distractive choices for open-domain cloze-style multiple-choice\nquestions, which incorporates a general-purpose knowledge base to effectively\ncreate a small distractor candidate set, and a feature-rich learning-to-rank\nmodel to select distractors that are both plausible and reliable. Experimental\nresults on datasets across four domains show that our framework yields\ndistractors that are more plausible and reliable than previous methods. This\ndataset can also be used as a benchmark for distractor generation in the\nfuture.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 09:29:50 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 13:24:20 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 01:53:44 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Ren", "Siyu", ""], ["Zhu", "Kenny Q.", ""]]}, {"id": "2004.09890", "submitter": "David Harbecke", "authors": "David Harbecke, Christoph Alt", "title": "Considering Likelihood in NLP Classification Explanations with Occlusion\n  and Language Modeling", "comments": "ACL 2020 Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, state-of-the-art NLP models gained an increasing syntactic and\nsemantic understanding of language, and explanation methods are crucial to\nunderstand their decisions. Occlusion is a well established method that\nprovides explanations on discrete language data, e.g. by removing a language\nunit from an input and measuring the impact on a model's decision. We argue\nthat current occlusion-based methods often produce invalid or syntactically\nincorrect language data, neglecting the improved abilities of recent NLP\nmodels. Furthermore, gradient-based explanation methods disregard the discrete\ndistribution of data in NLP. Thus, we propose OLM: a novel explanation method\nthat combines occlusion and language models to sample valid and syntactically\ncorrect replacements with high likelihood, given the context of the original\ninput. We lay out a theoretical foundation that alleviates these weaknesses of\nother explanation methods in NLP and provide results that underline the\nimportance of considering data likelihood in occlusion-based explanation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 10:37:44 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Harbecke", "David", ""], ["Alt", "Christoph", ""]]}, {"id": "2004.09894", "submitter": "KayYen Wong", "authors": "KayYen Wong, Sameen Maruf, Gholamreza Haffari", "title": "Contextual Neural Machine Translation Improves Translation of Cataphoric\n  Pronouns", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of context-aware NMT has resulted in promising improvements in the\noverall translation quality and specifically in the translation of discourse\nphenomena such as pronouns. Previous works have mainly focused on the use of\npast sentences as context with a focus on anaphora translation. In this work,\nwe investigate the effect of future sentences as context by comparing the\nperformance of a contextual NMT model trained with the future context to the\none trained with the past context. Our experiments and evaluation, using\ngeneric and pronoun-focused automatic metrics, show that the use of future\ncontext not only achieves significant improvements over the context-agnostic\nTransformer, but also demonstrates comparable and in some cases improved\nperformance over its counterpart trained on past context. We also perform an\nevaluation on a targeted cataphora test suite and report significant gains over\nthe context-agnostic Transformer in terms of BLEU.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 10:45:48 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 08:27:57 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Wong", "KayYen", ""], ["Maruf", "Sameen", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "2004.09930", "submitter": "Yaliang Li", "authors": "Daoyuan Chen, Yaliang Li, Kai Lei, Ying Shen", "title": "Relabel the Noise: Joint Extraction of Entities and Relations via\n  Cooperative Multiagents", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision based methods for entity and relation extraction have\nreceived increasing popularity due to the fact that these methods require light\nhuman annotation efforts. In this paper, we consider the problem of\n\\textit{shifted label distribution}, which is caused by the inconsistency\nbetween the noisy-labeled training set subject to external knowledge graph and\nthe human-annotated test set, and exacerbated by the pipelined\nentity-then-relation extraction manner with noise propagation. We propose a\njoint extraction approach to address this problem by re-labeling noisy\ninstances with a group of cooperative multiagents. To handle noisy instances in\na fine-grained manner, each agent in the cooperative group evaluates the\ninstance by calculating a continuous confidence score from its own perspective;\nTo leverage the correlations between these two extraction tasks, a confidence\nconsensus module is designed to gather the wisdom of all agents and\nre-distribute the noisy training set with confidence-scored labels. Further,\nthe confidences are used to adjust the training losses of extractors.\nExperimental results on two real-world datasets verify the benefits of\nre-labeling noisy instance, and show that the proposed model significantly\noutperforms the state-of-the-art entity and relation extraction methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 12:03:04 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Chen", "Daoyuan", ""], ["Li", "Yaliang", ""], ["Lei", "Kai", ""], ["Shen", "Ying", ""]]}, {"id": "2004.09936", "submitter": "Daksh Varshneya", "authors": "Tanja Bunk, Daksh Varshneya, Vladimir Vlasov, Alan Nichol", "title": "DIET: Lightweight Language Understanding for Dialogue Systems", "comments": "v3: Updated results for the best model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pre-trained language models have shown impressive results on\nlanguage understanding benchmarks like GLUE and SuperGLUE, improving\nconsiderably over other pre-training methods like distributed representations\n(GloVe) and purely supervised approaches. We introduce the Dual Intent and\nEntity Transformer (DIET) architecture, and study the effectiveness of\ndifferent pre-trained representations on intent and entity prediction, two\ncommon dialogue language understanding tasks. DIET advances the state of the\nart on a complex multi-domain NLU dataset and achieves similarly high\nperformance on other simpler datasets. Surprisingly, we show that there is no\nclear benefit to using large pre-trained models for this task, and in fact DIET\nimproves upon the current state of the art even in a purely supervised setup\nwithout any pre-trained embeddings. Our best performing model outperforms\nfine-tuning BERT and is about six times faster to train.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 12:10:48 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 07:41:07 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 06:13:56 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Bunk", "Tanja", ""], ["Varshneya", "Daksh", ""], ["Vlasov", "Vladimir", ""], ["Nichol", "Alan", ""]]}, {"id": "2004.09968", "submitter": "Viet Duong", "authors": "Viet Duong, Phu Pham, Tongyu Yang, Yu Wang, Jiebo Luo", "title": "The Ivory Tower Lost: How College Students Respond Differently than the\n  General Public to the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the pandemic of the novel Coronavirus Disease-2019 (COVID-19) has\npresented governments with ultimate challenges. In the United States, the\ncountry with the highest confirmed COVID-19 infection cases, a nationwide\nsocial distancing protocol has been implemented by the President. For the first\ntime in a hundred years since the 1918 flu pandemic, the US population is\nmandated to stay in their households and avoid public contact. As a result, the\nmajority of public venues and services have ceased their operations. Following\nthe closure of the University of Washington on March 7th, more than a thousand\ncolleges and universities in the United States have cancelled in-person classes\nand campus activities, impacting millions of students. This paper aims to\ndiscover the social implications of this unprecedented disruption in our\ninteractive society regarding both the general public and higher education\npopulations by mining people's opinions on social media. We discover several\ntopics embedded in a large number of COVID-19 tweets that represent the most\ncentral issues related to the pandemic, which are of great concerns for both\ncollege students and the general public. Moreover, we find significant\ndifferences between these two groups of Twitter users with respect to the\nsentiments they expressed towards the COVID-19 issues. To our best knowledge,\nthis is the first social media-based study which focuses on the college student\ncommunity's demographics and responses to prevalent social issues during a\nmajor crisis.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:02:38 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Duong", "Viet", ""], ["Pham", "Phu", ""], ["Yang", "Tongyu", ""], ["Wang", "Yu", ""], ["Luo", "Jiebo", ""]]}, {"id": "2004.09974", "submitter": "Canxiang Yan", "authors": "Canxiang Yan, Jianhao Yan, Yangyin Xu, Cheng Niu, Jie Zhou", "title": "Learning to Encode Evolutionary Knowledge for Automatic Commenting Long\n  Novels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static knowledge graph has been incorporated extensively into\nsequence-to-sequence framework for text generation. While effectively\nrepresenting structured context, static knowledge graph failed to represent\nknowledge evolution, which is required in modeling dynamic events. In this\npaper, an automatic commenting task is proposed for long novels, which involves\nunderstanding context of more than tens of thousands of words. To model the\ndynamic storyline, especially the transitions of the characters and their\nrelations, Evolutionary Knowledge Graph(EKG) is proposed and learned within a\nmulti-task framework. Given a specific passage to comment, sequential modeling\nis used to incorporate historical and future embedding for context\nrepresentation. Further, a graph-to-sequence model is designed to utilize the\nEKG for comment generation. Extensive experimental results show that our\nEKG-based method is superior to several strong baselines on both automatic and\nhuman evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:09:50 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Yan", "Canxiang", ""], ["Yan", "Jianhao", ""], ["Xu", "Yangyin", ""], ["Niu", "Cheng", ""], ["Zhou", "Jie", ""]]}, {"id": "2004.09980", "submitter": "David Graus", "authors": "Feng Lu, Anca Dumitrache, David Graus", "title": "Beyond Optimizing for Clicks: Incorporating Editorial Values in News\n  Recommendation", "comments": "To appear in UMAP 2020", "journal-ref": null, "doi": "10.1145/3340631.3394864", "report-no": null, "categories": "cs.IR cs.CL cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the uptake of algorithmic personalization in the news domain, news\norganizations increasingly trust automated systems with previously considered\neditorial responsibilities, e.g., prioritizing news to readers. In this paper\nwe study an automated news recommender system in the context of a news\norganization's editorial values. We conduct and present two online studies with\na news recommender system, which span one and a half months and involve over\n1,200 users. In our first study we explore how our news recommender steers\nreading behavior in the context of editorial values such as serendipity,\ndynamism, diversity, and coverage. Next, we present an intervention study where\nwe extend our news recommender to steer our readers to more dynamic reading\nbehavior. We find that (i) our recommender system yields more diverse reading\nbehavior and yields a higher coverage of articles compared to non-personalized\neditorial rankings, and (ii) we can successfully incorporate dynamism in our\nrecommender system as a re-ranking method, effectively steering our readers to\nmore dynamic articles without hurting our recommender system's accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:24:49 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lu", "Feng", ""], ["Dumitrache", "Anca", ""], ["Graus", "David", ""]]}, {"id": "2004.09984", "submitter": "Linyang Li", "authors": "Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu", "title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT", "comments": "Accepted by EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks for discrete data (such as texts) have been proved\nsignificantly more challenging than continuous data (such as images) since it\nis difficult to generate adversarial samples with gradient-based methods.\nCurrent successful attack methods for texts usually adopt heuristic replacement\nstrategies on the character or word level, which remains challenging to find\nthe optimal solution in the massive space of possible combinations of\nreplacements while preserving semantic consistency and language fluency. In\nthis paper, we propose \\textbf{BERT-Attack}, a high-quality and effective\nmethod to generate adversarial samples using pre-trained masked language models\nexemplified by BERT. We turn BERT against its fine-tuned models and other deep\nneural models in downstream tasks so that we can successfully mislead the\ntarget models to predict incorrectly. Our method outperforms state-of-the-art\nattack strategies in both success rate and perturb percentage, while the\ngenerated adversarial samples are fluent and semantically preserved. Also, the\ncost of calculation is low, thus possible for large-scale generations. The code\nis available at https://github.com/LinyangLee/BERT-Attack.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:30:02 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 02:02:23 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 03:08:04 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Li", "Linyang", ""], ["Ma", "Ruotian", ""], ["Guo", "Qipeng", ""], ["Xue", "Xiangyang", ""], ["Qiu", "Xipeng", ""]]}, {"id": "2004.10009", "submitter": "Lianwei Wu", "authors": "Lianwei Wu and Yuan Rao", "title": "Adaptive Interaction Fusion Networks for Fake News Detection", "comments": "Accepted at the 24th European Conference on Artificial Intelligence\n  (ECAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of existing methods for fake news detection universally focus on\nlearning and fusing various features for detection. However, the learning of\nvarious features is independent, which leads to a lack of cross-interaction\nfusion between features on social media, especially between posts and comments.\nGenerally, in fake news, there are emotional associations and semantic\nconflicts between posts and comments. How to represent and fuse the\ncross-interaction between both is a key challenge. In this paper, we propose\nAdaptive Interaction Fusion Networks (AIFN) to fulfill cross-interaction fusion\namong features for fake news detection. In AIFN, to discover semantic\nconflicts, we design gated adaptive interaction networks (GAIN) to capture\nadaptively similar semantics and conflicting semantics between posts and\ncomments. To establish feature associations, we devise semantic-level fusion\nself-attention networks (SFSN) to enhance semantic correlations and fusion\namong features. Extensive experiments on two real-world datasets, i.e.,\nRumourEval and PHEME, demonstrate that AIFN achieves the state-of-the-art\nperformance and boosts accuracy by more than 2.05% and 1.90%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:51:03 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Wu", "Lianwei", ""], ["Rao", "Yuan", ""]]}, {"id": "2004.10035", "submitter": "Mohammed Belkhatir", "authors": "Bhawani Selvaretnam, Mohammed Belkhatir", "title": "Leveraging Cognitive Search Patterns to Enhance Automated Natural\n  Language Retrieval Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search of information in large text repositories has been plagued by the\nso-called document-query vocabulary gap, i.e. the semantic discordance between\nthe contents in the stored document entities on the one hand and the human\nquery on the other hand. Over the past two decades, a significant body of works\nhas advanced technical retrieval prowess while several studies have shed light\non issues pertaining to human search behavior. We believe that these efforts\nshould be conjoined, in the sense that automated retrieval systems have to\nfully emulate human search behavior and thus consider the procedure according\nto which users incrementally enhance their initial query. To this end,\ncognitive reformulation patterns that mimic user search behaviour are\nhighlighted and enhancement terms which are statistically collocated with or\nlexical-semantically related to the original terms adopted in the retrieval\nprocess. We formalize the application of these patterns by considering a query\nconceptual representation and introducing a set of operations allowing to\noperate modifications on the initial query. A genetic algorithm-based weighting\nprocess allows placing emphasis on terms according to their conceptual\nrole-type. An experimental evaluation on real-world datasets against relevance,\nlanguage, conceptual and knowledge-based models is conducted. We also show,\nwhen compared to language and relevance models, a better performance in terms\nof mean average precision than a word embedding-based model instantiation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:13:33 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Selvaretnam", "Bhawani", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2004.10037", "submitter": "Jing Zhang", "authors": "Yanhui Peng and Jing Zhang", "title": "LineaRE: Simple but Powerful Knowledge Graph Embedding for Link\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of link prediction for knowledge graphs is to predict missing\nrelationships between entities. Knowledge graph embedding, which aims to\nrepresent entities and relations of a knowledge graph as low dimensional\nvectors in a continuous vector space, has achieved promising predictive\nperformance. If an embedding model can cover different types of connectivity\npatterns and mapping properties of relations as many as possible, it will\npotentially bring more benefits for link prediction tasks. In this paper, we\npropose a novel embedding model, namely LineaRE, which is capable of modeling\nfour connectivity patterns (i.e., symmetry, antisymmetry, inversion, and\ncomposition) and four mapping properties (i.e., one-to-one, one-to-many,\nmany-to-one, and many-to-many) of relations. Specifically, we regard knowledge\ngraph embedding as a simple linear regression task, where a relation is modeled\nas a linear function of two low-dimensional vector-presented entities with two\nweight vectors and a bias vector. Since the vectors are defined in a real\nnumber space and the scoring function of the model is linear, our model is\nsimple and scalable to large knowledge graphs. Experimental results on multiple\nwidely used real-world datasets show that the proposed LineaRE model\nsignificantly outperforms existing state-of-the-art models for link prediction\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:19:43 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 05:55:39 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Peng", "Yanhui", ""], ["Zhang", "Jing", ""]]}, {"id": "2004.10051", "submitter": "Yu-Ming Shang", "authors": "Yuming Shang, Heyan Huang, Xin Sun, Xianling Mao", "title": "Learning Relation Ties with a Force-Directed Graph in Distant Supervised\n  Relation Extraction", "comments": "Learning Relation Ties", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation ties, defined as the correlation and mutual exclusion between\ndifferent relations, are critical for distant supervised relation extraction.\nExisting approaches model this property by greedily learning local\ndependencies. However, they are essentially limited by failing to capture the\nglobal topology structure of relation ties. As a result, they may easily fall\ninto a locally optimal solution. To solve this problem, in this paper, we\npropose a novel force-directed graph based relation extraction model to\ncomprehensively learn relation ties. Specifically, we first build a graph\naccording to the global co-occurrence of relations. Then, we borrow the idea of\nCoulomb's Law from physics and introduce the concept of attractive force and\nrepulsive force to this graph to learn correlation and mutual exclusion between\nrelations. Finally, the obtained relation representations are applied as an\ninter-dependent relation classifier. Experimental results on a large scale\nbenchmark dataset demonstrate that our model is capable of modeling global\nrelation ties and significantly outperforms other baselines. Furthermore, the\nproposed force-directed graph can be used as a module to augment existing\nrelation extraction systems and improve their performance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:41:38 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Shang", "Yuming", ""], ["Huang", "Heyan", ""], ["Sun", "Xin", ""], ["Mao", "Xianling", ""]]}, {"id": "2004.10087", "submitter": "Libo Qin", "authors": "Libo Qin, Xiao Xu, Wanxiang Che, Ting Liu", "title": "AGIF: An Adaptive Graph-Interactive Framework for Joint Multiple Intent\n  Detection and Slot Filling", "comments": "Accepted at Findings of EMNLP 2020. Data and code are available at\n  this [URL] (https://github.com/LooperXX/AGIF)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world scenarios, users usually have multiple intents in the same\nutterance. Unfortunately, most spoken language understanding (SLU) models\neither mainly focused on the single intent scenario, or simply incorporated an\noverall intent context vector for all tokens, ignoring the fine-grained\nmultiple intents information integration for token-level slot prediction. In\nthis paper, we propose an Adaptive Graph-Interactive Framework (AGIF) for joint\nmultiple intent detection and slot filling, where we introduce an intent-slot\ngraph interaction layer to model the strong correlation between the slot and\nintents. Such an interaction layer is applied to each token adaptively, which\nhas the advantage to automatically extract the relevant intents information,\nmaking a fine-grained intent information integration for the token-level slot\nprediction. Experimental results on three multi-intent datasets show that our\nframework obtains substantial improvement and achieves the state-of-the-art\nperformance. In addition, our framework achieves new state-of-the-art\nperformance on two single-intent datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:07:34 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 12:44:09 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 02:23:43 GMT"}, {"version": "v4", "created": "Sat, 17 Oct 2020 04:28:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Qin", "Libo", ""], ["Xu", "Xiao", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""]]}, {"id": "2004.10093", "submitter": "Chengyi Wang", "authors": "Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou and Zhenglu Yang", "title": "Curriculum Pre-training for End-to-End Speech Translation", "comments": "accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end speech translation poses a heavy burden on the encoder, because it\nhas to transcribe, understand, and learn cross-lingual semantics\nsimultaneously. To obtain a powerful encoder, traditional methods pre-train it\non ASR data to capture speech features. However, we argue that pre-training the\nencoder only through simple speech recognition is not enough and high-level\nlinguistic knowledge should be considered. Inspired by this, we propose a\ncurriculum pre-training method that includes an elementary course for\ntranscription learning and two advanced courses for understanding the utterance\nand mapping words in two languages. The difficulty of these courses is\ngradually increasing. Experiments show that our curriculum pre-training method\nleads to significant improvements on En-De and En-Fr speech translation\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:12:07 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Wang", "Chengyi", ""], ["Wu", "Yu", ""], ["Liu", "Shujie", ""], ["Zhou", "Ming", ""], ["Yang", "Zhenglu", ""]]}, {"id": "2004.10102", "submitter": "Goro Kobayashi", "authors": "Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui", "title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms", "comments": "19 pages, accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention is a key component of Transformers, which have recently achieved\nconsiderable success in natural language processing. Hence, attention is being\nextensively studied to investigate various linguistic capabilities of\nTransformers, focusing on analyzing the parallels between attention weights and\nspecific linguistic phenomena. This paper shows that attention weights alone\nare only one of the two factors that determine the output of attention and\nproposes a norm-based analysis that incorporates the second factor, the norm of\nthe transformed input vectors. The findings of our norm-based analyses of BERT\nand a Transformer-based neural machine translation system include the\nfollowing: (i) contrary to previous studies, BERT pays poor attention to\nspecial tokens, and (ii) reasonable word alignment can be extracted from\nattention mechanisms of Transformer. These findings provide insights into the\ninner workings of Transformers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:22:27 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 15:15:38 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Kobayashi", "Goro", ""], ["Kuribayashi", "Tatsuki", ""], ["Yokoi", "Sho", ""], ["Inui", "Kentaro", ""]]}, {"id": "2004.10150", "submitter": "Reinald Kim Amplayo", "authors": "Reinald Kim Amplayo and Mirella Lapata", "title": "Unsupervised Opinion Summarization with Noising and Denoising", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The supervised training of high-capacity models on large datasets containing\nhundreds of thousands of document-summary pairs is critical to the recent\nsuccess of deep learning techniques for abstractive summarization.\nUnfortunately, in most domains (other than news) such training data is not\navailable and cannot be easily sourced. In this paper we enable the use of\nsupervised learning for the setting where there are only documents available\n(e.g.,~product or business reviews) without ground truth summaries. We create a\nsynthetic dataset from a corpus of user reviews by sampling a review,\npretending it is a summary, and generating noisy versions thereof which we\ntreat as pseudo-review input. We introduce several linguistically motivated\nnoise generation functions and a summarization model which learns to denoise\nthe input and generate the original review. At test time, the model accepts\ngenuine reviews and generates a summary containing salient opinions, treating\nthose that do not reach consensus as noise. Extensive automatic and human\nevaluation shows that our model brings substantial improvements over both\nabstractive and extractive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 16:54:57 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Amplayo", "Reinald Kim", ""], ["Lapata", "Mirella", ""]]}, {"id": "2004.10151", "submitter": "Ari Holtzman", "authors": "Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua\n  Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May,\n  Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian", "title": "Experience Grounds Language", "comments": "Empirical Methods in Natural Language Processing (EMNLP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language understanding research is held back by a failure to relate language\nto the physical world it describes and to the social interactions it\nfacilitates. Despite the incredible effectiveness of language processing models\nto tackle tasks after being trained on text alone, successful linguistic\ncommunication relies on a shared experience of the world. It is this shared\nexperience that makes utterances meaningful.\n  Natural language processing is a diverse field, and progress throughout its\ndevelopment has come from new representational theories, modeling techniques,\ndata collection paradigms, and tasks. We posit that the present success of\nrepresentation learning approaches trained on large, text-only corpora requires\nthe parallel tradition of research on the broader physical and social context\nof language to address the deeper questions of communication.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 16:56:27 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 02:03:56 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 00:40:12 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bisk", "Yonatan", ""], ["Holtzman", "Ari", ""], ["Thomason", "Jesse", ""], ["Andreas", "Jacob", ""], ["Bengio", "Yoshua", ""], ["Chai", "Joyce", ""], ["Lapata", "Mirella", ""], ["Lazaridou", "Angeliki", ""], ["May", "Jonathan", ""], ["Nisnevich", "Aleksandr", ""], ["Pinto", "Nicolas", ""], ["Turian", "Joseph", ""]]}, {"id": "2004.10157", "submitter": "Akari Asai", "authors": "Akari Asai, Hannaneh Hajishirzi", "title": "Logic-Guided Data Augmentation and Regularization for Consistent\n  Question Answering", "comments": "Published as a conference paper at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many natural language questions require qualitative, quantitative or logical\ncomparisons between two entities or events. This paper addresses the problem of\nimproving the accuracy and consistency of responses to comparison questions by\nintegrating logic rules and neural models. Our method leverages logical and\nlinguistic knowledge to augment labeled training data and then uses a\nconsistency-based regularizer to train the model. Improving the global\nconsistency of predictions, our approach achieves large improvements over\nprevious methods in a variety of question answering (QA) tasks including\nmultiple-choice qualitative reasoning, cause-effect reasoning, and extractive\nmachine reading comprehension. In particular, our method significantly improves\nthe performance of RoBERTa-based models by 1-5% across datasets. We advance the\nstate of the art by around 5-8% on WIQA and QuaRel and reduce consistency\nviolations by 58% on HotpotQA. We further demonstrate that our approach can\nlearn effectively from limited data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 17:03:08 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 17:53:40 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Asai", "Akari", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2004.10171", "submitter": "Haipeng Sun", "authors": "Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, and\n  Tiejun Zhao", "title": "Knowledge Distillation for Multilingual Unsupervised Neural Machine\n  Translation", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised neural machine translation (UNMT) has recently achieved\nremarkable results for several language pairs. However, it can only translate\nbetween a single language pair and cannot produce translation results for\nmultiple language pairs at the same time. That is, research on multilingual\nUNMT has been limited. In this paper, we empirically introduce a simple method\nto translate between thirteen languages using a single encoder and a single\ndecoder, making use of multilingual data to improve UNMT for all language\npairs. On the basis of the empirical findings, we propose two knowledge\ndistillation methods to further enhance multilingual UNMT performance. Our\nexperiments on a dataset with English translated to and from twelve other\nlanguages (including three language families and six language branches) show\nremarkable results, surpassing strong unsupervised individual baselines while\nachieving promising performance between non-English language pairs in zero-shot\ntranslation scenarios and alleviating poor performance in low-resource language\npairs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 17:26:16 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Sun", "Haipeng", ""], ["Wang", "Rui", ""], ["Chen", "Kehai", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""], ["Zhao", "Tiejun", ""]]}, {"id": "2004.10188", "submitter": "Marc'Aurelio Ranzato", "authors": "Anton Bakhtin and Yuntian Deng and Sam Gross and Myle Ott and\n  Marc'Aurelio Ranzato and Arthur Szlam", "title": "Residual Energy-Based Models for Text", "comments": "long journal version", "journal-ref": "Journal of Machine Learning Research 21 (2020) 1-41", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current large-scale auto-regressive language models display impressive\nfluency and can generate convincing text. In this work we start by asking the\nquestion: Can the generations of these models be reliably distinguished from\nreal text by statistical discriminators? We find experimentally that the answer\nis affirmative when we have access to the training data for the model, and\nguardedly affirmative even if we do not.\n  This suggests that the auto-regressive models can be improved by\nincorporating the (globally normalized) discriminators into the generative\nprocess. We give a formalism for this using the Energy-Based Model framework,\nand show that it indeed improves the results of the generative models, measured\nboth in terms of perplexity and in terms of human evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:44:03 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 15:50:36 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bakhtin", "Anton", ""], ["Deng", "Yuntian", ""], ["Gross", "Sam", ""], ["Ott", "Myle", ""], ["Ranzato", "Marc'Aurelio", ""], ["Szlam", "Arthur", ""]]}, {"id": "2004.10201", "submitter": "Payam Karisani", "authors": "Payam Karisani, Joyce C. Ho, and Eugene Agichtein", "title": "Domain-Guided Task Decomposition with Self-Training for Detecting\n  Personal Events in Social Media", "comments": "WWW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining social media content for tasks such as detecting personal experiences\nor events, suffer from lexical sparsity, insufficient training data, and\ninventive lexicons. To reduce the burden of creating extensive labeled data and\nimprove classification performance, we propose to perform these tasks in two\nsteps: 1. Decomposing the task into domain-specific sub-tasks by identifying\nkey concepts, thus utilizing human domain understanding; and 2. Combining the\nresults of learners for each key concept using co-training to reduce the\nrequirements for labeled training data. We empirically show the effectiveness\nand generality of our approach, Co-Decomp, using three representative social\nmedia mining tasks, namely Personal Health Mention detection, Crisis Report\ndetection, and Adverse Drug Reaction monitoring. The experiments show that our\nmodel is able to outperform the state-of-the-art text classification\nmodels--including those using the recently introduced BERT model--when small\namounts of training data are available.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:50:31 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Karisani", "Payam", ""], ["Ho", "Joyce C.", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2004.10220", "submitter": "Andriy Mulyar", "authors": "Andriy Mulyar and Bridget T. McInnes", "title": "MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical notes contain an abundance of important but not-readily accessible\ninformation about patients. Systems to automatically extract this information\nrely on large amounts of training data for which their exists limited resources\nto create. Furthermore, they are developed dis-jointly; meaning that no\ninformation can be shared amongst task-specific systems. This bottle-neck\nunnecessarily complicates practical application, reduces the performance\ncapabilities of each individual solution and associates the engineering debt of\nmanaging multiple information extraction systems. We address these challenges\nby developing Multitask-Clinical BERT: a single deep learning model that\nsimultaneously performs eight clinical tasks spanning entity extraction, PHI\nidentification, language entailment and similarity by sharing representations\namongst tasks. We find our single system performs competitively with all\nstate-the-art task-specific systems while also benefiting from massive\ncomputational benefits at inference.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 18:04:08 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Mulyar", "Andriy", ""], ["McInnes", "Bridget T.", ""]]}, {"id": "2004.10234", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson\n  Enrique Yalta Soplin, Tomoki Hayashi, Shinji Watanabe", "title": "ESPnet-ST: All-in-One Speech Translation Toolkit", "comments": "Accepted at ACL 2020 System Demonstration (update Table1, fix typo)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ESPnet-ST, which is designed for the quick development of\nspeech-to-speech translation systems in a single framework. ESPnet-ST is a new\nproject inside end-to-end speech processing toolkit, ESPnet, which integrates\nor newly implements automatic speech recognition, machine translation, and\ntext-to-speech functions for speech translation. We provide all-in-one recipes\nincluding data pre-processing, feature extraction, training, and decoding\npipelines for a wide range of benchmark datasets. Our reproducible results can\nmatch or even outperform the current state-of-the-art performances; these\npre-trained models are downloadable. The toolkit is publicly available at\nhttps://github.com/espnet/espnet.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 18:38:38 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 12:28:18 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Kiyono", "Shun", ""], ["Duh", "Kevin", ""], ["Karita", "Shigeki", ""], ["Soplin", "Nelson Enrique Yalta", ""], ["Hayashi", "Tomoki", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2004.10270", "submitter": "Sebastin Santy", "authors": "Devansh Mehta, Sebastin Santy, Ramaravind Kommiya Mothilal, Brij Mohan\n  Lal Srivastava, Alok Sharma, Anurag Shukla, Vishnu Prasad, Venkanna U, Amit\n  Sharma, Kalika Bali", "title": "Learnings from Technological Interventions in a Low Resource Language: A\n  Case-Study on Gondi", "comments": "Accepted at LREC 2020 (7 pages). D.M. and S.S. contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The primary obstacle to developing technologies for low-resource languages is\nthe lack of usable data. In this paper, we report the adoption and deployment\nof 4 technology-driven methods of data collection for Gondi, a low-resource\nvulnerable language spoken by around 2.3 million tribal people in south and\ncentral India. In the process of data collection, we also help in its revival\nby expanding access to information in Gondi through the creation of linguistic\nresources that can be used by the community, such as a dictionary, children's\nstories, an app with Gondi content from multiple sources and an Interactive\nVoice Response (IVR) based mass awareness platform. At the end of these\ninterventions, we collected a little less than 12,000 translated words and/or\nsentences and identified more than 650 community members whose help can be\nsolicited for future translation efforts. The larger goal of the project is\ncollecting enough data in Gondi to build and deploy viable language\ntechnologies like machine translation and speech to text systems that can help\ntake the language onto the internet.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 20:03:57 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 03:44:44 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Mehta", "Devansh", ""], ["Santy", "Sebastin", ""], ["Mothilal", "Ramaravind Kommiya", ""], ["Srivastava", "Brij Mohan Lal", ""], ["Sharma", "Alok", ""], ["Shukla", "Anurag", ""], ["Prasad", "Vishnu", ""], ["U", "Venkanna", ""], ["Sharma", "Amit", ""], ["Bali", "Kalika", ""]]}, {"id": "2004.10283", "submitter": "Georg Rehm", "authors": "Georg Rehm", "title": "Observations on Annotations", "comments": "To be published in: Annotations in Scholarly Editions and Research:\n  Functions, Differentiation, Systematization (2020), Julia Nantke and Frederik\n  Schlupkothen (editors). De Gruyter. In print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The annotation of textual information is a fundamental activity in\nLinguistics and Computational Linguistics. This article presents various\nobservations on annotations. It approaches the topic from several angles\nincluding Hypertext, Computational Linguistics and Language Technology,\nArtificial Intelligence and Open Science. Annotations can be examined along\ndifferent dimensions. In terms of complexity, they can range from trivial to\nhighly sophisticated, in terms of maturity from experimental to standardised.\nAnnotations can be annotated themselves using more abstract annotations.\nPrimary research data such as, e.g., text documents can be annotated on\ndifferent layers concurrently, which are independent but can be exploited using\nmulti-layer querying. Standards guarantee interoperability and reusability of\ndata sets. The chapter concludes with four final observations, formulated as\nresearch questions or rather provocative remarks on the current state of\nannotation research.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 20:29:50 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Rehm", "Georg", ""]]}, {"id": "2004.10320", "submitter": "Yanan Jia", "authors": "Yanan Jia and Sony SungChu", "title": "A Deep Learning System for Sentiment Analysis of Service Calls", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis is crucial for the advancement of artificial intelligence\n(AI). Sentiment understanding can help AI to replicate human language and\ndiscourse. Studying the formation and response of sentiment state from\nwell-trained Customer Service Representatives (CSRs) can help make the\ninteraction between humans and AI more intelligent. In this paper, a sentiment\nanalysis pipeline is first carried out with respect to real-world multi-party\nconversations - that is, service calls. Based on the acoustic and linguistic\nfeatures extracted from the source information, a novel aggregated method for\nvoice sentiment recognition framework is built. Each party's sentiment pattern\nduring the communication is investigated along with the interaction sentiment\npattern between all parties.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 22:02:43 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Jia", "Yanan", ""], ["SungChu", "Sony", ""]]}, {"id": "2004.10349", "submitter": "Ahmed Sabir", "authors": "Ahmed Sabir, Francesc Moreno-Noguer and Llu\\'is Padr\\'o", "title": "Textual Visual Semantic Dataset for Text Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Spotting in the wild consists of detecting and recognizing text\nappearing in images (e.g. signboards, traffic signals or brands in clothing or\nobjects). This is a challenging problem due to the complexity of the context\nwhere texts appear (uneven backgrounds, shading, occlusions, perspective\ndistortions, etc.). Only a few approaches try to exploit the relation between\ntext and its surrounding environment to better recognize text in the scene. In\nthis paper, we propose a visual context dataset for Text Spotting in the wild,\nwhere the publicly available dataset COCO-text [Veit et al. 2016] has been\nextended with information about the scene (such as objects and places appearing\nin the image) to enable researchers to include semantic relations between texts\nand scene in their Text Spotting systems, and to offer a common framework for\nsuch approaches. For each text in an image, we extract three kinds of context\ninformation: objects in the scene, image location label and a textual image\ndescription (caption). We use state-of-the-art out-of-the-box available tools\nto extract this additional information. Since this information has textual\nform, it can be used to leverage text similarity or semantic relation methods\ninto Text Spotting systems, either as a post-processing or in an end-to-end\ntraining strategy. Our data is publicly available at https://git.io/JeZTb.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 23:58:16 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Sabir", "Ahmed", ""], ["Moreno-Noguer", "Francesc", ""], ["Padr\u00f3", "Llu\u00eds", ""]]}, {"id": "2004.10353", "submitter": "Aryaman Arora", "authors": "Aryaman Arora, Luke Gessler, Nathan Schneider", "title": "Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in\n  Hindi and Punjabi", "comments": "4 pages, 1 figure. To be published in the 2020 Annual Conference of\n  the Association for Computational Linguistics (https://acl2020.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one\nexception: whether a schwa represented in the orthography is pronounced or\nunpronounced (deleted). Previous work has attempted to predict schwa deletion\nin a rule-based fashion using prosodic or phonetic analysis. We present the\nfirst statistical schwa deletion classifier for Hindi, which relies solely on\nthe orthography as the input and outperforms previous approaches. We trained\nour model on a newly-compiled pronunciation lexicon extracted from various\nonline dictionaries. Our best Hindi model achieves state of the art\nperformance, and also achieves good performance on a closely related language,\nPunjabi, without modification.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 00:53:40 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 16:47:46 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Arora", "Aryaman", ""], ["Gessler", "Luke", ""], ["Schneider", "Nathan", ""]]}, {"id": "2004.10361", "submitter": "Pinjia He", "authors": "Pinjia He, Clara Meister, Zhendong Su", "title": "Testing Machine Translation via Referential Transparency", "comments": "Accepted by ICSE21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation software has seen rapid progress in recent years due to\nthe advancement of deep neural networks. People routinely use machine\ntranslation software in their daily lives, such as ordering food in a foreign\nrestaurant, receiving medical diagnosis and treatment from foreign doctors, and\nreading international political news online. However, due to the complexity and\nintractability of the underlying neural networks, modern machine translation\nsoftware is still far from robust and can produce poor or incorrect\ntranslations; this can lead to misunderstanding, financial loss, threats to\npersonal safety and health, and political conflicts. To address this problem,\nwe introduce referentially transparent inputs (RTIs), a simple, widely\napplicable methodology for validating machine translation software. A\nreferentially transparent input is a piece of text that should have similar\ntranslations when used in different contexts. Our practical implementation,\nPurity, detects when this property is broken by a translation. To evaluate RTI,\nwe use Purity to test Google Translate and Bing Microsoft Translator with 200\nunlabeled sentences, which detected 123 and 142 erroneous translations with\nhigh precision (79.3% and 78.3%). The translation errors are diverse, including\nexamples of under-translation, over-translation, word/phrase mistranslation,\nincorrect modification, and unclear logic.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 01:37:18 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 12:56:49 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["He", "Pinjia", ""], ["Meister", "Clara", ""], ["Su", "Zhendong", ""]]}, {"id": "2004.10404", "submitter": "Wenhu Chen", "authors": "Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen and William Yang Wang", "title": "Logical Natural Language Generation from Open-Domain Tables", "comments": "Accepted to ACL 2020 as Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural natural language generation (NLG) models have recently shown\nremarkable progress in fluency and coherence. However, existing studies on\nneural NLG are primarily focused on surface-level realizations with limited\nemphasis on logical inference, an important aspect of human thinking and\nlanguage. In this paper, we suggest a new NLG task where a model is tasked with\ngenerating natural language statements that can be \\emph{logically entailed} by\nthe facts in an open-domain semi-structured table. To facilitate the study of\nthe proposed logical NLG problem, we use the existing TabFact dataset\n\\cite{chen2019tabfact} featured with a wide range of logical/symbolic\ninferences as our testbed, and propose new automatic metrics to evaluate the\nfidelity of generation models w.r.t.\\ logical inference. The new task poses\nchallenges to the existing monotonic generation frameworks due to the mismatch\nbetween sequence order and logical order. In our experiments, we\ncomprehensively survey different generation architectures (LSTM, Transformer,\nPre-Trained LM) trained with different algorithms (RL, Adversarial Training,\nCoarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained\nLM can significantly boost both the fluency and logical fidelity metrics, 2) RL\nand Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine\ngeneration can help partially alleviate the fidelity issue while maintaining\nhigh language fluency. The code and data are available at\n\\url{https://github.com/wenhuchen/LogicNLG}.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 06:03:10 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 00:26:21 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Chen", "Wenhu", ""], ["Chen", "Jianshu", ""], ["Su", "Yu", ""], ["Chen", "Zhiyu", ""], ["Wang", "William Yang", ""]]}, {"id": "2004.10450", "submitter": "Hugh Zhang", "authors": "Hugh Zhang, Daniel Duckworth, Daphne Ippolito, Arvind Neelakantan", "title": "Trading Off Diversity and Quality in Natural Language Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For open-ended language generation tasks such as storytelling and dialogue,\nchoosing the right decoding algorithm is critical to controlling the tradeoff\nbetween generation quality and diversity. However, there presently exists no\nconsensus on which decoding procedure is best or even the criteria by which to\ncompare them. We address these issues by casting decoding as a multi-objective\noptimization problem aiming to simultaneously maximize both response quality\nand diversity. Our framework enables us to perform the first large-scale\nevaluation of decoding methods along the entire quality-diversity spectrum. We\nfind that when diversity is a priority, all methods perform similarly, but when\nquality is viewed as more important, the recently proposed nucleus sampling\n(Holtzman et al. 2019) outperforms all other evaluated decoding algorithms. Our\nexperiments also confirm the existence of the `likelihood trap', the\ncounter-intuitive observation that high likelihood sequences are often\nsurprisingly low quality. We leverage our findings to create and evaluate an\nalgorithm called \\emph{selective sampling} which tractably approximates\nglobally-normalized temperature sampling.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 09:12:10 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Zhang", "Hugh", ""], ["Duckworth", "Daniel", ""], ["Ippolito", "Daphne", ""], ["Neelakantan", "Arvind", ""]]}, {"id": "2004.10454", "submitter": "Yi Ren", "authors": "Yi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng Zhao, Tie-Yan Liu", "title": "A Study of Non-autoregressive Model for Sequence Generation", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-autoregressive (NAR) models generate all the tokens of a sequence in\nparallel, resulting in faster generation speed compared to their autoregressive\n(AR) counterparts but at the cost of lower accuracy. Different techniques\nincluding knowledge distillation and source-target alignment have been proposed\nto bridge the gap between AR and NAR models in various tasks such as neural\nmachine translation (NMT), automatic speech recognition (ASR), and text to\nspeech (TTS). With the help of those techniques, NAR models can catch up with\nthe accuracy of AR models in some tasks but not in some others. In this work,\nwe conduct a study to understand the difficulty of NAR sequence generation and\ntry to answer: (1) Why NAR models can catch up with AR models in some tasks but\nnot all? (2) Why techniques like knowledge distillation and source-target\nalignment can help NAR models. Since the main difference between AR and NAR\nmodels is that NAR models do not use dependency among target tokens while AR\nmodels do, intuitively the difficulty of NAR sequence generation heavily\ndepends on the strongness of dependency among target tokens. To quantify such\ndependency, we propose an analysis model called CoMMA to characterize the\ndifficulty of different NAR sequence generation tasks. We have several\ninteresting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most\ntarget-token dependency while TTS has the least. 2) Knowledge distillation\nreduces the target-token dependency in target sequence and thus improves the\naccuracy of NAR models. 3) Source-target alignment constraint encourages\ndependency of a target token on source tokens and thus eases the training of\nNAR models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 09:16:09 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 00:17:11 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ren", "Yi", ""], ["Liu", "Jinglin", ""], ["Tan", "Xu", ""], ["Zhao", "Zhou", ""], ["Zhao", "Sheng", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2004.10462", "submitter": "Rui Liu", "authors": "Rui Liu, Zheng Lin, Weiping Wang", "title": "Keyphrase Prediction With Pre-trained Language Model", "comments": "7pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, generative methods have been widely used in keyphrase prediction,\nthanks to their capability to produce both present keyphrases that appear in\nthe source text and absent keyphrases that do not match any source text.\nHowever, the absent keyphrases are generated at the cost of the performance on\npresent keyphrase prediction, since previous works mainly use generative models\nthat rely on the copying mechanism and select words step by step. Besides, the\nextractive model that directly extracts a text span is more suitable for\npredicting the present keyphrase. Considering the different characteristics of\nextractive and generative methods, we propose to divide the keyphrase\nprediction into two subtasks, i.e., present keyphrase extraction (PKE) and\nabsent keyphrase generation (AKG), to fully exploit their respective\nadvantages. On this basis, a joint inference framework is proposed to make the\nmost of BERT in two subtasks. For PKE, we tackle this task as a sequence\nlabeling problem with the pre-trained language model BERT. For AKG, we\nintroduce a Transformer-based architecture, which fully integrates the present\nkeyphrase knowledge learned from PKE by the fine-tuned BERT. The experimental\nresults show that our approach can achieve state-of-the-art results on both\ntasks on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 09:35:02 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Liu", "Rui", ""], ["Lin", "Zheng", ""], ["Wang", "Weiping", ""]]}, {"id": "2004.10473", "submitter": "Johannes E. M. Mosig", "authors": "Johannes E. M. Mosig, Vladimir Vlasov, Alan Nichol", "title": "Where is the context? -- A critique of recent dialogue datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent dialogue datasets like MultiWOZ 2.1 and Taskmaster-1 constitute some\nof the most challenging tasks for present-day dialogue models and, therefore,\nare widely used for system evaluation. We identify several issues with the\nabove-mentioned datasets, such as history independence, strong knowledge base\ndependence, and ambiguous system responses. Finally, we outline key desiderata\nfor future datasets that we believe would be more suitable for the construction\nof conversational artificial intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 10:05:52 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Mosig", "Johannes E. M.", ""], ["Vlasov", "Vladimir", ""], ["Nichol", "Alan", ""]]}, {"id": "2004.10519", "submitter": "Mathias Louboutin", "authors": "Mathias Louboutin, Fabio Luporini, Philipp Witte, Rhodri Nelson,\n  George Bisbas, Jan Thorbecke, Felix J. Herrmann, and Gerard Gorman", "title": "Scaling through abstractions -- high-performance vectorial wave\n  simulations for seismic inversion with Devito", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CL cs.PF physics.ao-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  [Devito] is an open-source Python project based on domain-specific language\nand compiler technology. Driven by the requirements of rapid HPC applications\ndevelopment in exploration seismology, the language and compiler have evolved\nsignificantly since inception. Sophisticated boundary conditions, tensor\ncontractions, sparse operations and features such as staggered grids and\nsub-domains are all supported; operators of essentially arbitrary complexity\ncan be generated. To accommodate this flexibility whilst ensuring performance,\ndata dependency analysis is utilized to schedule loops and detect\ncomputational-properties such as parallelism. In this article, the generation\nand simulation of MPI-parallel propagators (along with their adjoints) for the\npseudo-acoustic wave-equation in tilted transverse isotropic media and the\nelastic wave-equation are presented. Simulations are carried out on industry\nscale synthetic models in a HPC Cloud system and reach a performance of\n28TFLOP/s, hence demonstrating Devito's suitability for production-grade\nseismic inversion problems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 12:20:07 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Louboutin", "Mathias", ""], ["Luporini", "Fabio", ""], ["Witte", "Philipp", ""], ["Nelson", "Rhodri", ""], ["Bisbas", "George", ""], ["Thorbecke", "Jan", ""], ["Herrmann", "Felix J.", ""], ["Gorman", "Gerard", ""]]}, {"id": "2004.10581", "submitter": "Yunsu Kim", "authors": "Yunsu Kim, Miguel Gra\\c{c}a, Hermann Ney", "title": "When and Why is Unsupervised Neural Machine Translation Useless?", "comments": "Will appear at EAMT 2020; Extended version of EAMT camera-ready\n  (including appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the practicality of the current state-of-the-art\nunsupervised methods in neural machine translation (NMT). In ten translation\ntasks with various data settings, we analyze the conditions under which the\nunsupervised methods fail to produce reasonable translations. We show that\ntheir performance is severely affected by linguistic dissimilarity and domain\nmismatch between source and target monolingual data. Such conditions are common\nfor low-resource language pairs, where unsupervised learning works poorly. In\nall of our experiments, supervised and semi-supervised baselines with\n50k-sentence bilingual data outperform the best unsupervised results. Our\nanalyses pinpoint the limits of the current unsupervised NMT and also suggest\nimmediate research directions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 14:00:55 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Kim", "Yunsu", ""], ["Gra\u00e7a", "Miguel", ""], ["Ney", "Hermann", ""]]}, {"id": "2004.10603", "submitter": "Yang Zhao", "authors": "Yang Zhao, Ping Yu, Suchismit Mahapatra, Qinliang Su and Changyou Chen", "title": "Improve Variational Autoencoder for Text Generationwith Discrete Latent\n  Bottleneck", "comments": "replaced", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) are essential tools in end-to-end\nrepresentation learning. However, the sequential text generation common pitfall\nwith VAEs is that the model tends to ignore latent variables with a strong\nauto-regressive decoder. In this paper, we propose a principled approach to\nalleviate this issue by applying a discretized bottleneck to enforce an\nimplicit latent feature matching in a more compact latent space. We impose a\nshared discrete latent space where each input is learned to choose a\ncombination of latent atoms as a regularized latent representation. Our model\nendows a promising capability to model underlying semantics of discrete\nsequences and thus provide more interpretative latent structures. Empirically,\nwe demonstrate our model's efficiency and effectiveness on a broad range of\ntasks, including language modeling, unaligned text style transfer, dialog\nresponse generation, and neural machine translation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 14:41:37 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 16:16:28 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zhao", "Yang", ""], ["Yu", "Ping", ""], ["Mahapatra", "Suchismit", ""], ["Su", "Qinliang", ""], ["Chen", "Changyou", ""]]}, {"id": "2004.10610", "submitter": "Irene Li", "authors": "Irene Li, Alexander Fabbri, Swapnil Hingmire and Dragomir Radev", "title": "R-VGAE: Relational-variational Graph Autoencoder for Unsupervised\n  Prerequisite Chain Learning", "comments": "2 Figures, 3 Tables, 9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of concept prerequisite chain learning is to automatically determine\nthe existence of prerequisite relationships among concept pairs. In this paper,\nwe frame learning prerequisite relationships among concepts as an unsupervised\ntask with no access to labeled concept pairs during training. We propose a\nmodel called the Relational-Variational Graph AutoEncoder (R-VGAE) to predict\nconcept relations within a graph consisting of concept and resource nodes.\nResults show that our unsupervised approach outperforms graph-based\nsemi-supervised methods and other baseline methods by up to 9.77% and 10.47% in\nterms of prerequisite relation prediction accuracy and F1 score. Our method is\nnotably the first graph-based model that attempts to make use of deep learning\nrepresentations for the task of unsupervised prerequisite learning. We also\nexpand an existing corpus which totals 1,717 English Natural Language\nProcessing (NLP)-related lecture slide files and manual concept pair\nannotations over 322 topics.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 14:48:03 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Li", "Irene", ""], ["Fabbri", "Alexander", ""], ["Hingmire", "Swapnil", ""], ["Radev", "Dragomir", ""]]}, {"id": "2004.10624", "submitter": "Angrosh Mandya", "authors": "Angrosh Mandya, Danushka Bollegala and Frans Coenen", "title": "Contextualised Graph Attention for Improved Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a contextualized graph attention network that combines\nedge features and multiple sub-graphs for improving relation extraction. A\nnovel method is proposed to use multiple sub-graphs to learn rich node\nrepresentations in graph-based networks. To this end multiple sub-graphs are\nobtained from a single dependency tree. Two types of edge features are\nproposed, which are effectively combined with GAT and GCN models to apply for\nrelation extraction. The proposed model achieves state-of-the-art performance\non Semeval 2010 Task 8 dataset, achieving an F1-score of 86.3.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:04:52 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Mandya", "Angrosh", ""], ["Bollegala", "Danushka", ""], ["Coenen", "Frans", ""]]}, {"id": "2004.10640", "submitter": "Genet Asefa Gesese", "authors": "Genet Asefa Gesese, Mehwish Alam and Harald Sack", "title": "Semantic Entity Enrichment by Leveraging Multilingual Descriptions for\n  Link Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most Knowledge Graphs (KGs) contain textual descriptions of entities in\nvarious natural languages. These descriptions of entities provide valuable\ninformation that may not be explicitly represented in the structured part of\nthe KG. Based on this fact, some link prediction methods which make use of the\ninformation presented in the textual descriptions of entities have been\nproposed to learn representations of (monolingual) KGs. However, these methods\nuse entity descriptions in only one language and ignore the fact that\ndescriptions given in different languages may provide complementary information\nand thereby also additional semantics. In this position paper, the problem of\neffectively leveraging multilingual entity descriptions for the purpose of link\nprediction in KGs will be discussed along with potential solutions to the\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:34:11 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Gesese", "Genet Asefa", ""], ["Alam", "Mehwish", ""], ["Sack", "Harald", ""]]}, {"id": "2004.10643", "submitter": "Sebastian Schuster", "authors": "Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan\n  Haji\\v{c}, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis\n  Tyers, Daniel Zeman", "title": "Universal Dependencies v2: An Evergrowing Multilingual Treebank\n  Collection", "comments": "LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Universal Dependencies is an open community effort to create\ncross-linguistically consistent treebank annotation for many languages within a\ndependency-based lexicalist framework. The annotation consists in a\nlinguistically motivated word segmentation; a morphological layer comprising\nlemmas, universal part-of-speech tags, and standardized morphological features;\nand a syntactic layer focusing on syntactic relations between predicates,\narguments and modifiers. In this paper, we describe version 2 of the guidelines\n(UD v2), discuss the major changes from UD v1 to UD v2, and give an overview of\nthe currently available treebanks for 90 languages.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:38:18 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Nivre", "Joakim", ""], ["de Marneffe", "Marie-Catherine", ""], ["Ginter", "Filip", ""], ["Haji\u010d", "Jan", ""], ["Manning", "Christopher D.", ""], ["Pyysalo", "Sampo", ""], ["Schuster", "Sebastian", ""], ["Tyers", "Francis", ""], ["Zeman", "Daniel", ""]]}, {"id": "2004.10645", "submitter": "Sewon Min", "authors": "Sewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer", "title": "AmbigQA: Answering Ambiguous Open-domain Questions", "comments": "Published as a conference paper at EMNLP 2020 (long)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambiguity is inherent to open-domain question answering; especially when\nexploring new topics, it can be difficult to ask questions that have a single,\nunambiguous answer. In this paper, we introduce AmbigQA, a new open-domain\nquestion answering task which involves finding every plausible answer, and then\nrewriting the question for each one to resolve the ambiguity. To study this\ntask, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open,\nan existing open-domain QA benchmark. We find that over half of the questions\nin NQ-open are ambiguous, with diverse sources of ambiguity such as event and\nentity references. We also present strong baseline models for AmbigQA which we\nshow benefit from weakly supervised learning that incorporates NQ-open,\nstrongly suggesting our new task and data will support significant future\nresearch effort. Our data and baselines are available at\nhttps://nlp.cs.washington.edu/ambigqa.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:42:13 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 03:28:21 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Min", "Sewon", ""], ["Michael", "Julian", ""], ["Hajishirzi", "Hannaneh", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2004.10663", "submitter": "Chenghua Lin", "authors": "Dingmin Wang, Chenghua Lin, Li Zhong, Kam-Fai Wong", "title": "Fast and Scalable Dialogue State Tracking with Explicit Modular\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast and scalable architecture called Explicit Modular\nDecomposition (EMD), in which we incorporate both classification-based and\nextraction-based methods and design four modules (for classification and\nsequence labelling) to jointly extract dialogue states. Experimental results\nbased on the MultiWoz 2.0 dataset validates the superiority of our proposed\nmodel in terms of both complexity and scalability when compared to the\nstate-of-the-art methods, especially in the scenario of multi-domain dialogues\nentangled with many turns of utterances.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 16:00:09 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 14:44:08 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Dingmin", ""], ["Lin", "Chenghua", ""], ["Zhong", "Li", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "2004.10703", "submitter": "Arun Maiya", "authors": "Arun S. Maiya", "title": "ktrain: A Low-Code Library for Augmented Machine Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ktrain, a low-code Python library that makes machine learning more\naccessible and easier to apply. As a wrapper to TensorFlow and many other\nlibraries (e.g., transformers, scikit-learn, stellargraph), it is designed to\nmake sophisticated, state-of-the-art machine learning models simple to build,\ntrain, inspect, and apply by both beginners and experienced practitioners.\nFeaturing modules that support text data (e.g., text classification, sequence\ntagging, open-domain question-answering), vision data (e.g., image\nclassification), graph data (e.g., node classification, link prediction), and\ntabular data, ktrain presents a simple unified interface enabling one to\nquickly solve a wide range of tasks in as little as three or four \"commands\" or\nlines of code.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 14:18:20 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 11:48:35 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 15:50:12 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 21:25:30 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Maiya", "Arun S.", ""]]}, {"id": "2004.10706", "submitter": "Lucy Lu Wang", "authors": "Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas,\n  Jiangjiang Yang, Doug Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis,\n  Rodney Kinney, Yunyao Li, Ziyang Liu, William Merrill, Paul Mooney, Dewey\n  Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex\n  Wade, Kuansan Wang, Nancy Xin Ru Wang, Chris Wilhelm, Boya Xie, Douglas\n  Raymond, Daniel S. Weld, Oren Etzioni, Sebastian Kohlmeier", "title": "CORD-19: The COVID-19 Open Research Dataset", "comments": "ACL NLP-COVID Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 Open Research Dataset (CORD-19) is a growing resource of\nscientific papers on COVID-19 and related historical coronavirus research.\nCORD-19 is designed to facilitate the development of text mining and\ninformation retrieval systems over its rich collection of metadata and\nstructured full text papers. Since its release, CORD-19 has been downloaded\nover 200K times and has served as the basis of many COVID-19 text mining and\ndiscovery systems. In this article, we describe the mechanics of dataset\nconstruction, highlighting challenges and key design decisions, provide an\noverview of how CORD-19 has been used, and describe several shared tasks built\naround the dataset. We hope this resource will continue to bring together the\ncomputing community, biomedical experts, and policy makers in the search for\neffective treatments and management policies for COVID-19.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:10:18 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 02:44:54 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 18:42:13 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 21:40:34 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wang", "Lucy Lu", ""], ["Lo", "Kyle", ""], ["Chandrasekhar", "Yoganand", ""], ["Reas", "Russell", ""], ["Yang", "Jiangjiang", ""], ["Burdick", "Doug", ""], ["Eide", "Darrin", ""], ["Funk", "Kathryn", ""], ["Katsis", "Yannis", ""], ["Kinney", "Rodney", ""], ["Li", "Yunyao", ""], ["Liu", "Ziyang", ""], ["Merrill", "William", ""], ["Mooney", "Paul", ""], ["Murdick", "Dewey", ""], ["Rishi", "Devvret", ""], ["Sheehan", "Jerry", ""], ["Shen", "Zhihong", ""], ["Stilson", "Brandon", ""], ["Wade", "Alex", ""], ["Wang", "Kuansan", ""], ["Wang", "Nancy Xin Ru", ""], ["Wilhelm", "Chris", ""], ["Xie", "Boya", ""], ["Raymond", "Douglas", ""], ["Weld", "Daniel S.", ""], ["Etzioni", "Oren", ""], ["Kohlmeier", "Sebastian", ""]]}, {"id": "2004.10741", "submitter": "James Hefford", "authors": "James Hefford, Vincent Wang, Matthew Wilson", "title": "Categories of Semantic Concepts", "comments": "Accepted at SemSpace 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO math.CT quant-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modelling concept representation is a foundational problem in the study of\ncognition and linguistics. This work builds on the confluence of conceptual\ntools from G\\\"ardenfors semantic spaces, categorical compositional linguistics,\nand applied category theory to present a domain-independent and categorical\nformalism of 'concept'.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:50:04 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 20:15:30 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Hefford", "James", ""], ["Wang", "Vincent", ""], ["Wilson", "Matthew", ""]]}, {"id": "2004.10793", "submitter": "Jason Krone", "authors": "Jason Krone, Yi Zhang, Mona Diab", "title": "Learning to Classify Intents and Slot Labels Given a Handful of Examples", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intent classification (IC) and slot filling (SF) are core components in most\ngoal-oriented dialogue systems. Current IC/SF models perform poorly when the\nnumber of training examples per class is small. We propose a new few-shot\nlearning task, few-shot IC/SF, to study and improve the performance of IC and\nSF models on classes not seen at training time in ultra low resource scenarios.\nWe establish a few-shot IC/SF benchmark by defining few-shot splits for three\npublic IC/SF datasets, ATIS, TOP, and Snips. We show that two popular few-shot\nlearning algorithms, model agnostic meta learning (MAML) and prototypical\nnetworks, outperform a fine-tuning baseline on this benchmark. Prototypical\nnetworks achieves significant gains in IC performance on the ATIS and TOP\ndatasets, while both prototypical networks and MAML outperform the baseline\nwith respect to SF on all three datasets. In addition, we demonstrate that\njoint training as well as the use of pre-trained language models, ELMo and BERT\nin our case, are complementary to these few-shot learning methods and yield\nfurther gains.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 18:54:38 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Krone", "Jason", ""], ["Zhang", "Yi", ""], ["Diab", "Mona", ""]]}, {"id": "2004.10796", "submitter": "Jae Sung Park", "authors": "Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi,\n  Yejin Choi", "title": "VisualCOMET: Reasoning about the Dynamic Context of a Still Image", "comments": "Project Page: http://visualcomet.xyz (ECCV 2020 Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even from a single frame of a still image, people can reason about the\ndynamic story of the image before, after, and beyond the frame. For example,\ngiven an image of a man struggling to stay afloat in water, we can reason that\nthe man fell into the water sometime in the past, the intent of that man at the\nmoment is to stay alive, and he will need help in the near future or else he\nwill get washed away. We propose VisualComet, the novel framework of visual\ncommonsense reasoning tasks to predict events that might have happened before,\nevents that might happen next, and the intents of the people at present. To\nsupport research toward visual commonsense reasoning, we introduce the first\nlarge-scale repository of Visual Commonsense Graphs that consists of over 1.4\nmillion textual descriptions of visual commonsense inferences carefully\nannotated over a diverse set of 60,000 images, each paired with short video\nsummaries of before and after. In addition, we provide person-grounding (i.e.,\nco-reference links) between people appearing in the image and people mentioned\nin the textual commonsense descriptions, allowing for tighter integration\nbetween images and text. We establish strong baseline performances on this task\nand demonstrate that integration between visual and textual commonsense\nreasoning is the key and wins over non-integrative alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:02:20 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 15:37:10 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 13:11:10 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Park", "Jae Sung", ""], ["Bhagavatula", "Chandra", ""], ["Mottaghi", "Roozbeh", ""], ["Farhadi", "Ali", ""], ["Choi", "Yejin", ""]]}, {"id": "2004.10799", "submitter": "Aleksandr Laptev", "authors": "Andrei Andrusenko, Aleksandr Laptev, Ivan Medennikov", "title": "Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner\n  Party Transcription", "comments": "Accepted by Interspeech 2020", "journal-ref": null, "doi": "10.21437/Interspeech.2020-1074", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While end-to-end ASR systems have proven competitive with the conventional\nhybrid approach, they are prone to accuracy degradation when it comes to noisy\nand low-resource conditions. In this paper, we argue that, even in such\ndifficult cases, some end-to-end approaches show performance close to the\nhybrid baseline. To demonstrate this, we use the CHiME-6 Challenge data as an\nexample of challenging environments and noisy conditions of everyday speech. We\nexperimentally compare and analyze CTC-Attention versus RNN-Transducer\napproaches along with RNN versus Transformer architectures. We also provide a\ncomparison of acoustic features and speech enhancements. Besides, we evaluate\nthe effectiveness of neural network language models for hypothesis re-scoring\nin low-resource conditions. Our best end-to-end model based on RNN-Transducer,\ntogether with improved beam search, reaches quality by only 3.8% WER abs. worse\nthan the LF-MMI TDNN-F CHiME-6 Challenge baseline. With the Guided Source\nSeparation based training data augmentation, this approach outperforms the\nhybrid baseline system by 2.7% WER abs. and the end-to-end system best known\nbefore by 25.7% WER abs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:08:33 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 18:12:17 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 19:36:44 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Andrusenko", "Andrei", ""], ["Laptev", "Aleksandr", ""], ["Medennikov", "Ivan", ""]]}, {"id": "2004.10809", "submitter": "Vikash Balasubramanian", "authors": "Vikash Balasubramanian, Ivan Kobyzev, Hareesh Bahuleyan, Ilya Shapiro,\n  Olga Vechtomova", "title": "Polarized-VAE: Proximity Based Disentangled Representation Learning for\n  Text Generation", "comments": "Camera Ready for EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representations of real-world data is a challenging\nopen problem. Most previous methods have focused on either supervised\napproaches which use attribute labels or unsupervised approaches that\nmanipulate the factorization in the latent space of models such as the\nvariational autoencoder (VAE) by training with task-specific losses. In this\nwork, we propose polarized-VAE, an approach that disentangles select attributes\nin the latent space based on proximity measures reflecting the similarity\nbetween data points with respect to these attributes. We apply our method to\ndisentangle the semantics and syntax of sentences and carry out transfer\nexperiments. Polarized-VAE outperforms the VAE baseline and is competitive with\nstate-of-the-art approaches, while being more a general framework that is\napplicable to other attribute disentanglement tasks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:26:09 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 03:43:24 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Balasubramanian", "Vikash", ""], ["Kobyzev", "Ivan", ""], ["Bahuleyan", "Hareesh", ""], ["Shapiro", "Ilya", ""], ["Vechtomova", "Olga", ""]]}, {"id": "2004.10813", "submitter": "Ryokan Ri", "authors": "Ryokan Ri and Yoshimasa Tsuruoka", "title": "Revisiting the Context Window for Cross-lingual Word Embeddings", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing approaches to mapping-based cross-lingual word embeddings are based\non the assumption that the source and target embedding spaces are structurally\nsimilar. The structures of embedding spaces largely depend on the co-occurrence\nstatistics of each word, which the choice of context window determines. Despite\nthis obvious connection between the context window and mapping-based\ncross-lingual embeddings, their relationship has been underexplored in prior\nwork. In this work, we provide a thorough evaluation, in various languages,\ndomains, and tasks, of bilingual embeddings trained with different context\nwindows. The highlight of our findings is that increasing the size of both the\nsource and target window sizes improves the performance of bilingual lexicon\ninduction, especially the performance on frequent nouns.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:29:43 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ri", "Ryokan", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "2004.10816", "submitter": "Majid Asgari-Bidhendi", "authors": "Majid Asgari-Bidhendi, Farzane Fakhrian and Behrouz Minaei-Bidgoli", "title": "ParsEL 1.0: Unsupervised Entity Linking in Persian Social Media Texts", "comments": "8 pages, 3 figures. ParsEL service (source code is available in\n  github)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, social media data has exponentially increased, which can be\nenumerated as one of the largest data repositories in the world. A large\nportion of this social media data is natural language text. However, the\nnatural language is highly ambiguous due to exposure to the frequent\noccurrences of entities, which have polysemous words or phrases. Entity linking\nis the task of linking the entity mentions in the text to their corresponding\nentities in a knowledge base. Recently, FarsBase, a Persian knowledge graph,\nhas been introduced containing almost half a million entities. In this paper,\nwe propose an unsupervised Persian Entity Linking system, the first entity\nlinking system specially focused on the Persian language, which utilizes\ncontext-dependent and context-independent features. For this purpose, we also\npublish the first entity linking corpus of the Persian language containing\n67,595 words that have been crawled from social media texts of some popular\nchannels in the Telegram messenger. The output of the proposed method is 86.94%\nf-score for the Persian language, which is comparable with the similar\nstate-of-the-art methods in the English language.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:34:13 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Asgari-Bidhendi", "Majid", ""], ["Fakhrian", "Farzane", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "2004.10827", "submitter": "Tal Linzen", "authors": "Tal Linzen and Marco Baroni", "title": "Syntactic Structure from Deep Learning", "comments": "In press at Annual Reviews of Linguistics", "journal-ref": null, "doi": "10.1146/annurev-linguistics-032020-051035", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks achieve impressive performance in engineering\napplications that require extensive linguistic skills, such as machine\ntranslation. This success has sparked interest in probing whether these models\nare inducing human-like grammatical knowledge from the raw data they are\nexposed to, and, consequently, whether they can shed new light on long-standing\ndebates concerning the innate structure necessary for language acquisition. In\nthis article, we survey representative studies of the syntactic abilities of\ndeep networks, and discuss the broader implications that this work has for\ntheoretical linguistics.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 20:02:49 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Linzen", "Tal", ""], ["Baroni", "Marco", ""]]}, {"id": "2004.10863", "submitter": "Canlin Zhang", "authors": "Canlin Zhang and Xiuwen Liu", "title": "Preserving the Hypernym Tree of WordNet in Dense Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a novel way to generate low-dimension (dense)\nvector embeddings for the noun and verb synsets in WordNet, so that the\nhypernym-hyponym tree structure is preserved in the embeddings. We call this\nembedding the sense spectrum (and sense spectra for embeddings). In order to\ncreate suitable labels for the training of sense spectra, we designed a new\nsimilarity measurement for noun and verb synsets in WordNet. We call this\nsimilarity measurement the hypernym intersection similarity (HIS), since it\ncompares the common and unique hypernyms between two synsets.\n  Our experiments show that on the noun and verb pairs of the SimLex-999\ndataset, HIS outperforms the three similarity measurements in WordNet.\nMoreover, to the best of our knowledge, the sense spectra is the first dense\nembedding system that can explicitly and completely measure the\nhypernym-hyponym relationship in WordNet.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 21:09:47 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Zhang", "Canlin", ""], ["Liu", "Xiuwen", ""]]}, {"id": "2004.10899", "submitter": "Irene Li", "authors": "Irene Li, Yixin Li, Tianxiao Li, Sergio Alvarez-Napagao, Dario\n  Garcia-Gasulla and Toyotaro Suzumura", "title": "What are We Depressed about When We Talk about COVID19: Mental Health\n  Analysis on Tweets Using Natural Language Processing", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of coronavirus disease 2019 (COVID-19) recently has affected\nhuman life to a great extent. Besides direct physical and economic threats, the\npandemic also indirectly impact people's mental health conditions, which can be\noverwhelming but difficult to measure. The problem may come from various\nreasons such as unemployment status, stay-at-home policy, fear for the virus,\nand so forth. In this work, we focus on applying natural language processing\n(NLP) techniques to analyze tweets in terms of mental health. We trained deep\nmodels that classify each tweet into the following emotions: anger,\nanticipation, disgust, fear, joy, sadness, surprise and trust. We build the\nEmoCT (Emotion-Covid19-Tweet) dataset for the training purpose by manually\nlabeling 1,000 English tweets. Furthermore, we propose and compare two methods\nto find out the reasons that are causing sadness and fear.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 23:45:04 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 18:32:01 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 23:06:46 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Li", "Irene", ""], ["Li", "Yixin", ""], ["Li", "Tianxiao", ""], ["Alvarez-Napagao", "Sergio", ""], ["Garcia-Gasulla", "Dario", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "2004.10919", "submitter": "Shuangyong Song", "authors": "Shuangyong Song, Chao Wang", "title": "TCNN: Triple Convolutional Neural Network Models for Retrieval-based\n  Question Answering System in E-commerce", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic question-answering (QA) systems have boomed during last few years,\nand commonly used techniques can be roughly categorized into Information\nRetrieval (IR)-based and generation-based. A key solution to the IR based\nmodels is to retrieve the most similar knowledge entries of a given query from\na QA knowledge base, and then rerank those knowledge entries with semantic\nmatching models. In this paper, we aim to improve an IR based e-commerce QA\nsystem-AliMe with proposed text matching models, including a basic Triple\nConvolutional Neural Network (TCNN) model and two Attention-based TCNN (ATCNN)\nmodels. Experimental results show their effect.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 01:02:15 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Song", "Shuangyong", ""], ["Wang", "Chao", ""]]}, {"id": "2004.10964", "submitter": "Suchin Gururangan", "authors": "Suchin Gururangan, Ana Marasovi\\'c, Swabha Swayamdipta, Kyle Lo, Iz\n  Beltagy, Doug Downey, Noah A. Smith", "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models pretrained on text from a wide variety of sources form the\nfoundation of today's NLP. In light of the success of these broad-coverage\nmodels, we investigate whether it is still helpful to tailor a pretrained model\nto the domain of a target task. We present a study across four domains\n(biomedical and computer science publications, news, and reviews) and eight\nclassification tasks, showing that a second phase of pretraining in-domain\n(domain-adaptive pretraining) leads to performance gains, under both high- and\nlow-resource settings. Moreover, adapting to the task's unlabeled data\n(task-adaptive pretraining) improves performance even after domain-adaptive\npretraining. Finally, we show that adapting to a task corpus augmented using\nsimple data selection strategies is an effective alternative, especially when\nresources for domain-adaptive pretraining might be unavailable. Overall, we\nconsistently find that multi-phase adaptive pretraining offers large gains in\ntask performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 04:21:19 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 05:07:34 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 22:00:44 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Gururangan", "Suchin", ""], ["Marasovi\u0107", "Ana", ""], ["Swayamdipta", "Swabha", ""], ["Lo", "Kyle", ""], ["Beltagy", "Iz", ""], ["Downey", "Doug", ""], ["Smith", "Noah A.", ""]]}, {"id": "2004.10966", "submitter": "Tasmia Tasrin", "authors": "Tasmia Tasrin, Md Sultan Al Nahian and Brent Harrison", "title": "Visual Question Answering Using Semantic Information from Image\n  Descriptions", "comments": "6 pages, 5 figures, The 34th International FLAIRS Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a deep neural architecture that uses an attention\nmechanism which utilizes region based image features, the natural language\nquestion asked, and semantic knowledge extracted from the regions of an image\nto produce open-ended answers for questions asked in a visual question\nanswering (VQA) task. The combination of both region based features and region\nbased textual information about the image bolsters a model to more accurately\nrespond to questions and potentially do so with less required training data. We\nevaluate our proposed architecture on a VQA task against a strong baseline and\nshow that our method achieves excellent results on this task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 04:35:04 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 18:09:22 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tasrin", "Tasmia", ""], ["Nahian", "Md Sultan Al", ""], ["Harrison", "Brent", ""]]}, {"id": "2004.10972", "submitter": "Jiaao Chen", "authors": "Jiaao Chen, Yuwei Wu, Diyi Yang", "title": "Semi-Supervised Models via Data Augmentationfor Classifying Interactive\n  Affective Responses", "comments": "The AAAI-20 Workshop On Affective Content Analysis AFFCON2020:\n  Interactive Affective Response", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present semi-supervised models with data augmentation (SMDA), a\nsemi-supervised text classification system to classify interactive affective\nresponses. SMDA utilizes recent transformer-based models to encode each\nsentence and employs back translation techniques to paraphrase given sentences\nas augmented data. For labeled sentences, we performed data augmentations to\nuniform the label distributions and computed supervised loss during training\nprocess. For unlabeled sentences, we explored self-training by regarding\nlow-entropy predictions over unlabeled sentences as pseudo labels, assuming\nhigh-confidence predictions as labeled data for training. We further introduced\nconsistency regularization as unsupervised loss after data augmentations on\nunlabeled data, based on the assumption that the model should predict similar\nclass distributions with original unlabeled sentences as input and augmented\nsentences as input. Via a set of experiments, we demonstrated that our system\noutperformed baseline models in terms of F1-score and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 05:02:31 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Chen", "Jiaao", ""], ["Wu", "Yuwei", ""], ["Yang", "Diyi", ""]]}, {"id": "2004.11005", "submitter": "Fabio Calefato", "authors": "Nicole Novielli, Fabio Calefato, Filippo Lanubile", "title": "Love, Joy, Anger, Sadness, Fear, and Surprise: SE Needs Special Kinds of\n  AI: A Case Study on Text Mining and SE", "comments": null, "journal-ref": "IEEE Software May/June 2020, Vol. 37, No. 3, pp. 86-91", "doi": "10.1109/MS.2020.2968557", "report-no": null, "categories": "cs.SE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do you like your code? What kind of code makes developers happiest? What\nmakes them angriest? Is it possible to monitor the mood of a large team of\ncoders to determine when and where a codebase needs additional help?\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 07:11:12 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Novielli", "Nicole", ""], ["Calefato", "Fabio", ""], ["Lanubile", "Filippo", ""]]}, {"id": "2004.11019", "submitter": "Libo Qin", "authors": "Libo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, Ting Liu", "title": "Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown remarkable success in end-to-end task-oriented\ndialog system. However, most neural models rely on large training data, which\nare only available for a certain number of task domains, such as navigation and\nscheduling.\n  This makes it difficult to scalable for a new domain with limited labeled\ndata. However, there has been relatively little research on how to effectively\nuse data from all domains to improve the performance of each domain and also\nunseen domains. To this end, we investigate methods that can make explicit use\nof domain knowledge and introduce a shared-private network to learn shared and\nspecific knowledge. In addition, we propose a novel Dynamic Fusion Network\n(DF-Net) which automatically exploit the relevance between the target domain\nand each domain. Results show that our model outperforms existing methods on\nmulti-domain dialogue, giving the state-of-the-art in the literature. Besides,\nwith little training data, we show its transferability by outperforming prior\nbest model by 13.9\\% on average.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 08:17:22 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 02:52:21 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 13:20:43 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Qin", "Libo", ""], ["Xu", "Xiao", ""], ["Che", "Wanxiang", ""], ["Zhang", "Yue", ""], ["Liu", "Ting", ""]]}, {"id": "2004.11026", "submitter": "Shashi Narayan", "authors": "Shashi Narayan, Gon\\c{c}alo Simoes, Ji Ma, Hannah Craighead and Ryan\n  Mcdonald", "title": "QURIOUS: Question Generation Pretraining for Text Generation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in natural language processing using pretraining have shifted\nfocus towards pretraining and fine-tuning approaches for text generation. Often\nthe focus has been on task-agnostic approaches that generalize the language\nmodeling objective. We propose question generation as a pretraining method,\nwhich better aligns with the text generation objectives. Our text generation\nmodels pretrained with this method are better at understanding the essence of\nthe input and are better language models for the target task. When evaluated on\ntwo text generation tasks, abstractive summarization and answer-focused\nquestion generation, our models result in state-of-the-art performances in\nterms of automatic metrics. Human evaluators also found our summaries and\ngenerated questions to be more natural, concise and informative.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 08:41:52 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Narayan", "Shashi", ""], ["Simoes", "Gon\u00e7alo", ""], ["Ma", "Ji", ""], ["Craighead", "Hannah", ""], ["Mcdonald", "Ryan", ""]]}, {"id": "2004.11045", "submitter": "Amir Vakili Tahami", "authors": "Amir Vakili Tahami, Kamyar Ghajar, Azadeh Shakery", "title": "Distilling Knowledge for Fast Retrieval-based Chat-bots", "comments": "Accepted for publication in the 43rd International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response retrieval is a subset of neural ranking in which a model selects a\nsuitable response from a set of candidates given a conversation history.\nRetrieval-based chat-bots are typically employed in information seeking\nconversational systems such as customer support agents. In order to make\npairwise comparisons between a conversation history and a candidate response,\ntwo approaches are common: cross-encoders performing full self-attention over\nthe pair and bi-encoders encoding the pair separately. The former gives better\nprediction quality but is too slow for practical use. In this paper, we propose\na new cross-encoder architecture and transfer knowledge from this model to a\nbi-encoder model using distillation. This effectively boosts bi-encoder\nperformance at no cost during inference time. We perform a detailed analysis of\nthis approach on three response retrieval datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 09:41:37 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Tahami", "Amir Vakili", ""], ["Ghajar", "Kamyar", ""], ["Shakery", "Azadeh", ""]]}, {"id": "2004.11054", "submitter": "Philip John Gorinski", "authors": "Gabriel Gordon-Hall, Philip John Gorinski, Shay B. Cohen", "title": "Learning Dialog Policies from Weak Demonstrations", "comments": "9 pages + 2 pages references + 1 page appendices, 6 figures, 2\n  tables, 1 algorithm, accepted as long paper at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning is a promising approach to training a dialog\nmanager, but current methods struggle with the large state and action spaces of\nmulti-domain dialog systems. Building upon Deep Q-learning from Demonstrations\n(DQfD), an algorithm that scores highly in difficult Atari games, we leverage\ndialog data to guide the agent to successfully respond to a user's requests. We\nmake progressively fewer assumptions about the data needed, using labeled,\nreduced-labeled, and even unlabeled data to train expert demonstrators. We\nintroduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to\novercome the domain gap between the datasets and the environment. Experiments\nin a challenging multi-domain dialog system framework validate our approaches,\nand get high success rates even when trained on out-of-domain data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 10:22:16 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 16:02:03 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Gordon-Hall", "Gabriel", ""], ["Gorinski", "Philip John", ""], ["Cohen", "Shay B.", ""]]}, {"id": "2004.11081", "submitter": "Mohammed Belkhatir", "authors": "Mohammed Maree, Mohammed Belkhatir", "title": "Coupling semantic and statistical techniques for dynamically enriching\n  web ontologies", "comments": null, "journal-ref": "Journal Intelligent Information Systems 2013", "doi": "10.1007/s10844-012-0233-4", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of the Semantic Web technology, the use of ontologies to\nstore and retrieve information covering several domains has increased. However,\nvery few ontologies are able to cope with the ever-growing need of frequently\nupdated semantic information or specific user requirements in specialized\ndomains. As a result, a critical issue is related to the unavailability of\nrelational information between concepts, also coined missing background\nknowledge. One solution to address this issue relies on the manual enrichment\nof ontologies by domain experts which is however a time consuming and costly\nprocess, hence the need for dynamic ontology enrichment. In this paper we\npresent an automatic coupled statistical/semantic framework for dynamically\nenriching large-scale generic ontologies from the World Wide Web. Using the\nmassive amount of information encoded in texts on the Web as a corpus, missing\nbackground knowledge can therefore be discovered through a combination of\nsemantic relatedness measures and pattern acquisition techniques and\nsubsequently exploited. The benefits of our approach are: (i) proposing the\ndynamic enrichment of large-scale generic ontologies with missing background\nknowledge, and thus, enabling the reuse of such knowledge, (ii) dealing with\nthe issue of costly ontological manual enrichment by domain experts.\nExperimental results in a precision-based evaluation setting demonstrate the\neffectiveness of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:21:30 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Maree", "Mohammed", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2004.11083", "submitter": "Mohammed Belkhatir", "authors": "Bhawani Selvaretnam, Mohammed Belkhatir", "title": "Coupled intrinsic and extrinsic human language resource-based query\n  expansion", "comments": null, "journal-ref": "Knowledge & Information Systems 2018", "doi": "10.1007/s10115-018-1267-x", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poor information retrieval performance has often been attributed to the\nquery-document vocabulary mismatch problem which is defined as the difficulty\nfor human users to formulate precise natural language queries that are in line\nwith the vocabulary of the documents deemed relevant to a specific search goal.\nTo alleviate this problem, query expansion processes are applied in order to\nspawn and integrate additional terms to an initial query. This requires\naccurate identification of main query concepts to ensure the intended search\ngoal is duly emphasized and relevant expansion concepts are extracted and\nincluded in the enriched query. Natural language queries have intrinsic\nlinguistic properties such as parts-of-speech labels and grammatical relations\nwhich can be utilized in determining the intended search goal. Additionally,\nextrinsic language-based resources such as ontologies are needed to suggest\nexpansion concepts semantically coherent with the query content. We present\nhere a query expansion framework which capitalizes on both linguistic\ncharacteristics of user queries and ontology resources for query constituent\nencoding, expansion concept extraction and concept weighting. A thorough\nempirical evaluation on real-world datasets validates our approach against\nunigram language model, relevance model and a sequential dependence based\ntechnique.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:22:38 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Selvaretnam", "Bhawani", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2004.11093", "submitter": "Mohammed Belkhatir", "authors": "Bhawani Selvaretnam, Mohammed Belkhatir", "title": "Natural language technology and query expansion: issues,\n  state-of-the-art and perspectives", "comments": null, "journal-ref": "J Intell Inf Syst 2012", "doi": "10.1007/s10844-011-0174-3", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of an abundance of knowledge sources has spurred a large\namount of effort in the development and enhancement of Information Retrieval\ntechniques. Users information needs are expressed in natural language and\nsuccessful retrieval is very much dependent on the effective communication of\nthe intended purpose. Natural language queries consist of multiple linguistic\nfeatures which serve to represent the intended search goal. Linguistic\ncharacteristics that cause semantic ambiguity and misinterpretation of queries\nas well as additional factors such as the lack of familiarity with the search\nenvironment affect the users ability to accurately represent their information\nneeds, coined by the concept intention gap. The latter directly affects the\nrelevance of the returned search results which may not be to the users\nsatisfaction and therefore is a major issue impacting the effectiveness of\ninformation retrieval systems. Central to our discussion is the identification\nof the significant constituents that characterize the query intent and their\nenrichment through the addition of meaningful terms, phrases or even latent\nrepresentations, either manually or automatically to capture their intended\nmeaning. Specifically, we discuss techniques to achieve the enrichment and in\nparticular those utilizing the information gathered from statistical processing\nof term dependencies within a document corpus or from external knowledge\nsources such as ontologies. We lay down the anatomy of a generic linguistic\nbased query expansion framework and propose its module-based decomposition,\ncovering topical issues from query processing, information retrieval,\ncomputational linguistics and ontology engineering. For each of the modules we\nreview state-of-the-art solutions in the literature categorized and analyzed\nunder the light of the techniques used.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:39:07 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Selvaretnam", "Bhawani", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2004.11142", "submitter": "Hongxuan Tang", "authors": "Hongxuan Tang, Hongyu Li, Jing Liu, Yu Hong, Hua Wu, Haifeng Wang", "title": "DuReader_robust: A Chinese Dataset Towards Evaluating Robustness and\n  Generalization of Machine Reading Comprehension in Real-World Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading comprehension (MRC) is a crucial task in natural language\nprocessing and has achieved remarkable advancements. However, most of the\nneural MRC models are still far from robust and fail to generalize well in\nreal-world applications. In order to comprehensively verify the robustness and\ngeneralization of MRC models, we introduce a real-world Chinese dataset --\nDuReader_robust. It is designed to evaluate the MRC models from three aspects:\nover-sensitivity, over-stability and generalization. Comparing to previous\nwork, the instances in DuReader_robust are natural texts, rather than the\naltered unnatural texts. It presents the challenges when applying MRC models to\nreal-world applications. The experimental results show that MRC models do not\nperform well on the challenge test set. Moreover, we analyze the behavior of\nexisting models on the challenge test set, which may provide suggestions for\nfuture model development. The dataset and codes are publicly available at\nhttps://github.com/baidu/DuReader.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:38:18 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 11:27:30 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Tang", "Hongxuan", ""], ["Li", "Hongyu", ""], ["Liu", "Jing", ""], ["Hong", "Yu", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "2004.11157", "submitter": "Andres Carvallo", "authors": "Vladimir Araujo, Andres Carvallo, Carlos Aspillaga and Denis Parra", "title": "On Adversarial Examples for Biomedical NLP Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of pre-trained word embeddings has motivated its use in tasks in\nthe biomedical domain. The BERT language model has shown remarkable results on\nstandard performance metrics in tasks such as Named Entity Recognition (NER)\nand Semantic Textual Similarity (STS), which has brought significant progress\nin the field of NLP. However, it is unclear whether these systems work\nseemingly well in critical domains, such as legal or medical. For that reason,\nin this work, we propose an adversarial evaluation scheme on two well-known\ndatasets for medical NER and STS. We propose two types of attacks inspired by\nnatural spelling errors and typos made by humans. We also propose another type\nof attack that uses synonyms of medical terms. Under these adversarial\nsettings, the accuracy of the models drops significantly, and we quantify the\nextent of this performance loss. We also show that we can significantly improve\nthe robustness of the models by training them with adversarial examples. We\nhope our work will motivate the use of adversarial examples to evaluate and\ndevelop models with increased robustness for medical tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:46:11 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Araujo", "Vladimir", ""], ["Carvallo", "Andres", ""], ["Aspillaga", "Carlos", ""], ["Parra", "Denis", ""]]}, {"id": "2004.11163", "submitter": "Lorik Dumani", "authors": "Stefan Ollinger, Lorik Dumani, Premtim Sahitaj, Ralph Bergmann, Ralf\n  Schenkel", "title": "Same Side Stance Classification Task: Facilitating Argument Stance\n  Classification by Fine-tuning a BERT Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on computational argumentation is currently being intensively\ninvestigated. The goal of this community is to find the best pro and con\narguments for a user given topic either to form an opinion for oneself, or to\npersuade others to adopt a certain standpoint. While existing argument mining\nmethods can find appropriate arguments for a topic, a correct classification\ninto pro and con is not yet reliable. The same side stance classification task\nprovides a dataset of argument pairs classified by whether or not both\narguments share the same stance and does not need to distinguish between\ntopic-specific pro and con vocabulary but only the argument similarity within a\nstance needs to be assessed. The results of our contribution to the task are\nbuild on a setup based on the BERT architecture. We fine-tuned a pre-trained\nBERT model for three epochs and used the first 512 tokens of each argument to\npredict if two arguments share the same stance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:54:31 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ollinger", "Stefan", ""], ["Dumani", "Lorik", ""], ["Sahitaj", "Premtim", ""], ["Bergmann", "Ralph", ""], ["Schenkel", "Ralf", ""]]}, {"id": "2004.11204", "submitter": "Keshab Parhi", "authors": "Lulu Ge and Keshab K. Parhi", "title": "Classification using Hyperdimensional Computing: A Review", "comments": "IEEE Circuits and Systems Magazine (2020)", "journal-ref": "IEEE Circuits and Systems Magazine, 20(2), pp. 30-47, June 2020", "doi": "10.1109/MCAS.2020.2988388", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperdimensional (HD) computing is built upon its unique data type referred\nto as hypervectors. The dimension of these hypervectors is typically in the\nrange of tens of thousands. Proposed to solve cognitive tasks, HD computing\naims at calculating similarity among its data. Data transformation is realized\nby three operations, including addition, multiplication and permutation. Its\nultra-wide data representation introduces redundancy against noise. Since\ninformation is evenly distributed over every bit of the hypervectors, HD\ncomputing is inherently robust. Additionally, due to the nature of those three\noperations, HD computing leads to fast learning ability, high energy efficiency\nand acceptable accuracy in learning and classification tasks. This paper\nintroduces the background of HD computing, and reviews the data representation,\ndata transformation, and similarity measurement. The orthogonality in high\ndimensions presents opportunities for flexible computing. To balance the\ntradeoff between accuracy and efficiency, strategies include but are not\nlimited to encoding, retraining, binarization and hardware acceleration.\nEvaluations indicate that HD computing shows great potential in addressing\nproblems using data in the form of letters, signals and images. HD computing\nespecially shows significant promise to replace machine learning algorithms as\na light-weight classifier in the field of internet of things (IoTs).\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 23:51:44 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Ge", "Lulu", ""], ["Parhi", "Keshab K.", ""]]}, {"id": "2004.11207", "submitter": "Li Dong", "authors": "Yaru Hao, Li Dong, Furu Wei, Ke Xu", "title": "Self-Attention Attribution: Interpreting Information Interactions Inside\n  Transformer", "comments": "AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The great success of Transformer-based models benefits from the powerful\nmulti-head self-attention mechanism, which learns token dependencies and\nencodes contextual information from the input. Prior work strives to attribute\nmodel decisions to individual input features with different saliency measures,\nbut they fail to explain how these input features interact with each other to\nreach predictions. In this paper, we propose a self-attention attribution\nmethod to interpret the information interactions inside Transformer. We take\nBERT as an example to conduct extensive studies. Firstly, we apply\nself-attention attribution to identify the important attention heads, while\nothers can be pruned with marginal performance degradation. Furthermore, we\nextract the most salient dependencies in each layer to construct an attribution\ntree, which reveals the hierarchical interactions inside Transformer. Finally,\nwe show that the attribution results can be used as adversarial patterns to\nimplement non-targeted attacks towards BERT.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 14:58:22 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 10:53:13 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Hao", "Yaru", ""], ["Dong", "Li", ""], ["Wei", "Furu", ""], ["Xu", "Ke", ""]]}, {"id": "2004.11222", "submitter": "Julia Kreutzer", "authors": "Julia Kreutzer, Nathaniel Berger, Stefan Riezler", "title": "Correct Me If You Can: Learning from Error Corrections and Markings", "comments": "To appear at EAMT 2020 (Research Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence learning involves a trade-off between signal strength\nand annotation cost of training data. For example, machine translation data\nrange from costly expert-generated translations that enable supervised\nlearning, to weak quality-judgment feedback that facilitate reinforcement\nlearning. We present the first user study on annotation cost and machine\nlearnability for the less popular annotation mode of error markings. We show\nthat error markings for translations of TED talks from English to German allow\nprecise credit assignment while requiring significantly less human effort than\ncorrecting/post-editing, and that error-marked data can be used successfully to\nfine-tune neural machine translation models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 15:17:37 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Kreutzer", "Julia", ""], ["Berger", "Nathaniel", ""], ["Riezler", "Stefan", ""]]}, {"id": "2004.11327", "submitter": "Ahmed Hasan Zaidi", "authors": "Ahmed Zaidi, Andrew Caines, Russell Moore, Paula Buttery and Andrew\n  Rice", "title": "Adaptive Forgetting Curves for Spaced Repetition Language Learning", "comments": "Artificial Intelligence for Education 2020 (AIED)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forgetting curve has been extensively explored by psychologists,\neducationalists and cognitive scientists alike. In the context of Intelligent\nTutoring Systems, modelling the forgetting curve for each user and knowledge\ncomponent (e.g. vocabulary word) should enable us to develop optimal revision\nstrategies that counteract memory decay and ensure long-term retention. In this\nstudy we explore a variety of forgetting curve models incorporating\npsychological and linguistic features, and we use these models to predict the\nprobability of word recall by learners of English as a second language. We\nevaluate the impact of the models and their features using data from an online\nvocabulary teaching platform and find that word complexity is a highly\ninformative feature which may be successfully learned by a neural network\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:22:38 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Zaidi", "Ahmed", ""], ["Caines", "Andrew", ""], ["Moore", "Russell", ""], ["Buttery", "Paula", ""], ["Rice", "Andrew", ""]]}, {"id": "2004.11339", "submitter": "Jimmy Lin", "authors": "Raphael Tang, Rodrigo Nogueira, Edwin Zhang, Nikhil Gupta, Phuong Cam,\n  Kyunghyun Cho, Jimmy Lin", "title": "Rapidly Bootstrapping a Question Answering Dataset for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CovidQA, the beginnings of a question answering dataset\nspecifically designed for COVID-19, built by hand from knowledge gathered from\nKaggle's COVID-19 Open Research Dataset Challenge. To our knowledge, this is\nthe first publicly available resource of its type, and intended as a stopgap\nmeasure for guiding research until more substantial evaluation resources become\navailable. While this dataset, comprising 124 question-article pairs as of the\npresent version 0.1 release, does not have sufficient examples for supervised\nmachine learning, we believe that it can be helpful for evaluating the\nzero-shot or transfer capabilities of existing models on topics specifically\nrelated to COVID-19. This paper describes our methodology for constructing the\ndataset and presents the effectiveness of a number of baselines, including\nterm-based techniques and various transformer-based models. The dataset is\navailable at http://covidqa.ai/\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:35:11 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Tang", "Raphael", ""], ["Nogueira", "Rodrigo", ""], ["Zhang", "Edwin", ""], ["Gupta", "Nikhil", ""], ["Cam", "Phuong", ""], ["Cho", "Kyunghyun", ""], ["Lin", "Jimmy", ""]]}, {"id": "2004.11405", "submitter": "Ori Terner", "authors": "Ori Terner, Kfir Bar, Nachum Dershowitz", "title": "Transliteration of Judeo-Arabic Texts into Arabic Script Using Recurrent\n  Neural Networks", "comments": "accepted for WANLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained a model to automatically transliterate Judeo-Arabic texts into\nArabic script, enabling Arabic readers to access those writings. We employ a\nrecurrent neural network (RNN), combined with the connectionist temporal\nclassification (CTC) loss to deal with unequal input/output lengths. This\nobligates adjustments in the training data to avoid input sequences that are\nshorter than their corresponding outputs. We also utilize a pretraining stage\nwith a different loss function to improve network converge. Since only a single\nsource of parallel text was available for training, we take advantage of the\npossibility of generating data synthetically. We train a model that has the\ncapability to memorize words in the output language, and that also utilizes\ncontext for distinguishing ambiguities in the transliteration. We obtain an\nimprovement over the baseline 9.5% character error, achieving 2% error with our\nbest configuration. To measure the contribution of context to learning, we also\ntested word-shuffled data, for which the error rises to 2.5%.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 18:03:41 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 09:08:53 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Terner", "Ori", ""], ["Bar", "Kfir", ""], ["Dershowitz", "Nachum", ""]]}, {"id": "2004.11419", "submitter": "Viet-Trung Dang", "authors": "Viet-Trung Dang, Tianyu Zhao, Sei Ueno, Hirofumi Inaguma, Tatsuya\n  Kawahara", "title": "End-to-end speech-to-dialog-act recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding, which extracts intents and/or semantic\nconcepts in utterances, is conventionally formulated as a post-processing of\nautomatic speech recognition. It is usually trained with oracle transcripts,\nbut needs to deal with errors by ASR. Moreover, there are acoustic features\nwhich are related with intents but not represented with the transcripts. In\nthis paper, we present an end-to-end model which directly converts speech into\ndialog acts without the deterministic transcription process. In the proposed\nmodel, the dialog act recognition network is conjunct with an acoustic-to-word\nASR model at its latent layer before the softmax layer, which provides a\ndistributed representation of word-level ASR decoding information. Then, the\nentire network is fine-tuned in an end-to-end manner. This allows for stable\ntraining as well as robustness against ASR errors. The model is further\nextended to conduct DA segmentation jointly. Evaluations with the Switchboard\ncorpus demonstrate that the proposed method significantly improves dialog act\nrecognition accuracy from the conventional pipeline framework.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 18:44:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 22:12:17 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Dang", "Viet-Trung", ""], ["Zhao", "Tianyu", ""], ["Ueno", "Sei", ""], ["Inaguma", "Hirofumi", ""], ["Kawahara", "Tatsuya", ""]]}, {"id": "2004.11449", "submitter": "Fangyu Liu", "authors": "Fangyu Liu, R\\'emi Lebret, Didier Orel, Philippe Sordet, Karl Aberer", "title": "Upgrading the Newsroom: An Automated Image Selection System for News\n  Articles", "comments": "Accepted to ACM Transactions on Multimedia Computing Communications\n  and Applications (ACM TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an automated image selection system to assist photo editors in\nselecting suitable images for news articles. The system fuses multiple textual\nsources extracted from news articles and accepts multilingual inputs. It is\nequipped with char-level word embeddings to help both modeling morphologically\nrich languages, e.g. German, and transferring knowledge across nearby\nlanguages. The text encoder adopts a hierarchical self-attention mechanism to\nattend more to both keywords within a piece of text and informative components\nof a news article. We extensively experiment with our system on a large-scale\ntext-image database containing multimodal multilingual news articles collected\nfrom Swiss local news media websites. The system is compared with multiple\nbaselines with ablation studies and is shown to beat existing text-image\nretrieval methods in a weakly-supervised learning setting. Besides, we also\noffer insights on the advantage of using multiple textual sources and\nmultilingual data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 20:29:26 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Liu", "Fangyu", ""], ["Lebret", "R\u00e9mi", ""], ["Orel", "Didier", ""], ["Sordet", "Philippe", ""], ["Aberer", "Karl", ""]]}, {"id": "2004.11464", "submitter": "Jocelyn Mazarura", "authors": "Jocelyn Mazarura, Alta de Waal and Pieter de Villiers", "title": "A Gamma-Poisson Mixture Topic Model for Short Text", "comments": "26 pages, 14 Figures, to be published in Mathematical Problems in\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most topic models are constructed under the assumption that documents follow\na multinomial distribution. The Poisson distribution is an alternative\ndistribution to describe the probability of count data. For topic modelling,\nthe Poisson distribution describes the number of occurrences of a word in\ndocuments of fixed length. The Poisson distribution has been successfully\napplied in text classification, but its application to topic modelling is not\nwell documented, specifically in the context of a generative probabilistic\nmodel. Furthermore, the few Poisson topic models in literature are admixture\nmodels, making the assumption that a document is generated from a mixture of\ntopics. In this study, we focus on short text. Many studies have shown that the\nsimpler assumption of a mixture model fits short text better. With mixture\nmodels, as opposed to admixture models, the generative assumption is that a\ndocument is generated from a single topic. One topic model, which makes this\none-topic-per-document assumption, is the Dirichlet-multinomial mixture model.\nThe main contributions of this work are a new Gamma-Poisson mixture model, as\nwell as a collapsed Gibbs sampler for the model. The benefit of the collapsed\nGibbs sampler derivation is that the model is able to automatically select the\nnumber of topics contained in the corpus. The results show that the\nGamma-Poisson mixture model performs better than the Dirichlet-multinomial\nmixture model at selecting the number of topics in labelled corpora.\nFurthermore, the Gamma-Poisson mixture produces better topic coherence scores\nthan the Dirichlet-multinomial mixture model, thus making it a viable option\nfor the challenging task of topic modelling of short text.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 21:13:53 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Mazarura", "Jocelyn", ""], ["de Waal", "Alta", ""], ["de Villiers", "Pieter", ""]]}, {"id": "2004.11471", "submitter": "Alberto Poncelas", "authors": "Alberto Poncelas, Mohammad Aboomar, Jan Buts, James Hadley, Andy Way", "title": "A Tool for Facilitating OCR Postediting in Historical Documents", "comments": null, "journal-ref": "Workshop on Language Technologies for Historical and Ancient\n  Languages, LT4HALA (2020)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical character recognition (OCR) for historical documents is a complex\nprocedure subject to a unique set of material issues, including inconsistencies\nin typefaces and low quality scanning. Consequently, even the most\nsophisticated OCR engines produce errors. This paper reports on a tool built\nfor postediting the output of Tesseract, more specifically for correcting\ncommon errors in digitized historical documents. The proposed tool suggests\nalternatives for word forms not found in a specified vocabulary. The assumed\nerror is replaced by a presumably correct alternative in the post-edition based\non the scores of a Language Model (LM). The tool is tested on a chapter of the\nbook An Essay Towards Regulating the Trade and Employing the Poor of this\nKingdom (Cary ,1719). As demonstrated below, the tool is successful in\ncorrecting a number of common errors. If sometimes unreliable, it is also\ntransparent and subject to human intervention.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 21:40:30 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Poncelas", "Alberto", ""], ["Aboomar", "Mohammad", ""], ["Buts", "Jan", ""], ["Hadley", "James", ""], ["Way", "Andy", ""]]}, {"id": "2004.11472", "submitter": "Alberto Poncelas", "authors": "Alberto Poncelas, Wichaya Pidchamook, Chao-Hong Liu, James Hadley,\n  Andy Way", "title": "Multiple Segmentations of Thai Sentences for Neural Machine Translation", "comments": null, "journal-ref": "Spoken Language Technologies for Under-resourced languages and\n  CCURL Collaboration and Computing for Under-Resourced Languages Workshop,\n  SLTU-CCURL (2020)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thai is a low-resource language, so it is often the case that data is not\navailable in sufficient quantities to train an Neural Machine Translation (NMT)\nmodel which perform to a high level of quality. In addition, the Thai script\ndoes not use white spaces to delimit the boundaries between words, which adds\nmore complexity when building sequence to sequence models. In this work, we\nexplore how to augment a set of English--Thai parallel data by replicating\nsentence-pairs with different word segmentation methods on Thai, as training\ndata for NMT model training. Using different merge operations of Byte Pair\nEncoding, different segmentations of Thai sentences can be obtained. The\nexperiments show that combining these datasets, performance is improved for NMT\nmodels trained with a dataset that has been split using a supervised splitting\ntool.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 21:48:58 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Poncelas", "Alberto", ""], ["Pidchamook", "Wichaya", ""], ["Liu", "Chao-Hong", ""], ["Hadley", "James", ""], ["Way", "Andy", ""]]}, {"id": "2004.11480", "submitter": "Kiran Garimella", "authors": "Pushkal Agarwal, Kiran Garimella, Sagar Joglekar, Nishanth Sastry,\n  Gareth Tyson", "title": "Characterising User Content on a Multi-lingual Social Network", "comments": "Accepted at ICWSM 2020, please cite the ICWSM version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has been on the vanguard of political information diffusion in\nthe 21st century. Most studies that look into disinformation, political\ninfluence and fake-news focus on mainstream social media platforms. This has\ninevitably made English an important factor in our current understanding of\npolitical activity on social media. As a result, there has only been a limited\nnumber of studies into a large portion of the world, including the largest,\nmultilingual and multi-cultural democracy: India. In this paper we present our\ncharacterisation of a multilingual social network in India called ShareChat. We\ncollect an exhaustive dataset across 72 weeks before and during the Indian\ngeneral elections of 2019, across 14 languages. We investigate the cross\nlingual dynamics by clustering visually similar images together, and exploring\nhow they move across language barriers. We find that Telugu, Malayalam, Tamil\nand Kannada languages tend to be dominant in soliciting political images (often\nreferred to as memes), and posts from Hindi have the largest cross-lingual\ndiffusion across ShareChat (as well as images containing text in English). In\nthe case of images containing text that cross language barriers, we see that\nlanguage translation is used to widen the accessibility. That said, we find\ncases where the same image is associated with very different text (and\ntherefore meanings). This initial characterisation paves the way for more\nadvanced pipelines to understand the dynamics of fake and political content in\na multi-lingual and non-textual setting.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 22:25:48 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Agarwal", "Pushkal", ""], ["Garimella", "Kiran", ""], ["Joglekar", "Sagar", ""], ["Sastry", "Nishanth", ""], ["Tyson", "Gareth", ""]]}, {"id": "2004.11493", "submitter": "Seid Muhie Yimam", "authors": "Gregor Wiedemann and Seid Muhie Yimam and Chris Biemann", "title": "UHH-LT at SemEval-2020 Task 12: Fine-Tuning of Pre-Trained Transformer\n  Networks for Offensive Language Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-tuning of pre-trained transformer networks such as BERT yield\nstate-of-the-art results for text classification tasks. Typically, fine-tuning\nis performed on task-specific training datasets in a supervised manner. One can\nalso fine-tune in unsupervised manner beforehand by further pre-training the\nmasked language modeling (MLM) task. Hereby, in-domain data for unsupervised\nMLM resembling the actual classification target dataset allows for domain\nadaptation of the model. In this paper, we compare current pre-trained\ntransformer networks with and without MLM fine-tuning on their performance for\noffensive language detection. Our MLM fine-tuned RoBERTa-based classifier\nofficially ranks 1st in the SemEval 2020 Shared Task~12 for the English\nlanguage. Further experiments with the ALBERT model even surpass this result.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 23:59:58 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 20:48:08 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Wiedemann", "Gregor", ""], ["Yimam", "Seid Muhie", ""], ["Biemann", "Chris", ""]]}, {"id": "2004.11546", "submitter": "Yiben Yang", "authors": "Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta,\n  Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, Doug Downey", "title": "Generative Data Augmentation for Commonsense Reasoning", "comments": "Findings of the Association for Computational Linguistics: EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in commonsense reasoning depend on large-scale\nhuman-annotated training data to achieve peak performance. However, manual\ncuration of training examples is expensive and has been shown to introduce\nannotation artifacts that neural models can readily exploit and overfit on. We\ninvestigate G-DAUG^C, a novel generative data augmentation method that aims to\nachieve more accurate and robust learning in the low-resource setting. Our\napproach generates synthetic examples using pretrained language models, and\nselects the most informative and diverse set of examples for data augmentation.\nIn experiments with multiple commonsense reasoning benchmarks, G-DAUG^C\nconsistently outperforms existing data augmentation methods based on\nback-translation, and establishes a new state-of-the-art on WinoGrande, CODAH,\nand CommonsenseQA. Further, in addition to improvements in in-distribution\naccuracy, G-DAUG^C-augmented training also enhances out-of-distribution\ngeneralization, showing greater robustness against adversarial or perturbed\nexamples. Our analysis demonstrates that G-DAUG^C produces a diverse set of\nfluent training examples, and that its selection and training approaches are\nimportant for performance. Our findings encourage future research toward\ngenerative data augmentation to enhance both in-distribution learning and\nout-of-distribution generalization.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 06:12:10 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 06:45:52 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 04:37:31 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Yang", "Yiben", ""], ["Malaviya", "Chaitanya", ""], ["Fernandez", "Jared", ""], ["Swayamdipta", "Swabha", ""], ["Bras", "Ronan Le", ""], ["Wang", "Ji-Ping", ""], ["Bhagavatula", "Chandra", ""], ["Choi", "Yejin", ""], ["Downey", "Doug", ""]]}, {"id": "2004.11579", "submitter": "Yi Liao", "authors": "Yi Liao, Xin Jiang, Qun Liu", "title": "Probabilistically Masked Language Model Capable of Autoregressive\n  Generation in Arbitrary Word Order", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Masked language model and autoregressive language model are two types of\nlanguage models. While pretrained masked language models such as BERT overwhelm\nthe line of natural language understanding (NLU) tasks, autoregressive language\nmodels such as GPT are especially capable in natural language generation (NLG).\nIn this paper, we propose a probabilistic masking scheme for the masked\nlanguage model, which we call probabilistically masked language model (PMLM).\nWe implement a specific PMLM with a uniform prior distribution on the masking\nratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive\npermutated language model. One main advantage of the model is that it supports\ntext generation in arbitrary order with surprisingly good quality, which could\npotentially enable new applications over traditional unidirectional generation.\nBesides, the pretrained u-PMLM also outperforms BERT on a set of downstream NLU\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 07:38:19 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Liao", "Yi", ""], ["Jiang", "Xin", ""], ["Liu", "Qun", ""]]}, {"id": "2004.11583", "submitter": "Claudia Savina Bianchini", "authors": "Claudia S. Bianchini (Poitiers UFR LL, FORELLIS), Fabrizio Borgia,\n  Margherita Castelli (DiFiLiLe)", "title": "Customization and modifications of SignWriting by LIS users", "comments": "in French. CORELA - COgnition, REpr{\\'e}sentation, LAngage,\n  CERLICO-Cercle Linguistique du Centre et de l'Ouest (France), A para{\\^i}tre", "journal-ref": null, "doi": null, "report-no": "pubblicazione #022", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, the various sign languages (SL) have not developed an own\nwriting system; nevertheless, some systems exist, among which the SignWriting\n(SW) is a powerful and flexible one. In this paper, we present the mechanisms\nadopted by signers of the Italian Sign Language (LIS), expert users of SW, to\nmodify the standard SW glyphs and increase their writing skills and/or\nrepresent peculiar linguistic phenomena. We identify these glyphs and show\nwhich characteristics make them \"acceptable\" by the expert community.\nEventually, we analyze the potentialities of these glyphs in hand writing and\nin computer-assisted writing, focusing on SWift, a software designed to allow\nthe electronic writing-down of user-modified glyphs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 07:49:45 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Bianchini", "Claudia S.", "", "Poitiers UFR LL, FORELLIS"], ["Borgia", "Fabrizio", "", "DiFiLiLe"], ["Castelli", "Margherita", "", "DiFiLiLe"]]}, {"id": "2004.11604", "submitter": "Giovanni Quattrone", "authors": "Giovanni Quattrone, Antonino Nocera, Licia Capra, Daniele Quercia", "title": "Social Interactions or Business Transactions? What customer reviews\n  disclose about Airbnb marketplace", "comments": "17 pages, 8 figures, Proceedings of The Web Conference 2020", "journal-ref": null, "doi": "10.1145/3366423.3380225", "report-no": null, "categories": "cs.CY cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Airbnb is one of the most successful examples of sharing economy\nmarketplaces. With rapid and global market penetration, understanding its\nattractiveness and evolving growth opportunities is key to plan business\ndecision making. There is an ongoing debate, for example, about whether Airbnb\nis a hospitality service that fosters social exchanges between hosts and\nguests, as the sharing economy manifesto originally stated, or whether it is\n(or is evolving into being) a purely business transaction platform, the way\nhotels have traditionally operated. To answer these questions, we propose a\nnovel market analysis approach that exploits customers' reviews. Key to the\napproach is a method that combines thematic analysis and machine learning to\ninductively develop a custom dictionary for guests' reviews. Based on this\ndictionary, we then use quantitative linguistic analysis on a corpus of 3.2\nmillion reviews collected in 6 different cities, and illustrate how to answer a\nvariety of market research questions, at fine levels of temporal, thematic,\nuser and spatial granularity, such as (i) how the business vs social dichotomy\nis evolving over the years, (ii) what exact words within such top-level\ncategories are evolving, (iii) whether such trends vary across different user\nsegments and (iv) in different neighbourhoods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:08:46 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Quattrone", "Giovanni", ""], ["Nocera", "Antonino", ""], ["Capra", "Licia", ""], ["Quercia", "Daniele", ""]]}, {"id": "2004.11622", "submitter": "Ivan Lerner", "authors": "Ivan Lerner, Jordan Jouffroy, Anita Burgun, Antoine Neuraz", "title": "Learning the grammar of prescription: recurrent neural network grammars\n  for medication information extraction in clinical texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we evaluated the RNNG, a neural top-down transition based\nparser, for medication information extraction in clinical texts. We evaluated\nthis model on a French clinical corpus. The task was to extract the name of a\ndrug (or class of drug), as well as fields informing its administration:\nfrequency, dosage, duration, condition and route of administration. We compared\nthe RNNG model that jointly identify entities and their relations with separate\nBiLSTMs models for entities and relations as baselines. We call seq-BiLSTMs the\nbaseline models for relations extraction that takes as extra-input the output\nof the BiLSTMs for entities. RNNG outperforms seq-BiLSTM for identifying\nrelations, with on average 88.5% [87.2-89.8] versus 84.6 [83.1-86.1] F-measure.\nHowever, RNNG is weaker than the baseline BiLSTM on detecting entities, with on\naverage 82.4 [80.8-83.8] versus 84.1 [82.7-85.6] % F- measure. RNNG trained\nonly for detecting relations is weaker than RNNG with the joint modelling\nobjective, 87.4 [85.8-88.8] versus 88.5% [87.2-89.8]. The performance of RNNG\non relations can be explained both by the model architecture, which provides\nshortcut between distant parts of the sentence, and the joint modelling\nobjective which allow the RNNG to learn richer representations. RNNG is\nefficient for modeling relations between entities in medical texts and its\nperformances are close to those of a BiLSTM for entity detection.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:43:14 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Lerner", "Ivan", ""], ["Jouffroy", "Jordan", ""], ["Burgun", "Anita", ""], ["Neuraz", "Antoine", ""]]}, {"id": "2004.11648", "submitter": "Cheng-Te Li", "authors": "Yi-Ju Lu and Cheng-Te Li", "title": "GCAN: Graph-aware Co-Attention Networks for Explainable Fake News\n  Detection on Social Media", "comments": "To appear in Proceedings of The 58th Annual Meeting of the\n  Association for Computational Linguistics, ACL 2020. Code is available here\n  https://github.com/l852888/GCAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper solves the fake news detection problem under a more realistic\nscenario on social media. Given the source short-text tweet and the\ncorresponding sequence of retweet users without text comments, we aim at\npredicting whether the source tweet is fake or not, and generating explanation\nby highlighting the evidences on suspicious retweeters and the words they\nconcern. We develop a novel neural network-based model, Graph-aware\nCo-Attention Networks (GCAN), to achieve the goal. Extensive experiments\nconducted on real tweet datasets exhibit that GCAN can significantly outperform\nstate-of-the-art methods by 16% in accuracy on average. In addition, the case\nstudies also show that GCAN can produce reasonable explanations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 10:42:49 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Lu", "Yi-Ju", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2004.11695", "submitter": "Hamed Jelodar", "authors": "Hamed Jelodar, Yongli Wang, Rita Orji, Hucheng Huang", "title": "Deep Sentiment Classification and Topic Discovery on Novel Coronavirus\n  or COVID-19 Online Discussions: NLP Using LSTM Recurrent Neural Network\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet forums and public social media, such as online healthcare forums,\nprovide a convenient channel for users (people/patients) concerned about health\nissues to discuss and share information with each other. In late December 2019,\nan outbreak of a novel coronavirus (infection from which results in the disease\nnamed COVID-19) was reported, and, due to the rapid spread of the virus in\nother parts of the world, the World Health Organization declared a state of\nemergency. In this paper, we used automated extraction of COVID-19 related\ndiscussions from social media and a natural language process (NLP) method based\non topic modeling to uncover various issues related to COVID-19 from public\nopinions. Moreover, we also investigate how to use LSTM recurrent neural\nnetwork for sentiment classification of COVID-19 comments. Our findings shed\nlight on the importance of using public opinions and suitable computational\ntechniques to understand issues surrounding COVID-19 and to guide related\ndecision-making.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 16:29:13 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Jelodar", "Hamed", ""], ["Wang", "Yongli", ""], ["Orji", "Rita", ""], ["Huang", "Hucheng", ""]]}, {"id": "2004.11714", "submitter": "Yuntian Deng", "authors": "Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, Marc'Aurelio\n  Ranzato", "title": "Residual Energy-Based Models for Text Generation", "comments": "published at ICLR 2020. arXiv admin note: substantial text overlap\n  with arXiv:2004.10188", "journal-ref": "ICLR 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generation is ubiquitous in many NLP tasks, from summarization, to\ndialogue and machine translation. The dominant parametric approach is based on\nlocally normalized models which predict one word at a time. While these work\nremarkably well, they are plagued by exposure bias due to the greedy nature of\nthe generation process. In this work, we investigate un-normalized energy-based\nmodels (EBMs) which operate not at the token but at the sequence level. In\norder to make training tractable, we first work in the residual of a pretrained\nlocally normalized language model and second we train using noise contrastive\nestimation. Furthermore, since the EBM works at the sequence level, we can\nleverage pretrained bi-directional contextual representations, such as BERT and\nRoBERTa. Our experiments on two large language modeling datasets show that\nresidual EBMs yield lower perplexity compared to locally normalized baselines.\nMoreover, generation via importance sampling is very efficient and of higher\nquality than the baseline models according to human evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 23:19:55 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Deng", "Yuntian", ""], ["Bakhtin", "Anton", ""], ["Ott", "Myle", ""], ["Szlam", "Arthur", ""], ["Ranzato", "Marc'Aurelio", ""]]}, {"id": "2004.11727", "submitter": "Zihan Liu", "authors": "Zihan Liu, Genta Indra Winata, Peng Xu, Pascale Fung", "title": "Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling", "comments": "Accepted in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an essential task in task-oriented dialog systems, slot filling requires\nextensive training data in a certain domain. However, such data are not always\navailable. Hence, cross-domain slot filling has naturally arisen to cope with\nthis data scarcity problem. In this paper, we propose a Coarse-to-fine approach\n(Coach) for cross-domain slot filling. Our model first learns the general\npattern of slot entities by detecting whether the tokens are slot entities or\nnot. It then predicts the specific types for the slot entities. In addition, we\npropose a template regularization approach to improve the adaptation robustness\nby regularizing the representation of utterances based on utterance templates.\nExperimental results show that our model significantly outperforms\nstate-of-the-art approaches in slot filling. Furthermore, our model can also be\napplied to the cross-domain named entity recognition task, and it achieves\nbetter adaptation performance than other existing baselines. The code is\navailable at https://github.com/zliucr/coach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 13:07:12 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Liu", "Zihan", ""], ["Winata", "Genta Indra", ""], ["Xu", "Peng", ""], ["Fung", "Pascale", ""]]}, {"id": "2004.11742", "submitter": "Xiwen Chen", "authors": "Xiwen Chen, Kenny Q. Zhu", "title": "ST$^2$: Small-data Text Style Transfer via Multi-task Meta-Learning", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text style transfer aims to paraphrase a sentence in one style into another\nstyle while preserving content. Due to lack of parallel training data,\nstate-of-art methods are unsupervised and rely on large datasets that share\ncontent. Furthermore, existing methods have been applied on very limited\ncategories of styles such as positive/negative and formal/informal. In this\nwork, we develop a meta-learning framework to transfer between any kind of text\nstyles, including personal writing styles that are more fine-grained, share\nless content and have much smaller training data. While state-of-art models\nfail in the few-shot style transfer task, our framework effectively utilizes\ninformation from other styles to improve both language fluency and style\ntransfer accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 13:36:38 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Chen", "Xiwen", ""], ["Zhu", "Kenny Q.", ""]]}, {"id": "2004.11779", "submitter": "Yang Gao", "authors": "Wang Haonan, Gao Yang, Bai Yu, Mirella Lapata, Huang Heyan", "title": "Exploring Explainable Selection to Control Abstractive Summarization", "comments": "Accepted by AAAI'2021. Include all Appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like humans, document summarization models can interpret a document's\ncontents in a number of ways. Unfortunately, the neural models of today are\nlargely black boxes that provide little explanation of how or why they\ngenerated a summary in the way they did. Therefore, to begin prying open the\nblack box and to inject a level of control into the substance of the final\nsummary, we developed a novel select-and-generate framework that focuses on\nexplainability. By revealing the latent centrality and interactions between\nsentences, along with scores for sentence novelty and relevance, users are\ngiven a window into the choices a model is making and an opportunity to guide\nthose choices in a more desirable direction. A novel pair-wise matrix captures\nthe sentence interactions, centrality, and attribute scores, and a mask with\ntunable attribute thresholds allows the user to control which sentences are\nlikely to be included in the extraction. A sentence-deployed attention\nmechanism in the abstractor ensures the final summary emphasizes the desired\ncontent. Additionally, the encoder is adaptable, supporting both Transformer-\nand BERT-based configurations. In a series of experiments assessed with ROUGE\nmetrics and two human evaluations, ESCA outperformed eight state-of-the-art\nmodels on the CNN/DailyMail and NYT50 benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:39:34 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 10:17:34 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Haonan", "Wang", ""], ["Yang", "Gao", ""], ["Yu", "Bai", ""], ["Lapata", "Mirella", ""], ["Heyan", "Huang", ""]]}, {"id": "2004.11795", "submitter": "Xiaonan Li", "authors": "Xiaonan Li, Hang Yan, Xipeng Qiu, Xuanjing Huang", "title": "FLAT: Chinese NER Using Flat-Lattice Transformer", "comments": "Accepted to the ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the character-word lattice structure has been proved to be\neffective for Chinese named entity recognition (NER) by incorporating the word\ninformation. However, since the lattice structure is complex and dynamic, most\nexisting lattice-based models are hard to fully utilize the parallel\ncomputation of GPUs and usually have a low inference-speed. In this paper, we\npropose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the\nlattice structure into a flat structure consisting of spans. Each span\ncorresponds to a character or latent word and its position in the original\nlattice. With the power of Transformer and well-designed position encoding,\nFLAT can fully leverage the lattice information and has an excellent\nparallelization ability. Experiments on four datasets show FLAT outperforms\nother lexicon-based models in performance and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 15:27:49 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 05:05:03 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Li", "Xiaonan", ""], ["Yan", "Hang", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2004.11854", "submitter": "Biao Zhang", "authors": "Biao Zhang, Ivan Titov, Rico Sennrich", "title": "On Sparsifying Encoder Outputs in Sequence-to-Sequence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models usually transfer all encoder outputs to the\ndecoder for generation. In this work, by contrast, we hypothesize that these\nencoder outputs can be compressed to shorten the sequence delivered for\ndecoding. We take Transformer as the testbed and introduce a layer of\nstochastic gates in-between the encoder and the decoder. The gates are\nregularized using the expected value of the sparsity-inducing L0penalty,\nresulting in completely masking-out a subset of encoder outputs. In other\nwords, via joint training, the L0DROP layer forces Transformer to route\ninformation through a subset of its encoder states. We investigate the effects\nof this sparsification on two machine translation and two summarization tasks.\nExperiments show that, depending on the task, around 40-70% of source encodings\ncan be pruned without significantly compromising quality. The decrease of the\noutput length endows L0DROP with the potential of improving decoding\nefficiency, where it yields a speedup of up to 1.65x on document summarization\ntasks against the standard Transformer. We analyze the L0DROP behaviour and\nobserve that it exhibits systematic preferences for pruning certain word types,\ne.g., function words and punctuation get pruned most. Inspired by these\nobservations, we explore the feasibility of specifying rule-based patterns that\nmask out encoder outputs based on information such as part-of-speech tags, word\nfrequency and word position.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 16:57:52 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Zhang", "Biao", ""], ["Titov", "Ivan", ""], ["Sennrich", "Rico", ""]]}, {"id": "2004.11861", "submitter": "Simon Gottschalk", "authors": "Tarc\\'isio Souza Costa, Simon Gottschalk, Elena Demidova", "title": "Event-QA: A Dataset for Event-Centric Question Answering over Knowledge\n  Graphs", "comments": "(c) 2020 Copyright held by the authors. This is the author's version\n  of the work. It is posted here for your personal use. Not for redistribution.\n  The definitive version was published in the Proceedings of the 29th ACM\n  International Conference on Information and Knowledge Management,\n  https://doi.org/10.1145/3340531.3412760", "journal-ref": null, "doi": "10.1145/3340531.3412760", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Question Answering (QA) is a crucial technology to facilitate\nintuitive user access to semantic information stored in knowledge graphs.\nWhereas most of the existing QA systems and datasets focus on entity-centric\nquestions, very little is known about these systems' performance in the context\nof events. As new event-centric knowledge graphs emerge, datasets for such\nquestions gain importance. In this paper, we present the Event-QA dataset for\nanswering event-centric questions over knowledge graphs. Event-QA contains 1000\nsemantic queries and the corresponding English, German and Portuguese\nverbalizations for EventKG - an event-centric knowledge graph with more than\n970 thousand events.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:11:37 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 14:07:47 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Costa", "Tarc\u00edsio Souza", ""], ["Gottschalk", "Simon", ""], ["Demidova", "Elena", ""]]}, {"id": "2004.11867", "submitter": "Biao Zhang", "authors": "Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich", "title": "Improving Massively Multilingual Neural Machine Translation and\n  Zero-Shot Translation", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively multilingual models for neural machine translation (NMT) are\ntheoretically attractive, but often underperform bilingual models and deliver\npoor zero-shot translations. In this paper, we explore ways to improve them. We\nargue that multilingual NMT requires stronger modeling capacity to support\nlanguage pairs with varying typological characteristics, and overcome this\nbottleneck via language-specific components and deepening NMT architectures. We\nidentify the off-target translation issue (i.e. translating into a wrong target\nlanguage) as the major source of the inferior zero-shot performance, and\npropose random online backtranslation to enforce the translation of unseen\ntraining language pairs. Experiments on OPUS-100 (a novel multilingual dataset\nwith 100 languages) show that our approach substantially narrows the\nperformance gap with bilingual models in both one-to-many and many-to-many\nsettings, and improves zero-shot performance by ~10 BLEU, approaching\nconventional pivot-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:21:32 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Zhang", "Biao", ""], ["Williams", "Philip", ""], ["Titov", "Ivan", ""], ["Sennrich", "Rico", ""]]}, {"id": "2004.11886", "submitter": "Zhijian Liu", "authors": "Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, Song Han", "title": "Lite Transformer with Long-Short Range Attention", "comments": "ICLR 2020. The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer has become ubiquitous in natural language processing (e.g.,\nmachine translation, question answering); however, it requires enormous amount\nof computations to achieve high performance, which makes it not suitable for\nmobile applications that are tightly constrained by the hardware resources and\nbattery. In this paper, we present an efficient mobile NLP architecture, Lite\nTransformer to facilitate deploying mobile NLP applications on edge devices.\nThe key primitive is the Long-Short Range Attention (LSRA), where one group of\nheads specializes in the local context modeling (by convolution) while another\ngroup specializes in the long-distance relationship modeling (by attention).\nSuch specialization brings consistent improvement over the vanilla transformer\non three well-established language tasks: machine translation, abstractive\nsummarization, and language modeling. Under constrained resources (500M/100M\nMACs), Lite Transformer outperforms transformer on WMT'14 English-French by\n1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of\ntransformer base model by 2.5x with 0.3 BLEU score degradation. Combining with\npruning and quantization, we further compressed the model size of Lite\nTransformer by 18.2x. For language modeling, Lite Transformer achieves 1.8\nlower perplexity than the transformer at around 500M MACs. Notably, Lite\nTransformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU\nfor the mobile NLP setting without the costly architecture search that requires\nmore than 250 GPU years. Code has been made available at\nhttps://github.com/mit-han-lab/lite-transformer.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:52:25 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Wu", "Zhanghao", ""], ["Liu", "Zhijian", ""], ["Lin", "Ji", ""], ["Lin", "Yujun", ""], ["Han", "Song", ""]]}, {"id": "2004.11892", "submitter": "Alexander Fabbri", "authors": "Alexander R. Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, Bing\n  Xiang", "title": "Template-Based Question Generation from Retrieved Sentences for Improved\n  Unsupervised Question Answering", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) is in increasing demand as the amount of information\navailable online and the desire for quick access to this content grows. A\ncommon approach to QA has been to fine-tune a pretrained language model on a\ntask-specific labeled dataset. This paradigm, however, relies on scarce, and\ncostly to obtain, large-scale human-labeled data. We propose an unsupervised\napproach to training QA models with generated pseudo-training data. We show\nthat generating questions for QA training by applying a simple template on a\nrelated, retrieved sentence rather than the original context sentence improves\ndownstream QA performance by allowing the model to learn more complex\ncontext-question relationships. Training a QA model on this data gives a\nrelative improvement over a previous unsupervised model in F1 score on the\nSQuAD dataset by about 14%, and 20% when the answer is a named entity,\nachieving state-of-the-art performance on SQuAD for unsupervised QA.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:57:45 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Fabbri", "Alexander R.", ""], ["Ng", "Patrick", ""], ["Wang", "Zhiguo", ""], ["Nallapati", "Ramesh", ""], ["Xiang", "Bing", ""]]}, {"id": "2004.11954", "submitter": "Aman Madaan", "authors": "Aman Madaan, Shruti Rijhwani, Antonios Anastasopoulos, Yiming Yang,\n  Graham Neubig", "title": "Practical Comparable Data Collection for Low-Resource Languages via\n  Images", "comments": "Accepted for poster presentation at the Practical Machine Learning\n  for Developing Countries (PML4DC) workshop, ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method of curating high-quality comparable training data for\nlow-resource languages with monolingual annotators. Our method involves using a\ncarefully selected set of images as a pivot between the source and target\nlanguages by getting captions for such images in both languages independently.\nHuman evaluations on the English-Hindi comparable corpora created with our\nmethod show that 81.1% of the pairs are acceptable translations, and only 2.47%\nof the pairs are not translations at all. We further establish the potential of\nthe dataset collected through our approach by experimenting on two downstream\ntasks - machine translation and dictionary extraction. All code and data are\navailable at https://github.com/madaan/PML4DC-Comparable-Data-Collection.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 19:30:38 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 19:51:43 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Madaan", "Aman", ""], ["Rijhwani", "Shruti", ""], ["Anastasopoulos", "Antonios", ""], ["Yang", "Yiming", ""], ["Neubig", "Graham", ""]]}, {"id": "2004.11964", "submitter": "Hana Al-Theiabat", "authors": "Hana Al-Theiabat and Aisha Al-Sadi", "title": "The Inception Team at NSURL-2019 Task 8: Semantic Question Similarity in\n  Arabic", "comments": "6 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our method for the task of Semantic Question Similarity\nin Arabic in the workshop on NLP Solutions for Under-Resourced Languages\n(NSURL). The aim is to build a model that is able to detect similar semantic\nquestions in the Arabic language for the provided dataset. Different methods of\ndetermining questions similarity are explored in this work. The proposed models\nachieved high F1-scores, which range from (88% to 96%). Our official best\nresult is produced from the ensemble model of using a pre-trained multilingual\nBERT model with different random seeds with 95.924% F1-Score, which ranks the\nfirst among nine participants teams.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 19:52:40 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Al-Theiabat", "Hana", ""], ["Al-Sadi", "Aisha", ""]]}, {"id": "2004.11980", "submitter": "Rishiraj Saha Roy", "authors": "Rishiraj Saha Roy, Avishek Anand", "title": "Question Answering over Curated and Open Web Sources", "comments": "SIGIR 2020 Tutorial", "journal-ref": null, "doi": "10.1145/3397271.3401421", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last few years have seen an explosion of research on the topic of\nautomated question answering (QA), spanning the communities of information\nretrieval, natural language processing, and artificial intelligence. This\ntutorial would cover the highlights of this really active period of growth for\nQA to give the audience a grasp over the families of algorithms that are\ncurrently being used. We partition research contributions by the underlying\nsource from where answers are retrieved: curated knowledge graphs, unstructured\ntext, or hybrid corpora. We choose this dimension of partitioning as it is the\nmost discriminative when it comes to algorithm design. Other key dimensions are\ncovered within each sub-topic: like the complexity of questions addressed, and\ndegrees of explainability and interactivity introduced in the systems. We would\nconclude the tutorial with the most promising emerging trends in the expanse of\nQA, that would help new entrants into this field make the best decisions to\ntake the community forward. Much has changed in the community since the last\ntutorial on QA in SIGIR 2016, and we believe that this timely overview will\nindeed benefit a large number of conference participants.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:35:11 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 18:30:58 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 20:42:15 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2020 11:36:15 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Roy", "Rishiraj Saha", ""], ["Anand", "Avishek", ""]]}, {"id": "2004.11997", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman, Jennimaria Palomaki, Livio Baldini Soares, and Emily\n  Pitler", "title": "New Protocols and Negative Results for Textual Entailment Data\n  Collection", "comments": "To appear at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language inference (NLI) data has proven useful in benchmarking and,\nespecially, as pretraining data for tasks requiring language understanding.\nHowever, the crowdsourcing protocol that was used to collect this data has\nknown issues and was not explicitly optimized for either of these purposes, so\nit is likely far from ideal. We propose four alternative protocols, each aimed\nat improving either the ease with which annotators can produce sound training\nexamples or the quality and diversity of those examples. Using these\nalternatives and a fifth baseline protocol, we collect and compare five new\n8.5k-example training sets. In evaluations focused on transfer learning\napplications, our results are solidly negative, with models trained on our\nbaseline dataset yielding good transfer performance to downstream tasks, but\nnone of our four new methods (nor the recent ANLI) showing any improvements\nover that baseline. In a small silver lining, we observe that all four new\nprotocols, especially those where annotators edit pre-filled text boxes, reduce\npreviously observed issues with annotation artifacts.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 21:31:57 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 22:06:16 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Bowman", "Samuel R.", ""], ["Palomaki", "Jennimaria", ""], ["Soares", "Livio Baldini", ""], ["Pitler", "Emily", ""]]}, {"id": "2004.11999", "submitter": "Tal Linzen", "authors": "Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler, Tal Linzen", "title": "Syntactic Data Augmentation Increases Robustness to Inference Heuristics", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained neural models such as BERT, when fine-tuned to perform natural\nlanguage inference (NLI), often show high accuracy on standard datasets, but\ndisplay a surprising lack of sensitivity to word order on controlled challenge\nsets. We hypothesize that this issue is not primarily caused by the pretrained\nmodel's limitations, but rather by the paucity of crowdsourced NLI examples\nthat might convey the importance of syntactic structure at the fine-tuning\nstage. We explore several methods to augment standard training sets with\nsyntactically informative examples, generated by applying syntactic\ntransformations to sentences from the MNLI corpus. The best-performing\naugmentation method, subject/object inversion, improved BERT's accuracy on\ncontrolled examples that diagnose sensitivity to word order from 0.28 to 0.73,\nwithout affecting performance on the MNLI test set. This improvement\ngeneralized beyond the particular construction used for data augmentation,\nsuggesting that augmentation causes BERT to recruit abstract syntactic\nrepresentations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 21:35:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Min", "Junghyun", ""], ["McCoy", "R. Thomas", ""], ["Das", "Dipanjan", ""], ["Pitler", "Emily", ""], ["Linzen", "Tal", ""]]}, {"id": "2004.12006", "submitter": "Mandar Joshi", "authors": "Mandar Joshi, Kenton Lee, Yi Luan, Kristina Toutanova", "title": "Contextualized Representations Using Textual Encyclopedic Knowledge", "comments": "Added experiments comparing linkers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to represent input texts by contextualizing them jointly\nwith dynamically retrieved textual encyclopedic background knowledge from\nmultiple documents. We apply our method to reading comprehension tasks by\nencoding questions and passages together with background sentences about the\nentities they mention. We show that integrating background knowledge from text\nis effective for tasks focusing on factual reasoning and allows direct reuse of\npowerful pretrained BERT-style encoders. Moreover, knowledge integration can be\nfurther improved with suitable pretraining via a self-supervised masked\nlanguage model objective over words in background-augmented input text. On\nTriviaQA, our approach obtains improvements of 1.6 to 3.1 F1 over comparable\nRoBERTa models which do not integrate background knowledge dynamically. On\nMRQA, a large collection of diverse QA datasets, we see consistent gains\nin-domain along with large improvements out-of-domain on BioASQ (2.1 to 4.2\nF1), TextbookQA (1.6 to 2.0 F1), and DuoRC (1.1 to 2.0 F1).\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 22:08:09 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 05:39:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Joshi", "Mandar", ""], ["Lee", "Kenton", ""], ["Luan", "Yi", ""], ["Toutanova", "Kristina", ""]]}, {"id": "2004.12031", "submitter": "Zakaria Aldeneh", "authors": "Zakaria Aldeneh, Anushree Prasanna Kumar, Barry-John Theobald, Erik\n  Marchi, Sachin Kajarekar, Devang Naik, Ahmed Hussen Abdelaziz", "title": "On the Role of Visual Cues in Audiovisual Speech Enhancement", "comments": "ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an introspection of an audiovisual speech enhancement model. In\nparticular, we focus on interpreting how a neural audiovisual speech\nenhancement model uses visual cues to improve the quality of the target speech\nsignal. We show that visual cues provide not only high-level information about\nspeech activity, i.e., speech/silence, but also fine-grained visual information\nabout the place of articulation. One byproduct of this finding is that the\nlearned visual embeddings can be used as features for other visual speech\napplications. We demonstrate the effectiveness of the learned visual embeddings\nfor classifying visemes (the visual analogy to phonemes). Our results provide\ninsight into important aspects of audiovisual speech enhancement and\ndemonstrate how such models can be used for self-supervision tasks for visual\nspeech applications.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 01:00:03 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:27:47 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 17:11:24 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 15:56:42 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Aldeneh", "Zakaria", ""], ["Kumar", "Anushree Prasanna", ""], ["Theobald", "Barry-John", ""], ["Marchi", "Erik", ""], ["Kajarekar", "Sachin", ""], ["Naik", "Devang", ""], ["Abdelaziz", "Ahmed Hussen", ""]]}, {"id": "2004.12043", "submitter": "Kenneth Joseph", "authors": "Kenneth Joseph and Jonathan H. Morgan", "title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About\n  People?", "comments": "Accepted at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social biases are encoded in word embeddings. This presents a unique\nopportunity to study society historically and at scale, and a unique danger\nwhen embeddings are used in downstream applications. Here, we investigate the\nextent to which publicly-available word embeddings accurately reflect beliefs\nabout certain kinds of people as measured via traditional survey methods. We\nfind that biases found in word embeddings do, on average, closely mirror survey\ndata across seventeen dimensions of social meaning. However, we also find that\nbiases in embeddings are much more reflective of survey data for some\ndimensions of meaning (e.g. gender) than others (e.g. race), and that we can be\nhighly confident that embedding-based measures reflect survey data only for the\nmost salient biases.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 02:42:12 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Joseph", "Kenneth", ""], ["Morgan", "Jonathan H.", ""]]}, {"id": "2004.12057", "submitter": "Wanjun Zhong", "authors": "Wanjun Zhong, Duyu Tang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin", "title": "A Heterogeneous Graph with Factual, Temporal and Logical Knowledge for\n  Question Answering Over Dynamic Contexts", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study question answering over a dynamic textual environment. Although\nneural network models achieve impressive accuracy via learning from\ninput-output examples, they rarely leverage various types of knowledge and are\ngenerally not interpretable. In this work, we propose a graph-based approach,\nwhere a heterogeneous graph is automatically built with factual knowledge of\nthe context, temporal knowledge of the past states, and logical knowledge that\ncombines human-curated knowledge bases and rule bases. We develop a graph\nneural network over the constructed graph, and train the model in an end-to-end\nmanner. Experimental results on a benchmark dataset show that the injection of\nvarious types of knowledge improves a strong neural network baseline. An\nadditional benefit of our approach is that the graph itself naturally serves as\na rational behind the decision making.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 04:53:54 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhong", "Wanjun", ""], ["Tang", "Duyu", ""], ["Duan", "Nan", ""], ["Zhou", "Ming", ""], ["Wang", "Jiahai", ""], ["Yin", "Jian", ""]]}, {"id": "2004.12070", "submitter": "Zhou Yu", "authors": "Zhou Yu, Yuhao Cui, Jun Yu, Meng Wang, Dacheng Tao, Qi Tian", "title": "Deep Multimodal Neural Architecture Search", "comments": "Accept to ACM MM2020, code available at\n  https://github.com/MILVLG/mmnas/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing effective neural networks is fundamentally important in deep\nmultimodal learning. Most existing works focus on a single task and design\nneural architectures manually, which are highly task-specific and hard to\ngeneralize to different tasks. In this paper, we devise a generalized deep\nmultimodal neural architecture search (MMnas) framework for various multimodal\nlearning tasks. Given multimodal input, we first define a set of primitive\noperations, and then construct a deep encoder-decoder based unified backbone,\nwhere each encoder or decoder block corresponds to an operation searched from a\npredefined operation pool. On top of the unified backbone, we attach\ntask-specific heads to tackle different multimodal learning tasks. By using a\ngradient-based NAS algorithm, the optimal architectures for different tasks are\nlearned efficiently. Extensive ablation studies, comprehensive analysis, and\ncomparative experimental results show that the obtained MMnasNet significantly\noutperforms existing state-of-the-art approaches across three multimodal\nlearning tasks (over five datasets), including visual question answering,\nimage-text matching, and visual grounding.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 07:00:32 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 03:28:08 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yu", "Zhou", ""], ["Cui", "Yuhao", ""], ["Yu", "Jun", ""], ["Wang", "Meng", ""], ["Tao", "Dacheng", ""], ["Tian", "Qi", ""]]}, {"id": "2004.12073", "submitter": "Sho Takase", "authors": "Sho Takase and Sosuke Kobayashi", "title": "All Word Embeddings from One Embedding", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural network-based models for natural language processing (NLP), the\nlargest part of the parameters often consists of word embeddings. Conventional\nmodels prepare a large embedding matrix whose size depends on the vocabulary\nsize. Therefore, storing these models in memory and disk storage is costly. In\nthis study, to reduce the total number of parameters, the embeddings for all\nwords are represented by transforming a shared embedding. The proposed method,\nALONE (all word embeddings from one), constructs the embedding of a word by\nmodifying the shared embedding with a filter vector, which is word-specific but\nnon-trainable. Then, we input the constructed embedding into a feed-forward\nneural network to increase its expressiveness. Naively, the filter vectors\noccupy the same memory size as the conventional embedding matrix, which depends\non the vocabulary size. To solve this issue, we also introduce a\nmemory-efficient filter construction approach. We indicate our ALONE can be\nused as word representation sufficiently through an experiment on the\nreconstruction of pre-trained word embeddings. In addition, we also conduct\nexperiments on NLP application tasks: machine translation and summarization. We\ncombined ALONE with the current state-of-the-art encoder-decoder model, the\nTransformer, and achieved comparable scores on WMT 2014 English-to-German\ntranslation and DUC 2004 very short summarization with less parameters.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 07:38:08 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 03:36:32 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 03:12:12 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Takase", "Sho", ""], ["Kobayashi", "Sosuke", ""]]}, {"id": "2004.12111", "submitter": "Hari Krishna Vydana Mr", "authors": "Hari Krishna Vydana, Martin Karafi'at, Katerina Zmolikova, Luk'as\n  Burget, Honza Cernocky", "title": "Jointly Trained Transformers models for Spoken Language Translation", "comments": "7-pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional spoken language translation (SLT) systems are pipeline based\nsystems, where we have an Automatic Speech Recognition (ASR) system to convert\nthe modality of source from speech to text and a Machine Translation (MT)\nsystems to translate source text to text in target language. Recent progress in\nthe sequence-sequence architectures have reduced the performance gap between\nthe pipeline based SLT systems (cascaded ASR-MT) and End-to-End approaches.\nThough End-to-End and cascaded ASR-MT systems are reaching to the comparable\nlevels of performances, we can see a large performance gap using the ASR\nhypothesis and oracle text w.r.t MT models. This performance gap indicates that\nthe MT systems are prone to large performance degradation due to noisy ASR\nhypothesis as opposed to oracle text transcript. In this work this degradation\nin the performance is reduced by creating an end to-end differentiable pipeline\nbetween the ASR and MT systems. In this work, we train SLT systems with ASR\nobjective as an auxiliary loss and both the networks are connected through the\nneural hidden representations. This train ing would have an End-to-End\ndifferentiable path w.r.t to the final objective function as well as utilize\nthe ASR objective for better performance of the SLT systems. This architecture\nhas improved from BLEU from 36.8 to 44.5. Due to the Multi-task training the\nmodel also generates the ASR hypothesis which are used by a pre-trained MT\nmodel. Combining the proposed systems with the MT model has increased the BLEU\nscore by 1. All the experiments are reported on English-Portuguese speech\ntranslation task using How2 corpus. The final BLEU score is on-par with the\nbest speech translation system on How2 dataset with no additional training data\nand language model and much less parameters.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 11:28:39 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Vydana", "Hari Krishna", ""], ["Karafi'at", "Martin", ""], ["Zmolikova", "Katerina", ""], ["Burget", "Luk'as", ""], ["Cernocky", "Honza", ""]]}, {"id": "2004.12126", "submitter": "Hongyu Lin", "authors": "Hongyu Lin, Yaojie Lu, Jialong Tang, Xianpei Han, Le Sun, Zhicheng\n  Wei, Nicholas Jing Yuan", "title": "A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained\n  Model Lead to the Promised Land?", "comments": "Accepted to EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning pretrained model has achieved promising performance on standard\nNER benchmarks. Generally, these benchmarks are blessed with strong name\nregularity, high mention coverage and sufficient context diversity.\nUnfortunately, when scaling NER to open situations, these advantages may no\nlonger exist. And therefore it raises a critical question of whether previous\ncreditable approaches can still work well when facing these challenges. As\nthere is no currently available dataset to investigate this problem, this paper\nproposes to conduct randomization test on standard benchmarks. Specifically, we\nerase name regularity, mention coverage and context diversity respectively from\nthe benchmarks, in order to explore their impact on the generalization ability\nof models. To further verify our conclusions, we also construct a new open NER\ndataset that focuses on entity types with weaker name regularity and lower\nmention coverage to verify our conclusion. From both randomization test and\nempirical experiments, we draw the conclusions that 1) name regularity is\ncritical for the models to generalize to unseen mentions; 2) high mention\ncoverage may undermine the model generalization ability and 3) context patterns\nmay not require enormous data to capture when using pretrained encoders.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 12:30:16 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 07:06:06 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Lin", "Hongyu", ""], ["Lu", "Yaojie", ""], ["Tang", "Jialong", ""], ["Han", "Xianpei", ""], ["Sun", "Le", ""], ["Wei", "Zhicheng", ""], ["Yuan", "Nicholas Jing", ""]]}, {"id": "2004.12158", "submitter": "Haoxi Zhong", "authors": "Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu,\n  Maosong Sun", "title": "How Does NLP Benefit Legal System: A Summary of Legal Artificial\n  Intelligence", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legal Artificial Intelligence (LegalAI) focuses on applying the technology of\nartificial intelligence, especially natural language processing, to benefit\ntasks in the legal domain. In recent years, LegalAI has drawn increasing\nattention rapidly from both AI researchers and legal professionals, as LegalAI\nis beneficial to the legal system for liberating legal professionals from a\nmaze of paperwork. Legal professionals often think about how to solve tasks\nfrom rule-based and symbol-based methods, while NLP researchers concentrate\nmore on data-driven and embedding methods. In this paper, we introduce the\nhistory, the current state, and the future directions of research in LegalAI.\nWe illustrate the tasks from the perspectives of legal professionals and NLP\nresearchers and show several representative applications in LegalAI. We conduct\nexperiments and provide an in-depth analysis of the advantages and\ndisadvantages of existing works to explore possible future directions. You can\nfind the implementation of our work from https://github.com/thunlp/CLAIM.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 14:45:15 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 06:10:44 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 11:43:46 GMT"}, {"version": "v4", "created": "Sat, 2 May 2020 06:02:24 GMT"}, {"version": "v5", "created": "Mon, 18 May 2020 06:32:27 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhong", "Haoxi", ""], ["Xiao", "Chaojun", ""], ["Tu", "Cunchao", ""], ["Zhang", "Tianyang", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "2004.12169", "submitter": "Sheena Panthaplackel", "authors": "Sheena Panthaplackel, Pengyu Nie, Milos Gligoric, Junyi Jessy Li,\n  Raymond J. Mooney", "title": "Learning to Update Natural Language Comments Based on Code Changes", "comments": "Accepted in Association for Computational Linguistics (ACL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the novel task of automatically updating an existing natural\nlanguage comment based on changes in the body of code it accompanies. We\npropose an approach that learns to correlate changes across two distinct\nlanguage representations, to generate a sequence of edits that are applied to\nthe existing comment to reflect the source code modifications. We train and\nevaluate our model using a dataset that we collected from commit histories of\nopen-source software projects, with each example consisting of a concurrent\nupdate to a method and its corresponding comment. We compare our approach\nagainst multiple baselines using both automatic metrics and human evaluation.\nResults reflect the challenge of this task and that our model outperforms\nbaselines with respect to making edits.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 15:37:46 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 02:53:17 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Panthaplackel", "Sheena", ""], ["Nie", "Pengyu", ""], ["Gligoric", "Milos", ""], ["Li", "Junyi Jessy", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "2004.12184", "submitter": "Ganesh Bagler Dr", "authors": "Nirav Diwan, Devansh Batra and Ganesh Bagler", "title": "A Named Entity Based Approach to Model Recipes", "comments": "36th IEEE International Conference on Data Engineering (ICDE 2020),\n  DECOR Workshop; 6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional cooking recipes follow a structure which can be modelled very\nwell if the rules and semantics of the different sections of the recipe text\nare analyzed and represented accurately. We propose a structure that can\naccurately represent the recipe as well as a pipeline to infer the best\nrepresentation of the recipe in this uniform structure. The Ingredients section\nin a recipe typically lists down the ingredients required and corresponding\nattributes such as quantity, temperature, and processing state. This can be\nmodelled by defining these attributes and their values. The physical entities\nwhich make up a recipe can be broadly classified into utensils, ingredients and\ntheir combinations that are related by cooking techniques. The instruction\nsection lists down a series of events in which a cooking technique or process\nis applied upon these utensils and ingredients. We model these relationships in\nthe form of tuples. Thus, using a combination of these methods we model cooking\nrecipe in the dataset RecipeDB to show the efficacy of our method. This mined\ninformation model can have several applications which include translating\nrecipes between languages, determining similarity between recipes, generation\nof novel recipes and estimation of the nutritional profile of recipes. For the\npurpose of recognition of ingredient attributes, we train the Named Entity\nRelationship (NER) models and analyze the inferences with the help of K-Means\nclustering. Our model presented with an F1 score of 0.95 across all datasets.\nWe use a similar NER tagging model for labelling cooking techniques (F1 score =\n0.88) and utensils (F1 score = 0.90) within the instructions section. Finally,\nwe determine the temporal sequence of relationships between ingredients,\nutensils and cooking techniques for modeling the instruction steps.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 16:37:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Diwan", "Nirav", ""], ["Batra", "Devansh", ""], ["Bagler", "Ganesh", ""]]}, {"id": "2004.12190", "submitter": "Georg Rehm", "authors": "Georg Rehm and Karolina Zaczynska and Juli\\'an Moreno-Schneider and\n  Malte Ostendorff and Peter Bourgonje and Maria Berger and Jens Rauenbusch and\n  Andr\\'e Schmidt and Mikka Wild", "title": "Towards Discourse Parsing-inspired Semantic Storytelling", "comments": "Proceedings of QURATOR 2020: The conference for intelligent content\n  solutions, Berlin, Germany, February 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous work of ours on Semantic Storytelling uses text analytics procedures\nincluding Named Entity Recognition and Event Detection. In this paper, we\noutline our longer-term vision on Semantic Storytelling and describe the\ncurrent conceptual and technical approach. In the project that drives our\nresearch we develop AI-based technologies that are verified by partners from\nindustry. One long-term goal is the development of an approach for Semantic\nStorytelling that has broad coverage and that is, furthermore, robust. We\nprovide first results on experiments that involve discourse parsing, applied to\na concrete use case, \"Explore the Neighbourhood!\", which is based on a\nsemi-automatically collected data set with documents about noteworthy people in\none of Berlin's districts. Though automatically obtaining annotations for\ncoherence relations from plain text is a non-trivial challenge, our preliminary\nresults are promising. We envision our approach to be combined with additional\nfeatures (NER, coreference resolution, knowledge graphs\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 17:09:56 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Rehm", "Georg", ""], ["Zaczynska", "Karolina", ""], ["Moreno-Schneider", "Juli\u00e1n", ""], ["Ostendorff", "Malte", ""], ["Bourgonje", "Peter", ""], ["Berger", "Maria", ""], ["Rauenbusch", "Jens", ""], ["Schmidt", "Andr\u00e9", ""], ["Wild", "Mikka", ""]]}, {"id": "2004.12195", "submitter": "Georg Rehm", "authors": "Georg Rehm, Peter Bourgonje, Stefanie Hegele, Florian Kintzel,\n  Juli\\'an Moreno Schneider, Malte Ostendorff, Karolina Zaczynska, Armin\n  Berger, Stefan Grill, S\\\"oren R\\\"auchle, Jens Rauenbusch, Lisa Rutenburg,\n  Andr\\'e Schmidt, Mikka Wild, Henry Hoffmann, Julian Fink, Sarah Schulz,\n  Jurica Seva, Joachim Quantz, Joachim B\\\"ottger, Josefine Matthey, Rolf\n  Fricke, Jan Thomsen, Adrian Paschke, Jamal Al Qundus, Thomas Hoppe, Naouel\n  Karam, Frauke Weichhardt, Christian Fillies, Clemens Neudecker, Mike Gerber,\n  Kai Labusch, Vahid Rezanezhad, Robin Schaefer, David Zellh\\\"ofer, Daniel\n  Siewert, Patrick Bunk, Lydia Pintscher, Elena Aleynikova, Franziska Heine", "title": "QURATOR: Innovative Technologies for Content and Data Curation", "comments": "Proceedings of QURATOR 2020: The conference for intelligent content\n  solutions, Berlin, Germany, February 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In all domains and sectors, the demand for intelligent systems to support the\nprocessing and generation of digital content is rapidly increasing. The\navailability of vast amounts of content and the pressure to publish new content\nquickly and in rapid succession requires faster, more efficient and smarter\nprocessing and generation methods. With a consortium of ten partners from\nresearch and industry and a broad range of expertise in AI, Machine Learning\nand Language Technologies, the QURATOR project, funded by the German Federal\nMinistry of Education and Research, develops a sustainable and innovative\ntechnology platform that provides services to support knowledge workers in\nvarious industries to address the challenges they face when curating digital\ncontent. The project's vision and ambition is to establish an ecosystem for\ncontent curation technologies that significantly pushes the current state of\nthe art and transforms its region, the metropolitan area Berlin-Brandenburg,\ninto a global centre of excellence for curation technologies.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 17:21:15 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Rehm", "Georg", ""], ["Bourgonje", "Peter", ""], ["Hegele", "Stefanie", ""], ["Kintzel", "Florian", ""], ["Schneider", "Juli\u00e1n Moreno", ""], ["Ostendorff", "Malte", ""], ["Zaczynska", "Karolina", ""], ["Berger", "Armin", ""], ["Grill", "Stefan", ""], ["R\u00e4uchle", "S\u00f6ren", ""], ["Rauenbusch", "Jens", ""], ["Rutenburg", "Lisa", ""], ["Schmidt", "Andr\u00e9", ""], ["Wild", "Mikka", ""], ["Hoffmann", "Henry", ""], ["Fink", "Julian", ""], ["Schulz", "Sarah", ""], ["Seva", "Jurica", ""], ["Quantz", "Joachim", ""], ["B\u00f6ttger", "Joachim", ""], ["Matthey", "Josefine", ""], ["Fricke", "Rolf", ""], ["Thomsen", "Jan", ""], ["Paschke", "Adrian", ""], ["Qundus", "Jamal Al", ""], ["Hoppe", "Thomas", ""], ["Karam", "Naouel", ""], ["Weichhardt", "Frauke", ""], ["Fillies", "Christian", ""], ["Neudecker", "Clemens", ""], ["Gerber", "Mike", ""], ["Labusch", "Kai", ""], ["Rezanezhad", "Vahid", ""], ["Schaefer", "Robin", ""], ["Zellh\u00f6fer", "David", ""], ["Siewert", "Daniel", ""], ["Bunk", "Patrick", ""], ["Pintscher", "Lydia", ""], ["Aleynikova", "Elena", ""], ["Heine", "Franziska", ""]]}, {"id": "2004.12198", "submitter": "Mengjie Zhao", "authors": "Mengjie Zhao, Philipp Dufter, Yadollah Yaghoobzadeh, Hinrich Sch\\\"utze", "title": "Quantifying the Contextualization of Word Representations with Semantic\n  Class Probing", "comments": "EMNLP Findings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretrained language models have achieved a new state of the art on many NLP\ntasks, but there are still many open questions about how and why they work so\nwell. We investigate the contextualization of words in BERT. We quantify the\namount of contextualization, i.e., how well words are interpreted in context,\nby studying the extent to which semantic classes of a word can be inferred from\nits contextualized embeddings. Quantifying contextualization helps in\nunderstanding and utilizing pretrained language models. We show that top layer\nrepresentations achieve high accuracy inferring semantic classes; that the\nstrongest contextualization effects occur in the lower layers; that local\ncontext is mostly sufficient for semantic class inference; and that top layer\nrepresentations are more task-specific after finetuning while lower layer\nrepresentations are more transferable. Finetuning uncovers task related\nfeatures, but pretrained knowledge is still largely preserved.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 17:49:37 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 12:26:20 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhao", "Mengjie", ""], ["Dufter", "Philipp", ""], ["Yaghoobzadeh", "Yadollah", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2004.12238", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Trisha Mittal, Dinesh Manocha", "title": "MCQA: Multimodal Co-attention Based Network for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MCQA, a learning-based algorithm for multimodal question\nanswering. MCQA explicitly fuses and aligns the multimodal input (i.e. text,\naudio, and video), which forms the context for the query (question and answer).\nOur approach fuses and aligns the question and the answer within this context.\nMoreover, we use the notion of co-attention to perform cross-modal alignment\nand multimodal context-query alignment. Our context-query alignment module\nmatches the relevant parts of the multimodal context and the query with each\nother and aligns them to improve the overall performance. We evaluate the\nperformance of MCQA on Social-IQ, a benchmark dataset for multimodal question\nanswering. We compare the performance of our algorithm with prior methods and\nobserve an accuracy improvement of 4-7%.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 21:37:12 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kumar", "Abhishek", ""], ["Mittal", "Trisha", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2004.12239", "submitter": "Jiaao Chen", "authors": "Jiaao Chen, Zichao Yang, Diyi Yang", "title": "MixText: Linguistically-Informed Interpolation of Hidden Space for\n  Semi-Supervised Text Classification", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents MixText, a semi-supervised learning method for text\nclassification, which uses our newly designed data augmentation method called\nTMix. TMix creates a large amount of augmented training samples by\ninterpolating text in hidden space. Moreover, we leverage recent advances in\ndata augmentation to guess low-entropy labels for unlabeled data, hence making\nthem as easy to use as labeled data.By mixing labeled, unlabeled and augmented\ndata, MixText significantly outperformed current pre-trained and fined-tuned\nmodels and other state-of-the-art semi-supervised learning methods on several\ntext classification benchmarks. The improvement is especially prominent when\nsupervision is extremely limited. We have publicly released our code at\nhttps://github.com/GT-SALT/MixText.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 21:37:36 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chen", "Jiaao", ""], ["Yang", "Zichao", ""], ["Yang", "Diyi", ""]]}, {"id": "2004.12247", "submitter": "Arda Akdemir", "authors": "Arda Akdemir and Tetsuo Shibuya and Tunga G\\\"ung\\\"or", "title": "Hierarchical Multi Task Learning with Subword Contextual Embeddings for\n  Languages with Rich Morphology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological information is important for many sequence labeling tasks in\nNatural Language Processing (NLP). Yet, existing approaches rely heavily on\nmanual annotations or external software to capture this information. In this\nstudy, we propose using subword contextual embeddings to capture the\nmorphological information for languages with rich morphology. In addition, we\nincorporate these embeddings in a hierarchical multi-task setting which is not\nemployed before, to the best of our knowledge. Evaluated on Dependency Parsing\n(DEP) and Named Entity Recognition (NER) tasks, which are shown to benefit\ngreatly from morphological information, our final model outperforms previous\nstate-of-the-art models on both tasks for the Turkish language. Besides, we\nshow a net improvement of 18.86% and 4.61% F-1 over the previously proposed\nmulti-task learner in the same setting for the DEP and the NER tasks,\nrespectively. Empirical results for five different MTL settings show that\nincorporating subword contextual embeddings brings significant improvements for\nboth tasks. In addition, we observed that multi-task learning consistently\nimproves the performance of the DEP component.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 22:55:56 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Akdemir", "Arda", ""], ["Shibuya", "Tetsuo", ""], ["G\u00fcng\u00f6r", "Tunga", ""]]}, {"id": "2004.12265", "submitter": "Yonatan Belinkov", "authors": "Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel\n  Nevo, Simas Sakenis, Jason Huang, Yaron Singer, Stuart Shieber", "title": "Causal Mediation Analysis for Interpreting Neural NLP: The Case of\n  Gender Bias", "comments": "Expanded version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common methods for interpreting neural models in natural language processing\ntypically examine either their structure or their behavior, but not both. We\npropose a methodology grounded in the theory of causal mediation analysis for\ninterpreting which parts of a model are causally implicated in its behavior. It\nenables us to analyze the mechanisms by which information flows from input to\noutput through various model components, known as mediators. We apply this\nmethodology to analyze gender bias in pre-trained Transformer language models.\nWe study the role of individual neurons and attention heads in mediating gender\nbias across three datasets designed to gauge a model's sensitivity to gender\nbias. Our mediation analysis reveals that gender bias effects are (i) sparse,\nconcentrated in a small part of the network; (ii) synergistic, amplified or\nrepressed by different components; and (iii) decomposable into effects flowing\ndirectly from the input and indirectly through the mediators.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 01:53:03 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 07:58:08 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Vig", "Jesse", ""], ["Gehrmann", "Sebastian", ""], ["Belinkov", "Yonatan", ""], ["Qian", "Sharon", ""], ["Nevo", "Daniel", ""], ["Sakenis", "Simas", ""], ["Huang", "Jason", ""], ["Singer", "Yaron", ""], ["Shieber", "Stuart", ""]]}, {"id": "2004.12274", "submitter": "Baoyu Jing", "authors": "Baoyu Jing, Zeya Wang, Eric Xing", "title": "Show, Describe and Conclude: On Exploiting the Structure Information of\n  Chest X-Ray Reports", "comments": "ACL 2019", "journal-ref": null, "doi": "10.18653/v1/P19-1657", "report-no": null, "categories": "cs.CL cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chest X-Ray (CXR) images are commonly used for clinical screening and\ndiagnosis. Automatically writing reports for these images can considerably\nlighten the workload of radiologists for summarizing descriptive findings and\nconclusive impressions. The complex structures between and within sections of\nthe reports pose a great challenge to the automatic report generation.\nSpecifically, the section Impression is a diagnostic summarization over the\nsection Findings; and the appearance of normality dominates each section over\nthat of abnormality. Existing studies rarely explore and consider this\nfundamental structure information. In this work, we propose a novel framework\nthat exploits the structure information between and within report sections for\ngenerating CXR imaging reports. First, we propose a two-stage strategy that\nexplicitly models the relationship between Findings and Impression. Second, we\ndesign a novel cooperative multi-agent system that implicitly captures the\nimbalanced distribution between abnormality and normality. Experiments on two\nCXR report datasets show that our method achieves state-of-the-art performance\nin terms of various evaluation metrics. Our results expose that the proposed\napproach is able to generate high-quality medical reports through integrating\nthe structure information.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 02:29:20 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 17:44:44 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Jing", "Baoyu", ""], ["Wang", "Zeya", ""], ["Xing", "Eric", ""]]}, {"id": "2004.12297", "submitter": "Liu Yang", "authors": "Liu Yang, Mingyang Zhang, Cheng Li, Michael Bendersky, Marc Najork", "title": "Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical\n  Encoder for Long-Form Document Matching", "comments": "Accepted as a full paper in CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3411908", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many natural language processing and information retrieval problems can be\nformalized as the task of semantic matching. Existing work in this area has\nbeen largely focused on matching between short texts (e.g., question\nanswering), or between a short and a long text (e.g., ad-hoc retrieval).\nSemantic matching between long-form documents, which has many important\napplications like news recommendation, related article recommendation and\ndocument clustering, is relatively less explored and needs more research\neffort. In recent years, self-attention based models like Transformers and BERT\nhave achieved state-of-the-art performance in the task of text matching. These\nmodels, however, are still limited to short text like a few sentences or one\nparagraph due to the quadratic computational complexity of self-attention with\nrespect to input text length. In this paper, we address the issue by proposing\nthe Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for\nlong-form document matching. Our model contains several innovations to adapt\nself-attention models for longer text input. In order to better capture\nsentence level semantic relations within a document, we pre-train the model\nwith a novel masked sentence block language modeling task in addition to the\nmasked word language modeling task used by BERT. Our experimental results on\nseveral benchmark datasets for long-form document matching show that our\nproposed SMITH model outperforms the previous state-of-the-art models including\nhierarchical attention, multi-depth attention-based hierarchical recurrent\nneural network, and BERT. Comparing to BERT based baselines, our model is able\nto increase maximum input text length from 512 to 2048. We will open source a\nWikipedia based benchmark dataset, code and a pre-trained checkpoint to\naccelerate future research on long-form document matching.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 07:04:08 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 01:48:52 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yang", "Liu", ""], ["Zhang", "Mingyang", ""], ["Li", "Cheng", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""]]}, {"id": "2004.12299", "submitter": "Su Zhu", "authors": "Su Zhu, Ruisheng Cao, and Kai Yu", "title": "Dual Learning for Semi-Supervised Natural Language Understanding", "comments": "12 pages, 4 figures; Accepted for IEEE/ACM Transactions on Audio\n  Speech and Language Processing", "journal-ref": null, "doi": "10.1109/TASLP.2020.3001684", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language understanding (NLU) converts sentences into structured\nsemantic forms. The paucity of annotated training samples is still a\nfundamental challenge of NLU. To solve this data sparsity problem, previous\nwork based on semi-supervised learning mainly focuses on exploiting unlabeled\nsentences. In this work, we introduce a dual task of NLU, semantic-to-sentence\ngeneration (SSG), and propose a new framework for semi-supervised NLU with the\ncorresponding dual model. The framework is composed of dual pseudo-labeling and\ndual learning method, which enables an NLU model to make full use of data\n(labeled and unlabeled) through a closed-loop of the primal and dual tasks. By\nincorporating the dual task, the framework can exploit pure semantic forms as\nwell as unlabeled sentences, and further improve the NLU and SSG models\niteratively in the closed-loop. The proposed approaches are evaluated on two\npublic datasets (ATIS and SNIPS). Experiments in the semi-supervised setting\nshow that our methods can outperform various baselines significantly, and\nextensive ablation studies are conducted to verify the effectiveness of our\nframework. Finally, our method can also achieve the state-of-the-art\nperformance on the two datasets in the supervised setting. Our code is\navailable at \\url{https://github.com/rhythmcao/slu-dual-learning.git}.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 07:17:48 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 02:41:55 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 08:00:58 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 09:53:54 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhu", "Su", ""], ["Cao", "Ruisheng", ""], ["Yu", "Kai", ""]]}, {"id": "2004.12302", "submitter": "Canwen Xu", "authors": "Canwen Xu and Jiaxin Pei and Hongtao Wu and Yiyu Liu and Chenliang Li", "title": "MATINF: A Jointly Labeled Large-Scale Dataset for Classification,\n  Question Answering and Summarization", "comments": "Accepted as a long paper at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, large-scale datasets have vastly facilitated the development in\nnearly all domains of Natural Language Processing. However, there is currently\nno cross-task dataset in NLP, which hinders the development of multi-task\nlearning. We propose MATINF, the first jointly labeled large-scale dataset for\nclassification, question answering and summarization. MATINF contains 1.07\nmillion question-answer pairs with human-labeled categories and user-generated\nquestion descriptions. Based on such rich information, MATINF is applicable for\nthree major NLP tasks, including classification, question answering, and\nsummarization. We benchmark existing methods and a novel multi-task baseline\nover MATINF to inspire further research. Our comprehensive comparison and\nexperiments over MATINF and other datasets demonstrate the merits held by\nMATINF.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 07:43:15 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 06:11:55 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Xu", "Canwen", ""], ["Pei", "Jiaxin", ""], ["Wu", "Hongtao", ""], ["Liu", "Yiyu", ""], ["Li", "Chenliang", ""]]}, {"id": "2004.12303", "submitter": "Xinyue Zheng", "authors": "Xinyue Zheng, Peng Wang, Qigang Wang, Zhongchao Shi", "title": "Challenge Closed-book Science Exam: A Meta-learning Based Question\n  Answering System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work in standardized science exams requires support from large text\ncorpus, such as targeted science corpus fromWikipedia or SimpleWikipedia.\nHowever, retrieving knowledge from the large corpus is time-consuming and\nquestions embedded in complex semantic representation may interfere with\nretrieval. Inspired by the dual process theory in cognitive science, we propose\na MetaQA framework, where system 1 is an intuitive meta-classifier and system 2\nis a reasoning module. Specifically, our method based on meta-learning method\nand large language model BERT, which can efficiently solve science problems by\nlearning from related example questions without relying on external knowledge\nbases. We evaluate our method on AI2 Reasoning Challenge (ARC), and the\nexperimental results show that meta-classifier yields considerable\nclassification performance on emerging question types. The information provided\nby meta-classifier significantly improves the accuracy of reasoning module from\n46.6% to 64.2%, which has a competitive advantage over retrieval-based QA\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 07:43:30 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zheng", "Xinyue", ""], ["Wang", "Peng", ""], ["Wang", "Qigang", ""], ["Shi", "Zhongchao", ""]]}, {"id": "2004.12307", "submitter": "Arindam Pal", "authors": "Paheli Bhattacharya, Kripabandhu Ghosh, Arindam Pal, Saptarshi Ghosh", "title": "Methods for Computing Legal Document Similarity: A Comparative Study", "comments": "This paper was published at the LDA 2019 workshop in the JURIX 2019\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing similarity between two legal documents is an important and\nchallenging task in the domain of Legal Information Retrieval. Finding similar\nlegal documents has many applications in downstream tasks, including prior-case\nretrieval, recommendation of legal articles, and so on. Prior works have\nproposed two broad ways of measuring similarity between legal documents -\nanalyzing the precedent citation network, and measuring similarity based on\ntextual content similarity measures. But there has not been a comprehensive\ncomparison of these existing methods on a common platform. In this paper, we\nperform the first systematic analysis of the existing methods. In addition, we\nexplore two promising new similarity computation methods - one text-based and\nthe other based on network embeddings, which have not been considered till now.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 08:26:04 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bhattacharya", "Paheli", ""], ["Ghosh", "Kripabandhu", ""], ["Pal", "Arindam", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2004.12316", "submitter": "Peixiang Zhong", "authors": "Peixiang Zhong, Chen Zhang, Hao Wang, Yong Liu, Chunyan Miao", "title": "Towards Persona-Based Empathetic Conversational Models", "comments": "Accepted to EMNLP 2020 (A new dataset is proposed:\n  https://github.com/zhongpeixiang/PEC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empathetic conversational models have been shown to improve user satisfaction\nand task outcomes in numerous domains. In Psychology, persona has been shown to\nbe highly correlated to personality, which in turn influences empathy. In\naddition, our empirical analysis also suggests that persona plays an important\nrole in empathetic conversations. To this end, we propose a new task towards\npersona-based empathetic conversations and present the first empirical study on\nthe impact of persona on empathetic responding. Specifically, we first present\na novel large-scale multi-domain dataset for persona-based empathetic\nconversations. We then propose CoBERT, an efficient BERT-based response\nselection model that obtains the state-of-the-art performance on our dataset.\nFinally, we conduct extensive experiments to investigate the impact of persona\non empathetic responding. Notably, our results show that persona improves\nempathetic responding more when CoBERT is trained on empathetic conversations\nthan non-empathetic ones, establishing an empirical link between persona and\nempathy in human conversations.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 08:51:01 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 01:55:05 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 03:40:56 GMT"}, {"version": "v4", "created": "Wed, 16 Sep 2020 06:48:24 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2020 08:23:51 GMT"}, {"version": "v6", "created": "Mon, 5 Oct 2020 09:21:06 GMT"}, {"version": "v7", "created": "Thu, 19 Nov 2020 11:00:23 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Zhong", "Peixiang", ""], ["Zhang", "Chen", ""], ["Wang", "Hao", ""], ["Liu", "Yong", ""], ["Miao", "Chunyan", ""]]}, {"id": "2004.12330", "submitter": "Adrian Groza", "authors": "Adrian Groza", "title": "Detecting fake news for the new coronavirus by reasoning on the Covid-19\n  ontology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the Covid-19 pandemic, many were quick to spread deceptive\ninformation. I investigate here how reasoning in Description Logics (DLs) can\ndetect inconsistencies between trusted medical sources and not trusted ones.\nThe not-trusted information comes in natural language (e.g. \"Covid-19 affects\nonly the elderly\"). To automatically convert into DLs, I used the FRED\nconverter. Reasoning in Description Logics is then performed with the Racer\ntool.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 09:34:32 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Groza", "Adrian", ""]]}, {"id": "2004.12331", "submitter": "Rui Wang", "authors": "Rui Wang, Xuemeng Hu, Deyu Zhou, Yulan He, Yuxuan Xiong, Chenchen Ye,\n  Haiyang Xu", "title": "Neural Topic Modeling with Bidirectional Adversarial Training", "comments": "To appear at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a surge of interests of using neural topic models\nfor automatic topic extraction from text, since they avoid the complicated\nmathematical derivations for model inference as in traditional topic models\nsuch as Latent Dirichlet Allocation (LDA). However, these models either\ntypically assume improper prior (e.g. Gaussian or Logistic Normal) over latent\ntopic space or could not infer topic distribution for a given document. To\naddress these limitations, we propose a neural topic modeling approach, called\nBidirectional Adversarial Topic (BAT) model, which represents the first attempt\nof applying bidirectional adversarial training for neural topic modeling. The\nproposed BAT builds a two-way projection between the document-topic\ndistribution and the document-word distribution. It uses a generator to capture\nthe semantic patterns from texts and an encoder for topic inference.\nFurthermore, to incorporate word relatedness information, the Bidirectional\nAdversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To\nverify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are\nused in our experiments. The experimental results show that BAT and\nGaussian-BAT obtain more coherent topics, outperforming several competitive\nbaselines. Moreover, when performing text clustering based on the extracted\ntopics, our models outperform all the baselines, with more significant\nimprovements achieved by Gaussian-BAT where an increase of near 6\\% is observed\nin accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 09:41:17 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Rui", ""], ["Hu", "Xuemeng", ""], ["Zhou", "Deyu", ""], ["He", "Yulan", ""], ["Xiong", "Yuxuan", ""], ["Ye", "Chenchen", ""], ["Xu", "Haiyang", ""]]}, {"id": "2004.12332", "submitter": "Kawin Ethayarajh", "authors": "Kawin Ethayarajh", "title": "Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty\n  with Bernstein Bounds", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most NLP datasets are not annotated with protected attributes such as gender,\nmaking it difficult to measure classification bias using standard measures of\nfairness (e.g., equal opportunity). However, manually annotating a large\ndataset with a protected attribute is slow and expensive. Instead of annotating\nall the examples, can we annotate a subset of them and use that sample to\nestimate the bias? While it is possible to do so, the smaller this annotated\nsample is, the less certain we are that the estimate is close to the true bias.\nIn this work, we propose using Bernstein bounds to represent this uncertainty\nabout the bias estimate as a confidence interval. We provide empirical evidence\nthat a 95% confidence interval derived this way consistently bounds the true\nbias. In quantifying this uncertainty, our method, which we call\nBernstein-bounded unfairness, helps prevent classifiers from being deemed\nbiased or unbiased when there is insufficient evidence to make either claim.\nOur findings suggest that the datasets currently used to measure specific\nbiases are too small to conclusively identify bias except in the most egregious\ncases. For example, consider a co-reference resolution system that is 5% more\naccurate on gender-stereotypical sentences -- to claim it is biased with 95%\nconfidence, we need a bias-specific dataset that is 3.8 times larger than\nWinoBias, the largest available.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 09:45:45 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Ethayarajh", "Kawin", ""]]}, {"id": "2004.12362", "submitter": "Xiaojun Quan", "authors": "Kai Wang and Weizhou Shen and Yunyi Yang and Xiaojun Quan and Rui Wang", "title": "Relational Graph Attention Network for Aspect-based Sentiment Analysis", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based sentiment analysis aims to determine the sentiment polarity\ntowards a specific aspect in online reviews. Most recent efforts adopt\nattention-based neural network models to implicitly connect aspects with\nopinion words. However, due to the complexity of language and the existence of\nmultiple aspects in a single sentence, these models often confuse the\nconnections. In this paper, we address this problem by means of effective\nencoding of syntax information. Firstly, we define a unified aspect-oriented\ndependency tree structure rooted at a target aspect by reshaping and pruning an\nordinary dependency parse tree. Then, we propose a relational graph attention\nnetwork (R-GAT) to encode the new tree structure for sentiment prediction.\nExtensive experiments are conducted on the SemEval 2014 and Twitter datasets,\nand the experimental results confirm that the connections between aspects and\nopinion words can be better established with our approach, and the performance\nof the graph attention network (GAT) is significantly improved as a\nconsequence.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 12:21:04 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Kai", ""], ["Shen", "Weizhou", ""], ["Yang", "Yunyi", ""], ["Quan", "Xiaojun", ""], ["Wang", "Rui", ""]]}, {"id": "2004.12363", "submitter": "Xiaojun Quan", "authors": "Kai Wang and Junfeng Tian and Rui Wang and Xiaojun Quan and Jianxing\n  Yu", "title": "Multi-Domain Dialogue Acts and Response Co-Generation", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating fluent and informative responses is of critical importance for\ntask-oriented dialogue systems. Existing pipeline approaches generally predict\nmultiple dialogue acts first and use them to assist response generation. There\nare at least two shortcomings with such approaches. First, the inherent\nstructures of multi-domain dialogue acts are neglected. Second, the semantic\nassociations between acts and responses are not taken into account for response\ngeneration. To address these issues, we propose a neural co-generation model\nthat generates dialogue acts and responses concurrently. Unlike those pipeline\napproaches, our act generation module preserves the semantic structures of\nmulti-domain dialogue acts and our response generation module dynamically\nattends to different acts as needed. We train the two modules jointly using an\nuncertainty loss to adjust their task weights adaptively. Extensive experiments\nare conducted on the large-scale MultiWOZ dataset and the results show that our\nmodel achieves very favorable improvement over several state-of-the-art models\nin both automatic and human evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 12:21:17 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Kai", ""], ["Tian", "Junfeng", ""], ["Wang", "Rui", ""], ["Quan", "Xiaojun", ""], ["Yu", "Jianxing", ""]]}, {"id": "2004.12376", "submitter": "Simran Khanuja", "authors": "Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana\n  Sitaram, Monojit Choudhury", "title": "GLUECoS : An Evaluation Benchmark for Code-Switched NLP", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code-switching is the use of more than one language in the same conversation\nor utterance. Recently, multilingual contextual embedding models, trained on\nmultiple monolingual corpora, have shown promising results on cross-lingual and\nmultilingual tasks. We present an evaluation benchmark, GLUECoS, for\ncode-switched languages, that spans several NLP tasks in English-Hindi and\nEnglish-Spanish. Specifically, our evaluation benchmark includes Language\nIdentification from text, POS tagging, Named Entity Recognition, Sentiment\nAnalysis, Question Answering and a new task for code-switching, Natural\nLanguage Inference. We present results on all these tasks using cross-lingual\nword embedding models and multilingual models. In addition, we fine-tune\nmultilingual models on artificially generated code-switched data. Although\nmultilingual models perform significantly better than cross-lingual models, our\nresults show that in most tasks, across both language pairs, multilingual\nmodels fine-tuned on code-switched data perform best, showing that multilingual\nmodels can be further optimized for code-switching tasks.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 13:28:34 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 05:57:38 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Khanuja", "Simran", ""], ["Dandapat", "Sandipan", ""], ["Srinivasan", "Anirudh", ""], ["Sitaram", "Sunayana", ""], ["Choudhury", "Monojit", ""]]}, {"id": "2004.12393", "submitter": "Danqing Wang", "authors": "Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, Xuanjing Huang", "title": "Heterogeneous Graph Neural Networks for Extractive Document\n  Summarization", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a crucial step in extractive document summarization, learning\ncross-sentence relations has been explored by a plethora of approaches. An\nintuitive way is to put them in the graph-based neural network, which has a\nmore complex structure for capturing inter-sentence relationships. In this\npaper, we present a heterogeneous graph-based neural network for extractive\nsummarization (HeterSumGraph), which contains semantic nodes of different\ngranularity levels apart from sentences. These additional nodes act as the\nintermediary between sentences and enrich the cross-sentence relations.\nBesides, our graph structure is flexible in natural extension from a\nsingle-document setting to multi-document via introducing document nodes. To\nour knowledge, we are the first one to introduce different types of nodes into\ngraph-based neural networks for extractive document summarization and perform a\ncomprehensive qualitative analysis to investigate their benefits. The code will\nbe released on Github\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 14:38:11 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Danqing", ""], ["Liu", "Pengfei", ""], ["Zheng", "Yining", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2004.12406", "submitter": "Mengjie Zhao", "authors": "Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, Hinrich Sch\\\"utze", "title": "Masking as an Efficient Alternative to Finetuning for Pretrained\n  Language Models", "comments": "EMNLP 2020; MZ and TL contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an efficient method of utilizing pretrained language models, where\nwe learn selective binary masks for pretrained weights in lieu of modifying\nthem through finetuning. Extensive evaluations of masking BERT and RoBERTa on a\nseries of NLP tasks show that our masking scheme yields performance comparable\nto finetuning, yet has a much smaller memory footprint when several tasks need\nto be inferred simultaneously. Through intrinsic evaluations, we show that\nrepresentations computed by masked language models encode information necessary\nfor solving downstream tasks. Analyzing the loss landscape, we show that\nmasking and finetuning produce models that reside in minima that can be\nconnected by a line segment with nearly constant test accuracy. This confirms\nthat masking can be utilized as an efficient alternative to finetuning.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 15:03:47 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 11:52:08 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhao", "Mengjie", ""], ["Lin", "Tao", ""], ["Mi", "Fei", ""], ["Jaggi", "Martin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2004.12429", "submitter": "Zekang Li", "authors": "Zeyang Lei, Zekang Li, Jinchao Zhang, Fandong Meng, Yang Feng, Yujiu\n  Yang, Cheng Niu, Jie Zhou", "title": "Towards Multimodal Response Generation with Exemplar Augmentation and\n  Curriculum Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, variational auto-encoder (VAE) based approaches have made\nimpressive progress on improving the diversity of generated responses. However,\nthese methods usually suffer the cost of decreased relevance accompanied by\ndiversity improvements. In this paper, we propose a novel multimodal response\ngeneration framework with exemplar augmentation and curriculum optimization to\nenhance relevance and diversity of generated responses. First, unlike existing\nVAE-based models that usually approximate a simple Gaussian posterior\ndistribution, we present a Gaussian mixture posterior distribution (i.e,\nmultimodal) to further boost response diversity, which helps capture complex\nsemantics of responses. Then, to ensure that relevance does not decrease while\ndiversity increases, we fully exploit similar examples (exemplars) retrieved\nfrom the training data into posterior distribution modeling to augment response\nrelevance. Furthermore, to facilitate the convergence of Gaussian mixture prior\nand posterior distributions, we devise a curriculum optimization strategy to\nprogressively train the model under multiple training criteria from easy to\nhard. Experimental results on widely used SwitchBoard and DailyDialog datasets\ndemonstrate that our model achieves significant improvements compared to strong\nbaselines in terms of diversity and relevance.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 16:29:06 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lei", "Zeyang", ""], ["Li", "Zekang", ""], ["Zhang", "Jinchao", ""], ["Meng", "Fandong", ""], ["Feng", "Yang", ""], ["Yang", "Yujiu", ""], ["Niu", "Cheng", ""], ["Zhou", "Jie", ""]]}, {"id": "2004.12440", "submitter": "Qianhui Wu", "authors": "Qianhui Wu, Zijia Lin, B\\\"orje F. Karlsson, Jian-Guang Lou, Biqing\n  Huang", "title": "Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on\n  Unlabeled Data in Target Language", "comments": "This paper is accepted by ACL2020. Code is available at\n  https://github.com/microsoft/vert-papers/tree/master/papers/SingleMulti-TS", "journal-ref": "In ACL, pages 6505-6514, 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better tackle the named entity recognition (NER) problem on languages with\nlittle/no labeled data, cross-lingual NER must effectively leverage knowledge\nlearned from source languages with rich labeled data. Previous works on\ncross-lingual NER are mostly based on label projection with pairwise texts or\ndirect model transfer. However, such methods either are not applicable if the\nlabeled data in the source languages is unavailable, or do not leverage\ninformation contained in unlabeled data in the target language. In this paper,\nwe propose a teacher-student learning method to address such limitations, where\nNER models in the source languages are used as teachers to train a student\nmodel on unlabeled data in the target language. The proposed method works for\nboth single-source and multi-source cross-lingual NER. For the latter, we\nfurther propose a similarity measuring method to better weight the supervision\nfrom different teacher models. Extensive experiments for 3 target languages on\nbenchmark datasets well demonstrate that our method outperforms existing\nstate-of-the-art methods for both single-source and multi-source cross-lingual\nNER.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 17:22:09 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 14:26:43 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Wu", "Qianhui", ""], ["Lin", "Zijia", ""], ["Karlsson", "B\u00f6rje F.", ""], ["Lou", "Jian-Guang", ""], ["Huang", "Biqing", ""]]}, {"id": "2004.12450", "submitter": "Piotr Rybak", "authors": "Piotr Rybak, Alina Wr\\'oblewska", "title": "Semi-Supervised Neural System for Tagging, Parsing and Lematization", "comments": null, "journal-ref": null, "doi": "10.18653/v1/K18-2004", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the ICS PAS system which took part in CoNLL 2018 shared\ntask on Multilingual Parsing from Raw Text to Universal Dependencies. The\nsystem consists of jointly trained tagger, lemmatizer, and dependency parser\nwhich are based on features extracted by a biLSTM network. The system uses both\nfully connected and dilated convolutional neural architectures. The novelty of\nour approach is the use of an additional loss function, which reduces the\nnumber of cycles in the predicted dependency graphs, and the use of\nself-training to increase the system performance. The proposed system, i.e. ICS\nPAS (Warszawa), ranked 3th/4th in the official evaluation obtaining the\nfollowing overall results: 73.02 (LAS), 60.25 (MLAS) and 64.44 (BLEX).\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 18:29:31 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Rybak", "Piotr", ""], ["Wr\u00f3blewska", "Alina", ""]]}, {"id": "2004.12495", "submitter": "Ilshat Gibadullin", "authors": "Ilshat Gibadullin, Aidar Valeev", "title": "Experiments with LVT and FRE for Transformer model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we experiment with Large Vocabulary Trick and Feature-rich\nencoding applied to the Transformer model for Text Summarization. We could not\nachieve better results, than the analogous RNN-based sequence-to-sequence\nmodel, so we tried more models to find out, what improves the results and what\ndeteriorates them.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 22:47:29 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Gibadullin", "Ilshat", ""], ["Valeev", "Aidar", ""]]}, {"id": "2004.12502", "submitter": "Paulo Almeida", "authors": "Paulo Almeida, Manuel Marques-Pita and Joana Gon\\c{c}alves-S\\'a", "title": "PTPARL-D: Annotated Corpus of 44 years of Portuguese Parliament debates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a representative democracy, some decide in the name of the rest, and these\nelected officials are commonly gathered in public assemblies, such as\nparliaments, where they discuss policies, legislate, and vote on fundamental\ninitiatives. A core aspect of such democratic processes are the plenary\ndebates, where important public discussions take place. Many parliaments around\nthe world are increasingly keeping the transcripts of such debates, and other\nparliamentary data, in digital formats accessible to the public, increasing\ntransparency and accountability. Furthermore, some parliaments are bringing old\npaper transcripts to semi-structured digital formats. However, these records\nare often only provided as raw text or even as images, with little to no\nannotation, and inconsistent formats, making them difficult to analyze and\nstudy, reducing both transparency and public reach. Here, we present PTPARL-D,\nan annotated corpus of debates in the Portuguese Parliament, from 1976 to 2019,\ncovering the entire period of Portuguese democracy.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 23:22:41 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Almeida", "Paulo", ""], ["Marques-Pita", "Manuel", ""], ["Gon\u00e7alves-S\u00e1", "Joana", ""]]}, {"id": "2004.12506", "submitter": "Wei-Jen Ko", "authors": "Wei-Jen Ko, Junyi Jessy Li", "title": "Assessing Discourse Relations in Language Generation from GPT-2", "comments": "INLG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in NLP have been attributed to the emergence of large-scale\npre-trained language models. GPT-2, in particular, is suited for generation\ntasks given its left-to-right language modeling objective, yet the linguistic\nquality of its generated text has largely remain unexplored. Our work takes a\nstep in understanding GPT-2's outputs in terms of discourse coherence. We\nperform a comprehensive study on the validity of explicit discourse relations\nin GPT-2's outputs under both organic generation and fine-tuned scenarios.\nResults show GPT-2 does not always generate text containing valid discourse\nrelations; nevertheless, its text is more aligned with human expectation in the\nfine-tuned scenario. We propose a decoupled strategy to mitigate these problems\nand highlight the importance of explicitly modeling discourse information.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 23:29:27 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:17:25 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 05:53:21 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ko", "Wei-Jen", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "2004.12527", "submitter": "Jerry Zikun Chen", "authors": "Jerrod Parker and Jerry Zikun Chen", "title": "Neural Machine Translation with Monte-Carlo Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent algorithms in machine translation have included a value network to\nassist the policy network when deciding which word to output at each step of\nthe translation. The addition of a value network helps the algorithm perform\nbetter on evaluation metrics like the BLEU score. After training the policy and\nvalue networks in a supervised setting, the policy and value networks can be\njointly improved through common actor-critic methods. The main idea of our\nproject is to instead leverage Monte-Carlo Tree Search (MCTS) to search for\ngood output words with guidance from a combined policy and value network\narchitecture in a similar fashion as AlphaZero. This network serves both as a\nlocal and a global look-ahead reference that uses the result of the search to\nimprove itself. Experiments using the IWLST14 German to English translation\ndataset show that our method outperforms the actor-critic methods used in\nrecent machine translation papers.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 01:03:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Parker", "Jerrod", ""], ["Chen", "Jerry Zikun", ""]]}, {"id": "2004.12585", "submitter": "Qile Zhu", "authors": "Qile Zhu, Jianlin Su, Wei Bi, Xiaojiang Liu, Xiyao Ma, Xiaolin Li and\n  Dapeng Wu", "title": "A Batch Normalized Inference Network Keeps the KL Vanishing Away", "comments": "An extension for the original ACL 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoder (VAE) is widely used as a generative model to\napproximate a model's posterior on latent variables by combining the amortized\nvariational inference and deep neural networks. However, when paired with\nstrong autoregressive decoders, VAE often converges to a degenerated local\noptimum known as \"posterior collapse\". Previous approaches consider the\nKullback Leibler divergence (KL) individual for each datapoint. We propose to\nlet the KL follow a distribution across the whole dataset, and analyze that it\nis sufficient to prevent posterior collapse by keeping the expectation of the\nKL's distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a\nsimple but effective approach to set a lower bound of the expectation by\nregularizing the distribution of the approximate posterior's parameters.\nWithout introducing any new model component or modifying the objective, our\napproach can avoid the posterior collapse effectively and efficiently. We\nfurther show that the proposed BN-VAE can be extended to conditional VAE\n(CVAE). Empirically, our approach surpasses strong autoregressive baselines on\nlanguage modeling, text classification and dialogue generation, and rivals more\ncomplex approaches while keeping almost the same training time as VAE.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 05:20:01 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 01:17:18 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zhu", "Qile", ""], ["Su", "Jianlin", ""], ["Bi", "Wei", ""], ["Liu", "Xiaojiang", ""], ["Ma", "Xiyao", ""], ["Li", "Xiaolin", ""], ["Wu", "Dapeng", ""]]}, {"id": "2004.12617", "submitter": "Xin Liu", "authors": "Xin Liu, Jiefu Ou, Yangqiu Song, Xin Jiang", "title": "On the Importance of Word and Sentence Representation Learning in\n  Implicit Discourse Relation Classification", "comments": "Accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit discourse relation classification is one of the most difficult parts\nin shallow discourse parsing as the relation prediction without explicit\nconnectives requires the language understanding at both the text span level and\nthe sentence level. Previous studies mainly focus on the interactions between\ntwo arguments. We argue that a powerful contextualized representation module, a\nbilateral multi-perspective matching module, and a global information fusion\nmodule are all important to implicit discourse analysis. We propose a novel\nmodel to combine these modules together. Extensive experiments show that our\nproposed model outperforms BERT and other state-of-the-art systems on the PDTB\ndataset by around 8% and CoNLL 2016 datasets around 16%. We also analyze the\neffectiveness of different modules in the implicit discourse relation\nclassification task and demonstrate how different levels of representation\nlearning can affect the results.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 07:41:02 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:49:48 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Liu", "Xin", ""], ["Ou", "Jiefu", ""], ["Song", "Yangqiu", ""], ["Jiang", "Xin", ""]]}, {"id": "2004.12651", "submitter": "Sanyuan Chen", "authors": "Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, Xiangzhan\n  Yu", "title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less\n  Forgetting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep pretrained language models have achieved great success in the way of\npretraining first and then fine-tuning. But such a sequential transfer learning\nparadigm often confronts the catastrophic forgetting problem and leads to\nsub-optimal performance. To fine-tune with less forgetting, we propose a recall\nand learn mechanism, which adopts the idea of multi-task learning and jointly\nlearns pretraining tasks and downstream tasks. Specifically, we propose a\nPretraining Simulation mechanism to recall the knowledge from pretraining tasks\nwithout data, and an Objective Shifting mechanism to focus the learning on\ndownstream tasks gradually. Experiments show that our method achieves\nstate-of-the-art performance on the GLUE benchmark. Our method also enables\nBERT-base to achieve better performance than directly fine-tuning of\nBERT-large. Further, we provide the open-source RecAdam optimizer, which\nintegrates the proposed mechanisms into Adam optimizer, to facility the NLP\ncommunity.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:59:57 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chen", "Sanyuan", ""], ["Hou", "Yutai", ""], ["Cui", "Yiming", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""], ["Yu", "Xiangzhan", ""]]}, {"id": "2004.12681", "submitter": "Raymond Hendy Susanto", "authors": "Raymond Hendy Susanto, Shamil Chollampatt, and Liling Tan", "title": "Lexically Constrained Neural Machine Translation with Levenshtein\n  Transformer", "comments": "8 pages, In Proceedings of ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple and effective algorithm for incorporating\nlexical constraints in neural machine translation. Previous work either\nrequired re-training existing models with the lexical constraints or\nincorporating them during beam search decoding with significantly higher\ncomputational overheads. Leveraging the flexibility and speed of a recently\nproposed Levenshtein Transformer model (Gu et al., 2019), our method injects\nterminology constraints at inference time without any impact on decoding speed.\nOur method does not require any modification to the training procedure and can\nbe easily applied at runtime with custom dictionaries. Experiments on\nEnglish-German WMT datasets show that our approach improves an unconstrained\nbaseline and previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 09:59:27 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Susanto", "Raymond Hendy", ""], ["Chollampatt", "Shamil", ""], ["Tan", "Liling", ""]]}, {"id": "2004.12704", "submitter": "Liangming Pan", "authors": "Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, Min-Yen Kan", "title": "Semantic Graphs for Generating Deep Questions", "comments": "ACL 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the problem of Deep Question Generation (DQG), which aims\nto generate complex questions that require reasoning over multiple pieces of\ninformation of the input passage. In order to capture the global structure of\nthe document and facilitate reasoning, we propose a novel framework which first\nconstructs a semantic-level graph for the input document and then encodes the\nsemantic graph by introducing an attention-based GGNN (Att-GGNN). Afterwards,\nwe fuse the document-level and graph-level representations to perform joint\ntraining of content selection and question decoding. On the HotpotQA\ndeep-question centric dataset, our model greatly improves performance over\nquestions requiring reasoning over multiple facts, leading to state-of-the-art\nperformance. The code is publicly available at\nhttps://github.com/WING-NUS/SG-Deep-Question-Generation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 10:52:52 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Pan", "Liangming", ""], ["Xie", "Yuxi", ""], ["Feng", "Yansong", ""], ["Chua", "Tat-Seng", ""], ["Kan", "Min-Yen", ""]]}, {"id": "2004.12726", "submitter": "Kawin Ethayarajh", "authors": "Kawin Ethayarajh and Dorsa Sadigh", "title": "BLEU Neighbors: A Reference-less Approach to Automatic Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation is a bottleneck in the development of natural language generation\n(NLG) models. Automatic metrics such as BLEU rely on references, but for tasks\nsuch as open-ended generation, there are no references to draw upon. Although\nlanguage diversity can be estimated using statistical measures such as\nperplexity, measuring language quality requires human evaluation. However,\nbecause human evaluation at scale is slow and expensive, it is used sparingly;\nit cannot be used to rapidly iterate on NLG models, in the way BLEU is used for\nmachine translation. To this end, we propose BLEU Neighbors, a nearest\nneighbors model for estimating language quality by using the BLEU score as a\nkernel function. On existing datasets for chitchat dialogue and open-ended\nsentence generation, we find that -- on average -- the quality estimation from\na BLEU Neighbors model has a lower mean squared error and higher Spearman\ncorrelation with the ground truth than individual human annotators. Despite its\nsimplicity, BLEU Neighbors even outperforms state-of-the-art models on\nautomatically grading essays, including models that have access to a\ngold-standard reference essay.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 11:51:28 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 04:54:12 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 21:20:37 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ethayarajh", "Kawin", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "2004.12727", "submitter": "Pinelopi Papalampidi", "authors": "Pinelopi Papalampidi, Frank Keller, Lea Frermann, Mirella Lapata", "title": "Screenplay Summarization Using Latent Narrative Structure", "comments": "Accepted to appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most general-purpose extractive summarization models are trained on news\narticles, which are short and present all important information upfront. As a\nresult, such models are biased on position and often perform a smart selection\nof sentences from the beginning of the document. When summarizing long\nnarratives, which have complex structure and present information piecemeal,\nsimple position heuristics are not sufficient. In this paper, we propose to\nexplicitly incorporate the underlying structure of narratives into general\nunsupervised and supervised extractive summarization models. We formalize\nnarrative structure in terms of key narrative events (turning points) and treat\nit as latent in order to summarize screenplays (i.e., extract an optimal\nsequence of scenes). Experimental results on the CSI corpus of TV screenplays,\nwhich we augment with scene-level summarization labels, show that latent\nturning points correlate with important aspects of a CSI episode and improve\nsummarization performance over general extractive algorithms leading to more\ncomplete and diverse summaries.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 11:54:19 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Papalampidi", "Pinelopi", ""], ["Keller", "Frank", ""], ["Frermann", "Lea", ""], ["Lapata", "Mirella", ""]]}, {"id": "2004.12744", "submitter": "Angela Fan", "authors": "Angela Fan, Claire Gardent, Chloe Braud, Antoine Bordes", "title": "Augmenting Transformers with KNN-Based Composite Memory for Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various machine learning tasks can benefit from access to external\ninformation of different modalities, such as text and images. Recent work has\nfocused on learning architectures with large memories capable of storing this\nknowledge. We propose augmenting generative Transformer neural networks with\nKNN-based Information Fetching (KIF) modules. Each KIF module learns a read\noperation to access fixed external knowledge. We apply these modules to\ngenerative dialog modeling, a challenging task where information must be\nflexibly retrieved and incorporated to maintain the topic and flow of\nconversation. We demonstrate the effectiveness of our approach by identifying\nrelevant knowledge required for knowledgeable but engaging dialog from\nWikipedia, images, and human-written dialog utterances, and show that\nleveraging this retrieved information improves model performance, measured by\nautomatic and human evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 12:37:26 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 22:40:34 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Fan", "Angela", ""], ["Gardent", "Claire", ""], ["Braud", "Chloe", ""], ["Bordes", "Antoine", ""]]}, {"id": "2004.12752", "submitter": "Richard Csaky", "authors": "Richard Csaky and Gabor Recski", "title": "The Gutenberg Dialogue Dataset", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large datasets are essential for neural modeling of many NLP tasks. Current\npublicly available open-domain dialogue datasets offer a trade-off between\nquality (e.g., DailyDialog) and size (e.g., Opensubtitles). We narrow this gap\nby building a high-quality dataset of 14.8M utterances in English, and smaller\ndatasets in German, Dutch, Spanish, Portuguese, Italian, and Hungarian. We\nextract and process dialogues from public-domain books made available by\nProject Gutenberg. We describe our dialogue extraction pipeline, analyze the\neffects of the various heuristics used, and present an error analysis of\nextracted dialogues. Finally, we conduct experiments showing that better\nresponse quality can be achieved in zero-shot and finetuning settings by\ntraining on our data than on the larger but much noisier Opensubtitles dataset.\nOur open-source pipeline (https://github.com/ricsinaruto/gutenberg-dialog) can\nbe extended to further languages with little additional effort. Researchers can\nalso build their versions of existing datasets by adjusting various trade-off\nparameters. We also built a web demo for interacting with our models:\nhttps://ricsinaruto.github.io/chatbot.html.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 12:52:20 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 17:54:25 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Csaky", "Richard", ""], ["Recski", "Gabor", ""]]}, {"id": "2004.12764", "submitter": "Mattia Samory", "authors": "Mattia Samory, Indira Sen, Julian Kohne, Fabian Floeck, Claudia Wagner", "title": "\"Call me sexist, but...\": Revisiting Sexism Detection Using\n  Psychological Scales and Adversarial Samples", "comments": "Indira Sen and Julian Kohne contributed equally to this work", "journal-ref": "Proceedings of the 15th International AAAI Conference on Web and\n  Social Media (ICWSM), 2021", "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has focused on automated methods to effectively detect sexism\nonline. Although overt sexism seems easy to spot, its subtle forms and manifold\nexpressions are not. In this paper, we outline the different dimensions of\nsexism by grounding them in their implementation in psychological scales. From\nthe scales, we derive a codebook for sexism in social media, which we use to\nannotate existing and novel datasets, surfacing their limitations in breadth\nand validity with respect to the construct of sexism. Next, we leverage the\nannotated datasets to generate adversarial examples, and test the reliability\nof sexism detection methods. Results indicate that current machine learning\nmodels pick up on a very narrow set of linguistic markers of sexism and do not\ngeneralize well to out-of-domain examples. Yet, including diverse data and\nadversarial examples at training time results in models that generalize better\nand that are more robust to artifacts of data collection. By providing a\nscale-based codebook and insights regarding the shortcomings of the\nstate-of-the-art, we hope to contribute to the development of better and\nbroader models for sexism detection, including reflections on theory-driven\napproaches to data collection.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:07:46 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 10:39:03 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Samory", "Mattia", ""], ["Sen", "Indira", ""], ["Kohne", "Julian", ""], ["Floeck", "Fabian", ""], ["Wagner", "Claudia", ""]]}, {"id": "2004.12765", "submitter": "Issa Annamoradnejad", "authors": "Issa Annamoradnejad and Gohar Zoghi", "title": "ColBERT: Using BERT Sentence Embedding for Humor Detection", "comments": "6 pages, 3 tables, 1 figure; Under review with THMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic humor detection has interesting use cases in modern technologies,\nsuch as chatbots and virtual assistants. In this paper, we propose a novel\napproach for detecting humor in short texts based on the general linguistic\nstructure of humor. Our proposed method uses BERT to generate embeddings for\nsentences of a given text and uses these embeddings as inputs of parallel lines\nof hidden layers in a neural network. These lines are finally concatenated to\npredict the target value. For evaluation purposes, we created a new dataset for\nhumor detection consisting of 200k formal short texts (100k positive and 100k\nnegative). Experimental results show that our proposed method can determine\nhumor in short texts with accuracy and an F1-score of 98.2 percent. Our 8-layer\nmodel with 110M parameters outperforms the baseline models with a large margin,\nshowing the importance of utilizing linguistic structure of texts in machine\nlearning models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:10:11 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 16:11:01 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 13:01:46 GMT"}, {"version": "v4", "created": "Tue, 23 Mar 2021 15:19:00 GMT"}, {"version": "v5", "created": "Mon, 5 Apr 2021 09:29:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Annamoradnejad", "Issa", ""], ["Zoghi", "Gohar", ""]]}, {"id": "2004.12817", "submitter": "Kaitao Song", "authors": "Kaitao Song, Hao Sun, Xu Tan, Tao Qin, Jianfeng Lu, Hongzhi Liu and\n  Tie-Yan Liu", "title": "LightPAFF: A Two-Stage Distillation Framework for Pre-training and\n  Fine-tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While pre-training and fine-tuning, e.g., BERT~\\citep{devlin2018bert},\nGPT-2~\\citep{radford2019language}, have achieved great success in language\nunderstanding and generation tasks, the pre-trained models are usually too big\nfor online deployment in terms of both memory cost and inference speed, which\nhinders them from practical online usage. In this paper, we propose LightPAFF,\na Lightweight Pre-training And Fine-tuning Framework that leverages two-stage\nknowledge distillation to transfer knowledge from a big teacher model to a\nlightweight student model in both pre-training and fine-tuning stages. In this\nway the lightweight model can achieve similar accuracy as the big teacher\nmodel, but with much fewer parameters and thus faster online inference speed.\nLightPAFF can support different pre-training methods (such as BERT, GPT-2 and\nMASS~\\citep{song2019mass}) and be applied to many downstream tasks. Experiments\non three language understanding tasks, three language modeling tasks and three\nsequence to sequence generation tasks demonstrate that while achieving similar\naccuracy with the big BERT, GPT-2 and MASS models, LightPAFF reduces the model\nsize by nearly 5x and improves online inference speed by 5x-7x.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:00:09 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Song", "Kaitao", ""], ["Sun", "Hao", ""], ["Tan", "Xu", ""], ["Qin", "Tao", ""], ["Lu", "Jianfeng", ""], ["Liu", "Hongzhi", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2004.12832", "submitter": "Omar Khattab", "authors": "Omar Khattab and Matei Zaharia", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late\n  Interaction over BERT", "comments": "Accepted at SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in Natural Language Understanding (NLU) is driving fast-paced\nadvances in Information Retrieval (IR), largely owed to fine-tuning deep\nlanguage models (LMs) for document ranking. While remarkably effective, the\nranking models based on these LMs increase computational cost by orders of\nmagnitude over prior approaches, particularly as they must feed each\nquery-document pair through a massive neural network to compute a single\nrelevance score. To tackle this, we present ColBERT, a novel ranking model that\nadapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT\nintroduces a late interaction architecture that independently encodes the query\nand the document using BERT and then employs a cheap yet powerful interaction\nstep that models their fine-grained similarity. By delaying and yet retaining\nthis fine-granular interaction, ColBERT can leverage the expressiveness of deep\nLMs while simultaneously gaining the ability to pre-compute document\nrepresentations offline, considerably speeding up query processing. Beyond\nreducing the cost of re-ranking the documents retrieved by a traditional model,\nColBERT's pruning-friendly interaction mechanism enables leveraging\nvector-similarity indexes for end-to-end retrieval directly from a large\ndocument collection. We extensively evaluate ColBERT using two recent passage\nsearch datasets. Results show that ColBERT's effectiveness is competitive with\nexisting BERT-based models (and outperforms every non-BERT baseline), while\nexecuting two orders-of-magnitude faster and requiring four orders-of-magnitude\nfewer FLOPs per query.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:21:03 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 05:28:21 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Khattab", "Omar", ""], ["Zaharia", "Matei", ""]]}, {"id": "2004.12835", "submitter": "Ivan P Yamshchikov", "authors": "Igor Samenko, Alexey Tikhonov, Ivan P. Yamshchikov", "title": "Synonyms and Antonyms: Embedded Conflict", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since modern word embeddings are motivated by a distributional hypothesis and\nare, therefore, based on local co-occurrences of words, it is only to be\nexpected that synonyms and antonyms can have very similar embeddings. Contrary\nto this widespread assumption, this paper shows that modern embeddings contain\ninformation that distinguishes synonyms and antonyms despite small cosine\nsimilarities between corresponding vectors. This information is encoded in the\ngeometry of the embeddings and could be extracted with a manifold learning\nprocedure or {\\em contrasting map}. Such a map is trained on a small labeled\nsubset of the data and can produce new empeddings that explicitly highlight\nspecific semantic attributes of the word. The new embeddings produced by the\nmap are shown to improve the performance on downstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:33:37 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Samenko", "Igor", ""], ["Tikhonov", "Alexey", ""], ["Yamshchikov", "Ivan P.", ""]]}, {"id": "2004.12864", "submitter": "Christopher Hidey", "authors": "Christopher Hidey and Tuhin Chakrabarty and Tariq Alhindi and\n  Siddharth Varia and Kriste Krstovski and Mona Diab and Smaranda Muresan", "title": "DeSePtion: Dual Sequence Prediction and Adversarial Examples for\n  Improved Fact-Checking", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increased focus on misinformation has spurred development of data and\nsystems for detecting the veracity of a claim as well as retrieving\nauthoritative evidence. The Fact Extraction and VERification (FEVER) dataset\nprovides such a resource for evaluating end-to-end fact-checking, requiring\nretrieval of evidence from Wikipedia to validate a veracity prediction. We show\nthat current systems for FEVER are vulnerable to three categories of realistic\nchallenges for fact-checking -- multiple propositions, temporal reasoning, and\nambiguity and lexical variation -- and introduce a resource with these types of\nclaims. Then we present a system designed to be resilient to these \"attacks\"\nusing multiple pointer networks for document selection and jointly modeling a\nsequence of evidence sentences and veracity relation predictions. We find that\nin handling these attacks we obtain state-of-the-art results on FEVER, largely\ndue to improved evidence retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:18:49 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Hidey", "Christopher", ""], ["Chakrabarty", "Tuhin", ""], ["Alhindi", "Tariq", ""], ["Varia", "Siddharth", ""], ["Krstovski", "Kriste", ""], ["Diab", "Mona", ""], ["Muresan", "Smaranda", ""]]}, {"id": "2004.12894", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov", "title": "Intelligent Translation Memory Matching and Retrieval with Sentence\n  Encoders", "comments": "Accepted to EAMT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matching and retrieving previously translated segments from a Translation\nMemory is the key functionality in Translation Memories systems. However this\nmatching and retrieving process is still limited to algorithms based on edit\ndistance which we have identified as a major drawback in Translation Memories\nsystems. In this paper we introduce sentence encoders to improve the matching\nand retrieving process in Translation Memories systems - an effective and\nefficient solution to replace edit distance based algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:54:29 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Orasan", "Constantin", ""], ["Mitkov", "Ruslan", ""]]}, {"id": "2004.12905", "submitter": "James Mullenbach", "authors": "James Mullenbach, Jordan Swartz, T. Greg McKelvey, Hui Dai, David\n  Sontag", "title": "Knowledge Base Completion for Constructing Problem-Oriented Medical\n  Records", "comments": "MLHC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both electronic health records and personal health records are typically\norganized by data type, with medical problems, medications, procedures, and\nlaboratory results chronologically sorted in separate areas of the chart. As a\nresult, it can be difficult to find all of the relevant information for\nanswering a clinical question about a given medical problem. A promising\nalternative is to instead organize by problems, with related medications,\nprocedures, and other pertinent information all grouped together. A recent\neffort by Buchanan (2017) manually defined, through expert consensus, 11\nmedical problems and the relevant labs and medications for each. We show how to\nuse machine learning on electronic health records to instead automatically\nconstruct these problem-based groupings of relevant medications, procedures,\nand laboratory tests. We formulate the learning task as one of knowledge base\ncompletion, and annotate a dataset that expands the set of problems from 11 to\n32. We develop a model architecture that exploits both pre-trained concept\nembeddings and usage data relating the concepts contained in a longitudinal\ndataset from a large health system. We evaluate our algorithms' ability to\nsuggest relevant medications, procedures, and lab tests, and find that the\napproach provides feasible suggestions even for problems that are hidden during\ntraining. The dataset, along with code to reproduce our results, is available\nat https://github.com/asappresearch/kbc-pomr.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 16:05:23 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 13:22:31 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Mullenbach", "James", ""], ["Swartz", "Jordan", ""], ["McKelvey", "T. Greg", ""], ["Dai", "Hui", ""], ["Sontag", "David", ""]]}, {"id": "2004.12934", "submitter": "Xiang Kong", "authors": "Xiang Kong, Varun Gangal, Eduard Hovy", "title": "SCDE: Sentence Cloze Dataset with High Quality Distractors From\n  Examinations", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SCDE, a dataset to evaluate the performance of computational\nmodels through sentence prediction. SCDE is a human-created sentence cloze\ndataset, collected from public school English examinations. Our task requires a\nmodel to fill up multiple blanks in a passage from a shared candidate set with\ndistractors designed by English teachers. Experimental results demonstrate that\nthis task requires the use of non-local, discourse-level context beyond the\nimmediate sentence neighborhood. The blanks require joint solving and\nsignificantly impair each other's context. Furthermore, through ablations, we\nshow that the distractors are of high quality and make the task more\nchallenging. Our experiments show that there is a significant performance gap\nbetween advanced models (72%) and humans (87%), encouraging future models to\nbridge this gap.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 16:48:54 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kong", "Xiang", ""], ["Gangal", "Varun", ""], ["Hovy", "Eduard", ""]]}, {"id": "2004.12935", "submitter": "Marco Basaldella", "authors": "Costanza Conforti, Stephanie Hirmer, David Morgan, Marco Basaldella,\n  Yau Ben Or", "title": "Natural language processing for achieving sustainable development: the\n  case of neural labelling to enhance community profiling", "comments": "18 pages, 9 figures. Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an increasing interest in the application of\nArtificial Intelligence - and especially Machine Learning - to the field of\nSustainable Development (SD). However, until now, NLP has not been applied in\nthis context. In this research paper, we show the high potential of NLP\napplications to enhance the sustainability of projects. In particular, we focus\non the case of community profiling in developing countries, where, in contrast\nto the developed world, a notable data gap exists. In this context, NLP could\nhelp to address the cost and time barrier of structuring qualitative data that\nprohibits its widespread use and associated benefits. We propose the new task\nof Automatic UPV classification, which is an extreme multi-class multi-label\nclassification problem. We release Stories2Insights, an expert-annotated\ndataset, provide a detailed corpus analysis, and implement a number of strong\nneural baselines to address the task. Experimental results show that the\nproblem is challenging, and leave plenty of room for future research at the\nintersection of NLP and SD.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 16:51:21 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 18:28:01 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Conforti", "Costanza", ""], ["Hirmer", "Stephanie", ""], ["Morgan", "David", ""], ["Basaldella", "Marco", ""], ["Or", "Yau Ben", ""]]}, {"id": "2004.12993", "submitter": "Ji Xin", "authors": "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pre-trained language models such as BERT have brought significant\nimprovements to NLP applications. However, they are also notorious for being\nslow in inference, which makes them difficult to deploy in real-time\napplications. We propose a simple but effective method, DeeBERT, to accelerate\nBERT inference. Our approach allows samples to exit earlier without passing\nthrough the entire model. Experiments show that DeeBERT is able to save up to\n~40% inference time with minimal degradation in model quality. Further analyses\nshow different behaviors in the BERT transformer layers and also reveal their\nredundancy. Our work provides new ideas to efficiently apply deep\ntransformer-based models to downstream tasks. Code is available at\nhttps://github.com/castorini/DeeBERT.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 17:58:05 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Xin", "Ji", ""], ["Tang", "Raphael", ""], ["Lee", "Jaejun", ""], ["Yu", "Yaoliang", ""], ["Lin", "Jimmy", ""]]}, {"id": "2004.13003", "submitter": "Tian Shi", "authors": "Tian Shi, Xuchao Zhang, Ping Wang, Chandan K. Reddy", "title": "Corpus-level and Concept-based Explanations for Interpretable Document\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using attention weights to identify information that is important for models'\ndecision-making is a popular approach to interpret attention-based neural\nnetworks. This is commonly realized in practice through the generation of a\nheat-map for every single document based on attention weights. However, this\ninterpretation method is fragile, and easy to find contradictory examples. In\nthis paper, we propose a corpus-level explanation approach, which aims to\ncapture causal relationships between keywords and model predictions via\nlearning the importance of keywords for predicted labels across a training\ncorpus based on attention weights. Based on this idea, we further propose a\nconcept-based explanation method that can automatically learn higher-level\nconcepts and their importance to model prediction tasks. Our concept-based\nexplanation method is built upon a novel Abstraction-Aggregation Network, which\ncan automatically cluster important keywords during an end-to-end training\nprocess. We apply these methods to the document classification task and show\nthat they are powerful in extracting semantically meaningful keywords and\nconcepts. Our consistency analysis results based on an attention-based Na\\\"ive\nBayes classifier also demonstrate these keywords and concepts are important for\nmodel predictions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:54:17 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 21:48:26 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 04:50:32 GMT"}, {"version": "v4", "created": "Mon, 31 May 2021 03:22:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shi", "Tian", ""], ["Zhang", "Xuchao", ""], ["Wang", "Ping", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "2004.13005", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Amro El-Jaroudi, William Hartmann, Damianos Karakos,\n  Lingjun Zhao", "title": "Cross-lingual Information Retrieval with BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple neural language models have been developed recently, e.g., BERT and\nXLNet, and achieved impressive results in various NLP tasks including sentence\nclassification, question answering and document ranking. In this paper, we\nexplore the use of the popular bidirectional language model, BERT, to model and\nlearn the relevance between English queries and foreign-language documents in\nthe task of cross-lingual information retrieval. A deep relevance matching\nmodel based on BERT is introduced and trained by finetuning a pretrained\nmultilingual BERT model with weak supervision, using home-made CLIR training\ndata derived from parallel corpora. Experimental results of the retrieval of\nLithuanian documents against short English queries show that our model is\neffective and outperforms the competitive baseline approaches.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 23:32:13 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Jiang", "Zhuolin", ""], ["El-Jaroudi", "Amro", ""], ["Hartmann", "William", ""], ["Karakos", "Damianos", ""], ["Zhao", "Lingjun", ""]]}, {"id": "2004.13012", "submitter": "Dara Bahri", "authors": "Dara Bahri, Yi Tay, Che Zheng, Donald Metzler, Andrew Tomkins", "title": "Choppy: Cut Transformer For Ranked List Truncation", "comments": "SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work in information retrieval has traditionally focused on ranking and\nrelevance: given a query, return some number of results ordered by relevance to\nthe user. However, the problem of determining how many results to return, i.e.\nhow to optimally truncate the ranked result list, has received less attention\ndespite being of critical importance in a range of applications. Such\ntruncation is a balancing act between the overall relevance, or usefulness of\nthe results, with the user cost of processing more results. In this work, we\npropose Choppy, an assumption-free model based on the widely successful\nTransformer architecture, to the ranked list truncation problem. Needing\nnothing more than the relevance scores of the results, the model uses a\npowerful multi-head attention mechanism to directly optimize any user-defined\nIR metric. We show Choppy improves upon recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 00:52:49 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Bahri", "Dara", ""], ["Tay", "Yi", ""], ["Zheng", "Che", ""], ["Metzler", "Donald", ""], ["Tomkins", "Andrew", ""]]}, {"id": "2004.13073", "submitter": "Matteo Stefanini", "authors": "Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara", "title": "A Novel Attention-based Aggregation Function to Combine Vision and\n  Language", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint understanding of vision and language has been recently gaining a\nlot of attention in both the Computer Vision and Natural Language Processing\ncommunities, with the emergence of tasks such as image captioning, image-text\nmatching, and visual question answering. As both images and text can be encoded\nas sets or sequences of elements -- like regions and words -- proper reduction\nfunctions are needed to transform a set of encoded elements into a single\nresponse, like a classification or similarity score. In this paper, we propose\na novel fully-attentive reduction method for vision and language. Specifically,\nour approach computes a set of scores for each element of each modality\nemploying a novel variant of cross-attention, and performs a learnable and\ncross-modal reduction, which can be used for both classification and ranking.\nWe test our approach on image-text matching and visual question answering,\nbuilding fair comparisons with other reduction choices, on both COCO and VQA\n2.0 datasets. Experimentally, we demonstrate that our approach leads to a\nperformance increase on both tasks. Further, we conduct ablation studies to\nvalidate the role of each component of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:09:46 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 12:22:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Stefanini", "Matteo", ""], ["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2004.13078", "submitter": "Iyiola E. Olatunji", "authors": "Iyiola E. Olatunji, Xin Li, Wai Lam", "title": "Context-aware Helpfulness Prediction for Online Product Reviews", "comments": "Published as a proceeding paper in AIRS 2019", "journal-ref": null, "doi": "10.1007/978-3-030-42835-8_6", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling and prediction of review helpfulness has become more predominant due\nto proliferation of e-commerce websites and online shops. Since the\nfunctionality of a product cannot be tested before buying, people often rely on\ndifferent kinds of user reviews to decide whether or not to buy a product.\nHowever, quality reviews might be buried deep in the heap of a large amount of\nreviews. Therefore, recommending reviews to customers based on the review\nquality is of the essence. Since there is no direct indication of review\nquality, most reviews use the information that ''X out of Y'' users found the\nreview helpful for obtaining the review quality. However, this approach\nundermines helpfulness prediction because not all reviews have statistically\nabundant votes. In this paper, we propose a neural deep learning model that\npredicts the helpfulness score of a review. This model is based on\nconvolutional neural network (CNN) and a context-aware encoding mechanism which\ncan directly capture relationships between words irrespective of their distance\nin a long sequence. We validated our model on human annotated dataset and the\nresult shows that our model significantly outperforms existing models for\nhelpfulness prediction.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:19:26 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Olatunji", "Iyiola E.", ""], ["Li", "Xin", ""], ["Lam", "Wai", ""]]}, {"id": "2004.13117", "submitter": "Magdalena Kaiser", "authors": "Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum", "title": "Conversational Question Answering over Passages by Leveraging Word\n  Proximity Networks", "comments": "SIGIR 2020 Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) over text passages is a problem of long-standing\ninterest in information retrieval. Recently, the conversational setting has\nattracted attention, where a user asks a sequence of questions to satisfy her\ninformation needs around a topic. While this setup is a natural one and similar\nto humans conversing with each other, it introduces two key research\nchallenges: understanding the context left implicit by the user in follow-up\nquestions, and dealing with ad hoc question formulations. In this work, we\ndemonstrate CROWN (Conversational passage ranking by Reasoning Over Word\nNetworks): an unsupervised yet effective system for conversational QA with\npassage responses, that supports several modes of context propagation over\nmultiple turns. To this end, CROWN first builds a word proximity network (WPN)\nfrom large corpora to store statistically significant term co-occurrences. At\nanswering time, passages are ranked by a combination of their similarity to the\nquestion, and coherence of query terms within: these factors are measured by\nreading off node and edge weights from the WPN. CROWN provides an interface\nthat is both intuitive for end-users, and insightful for experts for\nreconfiguration to individual setups. CROWN was evaluated on TREC CAsT data,\nwhere it achieved above-median performance in a pool of neural methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 19:30:47 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 22:57:08 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 15:21:00 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kaiser", "Magdalena", ""], ["Roy", "Rishiraj Saha", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2004.13138", "submitter": "Jinghui Lu", "authors": "Jinghui Lu and Brian MacNamee", "title": "Investigating the Effectiveness of Representations Based on Pretrained\n  Transformer-based Language Models in Active Learning for Labelling Text\n  Datasets", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.03505", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning has been shown to be an effective way to alleviate some of\nthe effort required in utilising large collections of unlabelled data for\nmachine learning tasks without needing to fully label them. The representation\nmechanism used to represent text documents when performing active learning,\nhowever, has a significant influence on how effective the process will be.\nWhile simple vector representations such as bag-of-words and embedding-based\nrepresentations based on techniques such as word2vec have been shown to be an\neffective way to represent documents during active learning, the emergence of\nrepresentation mechanisms based on the pre-trained transformer-based neural\nnetwork models popular in natural language processing research (e.g. BERT)\noffer a promising, and as yet not fully explored, alternative. This paper\ndescribes a comprehensive evaluation of the effectiveness of representations\nbased on pre-trained transformer-based language models for active learning.\nThis evaluation shows that transformer-based models, especially BERT-like\nmodels, that have not yet been widely used in active learning, achieve a\nsignificant improvement over more commonly used vector representations like\nbag-of-words or other classical word embeddings like word2vec. This paper also\ninvestigates the effectiveness of representations based on variants of BERT\nsuch as Roberta, Albert as well as comparing the effectiveness of the [CLS]\ntoken representation and the aggregated representation that can be generated\nusing BERT-like models. Finally, we propose an approach Adaptive Tuning Active\nLearning. Our experiments show that the limited label information acquired in\nactive learning can not only be used for training a classifier but can also\nadaptively improve the embeddings generated by the BERT-like language models as\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 02:37:44 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Lu", "Jinghui", ""], ["MacNamee", "Brian", ""]]}, {"id": "2004.13150", "submitter": "Zhe Zhang", "authors": "Zhe Zhang, Chung-Wei Hang, Munindar P. Singh", "title": "Octa: Omissions and Conflicts in Target-Aspect Sentiment Analysis", "comments": "Accepted by Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiments in opinionated text are often determined by both aspects and\ntarget words (or targets). We observe that targets and aspects interrelate in\nsubtle ways, often yielding conflicting sentiments. Thus, a naive aggregation\nof sentiments from aspects and targets treated separately, as in existing\nsentiment analysis models, impairs performance.\n  We propose Octa, an approach that jointly considers aspects and targets when\ninferring sentiments. To capture and quantify relationships between targets and\ncontext words, Octa uses a selective self-attention mechanism that handles\nimplicit or missing targets. Specifically, Octa involves two layers of\nattention mechanisms for, respectively, selective attention between targets and\ncontext words and attention over words based on aspects. On benchmark datasets,\nOcta outperforms leading models by a large margin, yielding (absolute) gains in\naccuracy of 1.6% to 4.3%.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 20:11:50 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 20:37:33 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhang", "Zhe", ""], ["Hang", "Chung-Wei", ""], ["Singh", "Munindar P.", ""]]}, {"id": "2004.13161", "submitter": "G\\\"ozde G\\\"ul \\c{S}ahin", "authors": "G\\\"ozde G\\\"ul \\c{S}ahin, Yova Kementchedjhieva, Phillip Rust, Iryna\n  Gurevych", "title": "PuzzLing Machines: A Challenge on Learning From Small Data", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural models have repeatedly proved excellent at memorizing surface\npatterns from large datasets for various ML and NLP benchmarks. They struggle\nto achieve human-like thinking, however, because they lack the skill of\niterative reasoning upon knowledge. To expose this problem in a new light, we\nintroduce a challenge on learning from small data, PuzzLing Machines, which\nconsists of Rosetta Stone puzzles from Linguistic Olympiads for high school\nstudents. These puzzles are carefully designed to contain only the minimal\namount of parallel text necessary to deduce the form of unseen expressions.\nSolving them does not require external information (e.g., knowledge bases,\nvisual signals) or linguistic expertise, but meta-linguistic awareness and\ndeductive skills. Our challenge contains around 100 puzzles covering a wide\nrange of linguistic phenomena from 81 languages. We show that both simple\nstatistical algorithms and state-of-the-art deep neural models perform\ninadequately on this challenge, as expected. We hope that this benchmark,\navailable at https://ukplab.github.io/PuzzLing-Machines/, inspires further\nefforts towards a new paradigm in NLP---one that is grounded in human-like\nreasoning and understanding.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 20:34:26 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["\u015eahin", "G\u00f6zde G\u00fcl", ""], ["Kementchedjhieva", "Yova", ""], ["Rust", "Phillip", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2004.13169", "submitter": "Baigong Zheng", "authors": "Baigong Zheng, Kaibo Liu, Renjie Zheng, Mingbo Ma, Hairong Liu, Liang\n  Huang", "title": "Simultaneous Translation Policies: From Fixed to Adaptive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive policies are better than fixed policies for simultaneous\ntranslation, since they can flexibly balance the tradeoff between translation\nquality and latency based on the current context information. But previous\nmethods on obtaining adaptive policies either rely on complicated training\nprocess, or underperform simple fixed policies. We design an algorithm to\nachieve adaptive policies via a simple heuristic composition of a set of fixed\npolicies. Experiments on Chinese -> English and German -> English show that our\nadaptive policies can outperform fixed ones by up to 4 BLEU points for the same\nlatency, and more surprisingly, it even surpasses the BLEU score of\nfull-sentence translation in the greedy mode (and very close to beam mode), but\nwith much lower latency.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 20:56:20 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 07:18:51 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zheng", "Baigong", ""], ["Liu", "Kaibo", ""], ["Zheng", "Renjie", ""], ["Ma", "Mingbo", ""], ["Liu", "Hairong", ""], ["Huang", "Liang", ""]]}, {"id": "2004.13195", "submitter": "Naomi Saphra", "authors": "Naomi Saphra and Adam Lopez", "title": "Word Interdependence Exposes How LSTMs Compose Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in NLP shows that LSTM language models capture compositional\nstructure in language data. For a closer look at how these representations are\ncomposed hierarchically, we present a novel measure of interdependence between\nword meanings in an LSTM, based on their interactions at the internal gates. To\nexplore how compositional representations arise over training, we conduct\nsimple experiments on synthetic data, which illustrate our measure by showing\nhow high interdependence can hurt generalization. These synthetic experiments\nalso illustrate a specific hypothesis about how hierarchical structures are\ndiscovered over the course of training: that parent constituents rely on\neffective representations of their children, rather than on learning long-range\nrelations independently. We further support this measure with experiments on\nEnglish language data, where interdependence is higher for more closely\nsyntactically linked word pairs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 21:48:08 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Saphra", "Naomi", ""], ["Lopez", "Adam", ""]]}, {"id": "2004.13203", "submitter": "Shruti Rijhwani", "authors": "Graham Neubig, Shruti Rijhwani, Alexis Palmer, Jordan MacKenzie,\n  Hilaria Cruz, Xinjian Li, Matthew Lee, Aditi Chaudhary, Luke Gessler, Steven\n  Abney, Shirley Anugrah Hayati, Antonios Anastasopoulos, Olga Zamaraeva, Emily\n  Prud'hommeaux, Jennette Child, Sara Child, Rebecca Knowles, Sarah Moeller,\n  Jeffrey Micher, Yiyuan Li, Sydney Zink, Mengzhou Xia, Roshan S Sharma and\n  Patrick Littell", "title": "A Summary of the First Workshop on Language Technology for Language\n  Documentation and Revitalization", "comments": "Accepted at SLTU-CCURL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in natural language processing and other language\ntechnology, the application of such technology to language documentation and\nconservation has been limited. In August 2019, a workshop was held at Carnegie\nMellon University in Pittsburgh to attempt to bring together language community\nmembers, documentary linguists, and technologists to discuss how to bridge this\ngap and create prototypes of novel and practical language revitalization\ntechnologies. This paper reports the results of this workshop, including issues\ndiscussed, and various conceived and implemented technologies for nine\nlanguages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw'ida, Kwak'wala,\nOjibwe, San Juan Quiahije Chatino, and Seneca.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 22:55:55 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Neubig", "Graham", ""], ["Rijhwani", "Shruti", ""], ["Palmer", "Alexis", ""], ["MacKenzie", "Jordan", ""], ["Cruz", "Hilaria", ""], ["Li", "Xinjian", ""], ["Lee", "Matthew", ""], ["Chaudhary", "Aditi", ""], ["Gessler", "Luke", ""], ["Abney", "Steven", ""], ["Hayati", "Shirley Anugrah", ""], ["Anastasopoulos", "Antonios", ""], ["Zamaraeva", "Olga", ""], ["Prud'hommeaux", "Emily", ""], ["Child", "Jennette", ""], ["Child", "Sara", ""], ["Knowles", "Rebecca", ""], ["Moeller", "Sarah", ""], ["Micher", "Jeffrey", ""], ["Li", "Yiyuan", ""], ["Zink", "Sydney", ""], ["Xia", "Mengzhou", ""], ["Sharma", "Roshan S", ""], ["Littell", "Patrick", ""]]}, {"id": "2004.13221", "submitter": "Kyubyong Park", "authors": "Kyubyong Park", "title": "KoParadigm: A Korean Conjugation Paradigm Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Korean is a morphologically rich language. Korean verbs change their forms in\na fickle manner depending on tense, mood, speech level, meaning, etc.\nTherefore, it is challenging to construct comprehensive conjugation paradigms\nof Korean verbs. In this paper we introduce a Korean (verb) conjugation\nparadigm generator, dubbed KoParadigm. To the best of our knowledge, it is the\nfirst Korean conjugation module that covers all contemporary Korean verbs and\nendings. KoParadigm is not only linguistically well established, but also\ncomputationally simple and efficient. We share it via PyPi.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 00:28:09 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Park", "Kyubyong", ""]]}, {"id": "2004.13230", "submitter": "Marjan Albooyeh", "authors": "Marjan Albooyeh, Rishab Goel, Seyed Mehran Kazemi", "title": "Out-of-Sample Representation Learning for Multi-Relational Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important problems can be formulated as reasoning in knowledge graphs.\nRepresentation learning has proved extremely effective for transductive\nreasoning, in which one needs to make new predictions for already observed\nentities. This is true for both attributed graphs(where each entity has an\ninitial feature vector) and non-attributed graphs (where the only initial\ninformation derives from known relations with other entities). For\nout-of-sample reasoning, where one needs to make predictions for entities that\nwere unseen at training time, much prior work considers attributed graph.\nHowever, this problem is surprisingly under-explored for non-attributed graphs.\nIn this paper, we study the out-of-sample representation learning problem for\nnon-attributed knowledge graphs, create benchmark datasets for this task,\ndevelop several models and baselines, and provide empirical analyses and\ncomparisons of the proposed models and baselines.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 00:53:01 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 16:22:50 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Albooyeh", "Marjan", ""], ["Goel", "Rishab", ""], ["Kazemi", "Seyed Mehran", ""]]}, {"id": "2004.13240", "submitter": "M Saiful Bari", "authors": "M Saiful Bari, Tasnim Mohiuddin, Shafiq Joty", "title": "UXLA: A Robust Unsupervised Data Augmentation Framework for\n  Zero-Resource Cross-Lingual NLP", "comments": "ACL-2021 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has yielded state-of-the-art (SoTA) results in many\nsupervised NLP tasks. However, annotated data for every target task in every\ntarget language is rare, especially for low-resource languages. We propose\nUXLA, a novel unsupervised data augmentation framework for zero-resource\ntransfer learning scenarios. In particular, UXLA aims to solve cross-lingual\nadaptation problems from a source language task distribution to an unknown\ntarget language task distribution, assuming no training label in the target\nlanguage. At its core, UXLA performs simultaneous self-training with data\naugmentation and unsupervised sample selection. To show its effectiveness, we\nconduct extensive experiments on three diverse zero-resource cross-lingual\ntransfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the\nbaselines by a good margin. With an in-depth framework dissection, we\ndemonstrate the cumulative contributions of different components to its\nsuccess.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 01:47:37 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 19:40:50 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 05:38:12 GMT"}, {"version": "v4", "created": "Sat, 26 Jun 2021 04:16:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bari", "M Saiful", ""], ["Mohiuddin", "Tasnim", ""], ["Joty", "Shafiq", ""]]}, {"id": "2004.13245", "submitter": "Dai Tran", "authors": "Dai Hoang Tran, Quan Z. Sheng, Wei Emma Zhang, Salma Abdalla Hamad,\n  Munazza Zaib, Nguyen H. Tran, Lina Yao, Nguyen Lu Dang Khoa", "title": "Deep Conversational Recommender Systems: A New Frontier for\n  Goal-Oriented Dialogue Systems", "comments": "7 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the emerging topics of recommender systems that take\nadvantage of natural language processing techniques have attracted much\nattention, and one of their applications is the Conversational Recommender\nSystem (CRS). Unlike traditional recommender systems with content-based and\ncollaborative filtering approaches, CRS learns and models user's preferences\nthrough interactive dialogue conversations. In this work, we provide a\nsummarization of the recent evolution of CRS, where deep learning approaches\nare applied to CRS and have produced fruitful results. We first analyze the\nresearch problems and present key challenges in the development of Deep\nConversational Recommender Systems (DCRS), then present the current state of\nthe field taken from the most recent researches, including the most common deep\nlearning models that benefit DCRS. Finally, we discuss future directions for\nthis vibrant area.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 02:20:42 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Tran", "Dai Hoang", ""], ["Sheng", "Quan Z.", ""], ["Zhang", "Wei Emma", ""], ["Hamad", "Salma Abdalla", ""], ["Zaib", "Munazza", ""], ["Tran", "Nguyen H.", ""], ["Yao", "Lina", ""], ["Khoa", "Nguyen Lu Dang", ""]]}, {"id": "2004.13248", "submitter": "Tuhin Chakrabarty Mr", "authors": "Tuhin Chakrabarty, and Debanjan Ghosh, and Smaranda Muresan, and\n  Nanyun Peng", "title": "$R^3$: Reverse, Retrieve, and Rank for Sarcasm Generation with\n  Commonsense Knowledge", "comments": "Accepted at the 2020 Annual Conference of the Association for\n  Computational Linguistics (ACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised approach for sarcasm generation based on a\nnon-sarcastic input sentence. Our method employs a retrieve-and-edit framework\nto instantiate two major characteristics of sarcasm: reversal of valence and\nsemantic incongruity with the context which could include shared commonsense or\nworld knowledge between the speaker and the listener. While prior works on\nsarcasm generation predominantly focus on context incongruity, we show that\ncombining valence reversal and semantic incongruity based on the commonsense\nknowledge generates sarcasm of higher quality. Human evaluation shows that our\nsystem generates sarcasm better than human annotators 34% of the time, and\nbetter than a reinforced hybrid baseline 90% of the time.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 02:30:09 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 20:34:04 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 11:14:11 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2020 06:42:06 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Chakrabarty", "Tuhin", ""], ["Ghosh", "Debanjan", ""], ["Muresan", "Smaranda", ""], ["Peng", "Nanyun", ""]]}, {"id": "2004.13249", "submitter": "Yiming Cui", "authors": "Wentao Ma, Yiming Cui, Ting Liu, Dong Wang, Shijin Wang, Guoping Hu", "title": "Conversational Word Embedding for Retrieval-Based Dialog System", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": "10.18653/v1/2020.acl-main.127", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human conversations contain many types of information, e.g., knowledge,\ncommon sense, and language habits. In this paper, we propose a conversational\nword embedding method named PR-Embedding, which utilizes the conversation pairs\n$ \\left\\langle{post, reply} \\right\\rangle$ to learn word embedding. Different\nfrom previous works, PR-Embedding uses the vectors from two different semantic\nspaces to represent the words in post and reply. To catch the information among\nthe pair, we first introduce the word alignment model from statistical machine\ntranslation to generate the cross-sentence window, then train the embedding on\nword-level and sentence-level. We evaluate the method on single-turn and\nmulti-turn response selection tasks for retrieval-based dialog systems. The\nexperiment results show that PR-Embedding can improve the quality of the\nselected response. PR-Embedding source code is available at\nhttps://github.com/wtma/PR-Embedding\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 02:43:36 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Ma", "Wentao", ""], ["Cui", "Yiming", ""], ["Liu", "Ting", ""], ["Wang", "Dong", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "2004.13255", "submitter": "Yaushian Wang", "authors": "Yau-Shian Wang and Hung-Yi Lee and Yun-Nung Chen", "title": "Learning Interpretable and Discrete Representations with Adversarial\n  Training for Unsupervised Text Classification", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning continuous representations from unlabeled textual data has been\nincreasingly studied for benefiting semi-supervised learning. Although it is\nrelatively easier to interpret discrete representations, due to the difficulty\nof training, learning discrete representations for unlabeled textual data has\nnot been widely explored. This work proposes TIGAN that learns to encode texts\ninto two disentangled representations, including a discrete code and a\ncontinuous noise, where the discrete code represents interpretable topics, and\nthe noise controls the variance within the topics. The discrete code learned by\nTIGAN can be used for unsupervised text classification. Compared to other\nunsupervised baselines, the proposed TIGAN achieves superior performance on six\ndifferent corpora. Also, the performance is on par with a recently proposed\nweakly-supervised text classification method. The extracted topical words for\nrepresenting latent topics show that TIGAN learns coherent and highly\ninterpretable topics.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 02:53:59 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Wang", "Yau-Shian", ""], ["Lee", "Hung-Yi", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2004.13270", "submitter": "Shilin He", "authors": "Shilin He, Xing Wang, Shuming Shi, Michael R. Lyu, Zhaopeng Tu", "title": "Assessing the Bilingual Knowledge Learned by Neural Machine Translation\n  Models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation (MT) systems translate text between different languages\nby automatically learning in-depth knowledge of bilingual lexicons, grammar and\nsemantics from the training examples. Although neural machine translation (NMT)\nhas led the field of MT, we have a poor understanding on how and why it works.\nIn this paper, we bridge the gap by assessing the bilingual knowledge learned\nby NMT models with phrase table -- an interpretable table of bilingual\nlexicons. We extract the phrase table from the training examples that an NMT\nmodel correctly predicts. Extensive experiments on widely-used datasets show\nthat the phrase table is reasonable and consistent against language pairs and\nrandom seeds. Equipped with the interpretable phrase table, we find that NMT\nmodels learn patterns from simple to complex and distill essential bilingual\nknowledge from the training examples. We also revisit some advances that\npotentially affect the learning of bilingual knowledge (e.g.,\nback-translation), and report some interesting findings. We believe this work\nopens a new angle to interpret NMT with statistic models, and provides\nempirical supports for recent advances in improving NMT models.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 03:44:34 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["He", "Shilin", ""], ["Wang", "Xing", ""], ["Shi", "Shuming", ""], ["Lyu", "Michael R.", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "2004.13278", "submitter": "Yue Wang", "authors": "Yue Wang, Shafiq Joty, Michael R. Lyu, Irwin King, Caiming Xiong,\n  Steven C.H. Hoi", "title": "VD-BERT: A Unified Vision and Dialog Transformer with BERT", "comments": "EMNLP 2020 (14 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual dialog is a challenging vision-language task, where a dialog agent\nneeds to answer a series of questions through reasoning on the image content\nand dialog history. Prior work has mostly focused on various attention\nmechanisms to model such intricate interactions. By contrast, in this work, we\npropose VD-BERT, a simple yet effective framework of unified vision-dialog\nTransformer that leverages the pretrained BERT language models for Visual\nDialog tasks. The model is unified in that (1) it captures all the interactions\nbetween the image and the multi-turn dialog using a single-stream Transformer\nencoder, and (2) it supports both answer ranking and answer generation\nseamlessly through the same architecture. More crucially, we adapt BERT for the\neffective fusion of vision and dialog contents via visually grounded training.\nWithout the need of pretraining on external vision-language data, our model\nyields new state of the art, achieving the top position in both single-model\nand ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog\nleaderboard. Our code and pretrained models are released at\nhttps://github.com/salesforce/VD-BERT.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:08:46 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 08:41:22 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 09:07:41 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wang", "Yue", ""], ["Joty", "Shafiq", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""], ["Xiong", "Caiming", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2004.13304", "submitter": "Katharina Kann", "authors": "Katharina Kann, Samuel R. Bowman, Kyunghyun Cho", "title": "Learning to Learn Morphological Inflection for Resource-Poor Languages", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to cast the task of morphological inflection - mapping a lemma to\nan indicated inflected form - for resource-poor languages as a meta-learning\nproblem. Treating each language as a separate task, we use data from\nhigh-resource source languages to learn a set of model parameters that can\nserve as a strong initialization point for fine-tuning on a resource-poor\ntarget language. Experiments with two model architectures on 29 target\nlanguages from 3 families show that our suggested approach outperforms all\nbaselines. In particular, it obtains a 31.7% higher absolute accuracy than a\npreviously proposed cross-lingual transfer model and outperforms the previous\nstate of the art by 1.7% absolute accuracy on average over languages.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 05:13:17 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Kann", "Katharina", ""], ["Bowman", "Samuel R.", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2004.13305", "submitter": "Katharina Kann", "authors": "Katharina Kann, Oph\\'elie Lacroix, Anders S{\\o}gaard", "title": "Weakly Supervised POS Taggers Perform Poorly on Truly Low-Resource\n  Languages", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-of-speech (POS) taggers for low-resource languages which are exclusively\nbased on various forms of weak supervision - e.g., cross-lingual transfer,\ntype-level supervision, or a combination thereof - have been reported to\nperform almost as well as supervised ones. However, weakly supervised POS\ntaggers are commonly only evaluated on languages that are very different from\ntruly low-resource languages, and the taggers use sources of information, like\nhigh-coverage and almost error-free dictionaries, which are likely not\navailable for resource-poor languages. We train and evaluate state-of-the-art\nweakly supervised POS taggers for a typologically diverse set of 15 truly\nlow-resource languages. On these languages, given a realistic amount of\nresources, even our best model gets only less than half of the words right. Our\nresults highlight the need for new and different approaches to POS tagging for\ntruly low-resource languages.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 05:14:08 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Kann", "Katharina", ""], ["Lacroix", "Oph\u00e9lie", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2004.13310", "submitter": "Liang Ding", "authors": "Liang Ding, Longyue Wang, Dacheng Tao", "title": "Self-Attention with Cross-Lingual Position Representation", "comments": "To appear in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Position encoding (PE), an essential part of self-attention networks (SANs),\nis used to preserve the word order information for natural language processing\ntasks, generating fixed position indices for input sequences. However, in\ncross-lingual scenarios, e.g. machine translation, the PEs of source and target\nsentences are modeled independently. Due to word order divergences in different\nlanguages, modeling the cross-lingual positional relationships might help SANs\ntackle this problem. In this paper, we augment SANs with \\emph{cross-lingual\nposition representations} to model the bilingually aware latent structure for\nthe input sentence. Specifically, we utilize bracketing transduction grammar\n(BTG)-based reordering information to encourage SANs to learn bilingual\ndiagonal alignments. Experimental results on WMT'14 English$\\Rightarrow$German,\nWAT'17 Japanese$\\Rightarrow$English, and WMT'17 Chinese$\\Leftrightarrow$English\ntranslation tasks demonstrate that our approach significantly and consistently\nimproves translation quality over strong baselines. Extensive analyses confirm\nthat the performance gains come from the cross-lingual information.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 05:23:43 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 03:39:19 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 09:47:27 GMT"}, {"version": "v4", "created": "Sat, 21 Nov 2020 17:07:06 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ding", "Liang", ""], ["Wang", "Longyue", ""], ["Tao", "Dacheng", ""]]}, {"id": "2004.13317", "submitter": "Hang Zhang", "authors": "Hang Zhang, Dayiheng Liu, Jiancheng Lv, Cheng Luo", "title": "Let's be Humorous: Knowledge Enhanced Humor Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of humor is an under-explored and challenging problem.\nPrevious works mainly utilize templates or replace phrases to generate humor.\nHowever, few works focus on freer forms and the background knowledge of humor.\nThe linguistic theory of humor defines the structure of a humor sentence as\nset-up and punchline. In this paper, we explore how to generate a punchline\ngiven the set-up with the relevant knowledge. We propose a framework that can\nfuse the knowledge to end-to-end models. To our knowledge, this is the first\nattempt to generate punchlines with knowledge enhanced model. Furthermore, we\ncreate the first humor-knowledge dataset. The experimental results demonstrate\nthat our method can make use of knowledge to generate fluent, funny punchlines,\nwhich outperforms several baselines.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 06:06:18 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 03:04:14 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhang", "Hang", ""], ["Liu", "Dayiheng", ""], ["Lv", "Jiancheng", ""], ["Luo", "Cheng", ""]]}, {"id": "2004.13338", "submitter": "Shuailiang Zhang", "authors": "Shuailiang Zhang, Hai Zhao, Junru Zhou", "title": "Semantics-Aware Inferential Network for Natural Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For natural language understanding tasks, either machine reading\ncomprehension or natural language inference, both semantics-aware and inference\nare favorable features of the concerned modeling for better understanding\nperformance. Thus we propose a Semantics-Aware Inferential Network (SAIN) to\nmeet such a motivation. Taking explicit contextualized semantics as a\ncomplementary input, the inferential module of SAIN enables a series of\nreasoning steps over semantic clues through an attention mechanism. By\nstringing these steps, the inferential network effectively learns to perform\niterative reasoning which incorporates both explicit semantics and\ncontextualized representations. In terms of well pre-trained language models as\nfront-end encoder, our model achieves significant improvement on 11 tasks\nincluding machine reading comprehension and natural language inference.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 07:24:43 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Zhang", "Shuailiang", ""], ["Zhao", "Hai", ""], ["Zhou", "Junru", ""]]}, {"id": "2004.13342", "submitter": "Wangchunshu Zhou", "authors": "Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, Ming Zhou", "title": "Scheduled DropHead: A Regularization Method for Transformer Models", "comments": "EMNLP 2020 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce DropHead, a structured dropout method\nspecifically designed for regularizing the multi-head attention mechanism,\nwhich is a key component of transformer, a state-of-the-art model for various\nNLP tasks. In contrast to the conventional dropout mechanisms which randomly\ndrop units or connections, the proposed DropHead is a structured dropout\nmethod. It drops entire attention-heads during training and It prevents the\nmulti-head attention model from being dominated by a small portion of attention\nheads while also reduces the risk of overfitting the training data, thus making\nuse of the multi-head attention mechanism more efficiently. Motivated by recent\nstudies about the learning dynamic of the multi-head attention mechanism, we\npropose a specific dropout rate schedule to adaptively adjust the dropout rate\nof DropHead and achieve better regularization effect. Experimental results on\nboth machine translation and text classification benchmark datasets demonstrate\nthe effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 07:33:14 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 15:57:37 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhou", "Wangchunshu", ""], ["Ge", "Tao", ""], ["Xu", "Ke", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "2004.13432", "submitter": "Wenliang Dai", "authors": "Wenliang Dai, Tiezheng Yu, Zihan Liu, Pascale Fung", "title": "Kungfupanda at SemEval-2020 Task 12: BERT-Based Multi-Task Learning for\n  Offensive Language Detection", "comments": "Submitted to SemEval-2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, offensive content in social media has become a serious problem, and\nautomatically detecting offensive language is an essential task. In this paper,\nwe build an offensive language detection system, which combines multi-task\nlearning with BERT-based models. Using a pre-trained language model such as\nBERT, we can effectively learn the representations for noisy text in social\nmedia. Besides, to boost the performance of offensive language detection, we\nleverage the supervision signals from other related tasks. In the\nOffensEval-2020 competition, our model achieves 91.51% F1 score in English\nSub-task A, which is comparable to the first place (92.23%F1). An empirical\nanalysis is provided to explain the effectiveness of our approaches.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 11:27:24 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 06:51:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Dai", "Wenliang", ""], ["Yu", "Tiezheng", ""], ["Liu", "Zihan", ""], ["Fung", "Pascale", ""]]}, {"id": "2004.13454", "submitter": "Xiang Dai", "authors": "Xiang Dai and Sarvnaz Karimi and Ben Hachey and Cecile Paris", "title": "An Effective Transition-based Model for Discontinuous NER", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unlike widely used Named Entity Recognition (NER) data sets in generic\ndomains, biomedical NER data sets often contain mentions consisting of\ndiscontinuous spans. Conventional sequence tagging techniques encode Markov\nassumptions that are efficient but preclude recovery of these mentions. We\npropose a simple, effective transition-based model with generic neural encoding\nfor discontinuous NER. Through extensive experiments on three biomedical data\nsets, we show that our model can effectively recognize discontinuous mentions\nwithout sacrificing the accuracy on continuous mentions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 12:19:12 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Dai", "Xiang", ""], ["Karimi", "Sarvnaz", ""], ["Hachey", "Ben", ""], ["Paris", "Cecile", ""]]}, {"id": "2004.13455", "submitter": "Lianwei Wu", "authors": "Lianwei Wu, Yuan Rao, Yongqiang Zhao, Hao Liang, Ambreen Nazir", "title": "DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim\n  Verification", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many methods discover effective evidence from reliable sources by\nappropriate neural networks for explainable claim verification, which has been\nwidely recognized. However, in these methods, the discovery process of evidence\nis nontransparent and unexplained. Simultaneously, the discovered evidence only\nroughly aims at the interpretability of the whole sequence of claims but\ninsufficient to focus on the false parts of claims. In this paper, we propose a\nDecision Tree-based Co-Attention model (DTCA) to discover evidence for\nexplainable claim verification. Specifically, we first construct Decision\nTree-based Evidence model (DTE) to select comments with high credibility as\nevidence in a transparent and interpretable way. Then we design Co-attention\nSelf-attention networks (CaSa) to make the selected evidence interact with\nclaims, which is for 1) training DTE to determine the optimal decision\nthresholds and obtain more powerful evidence; and 2) utilizing the evidence to\nfind the false parts in the claim. Experiments on two public datasets,\nRumourEval and PHEME, demonstrate that DTCA not only provides explanations for\nthe results of claim verification but also achieves the state-of-the-art\nperformance, boosting the F1-score by 3.11%, 2.41%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 12:19:46 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Wu", "Lianwei", ""], ["Rao", "Yuan", ""], ["Zhao", "Yongqiang", ""], ["Liang", "Hao", ""], ["Nazir", "Ambreen", ""]]}, {"id": "2004.13480", "submitter": "Zhong Meng", "authors": "Zhong Meng, Hu Hu, Jinyu Li, Changliang Liu, Yan Huang, Yifan Gong,\n  Chin-Hui Lee", "title": "L-Vector: Neural Label Embedding for Domain Adaptation", "comments": "5 pages, 2 figure, ICASSP 2020", "journal-ref": "2019 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Barcelona, Spain", "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural label embedding (NLE) scheme for the domain\nadaptation of a deep neural network (DNN) acoustic model with unpaired data\nsamples from source and target domains. With NLE method, we distill the\nknowledge from a powerful source-domain DNN into a dictionary of label\nembeddings, or l-vectors, one for each senone class. Each l-vector is a\nrepresentation of the senone-specific output distributions of the source-domain\nDNN and is learned to minimize the average L2, Kullback-Leibler (KL) or\nsymmetric KL distance to the output vectors with the same label through simple\naveraging or standard back-propagation. During adaptation, the l-vectors serve\nas the soft targets to train the target-domain model with cross-entropy loss.\nWithout parallel data constraint as in the teacher-student learning, NLE is\nspecially suited for the situation where the paired target-domain data cannot\nbe simulated from the source-domain data. We adapt a 6400 hours\nmulti-conditional US English acoustic model to each of the 9 accented English\n(80 to 830 hours) and kids' speech (80 hours). NLE achieves up to 14.1%\nrelative word error rate reduction over direct re-training with one-hot labels.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 06:40:31 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Meng", "Zhong", ""], ["Hu", "Hu", ""], ["Li", "Jinyu", ""], ["Liu", "Changliang", ""], ["Huang", "Yan", ""], ["Gong", "Yifan", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "2004.13481", "submitter": "Mohammed Belkhatir", "authors": "Bhawani Selvaretnam, Mohammed Belkhatir", "title": "A Linguistically Driven Framework for Query Expansion via Grammatical\n  Constituent Highlighting and Role-Based Concept Weighting", "comments": "arXiv admin note: text overlap with arXiv:2004.11083", "journal-ref": null, "doi": "10.1016/j.ipm.2015.04.002", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a linguistically-motivated query expansion\nframework that recognizes and en-codes significant query constituents that\ncharacterize query intent in order to improve retrieval performance.\nConcepts-of-Interest are recognized as the core concepts that represent the\ngist of the search goal whilst the remaining query constituents which serve to\nspecify the search goal and complete the query structure are classified as\ndescriptive, relational or structural. Acknowledging the need to form\nsemantically-associated base pairs for the purpose of extracting related\npotential expansion concepts, an algorithm which capitalizes on syntactical\ndependencies to capture relationships between adjacent and non-adjacent query\nconcepts is proposed. Lastly, a robust weighting scheme that duly emphasizes\nthe importance of query constituents based on their linguistic role within the\nexpanded query is presented. We demonstrate improvements in retrieval\neffectiveness in terms of increased mean average precision (MAP) garnered by\nthe proposed linguistic-based query expansion framework through experimentation\non the TREC ad hoc test collections.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 01:43:00 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Selvaretnam", "Bhawani", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2004.13486", "submitter": "Bhaskar Mitra", "authors": "Emine Yilmaz, Nick Craswell, Bhaskar Mitra and Daniel Campos", "title": "On the Reliability of Test Collections for Evaluating Systems of\n  Different Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep learning based models are increasingly being used for information\nretrieval (IR), a major challenge is to ensure the availability of test\ncollections for measuring their quality. Test collections are generated based\non pooling results of various retrieval systems, but until recently this did\nnot include deep learning systems. This raises a major challenge for reusable\nevaluation: Since deep learning based models use external resources (e.g. word\nembeddings) and advanced representations as opposed to traditional methods that\nare mainly based on lexical similarity, they may return different types of\nrelevant document that were not identified in the original pooling. If so, test\ncollections constructed using traditional methods are likely to lead to biased\nand unfair evaluation results for deep learning (neural) systems. This paper\nuses simulated pooling to test the fairness and reusability of test\ncollections, showing that pooling based on traditional systems only can lead to\nbiased evaluation of deep learning systems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:22:26 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Yilmaz", "Emine", ""], ["Craswell", "Nick", ""], ["Mitra", "Bhaskar", ""], ["Campos", "Daniel", ""]]}, {"id": "2004.13521", "submitter": "Sourav Sen", "authors": "Sourav Sen", "title": "Detect Language of Transliterated Texts", "comments": "10 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informal transliteration from other languages to English is prevalent in\nsocial media threads, instant messaging, and discussion forums. Without\nidentifying the language of such transliterated text, users who do not speak\nthat language cannot understand its content using translation tools. We propose\na Language Identification (LID) system, with an approach for feature\nextraction, which can detect the language of transliterated texts reasonably\nwell even with limited training data and computational resources. We tokenize\nthe words into phonetic syllables and use a simple Long Short-term Memory\n(LSTM) network architecture to detect the language of transliterated texts.\nWith intensive experiments, we show that the tokenization of transliterated\nwords as phonetic syllables effectively represents their causal sound patterns.\nPhonetic syllable tokenization, therefore, makes it easier for even simpler\nmodel architectures to learn the characteristic patterns to identify any\nlanguage.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 10:28:02 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Sen", "Sourav", ""]]}, {"id": "2004.13522", "submitter": "Li Fu", "authors": "Li Fu, Xiaoxiao Li, Libo Zi", "title": "Research on Modeling Units of Transformer Transducer for Mandarin Speech\n  Recognition", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling unit and model architecture are two key factors of Recurrent Neural\nNetwork Transducer (RNN-T) in end-to-end speech recognition. To improve the\nperformance of RNN-T for Mandarin speech recognition task, a novel transformer\ntransducer with the combination architecture of self-attention transformer and\nRNN is proposed. And then the choice of different modeling units for\ntransformer transducer is explored. In addition, we present a new mix-bandwidth\ntraining method to obtain a general model that is able to accurately recognize\nMandarin speech with different sampling rates simultaneously. All of our\nexperiments are conducted on about 12,000 hours of Mandarin speech with\nsampling rate in 8kHz and 16kHz. Experimental results show that Mandarin\ntransformer transducer using syllable with tone achieves the best performance.\nIt yields an average of 14.4% and 44.1% relative Word Error Rate (WER)\nreduction when compared with the models using syllable initial/final with tone\nand Chinese character, respectively. Also, it outperforms the model based on\nsyllable initial/final with tone with an average of 13.5% relative Character\nError Rate (CER) reduction.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 05:12:52 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Fu", "Li", ""], ["Li", "Xiaoxiao", ""], ["Zi", "Libo", ""]]}, {"id": "2004.13530", "submitter": "Luca Benedetto", "authors": "Luca Benedetto, Andrea Cappelli, Roberto Turrin, Paolo Cremonesi", "title": "Introducing a framework to assess newly created questions with Natural\n  Language Processing", "comments": "Accepted at the International Conference of Artificial Intelligence\n  in Education", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models such as those derived from Item Response Theory (IRT)\nenable the assessment of students on a specific subject, which can be useful\nfor several purposes (e.g., learning path customization, drop-out prediction).\nHowever, the questions have to be assessed as well and, although it is possible\nto estimate with IRT the characteristics of questions that have already been\nanswered by several students, this technique cannot be used on newly generated\nquestions. In this paper, we propose a framework to train and evaluate models\nfor estimating the difficulty and discrimination of newly created Multiple\nChoice Questions by extracting meaningful features from the text of the\nquestion and of the possible choices. We implement one model using this\nframework and test it on a real-world dataset provided by CloudAcademy, showing\nthat it outperforms previously proposed models, reducing by 6.7% the RMSE for\ndifficulty estimation and by 10.8% the RMSE for discrimination estimation. We\nalso present the results of an ablation study performed to support our features\nchoice and to show the effects of different characteristics of the questions'\ntext on difficulty and discrimination.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:57:21 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 09:06:57 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Benedetto", "Luca", ""], ["Cappelli", "Andrea", ""], ["Turrin", "Roberto", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "2004.13542", "submitter": "Yijin Liu", "authors": "Yijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen, Jinan Xu", "title": "Faster Depth-Adaptive Transformers", "comments": "AAAI-2021. Code will appear at:\n  https://github.com/Adaxry/Adaptive-Transformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth-adaptive neural networks can dynamically adjust depths according to the\nhardness of input words, and thus improve efficiency. The main challenge is how\nto measure such hardness and decide the required depths (i.e., layers) to\nconduct. Previous works generally build a halting unit to decide whether the\ncomputation should continue or stop at each layer. As there is no specific\nsupervision of depth selection, the halting unit may be under-optimized and\ninaccurate, which results in suboptimal and unstable performance when modeling\nsentences. In this paper, we get rid of the halting unit and estimate the\nrequired depths in advance, which yields a faster depth-adaptive model.\nSpecifically, two approaches are proposed to explicitly measure the hardness of\ninput words and estimate corresponding adaptive depth, namely 1) mutual\ninformation (MI) based estimation and 2) reconstruction loss based estimation.\nWe conduct experiments on the text classification task with 24 datasets in\nvarious sizes and domains. Results confirm that our approaches can speed up the\nvanilla Transformer (up to 7x) while preserving high accuracy. Moreover,\nefficiency and robustness are significantly improved when compared with other\ndepth-adaptive approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:08:10 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 03:32:40 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 07:19:10 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2020 09:01:38 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Liu", "Yijin", ""], ["Meng", "Fandong", ""], ["Zhou", "Jie", ""], ["Chen", "Yufeng", ""], ["Xu", "Jinan", ""]]}, {"id": "2004.13579", "submitter": "Zequn Sun", "authors": "Zequn Sun, Jiacheng Huang, Wei Hu, Muchao Chen, Lingbing Guo, Yuzhong\n  Qu", "title": "TransEdge: Translating Relation-contextualized Embeddings for Knowledge\n  Graphs", "comments": "Published in proceedings of the 18th International Semantic Web\n  Conference (ISWC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning knowledge graph (KG) embeddings has received increasing attention in\nrecent years. Most embedding models in literature interpret relations as linear\nor bilinear mapping functions to operate on entity embeddings. However, we find\nthat such relation-level modeling cannot capture the diverse relational\nstructures of KGs well. In this paper, we propose a novel edge-centric\nembedding model TransEdge, which contextualizes relation representations in\nterms of specific head-tail entity pairs. We refer to such contextualized\nrepresentations of a relation as edge embeddings and interpret them as\ntranslations between entity embeddings. TransEdge achieves promising\nperformance on different prediction tasks. Our experiments on benchmark\ndatasets indicate that it obtains the state-of-the-art results on\nembedding-based entity alignment. We also show that TransEdge is complementary\nwith conventional entity alignment methods. Moreover, it shows very competitive\nperformance on link prediction.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 03:00:45 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Sun", "Zequn", ""], ["Huang", "Jiacheng", ""], ["Hu", "Wei", ""], ["Chen", "Muchao", ""], ["Guo", "Lingbing", ""], ["Qu", "Yuzhong", ""]]}, {"id": "2004.13580", "submitter": "Andreas van Cranenburgh", "authors": "St\\'ephan Tulkens, Andreas van Cranenburgh", "title": "Embarrassingly Simple Unsupervised Aspect Extraction", "comments": "Accepted as ACL 2020 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple but effective method for aspect identification in\nsentiment analysis. Our unsupervised method only requires word embeddings and a\nPOS tagger, and is therefore straightforward to apply to new domains and\nlanguages. We introduce Contrastive Attention (CAt), a novel single-head\nattention mechanism based on an RBF kernel, which gives a considerable boost in\nperformance and makes the model interpretable. Previous work relied on\nsyntactic features and complex neural models. We show that given the simplicity\nof current benchmark datasets for aspect extraction, such complex models are\nnot needed. The code to reproduce the experiments reported in this paper is\navailable at https://github.com/clips/cat\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 15:09:51 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Tulkens", "St\u00e9phan", ""], ["van Cranenburgh", "Andreas", ""]]}, {"id": "2004.13590", "submitter": "Xiaozhi Wang", "authors": "Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu,\n  Juanzi Li, Peng Li, Yankai Lin, Jie Zhou", "title": "MAVEN: A Massive General Domain Event Detection Dataset", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event detection (ED), which means identifying event trigger words and\nclassifying event types, is the first and most fundamental step for extracting\nevent knowledge from plain text. Most existing datasets exhibit the following\nissues that limit further development of ED: (1) Data scarcity. Existing\nsmall-scale datasets are not sufficient for training and stably benchmarking\nincreasingly sophisticated modern neural methods. (2) Low coverage. Limited\nevent types of existing datasets cannot well cover general-domain events, which\nrestricts the applications of ED models. To alleviate these problems, we\npresent a MAssive eVENt detection dataset (MAVEN), which contains 4,480\nWikipedia documents, 118,732 event mention instances, and 168 event types.\nMAVEN alleviates the data scarcity problem and covers much more general event\ntypes. We reproduce the recent state-of-the-art ED models and conduct a\nthorough evaluation on MAVEN. The experimental results show that existing ED\nmethods cannot achieve promising results on MAVEN as on the small datasets,\nwhich suggests that ED in the real world remains a challenging task and\nrequires further research efforts. We also discuss further directions for\ngeneral domain ED with empirical analyses. The source code and dataset can be\nobtained from https://github.com/THU-KEG/MAVEN-dataset.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 15:25:19 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 09:19:13 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Wang", "Xiaozhi", ""], ["Wang", "Ziqi", ""], ["Han", "Xu", ""], ["Jiang", "Wangyi", ""], ["Han", "Rong", ""], ["Liu", "Zhiyuan", ""], ["Li", "Juanzi", ""], ["Li", "Peng", ""], ["Lin", "Yankai", ""], ["Zhou", "Jie", ""]]}, {"id": "2004.13606", "submitter": "Xiang Zhou", "authors": "Xiang Zhou, Yixin Nie, Hao Tan, Mohit Bansal", "title": "The Curse of Performance Instability in Analysis Datasets: Consequences,\n  Source, and Suggestions", "comments": "EMNLP 2020 (14 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find that the performance of state-of-the-art models on Natural Language\nInference (NLI) and Reading Comprehension (RC) analysis/stress sets can be\nhighly unstable. This raises three questions: (1) How will the instability\naffect the reliability of the conclusions drawn based on these analysis sets?\n(2) Where does this instability come from? (3) How should we handle this\ninstability and what are some potential solutions? For the first question, we\nconduct a thorough empirical study over analysis sets and find that in addition\nto the unstable final performance, the instability exists all along the\ntraining curve. We also observe lower-than-expected correlations between the\nanalysis validation set and standard validation set, questioning the\neffectiveness of the current model-selection routine. Next, to answer the\nsecond question, we give both theoretical explanations and empirical evidence\nregarding the source of the instability, demonstrating that the instability\nmainly comes from high inter-example correlations within analysis sets.\nFinally, for the third question, we discuss an initial attempt to mitigate the\ninstability and suggest guidelines for future work such as reporting the\ndecomposed variance for more interpretable results and fair comparison across\nmodels. Our code is publicly available at:\nhttps://github.com/owenzx/InstabilityAnalysis\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 15:41:12 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 02:22:35 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zhou", "Xiang", ""], ["Nie", "Yixin", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2004.13609", "submitter": "Jonathan P. Chang", "authors": "Jonathan P. Chang, Justin Cheng, Cristian Danescu-Niculescu-Mizil", "title": "Don't Let Me Be Misunderstood: Comparing Intentions and Perceptions in\n  Online Discussions", "comments": "Proceedings of The Web Conference (WWW) 2020", "journal-ref": null, "doi": "10.1145/3366423.3380273", "report-no": null, "categories": "cs.CY cs.CL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discourse involves two perspectives: a person's intention in making an\nutterance and others' perception of that utterance. The misalignment between\nthese perspectives can lead to undesirable outcomes, such as misunderstandings,\nlow productivity and even overt strife. In this work, we present a\ncomputational framework for exploring and comparing both perspectives in online\npublic discussions.\n  We combine logged data about public comments on Facebook with a survey of\nover 16,000 people about their intentions in writing these comments or about\ntheir perceptions of comments that others had written. Unlike previous studies\nof online discussions that have largely relied on third-party labels to\nquantify properties such as sentiment and subjectivity, our approach also\ndirectly captures what the speakers actually intended when writing their\ncomments. In particular, our analysis focuses on judgments of whether a comment\nis stating a fact or an opinion, since these concepts were shown to be often\nconfused.\n  We show that intentions and perceptions diverge in consequential ways. People\nare more likely to perceive opinions than to intend them, and linguistic cues\nthat signal how an utterance is intended can differ from those that signal how\nit will be perceived. Further, this misalignment between intentions and\nperceptions can be linked to the future health of a conversation: when a\ncomment whose author intended to share a fact is misperceived as sharing an\nopinion, the subsequent conversation is more likely to derail into uncivil\nbehavior than when the comment is perceived as intended. Altogether, these\nfindings may inform the design of discussion platforms that better promote\npositive interactions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 15:43:46 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Chang", "Jonathan P.", ""], ["Cheng", "Justin", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "2004.13625", "submitter": "Xinya Du", "authors": "Xinya Du and Claire Cardie", "title": "Event Extraction by Answering (Almost) Natural Questions", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of event extraction requires detecting the event trigger and\nextracting its corresponding arguments. Existing work in event argument\nextraction typically relies heavily on entity recognition as a\npreprocessing/concurrent step, causing the well-known problem of error\npropagation. To avoid this issue, we introduce a new paradigm for event\nextraction by formulating it as a question answering (QA) task that extracts\nthe event arguments in an end-to-end manner. Empirical results demonstrate that\nour framework outperforms prior methods substantially; in addition, it is\ncapable of extracting event arguments for roles not seen at training time\n(zero-shot learning setting).\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 16:15:46 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 23:17:22 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Du", "Xinya", ""], ["Cardie", "Claire", ""]]}, {"id": "2004.13631", "submitter": "Jie Zhou", "authors": "Jie Zhou, Shengding Hu, Xin Lv, Cheng Yang, Zhiyuan Liu, Wei Xu, Jie\n  Jiang, Juanzi Li, Maosong Sun", "title": "KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization\n  and Completion", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive knowledge graph (KG) contains an instance-level entity graph\nand an ontology-level concept graph. The two-view KG provides a testbed for\nmodels to \"simulate\" human's abilities on knowledge abstraction,\nconcretization, and completion (KACC), which are crucial for human to recognize\nthe world and manage learned knowledge. Existing studies mainly focus on\npartial aspects of KACC. In order to promote thorough analyses for KACC\nabilities of models, we propose a unified KG benchmark by improving existing\nbenchmarks in terms of dataset scale, task coverage, and difficulty.\nSpecifically, we collect new datasets that contain larger concept graphs,\nabundant cross-view links as well as dense entity graphs. Based on the\ndatasets, we propose novel tasks such as multi-hop knowledge abstraction (MKA),\nmulti-hop knowledge concretization (MKC) and then design a comprehensive\nbenchmark. For MKA and MKC tasks, we further annotate multi-hop hierarchical\ntriples as harder samples. The experimental results of existing methods\ndemonstrate the challenges of our benchmark. The resource is available at\nhttps://github.com/thunlp/KACC.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 16:21:57 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 03:23:10 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhou", "Jie", ""], ["Hu", "Shengding", ""], ["Lv", "Xin", ""], ["Yang", "Cheng", ""], ["Liu", "Zhiyuan", ""], ["Xu", "Wei", ""], ["Jiang", "Jie", ""], ["Li", "Juanzi", ""], ["Sun", "Maosong", ""]]}, {"id": "2004.13637", "submitter": "Jason  Weston", "authors": "Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson,\n  Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau,\n  and Jason Weston", "title": "Recipes for building an open-domain chatbot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building open-domain chatbots is a challenging area for machine learning\nresearch. While prior work has shown that scaling neural models in the number\nof parameters and the size of the data they are trained on gives improved\nresults, we show that other ingredients are important for a high-performing\nchatbot. Good conversation requires a number of skills that an expert\nconversationalist blends in a seamless way: providing engaging talking points\nand listening to their partners, and displaying knowledge, empathy and\npersonality appropriately, while maintaining a consistent persona. We show that\nlarge scale models can learn these skills when given appropriate training data\nand choice of generation strategy. We build variants of these recipes with 90M,\n2.7B and 9.4B parameter models, and make our models and code publicly\navailable. Human evaluations show our best models are superior to existing\napproaches in multi-turn dialogue in terms of engagingness and humanness\nmeasurements. We then discuss the limitations of this work by analyzing failure\ncases of our models.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 16:33:25 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 15:36:52 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Roller", "Stephen", ""], ["Dinan", "Emily", ""], ["Goyal", "Naman", ""], ["Ju", "Da", ""], ["Williamson", "Mary", ""], ["Liu", "Yinhan", ""], ["Xu", "Jing", ""], ["Ott", "Myle", ""], ["Shuster", "Kurt", ""], ["Smith", "Eric M.", ""], ["Boureau", "Y-Lan", ""], ["Weston", "Jason", ""]]}, {"id": "2004.13639", "submitter": "Si Sun", "authors": "Si Sun, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu, Jie Bao", "title": "Joint Keyphrase Chunking and Salience Ranking with BERT", "comments": "6 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective keyphrase extraction system requires to produce self-contained\nhigh quality phrases that are also key to the document topic. This paper\npresents BERT-JointKPE, a multi-task BERT-based model for keyphrase extraction.\nJointKPE employs a chunking network to identify high-quality phrases and a\nranking network to learn their salience in the document. The model is trained\njointly on the chunking task and the ranking task, balancing the estimation of\nkeyphrase quality and salience. Experiments on two benchmarks demonstrate\nJointKPE's robust effectiveness with different BERT variants. Our analyses show\nthat JointKPE has advantages in predicting long keyphrases and extracting\nphrases that are not entities but also meaningful. The source code of this\npaper can be obtained from https://github.com/thunlp/BERT-KPE\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 16:34:35 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Sun", "Si", ""], ["Xiong", "Chenyan", ""], ["Liu", "Zhenghao", ""], ["Liu", "Zhiyuan", ""], ["Bao", "Jie", ""]]}, {"id": "2004.13640", "submitter": "Zihan Wang", "authors": "Zihan Wang, Karthikeyan K, Stephen Mayhew, Dan Roth", "title": "Extending Multilingual BERT to Low-Resource Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual BERT (M-BERT) has been a huge success in both supervised and\nzero-shot cross-lingual transfer learning. However, this success has focused\nonly on the top 104 languages in Wikipedia that it was trained on. In this\npaper, we propose a simple but effective approach to extend M-BERT (E-BERT) so\nthat it can benefit any new language, and show that our approach benefits\nlanguages that are already in M-BERT as well. We perform an extensive set of\nexperiments with Named Entity Recognition (NER) on 27 languages, only 16 of\nwhich are in M-BERT, and show an average increase of about 6% F1 on languages\nthat are already in M-BERT and 23% F1 increase on new languages.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 16:36:41 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Wang", "Zihan", ""], ["K", "Karthikeyan", ""], ["Mayhew", "Stephen", ""], ["Roth", "Dan", ""]]}, {"id": "2004.13645", "submitter": "Jacob Andreas", "authors": "Alana Marzoev, Samuel Madden, M. Frans Kaashoek, Michael Cafarella,\n  Jacob Andreas", "title": "Unnatural Language Processing: Bridging the Gap Between Synthetic and\n  Natural Language Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, human-annotated datasets are central to the development of natural\nlanguage processing models. Collecting these datasets can be the most\nchallenging part of the development process. We address this problem by\nintroducing a general purpose technique for ``simulation-to-real'' transfer in\nlanguage understanding problems with a delimited set of target behaviors,\nmaking it possible to develop models that can interpret natural utterances\nwithout natural training data. We begin with a synthetic data generation\nprocedure, and train a model that can accurately interpret utterances produced\nby the data generator. To generalize to natural utterances, we automatically\nfind projections of natural language utterances onto the support of the\nsynthetic language, using learned sentence embeddings to define a distance\nmetric. With only synthetic training data, our approach matches or outperforms\nstate-of-the-art models trained on natural language data in several domains.\nThese results suggest that simulation-to-real transfer is a practical framework\nfor developing NLP applications, and that improved models for transfer might\nprovide wide-ranging improvements in downstream tasks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 16:41:00 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Marzoev", "Alana", ""], ["Madden", "Samuel", ""], ["Kaashoek", "M. Frans", ""], ["Cafarella", "Michael", ""], ["Andreas", "Jacob", ""]]}, {"id": "2004.13659", "submitter": "Wanjun Zhong", "authors": "Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming\n  Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, Jian Yin", "title": "LogicalFactChecker: Leveraging Logical Operations for Fact Checking with\n  Graph Module Network", "comments": "13 pages; 7 figures; Accepted by ACL2020 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying the correctness of a textual statement requires not only semantic\nreasoning about the meaning of words, but also symbolic reasoning about logical\noperations like count, superlative, aggregation, etc. In this work, we propose\nLogicalFactChecker, a neural network approach capable of leveraging logical\noperations for fact checking. It achieves the state-of-the-art performance on\nTABFACT, a large-scale, benchmark dataset built for verifying a textual\nstatement with semi-structured tables. This is achieved by a graph module\nnetwork built upon the Transformer-based architecture. With a textual statement\nand a table as the input, LogicalFactChecker automatically derives a program\n(a.k.a. logical form) of the statement in a semantic parsing manner. A\nheterogeneous graph is then constructed to capture not only the structures of\nthe table and the program, but also the connections between inputs with\ndifferent modalities. Such a graph reveals the related contexts of each word in\nthe statement, the table and the program. The graph is used to obtain\ngraph-enhanced contextual representations of words in Transformer-based\narchitecture. After that, a program-driven module network is further introduced\nto exploit the hierarchical structure of the program, where semantic\ncompositionality is dynamically modeled along the program structure with a set\nof function-specific modules. Ablation experiments suggest that both the\nheterogeneous graph and the module network are important to obtain strong\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:04:19 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Zhong", "Wanjun", ""], ["Tang", "Duyu", ""], ["Feng", "Zhangyin", ""], ["Duan", "Nan", ""], ["Zhou", "Ming", ""], ["Gong", "Ming", ""], ["Shou", "Linjun", ""], ["Jiang", "Daxin", ""], ["Wang", "Jiahai", ""], ["Yin", "Jian", ""]]}, {"id": "2004.13670", "submitter": "Dongmei Wang", "authors": "Dongmei Wang, Zhuo Chen and Takuya Yoshioka", "title": "Neural Speech Separation Using Spatially Distributed Microphones", "comments": "5 pages, 2 figures, Interspeech2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a neural network based speech separation method using\nspatially distributed microphones. Unlike with traditional microphone array\nsettings, neither the number of microphones nor their spatial arrangement is\nknown in advance, which hinders the use of conventional multi-channel speech\nseparation neural networks based on fixed size input. To overcome this, a novel\nnetwork architecture is proposed that interleaves inter-channel processing\nlayers and temporal processing layers. The inter-channel processing layers\napply a self-attention mechanism along the channel dimension to exploit the\ninformation obtained with a varying number of microphones. The temporal\nprocessing layers are based on a bidirectional long short term memory (BLSTM)\nmodel and applied to each channel independently. The proposed network leverages\ninformation across time and space by stacking these two kinds of layers\nalternately. Our network estimates time-frequency (TF) masks for each speaker,\nwhich are then used to generate enhanced speech signals either with TF masking\nor beamforming. Speech recognition experimental results show that the proposed\nmethod significantly outperforms baseline multi-channel speech separation\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:16:31 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Wang", "Dongmei", ""], ["Chen", "Zhuo", ""], ["Yoshioka", "Takuya", ""]]}, {"id": "2004.13671", "submitter": "Belinda Z. Li", "authors": "Belinda Z. Li, Gabriel Stanovsky, Luke Zettlemoyer", "title": "Active Learning for Coreference Resolution using Discrete Annotation", "comments": "12 pages, 7 figures, ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve upon pairwise annotation for active learning in coreference\nresolution, by asking annotators to identify mention antecedents if a presented\nmention pair is deemed not coreferent. This simple modification, when combined\nwith a novel mention clustering algorithm for selecting which examples to\nlabel, is much more efficient in terms of the performance obtained per\nannotation budget. In experiments with existing benchmark coreference datasets,\nwe show that the signal from this additional question leads to significant\nperformance gains per human-annotation hour. Future work can use our annotation\nprotocol to effectively develop coreference models for new domains. Our code is\npublicly available at\nhttps://github.com/belindal/discrete-active-learning-coref .\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:17:11 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 17:25:01 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 00:31:23 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Li", "Belinda Z.", ""], ["Stanovsky", "Gabriel", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2004.13702", "submitter": "Russa Biswas", "authors": "Russa Biswas, Radina Sofronova, Mehwish Alam, Harald Sack", "title": "Entity Type Prediction in Knowledge Graphs using Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Open Knowledge Graphs (such as DBpedia, Wikidata, YAGO) have been recognized\nas the backbone of diverse applications in the field of data mining and\ninformation retrieval. Hence, the completeness and correctness of the Knowledge\nGraphs (KGs) are vital. Most of these KGs are mostly created either via an\nautomated information extraction from Wikipedia snapshots or information\naccumulation provided by the users or using heuristics. However, it has been\nobserved that the type information of these KGs is often noisy, incomplete, and\nincorrect. To deal with this problem a multi-label classification approach is\nproposed in this work for entity typing using KG embeddings. We compare our\napproach with the current state-of-the-art type prediction method and report on\nexperiments with the KGs.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:57:08 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 14:16:54 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Biswas", "Russa", ""], ["Sofronova", "Radina", ""], ["Alam", "Mehwish", ""], ["Sack", "Harald", ""]]}, {"id": "2004.13703", "submitter": "Adam Tsakalidis", "authors": "Adam Tsakalidis and Maria Liakata", "title": "Autoencoding Word Representations through Time for Semantic Change\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic change detection concerns the task of identifying words whose\nmeaning has changed over time. The current state-of-the-art detects the level\nof semantic change in a word by comparing its vector representation in two\ndistinct time periods, without considering its evolution through time. In this\nwork, we propose three variants of sequential models for detecting semantically\nshifted words, effectively accounting for the changes in the word\nrepresentations over time, in a temporally sensitive manner. Through extensive\nexperimentation under various settings with both synthetic and real data we\nshowcase the importance of sequential modelling of word vectors through time\nfor detecting the words whose semantics have changed the most. Finally, we take\na step towards comparing different approaches in a quantitative manner,\ndemonstrating that the temporal modelling of word representations yields a\nclear-cut advantage in performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:58:14 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Tsakalidis", "Adam", ""], ["Liakata", "Maria", ""]]}, {"id": "2004.13705", "submitter": "Raphael Tang", "authors": "Raphael Tang, Jaejun Lee, Ji Xin, Xinyu Liu, Yaoliang Yu, Jimmy Lin", "title": "Showing Your Work Doesn't Always Work", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural language processing, a recently popular line of work explores how\nto best report the experimental results of neural networks. One exemplar\npublication, titled \"Show Your Work: Improved Reporting of Experimental\nResults,\" advocates for reporting the expected validation effectiveness of the\nbest-tuned model, with respect to the computational budget. In the present\nwork, we critically examine this paper. As far as statistical generalizability\nis concerned, we find unspoken pitfalls and caveats with this approach. We\nanalytically show that their estimator is biased and uses error-prone\nassumptions. We find that the estimator favors negative errors and yields poor\nbootstrapped confidence intervals. We derive an unbiased alternative and\nbolster our claims with empirical evidence from statistical simulation. Our\ncodebase is at http://github.com/castorini/meanmax.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:59:01 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Tang", "Raphael", ""], ["Lee", "Jaejun", ""], ["Xin", "Ji", ""], ["Liu", "Xinyu", ""], ["Yu", "Yaoliang", ""], ["Lin", "Jimmy", ""]]}, {"id": "2004.13717", "submitter": "Neslihan Suzen", "authors": "Neslihan Suzen, Evgeny M. Mirkes, Alexander N. Gorban", "title": "Informational Space of Meaning for Scientific Texts", "comments": "320 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Natural Language Processing, automatic extracting the meaning of texts\nconstitutes an important problem. Our focus is the computational analysis of\nmeaning of short scientific texts (abstracts or brief reports). In this paper,\na vector space model is developed for quantifying the meaning of words and\ntexts. We introduce the Meaning Space, in which the meaning of a word is\nrepresented by a vector of Relative Information Gain (RIG) about the subject\ncategories that the text belongs to, which can be obtained from observing the\nword in the text. This new approach is applied to construct the Meaning Space\nbased on Leicester Scientific Corpus (LSC) and Leicester Scientific\nDictionary-Core (LScDC). The LSC is a scientific corpus of 1,673,350 abstracts\nand the LScDC is a scientific dictionary which words are extracted from the\nLSC. Each text in the LSC belongs to at least one of 252 subject categories of\nWeb of Science (WoS). These categories are used in construction of vectors of\ninformation gains. The Meaning Space is described and statistically analysed\nfor the LSC with the LScDC. The usefulness of the proposed representation model\nis evaluated through top-ranked words in each category. The most informative n\nwords are ordered. We demonstrated that RIG-based word ranking is much more\nuseful than ranking based on raw word frequency in determining the\nscience-specific meaning and importance of a word. The proposed model based on\nRIG is shown to have ability to stand out topic-specific words in categories.\nThe most informative words are presented for 252 categories. The new scientific\ndictionary and the 103,998 x 252 Word-Category RIG Matrix are available online.\nAnalysis of the Meaning Space provides us with a tool to further explore\nquantifying the meaning of a text using more complex and context-dependent\nmeaning models that use co-occurrence of words and their combinations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 14:26:12 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Suzen", "Neslihan", ""], ["Mirkes", "Evgeny M.", ""], ["Gorban", "Alexander N.", ""]]}, {"id": "2004.13780", "submitter": "Shah Nawaz", "authors": "Muhammad Saad Saeed, Shah Nawaz, Pietro Morerio, Arif Mahmood, Ignazio\n  Gallo, Muhammad Haroon Yousaf, and Alessio Del Bue", "title": "Cross-modal Speaker Verification and Recognition: A Multilingual\n  Perspective", "comments": "Accepted: CVPRW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen a surge in finding association between faces and\nvoices within a cross-modal biometric application along with speaker\nrecognition. Inspired from this, we introduce a challenging task in\nestablishing association between faces and voices across multiple languages\nspoken by the same set of persons. The aim of this paper is to answer two\nclosely related questions: \"Is face-voice association language independent?\"\nand \"Can a speaker be recognised irrespective of the spoken language?\". These\ntwo questions are very important to understand effectiveness and to boost\ndevelopment of multilingual biometric systems. To answer them, we collected a\nMultilingual Audio-Visual dataset, containing human speech clips of $154$\nidentities with $3$ language annotations extracted from various videos uploaded\nonline. Extensive experiments on the three splits of the proposed dataset have\nbeen performed to investigate and answer these novel research questions that\nclearly point out the relevance of the multilingual problem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 19:15:23 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 15:10:21 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Saeed", "Muhammad Saad", ""], ["Nawaz", "Shah", ""], ["Morerio", "Pietro", ""], ["Mahmood", "Arif", ""], ["Gallo", "Ignazio", ""], ["Yousaf", "Muhammad Haroon", ""], ["Del Bue", "Alessio", ""]]}, {"id": "2004.13781", "submitter": "Shu Cheng Li", "authors": "Shucheng Li, Lingfei Wu, Shiwei Feng, Fangli Xu, Fengyuan Xu and Sheng\n  Zhong", "title": "Graph-to-Tree Neural Networks for Learning Structured Input-Output\n  Translation with Applications to Semantic Parsing and Math Word Problem", "comments": "Long Paper in EMNLP 2020. 12 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The celebrated Seq2Seq technique and its numerous variants achieve excellent\nperformance on many tasks such as neural machine translation, semantic parsing,\nand math word problem solving. However, these models either only consider input\nobjects as sequences while ignoring the important structural information for\nencoding, or they simply treat output objects as sequence outputs instead of\nstructural objects for decoding. In this paper, we present a novel\nGraph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder\nand a hierarchical tree decoder, that encodes an augmented graph-structured\ninput and decodes a tree-structured output. In particular, we investigated our\nmodel for solving two problems, neural semantic parsing and math word problem.\nOur extensive experiments demonstrate that our Graph2Tree model outperforms or\nmatches the performance of other state-of-the-art models on these tasks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:36:38 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 09:07:57 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Li", "Shucheng", ""], ["Wu", "Lingfei", ""], ["Feng", "Shiwei", ""], ["Xu", "Fangli", ""], ["Xu", "Fengyuan", ""], ["Zhong", "Sheng", ""]]}, {"id": "2004.13783", "submitter": "Shadi Shahsavari", "authors": "Shadi Shahsavari, Pavan Holur, Timothy R. Tangherlini, Vwani\n  Roychowdhury", "title": "Conspiracy in the Time of Corona: Automatic detection of Covid-19\n  Conspiracy Theories in Social Media and the News", "comments": "Covid-19, Corona virus, conspiracy theories, 5G, Bill Gates, China,\n  bio-weapons, rumor, narrative, machinelearning, social media, 4Chan, Reddit,\n  networks, data visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rumors and conspiracy theories thrive in environments of low confidence and\nlow trust. Consequently, it is not surprising that ones related to the Covid-19\npandemic are proliferating given the lack of any authoritative scientific\nconsensus on the virus, its spread and containment, or on the long term social\nand economic ramifications of the pandemic. Among the stories currently\ncirculating are ones suggesting that the 5G network activates the virus, that\nthe pandemic is a hoax perpetrated by a global cabal, that the virus is a\nbio-weapon released deliberately by the Chinese, or that Bill Gates is using it\nas cover to launch a global surveillance regime. While some may be quick to\ndismiss these stories as having little impact on real-world behavior, recent\nevents including the destruction of property, racially fueled attacks against\nAsian Americans, and demonstrations espousing resistance to public health\norders countermand such conclusions. Inspired by narrative theory, we crawl\nsocial media sites and news reports and, through the application of automated\nmachine-learning methods, discover the underlying narrative frameworks\nsupporting the generation of these stories. We show how the various narrative\nframeworks fueling rumors and conspiracy theories rely on the alignment of\notherwise disparate domains of knowledge, and consider how they attach to the\nbroader reporting on the pandemic. These alignments and attachments, which can\nbe monitored in near real-time, may be useful for identifying areas in the news\nthat are particularly vulnerable to reinterpretation by conspiracy theorists.\nUnderstanding the dynamics of storytelling on social media and the narrative\nframeworks that provide the generative basis for these stories may also be\nhelpful for devising methods to disrupt their spread.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 19:27:48 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Shahsavari", "Shadi", ""], ["Holur", "Pavan", ""], ["Tangherlini", "Timothy R.", ""], ["Roychowdhury", "Vwani", ""]]}, {"id": "2004.13786", "submitter": "Shanchan Wu", "authors": "Shanchan Wu and Kai Fan", "title": "A Practical Framework for Relation Extraction with Noisy Labels Based on\n  Doubly Transitional Loss", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Either human annotation or rule based automatic labeling is an effective\nmethod to augment data for relation extraction. However, the inevitable wrong\nlabeling problem for example by distant supervision may deteriorate the\nperformance of many existing methods. To address this issue, we introduce a\npractical end-to-end deep learning framework, including a standard feature\nextractor and a novel noisy classifier with our proposed doubly transitional\nmechanism. One transition is basically parameterized by a non-linear\ntransformation between hidden layers that implicitly represents the conversion\nbetween the true and noisy labels, and it can be readily optimized together\nwith other model parameters. Another is an explicit probability transition\nmatrix that captures the direct conversion between labels but needs to be\nderived from an EM algorithm. We conduct experiments on the NYT dataset and\nSemEval 2018 Task 7. The empirical results show comparable or better\nperformance over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 19:38:20 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Wu", "Shanchan", ""], ["Fan", "Kai", ""]]}, {"id": "2004.13796", "submitter": "Qingyang Wu", "authors": "Qingyang Wu, Lei Li, Zhou Yu", "title": "TextGAIL: Generative Adversarial Imitation Learning for Text Generation", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) for text generation have recently\nreceived many criticisms, as they perform worse than their MLE counterparts. We\nsuspect previous text GANs' inferior performance is due to the lack of a\nreliable guiding signal in their discriminators. To address this problem, we\npropose a generative adversarial imitation learning framework for text\ngeneration that uses large pre-trained language models to provide more reliable\nreward guidance. Our approach uses contrastive discriminator, and proximal\npolicy optimization (PPO) to stabilize and improve text generation performance.\nFor evaluation, we conduct experiments on a diverse set of unconditional and\nconditional text generation tasks. Experimental results show that TextGAIL\nachieves better performance in terms of both quality and diversity than the MLE\nbaseline. We also validate our intuition that TextGAIL's discriminator\ndemonstrates the capability of providing reasonable rewards with an additional\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 00:24:35 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 23:22:07 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 02:40:09 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 19:32:42 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wu", "Qingyang", ""], ["Li", "Lei", ""], ["Yu", "Zhou", ""]]}, {"id": "2004.13805", "submitter": "Taeuk Kim", "authors": "Taeuk Kim, Bowen Li, Sang-goo Lee", "title": "Multilingual Chart-based Constituency Parse Extraction from Pre-trained\n  Language Models", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As it has been unveiled that pre-trained language models (PLMs) are to some\nextent capable of recognizing syntactic concepts in natural language, much\neffort has been made to develop a method for extracting complete (binary)\nparses from PLMs without training separate parsers. We improve upon this\nparadigm by proposing a novel chart-based method and an effective top-K\nensemble technique. Moreover, we demonstrate that we can broaden the scope of\napplication of the approach into multilingual settings. Specifically, we show\nthat by applying our method on multilingual PLMs, it becomes possible to induce\nnon-trivial parses for sentences from nine languages in an integrated and\nlanguage-agnostic manner, attaining performance superior or comparable to that\nof unsupervised PCFGs. We also verify that our approach is robust to\ncross-lingual transfer. Finally, we provide analyses on the inner workings of\nour method. For instance, we discover universal attention heads which are\nconsistently sensitive to syntactic information irrespective of the input\nlanguage.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 05:42:26 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 05:40:25 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 03:52:38 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kim", "Taeuk", ""], ["Li", "Bowen", ""], ["Lee", "Sang-goo", ""]]}, {"id": "2004.13816", "submitter": "Hu Xu", "authors": "Hu Xu, Bing Liu, Lei Shu, Philip S. Yu", "title": "DomBERT: Domain-oriented Language Model for Aspect-based Sentiment\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on learning domain-oriented language models driven by end\ntasks, which aims to combine the worlds of both general-purpose language models\n(such as ELMo and BERT) and domain-specific language understanding. We propose\nDomBERT, an extension of BERT to learn from both in-domain corpus and relevant\ndomain corpora. This helps in learning domain language models with\nlow-resources. Experiments are conducted on an assortment of tasks in\naspect-based sentiment analysis, demonstrating promising results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 21:07:32 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Xu", "Hu", ""], ["Liu", "Bing", ""], ["Shu", "Lei", ""], ["Yu", "Philip S.", ""]]}, {"id": "2004.13818", "submitter": "Long Xuan Ma", "authors": "Longxuan Ma and Wei-Nan Zhang and Mingda Li and Ting Liu", "title": "A Survey of Document Grounded Dialogue Systems (DGDS)", "comments": "30 pages, 4 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue system (DS) attracts great attention from industry and academia\nbecause of its wide application prospects. Researchers usually divide the DS\naccording to the function. However, many conversations require the DS to switch\nbetween different functions. For example, movie discussion can change from\nchit-chat to QA, the conversational recommendation can transform from chit-chat\nto recommendation, etc. Therefore, classification according to functions may\nnot be enough to help us appreciate the current development trend. We classify\nthe DS based on background knowledge. Specifically, study the latest DS based\non the unstructured document(s). We define Document Grounded Dialogue System\n(DGDS) as the DS that the dialogues are centering on the given document(s). The\nDGDS can be used in scenarios such as talking over merchandise against product\nManual, commenting on news reports, etc. We believe that extracting\nunstructured document(s) information is the future trend of the DS because a\ngreat amount of human knowledge lies in these document(s). The research of the\nDGDS not only possesses a broad application prospect but also facilitates AI to\nbetter understand human knowledge and natural language. We analyze the\nclassification, architecture, datasets, models, and future development trends\nof the DGDS, hoping to help researchers in this field.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 03:22:28 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Ma", "Longxuan", ""], ["Zhang", "Wei-Nan", ""], ["Li", "Mingda", ""], ["Liu", "Ting", ""]]}, {"id": "2004.13819", "submitter": "Shivansh Rao", "authors": "Himanshu Choudhary, Shivansh Rao, Rajesh Rohilla", "title": "Neural Machine Translation for Low-Resourced Indian Languages", "comments": "Conference paper accepted to LREC-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A large number of significant assets are available online in English, which\nis frequently translated into native languages to ease the information sharing\namong local people who are not much familiar with English. However, manual\ntranslation is a very tedious, costly, and time-taking process. To this end,\nmachine translation is an effective approach to convert text to a different\nlanguage without any human involvement. Neural machine translation (NMT) is one\nof the most proficient translation techniques amongst all existing machine\ntranslation systems. In this paper, we have applied NMT on two of the most\nmorphological rich Indian languages, i.e. English-Tamil and English-Malayalam.\nWe proposed a novel NMT model using Multihead self-attention along with\npre-trained Byte-Pair-Encoded (BPE) and MultiBPE embeddings to develop an\nefficient translation system that overcomes the OOV (Out Of Vocabulary) problem\nfor low resourced morphological rich Indian languages which do not have much\ntranslation available online. We also collected corpus from different sources,\naddressed the issues with these publicly available data and refined them for\nfurther uses. We used the BLEU score for evaluating our system performance.\nExperimental results and survey confirmed that our proposed translator (24.34\nand 9.78 BLEU score) outperforms Google translator (9.40 and 5.94 BLEU score)\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 17:29:34 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Choudhary", "Himanshu", ""], ["Rao", "Shivansh", ""], ["Rohilla", "Rajesh", ""]]}, {"id": "2004.13820", "submitter": "Dhivya Chandrasekaran", "authors": "Dhivya Chandrasekaran and Vijay Mago", "title": "Evolution of Semantic Similarity -- A Survey", "comments": "29 pages, 5 figures, submitted to \"ACM Computing Survey\"", "journal-ref": "ACM Computing Surveys 54(2):1-37 (2021)", "doi": "10.1145/3440755", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the semantic similarity between text data is one of the\nchallenging and open research problems in the field of Natural Language\nProcessing (NLP). The versatility of natural language makes it difficult to\ndefine rule-based methods for determining semantic similarity measures. In\norder to address this issue, various semantic similarity methods have been\nproposed over the years. This survey article traces the evolution of such\nmethods, categorizing them based on their underlying principles as\nknowledge-based, corpus-based, deep neural network-based methods, and hybrid\nmethods. Discussing the strengths and weaknesses of each method, this survey\nprovides a comprehensive view of existing systems in place, for new researchers\nto experiment and develop innovative ideas to address the issue of semantic\nsimilarity.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 22:07:39 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 15:57:06 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chandrasekaran", "Dhivya", ""], ["Mago", "Vijay", ""]]}, {"id": "2004.13821", "submitter": "GuanMing Xiong", "authors": "Guanming Xiong", "title": "Fine-tuning Multi-hop Question Answering with Hierarchical Graph Network", "comments": "the experience result is not as good as I except", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a two stage model for multi-hop question answering.\nThe first stage is a hierarchical graph network, which is used to reason over\nmulti-hop question and is capable to capture different levels of granularity\nusing the nature structure(i.e., paragraphs, questions, sentences and entities)\nof documents. The reasoning process is convert to node classify task(i.e.,\nparagraph nodes and sentences nodes). The second stage is a language model\nfine-tuning task. In a word, stage one use graph neural network to select and\nconcatenate support sentences as one paragraph, and stage two find the answer\nspan in language model fine-tuning paradigm.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:34:16 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 03:33:21 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Xiong", "Guanming", ""]]}, {"id": "2004.13822", "submitter": "Simon Vandenhende", "authors": "Simon Vandenhende, Thierry Deruyttere and Dusan Grujicic", "title": "A Baseline for the Commands For Autonomous Vehicles Challenge", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Commands For Autonomous Vehicles (C4AV) challenge requires participants\nto solve an object referral task in a real-world setting. More specifically, we\nconsider a scenario where a passenger can pass free-form natural language\ncommands to a self-driving car. This problem is particularly challenging, as\nthe language is much less constrained compared to existing benchmarks, and\nobject references are often implicit. The challenge is based on the recent\n\\texttt{Talk2Car} dataset. This document provides a technical overview of a\nmodel that we released to help participants get started in the competition. The\ncode can be found at https://github.com/talk2car/Talk2Car.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 13:35:47 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Vandenhende", "Simon", ""], ["Deruyttere", "Thierry", ""], ["Grujicic", "Dusan", ""]]}, {"id": "2004.13823", "submitter": "Xiangpeng Wan", "authors": "Xiangpeng Wan, Hakim Ghazzai, and Yehia Massoud", "title": "Leveraging Personal Navigation Assistant Systems Using Automated Social\n  Media Traffic Reporting", "comments": "This paper is accepted for publication in IEEE Technology Engineering\n  Management Society International Conference (TEMSCON'20), Metro Detroit,\n  Michigan (USA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern urbanization is demanding smarter technologies to improve a variety of\napplications in intelligent transportation systems to relieve the increasing\namount of vehicular traffic congestion and incidents. Existing incident\ndetection techniques are limited to the use of sensors in the transportation\nnetwork and hang on human-inputs. Despite of its data abundance, social media\nis not well-exploited in such context. In this paper, we develop an automated\ntraffic alert system based on Natural Language Processing (NLP) that filters\nthis flood of information and extract important traffic-related bullets. To\nthis end, we employ the fine-tuning Bidirectional Encoder Representations from\nTransformers (BERT) language embedding model to filter the related traffic\ninformation from social media. Then, we apply a question-answering model to\nextract necessary information characterizing the report event such as its exact\nlocation, occurrence time, and nature of the events. We demonstrate the adopted\nNLP approaches outperform other existing approach and, after effectively\ntraining them, we focus on real-world situation and show how the developed\napproach can, in real-time, extract traffic-related information and\nautomatically convert them into alerts for navigation assistance applications\nsuch as navigation apps.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 02:26:06 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Wan", "Xiangpeng", ""], ["Ghazzai", "Hakim", ""], ["Massoud", "Yehia", ""]]}, {"id": "2004.13826", "submitter": "Yufeng Zhang", "authors": "Yufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu, Zhongzhen Wen and Liang Wang", "title": "Every Document Owns Its Structure: Inductive Text Classification via\n  Graph Neural Networks", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is fundamental in natural language processing (NLP), and\nGraph Neural Networks (GNN) are recently applied in this task. However, the\nexisting graph-based works can neither capture the contextual word\nrelationships within each document nor fulfil the inductive learning of new\nwords. In this work, to overcome such problems, we propose TextING for\ninductive text classification via GNN. We first build individual graphs for\neach document and then use GNN to learn the fine-grained word representations\nbased on their local structures, which can also effectively produce embeddings\nfor unseen words in the new document. Finally, the word nodes are aggregated as\nthe document embedding. Extensive experiments on four benchmark datasets show\nthat our method outperforms state-of-the-art text classification methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 07:23:47 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 08:28:27 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhang", "Yufeng", ""], ["Yu", "Xueli", ""], ["Cui", "Zeyu", ""], ["Wu", "Shu", ""], ["Wen", "Zhongzhen", ""], ["Wang", "Liang", ""]]}, {"id": "2004.13828", "submitter": "Prabhakar Gupta", "authors": "Prabhakar Gupta and Anil Nelakanti", "title": "DeepSubQE: Quality estimation for subtitle translations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Quality estimation (QE) for tasks involving language data is hard owing to\nnumerous aspects of natural language like variations in paraphrasing, style,\ngrammar, etc. There can be multiple answers with varying levels of\nacceptability depending on the application at hand. In this work, we look at\nestimating quality of translations for video subtitles. We show how existing QE\nmethods are inadequate and propose our method DeepSubQE as a system to estimate\nquality of translation given subtitles data for a pair of languages. We rely on\nvarious data augmentation strategies for automated labelling and synthesis for\ntraining. We create a hybrid network which learns semantic and syntactic\nfeatures of bilingual data and compare it with only-LSTM and only-CNN networks.\nOur proposed network outperforms them by significant margin.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 09:41:15 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Gupta", "Prabhakar", ""], ["Nelakanti", "Anil", ""]]}, {"id": "2004.13829", "submitter": "Makoto Nakatsuji Ph. D.", "authors": "Makoto Nakatsuji, Sohei Okui", "title": "Answer Generation through Unified Memories over Multiple Passages", "comments": "IJCAI-2020 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading comprehension methods that generate answers by referring to\nmultiple passages for a question have gained much attention in AI and NLP\ncommunities. The current methods, however, do not investigate the relationships\namong multiple passages in the answer generation process, even though topics\ncorrelated among the passages may be answer candidates. Our method, called\nneural answer Generation through Unified Memories over Multiple Passages\n(GUM-MP), solves this problem as follows. First, it determines which tokens in\nthe passages are matched to the question. In particular, it investigates\nmatches between tokens in positive passages, which are assigned to the\nquestion, and those in negative passages, which are not related to the\nquestion. Next, it determines which tokens in the passage are matched to other\npassages assigned to the same question and at the same time it investigates the\ntopics in which they are matched. Finally, it encodes the token sequences with\nthe above two matching results into unified memories in the passage encoders\nand learns the answer sequence by using an encoder-decoder with a\nmultiple-pointer-generator mechanism. As a result, GUM-MP can generate answers\nby pointing to important tokens present across passages. Evaluations indicate\nthat GUM-MP generates much more accurate results than the current models do.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 11:46:40 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Nakatsuji", "Makoto", ""], ["Okui", "Sohei", ""]]}, {"id": "2004.13831", "submitter": "Vid Kocijan", "authors": "Vid Kocijan, Thomas Lukasiewicz, Ernest Davis, Gary Marcus, Leora\n  Morgenstern", "title": "A Review of Winograd Schema Challenge Datasets and Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Winograd Schema Challenge is both a commonsense reasoning and natural\nlanguage understanding challenge, introduced as an alternative to the Turing\ntest. A Winograd schema is a pair of sentences differing in one or two words\nwith a highly ambiguous pronoun, resolved differently in the two sentences,\nthat appears to require commonsense knowledge to be resolved correctly. The\nexamples were designed to be easily solvable by humans but difficult for\nmachines, in principle requiring a deep understanding of the content of the\ntext and the situation it describes. This paper reviews existing Winograd\nSchema Challenge benchmark datasets and approaches that have been published\nsince its introduction.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 08:40:11 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Kocijan", "Vid", ""], ["Lukasiewicz", "Thomas", ""], ["Davis", "Ernest", ""], ["Marcus", "Gary", ""], ["Morgenstern", "Leora", ""]]}, {"id": "2004.13832", "submitter": "Luca Mariot", "authors": "Luca Manzoni, Domagoj Jakobovic, Luca Mariot, Stjepan Picek, Mauro\n  Castelli", "title": "Towards an evolutionary-based approach for natural language processing", "comments": "18 pages, 7 figures, 2 tables. Accepted for publication at the\n  Genetic and Evolutionary Computation Conference (GECCO 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tasks related to Natural Language Processing (NLP) have recently been the\nfocus of a large research endeavor by the machine learning community. The\nincreased interest in this area is mainly due to the success of deep learning\nmethods. Genetic Programming (GP), however, was not under the spotlight with\nrespect to NLP tasks. Here, we propose a first proof-of-concept that combines\nGP with the well established NLP tool word2vec for the next word prediction\ntask. The main idea is that, once words have been moved into a vector space,\ntraditional GP operators can successfully work on vectors, thus producing\nmeaningful words as the output. To assess the suitability of this approach, we\nperform an experimental evaluation on a set of existing newspaper headlines.\nIndividuals resulting from this (pre-)training phase can be employed as the\ninitial population in other NLP tasks, like sentence generation, which will be\nthe focus of future investigations, possibly employing adversarial\nco-evolutionary approaches.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 18:44:12 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Manzoni", "Luca", ""], ["Jakobovic", "Domagoj", ""], ["Mariot", "Luca", ""], ["Picek", "Stjepan", ""], ["Castelli", "Mauro", ""]]}, {"id": "2004.13833", "submitter": "Jing Gu", "authors": "Jing Gu, Zhou Yu", "title": "Data Annealing for Informal Language Understanding Tasks", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a huge performance gap between formal and informal language\nunderstanding tasks. The recent pre-trained models that improved the\nperformance of formal language understanding tasks did not achieve a comparable\nresult on informal language. We pro-pose a data annealing transfer learning\nprocedure to bridge the performance gap on informal natural language\nunderstanding tasks. It successfully utilizes a pre-trained model such as BERT\nin informal language. In our data annealing procedure, the training set\ncontains mainly formal text data at first; then, the proportion of the informal\ntext data is gradually increased during the training process. Our data\nannealing procedure is model-independent and can be applied to various tasks.\nWe validate its effectiveness in exhaustive experiments. When BERT is\nimplemented with our learning procedure, it outperforms all the\nstate-of-the-art models on the three common informal language tasks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:27:09 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Gu", "Jing", ""], ["Yu", "Zhou", ""]]}, {"id": "2004.13835", "submitter": "Jing Gu", "authors": "Jing Gu, Qingyang Wu, Chongruo Wu, Weiyan Shi, Zhou Yu", "title": "A Tailored Pre-Training Model for Task-Oriented Dialog Generation", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of large pre-trained language models such as BERT and\nGPT-2 has suggested the effectiveness of incorporating language priors in\ndownstream dialog generation tasks. However, the performance of pre-trained\nmodels on the dialog task is not as optimal as expected. In this paper, we\npropose a Pre-trained Role Alternating Language model (PRAL), designed\nspecifically for task-oriented conversational systems. We adopted (Wu et al.,\n2019) that models two speakers separately. We also design several techniques,\nsuch as start position randomization, knowledge distillation, and history\ndiscount to improve pre-training performance. We introduce a task-oriented\ndialog pretraining dataset by cleaning 13 existing data sets. We test PRAL on\nthree different downstream tasks. The results show that PRAL performs better or\non par with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:25:45 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Gu", "Jing", ""], ["Wu", "Qingyang", ""], ["Wu", "Chongruo", ""], ["Shi", "Weiyan", ""], ["Yu", "Zhou", ""]]}, {"id": "2004.13838", "submitter": "Pourya Vakilipourtakalou", "authors": "Pourya Vakilipourtakalou, Lili Mou", "title": "How Chaotic Are Recurrent Neural Networks?", "comments": "ICLR 2020 Workshop DeepDiffEq", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are non-linear dynamic systems. Previous\nwork believes that RNN may suffer from the phenomenon of chaos, where the\nsystem is sensitive to initial states and unpredictable in the long run. In\nthis paper, however, we perform a systematic empirical analysis, showing that a\nvanilla or long short term memory (LSTM) RNN does not exhibit chaotic behavior\nalong the training process in real applications such as text generation. Our\nfindings suggest that future work in this direction should address the other\nside of non-linear dynamics for RNN.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 21:14:38 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Vakilipourtakalou", "Pourya", ""], ["Mou", "Lili", ""]]}, {"id": "2004.13839", "submitter": "Louis Falissard", "authors": "Louis Falissard, Claire Morgand, Sylvie Roussel, Claire Imbaud, Walid\n  Ghosn, Karim Bounebache, Gr\\'egoire Rey", "title": "Neural translation and automated recognition of ICD10 medical entities\n  from natural language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of medical entities from natural language is an ubiquitous\nproblem in the medical field, with applications ranging from medical act coding\nto the analysis of electronic health data for public health. It is however a\ncomplex task usually requiring human expert intervention, thus making it\nexpansive and time consuming. The recent advances in artificial intelligence,\nspecifically the raise of deep learning methods, has enabled computers to make\nefficient decisions on a number of complex problems, with the notable example\nof neural sequence models and their powerful applications in natural language\nprocessing. They however require a considerable amount of data to learn from,\nwhich is typically their main limiting factor. However, the C\\'epiDc stores an\nexhaustive database of death certificates at the French national scale,\namounting to several millions of natural language examples provided with their\nassociated human coded medical entities available to the machine learning\npractitioner. This article investigates the applications of deep neural\nsequence models to the medical entity recognition from natural language\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 18:17:53 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 10:30:24 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Falissard", "Louis", ""], ["Morgand", "Claire", ""], ["Roussel", "Sylvie", ""], ["Imbaud", "Claire", ""], ["Ghosn", "Walid", ""], ["Bounebache", "Karim", ""], ["Rey", "Gr\u00e9goire", ""]]}, {"id": "2004.13840", "submitter": "Sileye Ba", "authors": "Lo Alla and Dione Cheikh Bamba and Nguer Elhadji Mamadou and Ba Sileye\n  O. Ba and Lo Moussa", "title": "Using LSTM to Translate French to Senegalese Local Languages: Wolof as a\n  Case Study", "comments": "4 pages, 2 tables, ICLR AfricaNLP2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a neural machine translation system for Wolof, a\nlow-resource Niger-Congo language. First we gathered a parallel corpus of 70000\naligned French-Wolof sentences. Then we developped a baseline LSTM based\nencoder-decoder architecture which was further extended to bidirectional LSTMs\nwith attention mechanisms. Our models are trained on a limited amount of\nparallel French-Wolof data of approximately 35000 parallel sentences.\nExperimental results on French-Wolof translation tasks show that our approach\nproduces promising translations in extremely low-resource conditions. The best\nmodel was able to achieve a good performance of 47% BLEU score.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 17:09:52 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Alla", "Lo", ""], ["Bamba", "Dione Cheikh", ""], ["Mamadou", "Nguer Elhadji", ""], ["Ba", "Ba Sileye O.", ""], ["Moussa", "Lo", ""]]}, {"id": "2004.13841", "submitter": "Michael Franklin Mbouopda", "authors": "Michael Franklin Mbouopda, Paulin Melatagia Yonta and Guy Stephane B.\n  Fedim Lombo", "title": "Neurals Networks for Projecting Named Entities from English to Ewondo", "comments": null, "journal-ref": null, "doi": null, "report-no": "2004.13841", "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition is an important task in natural language processing.\nIt is very well studied for rich language, but still under explored for\nlow-resource languages. The main reason is that the existing techniques\nrequired a lot of annotated data to reach good performance. Recently, a new\ndistributional representation of words has been proposed to project named\nentities from a rich language to a low-resource one. This representation has\nbeen coupled to a neural network in order to project named entities from\nEnglish to Ewondo, a Bantu language spoken in Cameroon. Although the proposed\nmethod reached appreciable results, the size of the used neural network was too\nlarge compared to the size of the dataset. Furthermore the impact of the model\nparameters has not been studied. In this paper, we show experimentally that the\nsame results can be obtained using a smaller neural network. We also emphasize\nthe parameters that are highly correlated to the network performance. This work\nis a step forward to build a reliable and robust network architecture for named\nentity projection in low resource languages.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 22:05:30 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Mbouopda", "Michael Franklin", ""], ["Yonta", "Paulin Melatagia", ""], ["Lombo", "Guy Stephane B. Fedim", ""]]}, {"id": "2004.13842", "submitter": "Vukosi Marivate", "authors": "Vukosi Marivate, Tshephisho Sefara, Vongani Chabalala, Keamogetswe\n  Makhaya, Tumisho Mokgonyane, Rethabile Mokoena, Abiodun Modupe", "title": "Low resource language dataset creation, curation and classification:\n  Setswana and Sepedi -- Extended Abstract", "comments": "Accepted for the AfricaNLP workshop at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The recent advances in Natural Language Processing have only been a boon for\nwell represented languages, negating research in lesser known global languages.\nThis is in part due to the availability of curated data and research resources.\nOne of the current challenges concerning low-resourced languages are clear\nguidelines on the collection, curation and preparation of datasets for\ndifferent use-cases. In this work, we take on the task of creating two datasets\nthat are focused on news headlines (i.e short text) for Setswana and Sepedi and\nthe creation of a news topic classification task from these datasets. In this\nstudy, we document our work, propose baselines for classification, and\ninvestigate an approach on data augmentation better suited to low-resourced\nlanguages in order to improve the performance of the classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 18:03:15 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Marivate", "Vukosi", ""], ["Sefara", "Tshephisho", ""], ["Chabalala", "Vongani", ""], ["Makhaya", "Keamogetswe", ""], ["Mokgonyane", "Tumisho", ""], ["Mokoena", "Rethabile", ""], ["Modupe", "Abiodun", ""]]}, {"id": "2004.13843", "submitter": "Ram G Athreya", "authors": "Ram G Athreya, Srividya Bansal, Axel-Cyrille Ngonga Ngomo, Ricardo\n  Usbeck", "title": "Template-based Question Answering using Recursive Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network-based approach to automatically learn and\nclassify natural language questions into its corresponding template using\nrecursive neural networks. An obvious advantage of using neural networks is the\nelimination of the need for laborious feature engineering that can be\ncumbersome and error-prone. The input question is encoded into a vector\nrepresentation. The model is trained and evaluated on the LC-QuAD dataset\n(Large-scale Complex Question Answering Dataset). The LC-QuAD queries are\nannotated based on 38 unique templates that the model attempts to classify. The\nresulting model is evaluated against both the LC-QuAD dataset and the 7th\nQuestion Answering Over Linked Data (QALD-7) dataset. The recursive neural\nnetwork achieves template classification accuracy of 0.828 on the LC-QuAD\ndataset and an accuracy of 0.618 on the QALD-7 dataset. When the top-2 most\nlikely templates were considered the model achieves an accuracy of 0.945 on the\nLC-QuAD dataset and 0.786 on the QALD-7 dataset. After slot filling, the\noverall system achieves a macro F-score 0.419 on the LC-QuAD dataset and a\nmacro F-score of 0.417 on the QALD-7 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 18:14:39 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 00:26:26 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 01:41:26 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Athreya", "Ram G", ""], ["Bansal", "Srividya", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""], ["Usbeck", "Ricardo", ""]]}, {"id": "2004.13844", "submitter": "Xin Liu", "authors": "Xin Liu, Qingcai Chen, Yan Liu, Joanna Siebert, Baotian Hu, Xiangping\n  Wu and Buzhou Tang", "title": "Decomposing Word Embedding with the Capsule Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word sense disambiguation tries to learn the appropriate sense of an\nambiguous word in a given context. The existing pre-trained language methods\nand the methods based on multi-embeddings of word did not explore the power of\nthe unsupervised word embedding sufficiently.\n  In this paper, we discuss a capsule network-based approach, taking advantage\nof capsule's potential for recognizing highly overlapping features and dealing\nwith segmentation. We propose a Capsule network-based method to Decompose the\nunsupervised word Embedding of an ambiguous word into context specific Sense\nembedding, called CapsDecE2S. In this approach, the unsupervised ambiguous\nembedding is fed into capsule network to produce its multiple morpheme-like\nvectors, which are defined as the basic semantic language units of meaning.\nWith attention operations, CapsDecE2S integrates the word context to\nreconstruct the multiple morpheme-like vectors into the context-specific sense\nembedding. To train CapsDecE2S, we propose a sense matching training method. In\nthis method, we convert the sense learning into a binary classification that\nexplicitly learns the relation between senses by the label of matching and\nnon-matching. The CapsDecE2S was experimentally evaluated on two sense learning\ntasks, i.e., word in context and word sense disambiguation. Results on two\npublic corpora Word-in-Context and English all-words Word Sense Disambiguation\nshow that, the CapsDecE2S model achieves the new state-of-the-art for the word\nin context and word sense disambiguation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 06:37:27 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 01:58:27 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Liu", "Xin", ""], ["Chen", "Qingcai", ""], ["Liu", "Yan", ""], ["Siebert", "Joanna", ""], ["Hu", "Baotian", ""], ["Wu", "Xiangping", ""], ["Tang", "Buzhou", ""]]}, {"id": "2004.13845", "submitter": "Yannis Papanikolaou", "authors": "Yannis Papanikolaou and Andrea Pierleoni", "title": "DARE: Data Augmented Relation Extraction with GPT-2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world Relation Extraction (RE) tasks are challenging to deal with,\neither due to limited training data or class imbalance issues. In this work, we\npresent Data Augmented Relation Extraction(DARE), a simple method to augment\ntraining data by properly fine-tuning GPT-2 to generate examples for specific\nrelation types. The generated training data is then used in combination with\nthe gold dataset to train a BERT-based RE classifier. In a series of\nexperiments we show the advantages of our method, which leads in improvements\nof up to 11 F1 score points against a strong base-line. Also, DARE achieves new\nstate of the art in three widely used biomedical RE datasets surpassing the\nprevious best results by 4.7 F1 points on average.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:38:36 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Papanikolaou", "Yannis", ""], ["Pierleoni", "Andrea", ""]]}, {"id": "2004.13846", "submitter": "Kenya Sakka", "authors": "Kenya Sakka, Kotaro Nakayama, Nisei Kimura, Taiki Inoue, Yusuke\n  Iwasawa, Ryohei Yamaguchi, Yosimasa Kawazoe, Kazuhiko Ohe, Yutaka Matsuo", "title": "Character-level Japanese Text Generation with Attention Mechanism for\n  Chest Radiography Diagnosis", "comments": "8 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiography is a general method for diagnosing a patient's condition\nand identifying important information; therefore, radiography is used\nextensively in routine medical practice in various situations, such as\nemergency medical care and medical checkup. However, a high level of expertise\nis required to interpret chest radiographs. Thus, medical specialists spend\nconsiderable time in diagnosing such huge numbers of radiographs. In order to\nsolve these problems, methods for generating findings have been proposed.\nHowever, the study of generating chest radiograph findings has primarily\nfocused on the English language, and to the best of our knowledge, no studies\nhave studied Japanese data on this subject. There are two challenges involved\nin generating findings in the Japanese language. The first challenge is that\nword splitting is difficult because the boundaries of Japanese word are not\nclear. The second challenge is that there are numerous orthographic variants.\nFor deal with these two challenges, we proposed an end-to-end model that\ngenerates Japanese findings at the character-level from chest radiographs. In\naddition, we introduced the attention mechanism to improve not only the\naccuracy, but also the interpretation ability of the results. We evaluated the\nproposed method using a public dataset with Japanese findings. The\neffectiveness of the proposed method was confirmed using the Bilingual\nEvaluation Understudy score. And, we were confirmed from the generated findings\nthat the proposed method was able to consider the orthographic variants.\nFurthermore, we confirmed via visual inspection that the attention mechanism\ncaptures the features and positional information of radiographs.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 18:19:27 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 05:37:51 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Sakka", "Kenya", ""], ["Nakayama", "Kotaro", ""], ["Kimura", "Nisei", ""], ["Inoue", "Taiki", ""], ["Iwasawa", "Yusuke", ""], ["Yamaguchi", "Ryohei", ""], ["Kawazoe", "Yosimasa", ""], ["Ohe", "Kazuhiko", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "2004.13847", "submitter": "Adly Templeton", "authors": "Adly Templeton", "title": "Inherently Interpretable Sparse Word Embeddings through Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are a powerful natural language processing technique, but\nthey are extremely difficult to interpret. In order to create more\ninterpretable word embeddings, we transform pretrained dense word embeddings\ninto sparse embeddings. These new embeddings are inherently interpretable: each\nof their dimensions are created from and represent a natural language word or\nspecific syntactic concept. We construct these embeddings through sparse\ncoding, where each vector in the basis set is itself a word embedding. We show\nthat models trained using these sparse embeddings can achieve good performance\nand are extremely interpretable.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 19:49:49 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Templeton", "Adly", ""]]}, {"id": "2004.13848", "submitter": "Honglei Liu", "authors": "Honglei Liu, Yan Xu, Zhiqiang Zhang, Ni Wang, Yanqun Huang, Yanjun Hu,\n  Zhenghan Yang, Rui Jiang, Hui Chen", "title": "A Natural Language Processing Pipeline of Chinese Free-text Radiology\n  Reports for Liver Cancer Diagnosis", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3020138", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid development of natural language processing (NLP)\nimplementation in electronic medical records (EMRs), Chinese EMRs processing\nremains challenging due to the limited corpus and specific grammatical\ncharacteristics, especially for radiology reports. In this study, we designed\nan NLP pipeline for the direct extraction of clinically relevant features from\nChinese radiology reports, which is the first key step in computer-aided\nradiologic diagnosis. The pipeline was comprised of named entity recognition,\nsynonyms normalization, and relationship extraction to finally derive the\nradiological features composed of one or more terms. In named entity\nrecognition, we incorporated lexicon into deep learning model bidirectional\nlong short-term memory-conditional random field (BiLSTM-CRF), and the model\nfinally achieved an F1 score of 93.00%. With the extracted radiological\nfeatures, least absolute shrinkage and selection operator and machine learning\nmethods (support vector machine, random forest, decision tree, and logistic\nregression) were used to build the classifiers for liver cancer prediction. For\nliver cancer diagnosis, random forest had the highest predictive performance in\nliver cancer diagnosis (F1 score 86.97%, precision 87.71%, and recall 86.25%).\nThis work was a comprehensive NLP study focusing on Chinese radiology reports\nand the application of NLP in cancer risk prediction. The proposed NLP pipeline\nfor the radiological feature extraction could be easily implemented in other\nkinds of Chinese clinical texts and other disease predictive tasks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 09:32:07 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 12:51:42 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Liu", "Honglei", ""], ["Xu", "Yan", ""], ["Zhang", "Zhiqiang", ""], ["Wang", "Ni", ""], ["Huang", "Yanqun", ""], ["Hu", "Yanjun", ""], ["Yang", "Zhenghan", ""], ["Jiang", "Rui", ""], ["Chen", "Hui", ""]]}, {"id": "2004.13850", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Fabian Brunn, Bj\\\"orn Schuller", "title": "Cross-lingual Zero- and Few-shot Hate Speech Detection Utilising Frozen\n  Transformer Language Models and AXEL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting hate speech, especially in low-resource languages, is a non-trivial\nchallenge. To tackle this, we developed a tailored architecture based on\nfrozen, pre-trained Transformers to examine cross-lingual zero-shot and\nfew-shot learning, in addition to uni-lingual learning, on the HatEval\nchallenge data set. With our novel attention-based classification block AXEL,\nwe demonstrate highly competitive results on the English and Spanish subsets.\nWe also re-sample the English subset, enabling additional, meaningful\ncomparisons in the future.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 09:58:33 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Stappen", "Lukas", ""], ["Brunn", "Fabian", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "2004.13851", "submitter": "Siqi Liu", "authors": "Siqi Liu", "title": "Sentiment Analysis of Yelp Reviews: A Comparison of Techniques and\n  Models", "comments": "7 pages, 12 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use over 350,000 Yelp reviews on 5,000 restaurants to perform an ablation\nstudy on text preprocessing techniques. We also compare the effectiveness of\nseveral machine learning and deep learning models on predicting user sentiment\n(negative, neutral, or positive). For machine learning models, we find that\nusing binary bag-of-word representation, adding bi-grams, imposing minimum\nfrequency constraints and normalizing texts have positive effects on model\nperformance. For deep learning models, we find that using pre-trained word\nembeddings and capping maximum length often boost model performance. Finally,\nusing macro F1 score as our comparison metric, we find simpler models such as\nLogistic Regression and Support Vector Machine to be more effective at\npredicting sentiments than more complex models such as Gradient Boosting, LSTM\nand BERT.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 18:50:49 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Liu", "Siqi", ""]]}, {"id": "2004.13852", "submitter": "Giannis Karamanolakis", "authors": "Giannis Karamanolakis, Jun Ma, Xin Luna Dong", "title": "TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product\n  Categories", "comments": "Accepted to ACL 2020 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting structured knowledge from product profiles is crucial for various\napplications in e-Commerce. State-of-the-art approaches for knowledge\nextraction were each designed for a single category of product, and thus do not\napply to real-life e-Commerce scenarios, which often contain thousands of\ndiverse categories. This paper proposes TXtract, a taxonomy-aware knowledge\nextraction model that applies to thousands of product categories organized in a\nhierarchical taxonomy. Through category conditional self-attention and\nmulti-task learning, our approach is both scalable, as it trains a single model\nfor thousands of categories, and effective, as it extracts category-specific\nattribute values. Experiments on products from a taxonomy with 4,000 categories\nshow that TXtract outperforms state-of-the-art approaches by up to 10% in F1\nand 15% in coverage across all categories.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 03:02:09 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 14:54:13 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Karamanolakis", "Giannis", ""], ["Ma", "Jun", ""], ["Dong", "Xin Luna", ""]]}, {"id": "2004.13876", "submitter": "Marcos Vin\\'icius Treviso", "authors": "Marcos V. Treviso and Andr\\'e F. T. Martins", "title": "The Explanation Game: Towards Prediction Explainability through Sparse\n  Communication", "comments": "BlackBoxNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainability is a topic of growing importance in NLP. In this work, we\nprovide a unified perspective of explainability as a communication problem\nbetween an explainer and a layperson about a classifier's decision. We use this\nframework to compare several prior approaches for extracting explanations,\nincluding gradient methods, representation erasure, and attention mechanisms,\nin terms of their communication success. In addition, we reinterpret these\nmethods at the light of classical feature selection, and we use this as\ninspiration to propose new embedded methods for explainability, through the use\nof selective, sparse attention. Experiments in text classification, natural\nlanguage entailment, and machine translation, using different configurations of\nexplainers and laypeople (including both machines and humans), reveal an\nadvantage of attention-based explainers over gradient and erasure methods.\nFurthermore, human evaluation experiments show promising results with post-hoc\nexplainers trained to optimize communication success and faithfulness.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 22:27:19 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 08:05:13 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Treviso", "Marcos V.", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "2004.13878", "submitter": "Antonios Maronikolakis", "authors": "Antonis Maronikolakis, Danae Sanchez Villegas, Daniel Preotiuc-Pietro,\n  Nikolaos Aletras", "title": "Analyzing Political Parody in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parody is a figurative device used to imitate an entity for comedic or\ncritical purposes and represents a widespread phenomenon in social media\nthrough many popular parody accounts. In this paper, we present the first\ncomputational study of parody. We introduce a new publicly available data set\nof tweets from real politicians and their corresponding parody accounts. We run\na battery of supervised machine learning models for automatically detecting\nparody tweets with an emphasis on robustness by testing on tweets from accounts\nunseen in training, across different genders and across countries. Our results\nshow that political parody tweets can be predicted with an accuracy up to 90%.\nFinally, we identify the markers of parody through a linguistic analysis.\nBeyond research in linguistics and political communication, accurately and\nautomatically detecting parody is important to improving fact checking for\njournalists and analytics such as sentiment analysis through filtering out\nparodical utterances.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 22:31:18 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 10:59:41 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Maronikolakis", "Antonis", ""], ["Villegas", "Danae Sanchez", ""], ["Preotiuc-Pietro", "Daniel", ""], ["Aletras", "Nikolaos", ""]]}, {"id": "2004.13886", "submitter": "Bradley Hauer", "authors": "Bradley Hauer, Grzegorz Kondrak", "title": "Synonymy = Translational Equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Synonymy and translational equivalence are the relations of sameness of\nmeaning within and across languages. As the principal relations in wordnets and\nmulti-wordnets, they are vital to computational lexical semantics, yet the\nfield suffers from the absence of a common formal framework to define their\nproperties and relationship. This paper proposes a unifying treatment of these\ntwo relations, which is validated by experiments on existing resources. In our\nview, synonymy and translational equivalence are simply different types of\nsemantic identity. The theory establishes a solid foundation for critically\nre-evaluating prior work in cross-lingual semantics, and facilitating the\ncreation, verification, and amelioration of lexical resources.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 23:15:02 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 22:58:05 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Hauer", "Bradley", ""], ["Kondrak", "Grzegorz", ""]]}, {"id": "2004.13889", "submitter": "Tasnim Mohiuddin", "authors": "Tasnim Mohiuddin, M Saiful Bari, and Shafiq Joty", "title": "LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon\n  Induction Through Non-Linear Mapping in Latent Space", "comments": "EMNLP 2020 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Most of the successful and predominant methods for bilingual lexicon\ninduction (BLI) are mapping-based, where a linear mapping function is learned\nwith the assumption that the word embedding spaces of different languages\nexhibit similar geometric structures (i.e., approximately isomorphic). However,\nseveral recent studies have criticized this simplified assumption showing that\nit does not hold in general even for closely related languages. In this work,\nwe propose a novel semi-supervised method to learn cross-lingual word\nembeddings for BLI. Our model is independent of the isomorphic assumption and\nuses nonlinear mapping in the latent space of two independently trained\nauto-encoders. Through extensive experiments on fifteen (15) different language\npairs (in both directions) comprising resource-rich and low-resource languages\nfrom two different datasets, we demonstrate that our method outperforms\nexisting models by a good margin. Ablation studies show the importance of\ndifferent model components and the necessity of non-linear mapping.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 23:28:26 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 00:42:16 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mohiuddin", "Tasnim", ""], ["Bari", "M Saiful", ""], ["Joty", "Shafiq", ""]]}, {"id": "2004.13897", "submitter": "Yunyi Zhang", "authors": "Yunyi Zhang, Jiaming Shen, Jingbo Shang and Jiawei Han", "title": "Empower Entity Set Expansion via Language Model Probing", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity set expansion, aiming at expanding a small seed entity set with new\nentities belonging to the same semantic class, is a critical task that benefits\nmany downstream NLP and IR applications, such as question answering, query\nunderstanding, and taxonomy construction. Existing set expansion methods\nbootstrap the seed entity set by adaptively selecting context features and\nextracting new entities. A key challenge for entity set expansion is to avoid\nselecting ambiguous context features which will shift the class semantics and\nlead to accumulative errors in later iterations. In this study, we propose a\nnovel iterative set expansion framework that leverages automatically generated\nclass names to address the semantic drift issue. In each iteration, we select\none positive and several negative class names by probing a pre-trained language\nmodel, and further score each candidate entity based on selected class names.\nExperiments on two datasets show that our framework generates high-quality\nclass names and outperforms previous state-of-the-art methods significantly.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 00:09:43 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 22:57:42 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Zhang", "Yunyi", ""], ["Shen", "Jiaming", ""], ["Shang", "Jingbo", ""], ["Han", "Jiawei", ""]]}, {"id": "2004.13922", "submitter": "Yiming Cui", "authors": "Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu", "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing", "comments": "12 pages, to appear at Findings of EMNLP 2020", "journal-ref": null, "doi": "10.18653/v1/2020.findings-emnlp.58", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional Encoder Representations from Transformers (BERT) has shown\nmarvelous improvements across various NLP tasks, and consecutive variants have\nbeen proposed to further improve the performance of the pre-trained language\nmodels. In this paper, we target on revisiting Chinese pre-trained language\nmodels to examine their effectiveness in a non-English language and release the\nChinese pre-trained language model series to the community. We also propose a\nsimple but effective model called MacBERT, which improves upon RoBERTa in\nseveral ways, especially the masking strategy that adopts MLM as correction\n(Mac). We carried out extensive experiments on eight Chinese NLP tasks to\nrevisit the existing pre-trained language models as well as the proposed\nMacBERT. Experimental results show that MacBERT could achieve state-of-the-art\nperformances on many NLP tasks, and we also ablate details with several\nfindings that may help future research. Resources available:\nhttps://github.com/ymcui/MacBERT\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 02:08:30 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 06:27:52 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Cui", "Yiming", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""], ["Qin", "Bing", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "2004.13931", "submitter": "Hao Zhang", "authors": "Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou", "title": "Span-based Localizing Network for Natural Language Video Localization", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an untrimmed video and a text query, natural language video\nlocalization (NLVL) is to locate a matching span from the video that\nsemantically corresponds to the query. Existing solutions formulate NLVL either\nas a ranking task and apply multimodal matching architecture, or as a\nregression task to directly regress the target video span. In this work, we\naddress NLVL task with a span-based QA approach by treating the input video as\ntext passage. We propose a video span localizing network (VSLNet), on top of\nthe standard span-based QA framework, to address NLVL. The proposed VSLNet\ntackles the differences between NLVL and span-based QA through a simple yet\neffective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to\nsearch for matching video span within a highlighted region. Through extensive\nexperiments on three benchmark datasets, we show that the proposed VSLNet\noutperforms the state-of-the-art methods; and adopting span-based QA framework\nis a promising direction to solve NLVL.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 02:47:04 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 08:49:07 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Hao", ""], ["Sun", "Aixin", ""], ["Jing", "Wei", ""], ["Zhou", "Joey Tianyi", ""]]}, {"id": "2004.13937", "submitter": "Jihyung Moon", "authors": "Jihyung Moon, Hyunchang Cho, Eunjeong L. Park", "title": "Revisiting Round-Trip Translation for Quality Estimation", "comments": "To be published in EAMT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Quality estimation (QE) is the task of automatically evaluating the quality\nof translations without human-translated references. Calculating BLEU between\nthe input sentence and round-trip translation (RTT) was once considered as a\nmetric for QE, however, it was found to be a poor predictor of translation\nquality. Recently, various pre-trained language models have made breakthroughs\nin NLP tasks by providing semantically meaningful word and sentence embeddings.\nIn this paper, we employ semantic embeddings to RTT-based QE. Our method\nachieves the highest correlations with human judgments, compared to previous\nWMT 2019 quality estimation metric task submissions. While backward translation\nmodels can be a drawback when using RTT, we observe that with semantic-level\nmetrics, RTT-based QE is robust to the choice of the backward translation\nsystem. Additionally, the proposed method shows consistent performance for both\nSMT and NMT forward translation systems, implying the method does not penalize\na certain type of model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 03:20:22 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Moon", "Jihyung", ""], ["Cho", "Hyunchang", ""], ["Park", "Eunjeong L.", ""]]}, {"id": "2004.13939", "submitter": "Sophie Groenwold", "authors": "Sophie Groenwold, Samhita Honnavalli, Lily Ou, Aesha Parekh, Sharon\n  Levy, Diba Mirza, William Yang Wang", "title": "Evaluating Transformer-Based Multilingual Text Classification", "comments": "Total of 15 pages (9 pages for paper, 2 pages for references, 4 pages\n  for appendix). Changed title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As NLP tools become ubiquitous in today's technological landscape, they are\nincreasingly applied to languages with a variety of typological structures.\nHowever, NLP research does not focus primarily on typological differences in\nits analysis of state-of-the-art language models. As a result, NLP tools\nperform unequally across languages with different syntactic and morphological\nstructures. Through a detailed discussion of word order typology, morphological\ntypology, and comparative linguistics, we identify which variables most affect\nlanguage modeling efficacy; in addition, we calculate word order and\nmorphological similarity indices to aid our empirical study. We then use this\nbackground to support our analysis of an experiment we conduct using\nmulti-class text classification on eight languages and eight models.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 03:34:53 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 20:31:38 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Groenwold", "Sophie", ""], ["Honnavalli", "Samhita", ""], ["Ou", "Lily", ""], ["Parekh", "Aesha", ""], ["Levy", "Sharon", ""], ["Mirza", "Diba", ""], ["Wang", "William Yang", ""]]}, {"id": "2004.13945", "submitter": "Rajesh Kumar Mundotiya", "authors": "Rajesh Kumar Mundotiya, Manish Kumar Singh, Rahul Kapur, Swasti\n  Mishra, Anil Kumar Singh", "title": "Basic Linguistic Resources and Baselines for Bhojpuri, Magahi and\n  Maithili for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Corpus preparation for low-resource languages and for development of human\nlanguage technology to analyze or computationally process them is a laborious\ntask, primarily due to the unavailability of expert linguists who are native\nspeakers of these languages and also due to the time and resources required.\nBhojpuri, Magahi, and Maithili, languages of the Purvanchal region of India (in\nthe north-eastern parts), are low-resource languages belonging to the\nIndo-Aryan (or Indic) family. They are closely related to Hindi, which is a\nrelatively high-resource language, which is why we make our comparisons with\nHindi. We collected corpora for these three languages from various sources and\ncleaned them to the extent possible, without changing the data in them. The\ntext belongs to different domains and genres. We calculated some basic\nstatistical measures for these corpora at character, word, syllable, and\nmorpheme levels. These corpora were also annotated with parts-of-speech (POS)\nand chunk tags. The basic statistical measures were both absolute and relative\nand were meant to give an indication of linguistic properties such as\nmorphological, lexical, phonological, and syntactic complexities (or richness).\nThe results were compared with a standard Hindi corpus. For most of the\nmeasures, we tried to keep the size of the corpus the same across the languages\nso as to avoid the effect of corpus size, but in some cases it turned out that\nusing the full corpus was better, even if sizes were very different. Although\nthe results are not very clear, we try to draw some conclusions about the\nlanguages and the corpora. For POS tagging and chunking, the BIS tagset was\nused to manually annotate the data. The sizes of the POS tagged data are 16067,\n14669 and 12310 sentences, respectively for Bhojpuri, Magahi and Maithili. The\nsizes for chunking are 9695 and 1954 sentences for Bhojpuri and Maithili,\nrespect\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 03:58:55 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Mundotiya", "Rajesh Kumar", ""], ["Singh", "Manish Kumar", ""], ["Kapur", "Rahul", ""], ["Mishra", "Swasti", ""], ["Singh", "Anil Kumar", ""]]}, {"id": "2004.13947", "submitter": "Yian Li", "authors": "Yian Li and Hai Zhao", "title": "BURT: BERT-inspired Universal Representation from Twin Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained contextualized language models such as BERT have shown great\neffectiveness in a wide range of downstream Natural Language Processing (NLP)\ntasks. However, the effective representations offered by the models target at\neach token inside a sequence rather than each sequence and the fine-tuning step\ninvolves the input of both sequences at one time, leading to unsatisfying\nrepresentations of various sequences with different granularities. Especially,\nas sentence-level representations taken as the full training context in these\nmodels, there comes inferior performance on lower-level linguistic units\n(phrases and words). In this work, we present BURT (BERT inspired Universal\nRepresentation from Twin Structure) that is capable of generating universal,\nfixed-size representations for input sequences of any granularity, i.e., words,\nphrases, and sentences, using a large scale of natural language inference and\nparaphrase data with multiple training objectives. Our proposed BURT adopts the\nSiamese network, learning sentence-level representations from natural language\ninference dataset and word/phrase-level representations from paraphrasing\ndataset, respectively. We evaluate BURT across different granularities of text\nsimilarity tasks, including STS tasks, SemEval2013 Task 5(a) and some commonly\nused word similarity tasks, where BURT substantially outperforms other\nrepresentation models on sentence-level datasets and achieves significant\nimprovements in word/phrase-level representation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 04:01:52 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 13:04:22 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Li", "Yian", ""], ["Zhao", "Hai", ""]]}, {"id": "2004.13952", "submitter": "Chenguang Zhu", "authors": "Baolin Peng, Chenguang Zhu, Michael Zeng, Jianfeng Gao", "title": "Data Augmentation for Spoken Language Understanding via Pretrained\n  Language Models", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of spoken language understanding (SLU) models often faces the\nproblem of data scarcity. In this paper, we put forward a data augmentation\nmethod using pretrained language models to boost the variability and accuracy\nof generated utterances. Furthermore, we investigate and propose solutions to\ntwo previously overlooked semi-supervised learning scenarios of data scarcity\nin SLU: i) Rich-in-Ontology: ontology information with numerous valid dialogue\nacts is given; ii) Rich-in-Utterance: a large number of unlabelled utterances\nare available. Empirical results show that our method can produce synthetic\ntraining data that boosts the performance of language understanding models in\nvarious scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 04:07:12 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 01:36:00 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Peng", "Baolin", ""], ["Zhu", "Chenguang", ""], ["Zeng", "Michael", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2004.13956", "submitter": "Oleg Vasilyev", "authors": "Oleg Vasilyev, Kathryn Evans, Anna Venancio-Marques, John Bohannon", "title": "Zero-shot topic generation", "comments": "12 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to generating topics using a model trained only for\ndocument title generation, with zero examples of topics given during training.\nWe leverage features that capture the relevance of a candidate span in a\ndocument for the generation of a title for that document. The output is a\nweighted collection of the phrases that are most relevant for describing the\ndocument and distinguishing it within a corpus, without requiring access to the\nrest of the corpus. We conducted a double-blind trial in which human annotators\nscored the quality of our machine-generated topics along with original\nhuman-written topics associated with news articles from The Guardian and The\nHuffington Post. The results show that our zero-shot model generates topic\nlabels for news documents that are on average equal to or higher quality than\nthose written by humans, as judged by humans.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 04:39:28 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Vasilyev", "Oleg", ""], ["Evans", "Kathryn", ""], ["Venancio-Marques", "Anna", ""], ["Bohannon", "John", ""]]}, {"id": "2004.13980", "submitter": "Matthew Sims", "authors": "Matthew Sims, David Bamman", "title": "Measuring Information Propagation in Literary Social Networks", "comments": "EMNLP 2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the task of modeling information propagation in literature, in\nwhich we seek to identify pieces of information passing from character A to\ncharacter B to character C, only given a description of their activity in text.\nWe describe a new pipeline for measuring information propagation in this domain\nand publish a new dataset for speaker attribution, enabling the evaluation of\nan important component of this pipeline on a wider range of literary texts than\npreviously studied. Using this pipeline, we analyze the dynamics of information\npropagation in over 5,000 works of fiction, finding that information flows\nthrough characters that fill structural holes connecting different communities,\nand that characters who are women are depicted as filling this role much more\nfrequently than characters who are men.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:41:26 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 19:09:34 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Sims", "Matthew", ""], ["Bamman", "David", ""]]}, {"id": "2004.13983", "submitter": "Zhengyuan Liu", "authors": "Zhengyuan Liu, Ke Shi, Nancy F. Chen", "title": "Conditional Neural Generation using Sub-Aspect Functions for Extractive\n  News Summarization", "comments": "Accepted to Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much progress has been made in text summarization, fueled by neural\narchitectures using large-scale training corpora. However, in the news domain,\nneural models easily overfit by leveraging position-related features due to the\nprevalence of the inverted pyramid writing style. In addition, there is an\nunmet need to generate a variety of summaries for different users. In this\npaper, we propose a neural framework that can flexibly control summary\ngeneration by introducing a set of sub-aspect functions (i.e. importance,\ndiversity, position). These sub-aspect functions are regulated by a set of\ncontrol codes to decide which sub-aspect to focus on during summary generation.\nWe demonstrate that extracted summaries with minimal position bias is\ncomparable with those generated by standard models that take advantage of\nposition preference. We also show that news summaries generated with a focus on\ndiversity can be more preferred by human raters. These results suggest that a\nmore flexible neural summarization framework providing more control options\ncould be desirable in tailoring to different user preferences, which is useful\nsince it is often impractical to articulate such preferences for different\napplications a priori.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:52:15 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 02:57:55 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 04:57:16 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Liu", "Zhengyuan", ""], ["Shi", "Ke", ""], ["Chen", "Nancy F.", ""]]}, {"id": "2004.13988", "submitter": "Junlong Li", "authors": "Junlong Li, Zhuosheng Zhang, Hai Zhao", "title": "Knowledgeable Dialogue Reading Comprehension on Key Turns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-choice machine reading comprehension (MRC) requires models to choose\nthe correct answer from candidate options given a passage and a question. Our\nresearch focuses dialogue-based MRC, where the passages are multi-turn\ndialogues. It suffers from two challenges, the answer selection decision is\nmade without support of latently helpful commonsense, and the multi-turn\ncontext may hide considerable irrelevant information. This work thus makes the\nfirst attempt to tackle those two challenges by extracting substantially\nimportant turns and utilizing external knowledge to enhance the representation\nof context. In this paper, the relevance of each turn to the question are\ncalculated to choose key turns. Besides, terms related to the context and the\nquestion in a knowledge graph are extracted as external knowledge. The original\ncontext, question and external knowledge are encoded with the pre-trained\nlanguage model, then the language representation and key turns are combined\ntogether with a will-designed mechanism to predict the answer. Experimental\nresults on a DREAM dataset show that our proposed model achieves great\nimprovements on baselines.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 07:04:43 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 16:52:47 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Li", "Junlong", ""], ["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""]]}, {"id": "2004.14004", "submitter": "Chenglei Si", "authors": "Chenglei Si, Ziqing Yang, Yiming Cui, Wentao Ma, Ting Liu, Shijin Wang", "title": "Benchmarking Robustness of Machine Reading Comprehension Models", "comments": "ACL 2021 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Reading Comprehension (MRC) is an important testbed for evaluating\nmodels' natural language understanding (NLU) ability. There has been rapid\nprogress in this area, with new models achieving impressive performance on\nvarious benchmarks. However, existing benchmarks only evaluate models on\nin-domain test sets without considering their robustness under test-time\nperturbations or adversarial attacks. To fill this important gap, we construct\nAdvRACE (Adversarial RACE), a new model-agnostic benchmark for evaluating the\nrobustness of MRC models under four different types of adversarial attacks,\nincluding our novel distractor extraction and generation attacks. We show that\nstate-of-the-art (SOTA) models are vulnerable to all of these attacks. We\nconclude that there is substantial room for building more robust MRC models and\nour benchmark can help motivate and measure progress in this area. We release\nour data and code at https://github.com/NoviScl/AdvRACE .\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:05:32 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 06:16:19 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Si", "Chenglei", ""], ["Yang", "Ziqing", ""], ["Cui", "Yiming", ""], ["Ma", "Wentao", ""], ["Liu", "Ting", ""], ["Wang", "Shijin", ""]]}, {"id": "2004.14008", "submitter": "Reina Akama", "authors": "Reina Akama, Sho Yokoi, Jun Suzuki, Kentaro Inui", "title": "Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness", "comments": "18 pages, Accepted at The 2020 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale dialogue datasets have recently become available for training\nneural dialogue agents. However, these datasets have been reported to contain a\nnon-negligible number of unacceptable utterance pairs. In this paper, we\npropose a method for scoring the quality of utterance pairs in terms of their\nconnectivity and relatedness. The proposed scoring method is designed based on\nfindings widely shared in the dialogue and linguistics research communities. We\ndemonstrate that it has a relatively good correlation with the human judgment\nof dialogue quality. Furthermore, the method is applied to filter out\npotentially unacceptable utterance pairs from a large-scale noisy dialogue\ncorpus to ensure its quality. We experimentally confirm that training data\nfiltered by the proposed method improves the quality of neural dialogue agents\nin response generation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:08:32 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 16:19:58 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Akama", "Reina", ""], ["Yokoi", "Sho", ""], ["Suzuki", "Jun", ""], ["Inui", "Kentaro", ""]]}, {"id": "2004.14021", "submitter": "Xiangpeng Wei", "authors": "Xiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, Rongxiang Weng, Weihua Luo", "title": "Multiscale Collaborative Deep Models for Neural Machine Translation", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent evidence reveals that Neural Machine Translation (NMT) models with\ndeeper neural networks can be more effective but are difficult to train. In\nthis paper, we present a MultiScale Collaborative (MSC) framework to ease the\ntraining of NMT models that are substantially deeper than those used\npreviously. We explicitly boost the gradient back-propagation from top to\nbottom levels by introducing a block-scale collaboration mechanism into deep\nNMT models. Then, instead of forcing the whole encoder stack directly learns a\ndesired representation, we let each encoder block learns a fine-grained\nrepresentation and enhance it by encoding spatial dependencies using a\ncontext-scale collaboration. We provide empirical evidence showing that the MSC\nnets are easy to optimize and can obtain improvements of translation quality\nfrom considerably increased depth. On IWSLT translation tasks with three\ntranslation directions, our extremely deep models (with 72-layer encoders)\nsurpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC\nachieves a BLEU score of 30.56 on WMT14 English-German task that significantly\noutperforms state-of-the-art deep NMT models.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:36:08 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 14:24:24 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 01:21:22 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Wei", "Xiangpeng", ""], ["Yu", "Heng", ""], ["Hu", "Yue", ""], ["Zhang", "Yue", ""], ["Weng", "Rongxiang", ""], ["Luo", "Weihua", ""]]}, {"id": "2004.14025", "submitter": "Sungjin Park", "authors": "Sungjin Park, Taesun Whang, Yeochan Yoon, Heuiseok Lim", "title": "Multi-View Attention Network for Visual Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual dialog is a challenging vision-language task in which a series of\nquestions visually grounded by a given image are answered. To resolve the\nvisual dialog task, a high-level understanding of various multimodal inputs\n(e.g., question, dialog history, and image) is required. Specifically, it is\nnecessary for an agent to 1) determine the semantic intent of question and 2)\nalign question-relevant textual and visual contents among heterogeneous\nmodality inputs. In this paper, we propose Multi-View Attention Network (MVAN),\nwhich leverages multiple views about heterogeneous inputs based on attention\nmechanisms. MVAN effectively captures the question-relevant information from\nthe dialog history with two complementary modules (i.e., Topic Aggregation and\nContext Matching), and builds multimodal representations through sequential\nalignment processes (i.e., Modality Alignment). Experimental results on VisDial\nv1.0 dataset show the effectiveness of our proposed model, which outperforms\nthe previous state-of-the-art methods with respect to all evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:46:38 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 11:28:57 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 00:51:40 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Park", "Sungjin", ""], ["Whang", "Taesun", ""], ["Yoon", "Yeochan", ""], ["Lim", "Heuiseok", ""]]}, {"id": "2004.14054", "submitter": "Nicola Tonellotto", "authors": "I. Mele, C. I. Muntean, F. M. Nardini, R. Perego, N. Tonellotto, O.\n  Frieder", "title": "Topic Propagation in Conversational Search", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a conversational context, a user expresses her multi-faceted information\nneed as a sequence of natural-language questions, i.e., utterances. Starting\nfrom a given topic, the conversation evolves through user utterances and system\nreplies. The retrieval of documents relevant to a given utterance in a\nconversation is challenging due to ambiguity of natural language and to the\ndifficulty of detecting possible topic shifts and semantic relationships among\nutterances. We adopt the 2019 TREC Conversational Assistant Track (CAsT)\nframework to experiment with a modular architecture performing: (i) topic-aware\nutterance rewriting, (ii) retrieval of candidate passages for the rewritten\nutterances, and (iii) neural-based re-ranking of candidate passages. We present\na comprehensive experimental evaluation of the architecture assessed in terms\nof traditional IR metrics at small cutoffs. Experimental results show the\neffectiveness of our techniques that achieve an improvement up to 0.28 (+93%)\nfor P@1 and 0.19 (+89.9%) for nDCG@3 w.r.t. the CAsT baseline.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:06:00 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Mele", "I.", ""], ["Muntean", "C. I.", ""], ["Nardini", "F. M.", ""], ["Perego", "R.", ""], ["Tonellotto", "N.", ""], ["Frieder", "O.", ""]]}, {"id": "2004.14062", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Mika H\\\"am\\\"al\\\"ainen, Linda Wiechetek", "title": "Morphological Disambiguation of South S\\'ami with FSTs and Neural\n  Networks", "comments": "1st Joint SLTU and CCURL Workshop (SLTU-CCURL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for conducting morphological disambiguation for South\nS\\'ami, which is an endangered language. Our method uses an FST-based\nmorphological analyzer to produce an ambiguous set of morphological readings\nfor each word in a sentence. These readings are disambiguated with a Bi-RNN\nmodel trained on the related North S\\'ami UD Treebank and some synthetically\ngenerated South S\\'ami data. The disambiguation is done on the level of\nmorphological tags ignoring word forms and lemmas; this makes it possible to\nuse North S\\'ami training data for South S\\'ami without the need for a\nbilingual dictionary or aligned word embeddings. Our approach requires only\nminimal resources for South S\\'ami, which makes it usable and applicable in the\ncontexts of any other endangered language as well.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:30:25 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Mika", ""], ["Wiechetek", "Linda", ""]]}, {"id": "2004.14065", "submitter": "Hila Gonen", "authors": "Hila Gonen and Kellie Webster", "title": "Automatically Identifying Gender Issues in Machine Translation using\n  Perturbations", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The successful application of neural methods to machine translation has\nrealized huge quality advances for the community. With these improvements, many\nhave noted outstanding challenges, including the modeling and treatment of\ngendered language. While previous studies have identified issues using\nsynthetic examples, we develop a novel technique to mine examples from real\nworld data to explore challenges for deployed systems. We use our method to\ncompile an evaluation benchmark spanning examples for four languages from three\nlanguage families, which we publicly release to facilitate research. The\nexamples in our benchmark expose where model representations are gendered, and\nthe unintended consequences these gendered representations can have in\ndownstream application.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:38:09 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 19:01:59 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Gonen", "Hila", ""], ["Webster", "Kellie", ""]]}, {"id": "2004.14069", "submitter": "Ming Gong", "authors": "Fei Yuan, Linjun Shou, Xuanyu Bai, Ming Gong, Yaobo Liang, Nan Duan,\n  Yan Fu, Daxin Jiang", "title": "Enhancing Answer Boundary Detection for Multilingual Machine Reading\n  Comprehension", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual pre-trained models could leverage the training data from a rich\nsource language (such as English) to improve performance on low resource\nlanguages. However, the transfer quality for multilingual Machine Reading\nComprehension (MRC) is significantly worse than sentence classification tasks\nmainly due to the requirement of MRC to detect the word level answer boundary.\nIn this paper, we propose two auxiliary tasks in the fine-tuning stage to\ncreate additional phrase boundary supervision: (1) A mixed MRC task, which\ntranslates the question or passage to other languages and builds cross-lingual\nquestion-passage pairs; (2) A language-agnostic knowledge masking task by\nleveraging knowledge phrases mined from web. Besides, extensive experiments on\ntwo cross-lingual MRC datasets show the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:44:00 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 13:17:28 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Yuan", "Fei", ""], ["Shou", "Linjun", ""], ["Bai", "Xuanyu", ""], ["Gong", "Ming", ""], ["Liang", "Yaobo", ""], ["Duan", "Nan", ""], ["Fu", "Yan", ""], ["Jiang", "Daxin", ""]]}, {"id": "2004.14074", "submitter": "Nicola Pellicano", "authors": "Alexandre Tamborrino, Nicola Pellicano, Baptiste Pannier, Pascal\n  Voitot and Louise Naudin", "title": "Pre-training Is (Almost) All You Need: An Application to Commonsense\n  Reasoning", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning of pre-trained transformer models has become the standard\napproach for solving common NLP tasks. Most of the existing approaches rely on\na randomly initialized classifier on top of such networks. We argue that this\nfine-tuning procedure is sub-optimal as the pre-trained model has no prior on\nthe specific classifier labels, while it might have already learned an\nintrinsic textual representation of the task. In this paper, we introduce a new\nscoring method that casts a plausibility ranking task in a full-text format and\nleverages the masked language modeling head tuned during the pre-training\nphase. We study commonsense reasoning tasks where the model must rank a set of\nhypotheses given a premise, focusing on the COPA, Swag, HellaSwag and\nCommonsenseQA datasets. By exploiting our scoring method without fine-tuning,\nwe are able to produce strong baselines (e.g. 80% test accuracy on COPA) that\nare comparable to supervised approaches. Moreover, when fine-tuning directly on\nthe proposed scoring function, we show that our method provides a much more\nstable training phase across random restarts (e.g $\\times 10$ standard\ndeviation reduction on COPA test accuracy) and requires less annotated data\nthan the standard classifier approach to reach equivalent performances.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:54:40 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Tamborrino", "Alexandre", ""], ["Pellicano", "Nicola", ""], ["Pannier", "Baptiste", ""], ["Voitot", "Pascal", ""], ["Naudin", "Louise", ""]]}, {"id": "2004.14080", "submitter": "Jun Quan", "authors": "Jun Quan and Deyi Xiong", "title": "Modeling Long Context for Task-Oriented Dialogue State Generation", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the recently proposed transferable dialogue state generator (TRADE)\nthat predicts dialogue states from utterance-concatenated dialogue context, we\npropose a multi-task learning model with a simple yet effective utterance\ntagging technique and a bidirectional language model as an auxiliary task for\ntask-oriented dialogue state generation. By enabling the model to learn a\nbetter representation of the long dialogue context, our approaches attempt to\nsolve the problem that the performance of the baseline significantly drops when\nthe input dialogue context sequence is long. In our experiments, our proposed\nmodel achieves a 7.03% relative improvement over the baseline, establishing a\nnew state-of-the-art joint goal accuracy of 52.04% on the MultiWOZ 2.0 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 11:02:25 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Quan", "Jun", ""], ["Xiong", "Deyi", ""]]}, {"id": "2004.14088", "submitter": "Guanhua Zhang", "authors": "Guanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Conghui Zhu and Tiejun\n  Zhao", "title": "Demographics Should Not Be the Reason of Toxicity: Mitigating\n  Discrimination in Text Classifications with Instance Weighting", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent proliferation of the use of text classifications, researchers\nhave found that there are certain unintended biases in text classification\ndatasets. For example, texts containing some demographic identity-terms (e.g.,\n\"gay\", \"black\") are more likely to be abusive in existing abusive language\ndetection datasets. As a result, models trained with these datasets may\nconsider sentences like \"She makes me happy to be gay\" as abusive simply\nbecause of the word \"gay.\" In this paper, we formalize the unintended biases in\ntext classification datasets as a kind of selection bias from the\nnon-discrimination distribution to the discrimination distribution. Based on\nthis formalization, we further propose a model-agnostic debiasing training\nframework by recovering the non-discrimination distribution using instance\nweighting, which does not require any extra resources or annotations apart from\na pre-defined set of demographic identity-terms. Experiments demonstrate that\nour method can effectively alleviate the impacts of the unintended biases\nwithout significantly hurting models' generalization ability.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 11:22:19 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 07:44:34 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 14:22:11 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zhang", "Guanhua", ""], ["Bai", "Bing", ""], ["Zhang", "Junqi", ""], ["Bai", "Kun", ""], ["Zhu", "Conghui", ""], ["Zhao", "Tiejun", ""]]}, {"id": "2004.14096", "submitter": "Artur Kulmizev", "authors": "Artur Kulmizev, Vinit Ravishankar, Mostafa Abdou, Joakim Nivre", "title": "Do Neural Language Models Show Preferences for Syntactic Formalisms?", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on the interpretability of deep neural language models has\nconcluded that many properties of natural language syntax are encoded in their\nrepresentational spaces. However, such studies often suffer from limited scope\nby focusing on a single language and a single linguistic formalism. In this\nstudy, we aim to investigate the extent to which the semblance of syntactic\nstructure captured by language models adheres to a surface-syntactic or deep\nsyntactic style of analysis, and whether the patterns are consistent across\ndifferent languages. We apply a probe for extracting directed dependency trees\nto BERT and ELMo models trained on 13 different languages, probing for two\ndifferent syntactic annotation styles: Universal Dependencies (UD),\nprioritizing deep syntactic relations, and Surface-Syntactic Universal\nDependencies (SUD), focusing on surface structure. We find that both models\nexhibit a preference for UD over SUD - with interesting variations across\nlanguages and layers - and that the strength of this preference is correlated\nwith differences in tree shape.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 11:37:53 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Kulmizev", "Artur", ""], ["Ravishankar", "Vinit", ""], ["Abdou", "Mostafa", ""], ["Nivre", "Joakim", ""]]}, {"id": "2004.14109", "submitter": "Jungsoo Park", "authors": "Jungsoo Park, Mujeen Sung, Jinhyuk Lee, Jaewoo Kang", "title": "Adversarial Subword Regularization for Robust Neural Machine Translation", "comments": "9 pages,1 figure, Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposing diverse subword segmentations to neural machine translation (NMT)\nmodels often improves the robustness of machine translation as NMT models can\nexperience various subword candidates. However, the diversification of subword\nsegmentations mostly relies on the pre-trained subword language models from\nwhich erroneous segmentations of unseen words are less likely to be sampled. In\nthis paper, we present adversarial subword regularization (ADVSR) to study\nwhether gradient signals during training can be a substitute criterion for\nexposing diverse subword segmentations. We experimentally show that our\nmodel-based adversarial samples effectively encourage NMT models to be less\nsensitive to segmentation errors and improve the performance of NMT models in\nlow-resource and out-domain datasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:06:42 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 05:25:34 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Park", "Jungsoo", ""], ["Sung", "Mujeen", ""], ["Lee", "Jinhyuk", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2004.14118", "submitter": "Mario Giulianelli", "authors": "Mario Giulianelli, Marco Del Tredici, Raquel Fern\\'andez", "title": "Analysing Lexical Semantic Change with Contextualised Word\n  Representations", "comments": "To appear in Proceedings of the 58th Annual Meeting of the\n  Association for Computational Linguistics (ACL-2020)", "journal-ref": null, "doi": "10.18653/v1/2020.acl-main.365", "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first unsupervised approach to lexical semantic\nchange that makes use of contextualised word representations. We propose a\nnovel method that exploits the BERT neural language model to obtain\nrepresentations of word usages, clusters these representations into usage\ntypes, and measures change along time with three proposed metrics. We create a\nnew evaluation dataset and show that the model representations and the detected\nsemantic shifts are positively correlated with human judgements. Our extensive\nqualitative analysis demonstrates that our method captures a variety of\nsynchronic and diachronic linguistic phenomena. We expect our work to inspire\nfurther research in this direction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:18:14 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Giulianelli", "Mario", ""], ["Del Tredici", "Marco", ""], ["Fern\u00e1ndez", "Raquel", ""]]}, {"id": "2004.14119", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Manaj Srivastava, Sanjay Krishna, David Akodes, Richard\n  Schwartz", "title": "Combining Word Embeddings and N-grams for Unsupervised Document\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based extractive document summarization relies on the quality of the\nsentence similarity graph. Bag-of-words or tf-idf based sentence similarity\nuses exact word matching, but fails to measure the semantic similarity between\nindividual words or to consider the semantic structure of sentences. In order\nto improve the similarity measure between sentences, we employ off-the-shelf\ndeep embedding features and tf-idf features, and introduce a new text\nsimilarity metric. An improved sentence similarity graph is built and used in a\nsubmodular objective function for extractive summarization, which consists of a\nweighted coverage term and a diversity term. A Transformer based compression\nmodel is developed for sentence compression to aid in document summarization.\nOur summarization approach is extractive and unsupervised. Experiments\ndemonstrate that our approach can outperform the tf-idf based approach and\nachieve state-of-the-art performance on the DUC04 dataset, and comparable\nperformance to the fully supervised learning methods on the CNN/DM and NYT\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 00:22:46 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Jiang", "Zhuolin", ""], ["Srivastava", "Manaj", ""], ["Krishna", "Sanjay", ""], ["Akodes", "David", ""], ["Schwartz", "Richard", ""]]}, {"id": "2004.14120", "submitter": "Ant\\'onio G\\'ois", "authors": "Ant\\'onio G\\'ois, Kyunghyun Cho, Andr\\'e Martins", "title": "Learning Non-Monotonic Automatic Post-Editing of Translations from Human\n  Orderings", "comments": "Accepted at EAMT 2020; dataset available here:\n  https://github.com/antoniogois/keystrokes_ape", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in neural machine translation has explored flexible\ngeneration orders, as an alternative to left-to-right generation. However,\ntraining non-monotonic models brings a new complication: how to search for a\ngood ordering when there is a combinatorial explosion of orderings arriving at\nthe same final result? Also, how do these automatic orderings compare with the\nactual behaviour of human translators? Current models rely on manually built\nbiases or are left to explore all possibilities on their own. In this paper, we\nanalyze the orderings produced by human post-editors and use them to train an\nautomatic post-editing system. We compare the resulting system with those\ntrained with left-to-right and random post-editing orderings. We observe that\nhumans tend to follow a nearly left-to-right order, but with interesting\ndeviations, such as preferring to start by correcting punctuation or verbs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:19:50 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["G\u00f3is", "Ant\u00f3nio", ""], ["Cho", "Kyunghyun", ""], ["Martins", "Andr\u00e9", ""]]}, {"id": "2004.14129", "submitter": "Xin Wang", "authors": "Evani Radiya-Dixit and Xin Wang", "title": "How fine can fine-tuning be? Learning efficient language models", "comments": "11 pages, 11 figures and 6 tables; accepted to the 23rd International\n  Conference on Artificial Intelligence and Statistics (AISTATS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art performance on language understanding tasks is now achieved\nwith increasingly large networks; the current record holder has billions of\nparameters. Given a language model pre-trained on massive unlabeled text\ncorpora, only very light supervised fine-tuning is needed to learn a task: the\nnumber of fine-tuning steps is typically five orders of magnitude lower than\nthe total parameter count. Does this mean that fine-tuning only introduces\nsmall differences from the pre-trained model in the parameter space? If so, can\none avoid storing and computing an entire model for each task? In this work, we\naddress these questions by using Bidirectional Encoder Representations from\nTransformers (BERT) as an example. As expected, we find that the fine-tuned\nmodels are close in parameter space to the pre-trained one, with the closeness\nvarying from layer to layer. We show that it suffices to fine-tune only the\nmost critical layers. Further, we find that there are surprisingly many good\nsolutions in the set of sparsified versions of the pre-trained model. As a\nresult, fine-tuning of huge language models can be achieved by simply setting a\ncertain number of entries in certain layers of the pre-trained parameters to\nzero, saving both task-specific parameter storage and computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:31:28 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Radiya-Dixit", "Evani", ""], ["Wang", "Xin", ""]]}, {"id": "2004.14130", "submitter": "Georg Rehm", "authors": "Juli\\'an Moreno-Schneider, Peter Bourgonje, Florian Kintzel, Georg\n  Rehm", "title": "A Workflow Manager for Complex NLP and Content Curation Pipelines", "comments": "Proceedings of the 1st International Workshop on Language Technology\n  Platforms (IWLTP 2020). To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a workflow manager for the flexible creation and customisation of\nNLP processing pipelines. The workflow manager addresses challenges in\ninteroperability across various different NLP tasks and hardware-based resource\nusage. Based on the four key principles of generality, flexibility, scalability\nand efficiency, we present the first version of the workflow manager by\nproviding details on its custom definition language, explaining the\ncommunication components and the general system architecture and setup. We\ncurrently implement the system, which is grounded and motivated by real-world\nindustry use cases in several innovation and transfer projects.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 21:23:28 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Moreno-Schneider", "Juli\u00e1n", ""], ["Bourgonje", "Peter", ""], ["Kintzel", "Florian", ""], ["Rehm", "Georg", ""]]}, {"id": "2004.14134", "submitter": "Hossein Hassani", "authors": "Roshna Omer Abdulrahman, Hossein Hassani", "title": "Using Punkt for Sentence Segmentation in non-Latin Scripts: Experiments\n  on Kurdish (Sorani) Texts", "comments": "Accepted for AfricaNLP Workshop at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Segmentation is a fundamental step for most Natural Language Processing\ntasks. The Kurdish language is a multi-dialect, under-resourced language which\nis written in different scripts. The lack of various segmented corpora is one\nof the major bottlenecks in Kurdish language processing. We used Punkt, an\nunsupervised machine learning method, to segment a Kurdish corpus of Sorani\ndialect, written in Persian-Arabic script. According to the literature, studies\non using Punkt on non-Latin data are scanty. In our experiment, we achieved an\nF1 score of 91.10% and had an Error Rate of 16.32%. The high Error Rate is\nmainly due to the situation of abbreviations in Kurdish and partly because of\nordinal numerals. The data is publicly available at\nhttps://github.com/KurdishBLARK/ KTC-Segmented for non-commercial use under the\nCC BY-NC-SA 4.0 licence.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 06:44:08 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 08:09:11 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Abdulrahman", "Roshna Omer", ""], ["Hassani", "Hossein", ""]]}, {"id": "2004.14135", "submitter": "Khalid Elmadani", "authors": "Khalid N. Elmadani, Mukhtar Elgezouli, Anas Showk", "title": "BERT Fine-tuning For Arabic Text Summarization", "comments": "4 pages, 2 tables, Published as a conference paper at AfricaNLP\n  workshop at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning a pretrained BERT model is the state of the art method for\nextractive/abstractive text summarization, in this paper we showcase how this\nfine-tuning method can be applied to the Arabic language to both construct the\nfirst documented model for abstractive Arabic text summarization and show its\nperformance in Arabic extractive summarization. Our model works with\nmultilingual BERT (as Arabic language does not have a pretrained BERT of its\nown). We show its performance in English corpus first before applying it to\nArabic corpora in both extractive and abstractive tasks.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 20:23:14 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Elmadani", "Khalid N.", ""], ["Elgezouli", "Mukhtar", ""], ["Showk", "Anas", ""]]}, {"id": "2004.14145", "submitter": "Richard He", "authors": "Wendong He, Yizhen Shao, Pingjian Zhang", "title": "Entity Candidate Network for Whole-Aware Named Entity Recognition", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) is a crucial upstream task in Natural Language\nProcessing (NLP). Traditional tag scheme approaches offer a single recognition\nthat does not meet the needs of many downstream tasks such as coreference\nresolution. Meanwhile, Tag scheme approaches ignore the continuity of entities.\nInspired by one-stage object detection models in computer vision (CV), this\npaper proposes a new no-tag scheme, the Whole-Aware Detection, which makes NER\nan object detection task. Meanwhile, this paper presents a novel model, Entity\nCandidate Network (ECNet), and a specific convolution network, Adaptive Context\nConvolution Network (ACCN), to fuse multi-scale contexts and encode entity\ninformation at each position. ECNet identifies the full span of a named entity\nand its type at each position based on Entity Loss. Furthermore, ECNet is\nregulable between the highest precision and the highest recall, while the tag\nscheme approaches are not. Experimental results on the CoNLL 2003 English\ndataset and the WNUT 2017 dataset show that ECNet outperforms other previous\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:47:02 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["He", "Wendong", ""], ["Shao", "Yizhen", ""], ["Zhang", "Pingjian", ""]]}, {"id": "2004.14164", "submitter": "Xiaoqing Geng", "authors": "Xiaoqing Geng, Xiwen Chen, Kenny Q. Zhu, Libin Shen, Yinggong Zhao", "title": "MICK: A Meta-Learning Framework for Few-shot Relation Classification\n  with Small Training Data", "comments": null, "journal-ref": "CIKM 2020: The 29th ACM International Conference on Information\n  and Knowledge Management", "doi": "10.1145/3340531.3411858", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot relation classification seeks to classify incoming query instances\nafter meeting only few support instances. This ability is gained by training\nwith large amount of in-domain annotated data. In this paper, we tackle an even\nharder problem by further limiting the amount of data available at training\ntime. We propose a few-shot learning framework for relation classification,\nwhich is particularly powerful when the training data is very small. In this\nframework, models not only strive to classify query instances, but also seek\nunderlying knowledge about the support instances to obtain better instance\nrepresentations. The framework also includes a method for aggregating\ncross-domain knowledge into models by open-source task enrichment.\nAdditionally, we construct a brand new dataset: the TinyRel-CM dataset, a\nfew-shot relation classification dataset in health domain with purposely small\ntraining data and challenging relation classes. Experimental results\ndemonstrate that our framework brings performance gains for most underlying\nclassification models, outperforms the state-of-the-art results given small\ntraining data, and achieves competitive results with sufficiently large\ntraining data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 06:23:38 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 15:54:51 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Geng", "Xiaoqing", ""], ["Chen", "Xiwen", ""], ["Zhu", "Kenny Q.", ""], ["Shen", "Libin", ""], ["Zhao", "Yinggong", ""]]}, {"id": "2004.14165", "submitter": "Ganesh Bagler Dr", "authors": "Tript Sharma, Utkarsh Upadhyay and Ganesh Bagler", "title": "Classification of Cuisines from Sequentially Structured Recipes", "comments": "36th IEEE International Conference on Data Engineering (ICDE 2020),\n  DECOR Workshop; 4 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cultures across the world are distinguished by the idiosyncratic patterns in\ntheir cuisines. These cuisines are characterized in terms of their\nsubstructures such as ingredients, cooking processes and utensils. A complex\nfusion of these substructures intrinsic to a region defines the identity of a\ncuisine. Accurate classification of cuisines based on their culinary features\nis an outstanding problem and has hitherto been attempted to solve by\naccounting for ingredients of a recipe as features. Previous studies have\nattempted cuisine classification by using unstructured recipes without\naccounting for details of cooking techniques. In reality, the cooking\nprocesses/techniques and their order are highly significant for the recipe's\nstructure and hence for its classification. In this article, we have\nimplemented a range of classification techniques by accounting for this\ninformation on the RecipeDB dataset containing sequential data on recipes. The\nstate-of-the-art RoBERTa model presented the highest accuracy of 73.30% among a\nrange of classification models from Logistic Regression and Naive Bayes to\nLSTMs and Transformers.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 05:40:36 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Sharma", "Tript", ""], ["Upadhyay", "Utkarsh", ""], ["Bagler", "Ganesh", ""]]}, {"id": "2004.14166", "submitter": "Xingyi Cheng", "authors": "Xingyi Cheng, Weidi Xu, Kunlong Chen, Shaohua Jiang, Feng Wang,\n  Taifeng Wang, Wei Chu, Yuan Qi", "title": "SpellGCN: Incorporating Phonological and Visual Similarities into\n  Language Models for Chinese Spelling Check", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese Spelling Check (CSC) is a task to detect and correct spelling errors\nin Chinese natural language. Existing methods have made attempts to incorporate\nthe similarity knowledge between Chinese characters. However, they take the\nsimilarity knowledge as either an external input resource or just heuristic\nrules. This paper proposes to incorporate phonological and visual similarity\nknowledge into language models for CSC via a specialized graph convolutional\nnetwork (SpellGCN). The model builds a graph over the characters, and SpellGCN\nis learned to map this graph into a set of inter-dependent character\nclassifiers. These classifiers are applied to the representations extracted by\nanother network, such as BERT, enabling the whole network to be end-to-end\ntrainable. Experiments (The dataset and all code for this paper are available\nat https://github.com/ACL2020SpellGCN/SpellGCN) are conducted on three\nhuman-annotated datasets. Our method achieves superior performance against\nprevious models by a large margin.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 03:34:06 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 07:23:11 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Cheng", "Xingyi", ""], ["Xu", "Weidi", ""], ["Chen", "Kunlong", ""], ["Jiang", "Shaohua", ""], ["Wang", "Feng", ""], ["Wang", "Taifeng", ""], ["Chu", "Wei", ""], ["Qi", "Yuan", ""]]}, {"id": "2004.14171", "submitter": "Gengchen Mai", "authors": "Gengchen Mai, Krzysztof Janowicz, Ling Cai, Rui Zhu, Blake Regalia, Bo\n  Yan, Meilin Shi, Ni Lao", "title": "SE-KGE: A Location-Aware Knowledge Graph Embedding Model for Geographic\n  Question Answering and Spatial Semantic Lifting", "comments": "Accepted to Transactions in GIS", "journal-ref": "Transactions in GIS, 2020", "doi": "10.1111/TGIS.12629", "report-no": null, "categories": "cs.DB cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning knowledge graph (KG) embeddings is an emerging technique for a\nvariety of downstream tasks such as summarization, link prediction, information\nretrieval, and question answering. However, most existing KG embedding models\nneglect space and, therefore, do not perform well when applied to (geo)spatial\ndata and tasks. For those models that consider space, most of them primarily\nrely on some notions of distance. These models suffer from higher computational\ncomplexity during training while still losing information beyond the relative\ndistance between entities. In this work, we propose a location-aware KG\nembedding model called SE-KGE. It directly encodes spatial information such as\npoint coordinates or bounding boxes of geographic entities into the KG\nembedding space. The resulting model is capable of handling different types of\nspatial reasoning. We also construct a geographic knowledge graph as well as a\nset of geographic query-answer pairs called DBGeo to evaluate the performance\nof SE-KGE in comparison to multiple baselines. Evaluation results show that\nSE-KGE outperforms these baselines on the DBGeo dataset for geographic logic\nquery answering task. This demonstrates the effectiveness of our\nspatially-explicit model and the importance of considering the scale of\ndifferent geographic entities. Finally, we introduce a novel downstream task\ncalled spatial semantic lifting which links an arbitrary location in the study\narea to entities in the KG via some relations. Evaluation on DBGeo shows that\nour model outperforms the baseline by a substantial margin.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 17:46:31 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Mai", "Gengchen", ""], ["Janowicz", "Krzysztof", ""], ["Cai", "Ling", ""], ["Zhu", "Rui", ""], ["Regalia", "Blake", ""], ["Yan", "Bo", ""], ["Shi", "Meilin", ""], ["Lao", "Ni", ""]]}, {"id": "2004.14174", "submitter": "John Morris", "authors": "John X. Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, Yanjun Qi", "title": "Reevaluating Adversarial Examples in Natural Language", "comments": "15 pages; 9 Tables; 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art attacks on NLP models lack a shared definition of a what\nconstitutes a successful attack. We distill ideas from past work into a unified\nframework: a successful natural language adversarial example is a perturbation\nthat fools the model and follows some linguistic constraints. We then analyze\nthe outputs of two state-of-the-art synonym substitution attacks. We find that\ntheir perturbations often do not preserve semantics, and 38% introduce\ngrammatical errors. Human surveys reveal that to successfully preserve\nsemantics, we need to significantly increase the minimum cosine similarities\nbetween the embeddings of swapped words and between the sentence encodings of\noriginal and perturbed sentences.With constraints adjusted to better preserve\nsemantics and grammaticality, the attack success rate drops by over 70\npercentage points.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 03:09:48 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 04:16:23 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Morris", "John X.", ""], ["Lifland", "Eli", ""], ["Lanchantin", "Jack", ""], ["Ji", "Yangfeng", ""], ["Qi", "Yanjun", ""]]}, {"id": "2004.14176", "submitter": "Emeka Ogbuju Mr", "authors": "Emeka Ogbuju and Moses Onyesolu", "title": "Development of a General Purpose Sentiment Lexicon for Igbo Language", "comments": "Accepted and presented at the Widening Natural Language Processing\n  (WiNLP) workshop, co-located with the Association for Computational\n  Linguistics (ACL) conference 2019 in Florence, Italy. See\n  https://www.winlp.org/wp-content/uploads/2019/final_papers/103_Paper.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are publicly available general purpose sentiment lexicons in some high\nresource languages but very few exist in the low resource languages. This makes\nit difficult to directly perform sentiment analysis tasks in such languages.\nThe objective of this work is to create a general purpose sentiment lexicon for\nthe Igbo language that can determine the sentiment of documents written in the\nIgbo language without having to translate it to the English language. The\nmaterial used was an automatically translated lexicon by Liu and the manual\naddition of Igbo native words. The result of this work is a general purpose\nlexicon called IgboSentilex. The performance was tested on the BBC Igbo news\nchannel. It returned an average polarity agreement of 95.75 percent with other\ngeneral purpose sentiment lexicons.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 22:10:34 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Ogbuju", "Emeka", ""], ["Onyesolu", "Moses", ""]]}, {"id": "2004.14198", "submitter": "Muqiao Yang", "authors": "Yao-Hung Hubert Tsai, Martin Q. Ma, Muqiao Yang, Ruslan Salakhutdinov,\n  and Louis-Philippe Morency", "title": "Multimodal Routing: Improving Local and Global Interpretability of\n  Multimodal Language Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human language can be expressed through multiple sources of information\nknown as modalities, including tones of voice, facial gestures, and spoken\nlanguage. Recent multimodal learning with strong performances on human-centric\ntasks such as sentiment analysis and emotion recognition are often black-box,\nwith very limited interpretability. In this paper we propose Multimodal\nRouting, which dynamically adjusts weights between input modalities and output\nrepresentations differently for each input sample. Multimodal routing can\nidentify relative importance of both individual modalities and cross-modality\nfeatures. Moreover, the weight assignment by routing allows us to interpret\nmodality-prediction relationships not only globally (i.e. general trends over\nthe whole dataset), but also locally for each single input sample, meanwhile\nkeeping competitive performance compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 13:42:22 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 04:56:42 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Tsai", "Yao-Hung Hubert", ""], ["Ma", "Martin Q.", ""], ["Yang", "Muqiao", ""], ["Salakhutdinov", "Ruslan", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2004.14200", "submitter": "Sufeng Duan", "authors": "Sufeng Duan, Hai Zhao, Dongdong Zhang, Rui Wang", "title": "Syntax-aware Data Augmentation for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is an effective performance enhancement in neural machine\ntranslation (NMT) by generating additional bilingual data. In this paper, we\npropose a novel data augmentation enhancement strategy for neural machine\ntranslation. Different from existing data augmentation methods which simply\nchoose words with the same probability across different sentences for\nmodification, we set sentence-specific probability for word selection by\nconsidering their roles in sentence. We use dependency parse tree of input\nsentence as an effective clue to determine selecting probability for every\nwords in each sentence. Our proposed method is evaluated on WMT14\nEnglish-to-German dataset and IWSLT14 German-to-English dataset. The result of\nextensive experiments show our proposed syntax-aware data augmentation method\nmay effectively boost existing sentence-independent methods for significant\ntranslation performance improvement.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 13:45:30 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Duan", "Sufeng", ""], ["Zhao", "Hai", ""], ["Zhang", "Dongdong", ""], ["Wang", "Rui", ""]]}, {"id": "2004.14201", "submitter": "Ruize Wang", "authors": "Ruize Wang, Duyu Tang, Nan Duan, Wanjun Zhong, Zhongyu Wei, Xuanjing\n  Huang, Daxin Jiang, Ming Zhou", "title": "Leveraging Declarative Knowledge in Text and First-Order Logic for\n  Fine-Grained Propaganda Detection", "comments": "Accepted as a long paper to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the detection of propagandistic text fragments in news articles.\nInstead of merely learning from input-output datapoints in training data, we\nintroduce an approach to inject declarative knowledge of fine-grained\npropaganda techniques. Specifically, we leverage the declarative knowledge\nexpressed in both first-order logic and natural language. The former refers to\nthe logical consistency between coarse- and fine-grained predictions, which is\nused to regularize the training process with propositional Boolean expressions.\nThe latter refers to the literal definition of each propaganda technique, which\nis utilized to get class representations for regularizing the model parameters.\nWe conduct experiments on Propaganda Techniques Corpus, a large manually\nannotated dataset for fine-grained propaganda detection. Experiments show that\nour method achieves superior performance, demonstrating that leveraging\ndeclarative knowledge can help the model to make more accurate predictions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 13:46:15 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 13:08:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wang", "Ruize", ""], ["Tang", "Duyu", ""], ["Duan", "Nan", ""], ["Zhong", "Wanjun", ""], ["Wei", "Zhongyu", ""], ["Huang", "Xuanjing", ""], ["Jiang", "Daxin", ""], ["Zhou", "Ming", ""]]}, {"id": "2004.14218", "submitter": "Zihan Liu", "authors": "Zihan Liu, Genta Indra Winata, Andrea Madotto, Pascale Fung", "title": "Exploring Fine-tuning Techniques for Pre-trained Cross-lingual Models\n  via Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, fine-tuning pre-trained language models (e.g., multilingual BERT)\nto downstream cross-lingual tasks has shown promising results. However, the\nfine-tuning process inevitably changes the parameters of the pre-trained model\nand weakens its cross-lingual ability, which leads to sub-optimal performance.\nTo alleviate this problem, we leverage continual learning to preserve the\noriginal cross-lingual ability of the pre-trained model when we fine-tune it to\ndownstream tasks. The experimental result shows that our fine-tuning methods\ncan better preserve the cross-lingual ability of the pre-trained model in a\nsentence retrieval task. Our methods also achieve better performance than other\nfine-tuning baselines on the zero-shot cross-lingual part-of-speech tagging and\nnamed entity recognition tasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 14:07:18 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 08:43:24 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Liu", "Zihan", ""], ["Winata", "Genta Indra", ""], ["Madotto", "Andrea", ""], ["Fung", "Pascale", ""]]}, {"id": "2004.14224", "submitter": "Tao Shen", "authors": "Tao Shen, Yi Mao, Pengcheng He, Guodong Long, Adam Trischler, Weizhu\n  Chen", "title": "Exploiting Structured Knowledge in Text via Graph-Guided Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim at equipping pre-trained language models with structured\nknowledge. We present two self-supervised tasks learning over raw text with the\nguidance from knowledge graphs. Building upon entity-level masked language\nmodels, our first contribution is an entity masking scheme that exploits\nrelational knowledge underlying the text. This is fulfilled by using a linked\nknowledge graph to select informative entities and then masking their mentions.\nIn addition we use knowledge graphs to obtain distractors for the masked\nentities, and propose a novel distractor-suppressed ranking objective which is\noptimized jointly with masked language model. In contrast to existing\nparadigms, our approach uses knowledge graphs implicitly, only during\npre-training, to inject language models with structured knowledge via learning\nfrom raw text. It is more efficient than retrieval-based methods that perform\nentity linking and integration during finetuning and inference, and generalizes\nmore effectively than the methods that directly learn from concatenated graph\ntriples. Experiments show that our proposed model achieves improved performance\non five benchmark datasets, including question answering and knowledge base\ncompletion tasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 14:22:42 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Shen", "Tao", ""], ["Mao", "Yi", ""], ["He", "Pengcheng", ""], ["Long", "Guodong", ""], ["Trischler", "Adam", ""], ["Chen", "Weizhu", ""]]}, {"id": "2004.14228", "submitter": "Genta Indra Winata", "authors": "Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, Peng\n  Xu, Pascale Fung", "title": "Meta-Transfer Learning for Code-Switched Speech Recognition", "comments": "Accepted in ACL 2020. The first two authors contributed equally to\n  this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of people in the world today speak a mixed-language as a\nresult of being multilingual. However, building a speech recognition system for\ncode-switching remains difficult due to the availability of limited resources\nand the expense and significant effort required to collect mixed-language data.\nWe therefore propose a new learning method, meta-transfer learning, to transfer\nlearn on a code-switched speech recognition system in a low-resource setting by\njudiciously extracting information from high-resource monolingual datasets. Our\nmodel learns to recognize individual languages, and transfer them so as to\nbetter recognize mixed-language speech by conditioning the optimization on the\ncode-switching data. Based on experimental results, our model outperforms\nexisting baselines on speech recognition and language modeling tasks, and is\nfaster to converge.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 14:27:19 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Winata", "Genta Indra", ""], ["Cahyawijaya", "Samuel", ""], ["Lin", "Zhaojiang", ""], ["Liu", "Zihan", ""], ["Xu", "Peng", ""], ["Fung", "Pascale", ""]]}, {"id": "2004.14236", "submitter": "Lucia Donatelli", "authors": "Lucia Donatelli, Jonas Groschwitz, Alexander Koller, Matthias\n  Lindemann, Pia Wei{\\ss}enhorn", "title": "Normalizing Compositional Structures Across Graphbanks", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of a variety of graph-based meaning representations (MRs) has\nsparked an important conversation about how to adequately represent semantic\nstructure. These MRs exhibit structural differences that reflect different\ntheoretical and design considerations, presenting challenges to uniform\nlinguistic analysis and cross-framework semantic parsing. Here, we ask the\nquestion of which design differences between MRs are meaningful and\nsemantically-rooted, and which are superficial. We present a methodology for\nnormalizing discrepancies between MRs at the compositional level (Lindemann et\nal., 2019), finding that we can normalize the majority of divergent phenomena\nusing linguistically-grounded rules. Our work significantly increases the match\nin compositional structure between MRs and improves multi-task learning (MTL)\nin a low-resource setting, demonstrating the usefulness of careful MR design\nanalysis and comparison.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 14:35:50 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 10:04:12 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Donatelli", "Lucia", ""], ["Groschwitz", "Jonas", ""], ["Koller", "Alexander", ""], ["Lindemann", "Matthias", ""], ["Wei\u00dfenhorn", "Pia", ""]]}, {"id": "2004.14243", "submitter": "Preksha Nema I", "authors": "Akash Kumar Mohankumar, Preksha Nema, Sharan Narasimhan, Mitesh M.\n  Khapra, Balaji Vasan Srinivasan, Balaraman Ravindran", "title": "Towards Transparent and Explainable Attention Models", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on interpretability of attention distributions have led to\nnotions of faithful and plausible explanations for a model's predictions.\nAttention distributions can be considered a faithful explanation if a higher\nattention weight implies a greater impact on the model's prediction. They can\nbe considered a plausible explanation if they provide a human-understandable\njustification for the model's predictions. In this work, we first explain why\ncurrent attention mechanisms in LSTM based encoders can neither provide a\nfaithful nor a plausible explanation of the model's predictions. We observe\nthat in LSTM based encoders the hidden representations at different time-steps\nare very similar to each other (high conicity) and attention weights in these\nsituations do not carry much meaning because even a random permutation of the\nattention weights does not affect the model's predictions. Based on experiments\non a wide variety of tasks and datasets, we observe attention distributions\noften attribute the model's predictions to unimportant words such as\npunctuation and fail to offer a plausible explanation for the predictions. To\nmake attention mechanisms more faithful and plausible, we propose a modified\nLSTM cell with a diversity-driven training objective that ensures that the\nhidden representations learned at different time steps are diverse. We show\nthat the resulting attention distributions offer more transparency as they (i)\nprovide a more precise importance ranking of the hidden states (ii) are better\nindicative of words important for the model's predictions (iii) correlate\nbetter with gradient-based attribution methods. Human evaluations indicate that\nthe attention distributions learned by our model offer a plausible explanation\nof the model's predictions. Our code has been made publicly available at\nhttps://github.com/akashkm99/Interpretable-Attention\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 14:47:50 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Mohankumar", "Akash Kumar", ""], ["Nema", "Preksha", ""], ["Narasimhan", "Sharan", ""], ["Khapra", "Mitesh M.", ""], ["Srinivasan", "Balaji Vasan", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "2004.14253", "submitter": "Marco Guerini", "authors": "Lorenzo De Mattei, Michele Cafagna, Felice Dell'Orletta, Malvina\n  Nissim, Marco Guerini", "title": "GePpeTto Carves Italian into a Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, pre-trained neural architectures have provided\nimpressive improvements across several NLP tasks. Still, generative language\nmodels are available mainly for English. We develop GePpeTto, the first\ngenerative language model for Italian, built using the GPT-2 architecture. We\nprovide a thorough analysis of GePpeTto's quality by means of both an automatic\nand a human-based evaluation. The automatic assessment consists in (i)\ncalculating perplexity across different genres and (ii) a profiling analysis\nover GePpeTto's writing characteristics. We find that GePpeTto's production is\na sort of bonsai version of human production, with shorter but yet complex\nsentences. Human evaluation is performed over a sentence completion task, where\nGePpeTto's output is judged as natural more often than not, and much closer to\nthe original human texts than to a simpler language model which we take as\nbaseline.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:02:01 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["De Mattei", "Lorenzo", ""], ["Cafagna", "Michele", ""], ["Dell'Orletta", "Felice", ""], ["Nissim", "Malvina", ""], ["Guerini", "Marco", ""]]}, {"id": "2004.14254", "submitter": "Kangenbei Liao", "authors": "Kangenbei Liao, Qianlong Liu, Zhongyu Wei, Baolin Peng, Qin Chen,\n  Weijian Sun, Xuanjing Huang", "title": "Task-oriented Dialogue System for Automatic Disease Diagnosis via\n  Hierarchical Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on automatic disease diagnosis with reinforcement\nlearning (RL) methods in task-oriented dialogues setting. Different from\nconventional RL tasks, the action space for disease diagnosis (i.e., symptoms)\nis inevitably large, especially when the number of diseases increases. However,\nexisting approaches to this problem employ a flat RL policy, which typically\nworks well in simple tasks but has significant challenges in complex scenarios\nlike disease diagnosis. Towards this end, we propose to integrate a\nhierarchical policy of two levels into the dialogue policy learning. The high\nlevel policy consists of a model named master that is responsible for\ntriggering a model in low level, the low level policy consists of several\nsymptom checkers and a disease classifier. Experimental results on both\nself-constructed real-world and synthetic datasets demonstrate that our\nhierarchical framework achieves higher accuracy in disease diagnosis compared\nwith existing systems. Besides, the datasets\n(http://www.sdspeople.fudan.edu.cn/zywei/data/Fudan-Medical-Dialogue2.0) and\ncodes (https://github.com/nnbay/MeicalChatbot-HRL) are all available now.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:02:41 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Liao", "Kangenbei", ""], ["Liu", "Qianlong", ""], ["Wei", "Zhongyu", ""], ["Peng", "Baolin", ""], ["Chen", "Qin", ""], ["Sun", "Weijian", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2004.14257", "submitter": "Aman Madaan", "authors": "Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos, Graham\n  Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W Black, Shrimai Prabhumoye", "title": "Politeness Transfer: A Tag and Generate Approach", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper introduces a new task of politeness transfer which involves\nconverting non-polite sentences to polite sentences while preserving the\nmeaning. We also provide a dataset of more than 1.39 instances automatically\nlabeled for politeness to encourage benchmark evaluations on this new task. We\ndesign a tag and generate pipeline that identifies stylistic attributes and\nsubsequently generates a sentence in the target style while preserving most of\nthe source content. For politeness as well as five other transfer tasks, our\nmodel outperforms the state-of-the-art methods on automatic metrics for content\npreservation, with a comparable or better performance on style transfer\naccuracy. Additionally, our model surpasses existing methods on human\nevaluations for grammaticality, meaning preservation and transfer accuracy\nacross all the six style transfer tasks. The data and code is located at\nhttps://github.com/tag-and-generate.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:08:53 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 22:33:41 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Madaan", "Aman", ""], ["Setlur", "Amrith", ""], ["Parekh", "Tanmay", ""], ["Poczos", "Barnabas", ""], ["Neubig", "Graham", ""], ["Yang", "Yiming", ""], ["Salakhutdinov", "Ruslan", ""], ["Black", "Alan W", ""], ["Prabhumoye", "Shrimai", ""]]}, {"id": "2004.14265", "submitter": "Epaminondas Kapetanios", "authors": "Epaminondas Kapetanios, Vijayan Sugumaran, and Anastassia Angelopoulou", "title": "Exploring the Suitability of Semantic Spaces as Word Association Models\n  for the Extraction of Semantic Relationships", "comments": "10 pages, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Given the recent advances and progress in Natural Language Processing (NLP),\nextraction of semantic relationships has been at the top of the research agenda\nin the last few years. This work has been mainly motivated by the fact that\nbuilding knowledge graphs (KG) and bases (KB), as a key ingredient of\nintelligent applications, is a never-ending challenge, since new knowledge\nneeds to be harvested while old knowledge needs to be revised. Currently,\napproaches towards relation extraction from text are dominated by neural models\npracticing some sort of distant (weak) supervision in machine learning from\nlarge corpora, with or without consulting external knowledge sources. In this\npaper, we empirically study and explore the potential of a novel idea of using\nclassical semantic spaces and models, e.g., Word Embedding, generated for\nextracting word association, in conjunction with relation extraction\napproaches. The goal is to use these word association models to reinforce\ncurrent relation extraction approaches. We believe that this is a first attempt\nof this kind and the results of the study should shed some light on the extent\nto which these word association models can be used as well as the most\npromising types of relationships to be considered for extraction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:25:28 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Kapetanios", "Epaminondas", ""], ["Sugumaran", "Vijayan", ""], ["Angelopoulou", "Anastassia", ""]]}, {"id": "2004.14280", "submitter": "Jind\\v{r}ich Libovick\\'y", "authors": "Jind\\v{r}ich Libovick\\'y, Alexander Fraser", "title": "Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning\n  Subword Systems", "comments": "8 pages, 1 figure; Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying the Transformer architecture on the character level usually requires\nvery deep architectures that are difficult and slow to train. These problems\ncan be partially overcome by incorporating a segmentation into tokens in the\nmodel. We show that by initially training a subword model and then finetuning\nit on characters, we can obtain a neural machine translation model that works\nat the character level without requiring token segmentation. We use only the\nvanilla 6-layer Transformer Base architecture. Our character-level models\nbetter capture morphological phenomena and show more robustness to noise at the\nexpense of somewhat worse overall translation quality. Our study is a\nsignificant step towards high-performance and easy to train character-based\nmodels that are not extremely large.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:56:02 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 14:46:28 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Libovick\u00fd", "Jind\u0159ich", ""], ["Fraser", "Alexander", ""]]}, {"id": "2004.14283", "submitter": "Johannes Bjerva", "authors": "Johannes Bjerva, Nikita Bhutani, Behzad Golshan, Wang-Chiew Tan, and\n  Isabelle Augenstein", "title": "SubjQA: A Dataset for Subjectivity and Review Comprehension", "comments": "EMNLP 2020 Long Paper - Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Subjectivity is the expression of internal opinions or beliefs which cannot\nbe objectively observed or verified, and has been shown to be important for\nsentiment analysis and word-sense disambiguation. Furthermore, subjectivity is\nan important aspect of user-generated data. In spite of this, subjectivity has\nnot been investigated in contexts where such data is widespread, such as in\nquestion answering (QA). We therefore investigate the relationship between\nsubjectivity and QA, while developing a new dataset. We compare and contrast\nwith analyses from previous work, and verify that findings regarding\nsubjectivity still hold when using recently developed NLP architectures. We\nfind that subjectivity is also an important feature in the case of QA, albeit\nwith more intricate interactions between subjectivity and QA performance. For\ninstance, a subjective question may or may not be associated with a subjective\nanswer. We release an English QA dataset (SubjQA) based on customer reviews,\ncontaining subjectivity annotations for questions and answer spans across 6\ndistinct domains.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:59:30 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 13:36:44 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 06:04:27 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Bjerva", "Johannes", ""], ["Bhutani", "Nikita", ""], ["Golshan", "Behzad", ""], ["Tan", "Wang-Chiew", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2004.14287", "submitter": "Myle Ott", "authors": "Jingfei Du, Myle Ott, Haoran Li, Xing Zhou, Veselin Stoyanov", "title": "General Purpose Text Embeddings from Pre-trained Language Models for\n  Scalable Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of the art on many NLP tasks is currently achieved by large\npre-trained language models, which require a considerable amount of\ncomputation. We explore a setting where many different predictions are made on\na single piece of text. In that case, some of the computational cost during\ninference can be amortized over the different tasks using a shared text\nencoder. We compare approaches for training such an encoder and show that\nencoders pre-trained over multiple tasks generalize well to unseen tasks. We\nalso compare ways of extracting fixed- and limited-size representations from\nthis encoder, including different ways of pooling features extracted from\nmultiple layers or positions. Our best approach compares favorably to knowledge\ndistillation, achieving higher accuracy and lower computational cost once the\nsystem is handling around 7 tasks. Further, we show that through binary\nquantization, we can reduce the size of the extracted representations by a\nfactor of 16 making it feasible to store them for later use. The resulting\nmethod offers a compelling solution for using large-scale pre-trained models at\na fraction of the computational cost when multiple tasks are performed on the\nsame text.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:11:26 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Du", "Jingfei", ""], ["Ott", "Myle", ""], ["Li", "Haoran", ""], ["Zhou", "Xing", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "2004.14299", "submitter": "Shrey Desai", "authors": "Shrey Desai, Cornelia Caragea, and Junyi Jessy Li", "title": "Detecting Perceived Emotions in Hurricane Disasters", "comments": "Accepted to ACL 2020; code available at\n  https://github.com/shreydesai/hurricane", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural disasters (e.g., hurricanes) affect millions of people each year,\ncausing widespread destruction in their wake. People have recently taken to\nsocial media websites (e.g., Twitter) to share their sentiments and feelings\nwith the larger community. Consequently, these platforms have become\ninstrumental in understanding and perceiving emotions at scale. In this paper,\nwe introduce HurricaneEmo, an emotion dataset of 15,000 English tweets spanning\nthree hurricanes: Harvey, Irma, and Maria. We present a comprehensive study of\nfine-grained emotions and propose classification tasks to discriminate between\ncoarse-grained emotion groups. Our best BERT model, even after task-guided\npre-training which leverages unlabeled Twitter data, achieves only 68% accuracy\n(averaged across all groups). HurricaneEmo serves not only as a challenging\nbenchmark for models but also as a valuable resource for analyzing emotions in\ndisaster-centric domains.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:17:49 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Desai", "Shrey", ""], ["Caragea", "Cornelia", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "2004.14302", "submitter": "Shiki Sato", "authors": "Shiki Sato, Reina Akama, Hiroki Ouchi, Jun Suzuki, Kentaro Inui", "title": "Evaluating Dialogue Generation Systems via Response Selection", "comments": "accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing automatic evaluation metrics for open-domain dialogue response\ngeneration systems correlate poorly with human evaluation. We focus on\nevaluating response generation systems via response selection. To evaluate\nsystems properly via response selection, we propose the method to construct\nresponse selection test sets with well-chosen false candidates. Specifically,\nwe propose to construct test sets filtering out some types of false candidates:\n(i) those unrelated to the ground-truth response and (ii) those acceptable as\nappropriate responses. Through experiments, we demonstrate that evaluating\nsystems via response selection with the test sets developed by our method\ncorrelates more strongly with human evaluation, compared with widely used\nautomatic evaluation metrics such as BLEU.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:21:50 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Sato", "Shiki", ""], ["Akama", "Reina", ""], ["Ouchi", "Hiroki", ""], ["Suzuki", "Jun", ""], ["Inui", "Kentaro", ""]]}, {"id": "2004.14303", "submitter": "Hatem Haddad", "authors": "Chayma Fourati, Abir Messaoudi and Hatem Haddad", "title": "TUNIZI: a Tunisian Arabizi sentiment analysis Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  On social media, Arabic people tend to express themselves in their own local\ndialects. More particularly, Tunisians use the informal way called \"Tunisian\nArabizi\". Analytical studies seek to explore and recognize online opinions\naiming to exploit them for planning and prediction purposes such as measuring\nthe customer satisfaction and establishing sales and marketing strategies.\nHowever, analytical studies based on Deep Learning are data hungry. On the\nother hand, African languages and dialects are considered low resource\nlanguages. For instance, to the best of our knowledge, no annotated Tunisian\nArabizi dataset exists. In this paper, we introduce TUNIZI a sentiment analysis\nTunisian Arabizi Dataset, collected from social networks, preprocessed for\nanalytical studies and annotated manually by Tunisian native speakers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:24:02 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Fourati", "Chayma", ""], ["Messaoudi", "Abir", ""], ["Haddad", "Hatem", ""]]}, {"id": "2004.14307", "submitter": "Hung Le", "authors": "Hung Le, Doyen Sahoo, Chenghao Liu, Nancy F. Chen, Steven C.H. Hoi", "title": "UniConv: A Unified Conversational Neural Architecture for Multi-domain\n  Task-oriented Dialogues", "comments": "Accepted The 2020 Conference on Empirical Methods in Natural Language\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building an end-to-end conversational agent for multi-domain task-oriented\ndialogues has been an open challenge for two main reasons. First, tracking\ndialogue states of multiple domains is non-trivial as the dialogue agent must\nobtain complete states from all relevant domains, some of which might have\nshared slots among domains as well as unique slots specifically for one domain\nonly. Second, the dialogue agent must also process various types of information\nacross domains, including dialogue context, dialogue states, and database, to\ngenerate natural responses to users. Unlike the existing approaches that are\noften designed to train each module separately, we propose \"UniConv\" -- a novel\nunified neural architecture for end-to-end conversational systems in\nmulti-domain task-oriented dialogues, which is designed to jointly train (i) a\nBi-level State Tracker which tracks dialogue states by learning signals at both\nslot and domain level independently, and (ii) a Joint Dialogue Act and Response\nGenerator which incorporates information from various input components and\nmodels dialogue acts and target responses simultaneously. We conduct\ncomprehensive experiments in dialogue state tracking, context-to-text, and\nend-to-end settings on the MultiWOZ2.1 benchmark, achieving superior\nperformance over competitive baselines.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:28:22 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 03:52:34 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Le", "Hung", ""], ["Sahoo", "Doyen", ""], ["Liu", "Chenghao", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2004.14312", "submitter": "Shabnam Behzad", "authors": "Shabnam Behzad, Amir Zeldes", "title": "A Cross-Genre Ensemble Approach to Robust Reddit Part of Speech Tagging", "comments": "Proceedings of the 12th Web as Corpus Workshop (WAC-XII)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part of speech tagging is a fundamental NLP task often regarded as solved for\nhigh-resource languages such as English. Current state-of-the-art models have\nachieved high accuracy, especially on the news domain. However, when these\nmodels are applied to other corpora with different genres, and especially\nuser-generated data from the Web, we see substantial drops in performance. In\nthis work, we study how a state-of-the-art tagging model trained on different\ngenres performs on Web content from unfiltered Reddit forum discussions. More\nspecifically, we use data from multiple sources: OntoNotes, a large benchmark\ncorpus with 'well-edited' text, the English Web Treebank with 5 Web genres, and\nGUM, with 7 further genres other than Reddit. We report the results when\ntraining on different splits of the data, tested on Reddit. Our results show\nthat even small amounts of in-domain data can outperform the contribution of\ndata an order of magnitude larger coming from other Web domains. To make\nprogress on out-of-domain tagging, we also evaluate an ensemble approach using\nmultiple single-genre taggers as input features to a meta-classifier. We\npresent state of the art performance on tagging Reddit data, as well as error\nanalysis of the results of these models, and offer a typology of the most\ncommon error types among them, broken down by training corpus.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:36:38 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Behzad", "Shabnam", ""], ["Zeldes", "Amir", ""]]}, {"id": "2004.14325", "submitter": "Daniel Loureiro", "authors": "Daniel Loureiro and Jose Camacho-Collados", "title": "Don't Neglect the Obvious: On the Role of Unambiguous Words in Word\n  Sense Disambiguation", "comments": "Accepted to EMNLP 2020. Website: http://danlou.github.io/uwa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for Word Sense Disambiguation (WSD) combine two\ndifferent features: the power of pre-trained language models and a propagation\nmethod to extend the coverage of such models. This propagation is needed as\ncurrent sense-annotated corpora lack coverage of many instances in the\nunderlying sense inventory (usually WordNet). At the same time, unambiguous\nwords make for a large portion of all words in WordNet, while being poorly\ncovered in existing sense-annotated corpora. In this paper, we propose a simple\nmethod to provide annotations for most unambiguous words in a large corpus. We\nintroduce the UWA (Unambiguous Word Annotations) dataset and show how a\nstate-of-the-art propagation-based model can use it to extend the coverage and\nquality of its word sense embeddings by a significant margin, improving on its\noriginal results on WSD.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:51:21 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 08:57:53 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 09:20:11 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Loureiro", "Daniel", ""], ["Camacho-Collados", "Jose", ""]]}, {"id": "2004.14327", "submitter": "Ahmet \\\"Ust\\\"un", "authors": "Ahmet \\\"Ust\\\"un, Arianna Bisazza, Gosse Bouma, Gertjan van Noord", "title": "UDapter: Language Adaptation for Truly Universal Dependency Parsing", "comments": "In EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in multilingual dependency parsing have brought the idea of a\ntruly universal parser closer to reality. However, cross-language interference\nand restrained model capacity remain major obstacles. To address this, we\npropose a novel multilingual task adaptation approach based on contextual\nparameter generation and adapter modules. This approach enables to learn\nadapters via language embeddings while sharing model parameters across\nlanguages. It also allows for an easy but effective integration of existing\nlinguistic typology features into the parsing network. The resulting parser,\nUDapter, outperforms strong monolingual and multilingual baselines on the\nmajority of both high-resource and low-resource (zero-shot) languages, showing\nthe success of the proposed adaptation approach. Our in-depth analyses show\nthat soft parameter sharing via typological features is key to this success.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:52:50 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 15:46:41 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["\u00dcst\u00fcn", "Ahmet", ""], ["Bisazza", "Arianna", ""], ["Bouma", "Gosse", ""], ["van Noord", "Gertjan", ""]]}, {"id": "2004.14338", "submitter": "Jack Hessel", "authors": "Jack Hessel, Zhenhai Zhu, Bo Pang, Radu Soricut", "title": "Beyond Instructional Videos: Probing for More Diverse Visual-Textual\n  Grounding on YouTube", "comments": "11 pages including supplementary materials", "journal-ref": "Published in EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining from unlabelled web videos has quickly become the de-facto means\nof achieving high performance on many video understanding tasks. Features are\nlearned via prediction of grounded relationships between visual content and\nautomatic speech recognition (ASR) tokens. However, prior pretraining work has\nbeen limited to only instructional videos; a priori, we expect this domain to\nbe relatively \"easy:\" speakers in instructional videos will often reference the\nliteral objects/actions being depicted. We ask: can similar models be trained\non more diverse video corpora? And, if so, what types of videos are \"grounded\"\nand what types are not? We fit a representative pretraining model to the\ndiverse YouTube8M dataset, and study its success and failure cases. We find\nthat visual-textual grounding is indeed possible across previously unexplored\nvideo categories, and that pretraining on a more diverse set results in\nrepresentations that generalize to both non-instructional and instructional\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:10:10 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 17:30:51 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hessel", "Jack", ""], ["Zhu", "Zhenhai", ""], ["Pang", "Bo", ""], ["Soricut", "Radu", ""]]}, {"id": "2004.14353", "submitter": "Weijia Xu", "authors": "Weijia Xu, Batool Haider, Saab Mansour", "title": "End-to-End Slot Alignment and Recognition for Cross-Lingual NLU", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language understanding (NLU) in the context of goal-oriented dialog\nsystems typically includes intent classification and slot labeling tasks.\nExisting methods to expand an NLU system to new languages use machine\ntranslation with slot label projection from source to the translated\nutterances, and thus are sensitive to projection errors. In this work, we\npropose a novel end-to-end model that learns to align and predict target slot\nlabels jointly for cross-lingual transfer. We introduce MultiATIS++, a new\nmultilingual NLU corpus that extends the Multilingual ATIS corpus to nine\nlanguages across four language families, and evaluate our method using the\ncorpus. Results show that our method outperforms a simple label projection\nmethod using fast-align on most languages, and achieves competitive performance\nto the more complex, state-of-the-art projection method with only half of the\ntraining time. We release our MultiATIS++ corpus to the community to continue\nfuture research on cross-lingual NLU.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:31:11 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 04:36:04 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Xu", "Weijia", ""], ["Haider", "Batool", ""], ["Mansour", "Saab", ""]]}, {"id": "2004.14355", "submitter": "Nithin Holla", "authors": "Nithin Holla, Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova", "title": "Learning to Learn to Disambiguate: Meta-Learning for Few-Shot Word Sense\n  Disambiguation", "comments": "Camera-ready: Findings of EMNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning methods hinges on the availability of large\ntraining datasets annotated for the task of interest. In contrast to human\nintelligence, these methods lack versatility and struggle to learn and adapt\nquickly to new tasks, where labeled data is scarce. Meta-learning aims to solve\nthis problem by training a model on a large number of few-shot tasks, with an\nobjective to learn new tasks quickly from a small number of examples. In this\npaper, we propose a meta-learning framework for few-shot word sense\ndisambiguation (WSD), where the goal is to learn to disambiguate unseen words\nfrom only a few labeled instances. Meta-learning approaches have so far been\ntypically tested in an $N$-way, $K$-shot classification setting where each task\nhas $N$ classes with $K$ examples per class. Owing to its nature, WSD deviates\nfrom this controlled setup and requires the models to handle a large number of\nhighly unbalanced classes. We extend several popular meta-learning approaches\nto this scenario, and analyze their strengths and weaknesses in this new\nchallenging setting.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:33:31 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 14:40:51 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 10:09:05 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Holla", "Nithin", ""], ["Mishra", "Pushkar", ""], ["Yannakoudakis", "Helen", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2004.14356", "submitter": "Robert Stojnic", "authors": "Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder,\n  Sebastian Riedel, Ross Taylor, Robert Stojnic", "title": "AxCell: Automatic Extraction of Results from Machine Learning Papers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Tracking progress in machine learning has become increasingly difficult with\nthe recent explosion in the number of papers. In this paper, we present AxCell,\nan automatic machine learning pipeline for extracting results from papers.\nAxCell uses several novel components, including a table segmentation subtask,\nto learn relevant structural knowledge that aids extraction. When compared with\nexisting methods, our approach significantly improves the state of the art for\nresults extraction. We also release a structured, annotated dataset for\ntraining models for results extraction, and a dataset for evaluating the\nperformance of models on this task. Lastly, we show the viability of our\napproach enables it to be used for semi-automated results extraction in\nproduction, suggesting our improvements make this task practically viable for\nthe first time. Code is available on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:33:41 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Kardas", "Marcin", ""], ["Czapla", "Piotr", ""], ["Stenetorp", "Pontus", ""], ["Ruder", "Sebastian", ""], ["Riedel", "Sebastian", ""], ["Taylor", "Ross", ""], ["Stojnic", "Robert", ""]]}, {"id": "2004.14357", "submitter": "Shuai Wang", "authors": "Shuai Wang, Guangyi Lv, Sahisnu Mazumder, Bing Liu", "title": "Detecting Domain Polarity-Changes of Words in a Sentiment Lexicon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment lexicons are instrumental for sentiment analysis. One can use a set\nof sentiment words provided in a sentiment lexicon and a lexicon-based\nclassifier to perform sentiment classification. One major issue with this\napproach is that many sentiment words are domain dependent. That is, they may\nbe positive in some domains but negative in some others. We refer to this\nproblem as domain polarity-changes of words. Detecting such words and\ncorrecting their sentiment for an application domain is very important. In this\npaper, we propose a graph-based technique to tackle this problem. Experimental\nresults show its effectiveness on multiple real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:35:05 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Wang", "Shuai", ""], ["Lv", "Guangyi", ""], ["Mazumder", "Sahisnu", ""], ["Liu", "Bing", ""]]}, {"id": "2004.14364", "submitter": "Gerasimos Lampouras", "authors": "Giulio Zhou and Gerasimos Lampouras", "title": "Generating Safe Diversity in NLG via Imitation Learning", "comments": "10 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning models for language generation tasks tend to produce repetitive\noutput. Various methods have been proposed to encourage lexical diversity\nduring decoding, but this often comes at a cost to the perceived fluency and\nadequacy of the output. In this work, we propose to ameliorate this cost by\nusing an Imitation Learning approach to explore the level of diversity that a\nlanguage generation model can safely produce. Specifically, we augment the\ndecoding process with a meta-classifier trained to distinguish which words at\nany given timestep will lead to high-quality output. We focus our experiments\non concept-to-text generation where models are sensitive to the inclusion of\nirrelevant words due to the strict relation between input and output. Our\nanalysis shows that previous methods for diversity underperform in this\nsetting, while human evaluation suggests that our proposed method achieves a\nhigh level of diversity with minimal effect to the output's fluency and\nadequacy.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:43:24 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Zhou", "Giulio", ""], ["Lampouras", "Gerasimos", ""]]}, {"id": "2004.14366", "submitter": "James Thorne", "authors": "James Thorne, Andreas Vlachos", "title": "Elastic weight consolidation for better bias inoculation", "comments": "Accepted at EACL 2021. Was previously submitted to arxiv with the\n  title \"Avoiding catastrophic forgetting in mitigating model biases in\n  sentence-pair classification with elastic weight consolidation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The biases present in training datasets have been shown to affect models for\nsentence pair classification tasks such as natural language inference (NLI) and\nfact verification. While fine-tuning models on additional data has been used to\nmitigate them, a common issue is that of catastrophic forgetting of the\noriginal training dataset. In this paper, we show that elastic weight\nconsolidation (EWC) allows fine-tuning of models to mitigate biases while being\nless susceptible to catastrophic forgetting. In our evaluation on fact\nverification and NLI stress tests, we show that fine-tuning with EWC dominates\nstandard fine-tuning, yielding models with lower levels of forgetting on the\noriginal (biased) dataset for equivalent gains in accuracy on the fine-tuning\n(unbiased) dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:45:12 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 10:57:26 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Thorne", "James", ""], ["Vlachos", "Andreas", ""]]}, {"id": "2004.14373", "submitter": "Ankur Parikh", "authors": "Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui,\n  Bhuwan Dhingra, Diyi Yang, Dipanjan Das", "title": "ToTTo: A Controlled Table-To-Text Generation Dataset", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ToTTo, an open-domain English table-to-text dataset with over\n120,000 training examples that proposes a controlled generation task: given a\nWikipedia table and a set of highlighted table cells, produce a one-sentence\ndescription. To obtain generated targets that are natural but also faithful to\nthe source table, we introduce a dataset construction process where annotators\ndirectly revise existing candidate sentences from Wikipedia. We present\nsystematic analyses of our dataset and annotation process as well as results\nachieved by several state-of-the-art baselines. While usually fluent, existing\nmethods often hallucinate phrases that are not supported by the table,\nsuggesting that this dataset can serve as a useful research benchmark for\nhigh-precision conditional text generation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:53:45 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 05:18:35 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 06:07:06 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Parikh", "Ankur P.", ""], ["Wang", "Xuezhi", ""], ["Gehrmann", "Sebastian", ""], ["Faruqui", "Manaal", ""], ["Dhingra", "Bhuwan", ""], ["Yang", "Diyi", ""], ["Das", "Dipanjan", ""]]}, {"id": "2004.14425", "submitter": "Fatma Arslan", "authors": "Fatma Arslan, Naeemul Hassan, Chengkai Li, Mark Tremayne", "title": "A Benchmark Dataset of Check-worthy Factual Claims", "comments": "Accepted to ICWSM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the ClaimBuster dataset of 23,533 statements\nextracted from all U.S. general election presidential debates and annotated by\nhuman coders. The ClaimBuster dataset can be leveraged in building\ncomputational methods to identify claims that are worth fact-checking from the\nmyriad of sources of digital or traditional media. The ClaimBuster dataset is\npublicly available to the research community, and it can be found at\nhttp://doi.org/10.5281/zenodo.3609356.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 18:39:15 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Arslan", "Fatma", ""], ["Hassan", "Naeemul", ""], ["Li", "Chengkai", ""], ["Tremayne", "Mark", ""]]}, {"id": "2004.14443", "submitter": "Johny Moreira", "authors": "Johny Moreira, Chaina Oliveira, David Mac\\^edo, Cleber Zanchettin,\n  Luciano Barbosa", "title": "Distantly-Supervised Neural Relation Extraction with Side Information\n  using BERT", "comments": "2020 International Joint Conference on Neural Networks (IJCNN)", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN48605.2020.9206648", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction (RE) consists in categorizing the relationship between\nentities in a sentence. A recent paradigm to develop relation extractors is\nDistant Supervision (DS), which allows the automatic creation of new datasets\nby taking an alignment between a text corpus and a Knowledge Base (KB). KBs can\nsometimes also provide additional information to the RE task. One of the\nmethods that adopt this strategy is the RESIDE model, which proposes a\ndistantly-supervised neural relation extraction using side information from\nKBs. Considering that this method outperformed state-of-the-art baselines, in\nthis paper, we propose a related approach to RESIDE also using additional side\ninformation, but simplifying the sentence encoding with BERT embeddings.\nThrough experiments, we show the effectiveness of the proposed method in Google\nDistant Supervision and Riedel datasets concerning the BGWA and RESIDE baseline\nmethods. Although Area Under the Curve is decreased because of unbalanced\ndatasets, P@N results have shown that the use of BERT as sentence encoding\nallows superior performance to baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 19:29:10 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 21:45:15 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 20:30:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Moreira", "Johny", ""], ["Oliveira", "Chaina", ""], ["Mac\u00eado", "David", ""], ["Zanchettin", "Cleber", ""], ["Barbosa", "Luciano", ""]]}, {"id": "2004.14444", "submitter": "John Miller", "authors": "John Miller, Karl Krauth, Benjamin Recht, Ludwig Schmidt", "title": "The Effect of Natural Distribution Shift on Question Answering Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build four new test sets for the Stanford Question Answering Dataset\n(SQuAD) and evaluate the ability of question-answering systems to generalize to\nnew data. Our first test set is from the original Wikipedia domain and measures\nthe extent to which existing systems overfit the original test set. Despite\nseveral years of heavy test set re-use, we find no evidence of adaptive\noverfitting. The remaining three test sets are constructed from New York Times\narticles, Reddit posts, and Amazon product reviews and measure robustness to\nnatural distribution shifts. Across a broad range of models, we observe average\nperformance drops of 3.8, 14.0, and 17.4 F1 points, respectively. In contrast,\na strong human baseline matches or exceeds the performance of SQuAD models on\nthe original domain and exhibits little to no drop in new domains. Taken\ntogether, our results confirm the surprising resilience of the holdout method\nand emphasize the need to move towards evaluation metrics that incorporate\nrobustness to natural distribution shifts.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 19:34:19 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Miller", "John", ""], ["Krauth", "Karl", ""], ["Recht", "Benjamin", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "2004.14448", "submitter": "Amil Merchant", "authors": "Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, Ian Tenney", "title": "What Happens To BERT Embeddings During Fine-tuning?", "comments": "9 pages (not including references), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there has been much recent work studying how linguistic information is\nencoded in pre-trained sentence representations, comparatively little is\nunderstood about how these models change when adapted to solve downstream\ntasks. Using a suite of analysis techniques (probing classifiers,\nRepresentational Similarity Analysis, and model ablations), we investigate how\nfine-tuning affects the representations of the BERT model. We find that while\nfine-tuning necessarily makes significant changes, it does not lead to\ncatastrophic forgetting of linguistic phenomena. We instead find that\nfine-tuning primarily affects the top layers of BERT, but with noteworthy\nvariation across tasks. In particular, dependency parsing reconfigures most of\nthe model, whereas SQuAD and MNLI appear to involve much shallower processing.\nFinally, we also find that fine-tuning has a weaker effect on representations\nof out-of-domain sentences, suggesting room for improvement in model\ngeneralization.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 19:46:26 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Merchant", "Amil", ""], ["Rahimtoroghi", "Elahe", ""], ["Pavlick", "Ellie", ""], ["Tenney", "Ian", ""]]}, {"id": "2004.14451", "submitter": "Allen Nie", "authors": "Allen Nie, Reuben Cohn-Gordon, and Christopher Potts", "title": "Pragmatic Issue-Sensitive Image Captioning", "comments": "15 pages, 7 figures. EMNLP 2020 Findings Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Image captioning systems have recently improved dramatically, but they still\ntend to produce captions that are insensitive to the communicative goals that\ncaptions should meet. To address this, we propose Issue-Sensitive Image\nCaptioning (ISIC). In ISIC, a captioning system is given a target image and an\nissue, which is a set of images partitioned in a way that specifies what\ninformation is relevant. The goal of the captioner is to produce a caption that\nresolves this issue. To model this task, we use an extension of the Rational\nSpeech Acts model of pragmatic language use. Our extension is built on top of\nstate-of-the-art pretrained neural image captioners and explicitly reasons\nabout issues in our sense. We establish experimentally that these models\ngenerate captions that are both highly descriptive and issue-sensitive, and we\nshow how ISIC can complement and enrich the related task of Visual Question\nAnswering.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 20:00:53 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 23:24:41 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Nie", "Allen", ""], ["Cohn-Gordon", "Reuben", ""], ["Potts", "Christopher", ""]]}, {"id": "2004.14454", "submitter": "Marcos Zampieri", "authors": "Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Marcos Zampieri,\n  Preslav Nakov", "title": "A Large-Scale Semi-Supervised Dataset for Offensive Language\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of offensive language is a major problem in social media which has\nled to an abundance of research in detecting content such as hate speech,\ncyberbulling, and cyber-aggression. There have been several attempts to\nconsolidate and categorize these efforts. Recently, the OLID dataset used at\nSemEval-2019 proposed a hierarchical three-level annotation taxonomy which\naddresses different types of offensive language as well as important\ninformation such as the target of such content. The categorization provides\nmeaningful and important information for understanding offensive language.\nHowever, the OLID dataset is limited in size, especially for some of the\nlow-level categories, which included only a few hundred instances, thus making\nit challenging to train robust deep learning models. Here, we address this\nlimitation by creating the largest available dataset for this task, SOLID.\nSOLID contains over nine million English tweets labeled in a semi-supervised\nmanner. We further demonstrate experimentally that using SOLID along with OLID\nyields improved performance on the OLID test set for two different models,\nespecially for the lower levels of the taxonomy. Finally, we perform analysis\nof the models' performance on easy and hard examples of offensive language\nusing data annotated in a semi-supervised way.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 20:02:58 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Rosenthal", "Sara", ""], ["Atanasova", "Pepa", ""], ["Karadzhov", "Georgi", ""], ["Zampieri", "Marcos", ""], ["Nakov", "Preslav", ""]]}, {"id": "2004.14457", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Jyun-Yu Jiang, Jieyu Zhao, Kai-Wei Chang and Wei Wang", "title": "\"The Boating Store Had Its Best Sail Ever\": Pronunciation-attentive\n  Contextualized Pun Recognition", "comments": "10 pages, 4 figures, 7 tables, accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humor plays an important role in human languages and it is essential to model\nhumor when building intelligence systems. Among different forms of humor, puns\nperform wordplay for humorous effects by employing words with double entendre\nand high phonetic similarity. However, identifying and modeling puns are\nchallenging as puns usually involved implicit semantic or phonological tricks.\nIn this paper, we propose Pronunciation-attentive Contextualized Pun\nRecognition (PCPR) to perceive human humor, detect if a sentence contains puns\nand locate them in the sentence. PCPR derives contextualized representation for\neach word in a sentence by capturing the association between the surrounding\ncontext and its corresponding phonetic symbols. Extensive experiments are\nconducted on two benchmark datasets. Results demonstrate that the proposed\napproach significantly outperforms the state-of-the-art methods in pun\ndetection and location tasks. In-depth analyses verify the effectiveness and\nrobustness of PCPR.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 20:12:20 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Zhou", "Yichao", ""], ["Jiang", "Jyun-Yu", ""], ["Zhao", "Jieyu", ""], ["Chang", "Kai-Wei", ""], ["Wang", "Wei", ""]]}, {"id": "2004.14500", "submitter": "Jung Taehee", "authors": "Taehee Jung, Dongyeop Kang, Hua Cheng, Lucas Mentch, Thomas Schaaf", "title": "Posterior Calibrated Training on Sentence Classification Tasks", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classification models work by first predicting a posterior probability\ndistribution over all classes and then selecting that class with the largest\nestimated probability. In many settings however, the quality of posterior\nprobability itself (e.g., 65% chance having diabetes), gives more reliable\ninformation than the final predicted class alone. When these methods are shown\nto be poorly calibrated, most fixes to date have relied on posterior\ncalibration, which rescales the predicted probabilities but often has little\nimpact on final classifications. Here we propose an end-to-end training\nprocedure called posterior calibrated (PosCal) training that directly optimizes\nthe objective while minimizing the difference between the predicted and\nempirical posterior probabilities.We show that PosCal not only helps reduce the\ncalibration error but also improve task performance by penalizing drops in\nperformance of both objectives. Our PosCal achieves about 2.5% of task\nperformance gain and 16.1% of calibration error reduction on GLUE (Wang et al.,\n2018) compared to the baseline. We achieved the comparable task performance\nwith 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not\noutperforming the two-stage calibration baseline. PosCal training can be easily\nextendable to any types of classification tasks as a form of regularization\nterm. Also, PosCal has the advantage that it incrementally tracks needed\nstatistics for the calibration objective during the training process, making\nefficient use of large training sets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 22:13:15 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 16:26:16 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Jung", "Taehee", ""], ["Kang", "Dongyeop", ""], ["Cheng", "Hua", ""], ["Mentch", "Lucas", ""], ["Schaaf", "Thomas", ""]]}, {"id": "2004.14503", "submitter": "Ji Ma", "authors": "Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall and Ryan McDonald", "title": "Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic\n  Question Generation", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major obstacle to the wide-spread adoption of neural retrieval models is\nthat they require large supervised training sets to surpass traditional\nterm-based techniques, which are constructed from raw corpora. In this paper,\nwe propose an approach to zero-shot learning for passage retrieval that uses\nsynthetic question generation to close this gap. The question generation system\nis trained on general domain data, but is applied to documents in the targeted\ndomain. This allows us to create arbitrarily large, yet noisy, question-passage\nrelevance pairs that are domain specific. Furthermore, when this is coupled\nwith a simple hybrid term-neural model, first-stage retrieval performance can\nbe improved further. Empirically, we show that this is an effective strategy\nfor building neural passage retrieval models in the absence of large training\ncorpora. Depending on the domain, this technique can even approach the accuracy\nof supervised models.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 22:21:31 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 13:29:55 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 16:04:12 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ma", "Ji", ""], ["Korotkov", "Ivan", ""], ["Yang", "Yinfei", ""], ["Hall", "Keith", ""], ["McDonald", "Ryan", ""]]}, {"id": "2004.14507", "submitter": "Qingfu Zhu", "authors": "Qingfu Zhu, Weinan Zhang, Ting Liu, William Yang Wang", "title": "Counterfactual Off-Policy Training for Neural Response Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain dialogue generation suffers from the data insufficiency problem\ndue to the vast size of potential responses. In this paper, we propose to\nexplore potential responses by counterfactual reasoning. Given an observed\nresponse, the counterfactual reasoning model automatically infers the outcome\nof an alternative policy that could have been taken. The resulting\ncounterfactual response synthesized in hindsight is of higher quality than the\nresponse synthesized from scratch. Training on the counterfactual responses\nunder the adversarial learning framework helps to explore the high-reward area\nof the potential response space. An empirical study on the DailyDialog dataset\nshows that our approach significantly outperforms the HRED model as well as the\nconventional adversarial learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 22:46:28 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 07:47:45 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Zhu", "Qingfu", ""], ["Zhang", "Weinan", ""], ["Liu", "Ting", ""], ["Wang", "William Yang", ""]]}, {"id": "2004.14513", "submitter": "Julian Michael", "authors": "Julian Michael, Jan A. Botha, Ian Tenney", "title": "Asking without Telling: Exploring Latent Ontologies in Contextual\n  Representations", "comments": "21 pages, 8 figures, 11 tables. Published in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of pretrained contextual encoders, such as ELMo and BERT, has\nbrought a great deal of interest in what these models learn: do they, without\nexplicit supervision, learn to encode meaningful notions of linguistic\nstructure? If so, how is this structure encoded? To investigate this, we\nintroduce latent subclass learning (LSL): a modification to existing\nclassifier-based probing methods that induces a latent categorization (or\nontology) of the probe's inputs. Without access to fine-grained gold labels,\nLSL extracts emergent structure from input representations in an interpretable\nand quantifiable form. In experiments, we find strong evidence of familiar\ncategories, such as a notion of personhood in ELMo, as well as novel\nontological distinctions, such as a preference for fine-grained semantic roles\non core arguments. Our results provide unique new evidence of emergent\nstructure in pretrained encoders, including departures from existing\nannotations which are inaccessible to earlier methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 23:20:40 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 00:28:21 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Michael", "Julian", ""], ["Botha", "Jan A.", ""], ["Tenney", "Ian", ""]]}, {"id": "2004.14514", "submitter": "Hiroki Ouchi", "authors": "Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, Tatsuki\n  Kuribayashi, Ryuto Konno, Kentaro Inui", "title": "Instance-Based Learning of Span Representations: A Case Study through\n  Named Entity Recognition", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretable rationales for model predictions play a critical role in\npractical applications. In this study, we develop models possessing\ninterpretable inference process for structured prediction. Specifically, we\npresent a method of instance-based learning that learns similarities between\nspans. At inference time, each span is assigned a class label based on its\nsimilar spans in the training set, where it is easy to understand how much each\ntraining instance contributes to the predictions. Through empirical analysis on\nnamed entity recognition, we demonstrate that our method enables to build\nmodels that have high interpretability without sacrificing performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 23:32:42 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Ouchi", "Hiroki", ""], ["Suzuki", "Jun", ""], ["Kobayashi", "Sosuke", ""], ["Yokoi", "Sho", ""], ["Kuribayashi", "Tatsuki", ""], ["Konno", "Ryuto", ""], ["Inui", "Kentaro", ""]]}, {"id": "2004.14516", "submitter": "Masaaki Nagata Dr.", "authors": "Masaaki Nagata, Chousa Katsuki, Masaaki Nishino", "title": "A Supervised Word Alignment Method based on Cross-Language Span\n  Prediction using Multilingual BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel supervised word alignment method based on cross-language\nspan prediction. We first formalize a word alignment problem as a collection of\nindependent predictions from a token in the source sentence to a span in the\ntarget sentence. As this is equivalent to a SQuAD v2.0 style question answering\ntask, we then solve this problem by using multilingual BERT, which is\nfine-tuned on a manually created gold word alignment data. We greatly improved\nthe word alignment accuracy by adding the context of the token to the question.\nIn the experiments using five word alignment datasets among Chinese, Japanese,\nGerman, Romanian, French, and English, we show that the proposed method\nsignificantly outperformed previous supervised and unsupervised word alignment\nmethods without using any bitexts for pretraining. For example, we achieved an\nF1 score of 86.7 for the Chinese-English data, which is 13.3 points higher than\nthe previous state-of-the-art supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 23:40:08 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Nagata", "Masaaki", ""], ["Katsuki", "Chousa", ""], ["Nishino", "Masaaki", ""]]}, {"id": "2004.14517", "submitter": "Masaaki Nagata Dr.", "authors": "Katsuki Chousa, Masaaki Nagata, Masaaki Nishino", "title": "Bilingual Text Extraction as Reading Comprehension", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to extract bilingual texts automatically\nfrom noisy parallel corpora by framing the problem as a token-level span\nprediction, such as SQuAD-style Reading Comprehension. To extract a span of the\ntarget document that is a translation of a given source sentence (span), we use\neither QANet or multilingual BERT. QANet can be trained for a specific parallel\ncorpus from scratch, while multilingual BERT can utilize pre-trained\nmultilingual representations. For the span prediction method using QANet, we\nintroduce a total optimization method using integer linear programming to\nachieve consistency in the predicted parallel spans. We conduct a parallel\nsentence extraction experiment using simulated noisy parallel corpora with two\nlanguage pairs (En-Fr and En-Ja) and find that the proposed method using QANet\nachieves significantly better accuracy than a baseline method using two\nbi-directional RNN encoders, particularly for distant language pairs (En-Ja).\nWe also conduct a sentence alignment experiment using En-Ja newspaper articles\nand find that the proposed method using multilingual BERT achieves\nsignificantly better accuracy than a baseline method using a bilingual\ndictionary and dynamic programming.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 23:41:32 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Chousa", "Katsuki", ""], ["Nagata", "Masaaki", ""], ["Nishino", "Masaaki", ""]]}, {"id": "2004.14519", "submitter": "Wuwei Lan", "authors": "Wuwei Lan, Yang Chen, Wei Xu and Alan Ritter", "title": "An Empirical Study of Pre-trained Transformers for Arabic Information\n  Extraction", "comments": "8 pages, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019)\nand XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable the\neffective cross-lingual zero-shot transfer. However, their performance on\nArabic information extraction (IE) tasks is not very well studied. In this\npaper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is\ndesigned specifically for Arabic NLP and English-to-Arabic zero-shot transfer\nlearning. We study GigaBERT's effectiveness on zero-short transfer across four\nIE tasks: named entity recognition, part-of-speech tagging, argument role\nlabeling, and relation extraction. Our best model significantly outperforms\nmBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised\nand zero-shot transfer settings. We have made our pre-trained models publicly\navailable at https://github.com/lanwuwei/GigaBERT.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:01:08 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 21:52:04 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 21:00:21 GMT"}, {"version": "v4", "created": "Fri, 9 Oct 2020 00:24:43 GMT"}, {"version": "v5", "created": "Sat, 7 Nov 2020 14:40:25 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lan", "Wuwei", ""], ["Chen", "Yang", ""], ["Xu", "Wei", ""], ["Ritter", "Alan", ""]]}, {"id": "2004.14523", "submitter": "Brian Thompson", "authors": "Brian Thompson and Philipp Koehn", "title": "Exploiting Sentence Order in Document Alignment", "comments": "EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple document alignment method that incorporates sentence\norder information in both candidate generation and candidate re-scoring. Our\nmethod results in 61% relative reduction in error compared to the best\npreviously published result on the WMT16 document alignment shared task. Our\nmethod improves downstream MT performance on web-scraped Sinhala--English\ndocuments from ParaCrawl, outperforming the document alignment method used in\nthe most recent ParaCrawl release. It also outperforms a comparable corpora\nmethod which uses the same multilingual embeddings, demonstrating that\nexploiting sentence order is beneficial even if the end goal is sentence-level\nbitext.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:11:34 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 01:23:22 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Thompson", "Brian", ""], ["Koehn", "Philipp", ""]]}, {"id": "2004.14524", "submitter": "Huda Khayrallah", "authors": "Huda Khayrallah, Brian Thompson, Matt Post, Philipp Koehn", "title": "Simulated Multiple Reference Training Improves Low-Resource Machine\n  Translation", "comments": "EMNLP 2020 camera ready", "journal-ref": null, "doi": "10.18653/v1/2020.emnlp-main.7", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many valid translations exist for a given sentence, yet machine translation\n(MT) is trained with a single reference translation, exacerbating data sparsity\nin low-resource settings. We introduce Simulated Multiple Reference Training\n(SMRT), a novel MT training method that approximates the full space of possible\ntranslations by sampling a paraphrase of the reference sentence from a\nparaphraser and training the MT model to predict the paraphraser's distribution\nover possible tokens. We demonstrate the effectiveness of SMRT in low-resource\nsettings when translating to English, with improvements of 1.2 to 7.0 BLEU. We\nalso find SMRT is complementary to back-translation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:11:53 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 15:43:57 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Khayrallah", "Huda", ""], ["Thompson", "Brian", ""], ["Post", "Matt", ""], ["Koehn", "Philipp", ""]]}, {"id": "2004.14530", "submitter": "Peng Qi", "authors": "Peng Qi, Yuhao Zhang, Christopher D. Manning", "title": "Stay Hungry, Stay Focused: Generating Informative and Specific Questions\n  in Information-Seeking Conversations", "comments": "Findings of ACL: EMNLP 2020. Code available at:\n  https://github.com/qipeng/stay-hungry-stay-focused", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of generating informative questions in\ninformation-asymmetric conversations. Unlike previous work on question\ngeneration which largely assumes knowledge of what the answer might be, we are\ninterested in the scenario where the questioner is not given the context from\nwhich answers are drawn, but must reason pragmatically about how to acquire new\ninformation, given the shared conversation history. We identify two core\nchallenges: (1) formally defining the informativeness of potential questions,\nand (2) exploring the prohibitively large space of potential questions to find\nthe good candidates. To generate pragmatic questions, we use reinforcement\nlearning to optimize an informativeness metric we propose, combined with a\nreward function designed to promote more specific questions. We demonstrate\nthat the resulting pragmatic questioner substantially improves the\ninformativeness and specificity of questions generated over a baseline model,\nas evaluated by our metrics as well as humans.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:49:14 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 16:53:32 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Qi", "Peng", ""], ["Zhang", "Yuhao", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2004.14532", "submitter": "Avneesh Saluja", "authors": "Gayatri Bhat, Avneesh Saluja, Melody Dye, and Jan Florjanczyk", "title": "Hierarchical Encoders for Modeling and Interpreting Screenplays", "comments": "12 pages, including references and appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While natural language understanding of long-form documents is still an open\nchallenge, such documents often contain structural information that can inform\nthe design of models for encoding them. Movie scripts are an example of such\nrichly structured text - scripts are segmented into scenes, which are further\ndecomposed into dialogue and descriptive components. In this work, we propose a\nneural architecture for encoding this structure, which performs robustly on a\npair of multi-label tag classification datasets, without the need for\nhandcrafted features. We add a layer of insight by augmenting an unsupervised\n\"interpretability\" module to the encoder, allowing for the extraction and\nvisualization of narrative trajectories. Though this work specifically tackles\nscreenplays, we discuss how the underlying approach can be generalized to a\nrange of structured documents.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 01:15:40 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Bhat", "Gayatri", ""], ["Saluja", "Avneesh", ""], ["Dye", "Melody", ""], ["Florjanczyk", "Jan", ""]]}, {"id": "2004.14535", "submitter": "Boris Dadachev", "authors": "Michal Lukasik, Boris Dadachev, Gon\\c{c}alo Sim\\~oes, Kishore Papineni", "title": "Text Segmentation by Cross Segment Attention", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document and discourse segmentation are two fundamental NLP tasks pertaining\nto breaking up text into constituents, which are commonly used to help\ndownstream tasks such as information retrieval or text summarization. In this\nwork, we propose three transformer-based architectures and provide\ncomprehensive comparisons with previously proposed approaches on three standard\ndatasets. We establish a new state-of-the-art, reducing in particular the error\nrates by a large margin in all cases. We further analyze model sizes and find\nthat we can build models with many fewer parameters while keeping good\nperformance, thus facilitating real-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 01:36:52 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 16:00:42 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lukasik", "Michal", ""], ["Dadachev", "Boris", ""], ["Sim\u00f5es", "Gon\u00e7alo", ""], ["Papineni", "Kishore", ""]]}, {"id": "2004.14543", "submitter": "Linyang Li", "authors": "Linyang Li, Xipeng Qiu", "title": "TAVAT: Token-Aware Virtual Adversarial Training for Language\n  Understanding", "comments": "AAAI2020-preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based adversarial training is widely used in improving the\nrobustness of neural networks, while it cannot be easily adapted to natural\nlanguage processing tasks since the embedding space is discrete. In natural\nlanguage processing fields, virtual adversarial training is introduced since\ntexts are discrete and cannot be perturbed by gradients directly.\nAlternatively, virtual adversarial training, which generates perturbations on\nthe embedding space, is introduced in NLP tasks. Despite its success, existing\nvirtual adversarial training methods generate perturbations roughly constrained\nby Frobenius normalization balls. To craft fine-grained perturbations, we\npropose a Token-Aware Virtual Adversarial Training method. We introduce a\ntoken-level accumulated perturbation vocabulary to initialize the perturbations\nbetter and use a token-level normalization ball to constrain these\nperturbations pertinently. Experiments show that our method improves the\nperformance of pre-trained models such as BERT and ALBERT in various tasks by a\nconsiderable margin. The proposed method improves the score of the GLUE\nbenchmark from 78.3 to 80.9 using BERT model and it also enhances the\nperformance of sequence labeling and text classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 02:03:24 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 05:28:10 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 13:08:56 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Li", "Linyang", ""], ["Qiu", "Xipeng", ""]]}, {"id": "2004.14546", "submitter": "Sharan Narang", "authors": "Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel,\n  Karishma Malkan", "title": "WT5?! Training Text-to-Text Models to Explain their Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have recently achieved human-level performance on various\nchallenging natural language processing (NLP) tasks, but it is notoriously\ndifficult to understand why a neural network produced a particular prediction.\nIn this paper, we leverage the text-to-text framework proposed by Raffel et\nal.(2019) to train language models to output a natural text explanation\nalongside their prediction. Crucially, this requires no modifications to the\nloss function or training and decoding procedures -- we simply train the model\nto output the explanation after generating the (natural text) prediction. We\nshow that this approach not only obtains state-of-the-art results on\nexplainability benchmarks, but also permits learning from a limited set of\nlabeled explanations and transferring rationalization abilities across\ndatasets. To facilitate reproducibility and future work, we release our code\nuse to train the models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 02:20:14 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Narang", "Sharan", ""], ["Raffel", "Colin", ""], ["Lee", "Katherine", ""], ["Roberts", "Adam", ""], ["Fiedel", "Noah", ""], ["Malkan", "Karishma", ""]]}, {"id": "2004.14550", "submitter": "Jia-Chen Gu", "authors": "Jia-Chen Gu, Zhen-Hua Ling, Quan Liu, Zhigang Chen, Xiaodan Zhu", "title": "Filtering before Iteratively Referring for Knowledge-Grounded Response\n  Selection in Retrieval-Based Chatbots", "comments": "Accepted by EMNLP 2020 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenges of building knowledge-grounded retrieval-based chatbots lie in\nhow to ground a conversation on its background knowledge and how to match\nresponse candidates with both context and knowledge simultaneously. This paper\nproposes a method named Filtering before Iteratively REferring (FIRE) for this\ntask. In this method, a context filter and a knowledge filter are first built,\nwhich derive knowledge-aware context representations and context-aware\nknowledge representations respectively by global and bidirectional attention.\nBesides, the entries irrelevant to the conversation are discarded by the\nknowledge filter. After that, iteratively referring is performed between\ncontext and response representations as well as between knowledge and response\nrepresentations, in order to collect deep matching features for scoring\nresponse candidates. Experimental results show that FIRE outperforms previous\nmethods by margins larger than 2.8% and 4.1% on the PERSONA-CHAT dataset with\noriginal and revised personas respectively, and margins larger than 3.1% on the\nCMU_DoG dataset in terms of top-1 accuracy. We also show that FIRE is more\ninterpretable by visualizing the knowledge grounding process.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 02:27:12 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 06:50:18 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Gu", "Jia-Chen", ""], ["Ling", "Zhen-Hua", ""], ["Liu", "Quan", ""], ["Chen", "Zhigang", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2004.14554", "submitter": "Kristen Allen", "authors": "Kristen C. Allen, Alex Davis, and Tamar Krishnamurti", "title": "Indirect Identification of Psychosocial Risks from Natural Language", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the perinatal period, psychosocial health risks, including depression\nand intimate partner violence, are associated with serious adverse health\noutcomes for parents and children. To appropriately intervene, healthcare\nprofessionals must first identify those at risk, yet stigma often prevents\npeople from directly disclosing the information needed to prompt an assessment.\nWe examine indirect methods of eliciting and analyzing information that could\nindicate psychosocial risks. Short diary entries by peripartum women exhibit\nthematic patterns, extracted by topic modeling, and emotional perspective,\ndrawn from dictionary-informed sentiment features. Using these features, we use\nregularized regression to predict screening measures of depression and\npsychological aggression by an intimate partner. Journal text entries\nquantified through topic models and sentiment features show promise for\ndepression prediction, with performance almost as good as closed-form\nquestions. Text-based features were less useful for prediction of intimate\npartner violence, but moderately indirect multiple-choice questioning allowed\nfor detection without explicit disclosure. Both methods may serve as an initial\nor complementary screening approach to detecting stigmatized risks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:13:28 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Allen", "Kristen C.", ""], ["Davis", "Alex", ""], ["Krishnamurti", "Tamar", ""]]}, {"id": "2004.14555", "submitter": "Jingbo Shang", "authors": "Peiran Li, Fang Guo, Jingbo Shang", "title": "User-Guided Aspect Classification for Domain-Specific Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect classification, identifying aspects of text segments, facilitates\nnumerous applications, such as sentiment analysis and review summarization. To\nalleviate the human effort on annotating massive texts, in this paper, we study\nthe problem of classifying aspects based on only a few user-provided seed words\nfor pre-defined aspects. The major challenge lies in how to handle the noisy\nmisc aspect, which is designed for texts without any pre-defined aspects. Even\ndomain experts have difficulties to nominate seed words for the misc aspect,\nmaking existing seed-driven text classification methods not applicable. We\npropose a novel framework, ARYA, which enables mutual enhancements between\npre-defined aspects and the misc aspect via iterative classifier training and\nseed updating. Specifically, it trains a classifier for pre-defined aspects and\nthen leverages it to induce the supervision for the misc aspect. The prediction\nresults of the misc aspect are later utilized to filter out noisy seed words\nfor pre-defined aspects. Experiments in two domains demonstrate the superior\nperformance of our proposed framework, as well as the necessity and importance\nof properly modeling the misc aspect.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:14:16 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Li", "Peiran", ""], ["Guo", "Fang", ""], ["Shang", "Jingbo", ""]]}, {"id": "2004.14560", "submitter": "Dayiheng Liu", "authors": "Dayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng Chen, Daxin Jiang,\n  Jiancheng Lv and Nan Duan", "title": "RikiNet: Reading Wikipedia Pages for Natural Question Answering", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading long documents to answer open-domain questions remains challenging in\nnatural language understanding. In this paper, we introduce a new model, called\nRikiNet, which reads Wikipedia pages for natural question answering. RikiNet\ncontains a dynamic paragraph dual-attention reader and a multi-level cascaded\nanswer predictor. The reader dynamically represents the document and question\nby utilizing a set of complementary attention mechanisms. The representations\nare then fed into the predictor to obtain the span of the short answer, the\nparagraph of the long answer, and the answer type in a cascaded manner. On the\nNatural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1\non long-answer and short-answer tasks. To our best knowledge, it is the first\nsingle model that outperforms the single human performance. Furthermore, an\nensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer\ntasks, achieving the best performance on the official NQ leaderboard\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:29:21 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Liu", "Dayiheng", ""], ["Gong", "Yeyun", ""], ["Fu", "Jie", ""], ["Yan", "Yu", ""], ["Chen", "Jiusheng", ""], ["Jiang", "Daxin", ""], ["Lv", "Jiancheng", ""], ["Duan", "Nan", ""]]}, {"id": "2004.14564", "submitter": "Brian Thompson", "authors": "Brian Thompson and Matt Post", "title": "Automatic Machine Translation Evaluation in Many Languages via Zero-Shot\n  Paraphrasing", "comments": "EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We frame the task of machine translation evaluation as one of scoring machine\ntranslation output with a sequence-to-sequence paraphraser, conditioned on a\nhuman reference. We propose training the paraphraser as a multilingual NMT\nsystem, treating paraphrasing as a zero-shot translation task (e.g., Czech to\nCzech). This results in the paraphraser's output mode being centered around a\ncopy of the input sequence, which represents the best case scenario where the\nMT system output matches a human reference. Our method is simple and intuitive,\nand does not require human judgements for training. Our single model (trained\nin 39 languages) outperforms or statistically ties with all prior metrics on\nthe WMT 2019 segment-level shared metrics task in all languages (excluding\nGujarati where the model had no training data). We also explore using our model\nfor the task of quality estimation as a metric--conditioning on the source\ninstead of the reference--and find that it significantly outperforms every\nsubmission to the WMT 2019 shared task on quality estimation in every language\npair.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:32:34 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 23:54:02 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Thompson", "Brian", ""], ["Post", "Matt", ""]]}, {"id": "2004.14565", "submitter": "Chenguang Zhu", "authors": "Chenguang Zhu", "title": "Boosting Naturalness of Language in Task-oriented Dialogues via\n  Adversarial Training", "comments": "SIGDial 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural language generation (NLG) module in a task-oriented dialogue\nsystem produces user-facing utterances conveying required information. Thus, it\nis critical for the generated response to be natural and fluent. We propose to\nintegrate adversarial training to produce more human-like responses. The model\nuses Straight-Through Gumbel-Softmax estimator for gradient computation. We\nalso propose a two-stage training scheme to boost performance. Empirical\nresults show that the adversarial training can effectively improve the quality\nof language generation in both automatic and human evaluations. For example, in\nthe RNN-LG Restaurant dataset, our model AdvNLG outperforms the previous\nstate-of-the-art result by 3.6% in BLEU.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:35:20 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 04:44:38 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Zhu", "Chenguang", ""]]}, {"id": "2004.14571", "submitter": "Sadasivam Aadhavan", "authors": "Aadhavan Sadasivam, Kausic Gunasekar, Hasan Davulcu, Yezhou Yang", "title": "memeBot: Towards Automatic Image Meme Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image memes have become a widespread tool used by people for interacting and\nexchanging ideas over social media, blogs, and open messengers. This work\nproposes to treat automatic image meme generation as a translation process, and\nfurther present an end to end neural and probabilistic approach to generate an\nimage-based meme for any given sentence using an encoder-decoder architecture.\nFor a given input sentence, an image meme is generated by combining a meme\ntemplate image and a text caption where the meme template image is selected\nfrom a set of popular candidates using a selection module, and the meme caption\nis generated by an encoder-decoder model. An encoder is used to map the\nselected meme template and the input sentence into a meme embedding and a\ndecoder is used to decode the meme caption from the meme embedding. The\ngenerated natural language meme caption is conditioned on the input sentence\nand the selected meme template. The model learns the dependencies between the\nmeme captions and the meme template images and generates new memes using the\nlearned dependencies. The quality of the generated captions and the generated\nmemes is evaluated through both automated and human evaluation. An experiment\nis designed to score how well the generated memes can represent the tweets from\nTwitter conversations. Experiments on Twitter data show the efficacy of the\nmodel in generating memes for sentences in online social interaction.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:48:14 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Sadasivam", "Aadhavan", ""], ["Gunasekar", "Kausic", ""], ["Davulcu", "Hasan", ""], ["Yang", "Yezhou", ""]]}, {"id": "2004.14577", "submitter": "Bonan Min", "authors": "Hayley Ross, Jonathon Cai, Bonan Min", "title": "Exploring Contextualized Neural Language Models for Temporal Dependency\n  Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting temporal relations between events and time expressions has many\napplications such as constructing event timelines and time-related question\nanswering. It is a challenging problem which requires syntactic and semantic\ninformation at sentence or discourse levels, which may be captured by deep\ncontextualized language models (LMs) such as BERT (Devlin et al., 2019). In\nthis paper, we develop several variants of BERT-based temporal dependency\nparser, and show that BERT significantly improves temporal dependency parsing\n(Zhang and Xue, 2018a). We also present a detailed analysis on why deep\ncontextualized neural LMs help and where they may fall short. Source code and\nresources are made available at https://github.com/bnmin/tdp_ranking.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:59:13 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 00:25:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ross", "Hayley", ""], ["Cai", "Jonathon", ""], ["Min", "Bonan", ""]]}, {"id": "2004.14579", "submitter": "Zhiyu Chen", "authors": "Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam\n  Sundaresan, William Yang Wang", "title": "Logic2Text: High-Fidelity Natural Language Generation from Logical Forms", "comments": "Findings of EMNLP 2020, 9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works on Natural Language Generation (NLG) from structured data have\nprimarily focused on surface-level descriptions of record sequences. However,\nfor complex structured data, e.g., multi-row tables, it is often desirable for\nan NLG system to describe interesting facts from logical inferences across\nrecords. If only provided with the table, it is hard for existing models to\nproduce controllable and high-fidelity logical generations. In this work, we\nformulate logical level NLG as generation from logical forms in order to obtain\ncontrollable, high-fidelity, and faithful generations. We present a new\nlarge-scale dataset, \\textsc{Logic2Text}, with 10,753 descriptions involving\ncommon logic types paired with the underlying logical forms. The logical forms\nshow diversified graph structure of free schema, which poses great challenges\non the model's ability to understand the semantics. We experiment on (1)\nFully-supervised training with the full datasets, and (2) Few-shot setting,\nprovided with hundreds of paired examples; We compare several popular\ngeneration models and analyze their performances. We hope our dataset can\nencourage research towards building an advanced NLG system capable of natural,\nfaithful, and human-like generation. The dataset and code are available at\nhttps://github.com/czyssrs/Logic2Text.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 04:06:06 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 01:29:08 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Chen", "Zhiyu", ""], ["Chen", "Wenhu", ""], ["Zha", "Hanwen", ""], ["Zhou", "Xiyou", ""], ["Zhang", "Yunkai", ""], ["Sundaresan", "Sairam", ""], ["Wang", "William Yang", ""]]}, {"id": "2004.14589", "submitter": "Daniel Kang", "authors": "Daniel Kang and Tatsunori Hashimoto", "title": "Improved Natural Language Generation via Loss Truncation", "comments": "ACL 2020 Camera Ready Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models are usually trained to match the distributional\nproperties of a large-scale corpus by minimizing the log loss. While\nstraightforward to optimize, this approach forces the model to reproduce all\nvariations in the dataset, including noisy and invalid references (e.g.,\nmisannotation and hallucinated facts). Worse, the commonly used log loss is\noverly sensitive to such phenomena and even a small fraction of noisy data can\ndegrade performance. In this work, we show that the distinguishability of the\nmodels and reference serves as a principled and robust alternative for handling\ninvalid references. To optimize distinguishability, we propose loss truncation,\nwhich adaptively removes high loss examples during training. We show this is as\neasy to optimize as log loss and tightly bounds distinguishability under noise.\nEmpirically, we demonstrate that loss truncation outperforms existing baselines\non distinguishability on a summarization task, and show that samples generated\nby the loss truncation model have factual accuracy ratings that exceed those of\nbaselines and match human references.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 05:31:31 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 02:22:04 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Kang", "Daniel", ""], ["Hashimoto", "Tatsunori", ""]]}, {"id": "2004.14592", "submitter": "Chongyang Tao", "authors": "Jiayi Zhang, Chongyang Tao, Zhenjing Xu, Qiaojing Xie, Wei Chen, Rui\n  Yan", "title": "EnsembleGAN: Adversarial Learning for Retrieval-Generation Ensemble\n  Model on Short-Text Conversation", "comments": "10 pages, SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating qualitative responses has always been a challenge for\nhuman-computer dialogue systems. Existing dialogue systems generally derive\nfrom either retrieval-based or generative-based approaches, both of which have\ntheir own pros and cons. Despite the natural idea of an ensemble model of the\ntwo, existing ensemble methods only focused on leveraging one approach to\nenhance another, we argue however that they can be further mutually enhanced\nwith a proper training strategy. In this paper, we propose ensembleGAN, an\nadversarial learning framework for enhancing a retrieval-generation ensemble\nmodel in open-domain conversation scenario. It consists of a\nlanguage-model-like generator, a ranker generator, and one ranker\ndiscriminator. Aiming at generating responses that approximate the ground-truth\nand receive high ranking scores from the discriminator, the two generators\nlearn to generate improved highly relevant responses and competitive unobserved\ncandidates respectively, while the discriminative ranker is trained to identify\ntrue responses from adversarial ones, thus featuring the merits of both\ngenerator counterparts. The experimental results on a large short-text\nconversation data demonstrate the effectiveness of the ensembleGAN by the\namelioration on both human and automatic evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 05:59:12 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Zhang", "Jiayi", ""], ["Tao", "Chongyang", ""], ["Xu", "Zhenjing", ""], ["Xie", "Qiaojing", ""], ["Chen", "Wei", ""], ["Yan", "Rui", ""]]}, {"id": "2004.14601", "submitter": "Isabel Papadimitriou", "authors": "Isabel Papadimitriou and Dan Jurafsky", "title": "Learning Music Helps You Read: Using Transfer to Study Linguistic\n  Structure in Language Models", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose transfer learning as a method for analyzing the encoding of\ngrammatical structure in neural language models. We train LSTMs on\nnon-linguistic data and evaluate their performance on natural language to\nassess which kinds of data induce generalizable structural features that LSTMs\ncan use for natural language. We find that training on non-linguistic data with\nlatent structure (MIDI music or Java code) improves test performance on natural\nlanguage, despite no overlap in surface form or vocabulary. To pinpoint the\nkinds of abstract structure that models may be encoding to lead to this\nimprovement, we run similar experiments with two artificial parentheses\nlanguages: one which has a hierarchical recursive structure, and a control\nwhich has paired tokens but no recursion. Surprisingly, training a model on\neither of these artificial languages leads to the same substantial gains when\ntesting on natural language. Further experiments on transfer between natural\nlanguages controlling for vocabulary overlap show that zero-shot performance on\na test language is highly correlated with typological syntactic similarity to\nthe training language, suggesting that representations induced by pre-training\ncorrespond to the cross-linguistic syntactic properties. Our results provide\ninsights into the ways that neural models represent abstract syntactic\nstructure, and also about the kind of structural inductive biases which allow\nfor natural language acquisition.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 06:24:03 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 21:45:59 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 17:41:21 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Papadimitriou", "Isabel", ""], ["Jurafsky", "Dan", ""]]}, {"id": "2004.14602", "submitter": "Miyoung Ko", "authors": "Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, Jaewoo Kang", "title": "Look at the First Sentence: Position Bias in Question Answering", "comments": "13 pages, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many extractive question answering models are trained to predict start and\nend positions of answers. The choice of predicting answers as positions is\nmainly due to its simplicity and effectiveness. In this study, we hypothesize\nthat when the distribution of the answer positions is highly skewed in the\ntraining set (e.g., answers lie only in the k-th sentence of each passage), QA\nmodels predicting answers as positions can learn spurious positional cues and\nfail to give answers in different positions. We first illustrate this position\nbias in popular extractive QA models such as BiDAF and BERT and thoroughly\nexamine how position bias propagates through each layer of BERT. To safely\ndeliver position information without position bias, we train models with\nvarious de-biasing methods including entropy regularization and bias\nensembling. Among them, we found that using the prior distribution of answer\npositions as a bias model is very effective at reducing position bias,\nrecovering the performance of BERT from 37.48% to 81.64% when trained on a\nbiased SQuAD dataset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 06:25:16 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 09:59:16 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 02:48:40 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 15:09:45 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ko", "Miyoung", ""], ["Lee", "Jinhyuk", ""], ["Kim", "Hyunjae", ""], ["Kim", "Gangwoo", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2004.14607", "submitter": "Prathyusha Jwalapuram", "authors": "Prathyusha Jwalapuram, Barbara Rychalska, Shafiq Joty and Dominika\n  Basaj", "title": "Can Your Context-Aware MT System Pass the DiP Benchmark Tests? :\n  Evaluation Benchmarks for Discourse Phenomena in Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite increasing instances of machine translation (MT) systems including\ncontextual information, the evidence for translation quality improvement is\nsparse, especially for discourse phenomena. Popular metrics like BLEU are not\nexpressive or sensitive enough to capture quality improvements or drops that\nare minor in size but significant in perception. We introduce the first of\ntheir kind MT benchmark datasets that aim to track and hail improvements across\nfour main discourse phenomena: anaphora, lexical consistency, coherence and\nreadability, and discourse connective translation. We also introduce evaluation\nmethods for these tasks, and evaluate several baseline MT systems on the\ncurated datasets. Surprisingly, we find that existing context-aware models do\nnot improve discourse-related translations consistently across languages and\nphenomena.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 07:15:36 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Jwalapuram", "Prathyusha", ""], ["Rychalska", "Barbara", ""], ["Joty", "Shafiq", ""], ["Basaj", "Dominika", ""]]}, {"id": "2004.14614", "submitter": "Yi-Lin Tuan", "authors": "Yi-Lin Tuan, Wei Wei, William Yang Wang", "title": "Knowledge Injection into Dialogue Generation via Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue generation has been successfully learned from scratch by neural\nnetworks, but tends to produce the same general response, e.g., \"what are you\ntalking about?\", in many conversations. To reduce this homogeneity, external\nknowledge such as the speaker's profile and domain knowledge is applied as an\nadditional condition to diversify a model's output. The required knowledge to\ndevelop an effective conversation, however, is not always available, which is\ndifferent from prior work's assumption that a model always has acquired\nsufficient knowledge before chatting. This problem can be detrimental when\napplying a dialogue model like this chatting online with unconstrained people\nand topics, because the model does not have the needed knowledge. To address\nthis problem, we propose InjK, which is a two-stage approach to inject\nknowledge into a dialogue generation model. First, we train a large-scale\nlanguage model and query it as textual knowledge. Second, we frame a dialogue\ngeneration model to sequentially generate textual knowledge and a corresponding\nresponse. Empirically, when a dialogue generation model can only access limited\nknowledge, our method outperforms prior work by producing more coherent and\ninformative responses.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 07:31:24 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 22:50:48 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Tuan", "Yi-Lin", ""], ["Wei", "Wei", ""], ["Wang", "William Yang", ""]]}, {"id": "2004.14620", "submitter": "Tomasz Limisiewicz", "authors": "Tomasz Limisiewicz and Rudolf Rosa and David Mare\\v{c}ek", "title": "Universal Dependencies according to BERT: both more specific and more\n  general", "comments": null, "journal-ref": "Findings of the Association for Computational Linguistics: EMNLP\n  2020", "doi": "10.18653/v1/2020.findings-emnlp.245", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work focuses on analyzing the form and extent of syntactic abstraction\ncaptured by BERT by extracting labeled dependency trees from self-attentions.\n  Previous work showed that individual BERT heads tend to encode particular\ndependency relation types. We extend these findings by explicitly comparing\nBERT relations to Universal Dependencies (UD) annotations, showing that they\noften do not match one-to-one.\n  We suggest a method for relation identification and syntactic tree\nconstruction. Our approach produces significantly more consistent dependency\ntrees than previous work, showing that it better explains the syntactic\nabstractions in BERT. At the same time, it can be successfully applied with\nonly a minimal amount of supervision and generalizes well across languages.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 07:48:07 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 00:34:10 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 10:22:33 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Limisiewicz", "Tomasz", ""], ["Rosa", "Rudolf", ""], ["Mare\u010dek", "David", ""]]}, {"id": "2004.14623", "submitter": "Atticus Geiger", "authors": "Atticus Geiger, Kyle Richardson, and Christopher Potts", "title": "Neural Natural Language Inference Models Partially Embed Theories of\n  Lexical Entailment and Negation", "comments": "In Proceedings of BlackBoxNLP 2020 at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address whether neural models for Natural Language Inference (NLI) can\nlearn the compositional interactions between lexical entailment and negation,\nusing four methods: the behavioral evaluation methods of (1) challenge test\nsets and (2) systematic generalization tasks, and the structural evaluation\nmethods of (3) probes and (4) interventions. To facilitate this holistic\nevaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset\nfocused on lexical entailment and negation. In our behavioral evaluations, we\nfind that models trained on general-purpose NLI datasets fail systematically on\nMoNLI examples containing negation, but that MoNLI fine-tuning addresses this\nfailure. In our structural evaluations, we look for evidence that our\ntop-performing BERT-based model has learned to implement the monotonicity\nalgorithm behind MoNLI. Probes yield evidence consistent with this conclusion,\nand our intervention experiments bolster this, showing that the causal dynamics\nof the model mirror the causal dynamics of this algorithm on subsets of MoNLI.\nThis suggests that the BERT model at least partially embeds a theory of lexical\nentailment and negation at an algorithmic level.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 07:53:20 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 20:51:04 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 17:04:12 GMT"}, {"version": "v4", "created": "Sat, 21 Nov 2020 01:53:49 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Geiger", "Atticus", ""], ["Richardson", "Kyle", ""], ["Potts", "Christopher", ""]]}, {"id": "2004.14626", "submitter": "Tasnim Mohiuddin", "authors": "Tasnim Mohiuddin, Prathyusha Jwalapuram, Xiang Lin, and Shafiq Joty", "title": "Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks", "comments": "Accepted paper at EACL-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although coherence modeling has come a long way in developing novel models,\ntheir evaluation on downstream applications for which they are purportedly\ndeveloped has largely been neglected. With the advancements made by neural\napproaches in applications such as machine translation (MT), summarization and\ndialog systems, the need for coherence evaluation of these tasks is now more\ncrucial than ever. However, coherence models are typically evaluated only on\nsynthetic tasks, which may not be representative of their performance in\ndownstream applications. To investigate how representative the synthetic tasks\nare of downstream use cases, we conduct experiments on benchmarking well-known\ntraditional and neural coherence models on synthetic sentence ordering tasks,\nand contrast this with their performance on three downstream applications:\ncoherence evaluation for MT and summarization, and next utterance prediction in\nretrieval-based dialog. Our results demonstrate a weak correlation between the\nmodel performances in the synthetic tasks and the downstream applications,\n{motivating alternate training and evaluation methods for coherence models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 08:00:42 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 04:47:45 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Mohiuddin", "Tasnim", ""], ["Jwalapuram", "Prathyusha", ""], ["Lin", "Xiang", ""], ["Joty", "Shafiq", ""]]}, {"id": "2004.14648", "submitter": "Jifan Chen", "authors": "Jifan Chen and Greg Durrett", "title": "Robust Question Answering Through Sub-part Alignment", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current textual question answering models achieve strong performance on\nin-domain test sets, but often do so by fitting surface-level patterns in the\ndata, so they fail to generalize to out-of-distribution settings. To make a\nmore robust and understandable QA system, we model question answering as an\nalignment problem. We decompose both the question and context into smaller\nunits based on off-the-shelf semantic representations (here, semantic roles),\nand align the question to a subgraph of the context in order to find the\nanswer. We formulate our model as a structured SVM, with alignment scores\ncomputed via BERT, and we can train end-to-end despite using beam search for\napproximate inference. Our explicit use of alignments allows us to explore a\nset of constraints with which we can prohibit certain types of bad model\nbehavior arising in cross-domain settings. Furthermore, by investigating\ndifferences in scores across different potential answers, we can seek to\nunderstand what particular aspects of the input lead the model to choose the\nanswer without relying on post-hoc explanation techniques. We train our model\non SQuAD v1.1 and test it on several adversarial and out-of-domain datasets.\nThe results show that our model is more robust cross-domain than the standard\nBERT QA model, and constraints derived from alignment scores allow us to\neffectively trade off coverage and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 09:10:57 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 23:58:37 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 20:43:55 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Chen", "Jifan", ""], ["Durrett", "Greg", ""]]}, {"id": "2004.14649", "submitter": "Sufeng Duan", "authors": "Sufeng Duan, Juncheng Cao, Hai Zhao", "title": "Capsule-Transformer for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer hugely benefits from its key design of the multi-head\nself-attention network (SAN), which extracts information from various\nperspectives through transforming the given input into different subspaces.\nHowever, its simple linear transformation aggregation strategy may still\npotentially fail to fully capture deeper contextualized information. In this\npaper, we thus propose the capsule-Transformer, which extends the linear\ntransformation into a more general capsule routing algorithm by taking SAN as a\nspecial case of capsule network. So that the resulted capsule-Transformer is\ncapable of obtaining a better attention distribution representation of the\ninput sequence via information aggregation among different heads and words.\nSpecifically, we see groups of attention weights in SAN as low layer capsules.\nBy applying the iterative capsule routing algorithm they can be further\naggregated into high layer capsules which contain deeper contextualized\ninformation. Experimental results on the widely-used machine translation\ndatasets show our proposed capsule-Transformer outperforms strong Transformer\nbaseline significantly.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 09:11:38 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Duan", "Sufeng", ""], ["Cao", "Juncheng", ""], ["Zhao", "Hai", ""]]}, {"id": "2004.14667", "submitter": "Hassan Kane", "authors": "Hassan Kane, Muhammed Yusuf Kocyigit, Ali Abdalla, Pelkins Ajanoh,\n  Mohamed Coulibali", "title": "NUBIA: NeUral Based Interchangeability Assessor for Text Generation", "comments": "8 pages, 5 tables, and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NUBIA, a methodology to build automatic evaluation metrics for\ntext generation using only machine learning models as core components. A\ntypical NUBIA model is composed of three modules: a neural feature extractor,\nan aggregator and a calibrator. We demonstrate an implementation of NUBIA which\noutperforms metrics currently used to evaluate machine translation, summaries\nand slightly exceeds/matches state of the art metrics on correlation with human\njudgement on the WMT segment-level Direct Assessment task, sentence-level\nranking and image captioning evaluation. The model implemented is modular,\nexplainable and set to continuously improve over time.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 10:11:33 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 09:58:56 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Kane", "Hassan", ""], ["Kocyigit", "Muhammed Yusuf", ""], ["Abdalla", "Ali", ""], ["Ajanoh", "Pelkins", ""], ["Coulibali", "Mohamed", ""]]}, {"id": "2004.14675", "submitter": "Thomas Zenkel", "authors": "Thomas Zenkel, Joern Wuebker and John DeNero", "title": "End-to-End Neural Word Alignment Outperforms GIZA++", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word alignment was once a core unsupervised learning task in natural language\nprocessing because of its essential role in training statistical machine\ntranslation (MT) models. Although unnecessary for training neural MT models,\nword alignment still plays an important role in interactive applications of\nneural machine translation, such as annotation transfer and lexicon injection.\nWhile statistical MT methods have been replaced by neural approaches with\nsuperior performance, the twenty-year-old GIZA++ toolkit remains a key\ncomponent of state-of-the-art word alignment systems. Prior work on neural word\nalignment has only been able to outperform GIZA++ by using its output during\ntraining. We present the first end-to-end neural word alignment method that\nconsistently outperforms GIZA++ on three data sets. Our approach repurposes a\nTransformer model trained for supervised translation to also serve as an\nunsupervised word alignment model in a manner that is tightly integrated and\ndoes not affect translation quality.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 10:29:37 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Zenkel", "Thomas", ""], ["Wuebker", "Joern", ""], ["DeNero", "John", ""]]}, {"id": "2004.14677", "submitter": "Tuhin Chakrabarty Mr", "authors": "Tuhin Chakrabarty, Christopher Hidey, Smaranda Muresan, Kathy Mckeown,\n  Alyssa Hwang", "title": "AMPERSAND: Argument Mining for PERSuAsive oNline Discussions", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Argumentation is a type of discourse where speakers try to persuade their\naudience about the reasonableness of a claim by presenting supportive\narguments. Most work in argument mining has focused on modeling arguments in\nmonologues. We propose a computational model for argument mining in online\npersuasive discussion forums that brings together the micro-level (argument as\nproduct) and macro-level (argument as process) models of argumentation.\nFundamentally, this approach relies on identifying relations between components\nof arguments in a discussion thread. Our approach for relation prediction uses\ncontextual information in terms of fine-tuning a pre-trained language model and\nleveraging discourse relations based on Rhetorical Structure Theory. We\nadditionally propose a candidate selection method to automatically predict what\nparts of one's argument will be targeted by other participants in the\ndiscussion. Our models obtain significant improvements compared to recent\nstate-of-the-art approaches using pointer networks and a pre-trained language\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 10:33:40 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Chakrabarty", "Tuhin", ""], ["Hidey", "Christopher", ""], ["Muresan", "Smaranda", ""], ["Mckeown", "Kathy", ""], ["Hwang", "Alyssa", ""]]}, {"id": "2004.14693", "submitter": "Yanbin Zhao", "authors": "Yanbin Zhao, Lu Chen, Zhi Chen, Kai Yu", "title": "Semi-Supervised Text Simplification with Back-Translation and Asymmetric\n  Denoising Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text simplification (TS) rephrases long sentences into simplified variants\nwhile preserving inherent semantics. Traditional sequence-to-sequence models\nheavily rely on the quantity and quality of parallel sentences, which limits\ntheir applicability in different languages and domains. This work investigates\nhow to leverage large amounts of unpaired corpora in TS task. We adopt the\nback-translation architecture in unsupervised machine translation (NMT),\nincluding denoising autoencoders for language modeling and automatic generation\nof parallel data by iterative back-translation. However, it is non-trivial to\ngenerate appropriate complex-simple pair if we directly treat the set of simple\nand complex corpora as two different languages, since the two types of\nsentences are quite similar and it is hard for the model to capture the\ncharacteristics in different types of sentences. To tackle this problem, we\npropose asymmetric denoising methods for sentences with separate complexity.\nWhen modeling simple and complex sentences with autoencoders, we introduce\ndifferent types of noise into the training process. Such a method can\nsignificantly improve the simplification performance. Our model can be trained\nin both unsupervised and semi-supervised manner. Automatic and human\nevaluations show that our unsupervised model outperforms the previous systems,\nand with limited supervision, our model can perform competitively with multiple\nstate-of-the-art simplification systems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 11:19:04 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Zhao", "Yanbin", ""], ["Chen", "Lu", ""], ["Chen", "Zhi", ""], ["Yu", "Kai", ""]]}, {"id": "2004.14704", "submitter": "Yang Wei", "authors": "Yang Wei, Yuanbin Wu, and Man Lan", "title": "A Span-based Linearization for Constituent Trees", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel linearization of a constituent tree, together with a new\nlocally normalized model. For each split point in a sentence, our model\ncomputes the normalizer on all spans ending with that split point, and then\npredicts a tree span from them. Compared with global models, our model is fast\nand parallelizable. Different from previous local models, our linearization\nmethod is tied on the spans directly and considers more local features when\nperforming span prediction, which is more interpretable and effective.\nExperiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model\nsignificantly outperforms existing local models and efficiently achieves\ncompetitive results with global models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 11:36:33 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 11:42:39 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Wei", "Yang", ""], ["Wu", "Yuanbin", ""], ["Lan", "Man", ""]]}, {"id": "2004.14710", "submitter": "Shang-Yu Su", "authors": "Shang-Yu Su, Chao-Wei Huang, Yun-Nung Chen", "title": "Towards Unsupervised Language Understanding and Generation by Joint Dual\n  Learning", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modular dialogue systems, natural language understanding (NLU) and natural\nlanguage generation (NLG) are two critical components, where NLU extracts the\nsemantics from the given texts and NLG is to construct corresponding natural\nlanguage sentences based on the input semantic representations. However, the\ndual property between understanding and generation has been rarely explored.\nThe prior work is the first attempt that utilized the duality between NLU and\nNLG to improve the performance via a dual supervised learning framework.\nHowever, the prior work still learned both components in a supervised manner,\ninstead, this paper introduces a general learning framework to effectively\nexploit such duality, providing flexibility of incorporating both supervised\nand unsupervised learning algorithms to train language understanding and\ngeneration models in a joint fashion. The benchmark experiments demonstrate\nthat the proposed approach is capable of boosting the performance of both NLU\nand NLG.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:02:33 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Su", "Shang-Yu", ""], ["Huang", "Chao-Wei", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2004.14723", "submitter": "Pierre Lison", "authors": "Pierre Lison, Aliaksandr Hubin, Jeremy Barnes, and Samia Touileb", "title": "Named Entity Recognition without Labelled Data: A Weak Supervision\n  Approach", "comments": "Accepted to ACL 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Named Entity Recognition (NER) performance often degrades rapidly when\napplied to target domains that differ from the texts observed during training.\nWhen in-domain labelled data is available, transfer learning techniques can be\nused to adapt existing NER models to the target domain. But what should one do\nwhen there is no hand-labelled data for the target domain? This paper presents\na simple but powerful approach to learn NER models in the absence of labelled\ndata through weak supervision. The approach relies on a broad spectrum of\nlabelling functions to automatically annotate texts from the target domain.\nThese annotations are then merged together using a hidden Markov model which\ncaptures the varying accuracies and confusions of the labelling functions. A\nsequence labelling model can finally be trained on the basis of this unified\nannotation. We evaluate the approach on two English datasets (CoNLL 2003 and\nnews articles from Reuters and Bloomberg) and demonstrate an improvement of\nabout 7 percentage points in entity-level $F_1$ scores compared to an\nout-of-domain neural NER model.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:29:55 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Lison", "Pierre", ""], ["Hubin", "Aliaksandr", ""], ["Barnes", "Jeremy", ""], ["Touileb", "Samia", ""]]}, {"id": "2004.14754", "submitter": "Hady Elsahar Dr", "authors": "Hady Elsahar, Maximin Coavoux, Matthias Gall\\'e, Jos Rozen", "title": "Self-Supervised and Controlled Multi-Document Opinion Summarization", "comments": "18 pages including 5 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address the problem of unsupervised abstractive summarization of\ncollections of user generated reviews with self-supervision and control. We\npropose a self-supervised setup that considers an individual document as a\ntarget summary for a set of similar documents. This setting makes training\nsimpler than previous approaches by relying only on standard log-likelihood\nloss. We address the problem of hallucinations through the use of control\ncodes, to steer the generation towards more coherent and relevant\nsummaries.Finally, we extend the Transformer architecture to allow for multiple\nreviews as input. Our benchmarks on two datasets against graph-based and recent\nneural abstractive unsupervised models show that our proposed method generates\nsummaries with a superior quality and relevance.This is confirmed in our human\nevaluation which focuses explicitly on the faithfulness of generated summaries\nWe also provide an ablation study, which shows the importance of the control\nsetup in controlling hallucinations and achieve high sentiment and topic\nalignment of the summaries with the input reviews.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:20:18 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 00:26:52 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Elsahar", "Hady", ""], ["Coavoux", "Maximin", ""], ["Gall\u00e9", "Matthias", ""], ["Rozen", "Jos", ""]]}, {"id": "2004.14758", "submitter": "Serhii Havrylov", "authors": "Serhii Havrylov, Ivan Titov", "title": "Preventing Posterior Collapse with Levenshtein Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) are a standard framework for inducing latent\nvariable models that have been shown effective in learning text representations\nas well as in text generation. The key challenge with using VAEs is the {\\it\nposterior collapse} problem: learning tends to converge to trivial solutions\nwhere the generators ignore latent variables. In our Levenstein VAE, we propose\nto replace the evidence lower bound (ELBO) with a new objective which is simple\nto optimize and prevents posterior collapse. Intuitively, it corresponds to\ngenerating a sequence from the autoencoder and encouraging the model to predict\nan optimal continuation according to the Levenshtein distance (LD) with the\nreference sentence at each time step in the generated sequence. We motivate the\nmethod from the probabilistic perspective by showing that it is closely related\nto optimizing a bound on the intractable Kullback-Leibler divergence of an\nLD-based kernel density estimator from the model distribution. With this\nobjective, any generator disregarding latent variables will incur large\npenalties and hence posterior collapse does not happen. We relate our approach\nto policy distillation \\cite{RossGB11} and dynamic oracles \\cite{GoldbergN12}.\nBy considering Yelp and SNLI benchmarks, we show that Levenstein VAE produces\nmore informative latent representations than alternative approaches to\npreventing posterior collapse.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:27:26 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Havrylov", "Serhii", ""], ["Titov", "Ivan", ""]]}, {"id": "2004.14769", "submitter": "Xiaojun Quan", "authors": "Kun Li, Chengbo Chen, Xiaojun Quan, Qing Ling, and Yan Song", "title": "Conditional Augmentation for Aspect Term Extraction via Masked\n  Sequence-to-Sequence Generation", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect term extraction aims to extract aspect terms from review texts as\nopinion targets for sentiment analysis. One of the big challenges with this\ntask is the lack of sufficient annotated data. While data augmentation is\npotentially an effective technique to address the above issue, it is\nuncontrollable as it may change aspect words and aspect labels unexpectedly. In\nthis paper, we formulate the data augmentation as a conditional generation\ntask: generating a new sentence while preserving the original opinion targets\nand labels. We propose a masked sequence-to-sequence method for conditional\naugmentation of aspect term extraction. Unlike existing augmentation\napproaches, ours is controllable and allows us to generate more diversified\nsentences. Experimental results confirm that our method alleviates the data\nscarcity problem significantly. It also effectively boosts the performances of\nseveral current models for aspect term extraction.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:34:04 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 06:42:16 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Li", "Kun", ""], ["Chen", "Chengbo", ""], ["Quan", "Xiaojun", ""], ["Ling", "Qing", ""], ["Song", "Yan", ""]]}, {"id": "2004.14781", "submitter": "Bo Wang", "authors": "Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Yi Chang", "title": "Structure-Augmented Text Representation Learning for Efficient Knowledge\n  Graph Completion", "comments": "12 pages, WWW'21, April19-23, 2021, Ljubljana, Slovenia", "journal-ref": null, "doi": "10.1145/3442381.3450043", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-curated knowledge graphs provide critical supportive information to\nvarious natural language processing tasks, but these graphs are usually\nincomplete, urging auto-completion of them. Prevalent graph embedding\napproaches, e.g., TransE, learn structured knowledge via representing graph\nelements into dense embeddings and capturing their triple-level relationship\nwith spatial distance. However, they are hardly generalizable to the elements\nnever visited in training and are intrinsically vulnerable to graph\nincompleteness. In contrast, textual encoding approaches, e.g., KG-BERT, resort\nto graph triple's text and triple-level contextualized representations. They\nare generalizable enough and robust to the incompleteness, especially when\ncoupled with pre-trained encoders. But two major drawbacks limit the\nperformance: (1) high overheads due to the costly scoring of all possible\ntriples in inference, and (2) a lack of structured knowledge in the textual\nencoder. In this paper, we follow the textual encoding paradigm and aim to\nalleviate its drawbacks by augmenting it with graph embedding techniques -- a\ncomplementary hybrid of both paradigms. Specifically, we partition each triple\ninto two asymmetric parts as in translation-based graph embedding approach, and\nencode both parts into contextualized representations by a Siamese-style\ntextual encoder. Built upon the representations, our model employs both\ndeterministic classifier and spatial measurement for representation and\nstructure learning respectively. Moreover, we develop a self-adaptive ensemble\nscheme to further improve the performance by incorporating triple scores from\nan existing graph embedding model. In experiments, we achieve state-of-the-art\nperformance on three benchmarks and a zero-shot dataset for link prediction,\nwith highlights of inference costs reduced by 1-2 orders of magnitude compared\nto a textual encoding method.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:50:34 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 03:42:08 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Wang", "Bo", ""], ["Shen", "Tao", ""], ["Long", "Guodong", ""], ["Zhou", "Tianyi", ""], ["Chang", "Yi", ""]]}, {"id": "2004.14786", "submitter": "Zhiyong Wu", "authors": "Zhiyong Wu, Yun Chen, Ben Kao, Qun Liu", "title": "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting\n  BERT", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By introducing a small set of additional parameters, a probe learns to solve\nspecific linguistic tasks (e.g., dependency parsing) in a supervised manner\nusing feature representations (e.g., contextualized embeddings). The\neffectiveness of such probing tasks is taken as evidence that the pre-trained\nmodel encodes linguistic knowledge. However, this approach of evaluating a\nlanguage model is undermined by the uncertainty of the amount of knowledge that\nis learned by the probe itself. Complementary to those works, we propose a\nparameter-free probing technique for analyzing pre-trained language models\n(e.g., BERT). Our method does not require direct supervision from the probing\ntasks, nor do we introduce additional parameters to the probing process. Our\nexperiments on BERT show that syntactic trees recovered from BERT using our\nmethod are significantly better than linguistically-uninformed baselines. We\nfurther feed the empirically induced dependency structures into a downstream\nsentiment classification task and find its improvement compatible with or even\nsuperior to a human-designed dependency schema.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:02:29 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 07:38:42 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 04:17:32 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Wu", "Zhiyong", ""], ["Chen", "Yun", ""], ["Kao", "Ben", ""], ["Liu", "Qun", ""]]}, {"id": "2004.14788", "submitter": "Nikola Nikolov", "authors": "Yingqiang Gao, Nikola I. Nikolov, Yuhuang Hu, Richard H.R. Hahnloser", "title": "Character-Level Translation with Self-attention", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the suitability of self-attention models for character-level\nneural machine translation. We test the standard transformer model, as well as\na novel variant in which the encoder block combines information from nearby\ncharacters using convolutions. We perform extensive experiments on WMT and UN\ndatasets, testing both bilingual and multilingual translation to English using\nup to three input languages (French, Spanish, and Chinese). Our transformer\nvariant consistently outperforms the standard transformer at the\ncharacter-level and converges faster while learning more robust character-level\nalignments.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:05:26 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Gao", "Yingqiang", ""], ["Nikolov", "Nikola I.", ""], ["Hu", "Yuhuang", ""], ["Hahnloser", "Richard H. R.", ""]]}, {"id": "2004.14797", "submitter": "Yevgeni Berzak", "authors": "Yevgeni Berzak, Jonathan Malmaud, Roger Levy", "title": "STARC: Structured Annotations for Reading Comprehension", "comments": "ACL 2020. OneStopQA dataset, STARC guidelines and human experiments\n  data are available at https://github.com/berzak/onestop-qa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present STARC (Structured Annotations for Reading Comprehension), a new\nannotation framework for assessing reading comprehension with multiple choice\nquestions. Our framework introduces a principled structure for the answer\nchoices and ties them to textual span annotations. The framework is implemented\nin OneStopQA, a new high-quality dataset for evaluation and analysis of reading\ncomprehension in English. We use this dataset to demonstrate that STARC can be\nleveraged for a key new application for the development of SAT-like reading\ncomprehension materials: automatic annotation quality probing via span ablation\nexperiments. We further show that it enables in-depth analyses and comparisons\nbetween machine and human reading comprehension behavior, including error\ndistributions and guessing ability. Our experiments also reveal that the\nstandard multiple choice dataset in NLP, RACE, is limited in its ability to\nmeasure reading comprehension. 47% of its questions can be guessed by machines\nwithout accessing the passage, and 18% are unanimously judged by humans as not\nhaving a unique correct answer. OneStopQA provides an alternative test set for\nreading comprehension which alleviates these shortcomings and has a\nsubstantially higher human ceiling performance.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:08:50 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Berzak", "Yevgeni", ""], ["Malmaud", "Jonathan", ""], ["Levy", "Roger", ""]]}, {"id": "2004.14813", "submitter": "Liying Cheng", "authors": "Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu,\n  Luo Si", "title": "ENT-DESC: Entity Description Generation by Exploring Knowledge Graph", "comments": "11 pages, 6 figures, accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works on knowledge-to-text generation take as input a few RDF\ntriples or key-value pairs conveying the knowledge of some entities to generate\na natural language description. Existing datasets, such as WIKIBIO, WebNLG, and\nE2E, basically have a good alignment between an input triple/pair set and its\noutput text. However, in practice, the input knowledge could be more than\nenough, since the output description may only cover the most significant\nknowledge. In this paper, we introduce a large-scale and challenging dataset to\nfacilitate the study of such a practical scenario in KG-to-text. Our dataset\ninvolves retrieving abundant knowledge of various types of main entities from a\nlarge knowledge graph (KG), which makes the current graph-to-sequence models\nseverely suffer from the problems of information loss and parameter explosion\nwhile generating the descriptions. We address these challenges by proposing a\nmulti-graph structure that is able to represent the original graph information\nmore comprehensively. Furthermore, we also incorporate aggregation methods that\nlearn to extract the rich graph information. Extensive experiments demonstrate\nthe effectiveness of our model architecture.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:16:19 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 07:33:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Cheng", "Liying", ""], ["Wu", "Dekun", ""], ["Bing", "Lidong", ""], ["Zhang", "Yan", ""], ["Jie", "Zhanming", ""], ["Lu", "Wei", ""], ["Si", "Luo", ""]]}, {"id": "2004.14821", "submitter": "Shoetsu Sato", "authors": "Shoetsu Sato, Jin Sakuma, Naoki Yoshinaga, Masashi Toyoda, Masaru\n  Kitsuregawa", "title": "Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine\n  Translation", "comments": "9pages + citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network methods exhibit strong performance only in a few resource-rich\ndomains. Practitioners, therefore, employ domain adaptation from resource-rich\ndomains that are, in most cases, distant from the target domain. Domain\nadaptation between distant domains (e.g., movie subtitles and research papers),\nhowever, cannot be performed effectively due to mismatches in vocabulary; it\nwill encounter many domain-specific words (e.g., \"angstrom\") and words whose\nmeanings shift across domains(e.g., \"conductor\"). In this study, aiming to\nsolve these vocabulary mismatches in domain adaptation for neural machine\ntranslation (NMT), we propose vocabulary adaptation, a simple method for\neffective fine-tuning that adapts embedding layers in a given pre-trained NMT\nmodel to the target domain. Prior to fine-tuning, our method replaces the\nembedding layers of the NMT model by projecting general word embeddings induced\nfrom monolingual data in a target domain onto a source-domain embedding space.\nExperimental results indicate that our method improves the performance of\nconventional fine-tuning by 3.86 and 3.28 BLEU points in En-Ja and De-En\ntranslation, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:27:59 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 09:19:25 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sato", "Shoetsu", ""], ["Sakuma", "Jin", ""], ["Yoshinaga", "Naoki", ""], ["Toyoda", "Masashi", ""], ["Kitsuregawa", "Masaru", ""]]}, {"id": "2004.14837", "submitter": "Yun Chen", "authors": "Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, Qun Liu", "title": "Accurate Word Alignment Induction from Neural Machine Translation", "comments": "Accepted by EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its original goal to jointly learn to align and translate, prior\nresearches suggest that Transformer captures poor word alignments through its\nattention mechanism. In this paper, we show that attention weights DO capture\naccurate word alignments and propose two novel word alignment induction methods\nShift-Att and Shift-AET. The main idea is to induce alignments at the step when\nthe to-be-aligned target token is the decoder input rather than the decoder\noutput as in previous work. Shift-Att is an interpretation method that induces\nalignments from the attention weights of Transformer and does not require\nparameter update or architecture change. Shift-AET extracts alignments from an\nadditional alignment module which is tightly integrated into Transformer and\ntrained in isolation with supervision from symmetrized Shift-Att alignments.\nExperiments on three publicly available datasets demonstrate that both methods\nperform better than their corresponding neural baselines and Shift-AET\nsignificantly outperforms GIZA++ by 1.4-4.8 AER points.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:47:05 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 01:57:01 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Chen", "Yun", ""], ["Liu", "Yang", ""], ["Chen", "Guanhua", ""], ["Jiang", "Xin", ""], ["Liu", "Qun", ""]]}, {"id": "2004.14839", "submitter": "Hitomi Yanaka", "authors": "Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, and Kentaro Inui", "title": "Do Neural Models Learn Systematicity of Monotonicity Inference in\n  Natural Language?", "comments": "accepted by ACL2020 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of language models using neural networks, it remains\nunclear to what extent neural models have the generalization ability to perform\ninferences. In this paper, we introduce a method for evaluating whether neural\nmodels can learn systematicity of monotonicity inference in natural language,\nnamely, the regularity for performing arbitrary inferences with generalization\non composition. We consider four aspects of monotonicity inferences and test\nwhether the models can systematically interpret lexical and logical phenomena\non different training/test splits. A series of experiments show that three\nneural models systematically draw inferences on unseen combinations of lexical\nand logical phenomena when the syntactic structures of the sentences are\nsimilar between the training and test sets. However, the performance of the\nmodels significantly decreases when the structures are slightly changed in the\ntest set while retaining all vocabularies and constituents already appearing in\nthe training set. This indicates that the generalization ability of neural\nmodels is limited to cases where the syntactic structures are nearly the same\nas those in the training set.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:48:39 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 12:35:41 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yanaka", "Hitomi", ""], ["Mineshima", "Koji", ""], ["Bekki", "Daisuke", ""], ["Inui", "Kentaro", ""]]}, {"id": "2004.14843", "submitter": "Federico Bianchi", "authors": "Federico Bianchi and Gaetano Rossiello and Luca Costabello and Matteo\n  Palmonari and Pasquale Minervini", "title": "Knowledge Graph Embeddings and Explainable AI", "comments": "Federico Bianchi, Gaetano Rossiello, Luca Costabello, Matteo\n  Plamonari, Pasquale Minervini, Knowledge Graph Embeddings and Explainable AI.\n  In: Ilaria Tiddi, Freddy Lecue, Pascal Hitzler (eds.), Knowledge Graphs for\n  eXplainable AI -- Foundations, Applications and Challenges. Studies on the\n  Semantic Web, IOS Press, Amsterdam, 2020", "journal-ref": null, "doi": "10.3233/SSW200011", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embeddings are now a widely adopted approach to knowledge\nrepresentation in which entities and relationships are embedded in vector\nspaces. In this chapter, we introduce the reader to the concept of knowledge\ngraph embeddings by explaining what they are, how they can be generated and how\nthey can be evaluated. We summarize the state-of-the-art in this field by\ndescribing the approaches that have been introduced to represent knowledge in\nthe vector space. In relation to knowledge representation, we consider the\nproblem of explainability, and discuss models and methods for explaining\npredictions obtained via knowledge graph embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:55:09 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Bianchi", "Federico", ""], ["Rossiello", "Gaetano", ""], ["Costabello", "Luca", ""], ["Palmonari", "Matteo", ""], ["Minervini", "Pasquale", ""]]}, {"id": "2004.14846", "submitter": "Elizabeth Nielsen", "authors": "Elizabeth Nielsen, Mark Steedman, Sharon Goldwater", "title": "The role of context in neural pitch accent detection in English", "comments": null, "journal-ref": "Proceedings of the 2020 Conference on Empirical Methods in Natural\n  Language Processing", "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prosody is a rich information source in natural language, serving as a marker\nfor phenomena such as contrast. In order to make this information available to\ndownstream tasks, we need a way to detect prosodic events in speech. We propose\na new model for pitch accent detection, inspired by the work of Stehwien et al.\n(2018), who presented a CNN-based model for this task. Our model makes greater\nuse of context by using full utterances as input and adding an LSTM layer. We\nfind that these innovations lead to an improvement from 87.5% to 88.7% accuracy\non pitch accent detection on American English speech in the Boston University\nRadio News Corpus, a state-of-the-art result. We also find that a simple\nbaseline that just predicts a pitch accent on every content word yields 82.2%\naccuracy, and we suggest that this is the appropriate baseline for this task.\nFinally, we conduct ablation tests that show pitch is the most important\nacoustic feature for this task and this corpus.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:59:05 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 08:25:23 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Nielsen", "Elizabeth", ""], ["Steedman", "Mark", ""], ["Goldwater", "Sharon", ""]]}, {"id": "2004.14848", "submitter": "Momchil Hardalov", "authors": "Momchil Hardalov, Ivan Koychev and Preslav Nakov", "title": "Enriched Pre-trained Transformers for Joint Slot Filling and Intent\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the user's intent and finding the corresponding slots among the\nutterance's words are important tasks in natural language understanding. Their\ninterconnected nature makes their joint modeling a standard part of training\nsuch models. Moreover, data scarceness and specialized vocabularies pose\nadditional challenges. Recently, the advances in pre-trained language models,\nnamely contextualized models such as ELMo and BERT have revolutionized the\nfield by tapping the potential of training very large models with just a few\nsteps of fine-tuning on a task-specific dataset. Here, we leverage such model,\nnamely BERT, and we design a novel architecture on top it. Moreover, we propose\nan intent pooling attention mechanism, and we reinforce the slot filling task\nby fusing intent distributions, word features, and token representations. The\nexperimental results on standard datasets show that our model outperforms both\nthe current non-BERT state of the art as well as some stronger BERT-based\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:00:21 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Hardalov", "Momchil", ""], ["Koychev", "Ivan", ""], ["Nakov", "Preslav", ""]]}, {"id": "2004.14855", "submitter": "Leonhard Hennig", "authors": "Christoph Alt, Aleksandra Gabryszak, Leonhard Hennig", "title": "TACRED Revisited: A Thorough Evaluation of the TACRED Relation\n  Extraction Task", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TACRED (Zhang et al., 2017) is one of the largest, most widely used\ncrowdsourced datasets in Relation Extraction (RE). But, even with recent\nadvances in unsupervised pre-training and knowledge enhanced neural RE, models\nstill show a high error rate. In this paper, we investigate the questions: Have\nwe reached a performance ceiling or is there still room for improvement? And\nhow do crowd annotations, dataset, and models contribute to this error rate? To\nanswer these questions, we first validate the most challenging 5K examples in\nthe development and test sets using trained annotators. We find that label\nerrors account for 8% absolute F1 test error, and that more than 50% of the\nexamples need to be relabeled. On the relabeled test set the average F1 score\nof a large baseline model set improves from 62.1 to 70.1. After validation, we\nanalyze misclassifications on the challenging instances, categorize them into\nlinguistically motivated error groups, and verify the resulting error\nhypotheses on three state-of-the-art RE models. We show that two groups of\nambiguous relations are responsible for most of the remaining errors and that\nmodels may adopt shallow heuristics on the dataset when entities are not\nmasked.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:07:37 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Alt", "Christoph", ""], ["Gabryszak", "Aleksandra", ""], ["Hennig", "Leonhard", ""]]}, {"id": "2004.14858", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Alice Baird, Georgios Rizos, Panagiotis Tzirakis,\n  Xinchen Du, Felix Hafner, Lea Schumann, Adria Mallol-Ragolta, Bj\\\"orn W.\n  Schuller, Iulia Lefter, Erik Cambria, Ioannis Kompatsiaris", "title": "MuSe 2020 -- The First International Multimodal Sentiment Analysis in\n  Real-life Media Challenge and Workshop", "comments": "Baseline Paper MuSe 2020, MuSe Workshop Challenge, ACM Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal Sentiment Analysis in Real-life Media (MuSe) 2020 is a\nChallenge-based Workshop focusing on the tasks of sentiment recognition, as\nwell as emotion-target engagement and trustworthiness detection by means of\nmore comprehensively integrating the audio-visual and language modalities. The\npurpose of MuSe 2020 is to bring together communities from different\ndisciplines; mainly, the audio-visual emotion recognition community\n(signal-based), and the sentiment analysis community (symbol-based). We present\nthree distinct sub-challenges: MuSe-Wild, which focuses on continuous emotion\n(arousal and valence) prediction; MuSe-Topic, in which participants recognise\ndomain-specific topics as the target of 3-class (low, medium, high) emotions;\nand MuSe-Trust, in which the novel aspect of trustworthiness is to be\npredicted. In this paper, we provide detailed information on MuSe-CaR, the\nfirst of its kind in-the-wild database, which is utilised for the challenge, as\nwell as the state-of-the-art features and modelling approaches applied. For\neach sub-challenge, a competitive baseline for participants is set; namely, on\ntest we report for MuSe-Wild a combined (valence and arousal) CCC of .2568, for\nMuSe-Topic a score (computed as 0.34$\\cdot$ UAR + 0.66$\\cdot$F1) of 76.78 % on\nthe 10-class topic and 40.64 % on the 3-class emotion prediction, and for\nMuSe-Trust a CCC of .4359.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:54:22 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 16:05:49 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 08:37:43 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Stappen", "Lukas", ""], ["Baird", "Alice", ""], ["Rizos", "Georgios", ""], ["Tzirakis", "Panagiotis", ""], ["Du", "Xinchen", ""], ["Hafner", "Felix", ""], ["Schumann", "Lea", ""], ["Mallol-Ragolta", "Adria", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Lefter", "Iulia", ""], ["Cambria", "Erik", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "2004.14870", "submitter": "Samson Tan", "authors": "Samson Tan, Shafiq Joty, Lav R. Varshney, Min-Yen Kan", "title": "Mind Your Inflections! Improving NLP for Non-Standard Englishes with\n  Base-Inflection Encoding", "comments": "Published in the Proceedings of the 2020 Conference on Empirical\n  Methods in Natural Language Processing", "journal-ref": "2020.emnlp-main.455", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inflectional variation is a common feature of World Englishes such as\nColloquial Singapore English and African American Vernacular English. Although\ncomprehension by human readers is usually unimpaired by non-standard\ninflections, current NLP systems are not yet robust. We propose Base-Inflection\nEncoding (BITE), a method to tokenize English text by reducing inflected words\nto their base forms before reinjecting the grammatical information as special\nsymbols. Fine-tuning pretrained NLP models for downstream tasks using our\nencoding defends against inflectional adversaries while maintaining performance\non clean data. Models using BITE generalize better to dialects with\nnon-standard inflections without explicit training and translation models\nconverge faster when trained with BITE. Finally, we show that our encoding\nimproves the vocabulary efficiency of popular data-driven subword tokenizers.\nSince there has been no prior work on quantitatively evaluating vocabulary\nefficiency, we propose metrics to do so.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:15:40 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 18:54:40 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 05:20:28 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2020 06:16:31 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Tan", "Samson", ""], ["Joty", "Shafiq", ""], ["Varshney", "Lav R.", ""], ["Kan", "Min-Yen", ""]]}, {"id": "2004.14871", "submitter": "Libo Qin", "authors": "Libo Qin, Minheng Ni, Yue Zhang, Wanxiang Che, Yangming Li, Ting Liu", "title": "Multi-Domain Spoken Language Understanding Using Domain- and Task-Aware\n  Parameterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding has been addressed as a supervised learning\nproblem, where a set of training data is available for each domain. However,\nannotating data for each domain is both financially costly and non-scalable so\nwe should fully utilize information across all domains. One existing approach\nsolves the problem by conducting multi-domain learning, using shared parameters\nfor joint training across domains. We propose to improve the parameterization\nof this method by using domain-specific and task-specific model parameters to\nimprove knowledge learning and transfer. Experiments on 5 domains show that our\nmodel is more effective for multi-domain SLU and obtain the best results. In\naddition, we show its transferability by outperforming the prior best model by\n12.4\\% when adapting to a new domain with little data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:15:40 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Qin", "Libo", ""], ["Ni", "Minheng", ""], ["Zhang", "Yue", ""], ["Che", "Wanxiang", ""], ["Li", "Yangming", ""], ["Liu", "Ting", ""]]}, {"id": "2004.14874", "submitter": "Ben Saunders", "authors": "Ben Saunders and Necati Cihan Camgoz and Richard Bowden", "title": "Progressive Transformers for End-to-End Sign Language Production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of automatic Sign Language Production (SLP) is to translate spoken\nlanguage to a continuous stream of sign language video at a level comparable to\na human translator. If this was achievable, then it would revolutionise Deaf\nhearing communications. Previous work on predominantly isolated SLP has shown\nthe need for architectures that are better suited to the continuous domain of\nfull sign sequences.\n  In this paper, we propose Progressive Transformers, a novel architecture that\ncan translate from discrete spoken language sentences to continuous 3D skeleton\npose outputs representing sign language. We present two model configurations,\nan end-to-end network that produces sign direct from text and a stacked network\nthat utilises a gloss intermediary.\n  Our transformer network architecture introduces a counter that enables\ncontinuous sequence generation at training and inference. We also provide\nseveral data augmentation processes to overcome the problem of drift and\nimprove the performance of SLP models. We propose a back translation evaluation\nmechanism for SLP, presenting benchmark quantitative results on the challenging\nRWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset and setting baselines for future\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:20:25 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 10:20:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Saunders", "Ben", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2004.14876", "submitter": "Laura (Wendlandt) Burdick", "authors": "Laura Burdick, Jonathan K. Kummerfeld, Rada Mihalcea", "title": "Analyzing the Surprising Variability in Word Embedding Stability Across\n  Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are powerful representations that form the foundation of many\nnatural language processing architectures and tasks, both in English and in\nother languages. To gain further insight into word embeddings in multiple\nlanguages, we explore their stability, defined as the overlap between the\nnearest neighbors of a word in different embedding spaces. We discuss\nlinguistic properties that are related to stability, drawing out insights about\nhow morphological and other features relate to stability. This has implications\nfor the usage of embeddings, particularly in research that uses embeddings to\nstudy language trends.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:24:43 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Burdick", "Laura", ""], ["Kummerfeld", "Jonathan K.", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2004.14884", "submitter": "Arthur Bra\\v{z}inskas", "authors": "Arthur Bra\\v{z}inskas, Mirella Lapata, Ivan Titov", "title": "Few-Shot Learning for Opinion Summarization", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion summarization is the automatic creation of text reflecting subjective\ninformation expressed in multiple documents, such as user reviews of a product.\nThe task is practically important and has attracted a lot of attention.\nHowever, due to the high cost of summary production, datasets large enough for\ntraining supervised models are lacking. Instead, the task has been\ntraditionally approached with extractive methods that learn to select text\nfragments in an unsupervised or weakly-supervised way. Recently, it has been\nshown that abstractive summaries, potentially more fluent and better at\nreflecting conflicting information, can also be produced in an unsupervised\nfashion. However, these models, not being exposed to actual summaries, fail to\ncapture their essential properties. In this work, we show that even a handful\nof summaries is sufficient to bootstrap generation of the summary text with all\nexpected properties, such as writing style, informativeness, fluency, and\nsentiment preservation. We start by training a conditional Transformer language\nmodel to generate a new product review given other available reviews of the\nproduct. The model is also conditioned on review properties that are directly\nrelated to summaries; the properties are derived from reviews with no manual\neffort. In the second stage, we fine-tune a plug-in module that learns to\npredict property values on a handful of summaries. This lets us switch the\ngenerator to the summarization mode. We show on Amazon and Yelp datasets that\nour approach substantially outperforms previous extractive and abstractive\nmethods in automatic and human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:37:38 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 19:45:25 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 06:30:38 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bra\u017einskas", "Arthur", ""], ["Lapata", "Mirella", ""], ["Titov", "Ivan", ""]]}, {"id": "2004.14900", "submitter": "Jacopo Staiano", "authors": "Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin\n  Piwowarski, Jacopo Staiano", "title": "MLSUM: The Multilingual Summarization Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MLSUM, the first large-scale MultiLingual SUMmarization dataset.\nObtained from online newspapers, it contains 1.5M+ article/summary pairs in\nfive different languages -- namely, French, German, Spanish, Russian, Turkish.\nTogether with English newspapers from the popular CNN/Daily mail dataset, the\ncollected data form a large scale multilingual dataset which can enable new\nresearch directions for the text summarization community. We report\ncross-lingual comparative analyses based on state-of-the-art systems. These\nhighlight existing biases which motivate the use of a multi-lingual dataset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:58:34 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Scialom", "Thomas", ""], ["Dray", "Paul-Alexis", ""], ["Lamprier", "Sylvain", ""], ["Piwowarski", "Benjamin", ""], ["Staiano", "Jacopo", ""]]}, {"id": "2004.14905", "submitter": "David Wilmot", "authors": "David Wilmot and Frank Keller", "title": "Modelling Suspense in Short Stories as Uncertainty Reduction over Neural\n  Representation", "comments": "9 pages, 3 figures, accepted as long paper to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Suspense is a crucial ingredient of narrative fiction, engaging readers and\nmaking stories compelling. While there is a vast theoretical literature on\nsuspense, it is computationally not well understood. We compare two ways for\nmodelling suspense: surprise, a backward-looking measure of how unexpected the\ncurrent state is given the story so far; and uncertainty reduction, a\nforward-looking measure of how unexpected the continuation of the story is.\nBoth can be computed either directly over story representations or over their\nprobability distributions. We propose a hierarchical language model that\nencodes stories and computes surprise and uncertainty reduction. Evaluating\nagainst short stories annotated with human suspense judgements, we find that\nuncertainty reduction over representations is the best predictor, resulting in\nnear-human accuracy. We also show that uncertainty reduction can be used to\npredict suspenseful events in movie synopses.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:03:06 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Wilmot", "David", ""], ["Keller", "Frank", ""]]}, {"id": "2004.14907", "submitter": "Shraey Bhatia", "authors": "Shraey Bhatia, Jey Han Lau, Timothy Baldwin", "title": "You are right. I am ALARMED -- But by Climate Change Counter Movement", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is facing the challenge of climate crisis. Despite the consensus in\nscientific community about anthropogenic global warming, the web is flooded\nwith articles spreading climate misinformation. These articles are carefully\nconstructed by climate change counter movement (cccm) organizations to\ninfluence the narrative around climate change. We revisit the literature on\nclimate misinformation in social sciences and repackage it to introduce in the\ncommunity of NLP. Despite considerable work in detection of fake news, there is\nno misinformation dataset available that is specific to the domain.of climate\nchange. We try to bridge this gap by scraping and releasing articles with known\nclimate change misinformation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:06:02 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Bhatia", "Shraey", ""], ["Lau", "Jey Han", ""], ["Baldwin", "Timothy", ""]]}, {"id": "2004.14911", "submitter": "Asa Cooper Stickland", "authors": "Asa Cooper Stickland, Xian Li, Marjan Ghazvininejad", "title": "Recipes for Adapting Pre-trained Monolingual and Multilingual Models to\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been recent success in pre-training on monolingual data and\nfine-tuning on Machine Translation (MT), but it remains unclear how to best\nleverage a pre-trained model for a given MT task. This paper investigates the\nbenefits and drawbacks of freezing parameters, and adding new ones, when\nfine-tuning a pre-trained model on MT. We focus on 1) Fine-tuning a model\ntrained only on English monolingual data, BART. 2) Fine-tuning a model trained\non monolingual data from 25 languages, mBART. For BART we get the best\nperformance by freezing most of the model parameters, and adding extra\npositional embeddings. For mBART we match the performance of naive fine-tuning\nfor most language pairs, and outperform it for Nepali to English (0.5 BLEU) and\nCzech to English (0.6 BLEU), all with a lower memory cost at training time.\nWhen constraining ourselves to an out-of-domain training set for Vietnamese to\nEnglish we outperform the fine-tuning baseline by 0.9 BLEU.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:09:22 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Stickland", "Asa Cooper", ""], ["Li", "Xian", ""], ["Ghazvininejad", "Marjan", ""]]}, {"id": "2004.14914", "submitter": "Sabrina Mielke", "authors": "Suzanna Sia, Ayush Dalmia, Sabrina J. Mielke", "title": "Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for\n  Fast and Good Topics too!", "comments": "Published as a short paper at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are a useful analysis tool to uncover the underlying themes\nwithin document collections. The dominant approach is to use probabilistic\ntopic models that posit a generative story, but in this paper we propose an\nalternative way to obtain topics: clustering pre-trained word embeddings while\nincorporating document information for weighted clustering and reranking top\nwords. We provide benchmarks for the combination of different word embeddings\nand clustering algorithms, and analyse their performance under dimensionality\nreduction with PCA. The best performing combination for our approach performs\nas well as classical topic models, but with lower runtime and computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:18:18 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 19:23:46 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Sia", "Suzanna", ""], ["Dalmia", "Ayush", ""], ["Mielke", "Sabrina J.", ""]]}, {"id": "2004.14923", "submitter": "Arturo Oncevay", "authors": "Arturo Oncevay, Barry Haddow, Alexandra Birch", "title": "Bridging Linguistic Typology and Multilingual Machine Translation with\n  Multi-View Language Representations", "comments": "Accepted at EMNLP 2020. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse language vectors from linguistic typology databases and learned\nembeddings from tasks like multilingual machine translation have been\ninvestigated in isolation, without analysing how they could benefit from each\nother's language characterisation. We propose to fuse both views using singular\nvector canonical correlation analysis and study what kind of information is\ninduced from each source. By inferring typological features and language\nphylogenies, we observe that our representations embed typology and strengthen\ncorrelations with language relationships. We then take advantage of our\nmulti-view language vector space for multilingual machine translation, where we\nachieve competitive overall translation accuracy in tasks that require\ninformation about language similarities, such as language clustering and\nranking candidates for multilingual transfer. With our method, which is also\nreleased as a tool, we can easily project and assess new languages without\nexpensive retraining of massive multilingual or ranking models, which are major\ndisadvantages of related approaches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:25:39 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 20:51:46 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Oncevay", "Arturo", ""], ["Haddow", "Barry", ""], ["Birch", "Alexandra", ""]]}, {"id": "2004.14927", "submitter": "Dario Stojanovski", "authors": "Dario Stojanovski, Alexander Fraser", "title": "Addressing Zero-Resource Domains Using Document-Level Context in Neural\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving satisfying performance in machine translation on domains for which\nthere is no training data is challenging. Traditional supervised domain\nadaptation is not suitable for addressing such zero-resource domains because it\nrelies on in-domain parallel data. We show that when in-domain parallel data is\nnot available, access to document-level context enables better capturing of\ndomain generalities compared to only having access to a single sentence. Having\naccess to more information provides a more reliable domain estimation. We\npresent two document-level Transformer models which are capable of using large\ncontext sizes and we compare these models against strong Transformer baselines.\nWe obtain improvements for the two zero resource domains we study. We\nadditionally provide an analysis where we vary the amount of context and look\nat the case where in-domain data is available.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:28:19 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 11:25:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Stojanovski", "Dario", ""], ["Fraser", "Alexander", ""]]}, {"id": "2004.14928", "submitter": "Christos Baziotis", "authors": "Christos Baziotis, Barry Haddow, Alexandra Birch", "title": "Language Model Prior for Low-Resource Neural Machine Translation", "comments": "Accepted at EMNLP 2020. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity of large parallel corpora is an important obstacle for neural\nmachine translation. A common solution is to exploit the knowledge of language\nmodels (LM) trained on abundant monolingual data. In this work, we propose a\nnovel approach to incorporate a LM as prior in a neural translation model (TM).\nSpecifically, we add a regularization term, which pushes the output\ndistributions of the TM to be probable under the LM prior, while avoiding wrong\npredictions when the TM \"disagrees\" with the LM. This objective relates to\nknowledge distillation, where the LM can be viewed as teaching the TM about the\ntarget language. The proposed approach does not compromise decoding speed,\nbecause the LM is used only at training time, unlike previous work that\nrequires it during inference. We present an analysis of the effects that\ndifferent methods have on the distributions of the TM. Results on two\nlow-resource machine translation datasets show clear improvements even with\nlimited monolingual data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:29:56 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 21:39:55 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 08:56:46 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Baziotis", "Christos", ""], ["Haddow", "Barry", ""], ["Birch", "Alexandra", ""]]}, {"id": "2004.14958", "submitter": "Mikel Artetxe", "authors": "Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka, Eneko\n  Agirre", "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review motivations, definition, approaches, and methodology for\nunsupervised cross-lingual learning and call for a more rigorous position in\neach of them. An existing rationale for such research is based on the lack of\nparallel data for many of the world's languages. However, we argue that a\nscenario without any parallel data and abundant monolingual data is unrealistic\nin practice. We also discuss different training signals that have been used in\nprevious work, which depart from the pure unsupervised setting. We then\ndescribe common methodological issues in tuning and evaluation of unsupervised\ncross-lingual models and present best practices. Finally, we provide a unified\noutlook for different types of research in this area (i.e., cross-lingual word\nembeddings, deep multilingual pretraining, and unsupervised machine\ntranslation) and argue for comparable evaluation of these models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:06:23 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Artetxe", "Mikel", ""], ["Ruder", "Sebastian", ""], ["Yogatama", "Dani", ""], ["Labaka", "Gorka", ""], ["Agirre", "Eneko", ""]]}, {"id": "2004.14959", "submitter": "Deborah Ferreira", "authors": "Deborah Ferreira and Andre Freitas", "title": "Natural Language Premise Selection: Finding Supporting Statements for\n  Mathematical Text", "comments": "12th Language Resources and Evaluation Conference (LREC), Marseille,\n  France, 2020 (Language Resource Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical text is written using a combination of words and mathematical\nexpressions. This combination, along with a specific way of structuring\nsentences makes it challenging for state-of-art NLP tools to understand and\nreason on top of mathematical discourse. In this work, we propose a new NLP\ntask, the natural premise selection, which is used to retrieve supporting\ndefinitions and supporting propositions that are useful for generating an\ninformal mathematical proof for a particular statement. We also make available\na dataset, NL-PS, which can be used to evaluate different approaches for the\nnatural premise selection task. Using different baselines, we demonstrate the\nunderlying interpretation challenges associated with the task.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:08:03 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Ferreira", "Deborah", ""], ["Freitas", "Andre", ""]]}, {"id": "2004.14961", "submitter": "Mohammad Sadegh Rasooli", "authors": "Maryam Aminian, Mohammad Sadegh Rasooli, Mona Diab", "title": "Mutlitask Learning for Cross-Lingual Transfer of Semantic Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for developing broad-coverage semantic dependency\nparsers for languages for which no semantically annotated resource is\navailable. We leverage a multitask learning framework coupled with an\nannotation projection method. We transfer supervised semantic dependency parse\nannotations from a rich-resource language to a low-resource language through\nparallel data, and train a semantic parser on projected data. We make use of\nsupervised syntactic parsing as an auxiliary task in a multitask learning\nframework, and show that with different multitask learning settings, we\nconsistently improve over the single-task baseline. In the setting in which\nEnglish is the source, and Czech is the target language, our best multitask\nmodel improves the labeled F1 score over the single-task baseline by 1.8 in the\nin-domain SemEval data (Oepen et al., 2015), as well as 2.5 in the\nout-of-domain test set. Moreover, we observe that syntactic and semantic\ndependency direction match is an important factor in improving the results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:09:51 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Aminian", "Maryam", ""], ["Rasooli", "Mohammad Sadegh", ""], ["Diab", "Mona", ""]]}, {"id": "2004.14963", "submitter": "Emrah Budur", "authors": "Emrah Budur, R{\\i}za \\\"Oz\\c{c}elik, Tunga G\\\"ung\\\"or, and Christopher\n  Potts", "title": "Data and Representation for Turkish Natural Language Inference", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large annotated datasets in NLP are overwhelmingly in English. This is an\nobstacle to progress in other languages. Unfortunately, obtaining new annotated\nresources for each task in each language would be prohibitively expensive. At\nthe same time, commercial machine translation systems are now robust. Can we\nleverage these systems to translate English-language datasets automatically? In\nthis paper, we offer a positive response for natural language inference (NLI)\nin Turkish. We translated two large English NLI datasets into Turkish and had a\nteam of experts validate their translation quality and fidelity to the original\nlabels. Using these datasets, we address core issues of representation for\nTurkish NLI. We find that in-language embeddings are essential and that\nmorphological parsing can be avoided where the training set is large. Finally,\nwe show that models trained on our machine-translated datasets are successful\non human-translated evaluation sets. We share all code, models, and data\npublicly.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:12:52 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 23:15:03 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 15:25:07 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Budur", "Emrah", ""], ["\u00d6z\u00e7elik", "R\u0131za", ""], ["G\u00fcng\u00f6r", "Tunga", ""], ["Potts", "Christopher", ""]]}, {"id": "2004.14967", "submitter": "Hannah Rashkin", "authors": "Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and Jianfeng Gao", "title": "PlotMachines: Outline-Conditioned Generation with Dynamic Plot State\n  Tracking", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the task of outline-conditioned story generation: given an outline\nas a set of phrases that describe key characters and events to appear in a\nstory, the task is to generate a coherent narrative that is consistent with the\nprovided outline. This task is challenging as the input only provides a rough\nsketch of the plot, and thus, models need to generate a story by interweaving\nthe key points provided in the outline. This requires the model to keep track\nof the dynamic states of the latent plot, conditioning on the input outline\nwhile generating the full story. We present PlotMachines, a neural narrative\nmodel that learns to transform an outline into a coherent story by tracking the\ndynamic plot states. In addition, we enrich PlotMachines with high-level\ndiscourse structure so that the model can learn different writing styles\ncorresponding to different parts of the narrative. Comprehensive experiments\nover three fiction and non-fiction datasets demonstrate that large-scale\nlanguage models, such as GPT-2 and Grover, despite their impressive generation\nperformance, are not sufficient in generating coherent narratives for the given\noutline, and dynamic plot state tracking is important for composing narratives\nwith tighter, more consistent plots.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:16:31 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 23:40:11 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Rashkin", "Hannah", ""], ["Celikyilmaz", "Asli", ""], ["Choi", "Yejin", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2004.14969", "submitter": "Baoxu Shi", "authors": "Baoxu Shi, Shan Li, Jaewon Yang, Mustafa Emre Kazdagli, Qi He", "title": "Learning to Ask Screening Questions for Job Postings", "comments": "10 pages, to appear in SIGIR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At LinkedIn, we want to create economic opportunity for everyone in the\nglobal workforce. A critical aspect of this goal is matching jobs with\nqualified applicants. To improve hiring efficiency and reduce the need to\nmanually screening each applicant, we develop a new product where recruiters\ncan ask screening questions online so that they can filter qualified candidates\neasily. To add screening questions to all $20$M active jobs at LinkedIn, we\npropose a new task that aims to automatically generate screening questions for\na given job posting. To solve the task of generating screening questions, we\ndevelop a two-stage deep learning model called Job2Questions, where we apply a\ndeep learning model to detect intent from the text description, and then rank\nthe detected intents by their importance based on other contextual features.\nSince this is a new product with no historical data, we employ deep transfer\nlearning to train complex models with limited training data. We launched the\nscreening question product and our AI models to LinkedIn users and observed\nsignificant impact in the job marketplace. During our online A/B test, we\nobserved $+53.10\\%$ screening question suggestion acceptance rate, $+22.17\\%$\njob coverage, $+190\\%$ recruiter-applicant interaction, and $+11$ Net Promoter\nScore. In sum, the deployed Job2Questions model helps recruiters to find\nqualified applicants and job seekers to find jobs they are qualified for.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:18:17 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Shi", "Baoxu", ""], ["Li", "Shan", ""], ["Yang", "Jaewon", ""], ["Kazdagli", "Mustafa Emre", ""], ["He", "Qi", ""]]}, {"id": "2004.14973", "submitter": "Arjun Majumdar", "authors": "Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi\n  Parikh, Dhruv Batra", "title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the\n  Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following a navigation instruction such as 'Walk down the stairs and stop at\nthe brown sofa' requires embodied AI agents to ground scene elements referenced\nvia language (e.g. 'stairs') to visual content in the environment (pixels\ncorresponding to 'stairs').\n  We ask the following question -- can we leverage abundant 'disembodied'\nweb-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn\nvisual groundings (what do 'stairs' look like?) that improve performance on a\nrelatively data-starved embodied perception task (Vision-and-Language\nNavigation)? Specifically, we develop VLN-BERT, a visiolinguistic\ntransformer-based model for scoring the compatibility between an instruction\n('...stop at the brown sofa') and a sequence of panoramic RGB images captured\nby the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from\nthe web before fine-tuning on embodied path-instruction data significantly\nimproves performance on VLN -- outperforming the prior state-of-the-art in the\nfully-observed setting by 4 absolute percentage points on success rate.\nAblations of our pretraining curriculum show each stage to be impactful -- with\ntheir combination resulting in further positive synergistic effects.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:22:40 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 17:16:50 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Majumdar", "Arjun", ""], ["Shrivastava", "Ayush", ""], ["Lee", "Stefan", ""], ["Anderson", "Peter", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "2004.14974", "submitter": "David Wadden", "authors": "David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van\n  Zuylen, Arman Cohan, Hannaneh Hajishirzi", "title": "Fact or Fiction: Verifying Scientific Claims", "comments": "EMNLP 2020. GitHub: https://github.com/allenai/scifact. Leaderboard\n  and demo: https://scifact.apps.allenai.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce scientific claim verification, a new task to select abstracts\nfrom the research literature containing evidence that SUPPORTS or REFUTES a\ngiven scientific claim, and to identify rationales justifying each decision. To\nstudy this task, we construct SciFact, a dataset of 1.4K expert-written\nscientific claims paired with evidence-containing abstracts annotated with\nlabels and rationales. We develop baseline models for SciFact, and demonstrate\nthat simple domain adaptation techniques substantially improve performance\ncompared to models trained on Wikipedia or political news. We show that our\nsystem is able to verify claims related to COVID-19 by identifying evidence\nfrom the CORD-19 corpus. Our experiments indicate that SciFact will provide a\nchallenging testbed for the development of new systems designed to retrieve and\nreason over corpora containing specialized domain knowledge. Data and code for\nthis new task are publicly available at https://github.com/allenai/scifact. A\nleaderboard and COVID-19 fact-checking demo are available at\nhttps://scifact.apps.allenai.org.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:22:57 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 17:15:15 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 20:49:43 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2020 19:14:04 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2020 08:16:27 GMT"}, {"version": "v6", "created": "Sat, 3 Oct 2020 04:31:06 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wadden", "David", ""], ["Lin", "Shanchuan", ""], ["Lo", "Kyle", ""], ["Wang", "Lucy Lu", ""], ["van Zuylen", "Madeleine", ""], ["Cohan", "Arman", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2004.14975", "submitter": "Alex Tamkin", "authors": "Alex Tamkin, Trisha Singh, Davide Giovanardi, Noah Goodman", "title": "Investigating Transferability in Pretrained Language Models", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does language model pretraining help transfer learning? We consider a\nsimple ablation technique for determining the impact of each pretrained layer\non transfer task performance. This method, partial reinitialization, involves\nreplacing different layers of a pretrained model with random weights, then\nfinetuning the entire model on the transfer task and observing the change in\nperformance. This technique reveals that in BERT, layers with high probing\nperformance on downstream GLUE tasks are neither necessary nor sufficient for\nhigh accuracy on those tasks. Furthermore, the benefit of using pretrained\nparameters for a layer varies dramatically with finetuning dataset size:\nparameters that provide tremendous performance improvement when data is\nplentiful may provide negligible benefits in data-scarce settings. These\nresults reveal the complexity of the transfer learning process, highlighting\nthe limitations of methods that operate on frozen models or single data\nsamples.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:23:19 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 00:56:31 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Tamkin", "Alex", ""], ["Singh", "Trisha", ""], ["Giovanardi", "Davide", ""], ["Goodman", "Noah", ""]]}, {"id": "2004.14979", "submitter": "Vered Shwartz", "authors": "Yehudit Meged, Avi Caciularu, Vered Shwartz, Ido Dagan", "title": "Paraphrasing vs Coreferring: Two Sides of the Same Coin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the potential synergy between two different NLP tasks, both\nconfronting predicate lexical variability: identifying predicate paraphrases,\nand event coreference resolution. First, we used annotations from an event\ncoreference dataset as distant supervision to re-score heuristically-extracted\npredicate paraphrases. The new scoring gained more than 18 points in average\nprecision upon their ranking by the original scoring method. Then, we used the\nsame re-ranking features as additional inputs to a state-of-the-art event\ncoreference resolution model, which yielded modest but consistent improvements\nto the model's performance. The results suggest a promising direction to\nleverage data and models for each of the tasks to the benefit of the other.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:29:17 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 16:48:36 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Meged", "Yehudit", ""], ["Caciularu", "Avi", ""], ["Shwartz", "Vered", ""], ["Dagan", "Ido", ""]]}, {"id": "2004.14983", "submitter": "Nora Hollenstein", "authors": "Giuseppe Russo, Nora Hollenstein, Claudiu Musat, Ce Zhang", "title": "Control, Generate, Augment: A Scalable Framework for Multi-Attribute\n  Text Generation", "comments": "Accepted at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce CGA, a conditional VAE architecture, to control, generate, and\naugment text. CGA is able to generate natural English sentences controlling\nmultiple semantic and syntactic attributes by combining adversarial learning\nwith a context-aware loss and a cyclical word dropout routine. We demonstrate\nthe value of the individual model components in an ablation study. The\nscalability of our approach is ensured through a single discriminator,\nindependently of the number of attributes. We show high quality, diversity and\nattribute control in the generated sentences through a series of automatic and\nhuman assessments. As the main application of our work, we test the potential\nof this new NLG model in a data augmentation scenario. In a downstream NLP\ntask, the sentences generated by our CGA model show significant improvements\nover a strong baseline, and a classification performance often comparable to\nadding same amount of additional real data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:31:16 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 12:23:16 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Russo", "Giuseppe", ""], ["Hollenstein", "Nora", ""], ["Musat", "Claudiu", ""], ["Zhang", "Ce", ""]]}, {"id": "2004.14989", "submitter": "Rachel Bawden", "authors": "Rachel Bawden and Biao Zhang and Lisa Yankovskaya and Andre T\\\"attar\n  and Matt Post", "title": "A Study in Improving BLEU Reference Coverage with Diverse Automatic\n  Paraphrasing", "comments": "Accepted in the Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a long-perceived shortcoming in the typical use of BLEU: its\nreliance on a single reference. Using modern neural paraphrasing techniques, we\nstudy whether automatically generating additional diverse references can\nprovide better coverage of the space of valid translations and thereby improve\nits correlation with human judgments. Our experiments on the into-English\nlanguage directions of the WMT19 metrics task (at both the system and sentence\nlevel) show that using paraphrased references does generally improve BLEU, and\nwhen it does, the more diverse the better. However, we also show that better\nresults could be achieved if those paraphrases were to specifically target the\nparts of the space most relevant to the MT outputs being evaluated. Moreover,\nthe gains remain slight even when human paraphrases are used, suggesting\ninherent limitations to BLEU's capacity to correctly exploit multiple\nreferences. Surprisingly, we also find that adequacy appears to be less\nimportant, as shown by the high results of a strong sampling approach, which\neven beats human paraphrases when used with sentence-level BLEU.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:34:52 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 16:17:54 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 21:43:56 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Bawden", "Rachel", ""], ["Zhang", "Biao", ""], ["Yankovskaya", "Lisa", ""], ["T\u00e4ttar", "Andre", ""], ["Post", "Matt", ""]]}, {"id": "2004.14992", "submitter": "Nicola De Cao", "authors": "Nicola De Cao, Michael Schlichtkrull, Wilker Aziz, Ivan Titov", "title": "How do Decisions Emerge across Layers in Neural Models? Interpretation\n  with Differentiable Masking", "comments": "Accepted at the 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP). Source code available at\n  https://github.com/nicola-decao/diffmask . 18 pages, 15 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribution methods assess the contribution of inputs to the model\nprediction. One way to do so is erasure: a subset of inputs is considered\nirrelevant if it can be removed without affecting the prediction. Though\nconceptually simple, erasure's objective is intractable and approximate search\nremains expensive with modern deep NLP models. Erasure is also susceptible to\nthe hindsight bias: the fact that an input can be dropped does not mean that\nthe model `knows' it can be dropped. The resulting pruning is over-aggressive\nand does not reflect how the model arrives at the prediction. To deal with\nthese challenges, we introduce Differentiable Masking. DiffMask learns to\nmask-out subsets of the input while maintaining differentiability. The decision\nto include or disregard an input token is made with a simple model based on\nintermediate hidden layers of the analyzed model. First, this makes the\napproach efficient because we predict rather than search. Second, as with\nprobing classifiers, this reveals what the network `knows' at the corresponding\nlayers. This lets us not only plot attribution heatmaps but also analyze how\ndecisions are formed across network layers. We use DiffMask to study BERT\nmodels on sentiment classification and question answering.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:36:14 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 15:51:43 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 10:12:19 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["De Cao", "Nicola", ""], ["Schlichtkrull", "Michael", ""], ["Aziz", "Wilker", ""], ["Titov", "Ivan", ""]]}, {"id": "2004.14996", "submitter": "He Bai", "authors": "He Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen\n  Gao and Ming Li", "title": "Segatron: Segment-Aware Transformer for Language Modeling and\n  Understanding", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers are powerful for sequence modeling. Nearly all state-of-the-art\nlanguage models and pre-trained language models are based on the Transformer\narchitecture. However, it distinguishes sequential tokens only with the token\nposition index. We hypothesize that better contextual representations can be\ngenerated from the Transformer with richer positional information. To verify\nthis, we propose a segment-aware Transformer (Segatron), by replacing the\noriginal token position encoding with a combined position encoding of\nparagraph, sentence, and token. We first introduce the segment-aware mechanism\nto Transformer-XL, which is a popular Transformer-based language model with\nmemory extension and relative position encoding. We find that our method can\nfurther improve the Transformer-XL base model and large model, achieving 17.1\nperplexity on the WikiText-103 dataset. We further investigate the pre-training\nmasked language modeling task with Segatron. Experimental results show that\nBERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla\nTransformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:38:27 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 22:29:36 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Bai", "He", ""], ["Shi", "Peng", ""], ["Lin", "Jimmy", ""], ["Xie", "Yuqing", ""], ["Tan", "Luchen", ""], ["Xiong", "Kun", ""], ["Gao", "Wen", ""], ["Li", "Ming", ""]]}, {"id": "2004.14999", "submitter": "Ilia Kuznetsov", "authors": "Ilia Kuznetsov, Iryna Gurevych", "title": "A Matter of Framing: The Impact of Linguistic Formalism on Probing\n  Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep pre-trained contextualized encoders like BERT (Delvin et al., 2019)\ndemonstrate remarkable performance on a range of downstream tasks. A recent\nline of research in probing investigates the linguistic knowledge implicitly\nlearned by these models during pre-training. While most work in probing\noperates on the task level, linguistic tasks are rarely uniform and can be\nrepresented in a variety of formalisms. Any linguistics-based probing study\nthereby inevitably commits to the formalism used to annotate the underlying\ndata. Can the choice of formalism affect probing results? To investigate, we\nconduct an in-depth cross-formalism layer probing study in role semantics. We\nfind linguistically meaningful differences in the encoding of semantic role-\nand proto-role information by BERT depending on the formalism and demonstrate\nthat layer probing can detect subtle differences between the implementations of\nthe same linguistic formalism. Our results suggest that linguistic formalism is\nan important dimension in probing studies, along with the commonly used\ncross-task and cross-lingual experimental settings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:45:16 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Kuznetsov", "Ilia", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2004.15001", "submitter": "Phillip Keung", "authors": "Phillip Keung, Yichao Lu, Julian Salazar, Vikas Bhardwaj", "title": "Don't Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of\n  Contextual Embeddings", "comments": "To appear in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual contextual embeddings have demonstrated state-of-the-art\nperformance in zero-shot cross-lingual transfer learning, where multilingual\nBERT is fine-tuned on one source language and evaluated on a different target\nlanguage. However, published results for mBERT zero-shot accuracy vary as much\nas 17 points on the MLDoc classification task across four papers. We show that\nthe standard practice of using English dev accuracy for model selection in the\nzero-shot setting makes it difficult to obtain reproducible results on the\nMLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even\nanti-correlated) with target language accuracy, and zero-shot performance\nvaries greatly at different points in the same fine-tuning run and between\ndifferent fine-tuning runs. These reproducibility issues are also present for\nother tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We\nrecommend providing oracle scores alongside zero-shot results: still fine-tune\nusing English data, but choose a checkpoint with the target dev set. Reporting\nthis upper bound makes results more consistent by avoiding arbitrarily bad\ncheckpoints.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:47:17 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 09:50:52 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Keung", "Phillip", ""], ["Lu", "Yichao", ""], ["Salazar", "Julian", ""], ["Bhardwaj", "Vikas", ""]]}, {"id": "2004.15003", "submitter": "Sho Yokoi", "authors": "Sho Yokoi, Ryo Takahashi, Reina Akama, Jun Suzuki, Kentaro Inui", "title": "Word Rotator's Distance", "comments": "17 pages, accepted at EMNLP 2020", "journal-ref": "EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key principle in assessing textual similarity is measuring the degree of\nsemantic overlap between two texts by considering the word alignment. Such\nalignment-based approaches are intuitive and interpretable; however, they are\nempirically inferior to the simple cosine similarity between general-purpose\nsentence vectors. To address this issue, we focus on and demonstrate the fact\nthat the norm of word vectors is a good proxy for word importance, and their\nangle is a good proxy for word similarity. Alignment-based approaches do not\ndistinguish them, whereas sentence-vector approaches automatically use the norm\nas the word importance. Accordingly, we propose a method that first decouples\nword vectors into their norm and direction, and then computes alignment-based\nsimilarity using earth mover's distance (i.e., optimal transport cost), which\nwe refer to as word rotator's distance. Besides, we find how to grow the norm\nand direction of word vectors (vector converter), which is a new systematic\napproach derived from sentence-vector estimation methods. On several textual\nsimilarity datasets, the combination of these simple proposed methods\noutperformed not only alignment-based approaches but also strong baselines. The\nsource code is available at https://github.com/eumesy/wrd\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:48:42 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 17:56:57 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 17:57:08 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yokoi", "Sho", ""], ["Takahashi", "Ryo", ""], ["Akama", "Reina", ""], ["Suzuki", "Jun", ""], ["Inui", "Kentaro", ""]]}, {"id": "2004.15006", "submitter": "Mihir Kale", "authors": "Mihir Kale, Abhinav Rastogi", "title": "Template Guided Text Generation for Task-Oriented Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri\nenable users to interact with a large number of services and APIs on the web\nusing natural language. In this work, we investigate two methods for Natural\nLanguage Generation (NLG) using a single domain-independent model across a\nlarge number of APIs. First, we propose a schema-guided approach which\nconditions the generation on a schema describing the API in natural language.\nOur second method investigates the use of a small number of templates, growing\nlinearly in number of slots, to convey the semantics of the API. To generate\nutterances for an arbitrary slot combination, a few simple templates are first\nconcatenated to give a semantically correct, but possibly incoherent and\nungrammatical utterance. A pre-trained language model is subsequently employed\nto rewrite it into coherent, natural sounding text. Through automatic metrics\nand human evaluation, we show that our method improves over strong baselines,\nis robust to out-of-domain inputs and shows improved sample efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:51:08 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 21:08:36 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kale", "Mihir", ""], ["Rastogi", "Abhinav", ""]]}, {"id": "2004.15008", "submitter": "Nelson F. Liu", "authors": "Nelson F. Liu and Daniel Hershcovich and Michael Kranzlein and Nathan\n  Schneider", "title": "Lexical Semantic Recognition", "comments": "11 pages, 3 figures; to appear at MWE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In lexical semantics, full-sentence segmentation and segment labeling of\nvarious phenomena are generally treated separately, despite their\ninterdependence. We hypothesize that a unified lexical semantic recognition\ntask is an effective way to encapsulate previously disparate styles of\nannotation, including multiword expression identification / classification and\nsupersense tagging. Using the STREUSLE corpus, we train a neural CRF sequence\ntagger and evaluate its performance along various axes of annotation. As the\nlabel set generalizes that of previous tasks (PARSEME, DiMSUM), we additionally\nevaluate how well the model generalizes to those test sets, finding that it\napproaches or surpasses existing models despite training only on STREUSLE. Our\nwork also establishes baseline models and evaluation metrics for integrated and\naccurate modeling of lexical semantics, facilitating future work in this area.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:52:11 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 20:12:02 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Nelson F.", ""], ["Hershcovich", "Daniel", ""], ["Kranzlein", "Michael", ""], ["Schneider", "Nathan", ""]]}, {"id": "2004.15011", "submitter": "Isabel Cachola", "authors": "Isabel Cachola, Kyle Lo, Arman Cohan, Daniel S. Weld", "title": "TLDR: Extreme Summarization of Scientific Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce TLDR generation, a new form of extreme summarization, for\nscientific papers. TLDR generation involves high source compression and\nrequires expert background knowledge and understanding of complex\ndomain-specific language. To facilitate study on this task, we introduce\nSciTLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR\ncontains both author-written and expert-derived TLDRs, where the latter are\ncollected using a novel annotation protocol that produces high-quality\nsummaries while minimizing annotation burden. We propose CATTS, a simple yet\neffective learning strategy for generating TLDRs that exploits titles as an\nauxiliary training signal. CATTS improves upon strong baselines under both\nautomated metrics and human evaluations. Data and code are publicly available\nat https://github.com/allenai/scitldr.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:56:18 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 09:09:24 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 22:41:44 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Cachola", "Isabel", ""], ["Lo", "Kyle", ""], ["Cohan", "Arman", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2004.15012", "submitter": "Charles Lovering J", "authors": "Rohan Jha, Charles Lovering, Ellie Pavlick", "title": "Does Data Augmentation Improve Generalization in NLP?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models often exploit superficial features to achieve good performance,\nrather than deriving more general features. Overcoming this tendency is a\ncentral challenge in areas such as representation learning and ML fairness.\nRecent work has proposed using data augmentation, i.e., generating training\nexamples where the superficial features fail, as a means of encouraging models\nto prefer the stronger features. We design a series of toy learning problems to\ntest the hypothesis that data augmentation leads models to unlearn weaker\nheuristics, but not to learn stronger features in their place. We find partial\nsupport for this hypothesis: Data augmentation often hurts before it helps, and\nit is less effective when the preferred strong feature is much more difficult\nto extract than the competing weak feature.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:56:30 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 14:33:27 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Jha", "Rohan", ""], ["Lovering", "Charles", ""], ["Pavlick", "Ellie", ""]]}, {"id": "2004.15015", "submitter": "Eric Wallace", "authors": "Eric Wallace, Mitchell Stern, Dawn Song", "title": "Imitation Attacks and Defenses for Black-box Machine Translation Systems", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversaries may look to steal or attack black-box NLP systems, either for\nfinancial gain or to exploit model errors. One setting of particular interest\nis machine translation (MT), where models have high commercial value and errors\ncan be costly. We investigate possible exploits of black-box MT systems and\nexplore a preliminary defense against such threats. We first show that MT\nsystems can be stolen by querying them with monolingual sentences and training\nmodels to imitate their outputs. Using simulated experiments, we demonstrate\nthat MT model stealing is possible even when imitation models have different\ninput data or architectures than their target models. Applying these ideas, we\ntrain imitation models that reach within 0.6 BLEU of three production MT\nsystems on both high-resource and low-resource language pairs. We then leverage\nthe similarity of our imitation models to transfer adversarial examples to the\nproduction systems. We use gradient-based attacks that expose inputs which lead\nto semantically-incorrect translations, dropped content, and vulgar model\noutputs. To mitigate these vulnerabilities, we propose a defense that modifies\ntranslation outputs in order to misdirect the optimization of imitation models.\nThis defense degrades the adversary's BLEU score and attack success rate at\nsome cost in the defender's BLEU and inference speed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:56:49 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 22:05:02 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2021 19:05:24 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wallace", "Eric", ""], ["Stern", "Mitchell", ""], ["Song", "Dawn", ""]]}, {"id": "2004.15016", "submitter": "Jose Camacho-Collados", "authors": "Anna Breit and Artem Revenko and Kiamehr Rezaee and Mohammad Taher\n  Pilehvar and Jose Camacho-Collados", "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words\n  in Context", "comments": "Accepted to EACL 2021. Reference paper of the SemDeep WiC-TSV\n  challenge: https://competitions.codalab.org/competitions/23683", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present WiC-TSV, a new multi-domain evaluation benchmark for Word Sense\nDisambiguation. More specifically, we introduce a framework for Target Sense\nVerification of Words in Context which grounds its uniqueness in the\nformulation as a binary classification task thus being independent of external\nsense inventories, and the coverage of various domains. This makes the dataset\nhighly flexible for the evaluation of a diverse set of models and systems in\nand across domains. WiC-TSV provides three different evaluation settings,\ndepending on the input signals provided to the model. We set baseline\nperformance on the dataset using state-of-the-art language models. Experimental\nresults show that even though these models can perform decently on the task,\nthere remains a gap between machine and human performance, especially in\nout-of-domain settings. WiC-TSV data is available at\nhttps://competitions.codalab.org/competitions/23683\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:57:27 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 14:07:54 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 23:43:40 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Breit", "Anna", ""], ["Revenko", "Artem", ""], ["Rezaee", "Kiamehr", ""], ["Pilehvar", "Mohammad Taher", ""], ["Camacho-Collados", "Jose", ""]]}, {"id": "2004.15020", "submitter": "Yinfei Yang", "authors": "Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, Yinfei Yang", "title": "Crisscrossed Captions: Extended Intramodal and Intermodal Semantic\n  Similarity Judgments for MS-COCO", "comments": "To be presented at EACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By supporting multi-modal retrieval training and evaluation, image captioning\ndatasets have spurred remarkable progress on representation learning.\nUnfortunately, datasets have limited cross-modal associations: images are not\npaired with other images, captions are only paired with other captions of the\nsame image, there are no negative associations and there are missing positive\ncross-modal associations. This undermines research into how inter-modality\nlearning impacts intra-modality tasks. We address this gap with Crisscrossed\nCaptions (CxC), an extension of the MS-COCO dataset with human semantic\nsimilarity judgments for 267,095 intra- and inter-modality pairs. We report\nbaseline results on CxC for strong existing unimodal and multimodal models. We\nalso evaluate a multitask dual encoder trained on both image-caption and\ncaption-caption pairs that crucially demonstrates CxC's value for measuring the\ninfluence of intra- and inter-modality learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:59:17 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 07:07:52 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 06:43:05 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Parekh", "Zarana", ""], ["Baldridge", "Jason", ""], ["Cer", "Daniel", ""], ["Waters", "Austin", ""], ["Yang", "Yinfei", ""]]}]